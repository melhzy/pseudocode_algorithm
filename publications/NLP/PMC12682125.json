{
  "pmcid": "PMC12682125",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:17.781118",
  "metadata": {
    "journal_title": "iScience",
    "journal_nlm_ta": "iScience",
    "journal_iso_abbrev": "iScience",
    "journal": "iScience",
    "pmcid": "PMC12682125",
    "pmid": "41362614",
    "doi": "10.1016/j.isci.2025.113984",
    "title": "Multi agent large language models for biomedical hypothesis generation in drug combination discovery",
    "year": "2025",
    "month": "11",
    "day": "10",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "10"
    },
    "authors": [
      "Xu Qidi",
      "Soto Claudio",
      "Shahnawaz Mohammad",
      "Liu Xiaozhong",
      "Jiang Xiaoqian",
      "Kim Yejin"
    ],
    "abstract": "Summary Recent advancements in large language models (LLMs) have demonstrated their potential in scientific reasoning, but their ability to open-ended hypotheses in data-scarce domains remains underexplored. Here, we introduce  Co mbinatorial  A lzheimer’s Disease  T herapeutic  E fficacy  D ecision (Coated-LLM), an AI-driven framework that is inspired by scientific collaboration to predict efficacious combinatorial therapy when data-driven prediction is infeasible. Coated-LLM employs multiple specialized LLM agents— Researcher ,  Reviewer s , and  Moderator —to systematically generate and evaluate hypotheses through several in-context learning techniques. Using Alzheimer’s disease (AD) as a test case, Coated-LLM outperformed traditional knowledge-based methods (accuracy: 0.74 vs. 0.52), with external validation achieving an accuracy of 0.82. In addition, a drug combination predicted from Coated-LLM was experimentally validated to significantly reduce amyloid aggregation  in vitro . These findings highlight the potential of our framework to augment human reasoning in complex scientific reasoning tasks, offering a scalable approach for hypothesis generation in biomedical research.",
    "keywords": [
      "Health sciences",
      "Medicine",
      "Drugs",
      "Artificial intelligence"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">iScience</journal-id><journal-id journal-id-type=\"iso-abbrev\">iScience</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3532</journal-id><journal-id journal-id-type=\"pmc-domain\">isci</journal-id><journal-title-group><journal-title>iScience</journal-title></journal-title-group><issn pub-type=\"epub\">2589-0042</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682125</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682125.1</article-id><article-id pub-id-type=\"pmcaid\">12682125</article-id><article-id pub-id-type=\"pmcaiid\">12682125</article-id><article-id pub-id-type=\"pmid\">41362614</article-id><article-id pub-id-type=\"doi\">10.1016/j.isci.2025.113984</article-id><article-id pub-id-type=\"pii\">S2589-0042(25)02245-X</article-id><article-id pub-id-type=\"publisher-id\">113984</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi agent large language models for biomedical hypothesis generation in drug combination discovery</article-title></title-group><contrib-group><contrib contrib-type=\"author\" id=\"au1\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"Q\">Qidi</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\" id=\"au2\"><name name-style=\"western\"><surname>Soto</surname><given-names initials=\"C\">Claudio</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\" id=\"au3\"><name name-style=\"western\"><surname>Shahnawaz</surname><given-names initials=\"M\">Mohammad</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\" id=\"au4\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"X\">Xiaozhong</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\" id=\"au5\"><name name-style=\"western\"><surname>Jiang</surname><given-names initials=\"X\">Xiaoqian</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\" id=\"au6\"><name name-style=\"western\"><surname>Kim</surname><given-names initials=\"Y\">Yejin</given-names></name><email>yejin.kim@uth.tmc.edu</email><xref rid=\"aff1\" ref-type=\"aff\">1</xref><xref rid=\"fn1\" ref-type=\"fn\">4</xref><xref rid=\"cor1\" ref-type=\"corresp\">&#8727;</xref></contrib><aff id=\"aff1\"><label>1</label>McWilliams School of Biomedical Informatics, UTHealth Houston, Houston, TX 77030, US</aff><aff id=\"aff2\"><label>2</label>McGovern Medical School, UTHealth Houston, Houston, TX 77030, US</aff><aff id=\"aff3\"><label>3</label>Computer Science and Data Science, Worcester Polytechnic Institute, Worcester, MA 01609, US</aff></contrib-group><author-notes><corresp id=\"cor1\"><label>&#8727;</label>Corresponding author <email>yejin.kim@uth.tmc.edu</email></corresp><fn id=\"fn1\"><label>4</label><p id=\"ntpara0010\">Lead contact</p></fn></author-notes><pub-date pub-type=\"collection\"><day>19</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"epub\"><day>10</day><month>11</month><year>2025</year></pub-date><volume>28</volume><issue>12</issue><issue-id pub-id-type=\"pmc-issue-id\">501445</issue-id><elocation-id>113984</elocation-id><history><date date-type=\"received\"><day>1</day><month>7</month><year>2025</year></date><date date-type=\"rev-recd\"><day>24</day><month>9</month><year>2025</year></date><date date-type=\"accepted\"><day>5</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>10</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>08</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 09:25:13.687\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Author(s)</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"main.pdf\"/><abstract id=\"abs0010\"><title>Summary</title><p>Recent advancements in large language models (LLMs) have demonstrated their potential in scientific reasoning, but their ability to open-ended hypotheses in data-scarce domains remains underexplored. Here, we introduce <bold>Co</bold>mbinatorial <bold>A</bold>lzheimer&#8217;s Disease <bold>T</bold>herapeutic <bold>E</bold>fficacy <bold>D</bold>ecision (Coated-LLM), an AI-driven framework that is inspired by scientific collaboration to predict efficacious combinatorial therapy when data-driven prediction is infeasible. Coated-LLM employs multiple specialized LLM agents&#8212;<italic toggle=\"yes\">Researcher</italic>, <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic>, and <italic toggle=\"yes\">Moderator</italic>&#8212;to systematically generate and evaluate hypotheses through several in-context learning techniques. Using Alzheimer&#8217;s disease (AD) as a test case, Coated-LLM outperformed traditional knowledge-based methods (accuracy: 0.74 vs. 0.52), with external validation achieving an accuracy of 0.82. In addition, a drug combination predicted from Coated-LLM was experimentally validated to significantly reduce amyloid aggregation <italic toggle=\"yes\">in vitro</italic>. These findings highlight the potential of our framework to augment human reasoning in complex scientific reasoning tasks, offering a scalable approach for hypothesis generation in biomedical research.</p></abstract><abstract abstract-type=\"graphical\" id=\"abs0015\"><title>Graphical abstract</title><fig id=\"undfig1\" position=\"anchor\" orientation=\"portrait\"><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fx1.jpg\"/></fig></abstract><abstract abstract-type=\"author-highlights\" id=\"abs0020\"><title>Highlights</title><p><list list-type=\"simple\" id=\"ulist0010\"><list-item id=\"u0010\"><label>&#8226;</label><p id=\"p0010\">Multi-agent LLM enhances human hypothesis generation in open scientific domains</p></list-item><list-item id=\"u0015\"><label>&#8226;</label><p id=\"p0015\">Few-shot LLM framework offers a viable alternative when less data is available</p></list-item><list-item id=\"u0020\"><label>&#8226;</label><p id=\"p0020\">M266 + Gypenoside XVII validated to significantly reduce amyloid aggregation <italic toggle=\"yes\">in vitro</italic></p></list-item></list></p></abstract><abstract abstract-type=\"teaser\" id=\"abs0025\"><p>Health sciences; Medicine; Drugs; Artificial intelligence</p></abstract><kwd-group id=\"kwrds0010\"><title>Subject areas</title><kwd>Health sciences</kwd><kwd>Medicine</kwd><kwd>Drugs</kwd><kwd>Artificial intelligence</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta><notes><p id=\"misc9010\">Published: November 10, 2025</p></notes></front><body><sec id=\"sec1\"><title>Introduction</title><p id=\"p0025\">Recent advancements in large language models (LLMs) have demonstrated their disruptive potential in scientific discovery. These models have efficiently tackled combinatorial optimization problems, often surpassing traditional heuristics.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref><sup>,</sup><xref rid=\"bib2\" ref-type=\"bibr\"><sup>2</sup></xref> LLMs have also shown success in various chemistry and materials science tasks, such as predicting molecular properties and chemical reaction yields,<xref rid=\"bib3\" ref-type=\"bibr\"><sup>3</sup></xref> and autonomously searching for chemicals.<xref rid=\"bib4\" ref-type=\"bibr\"><sup>4</sup></xref><sup>,</sup><xref rid=\"bib5\" ref-type=\"bibr\"><sup>5</sup></xref> Our research aligns with efforts to develop &#8220;autonomous scientists&#8221; inspired by scientific collaboration.</p><p id=\"p0030\">In scientific investigation, given a question, human researchers apply deductive and inductive reasoning to drive predictions and draw conclusions.<xref rid=\"bib6\" ref-type=\"bibr\"><sup>6</sup></xref> However, such traditional scientific reasoning is often limited by human bias and cognitive capacity.<xref rid=\"bib6\" ref-type=\"bibr\"><sup>6</sup></xref> Researchers may exhibit confirmation bias, favoring data that supports their preconceptions, struggle to process the vast amount of existing literature, leading to incomplete reviews and overlooked insights, and have difficulty in managing multiple factors and identifying subtle patterns.</p><p id=\"p0035\">LLMs can mitigate these limitations by assisting human scientific reasoning, as evidenced in several prior studies.<xref rid=\"bib7\" ref-type=\"bibr\"><sup>7</sup></xref><sup>,</sup><xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref><sup>,</sup><xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> Recent reasoning-oriented LLMs, such as OpenAI o1 and DeepSeek V3,<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref> have achieved human-level performance on closed-form questions where the answer is already known, but remain underexplored for open-ended biomedical hypothesis generation due to the unclear reasoning and answers. While data-driven machine learning models work well when abundant data exists, our focus is on data-scarce research areas, which are more common in real world scientific discovery. Recent work by Qi et al. has demonstrated that LLMs can generate novel and validated biomedical hypotheses.<xref rid=\"bib11\" ref-type=\"bibr\"><sup>11</sup></xref><sup>,</sup><xref rid=\"bib12\" ref-type=\"bibr\"><sup>12</sup></xref> However, their approach requires background knowledge extracted from existing literature as input to generate hypotheses, which may not be available in data-scarce domains. Building on this motivation, we chose a specific biomedical question that requires deep domain knowledge and critical reasoning without definite answers to test LLM&#8217;s capacity beyond memorization: identifying effective drug combinations for <italic toggle=\"yes\">in vivo</italic> experiments in complex systemic diseases. We chose to focus on Alzheimer&#8217;s disease (AD), a complex neurodegenerative condition where multiple disease etiologies entangle together; thus, a comprehensive consideration of multiple underlying mechanisms is critical when developing therapeutics.</p><p id=\"p0040\">Drug combination therapy is to use of two or more therapeutic agents to treat a single disease, to achieve a more effective treatment outcome than what could be achieved with a single drug. This approach is particularly prevalent in the treatment of complex diseases such as diabetes and metabolic syndrome, cardiovascular disease, cancer, and others,<xref rid=\"bib13\" ref-type=\"bibr\"><sup>13</sup></xref><sup>,</sup><xref rid=\"bib14\" ref-type=\"bibr\"><sup>14</sup></xref><sup>,</sup><xref rid=\"bib15\" ref-type=\"bibr\"><sup>15</sup></xref><sup>,</sup><xref rid=\"bib16\" ref-type=\"bibr\"><sup>16</sup></xref><sup>,</sup><xref rid=\"bib17\" ref-type=\"bibr\"><sup>17</sup></xref><sup>,</sup><xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref><sup>,</sup><xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref><sup>,</sup><xref rid=\"bib20\" ref-type=\"bibr\"><sup>20</sup></xref> but no successes in AD due to the multifactorial nature of the disease. Until now, only Donepezil + Memantine has been FDA-approved for AD. Developing effective combinatorial therapies faces significant challenges, particularly in selecting the right therapeutic agents (drugs) and relevant <italic toggle=\"yes\">in vivo</italic> models. Researchers make specific predictions about the potential efficacy of various combinations from general principles and known mechanisms of action (deductive reasoning). The complexity arises from the factorial growth in the number of possible combinations (therapeutic agent 1 &#8727; therapeutic agent 2 &#8727; <italic toggle=\"yes\">in vivo</italic> model), making manual evaluation impractical.</p><p id=\"p0045\">Only a few studies have explored the use of LLMs for such combinatorial search problems. A recent study<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> introduced CancerGPT, an LLM-based model designed for predicting drug pair synergy in rare tissues with limited data. It demonstrates the capability of LLMs to handle complex biological inference tasks. However, CancerGPT primarily focuses on fine-tuning LLMs using high-throughput <italic toggle=\"yes\">in vitro</italic> experimental data, which is not available for most complex diseases. In contrast, we propose a versatile and generalizable LLM-based framework, named <bold>Co</bold>mbinatorial <bold>A</bold>lzheimer&#8217;s disease <bold>T</bold>herapeutic <bold>E</bold>fficacy <bold>D</bold>ecision (Coated-LLM), designed specifically to generate biomedical hypotheses even in data-scarce settings. Our innovative approach, inspired by collective human scientific reasoning, employs multiple specialized LLM agents (<italic toggle=\"yes\">Researcher</italic>, <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic>, <italic toggle=\"yes\">Moderator)</italic> to generate biomedical hypotheses. Critically, our framework identified a novel combinational therapy, m266 antibody with Gypenoside XVII, that experimentally demonstrated superior inhibition of amyloid beta aggregation compared to individual treatments alone, highlighting a previously unexplored synergistic combinatorial therapy in AD. Comprehensive computational evaluation alongside these <italic toggle=\"yes\">in vitro</italic> validations demonstrates that Coated-LLM effectively identifies potent therapeutic combinations, significantly augmenting human capabilities for scientific discovery in complex diseases.</p></sec><sec id=\"sec2\"><title>Results</title><sec id=\"sec2.1\"><title>Model summary</title><p id=\"p0050\">We create an AI model <italic toggle=\"yes\">f</italic> that automates scientific reasoning to generate hypotheses on efficacious combinatorial therapy for <italic toggle=\"yes\">in vivo</italic> experiments (<xref rid=\"tbox1\" ref-type=\"boxed-text\">Algorithm 1</xref>; <xref rid=\"fig1\" ref-type=\"fig\">Figure 1</xref>). Inspired by human scientific collaboration, our framework consists of multiple LLM agents playing different roles: <italic toggle=\"yes\">Researcher</italic>, <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic>, and <italic toggle=\"yes\">Moderator</italic>. The <italic toggle=\"yes\">R</italic><italic toggle=\"yes\">esearcher</italic> generates a series of reasoning steps to propose a prediction on the efficacy of combinatorial therapeutic agents. Multiple <italic toggle=\"yes\">Reviewers</italic> review and criticize the quality of the prediction generated by the <italic toggle=\"yes\">Researcher</italic> and offer feedback. Finally, the <italic toggle=\"yes\">Moderator</italic> integrates <italic toggle=\"yes\">Researcher</italic>&#8217;s proposed prediction and <italic toggle=\"yes\">Reviewers</italic>&#8217; feedback to suggest a more valid prediction.<boxed-text id=\"tbox1\" position=\"float\" orientation=\"portrait\"><label>Algorithm 1</label><caption><title>Framework for Drug Combination Efficacy in Alzheimer's Disease</title></caption><p id=\"p00215\">\nInput: Dataset D = {(t1, t2, m)}, external knowledge B_q\n</p><p id=\"p02020\">\nOutput: Predictions A_Q for test set\n</p><p id=\"p00252\">\nSplit D into training set D_train and test set D_test\n</p><p id=\"p00302\">\n# Phase I - Warm Up:\n</p><p id=\"p00352\">\n&#160;\nlearning_examples=[]\n</p><p id=\"p00420\">\n&#160;\nlearning_question_embeddings = []\n</p><p id=\"p00425\">\n&#160;\nfor each (t1, t2, m) in D_train\n</p><p id=\"p00520\">\n&#160;\n&#160;\nq = transform_into_question(t1, t2, m)\n</p><p id=\"p00525\">\n&#160;\n&#160;\nB_q = search_external_knowledge(t1, t2)\n</p><p id=\"p00620\">\n&#160;\n&#160;\nE_q = generate_learning_question_embedding(q)\n</p><p id=\"p00265\">\n&#160;\n&#160;\n(C_q, A_q) = LLM_<italic toggle=\"yes\">Researcher</italic>(instruction, q, B_q)\n</p><p id=\"p00720\">\n&#160;\n&#160;\nif correct(A_q) then\n</p><p id=\"p00725\">\n&#160;\n&#160;\n&#160;\nlearning_examples.append((q, C_q, A_q))\n</p><p id=\"p00280\">\n&#160;\n&#160;\n&#160;learning_question_embeddings.append(E_q)\n</p><p id=\"p00825\">\n# Phase II - Inference:\n</p><p id=\"p00290\">\n&#160;\nfor each (t1, t2, m) in D_test\n</p><p id=\"p00925\">\n&#160;\n&#160;\nQ = transform_into_question(t1, t2, m)\n</p><p id=\"p01020\">\n&#160;\n&#160;\nB_Q = search_external_knowledge(t1, t2)\n</p><p id=\"p01205\">\n&#160;\n&#160;\nE_Q = generate_testing_question_embedding(Q)\n</p><p id=\"p01210\">\n&#160;\n&#160;\ntop_k_examples = select_top_k_similar_question(E_Q, learning_question_embeddings, learning_examples, k=5)\n</p><p id=\"p01215\">\n&#160;\n&#160;\ninput = concatenate(top_k_examples, Q, B_Q)\n</p><p id=\"p01220\">\n&#160;\n&#160;\nhypotheses = []\n</p><p id=\"p01225\">\n&#160;\n&#160;\nRepeat 5 times do\n</p><p id=\"p01320\">\n&#160;\n&#160;\n&#160;\n&#160;(C_Q, A_Q) = LLM_<italic toggle=\"yes\">Researcher</italic>(instruction, input)\n</p><p id=\"p01352\">\n&#160;\n&#160;\n&#160;\nhypotheses.append((C_Q, A_Q))\n</p><p id=\"p01420\">\n&#160;\n&#160;\nA_Q*= majority_vote([A_Q for _, A_Q in hypotheses])\n</p><p id=\"p01245\">\n&#160;\n&#160;\nC_Q*=select_longest_CoT([C_Q for C_Q, A_Q in hypotheses if A_Q==A_Q*])\n</p><p id=\"p01250\">\n# Phase III - Revision\n</p><p id=\"p01525\">\n&#160;\nfor each (t1, t2, m) in D_test\n</p><p id=\"p01620\">\n&#160;\n&#160;\nQ = transform into_question(t1, t2, m)\n</p><p id=\"p01625\">\n&#160;\n&#160;\nF_Q = LLM_<italic toggle=\"yes\">Reviewers</italic>(instruction, C_Q*, A_Q*)\n</p><p id=\"p01720\">\n&#160;\n&#160;\nhypotheses = []\n</p><p id=\"p01725\">\n&#160;\n&#160;\nRepeat 5 times\n</p><p id=\"p01820\">\n&#160;\n&#160;\n&#160;\n(C_Q, A_Q) = LLM_<italic toggle=\"yes\">Moderator</italic>(instruction, Q, F_Q, C_Q*, A_Q*)\n</p><p id=\"p01825\">\n&#160;\n&#160;\n&#160;\nhypotheses.append((C_Q, A_Q))\n</p><p id=\"p01920\">\n&#160;\n&#160;\n&#160;\nA_Q* = majority_vote([A_Q for _, A_Q in hypotheses]))\n</p><p id=\"p01925\">\n&#160;\n&#160;\n&#160;\nReturn A_Q*\n</p></boxed-text><fig id=\"fig1\" position=\"float\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Study overview</p><p>(A) Traditional approach: Given the vast number of possible drug combinations, human experts rely on general principles and known mechanisms of action to manually select potential candidates. The top-score combinations are then subjected to <italic toggle=\"yes\">in vitro</italic> experiments to evaluate their efficacy.</p><p>(B) Coated-LLM workflow: Coated-LLM is a structured framework that was inspired by collective scientific discovery processes to generate hypotheses on efficacious combinatorial therapy. For a target drug combination, <italic toggle=\"yes\">Researcher</italic> learns from its top five relevant questions from learning examples, generates a series of reasoning steps to propose a prediction on the efficacy, and gets a consistent prediction. Multiple <italic toggle=\"yes\">Reviewers</italic> then provide feedback, and the <italic toggle=\"yes\">Moderator</italic> integrates consistency prediction from the <italic toggle=\"yes\">Researcher</italic> and feedback from the <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> to generate the final consensus prediction.</p><p>(C) <italic toggle=\"yes\">In vitro</italic> Experiments: Coated-LLM selects the most promising drug combinations from false-positive augmented data for <italic toggle=\"yes\">in vitro</italic> efficacy testing.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr1.jpg\"/></fig></p><p id=\"p0055\">Throughout all the communication among LLM agents, we prompt the LLM agents to utilize various in-context learning techniques such as integrating external biomedical knowledge as retrieval augmented generation (RAG),<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref> few-shot learning,<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref> self-generated chain-of-thoughts (CoT),<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref> and/or tree of thoughts (ToT),<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref> and self-consistency.<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></p></sec><sec id=\"sec2.2\"><title>Data collection and augmentation</title><p id=\"p0060\">Following literature mining, we identified 242 articles reporting 250 drug combinations with positive efficacy and 30 with negative efficacy (<xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>A). In comparison, even the rare tissue subsets in the CancerGPT study (e.g., 352 samples for soft tissue, 1,190 for stomach)<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> are significantly larger. This highlights the relative scarcity of AD-related combination data. In addition, our literature collection showed severe positive bias. To address it, we performed data augmentation. As a result, we have a total of 530 combinations (250 combinations with positive efficacy; 30 combinations with negative efficacy, augmented by 250 combinations with noisy, non-positive efficacy). <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>B summarizes the top five most frequently mentioned terms across therapeutic agents, animal models, and pathways.<fig id=\"fig2\" position=\"float\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Distribution of drug combinations and efficacy in the literature</p><p>(A) Data collection from literature. The process began with an initial pool of articles from the AlzPED, followed by additional searches conducted in PubMed. Articles were screened and excluded based on predefined criteria. The final selected literature included articles that reported drug combinations with positive or negative efficacy.</p><p>(B) Top 5 frequent terms in therapeutic agents, animal models, and pathways.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr2.jpg\"/></fig></p></sec><sec id=\"sec2.3\"><title>Model development</title><p id=\"p0065\">In the warmup phase, the <italic toggle=\"yes\">Researcher</italic> correctly predicted 231 training combinations (134 combinations with positive efficacy; 97 combinations with non-positive efficacy), which became learning examples for the inference phase. Following,<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> we used k = 5 examples for the few-shot learning, achieving a high average similarity score between test questions and selected five examples (0.919 &#177; 0.017), while also maintaining contextual diversity among selected examples. Further, to demonstrate that the selected questions were more relevant to the test questions than other questions from the learning examples in the inference phase, we calculated the mean cosine distance between the test question embeddings and the learning examples' embeddings. The mean top five average cosine distance was 0.08 (variance: 0.0002), while the mean overall average cosine distance was 0.13 (variance: 0.0003). <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Figure S2</xref> provided a visual representation of similarities between the target combination and learning examples. In the revision phase, among 156 testing combinations, 129 demonstrated more than 80% consistency across 5 rounds, with 83 of them achieving 100% consistency.</p></sec><sec id=\"sec2.4\"><title>Coated-LLM achieved significant accuracy in predicting drug combination efficacy</title><p id=\"p0070\">The Coated-LLM framework significantly surpassed the traditional network-based approach (no data-driven machine learning models available) in predicting the efficacy of drug combinations. Specifically, on a test set with 156 drug combinations, Coated-LLM achieved an accuracy of 0.74, precision of 0.71, recall of 0.80, and an F1-score of 0.75, with an average confidence of 0.87 and ECE of 0.17. <xref rid=\"tbl1\" ref-type=\"table\">Table 1</xref> presents the contingency table for Coated-LLM&#8217;s predictions and examples of misclassifications. In comparison, the traditional network-based model yielded substantially lower performance metrics, with an accuracy of 0.52, precision of 0.46, recall of 0.16, and an F1 score of 0.24 (<xref rid=\"mmc1\" ref-type=\"supplementary-material\">Table S1</xref>). Our model demonstrates superior flexibility by effectively accommodating therapeutic agents beyond conventional pharmaceuticals, such as membrane-free stem cell extracts, for which traditional gene-target data are often unavailable. This significant accuracy, even without data-driven model training, underscores the ability of Coated-LLM to identify effective combinatorial therapies in AD in a scalable way.<table-wrap position=\"float\" id=\"tbl1\" orientation=\"portrait\"><label>Table 1</label><caption><p>Contingency table of prediction outcomes with examples using Coated-LLM</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 1</th><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 2</th><th colspan=\"1\" rowspan=\"1\">Animal model</th><th colspan=\"1\" rowspan=\"1\">Predicted efficacy</th><th colspan=\"1\" rowspan=\"1\">Actual efficacy (reference)</th></tr></thead><tbody><tr><td colspan=\"5\" rowspan=\"1\"><bold>True positive (<italic toggle=\"yes\">n</italic> = 61)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Lycopene</td><td colspan=\"1\" rowspan=\"1\">Vitamin E</td><td colspan=\"1\" rowspan=\"1\">Tau P301L</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Positive<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Donepezil</td><td colspan=\"1\" rowspan=\"1\">Fluoroethylnormemantine</td><td colspan=\"1\" rowspan=\"1\">Swiss OF-1 mice</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Positive<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>False positive (<italic toggle=\"yes\">n</italic> = 25)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Cholesterol</td><td colspan=\"1\" rowspan=\"1\">Homocysteine</td><td colspan=\"1\" rowspan=\"1\">Sprague-Dawley rats</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">m266</td><td colspan=\"1\" rowspan=\"1\">Gypenoside XVII</td><td colspan=\"1\" rowspan=\"1\">3xTg-AD transgenic mice</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive (Augmented data)</td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>False negative (<italic toggle=\"yes\">n</italic> = 1</bold>5<bold>)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Gallic Acid</td><td colspan=\"1\" rowspan=\"1\">Sodium Arsenite</td><td colspan=\"1\" rowspan=\"1\">Male rats</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Positive<xref rid=\"bib29\" ref-type=\"bibr\"><sup>29</sup></xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">AMD3100</td><td colspan=\"1\" rowspan=\"1\">L-Lactate</td><td colspan=\"1\" rowspan=\"1\">3xTg</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Positive<xref rid=\"bib30\" ref-type=\"bibr\"><sup>30</sup></xref></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>True negative (<italic toggle=\"yes\">n</italic> = 5</bold>5<bold>)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Atorvastatin</td><td colspan=\"1\" rowspan=\"1\">Farnesol</td><td colspan=\"1\" rowspan=\"1\">C57BL/6</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive<xref rid=\"bib31\" ref-type=\"bibr\"><sup>31</sup></xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Galantamine</td><td colspan=\"1\" rowspan=\"1\">Mecamylamine</td><td colspan=\"1\" rowspan=\"1\">ICR</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive<xref rid=\"bib32\" ref-type=\"bibr\"><sup>32</sup></xref></td></tr></tbody></table></table-wrap></p></sec><sec id=\"sec2.5\"><title>Retrospective <italic toggle=\"yes\">in vitro</italic> validation</title><p id=\"p0075\">To evaluate the generalizability of Coated-LLM and address data leakage concerns, we conducted <italic toggle=\"yes\">in vitro</italic> validation with an independent private dataset. This dataset comprises eleven drug combinations and <italic toggle=\"yes\">in vitro</italic> efficacy in cell lines, and includes nine non-positive and two positive efficacies, skewed toward negative efficacy. Although our model is developed for predicting <italic toggle=\"yes\">in vivo</italic> efficacy, we assume that the model for <italic toggle=\"yes\">in vivo</italic> efficacy can also capture <italic toggle=\"yes\">in vitro</italic> efficacy. Despite the increased difficulty due to realistic distribution skew, Coated-LLM achieved an accuracy of 0.82 (<xref rid=\"tbl2\" ref-type=\"table\">Table 2</xref>), whereas the baseline network-based model achieved 0.27 (<xref rid=\"mmc1\" ref-type=\"supplementary-material\">Table S2</xref>). <xref rid=\"tbl2\" ref-type=\"table\">Table 2</xref> presents the contingency table for Coated-LLM&#8217;s predictions and examples of misclassifications in the external validation. The precision, recall, and F1-score for our model were each 0.50, reflecting the challenging nature of the task but still outperforming the baseline.<table-wrap position=\"float\" id=\"tbl2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Contingency table of prediction outcomes for the external data with examples</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 1</th><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 2</th><th colspan=\"1\" rowspan=\"1\">Model</th><th colspan=\"1\" rowspan=\"1\">Predicted efficacy</th><th colspan=\"1\" rowspan=\"1\">Actual efficacy</th></tr></thead><tbody><tr><td colspan=\"5\" rowspan=\"1\"><bold>True positive (<italic toggle=\"yes\">n</italic> = 1)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Galantamine</td><td colspan=\"1\" rowspan=\"1\">Caffeine</td><td colspan=\"1\" rowspan=\"1\">HT22 Mouse Hippocampal Neuronal Cell Line</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Positive</td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>False positive (<italic toggle=\"yes\">n</italic> = 1)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Donepezil</td><td colspan=\"1\" rowspan=\"1\">Salicylic</td><td colspan=\"1\" rowspan=\"1\">HT22 Mouse Hippocampal Neuronal Cell Line</td><td colspan=\"1\" rowspan=\"1\">Positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>False negative (<italic toggle=\"yes\">n</italic> = 1)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Galantamine</td><td colspan=\"1\" rowspan=\"1\">Mifepristone</td><td colspan=\"1\" rowspan=\"1\">HT22 Mouse Hippocampal Neuronal Cell Line</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Positive</td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><bold>True negative (<italic toggle=\"yes\">n</italic> = 8)</bold></td></tr><tr><td colspan=\"5\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Galantamine</td><td colspan=\"1\" rowspan=\"1\">Diclofenac</td><td colspan=\"1\" rowspan=\"1\">HT22 Mouse Hippocampal Neuronal Cell Line</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Rivastigmine</td><td colspan=\"1\" rowspan=\"1\">Lithium</td><td colspan=\"1\" rowspan=\"1\">HT22 Mouse Hippocampal Neuronal Cell Line</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td><td colspan=\"1\" rowspan=\"1\">Non-positive</td></tr></tbody></table></table-wrap></p></sec><sec id=\"sec2.6\"><title>Experimentally validated drug combinations demonstrate our model&#8217;s practical utility</title><p id=\"p0080\">To demonstrate practical utility, we prospectively tested the top-ranked drug combinations predicted by Coated-LLM through <italic toggle=\"yes\">in vitro</italic> amyloid beta aggregation experiments. We selected the top three most promising drug combinations from false positives for experimental validation (<xref rid=\"tbl3\" ref-type=\"table\">Table 3</xref>). Across a total of 24 experimental conditions, we assessed individual drugs, their combinations, and controls using two independent assays. Among these, Gypenoside alone resulted in approximately 50% inhibition of A&#946;42 aggregation (<xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref>). The m266 alone, an anti-amyloid beta monoclonal antibody, did not show significant inhibition. Remarkably, Gypenoside combined with m266 exhibited even greater inhibition of A&#946;42 aggregation. The synergy between Gypenoside and m266 not only validates the predictive capabilities of Coated-LLM but also highlights its potential to uncover novel and effective combinatorial therapies that could otherwise remain unexplored.<table-wrap position=\"float\" id=\"tbl3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Selected drug combination candidates</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 1</th><th colspan=\"1\" rowspan=\"1\">Therapeutic agent 2</th><th colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Researcher&#8217;s</italic> reasoning</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">M266, an anti-amyloid beta (A&#946;) monoclonal antibody</td><td colspan=\"1\" rowspan=\"1\">Gypenoside XVII, a bioactive compound derived from Gynostemma pentaphyllum</td><td colspan=\"1\" rowspan=\"1\">m266 directly targets and clears &#946;-amyloid plaques. Gypenoside XVII activates ER&#946;, which can reduce plaque production, enhance plaque clearance, and provide neuroprotection. Gypenoside XVII could potentiate m266&#8217;s plaque-clearing effect by enhancing clearance through ER&#946; activation.</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Acamprosate, an NMDA receptor modulator</td><td colspan=\"1\" rowspan=\"1\">Melatonin, a biogenic amine</td><td colspan=\"1\" rowspan=\"1\">Acamprosate reduces glutamate-induced neurotoxicity. Melatonin has neuroprotective properties, including anti-amyloid, antioxidant, and anti-inflammatory effects. Melatonin increases GABA receptor expression, which could potentially enhance the GABAergic effects of Acamprosate. Both drugs have anti-inflammatory properties, which could provide an additive neuroprotective effect.</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Memantine, an NMDA receptor antagonist</td><td colspan=\"1\" rowspan=\"1\">Atorvastatin, an HMG-CoA reductase inhibitor</td><td colspan=\"1\" rowspan=\"1\">Memantine blocks NMDA receptors, reducing excitotoxicity and slowing disease progression. Atorvastatin provides neuroprotection, reduces beta-amyloid peptide production, enhances cerebral blood flow, and modulates the immune response. Memantine&#8217;s reduction of excitotoxicity could be enhanced by Atorvastatin&#8217;s neuroprotective effects. Atorvastatin&#8217;s ability to reduce beta-amyloid peptide production could further slow disease progression alongside Memantine&#8217;s effects.</td></tr></tbody></table></table-wrap><fig id=\"fig3\" position=\"float\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Inhibitory effects of therapeutic agents on amyloid beta aggregation</p><p>(A) The aggregation profiles of amyloid beta, both in the absence and presence of various combinations of compounds, are depicted. Error bars represent the standard error of the mean (SEM).</p><p>(B) The percentage of aggregation is presented to better illustrate the effect of the different compounds.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr3.jpg\"/></fig></p></sec><sec id=\"sec2.7\"><title>Ablation study highlights key contributing factors</title><p id=\"p0085\">Aiming to understand the contributions of each component within our model, we conducted an ablation study (<xref rid=\"fig4\" ref-type=\"fig\">Figure 4</xref>). The ablation study begins with a zero-shot GPT-4 model, which serves as the baseline. In this setting, GPT-4 leverages its pre-learned knowledge to make predictions. Introducing dynamic few-shot<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> results in a slight performance decrease, likely due to probable mislabeled augmented combinations used as negative few-shot examples. Despite this performance decrease, this augmentation strategy still remains necessary to mitigate reporting bias. Augmented data helps balance the number of examples with positive and negative results in our learning examples, preventing the dynamic few-shot learning examples from being overoptimistic due to the reporting bias (always being positive efficacy), which would otherwise lead to biased predictions, skewing predictions toward more frequently observed positive combinations. For comparison, we conducted the same ablation study under a non-augmentation setting (no negative augmented data), in which the data contains only 250 positive and 30 negative combinations (<xref rid=\"mmc1\" ref-type=\"supplementary-material\">Figure S1</xref>) to assess the impact of data availability and class balance on model performance. With this unbalanced data, we observed a clear accuracy improvement thanks to the dynamic few-shot strategy. However, the models were overly optimistic, and the predicted results were biased toward positive efficacy.<fig id=\"fig4\" position=\"float\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Visual illustration of Coated-LLM components and additive contributions to the performance</p><p>Coated-LLM combines kNN-based five-shot dynamic learning example selection, external pathway knowledge, self-consistency (<italic toggle=\"yes\">n</italic> = 5), <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic>, and <italic toggle=\"yes\">Moderator</italic>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr4.jpg\"/></fig></p><p id=\"p0090\">Subsequently, applying a RAG, we integrated external biomedical knowledge on pathways to address knowledge gaps in GPT-4&#8217;s pre-trained model and led to significant improvements in accuracy (+17%), precision (+13%), recall (+40%), and F1-score (+23%). By implementing self-consistency via an ensemble strategy, we increased the accuracy rate by 6%, precision by 4%, recall by 3%, and F1-score by 4%.</p><p id=\"p0095\">Finally, incorporating feedback from the <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> and <italic toggle=\"yes\">Moderator</italic> further improved the model&#8217;s predictions, correcting potential errors to achieve the highest accuracy (+5%) and precision (+9%). However, we observed a decrease in recall due to <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> and <italic toggle=\"yes\">Moderator</italic>, suggesting that two LLM agents in the revision phase favor reducing false positives over false negatives, analogous to the fact that (human) reviewers are more skeptical than (human) researchers. Despite the decrease in the recall, we kept the <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> and <italic toggle=\"yes\">Moderator</italic> because, in real <italic toggle=\"yes\">in vivo</italic> experiments, our model is used to retrieve a top ranked list of probable positive combinations, thus high precision is more important than high recall.</p></sec></sec><sec id=\"sec3\"><title>Discussion</title><p id=\"p0100\">In this study, we introduced Coated-LLM, an innovative AI-driven framework that leverages multiple specialized LLM agents to systematically generate, evaluate, and revise biomedical hypotheses for identifying efficacious combinatorial therapies for AD. Our framework demonstrated robust predictive accuracy through both internal cross-validation and a retrospective <italic toggle=\"yes\">in vitro</italic> dataset, underscoring its potential for real-world application. In addition, experimental validation further reinforced the practical utility of our framework. Computational analysis showed that for a single testing drug combination, Coated-LLM required approximately 4 minutes and $0.95 in total. Specifically, the fully equipped <italic toggle=\"yes\">Researcher</italic> (incorporating dynamic few-shots, external knowledge integration, and self-consistency) required approximately 15,000 tokens, 90 seconds of processing time, and $0.53 in computational costs. The <italic toggle=\"yes\">R</italic><italic toggle=\"yes\">eviewer</italic><italic toggle=\"yes\">s</italic> consumed approximately 1,600 tokens, 27 seconds, and $0.07, and <italic toggle=\"yes\">Moderator</italic>, executing five self-consistent runs, utilized approximately 11,000 tokens, 100 seconds, and $0.35. This level of resource usage suggests that Coated-LLM is a viable solution for scalable application in academic and industrial environments, thereby enhancing the traditional drug discovery process by uncovering therapeutically valuable combinations that might remain unexplored due to the complexity, cost, and limitations of conventional experimental approaches.</p><p id=\"p0105\">In addition, Coated-LLM provides a generalizable insight for developing an LLM framework. The following lessons were derived from our study.<list list-type=\"simple\" id=\"ulist0015\"><list-item id=\"u0025\"><label>&#8226;</label><p id=\"p0110\">Interaction of Multi-Agent LLM: In our multi-agent LLM framework, <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> and <italic toggle=\"yes\">Moderator</italic> operate in a manner similar to human peer reviewers and journal editors, prioritizing rigor and skepticism over inclusivity, avoiding false positives over false negatives. Given that our model is designed to discover the most probable positive drug combinations for <italic toggle=\"yes\">in vivo</italic> experiments, where false positives carry high experimental costs, favoring precision over recall is a necessary trade-off.</p></list-item><list-item id=\"u0030\"><label>&#8226;</label><p id=\"p0115\">Implications of Dynamic Few-Shot Learning: Dynamic few-shot learning plays a crucial role in enhancing the predictive capabilities of LLMs by leveraging examples that are most similar to the target data. Our findings reveal that the inclusion of high-quality real learning examples significantly enhances the accuracy of predictions compared to the zero-shot strategy. In contrast, the use of augmented learning examples does not yield a similar increase in accuracy, showing the importance of high-quality examples.</p></list-item><list-item id=\"u0035\"><label>&#8226;</label><p id=\"p0120\">Importance of a balanced set of learning examples: Given that the initial dataset from literature mining contains 250 positive combinations (89.28%), the predominance of the positive class in the dynamic few-shot learning examples can introduce bias, potentially leading to skewed predictions toward more frequently observed positive combinations. Our data augmentation strategy balanced the distribution of positive and non-positive learning examples. Although this approach resulted in a slight performance decrease, it remains essential for reducing bias and preventing overly optimistic predictions.</p></list-item></list></p><p id=\"p0125\">In addition, we analyzed the failure modes of Coated-LLM&#8217;s reasoning when evaluated against experimental ground-truth. In the case of estradiol and continuous progesterone combination, our model incorrectly assumed that continuous progesterone would reduce amyloid pathology and improve cognition. However, experimental evidence demonstrated that while continuous progesterone reduced tau hyperphosphorylation, it failed to lower amyloid levels and in fact antagonized the beneficial cognitive effects of estradiol, which was distinct from the effects observed with cyclic progesterone.<xref rid=\"bib33\" ref-type=\"bibr\"><sup>33</sup></xref> Similarly, for the combination of cholesterol and homocysteine, our model predicted therapeutic benefit by jointly targeting two pathological mechanisms. In contrast, empirical findings showed only the partial reduction of inflammation without recovery of cognitive function.<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref> In the case of BACE1 haploinsufficiency and neprilysin overexpression combination, our model emphasized dual targeting of amyloid production and clearance. However, experimental results revealed no additional benefit beyond neprilysin overexpression alone, which was sufficient to abolish amyloid deposition and rescue memory, thus leaving no room for further improvement.<xref rid=\"bib34\" ref-type=\"bibr\"><sup>34</sup></xref> These failure cases showed common reasoning errors of Coated-LLM, such as the overgeneralization of prior knowledge and a lack of sensitivity to ceiling effects.</p><p id=\"p0130\">In all, Coated-LLM presents a transformative opportunity for drug discovery across AD and other multifactorial conditions by reducing reliance on extensive experimental screening. Its ability to effectively predict drug combination efficacy from limited or scarce data holds promise for accelerating therapeutic innovation, optimizing resource allocation, and substantially decreasing costs associated with drug development.</p><sec id=\"sec3.1\"><title>Limitations of the study</title><p id=\"p0135\">While we have obtained promising results, our study has several limitations. One of the primary limitations is the underrepresentation of negative combinations within the dataset obtained through literature mining, introducing unavoidable bias toward positive outcomes during the model&#8217;s inference phase. While the data augmentation approach helped balance this bias, it is important to acknowledge that these augmented combinations could be false negatives. Furthermore, determining the true efficacy or non-efficacy of such combinations ultimately requires experimental validation, and expert review alone may not fully resolve such uncertainty. As a result, a certain degree of mislabeling may exist and could potentially lead to incorrect predictions. However, given that truly efficacious drug combinations are relatively rare in the real world, the likelihood of falsely labeling an efficacious pair as negative is relatively low. In addition, our experts manually reviewed a representative subset of augmented combinations and concluded that the theoretical conclusion of non-positive remains justifiable without the empirical testing of these combinations. Based on these considerations, we regarded the trade-off acceptable in order to improve the class balance of the learning examples.</p><p id=\"p0140\">The use of LLMs in biomedical hypothesis generation may raise ethical concerns, particularly regarding potential hallucinated reasoning or misleading predictions that may influence human decisions.<xref rid=\"bib35\" ref-type=\"bibr\"><sup>35</sup></xref><sup>,</sup><xref rid=\"bib36\" ref-type=\"bibr\"><sup>36</sup></xref> However, recent studies have shown that multi-agent collaborative frameworks can effectively reduce hallucinations in LLM outputs,<xref rid=\"bib37\" ref-type=\"bibr\"><sup>37</sup></xref><sup>,</sup><xref rid=\"bib38\" ref-type=\"bibr\"><sup>38</sup></xref><sup>,</sup><xref rid=\"bib39\" ref-type=\"bibr\"><sup>39</sup></xref> which aligns with the design of our Coated-LLM that incorporates <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic> and <italic toggle=\"yes\">Moderator</italic> agents. While our framework is designed to generate hypotheses and predictions, we acknowledge that the outputs require rigorous reasoning checks and experimental validation before clinical consideration. In addition, false positive predictions could lead to costly experimental investigations. Our multi-agent system with <italic toggle=\"yes\">Reviewers</italic> and <italic toggle=\"yes\">Moderator</italic> specifically addresses this concern by prioritizing precision over recall to minimize false positives.</p><p id=\"p0145\">It is important to acknowledge that the retrospective <italic toggle=\"yes\">in vitro</italic> validation is limited by the small, private dataset, whereas our model was trained on <italic toggle=\"yes\">in vivo</italic> experimental outcomes. Despite this difference in modality, the <italic toggle=\"yes\">in vitro</italic> dataset remained within the same disease context (Alzheimer&#8217;s Disease), allowing a within-domain but cross-modality assessment, since both <italic toggle=\"yes\">in vitro</italic> and <italic toggle=\"yes\">in vivo</italic> assays ultimately aim to evaluate whether drug combinations can slow or treat Alzheimer&#8217;s Disease. In addition, the inclusion of the cross-validation test set provides a complementary evaluation, supporting the generalizability of our findings. Given that the initial learning examples contain only 97 non-positive efficacy combinations out of 231 (41.9%), such bias presents a considerable challenge for our model in accurately predicting non-positive outcomes. Despite these challenges, our model achieved a significant accuracy rate, demonstrating its effectiveness in generating hypotheses for synergistic drug combinations with minimal historical data. While only three top-ranked combinations were selected for experimental validation, this selection reflects our prioritization of disease-modifying mechanisms over symptomatic targets or herbal compounds. We acknowledge that expanding validation to a broader range of predicted candidates would strengthen the robustness and generalizability of our findings. Nevertheless, we interpret these results as preliminary, and further validation on larger <italic toggle=\"yes\">in vivo</italic> datasets, additional experimental assays, and applications to other disease domains is needed to substantiate broader claims of generalizability.</p></sec></sec><sec id=\"sec4\"><title>Resource availability</title><sec id=\"sec4.1\"><title>Lead contact</title><p id=\"p0150\">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Yejin Kim (<email>yejin.kim@uth.tmc.edu</email>).</p></sec><sec id=\"sec4.2\"><title>Materials availability</title><p id=\"p0155\">This study did not generate new unique reagents.</p></sec><sec sec-type=\"data-availability\" id=\"sec4.3\"><title>Data and code availability</title><p id=\"p0160\">\n<list list-type=\"simple\" id=\"ulist0020\"><list-item id=\"u0040\"><label>&#8226;</label><p id=\"p0165\">The data generated from literature mining and drug hit AD genes for this study can be accessed via the following link: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/QidiXu96/Coated-LLM\" id=\"intref0015\">https://github.com/QidiXu96/Coated-LLM</ext-link>.<xref rid=\"bib40\" ref-type=\"bibr\"><sup>40</sup></xref></p></list-item><list-item id=\"u0045\"><label>&#8226;</label><p id=\"p0170\">The code for <italic toggle=\"yes\">Researcher</italic>, <italic toggle=\"yes\">Reviewer</italic><italic toggle=\"yes\">s</italic>, and <italic toggle=\"yes\">Moderator</italic> can be found at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/QidiXu96/Coated-LLM\" id=\"intref0020\">https://github.com/QidiXu96/Coated-LLM</ext-link>.<xref rid=\"bib40\" ref-type=\"bibr\"><sup>40</sup></xref></p></list-item></list>\n</p></sec></sec><sec id=\"sec5\"><title>Acknowledgments</title><p id=\"p0175\">YK is supported in part by <funding-source id=\"gs1\"><institution-wrap><institution-id institution-id-type=\"doi\">10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source> under award number <award-id award-type=\"grant\" rid=\"gs1\">R01AG082721</award-id>, <award-id award-type=\"grant\" rid=\"gs1\">R01AG066749</award-id>, and <award-id award-type=\"grant\" rid=\"gs1\">R01AG084637</award-id>.</p></sec><sec id=\"sec6\"><title>Author contributions</title><p id=\"p0180\">Concept and design: QX and YK.; data access and analysis: Y.K. model development: Q.X. and Y.K. interpretation: C.S., M.S., Q.X., and Y.K.; draft article: C.S., M.S., Q.X., Y.K., and X.L. <italic toggle=\"yes\">in vitro</italic> experiments design: C.S., M.S., and X.J. All authors contributed to editing the article, approved the final article, and accepted the responsibility to submit it for publication.</p></sec><sec sec-type=\"COI-statement\" id=\"sec7\"><title>Declaration of interests</title><p id=\"p0185\">No competing interest to declare.</p></sec><sec id=\"sec8\"><title>STAR&#9733;Methods</title><sec id=\"sec8.1\"><title>Key resources table</title><p id=\"p0190\">\n<table-wrap position=\"float\" id=\"undtbl1\" orientation=\"portrait\"><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">REAGENT or RESOURCE</th><th colspan=\"1\" rowspan=\"1\">SOURCE</th><th colspan=\"1\" rowspan=\"1\">IDENTIFIER</th></tr></thead><tbody><tr><td colspan=\"3\" rowspan=\"1\"><bold>Data</bold></td></tr><tr><td colspan=\"3\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Literature mining and data augmentation</td><td colspan=\"1\" rowspan=\"1\">This paper</td><td colspan=\"1\" rowspan=\"1\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/QidiXu96/Coated-LLM\" id=\"intref0025\">https://github.com/QidiXu96/Coated-LLM</ext-link><xref rid=\"bib40\" ref-type=\"bibr\"><sup>40</sup></xref></td></tr><tr><td colspan=\"3\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"3\" rowspan=\"1\"><bold>Software and algorithms</bold></td></tr><tr><td colspan=\"3\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">OpenAI API</td><td colspan=\"1\" rowspan=\"1\">Python package</td><td colspan=\"1\" rowspan=\"1\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://openai.com/api/\" id=\"intref0030\">https://openai.com/api/</ext-link></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Claude API</td><td colspan=\"1\" rowspan=\"1\">Python package</td><td colspan=\"1\" rowspan=\"1\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.anthropic.com/api\" id=\"intref0035\">https://www.anthropic.com/api</ext-link></td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"sec8.2\"><title>Method details</title><sec id=\"sec8.2.1\"><title>Problem formulation</title><p id=\"p0195\">Our objective is to predict whether a combination of therapeutic agents <italic toggle=\"yes\">t</italic><sub>1</sub>and <italic toggle=\"yes\">t</italic><sub>2</sub> have a positive efficacy <italic toggle=\"yes\">y</italic> when tested in an <italic toggle=\"yes\">in vivo</italic> model <italic toggle=\"yes\">m</italic>. That is, we aim to develop a model <italic toggle=\"yes\">f</italic> such that <italic toggle=\"yes\">y</italic>=<italic toggle=\"yes\">f</italic>(<italic toggle=\"yes\">x</italic>), where <italic toggle=\"yes\">x</italic> is a triplet of (<italic toggle=\"yes\">t</italic><sub>1</sub>,<italic toggle=\"yes\">t</italic><sub>2</sub>,<italic toggle=\"yes\">m</italic>). Here, <italic toggle=\"yes\">t</italic><sub>1</sub>,<italic toggle=\"yes\">t</italic><sub>2</sub>, and <italic toggle=\"yes\">m</italic> are not only drawn from a finite set, but can be a new or investigational therapeutic agent (e.g. &#8220;<italic toggle=\"yes\">Membrane-free stem cell extract&#8221;</italic>) or <italic toggle=\"yes\">in vivo</italic> model (e.g. &#8220;<italic toggle=\"yes\">Rats induced with AD using aluminum chloride</italic>&#8221;), which are not registered with a formal identifier, thus best described as a natural text. We convert the structured input (<italic toggle=\"yes\">t</italic><sub>1</sub>,<italic toggle=\"yes\">t</italic><sub>2</sub>,<italic toggle=\"yes\">m</italic>) into a natural text question <italic toggle=\"yes\">Q</italic> following.<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> For example, we convert the combination (&#8220;Galantamine&#8221;, &#8220;Nicotine&#8221;, &#8220;ICR mice&#8221;) into &#8216;<italic toggle=\"yes\">Decide if the combination of Galantamine and Nicotine is effective or not to treat ICR mice model in theory.&#8217;</italic></p><p id=\"p0200\">In some previous studies,<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> the effectiveness of combinations of therapeutic agents was measured as synergy. However, the synergy quantification requires dose-dependent inhibition profiling, which is not available in an <italic toggle=\"yes\">in vivo</italic> model. As most <italic toggle=\"yes\">in vivo</italic> experiments only report efficacy (without formal calculation of synergy, toxicity, or dose&#8211;response relationships), our focus is also on efficacy (rather than synergy). Rather than a specific efficacy measurement, we focused on a broad sentiment as an efficacy measurement (positive or not). While this binary labeling simplifies the underlying pharmacological nuances, it makes more transferable to different studies.</p></sec><sec id=\"sec8.2.2\"><title>Data collection</title><p id=\"p0205\">We collected scientific articles that report the efficacy of therapeutic agent combinations on AD <italic toggle=\"yes\">in vivo</italic> models. We first utilized the Alzheimer&#8217;s Disease Preclinical Efficacy Database (AlzPED),<xref rid=\"bib41\" ref-type=\"bibr\"><sup>41</sup></xref> a data resource dedicated to the preclinical efficacy studies of candidate therapeutics for Alzheimer's Disease. Among the 1,463 articles in AlzPED, we manually reviewed and selected 39 articles that experimented with multiple therapeutic agents.</p><p id=\"p0210\">We further searched for more related articles based on the 39 selected articles. Specific search queries are available in <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S1</xref>. We extracted 376 additional articles meeting the query from PubMed. 10 out of 39 (25.64%) AlzPED articles were searchable from the query. We then extracted therapeutic agents, <italic toggle=\"yes\">in vivo</italic> models, and their efficacy from the abstract of the selected articles. We excluded articles in which drugs were used to induce AD or suppress mechanisms for mechanistic study. Among the 376 articles, 199 articles reported positive efficacy, and 3 reported mixed or partial effects, 16 reported negative efficacy. All others are not relevant.</p></sec><sec id=\"sec8.2.3\"><title>Data augmentation</title><p id=\"p0215\">Our initial dataset showed severe imbalance toward positive efficacy, as researchers tend to publish positive results more than negative ones. In addition to combinations reporting non-positive efficacy in the literature, we created plausible samples with unknown efficacy (unlabeled data), and used them as non-positive samples with noise in both the warm-up phase and inference phase. The non-positive samples were created by randomly replacing either one of the drugs or an <italic toggle=\"yes\">in vivo</italic> model from the positive efficacy combinations, with the replacement therapeutic agents or models selected from those commonly used in AD research from AlzPED dataset. This constraint ensures that the augmented combinations remain within the AD therapeutic domain, preventing obvious non-positive cases that would result from replacing AD drugs with treatments for unrelated conditions like obesity. For example, given an efficacious combination (<italic toggle=\"yes\">Acamprosate, Baclofen, mThy1-hAPP751 (TASD41)</italic>), we created a non-positive combination by replacing Baclofen. As a result, we have (<italic toggle=\"yes\">Acamprosate, Melatonin, mThy1-hAPP751 (TASD41)</italic>). This random replacement strategy generated combinations that were statistically unlikely to be efficacious, as the probability of two randomly selected therapeutic agents showing efficacy is extremely low by nature, thereby not only balance the dataset, but also creating more truly novel cases that couldn&#8217;t have been memorized during LLMs&#8217; pre-training. To further address potential data leakage concerns, we had an independent private dataset (see <xref rid=\"sec2.5\" ref-type=\"sec\">retrospective in vitro validation</xref>) that was never published, ensuring our model&#8217;s predictions were not simply regurgitations of previously seen information.</p></sec><sec id=\"sec8.2.4\"><title>Coated-LLM</title><sec id=\"sec8.2.4.1\"><title>Warm-up phase</title><sec id=\"sec8.2.4.1.1\"><title>Overview</title><p id=\"p0220\">In the warm-up phase, <italic toggle=\"yes\">Researcher</italic> generates answers to training questions and compares them with ground truth answers. The correctly generated answers are used as learning examples in the next inference phase. For this purpose, we split the data into 70% training and 30% testing sets, which are used to derive learning examples in the warm-up phase and actual inference in the next phase, respectively. This training set is not for actual training nor fine-tuning LLMs but for learning examples. We set aside a higher proportion for training to ensure <italic toggle=\"yes\">Researcher</italic> can be exposed to diverse learning examples.</p></sec><sec id=\"sec8.2.4.1.2\"><title>Chain-of-thoughts (CoT)</title><p id=\"p0225\">To improve the reasoning ability of <italic toggle=\"yes\">Researcher</italic>, we applied a chain-of-thought (CoT)<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref> prompting strategy by incorporating the instruction: &#8220;Take a deep breath and work on this problem step-by-step.&#8221;.<xref rid=\"bib42\" ref-type=\"bibr\"><sup>42</sup></xref> This approach encourages <italic toggle=\"yes\">Researcher</italic> to decompose complex drug combination efficacious task into a series of intermediate steps, such as identifying drug targets and mechanisms of action, analyzing biological pathways, evaluating multi-pathway targeting, before reaching a final conclusion. (Prompt at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S1.1).</p></sec><sec id=\"sec8.2.4.1.3\"><title>Retrieval augmented generation (RAG)</title><p id=\"p0230\">To make <italic toggle=\"yes\">Researcher</italic> answers a question <italic toggle=\"yes\">q</italic> in the training set more intelligently, we allowed it to use external biomedical knowledge. Since LLMs generate responses based on patterns learned during training, which are inherently limited by the data they were pre-trained on. However, information on therapeutic agents is vast and continuously expanding. To bridge this gap, we provided external biomedical knowledge to enhance <italic toggle=\"yes\">Researcher</italic>&#8217;s responses through retrieval-augmented generation (RAG),<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref> which complements static LLM parameters with up-to-date and dynamic information.<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref> We retrieved and provided specific external information <italic toggle=\"yes\">B</italic><sub><italic toggle=\"yes\">q</italic></sub> on therapeutic agents <italic toggle=\"yes\">t</italic><sub>1</sub>,<italic toggle=\"yes\">t</italic><sub>2</sub>. We used the Comparative Toxicogenomics Database (CTDbase),<xref rid=\"bib43\" ref-type=\"bibr\"><sup>43</sup></xref> a knowledge database encompassing 88,144,004 relationships in chemicals, genes, pathways, and diseases. We particularly focused on the pathway information that the therapeutic agents <italic toggle=\"yes\">t</italic><sub>1</sub>,<italic toggle=\"yes\">t</italic><sub>2</sub> targets. Only pathways with a corrected p-value below 0.01 are incorporated as external knowledge. Of the total combinations, 129 had pathway information for both drugs available from CTDbase, and 235 had information for only one of the two drugs. For example, for the therapeutic agent Galantamine, we provided molecular pathway information such as &#8220;<italic toggle=\"yes\">Galantamine has several pathway information, such as cholinergic synapse, transmission across chemical synapses, highly calcium permeable postsynaptic nicotinic acetylcholine receptors, &#8230;, and peptide hormone metabolism.</italic>&#8221;. Note that we also tried to incorporate a list of targeting genes as external knowledge, and it did not provide high-quality answers due to its high sparsity. Also note that the <italic toggle=\"yes\">in vivo</italic> model information, such as one available in AlzForum,<xref rid=\"bib44\" ref-type=\"bibr\"><sup>44</sup></xref> marginally increased the generation quality while consuming many tokens.</p><p id=\"p0235\">Based on the targeting pathway information <italic toggle=\"yes\">B</italic><sub><italic toggle=\"yes\">q</italic></sub>, we prompt <italic toggle=\"yes\">Researcher</italic> to generate a hypothesis. This hypothesis consists of a series of CoT reasoning (<italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">q</italic></sub>) and a final binary answer <italic toggle=\"yes\">A</italic><sub><italic toggle=\"yes\">q</italic></sub> (Output example at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S1.2). We only focused on the correct answers and its corresponding reasoning as learning examples and filtered out <italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">q</italic></sub> if the answer <italic toggle=\"yes\">A</italic><sub><italic toggle=\"yes\">q</italic></sub> is different from the ground truth efficacy label <italic toggle=\"yes\">y</italic>. This simple filtering has greatly decreased the low-quality chain-of-throughs examples.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> We used GPT-4 for <italic toggle=\"yes\">Researcher</italic>. To encourage <italic toggle=\"yes\">Researcher</italic> to be skeptical, we added the statement &#8216;It is rare for combinations of two drugs to be efficacious and synergistic in real world&#8217; into the prompt (Prompt at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S1.1).</p></sec></sec><sec id=\"sec8.2.4.2\"><title>Inference phase</title><sec id=\"sec8.2.4.2.1\"><title>Overview</title><p id=\"p0240\">Using the learning examples from the warm-up phase, <italic toggle=\"yes\">Researcher</italic> generates hypotheses to the questions in the testing set. In the inference phase, <italic toggle=\"yes\">Researcher</italic> leverages the learning examples (dynamic few-shot learning) and external biomedical knowledge (RAG), following the same methodology as in the warm-up phase.</p></sec><sec id=\"sec8.2.4.2.2\"><title>Dynamic Few-shot</title><p id=\"p0245\">When asked scientific questions, human researchers look for similar previously that were answered previously and performed inductive reasoning. So does <italic toggle=\"yes\">Researcher</italic> by leveraging dynamic few-shot learning.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> Few-shot learning<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref> is one of the most effective in-context learning methods to guide LLMs to learn the patterns from a few demonstration examples and to generate similar outcomes like the examples. Here, it is critical to provide examples that are relevant to the question of interest.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> However, in our application on AD combinatorial therapy discovery, the therapeutic and their associated biological mechanisms are very diverse, making randomly selected examples insufficient for pattern learning. For example, the question <italic toggle=\"yes\">Q</italic> from (<italic toggle=\"yes\">&#8216;Galantamine</italic>&#8217;, <italic toggle=\"yes\">&#8216;Nicotine</italic>&#8217;, &#8216;<italic toggle=\"yes\">ICR mice</italic>&#8217;) is more similar to one from (<italic toggle=\"yes\">&#8216;Galantamine&#8217;, Memantine&#8217;, &#8216;ICR mice</italic>&#8217;) than one from (<italic toggle=\"yes\">&#8216;Scyllo-inositol&#8217;, &#8216;neotrofin&#8217;, &#8216;TgCRND8&#8217;</italic>). Thus, we selected the most similar question <italic toggle=\"yes\">q</italic> in the learning examples and its associated reasoning <italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">q</italic></sub> for inductive reasoning in the inference phase. We derived textual embedding <italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">Q</italic></sub> of the question <italic toggle=\"yes\">Q</italic> of interest and <italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">q</italic></sub> of the question <italic toggle=\"yes\">q</italic> in the learning examples using OpenAI&#8217;s text-embedding-ada-002.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref> We then calculated cosine similarity &lt;<italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">q</italic></sub>,<italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">Q</italic></sub>&gt;/(&#8214;<italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">q</italic></sub>&#8214; &#183; &#8214;<italic toggle=\"yes\">E</italic><sub><italic toggle=\"yes\">Q</italic></sub>&#8214;)to identify the top five similar questions <italic toggle=\"yes\">q</italic> with the highest similarity. So, the prompt consisted of similar learning examples (<italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">q</italic></sub>, <italic toggle=\"yes\">A</italic><sub><italic toggle=\"yes\">q</italic></sub>), interest question <italic toggle=\"yes\">Q</italic>, and external biomedical knowledge <italic toggle=\"yes\">B</italic><sub><italic toggle=\"yes\">Q</italic></sub>. Note that we guided LLMs to have a series of CoT reasoning not only by simply encouraging LLM to &#8220;<italic toggle=\"yes\">think step by step,</italic>&#8221; but by providing the exact CoT demonstration <italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">q</italic></sub> in the learning example from dynamic few-shot learning. See the full prompt and the output examples at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S2.1 &amp; S2.2.</p></sec><sec id=\"sec8.2.4.2.3\"><title>Self-consistency via ensemble</title><p id=\"p0250\">To increase the reliability of LLM&#8217;s prediction, we generated the response (<italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">Q</italic></sub>,<italic toggle=\"yes\">A</italic><sub><italic toggle=\"yes\">Q</italic></sub>) multiple times. We aggregated them by obtaining consensus prediction <inline-formula><mml:math id=\"M1\" altimg=\"si1.gif\"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> via majority vote and selecting the most detailed (thus longest) chain of thought <inline-formula><mml:math id=\"M2\" altimg=\"si2.gif\"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> if its paired answer <italic toggle=\"yes\">A</italic><sub><italic toggle=\"yes\">q</italic></sub> is the same as the majority. This ensemble technique can minimize the risk of incorrect prediction by cross-verifying multiple outputs.<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></p></sec></sec><sec id=\"sec8.2.4.3\"><title>Revision phase</title><sec id=\"sec8.2.4.3.1\"><title>Evaluate</title><p id=\"p0255\">The theoretical inductive reasoning inevitably carries uncertainty. It is critical to independently evaluate the validity of hypotheses and revise accordingly. After we obtain the hypothesis on efficacy <inline-formula><mml:math id=\"M3\" altimg=\"si1.gif\"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and reasoning <inline-formula><mml:math id=\"M4\" altimg=\"si2.gif\"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> in the inference phase, <italic toggle=\"yes\">Reviewers</italic> need to critically evaluate whether the hypothesis is logical and reasonable. This review process should be independent, thus we used another LLM with comparable performance to <italic toggle=\"yes\">Researcher</italic> (GPT-4), Claude-3-opus, to enhance the independence of the reviewing process.</p></sec><sec id=\"sec8.2.4.3.2\"><title>Tree-of-thoughts (ToT)</title><p id=\"p0260\"><italic toggle=\"yes\">Reviewers</italic> should have diverse perspectives than <italic toggle=\"yes\">Researcher</italic> to critically evaluate <italic toggle=\"yes\">Researcher</italic>&#8217;s hypothesis and identify potential pitfalls that <italic toggle=\"yes\">Researcher</italic> could not spot. Thus we encouraged <italic toggle=\"yes\">Reviewers</italic> to have multiple perspectives and discuss different branches of thoughts via tree-of-thoughts (ToT) reasoning.<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref> This approach explores different possibilities and then converges on the most optimal solution. We prompted <italic toggle=\"yes\">Reviewers</italic> by instructing, &#8220;<italic toggle=\"yes\">Imagine three different experts who are in therapy development for Alzheimer&#8217;s disease, are tasked with critically reviewing the reasoning&#8230;</italic>&#8221; (Prompt and output example at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S3.1 &amp; S3.2).</p></sec><sec id=\"sec8.2.4.3.3\"><title>Revise</title><p id=\"p0265\">Once <italic toggle=\"yes\">Reviewers</italic> finish the discussion and provide feedback <italic toggle=\"yes\">F</italic><sub><italic toggle=\"yes\">Q</italic></sub>, <italic toggle=\"yes\">Moderator</italic> aggregated the <italic toggle=\"yes\">Reviewers&#8217;</italic> feedback and <italic toggle=\"yes\">Researcher</italic>&#8217;s hypothesis to obtain the final decision. <italic toggle=\"yes\">Moderator</italic> took input of <italic toggle=\"yes\">Q</italic>, <inline-formula><mml:math id=\"M5\" altimg=\"si2.gif\"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M6\" altimg=\"si1.gif\"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>\n<italic toggle=\"yes\">F</italic><sub><italic toggle=\"yes\">Q</italic></sub> and deduced the final revised reasoning <inline-formula><mml:math id=\"M7\" altimg=\"si2.gif\"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and answer <inline-formula><mml:math id=\"M8\" altimg=\"si1.gif\"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>Q</mml:mi><mml:mo>&#8727;</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> . See the full prompt and output example at <xref rid=\"mmc1\" ref-type=\"supplementary-material\">Data S2</xref> List S4.1 &amp; S4.2.</p></sec></sec><sec id=\"sec8.2.4.4\"><title>Evaluation</title><p id=\"p0270\">We evaluate whether the prediction of Coated-LLM is accurate by comparing the binary prediction (i.e., positive vs. non-positive) with the ground-truth label. We reported accuracy, precision, recall, and F1. We also quantified our model confidence based on self-consistency, defined as the proportion of repeated <italic toggle=\"yes\">Moderator</italic> runs that agreed with the majority prediction. To assess how well this confidence aligns with actual prediction correctness, we calculated the Expected Calibration Error (ECE), which is defined as the average absolute difference between the model's confidence and accuracy across binned confidence intervals. We first evaluated the accuracy via cross-validation using the test set and via retrospective <italic toggle=\"yes\">in vitro</italic> validation using in-house private data. This <italic toggle=\"yes\">in vitro</italic> dataset has 11 drug combinations, all of which were entirely unseen in our main dataset, including literature mining and data augmentation. Of these, 9 combinations are labeled as non-positive efficacy. Furthermore, during the evaluation of the <italic toggle=\"yes\">in vitro</italic> data, we augmented the initial learning examples from the warm-up phase by incorporating combinations that were correctly predicted during the revision phase. After augmenting the learning examples, we had 347 combinations (195 combinations with positive efficacy; 152 showing non-positive efficacy) serving as learning examples for predicting efficacy on our private data.</p><p id=\"p0275\">We conducted an ablation study to understand the relative contributions of each component in our model. We iteratively introduced each component and measured the performance differences. Since these components are not statistically independent,<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref><sup>,</sup><xref rid=\"bib2\" ref-type=\"bibr\"><sup>2</sup></xref> we should consider the performance differences as the components' relative contributions.</p><sec id=\"sec8.2.4.4.1\"><title>Baseline</title><p id=\"p0280\">Due to the lack of sufficient data, data-driven machine learning models are not appropriate or available. Instead, we developed a rule-based baseline model to predict the efficacy of drug combinations. We utilize complementary exposure patterns,<xref rid=\"bib45\" ref-type=\"bibr\"><sup>45</sup></xref> stating that drug combination is therapeutically effective if the targets of the therapeutic agents hit the disease module without overlap (<xref rid=\"mmc1\" ref-type=\"supplementary-material\">Methods S1</xref>). The target genes of the therapeutic agents were collated from multiple sources, including Drug Target Commons, PubChem, and CTDbase,<xref rid=\"bib43\" ref-type=\"bibr\"><sup>43</sup></xref><sup>,</sup><xref rid=\"bib46\" ref-type=\"bibr\"><sup>46</sup></xref><sup>,</sup><xref rid=\"bib47\" ref-type=\"bibr\"><sup>47</sup></xref> whereas AD-related genes were derived from Agora&#8217;s nominated gene list.<xref rid=\"bib48\" ref-type=\"bibr\"><sup>48</sup></xref> Within the test set, target gene information was unobtainable for 76 therapeutic agent (34.3%). Additionally, 76 therapeutic agent combinations (48.72%), can not be evaluated using the baseline model due to the absence of necessary target gene information.</p></sec></sec></sec><sec id=\"sec8.2.5\"><title>Selecting drug combination for <italic toggle=\"yes\">in vitro</italic> experiments</title><p id=\"p0285\">We had created augmented non-positive samples. These augmented data that were predicted to be positive (i.e., false positive) may suggest intriguing hypotheses. Although these combinations were unknown thus marked as non-positive, we hypothesize that they may, in fact, be efficacious combinations that have not yet been empirically tested. Therefore, we selected the top most promising drug combinations out of them and tested for its <italic toggle=\"yes\">in vitro</italic> efficacy. Human experts carefully examined the 25 false positives. We selected drug combinations if the drug combination co-inhibit functionally complementary pathways according to <italic toggle=\"yes\">Researcher&#8217;s</italic> reasoning. Recent studies in network pharmacology indicate that synergy occurs when drug targets are functionally complementary but do not share direct interactions, reducing the risk of redundant inhibition, such as the combination of Lenalidomide and BET inhibitors (e.g., I-BET-762) in bortezomib-resistant mantle cell lymphoma,<xref rid=\"bib49\" ref-type=\"bibr\"><sup>49</sup></xref> Geldanamycin + Tofacitinib in myeloproliferative neoplasm cells,<xref rid=\"bib49\" ref-type=\"bibr\"><sup>49</sup></xref> co-inhibition of NOX4-derived ROS and NOS-derived NO in ischemic stroke.<xref rid=\"bib50\" ref-type=\"bibr\"><sup>50</sup></xref> In addition, we excluded combinations if they only target symptom relief (e.g., targeting neurotransmitters) as we are more interested in disease-modifying therapies. We excluded drug combinations if they contain natural products (e.g., Herbal medicine like Huperzine A).</p></sec><sec id=\"sec8.2.6\"><title>In-vitro amyloid beta 1&#8211;42 aggregation assay</title><p id=\"p0290\">Misfolding, aggregation, and the progressive accumulation of amyloid beta (A&#946;) and Tau proteins in the brain are hallmark events in AD. To evaluate the efficacy of selected drug combinations, we performed an A&#946;42 aggregation assay.</p><p id=\"p0295\">A&#946; peptide (A&#946;42) was synthesized at the W. M. Keck Facility at Yale University using solid-phase N-tert-butyloxycarbonyl chemistry and purified via reverse-phase high-performance liquid chromatography (HPLC). The purified A&#946;42 was lyophilized and stored at -80&#176;C until use. To ensure seed-free preparations, the lyophilized A&#946;42 powder was dissolved in a high pH solution (10 mM NaOH) and filtered through a 30-kDa cutoff filter to eliminate residual aggregates.</p><p id=\"p0300\">For the aggregation assay, 200 &#956;l of seed-free A&#946;42 at a concentration of 2 &#956;M was prepared in aggregation buffer (0.1 M Tris-Cl pH 7.4, 500 mM NaCl, and 5 &#956;M Thioflavin T). The assay was conducted either with A&#946;42 alone as a control or in the presence of individual therapeutic agents (Gypenoside, Acamprosate, Melatonin, Memantine, Atorvastatin), drug combinations (each at a final concentration of 100 &#956;M), or an amyloid beta-specific antibody (m266, 5 &#956;g/ml). The mixtures were placed in a 96-well plate and incubated at 25 &#176;C for 150 hours, with intermittent shaking at 500 rpm.</p><p id=\"p0305\">Aggregation was monitored periodically by measuring Thioflavin T (ThT) fluorescence intensity using a Gemini-XS microplate spectrofluorometer (Molecular Devices, Sunnyvale, CA), with excitation and emission wavelengths set at 435 nm and 485 nm, respectively. Differences in aggregation were quantified by calculating percent aggregation, using the maximum fluorescence (Fmax) of the A&#946;42-alone control as a reference.</p></sec></sec><sec id=\"sec8.3\"><title>Quantification and statistical analysis</title><p id=\"p0310\">Statistical analysis in our study was primarily descriptive and focused on evaluating prediction performance of large language models. All analyses were performed using Python (v3.11.1). LLM-based outputs were generated using OpenAI GPT-4 and Claude-3-Opus-20240229 APIs. Embeddings were produced using the OpenAI text-embedding-ada-002 API.</p><p id=\"p0315\">Evaluation metrics included accuracy, precision, recall, and F1 score for binary classification of drug combination efficacy. Cosine similarity was used for k-nearest neighbor (where k = 5) retrieval of learning examples, which were used in dynamic few-shot prompting. Performance metrics reflected model behavior across test instances, not biological replicates.</p></sec></sec></body><back><ref-list id=\"cebib0010\"><title>References</title><ref id=\"bib1\"><label>1</label><element-citation publication-type=\"journal\" id=\"sref1\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nori</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>Y.T.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Carignan</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Edgar</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Fusi</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>King</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Larson</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2311.16452</pub-id></element-citation></ref><ref id=\"bib2\"><label>2</label><element-citation publication-type=\"journal\" id=\"sref2\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Romera-Paredes</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Barekatain</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Novikov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Balog</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>M.P.</given-names></name><name name-style=\"western\"><surname>Dupont</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Ruiz</surname><given-names>F.J.R.</given-names></name><name name-style=\"western\"><surname>Ellenberg</surname><given-names>J.S.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Fawzi</surname><given-names>O.</given-names></name><etal/></person-group><article-title>Mathematical discoveries from program search with large language models</article-title><source>Nature</source><volume>625</volume><year>2024</year><fpage>468</fpage><lpage>475</lpage><pub-id pub-id-type=\"pmid\">38096900</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41586-023-06924-6</pub-id><pub-id pub-id-type=\"pmcid\">PMC10794145</pub-id></element-citation></ref><ref id=\"bib3\"><label>3</label><element-citation publication-type=\"journal\" id=\"sref3\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jablonka</surname><given-names>K.M.</given-names></name><name name-style=\"western\"><surname>Schwaller</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Ortega-Guerrero</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Smit</surname><given-names>B.</given-names></name></person-group><article-title>Leveraging large language models for predictive chemistry</article-title><source>Nat. Mach. Intell.</source><volume>6</volume><year>2024</year><fpage>161</fpage><lpage>169</lpage></element-citation></ref><ref id=\"bib4\"><label>4</label><element-citation publication-type=\"journal\" id=\"sref4\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Boiko</surname><given-names>D.A.</given-names></name><name name-style=\"western\"><surname>MacKnight</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Kline</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Gomes</surname><given-names>G.</given-names></name></person-group><article-title>Autonomous chemical research with large language models</article-title><source>Nature</source><volume>624</volume><year>2023</year><fpage>570</fpage><lpage>578</lpage><pub-id pub-id-type=\"pmid\">38123806</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41586-023-06792-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC10733136</pub-id></element-citation></ref><ref id=\"bib5\"><label>5</label><element-citation publication-type=\"journal\" id=\"sref5\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>M Bran</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Cox</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Schilter</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Baldassari</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>White</surname><given-names>A.D.</given-names></name><name name-style=\"western\"><surname>Schwaller</surname><given-names>P.A.</given-names></name></person-group><article-title>Augmenting large language models with chemistry tools</article-title><source>Nat. Mach. Intell.</source><volume>6</volume><year>2024</year><fpage>525</fpage><lpage>535</lpage><pub-id pub-id-type=\"pmid\">38799228</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s42256-024-00832-8</pub-id><pub-id pub-id-type=\"pmcid\">PMC11116106</pub-id></element-citation></ref><ref id=\"bib6\"><label>6</label><element-citation publication-type=\"book\" id=\"sref6\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Holyoak</surname><given-names>K.J.</given-names></name><name name-style=\"western\"><surname>Morrison</surname><given-names>R.G.</given-names></name></person-group><part-title>The Cambridge Handbook of Thinking and Reasoning</part-title><year>2005</year><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id=\"bib7\"><label>7</label><element-citation publication-type=\"journal\" id=\"sref7\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kalyanpur</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Saravanakumar</surname><given-names>K.K.</given-names></name><name name-style=\"western\"><surname>Barres</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Chu-Carroll</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Melville</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Ferrucci</surname><given-names>D.</given-names></name></person-group><article-title>LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2406.17663</pub-id></element-citation></ref><ref id=\"bib8\"><label>8</label><element-citation publication-type=\"journal\" id=\"sref8\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Haji</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Bethany</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Tabar</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Chiang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Rios</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Najafirad</surname><given-names>P.</given-names></name></person-group><article-title>Improving LLM reasoning with multi-agent Tree-of-Thought Validator agent</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2409.11527</pub-id></element-citation></ref><ref id=\"bib9\"><label>9</label><element-citation publication-type=\"journal\" id=\"sref9\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Shetty</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Kamath</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Jaiswal</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>Y.</given-names></name></person-group><article-title>CancerGPT for few shot drug pair synergy prediction using large pretrained language models</article-title><source>npj Digit. Med.</source><volume>7</volume><year>2024</year><fpage>40</fpage><pub-id pub-id-type=\"pmid\">38374445</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41746-024-01024-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC10876664</pub-id></element-citation></ref><ref id=\"bib10\"><label>10</label><element-citation publication-type=\"journal\" id=\"sref10\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Feng</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Deng</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Ruan</surname><given-names>C.</given-names></name><etal/></person-group><article-title>DeepSeek-V3 Technical Report</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2412.19437</pub-id></element-citation></ref><ref id=\"bib11\"><label>11</label><element-citation publication-type=\"journal\" id=\"sref11\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qi</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>B.</given-names></name></person-group><article-title>Large language models as biomedical hypothesis generators: A comprehensive evaluation</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2311.05965</pub-id></element-citation></ref><ref id=\"bib12\"><label>12</label><element-citation publication-type=\"journal\" id=\"sref12\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qi</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>B.</given-names></name></person-group><article-title>Large Language Models are zero shot hypothesis proposers</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2311.05965</pub-id></element-citation></ref><ref id=\"bib13\"><label>13</label><element-citation publication-type=\"journal\" id=\"sref13\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Vilar</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Tatonetti</surname><given-names>N.P.</given-names></name></person-group><article-title>High-throughput methods for combinatorial drug discovery</article-title><source>Sci. Transl. Med.</source><volume>5</volume><year>2013</year><object-id pub-id-type=\"publisher-id\">205rv1</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1126/scitranslmed.3006667</pub-id><pub-id pub-id-type=\"pmid\">24089409</pub-id></element-citation></ref><ref id=\"bib14\"><label>14</label><element-citation publication-type=\"journal\" id=\"sref14\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Celebi</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Bear Don&#8217;t Walk</surname><given-names>O.</given-names><suffix>4th</suffix></name><name name-style=\"western\"><surname>Movva</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Alpsoy</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Dumontier</surname><given-names>M.</given-names></name></person-group><article-title>In-silico Prediction of Synergistic Anti-Cancer Drug Combinations Using Multi-omics Data</article-title><source>Sci. Rep.</source><volume>9</volume><year>2019</year><fpage>8949</fpage><pub-id pub-id-type=\"pmid\">31222109</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-019-45236-6</pub-id><pub-id pub-id-type=\"pmcid\">PMC6586895</pub-id></element-citation></ref><ref id=\"bib15\"><label>15</label><element-citation publication-type=\"journal\" id=\"sref15\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Preuer</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Lewis</surname><given-names>R.P.I.</given-names></name><name name-style=\"western\"><surname>Hochreiter</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Bender</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Bulusu</surname><given-names>K.C.</given-names></name><name name-style=\"western\"><surname>Klambauer</surname><given-names>G.</given-names></name></person-group><article-title>DeepSynergy: predicting anti-cancer drug synergy with Deep Learning</article-title><source>Bioinformatics</source><volume>34</volume><year>2018</year><fpage>1538</fpage><lpage>1546</lpage><pub-id pub-id-type=\"pmid\">29253077</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/btx806</pub-id><pub-id pub-id-type=\"pmcid\">PMC5925774</pub-id></element-citation></ref><ref id=\"bib16\"><label>16</label><element-citation publication-type=\"journal\" id=\"sref16\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Sheng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhan</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Wong</surname><given-names>S.T.C.</given-names></name></person-group><article-title>DrugComboRanker: drug combination discovery based on target network analysis</article-title><source>Bioinformatics</source><volume>30</volume><year>2014</year><fpage>i228</fpage><lpage>i236</lpage><pub-id pub-id-type=\"pmid\">24931988</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/btu278</pub-id><pub-id pub-id-type=\"pmcid\">PMC4058933</pub-id></element-citation></ref><ref id=\"bib17\"><label>17</label><element-citation publication-type=\"journal\" id=\"sref17\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bansal</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Karan</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Menden</surname><given-names>M.P.</given-names></name><name name-style=\"western\"><surname>Costello</surname><given-names>J.C.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Allen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhong</surname><given-names>R.</given-names></name><etal/></person-group><article-title>A community computational challenge to predict the activity of pairs of compounds</article-title><source>Nat. Biotechnol.</source><volume>32</volume><year>2014</year><fpage>1213</fpage><lpage>1222</lpage><pub-id pub-id-type=\"pmid\">25419740</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/nbt.3052</pub-id><pub-id pub-id-type=\"pmcid\">PMC4399794</pub-id></element-citation></ref><ref id=\"bib18\"><label>18</label><element-citation publication-type=\"journal\" id=\"sref18\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>X.-M.</given-names></name><name name-style=\"western\"><surname>Iskar</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zeller</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Kuhn</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>van Noort</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Bork</surname><given-names>P.</given-names></name></person-group><article-title>Prediction of drug combinations by integrating molecular and pharmacological data</article-title><source>PLoS Comput. Biol.</source><volume>7</volume><year>2011</year><object-id pub-id-type=\"publisher-id\">e1002323</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1371/journal.pcbi.1002323</pub-id><pub-id pub-id-type=\"pmcid\">PMC3248384</pub-id><pub-id pub-id-type=\"pmid\">22219721</pub-id></element-citation></ref><ref id=\"bib19\"><label>19</label><element-citation publication-type=\"journal\" id=\"sref19\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Tsoi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>W.J.</given-names></name></person-group><article-title>Predict effective drug combination by deep belief network and ontology fingerprints</article-title><comment>Preprint at</comment><source>J. Biomed. Inform.</source><volume>85</volume><year>2018</year><fpage>149</fpage><lpage>154</lpage><pub-id pub-id-type=\"doi\">10.1016/j.jbi.2018.07.024</pub-id><pub-id pub-id-type=\"pmid\">30081101</pub-id></element-citation></ref><ref id=\"bib20\"><label>20</label><element-citation publication-type=\"journal\" id=\"sref20\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Gautam</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Timonen</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Akimov</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Szwajda</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Jaiswal</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Turei</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Network pharmacology modeling identifies synergistic Aurora B and ZAK interaction in triple-negative breast cancer</article-title><source>NPJ Syst. Biol. Appl.</source><volume>5</volume><year>2019</year><fpage>20</fpage><pub-id pub-id-type=\"pmid\">31312514</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41540-019-0098-z</pub-id><pub-id pub-id-type=\"pmcid\">PMC6614366</pub-id></element-citation></ref><ref id=\"bib21\"><label>21</label><element-citation publication-type=\"journal\" id=\"sref21\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lewis</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Perez</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Piktus</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Petroni</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Karpukhin</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Goyal</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>K&#252;ttler</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lewis</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yih</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Rockt&#228;schel</surname><given-names>T.</given-names></name><etal/></person-group><article-title>Retrieval-augmented generation for knowledge-intensive NLP tasks</article-title><comment>Preprint at</comment><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2005.11401</pub-id></element-citation></ref><ref id=\"bib22\"><label>22</label><element-citation publication-type=\"journal\" id=\"sref22\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brown</surname><given-names>T.B.</given-names></name><name name-style=\"western\"><surname>Mann</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Ryder</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Subbiah</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kaplan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Dhariwal</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Neelakantan</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Shyam</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Sastry</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Askell</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Language Models are Few-Shot Learners</article-title><comment>Preprint at</comment><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"doi\">10.48550/ARXIV.2005.14165</pub-id></element-citation></ref><ref id=\"bib23\"><label>23</label><element-citation publication-type=\"journal\" id=\"sref23\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wei</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Schuurmans</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Bosma</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Ichter</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Chi</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Le</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>D.</given-names></name></person-group><article-title>Chain-of-thought prompting elicits reasoning in large language models</article-title><comment>Preprint at</comment><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"doi\">10.48550/ARXIV.2201.11903</pub-id></element-citation></ref><ref id=\"bib24\"><label>24</label><element-citation publication-type=\"journal\" id=\"sref24\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yao</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>Tree of thoughts: Deliberate problem solving with large language models</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2305.10601</pub-id></element-citation></ref><ref id=\"bib25\"><label>25</label><element-citation publication-type=\"journal\" id=\"sref25\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Self-consistency improves chain of thought reasoning in language models</article-title><comment>Preprint at</comment><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2203.11171</pub-id></element-citation></ref><ref id=\"bib26\"><label>26</label><element-citation publication-type=\"journal\" id=\"sref26\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Pang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Hong</surname><given-names>Y.</given-names></name></person-group><article-title>Dietary lycopene supplementation improves cognitive performances in tau transgenic mice expressing P301L mutation via inhibiting oxidative stress and tau hyperphosphorylation</article-title><source>J. Alzheimers Dis.</source><volume>57</volume><year>2017</year><fpage>475</fpage><lpage>482</lpage><pub-id pub-id-type=\"pmid\">28269786</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3233/JAD-161216</pub-id></element-citation></ref><ref id=\"bib27\"><label>27</label><element-citation publication-type=\"journal\" id=\"sref27\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Freyssin</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Carles</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Guehairia</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Rubinstenn</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Maurice</surname><given-names>T.</given-names></name></person-group><article-title>Fluoroethylnormemantine (FENM) shows synergistic protection in combination with a sigma-1 receptor agonist in a mouse model of Alzheimer&#8217;s disease</article-title><source>Neuropharmacology</source><volume>242</volume><year>2024</year><object-id pub-id-type=\"publisher-id\">109733</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.neuropharm.2023.109733</pub-id><pub-id pub-id-type=\"pmid\">37844867</pub-id></element-citation></ref><ref id=\"bib28\"><label>28</label><element-citation publication-type=\"journal\" id=\"sref28\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pirchl</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Ullrich</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Sperner-Unterweger</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Humpel</surname><given-names>C.</given-names></name></person-group><article-title>Homocysteine has anti-inflammatory properties in a hypercholesterolemic rat model <italic toggle=\"yes\">in vivo</italic></article-title><source>Mol. Cell. Neurosci.</source><volume>49</volume><year>2012</year><fpage>456</fpage><lpage>463</lpage><pub-id pub-id-type=\"pmid\">22425561</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.mcn.2012.03.001</pub-id><pub-id pub-id-type=\"pmcid\">PMC3359503</pub-id></element-citation></ref><ref id=\"bib29\"><label>29</label><element-citation publication-type=\"journal\" id=\"sref29\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Samad</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Jabeen</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Imran</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Zulfiqar</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Bilal</surname><given-names>K.</given-names></name></person-group><article-title>Protective effect of gallic acid against arsenic-induced anxiety-/depression- like behaviors and memory impairment in male rats</article-title><source>Metab. Brain Dis.</source><volume>34</volume><year>2019</year><fpage>1091</fpage><lpage>1102</lpage><pub-id pub-id-type=\"pmid\">31119507</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11011-019-00432-1</pub-id></element-citation></ref><ref id=\"bib30\"><label>30</label><element-citation publication-type=\"journal\" id=\"sref30\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gavriel</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Rabinovich-Nikitin</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Ezra</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Barbiro</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Solomon</surname><given-names>B.</given-names></name></person-group><article-title>Subcutaneous administration of AMD3100 into mice models of Alzheimer&#8217;s disease ameliorated cognitive impairment, reduced neuroinflammation, and improved pathophysiological markers</article-title><source>J. Alzheimers. Dis.</source><volume>78</volume><year>2020</year><fpage>653</fpage><lpage>671</lpage><pub-id pub-id-type=\"pmid\">33016905</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3233/JAD-200506</pub-id></element-citation></ref><ref id=\"bib31\"><label>31</label><element-citation publication-type=\"journal\" id=\"sref31\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhi</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Yin</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L.</given-names></name></person-group><article-title>Atorvastatin in improvement of cognitive impairments caused by amyloid &#946; in mice: involvement of inflammatory reaction</article-title><source>BMC Neurol.</source><volume>16</volume><year>2016</year><fpage>18</fpage><pub-id pub-id-type=\"pmid\">26846170</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s12883-016-0533-3</pub-id><pub-id pub-id-type=\"pmcid\">PMC4743318</pub-id></element-citation></ref><ref id=\"bib32\"><label>32</label><element-citation publication-type=\"journal\" id=\"sref32\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Noda</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Mouri</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mizoguchi</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Nitta</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Nabeshima</surname><given-names>T.</given-names></name></person-group><article-title>The allosteric potentiation of nicotinic acetylcholine receptors by galantamine ameliorates the cognitive dysfunction in beta amyloid25-35 i.c.v.-injected mice: involvement of dopaminergic systems</article-title><source>Neuropsychopharmacology</source><volume>32</volume><year>2007</year><fpage>1261</fpage><lpage>1271</lpage><pub-id pub-id-type=\"pmid\">17133263</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/sj.npp.1301256</pub-id></element-citation></ref><ref id=\"bib33\"><label>33</label><element-citation publication-type=\"journal\" id=\"sref33\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Carroll</surname><given-names>J.C.</given-names></name><name name-style=\"western\"><surname>Rosario</surname><given-names>E.R.</given-names></name><name name-style=\"western\"><surname>Villamagna</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Pike</surname><given-names>C.J.</given-names></name></person-group><article-title>Continuous and cyclic progesterone differentially interact with estradiol in the regulation of Alzheimer-like pathology in female 3xTransgenic-Alzheimer&#8217;s disease mice</article-title><source>Endocrinology</source><volume>151</volume><year>2010</year><fpage>2713</fpage><lpage>2722</lpage><pub-id pub-id-type=\"pmid\">20410196</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1210/en.2009-1487</pub-id><pub-id pub-id-type=\"pmcid\">PMC2875823</pub-id></element-citation></ref><ref id=\"bib34\"><label>34</label><element-citation publication-type=\"journal\" id=\"sref34\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Devi</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Ohno</surname><given-names>M.</given-names></name></person-group><article-title>A combination Alzheimer&#8217;s therapy targeting BACE1 and neprilysin in 5XFAD transgenic mice</article-title><source>Mol. Brain</source><volume>8</volume><year>2015</year><fpage>19</fpage><pub-id pub-id-type=\"pmid\">25884928</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s13041-015-0110-5</pub-id><pub-id pub-id-type=\"pmcid\">PMC4397831</pub-id></element-citation></ref><ref id=\"bib35\"><label>35</label><element-citation publication-type=\"journal\" id=\"sref35\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Deng</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>Q.L.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>Y.</given-names></name></person-group><article-title>Exploring DeepSeek: A survey on advances, applications, challenges and future directions</article-title><source>IEEE/CAA J. Autom. Sin.</source><volume>12</volume><year>2025</year><fpage>872</fpage><lpage>893</lpage></element-citation></ref><ref id=\"bib36\"><label>36</label><element-citation publication-type=\"journal\" id=\"sref36\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>W.</given-names></name></person-group><article-title>The security of using large language models: A survey with emphasis on ChatGPT</article-title><source>IEEE/CAA J. Autom. Sin.</source><volume>12</volume><year>2025</year><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id=\"bib37\"><label>37</label><element-citation publication-type=\"journal\" id=\"sref37\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Deng</surname><given-names>Z.</given-names></name></person-group><article-title>AI agents under threat: A survey of key security challenges and future pathways</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2406.02630</pub-id></element-citation></ref><ref id=\"bib38\"><label>38</label><element-citation publication-type=\"journal\" id=\"sref38\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Huo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>GameGPT: Multi-agent collaborative framework for game development</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2310.08067</pub-id></element-citation></ref><ref id=\"bib39\"><label>39</label><element-citation publication-type=\"journal\" id=\"sref39\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Du</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Torralba</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tenenbaum</surname><given-names>J.B.</given-names></name><name name-style=\"western\"><surname>Mordatch</surname><given-names>I.</given-names></name></person-group><article-title>Improving factuality and reasoning in language models through multiagent debate</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2305.14325</pub-id></element-citation></ref><ref id=\"bib40\"><label>40</label><element-citation publication-type=\"other\" id=\"sref40\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>Q.</given-names></name></person-group><article-title>Coated-LLM. Github</article-title><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/QidiXu96/Coated-LLM\" id=\"interref0010\">https://github.com/QidiXu96/Coated-LLM</ext-link></element-citation></ref><ref id=\"bib41\"><label>41</label><mixed-citation publication-type=\"other\" id=\"sref41\">AlzPED. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://alzped.nia.nih.gov/\" id=\"intref0045\">https://alzped.nia.nih.gov/</ext-link>.</mixed-citation></ref><ref id=\"bib42\"><label>42</label><element-citation publication-type=\"journal\" id=\"sref42\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>C.</given-names></name></person-group><article-title>Large Language Models as Optimizers</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/ARXIV.2309.03409</pub-id></element-citation></ref><ref id=\"bib43\"><label>43</label><element-citation publication-type=\"journal\" id=\"sref43\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Davis</surname><given-names>A.P.</given-names></name><name name-style=\"western\"><surname>Wiegers</surname><given-names>T.C.</given-names></name><name name-style=\"western\"><surname>Johnson</surname><given-names>R.J.</given-names></name><name name-style=\"western\"><surname>Sciaky</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Wiegers</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Mattingly</surname><given-names>C.J.</given-names></name></person-group><article-title>Comparative Toxicogenomics Database (CTD): update 2023</article-title><source>Nucleic Acids Res.</source><volume>51</volume><year>2023</year><fpage>D1257</fpage><lpage>D1262</lpage><pub-id pub-id-type=\"pmid\">36169237</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/nar/gkac833</pub-id><pub-id pub-id-type=\"pmcid\">PMC9825590</pub-id></element-citation></ref><ref id=\"bib44\"><label>44</label><mixed-citation publication-type=\"other\" id=\"sref44\">Research Models. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.alzforum.org/research-models\" id=\"intref0050\">https://www.alzforum.org/research-models</ext-link>.</mixed-citation></ref><ref id=\"bib45\"><label>45</label><element-citation publication-type=\"journal\" id=\"sref45\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cheng</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Kov&#225;cs</surname><given-names>I.A.</given-names></name><name name-style=\"western\"><surname>Barab&#225;si</surname><given-names>A.-L.</given-names></name></person-group><article-title>Network-based prediction of drug combinations</article-title><source>Nat. Commun.</source><volume>10</volume><year>2019</year><fpage>1197</fpage><pub-id pub-id-type=\"pmid\">30867426</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41467-019-09186-x</pub-id><pub-id pub-id-type=\"pmcid\">PMC6416394</pub-id></element-citation></ref><ref id=\"bib46\"><label>46</label><element-citation publication-type=\"journal\" id=\"sref46\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tanoli</surname><given-names>Z.U.R.</given-names></name><name name-style=\"western\"><surname>Ravikumar</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Alam</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Rebane</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>V&#228;h&#228;-Koskela</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Peddinti</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>van Adrichem</surname><given-names>A.J.</given-names></name><name name-style=\"western\"><surname>Wakkinen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jaiswal</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Drug Target Commons: A community effort to build a consensus knowledge base for drug-target interactions</article-title><source>Cell Chem. Biol.</source><volume>25</volume><year>2018</year><fpage>224</fpage><lpage>229.e2</lpage><pub-id pub-id-type=\"pmid\">29276046</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.chembiol.2017.11.009</pub-id><pub-id pub-id-type=\"pmcid\">PMC5814751</pub-id></element-citation></ref><ref id=\"bib47\"><label>47</label><mixed-citation publication-type=\"other\" id=\"sref47\">PubMed. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pubmed.ncbi.nlm.nih.gov/\" id=\"intref0055\">https://pubmed.ncbi.nlm.nih.gov/</ext-link>.</mixed-citation></ref><ref id=\"bib48\"><label>48</label><mixed-citation publication-type=\"other\" id=\"sref48\">Agora. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://agora.adknowledgeportal.org/\" id=\"intref0060\">https://agora.adknowledgeportal.org/</ext-link>.</mixed-citation></ref><ref id=\"bib49\"><label>49</label><element-citation publication-type=\"journal\" id=\"sref49\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Unsal-Beyge</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Tuncbag</surname><given-names>N.</given-names></name></person-group><article-title>Functional stratification of cancer drugs through integrated network similarity</article-title><source>NPJ Syst. Biol. Appl.</source><volume>8</volume><year>2022</year><fpage>11</fpage><pub-id pub-id-type=\"pmid\">35440787</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41540-022-00219-8</pub-id><pub-id pub-id-type=\"pmcid\">PMC9018743</pub-id></element-citation></ref><ref id=\"bib50\"><label>50</label><element-citation publication-type=\"journal\" id=\"sref50\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Casas</surname><given-names>A.I.</given-names></name><name name-style=\"western\"><surname>Hassan</surname><given-names>A.A.</given-names></name><name name-style=\"western\"><surname>Larsen</surname><given-names>S.J.</given-names></name><name name-style=\"western\"><surname>Gomez-Rangel</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Elbatreek</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kleikers</surname><given-names>P.W.M.</given-names></name><name name-style=\"western\"><surname>Guney</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Egea</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>L&#243;pez</surname><given-names>M.G.</given-names></name><name name-style=\"western\"><surname>Baumbach</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Schmidt</surname><given-names>H.H.H.W.</given-names></name></person-group><article-title>From single drug targets to synergistic network pharmacology in ischemic stroke</article-title><source>Proc. Natl. Acad. Sci. USA</source><volume>116</volume><year>2019</year><fpage>7129</fpage><lpage>7136</lpage><pub-id pub-id-type=\"pmid\">30894481</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1073/pnas.1820799116</pub-id><pub-id pub-id-type=\"pmcid\">PMC6452748</pub-id></element-citation></ref></ref-list><sec id=\"appsec2\" sec-type=\"supplementary-material\"><title>Supplemental information</title><p id=\"p0325\">\n<supplementary-material content-type=\"local-data\" id=\"mmc1\" position=\"float\" orientation=\"portrait\"><caption><title>Document S1. Figures S1 and S2, Tables S1 and S2, Methods S1, and Data S1 and S2</title></caption><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mmc1.pdf\" position=\"float\" orientation=\"portrait\"/></supplementary-material>\n</p></sec><fn-group><fn id=\"appsec1\" fn-type=\"supplementary-material\"><p id=\"p0320\">Supplemental information can be found online at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://doi.org/10.1016/j.isci.2025.113984\" id=\"intref0040\">https://doi.org/10.1016/j.isci.2025.113984</ext-link>.</p></fn></fn-group></back></article></pmc-articleset>",
  "text": "pmc iScience iScience 3532 isci iScience 2589-0042 Elsevier PMC12682125 PMC12682125.1 12682125 12682125 41362614 10.1016/j.isci.2025.113984 S2589-0042(25)02245-X 113984 1 Article Multi agent large language models for biomedical hypothesis generation in drug combination discovery Xu Qidi 1 Soto Claudio 2 Shahnawaz Mohammad 2 Liu Xiaozhong 3 Jiang Xiaoqian 1 Kim Yejin yejin.kim@uth.tmc.edu 1 4 &#8727; 1 McWilliams School of Biomedical Informatics, UTHealth Houston, Houston, TX 77030, US 2 McGovern Medical School, UTHealth Houston, Houston, TX 77030, US 3 Computer Science and Data Science, Worcester Polytechnic Institute, Worcester, MA 01609, US &#8727; Corresponding author yejin.kim@uth.tmc.edu 4 Lead contact 19 12 2025 10 11 2025 28 12 501445 113984 1 7 2025 24 9 2025 5 11 2025 10 11 2025 08 12 2025 09 12 2025 &#169; 2025 The Author(s) 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Summary Recent advancements in large language models (LLMs) have demonstrated their potential in scientific reasoning, but their ability to open-ended hypotheses in data-scarce domains remains underexplored. Here, we introduce Co mbinatorial A lzheimer&#8217;s Disease T herapeutic E fficacy D ecision (Coated-LLM), an AI-driven framework that is inspired by scientific collaboration to predict efficacious combinatorial therapy when data-driven prediction is infeasible. Coated-LLM employs multiple specialized LLM agents&#8212; Researcher , Reviewer s , and Moderator &#8212;to systematically generate and evaluate hypotheses through several in-context learning techniques. Using Alzheimer&#8217;s disease (AD) as a test case, Coated-LLM outperformed traditional knowledge-based methods (accuracy: 0.74 vs. 0.52), with external validation achieving an accuracy of 0.82. In addition, a drug combination predicted from Coated-LLM was experimentally validated to significantly reduce amyloid aggregation in vitro . These findings highlight the potential of our framework to augment human reasoning in complex scientific reasoning tasks, offering a scalable approach for hypothesis generation in biomedical research. Graphical abstract Highlights &#8226; Multi-agent LLM enhances human hypothesis generation in open scientific domains &#8226; Few-shot LLM framework offers a viable alternative when less data is available &#8226; M266 + Gypenoside XVII validated to significantly reduce amyloid aggregation in vitro Health sciences; Medicine; Drugs; Artificial intelligence Subject areas Health sciences Medicine Drugs Artificial intelligence pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Published: November 10, 2025 Introduction Recent advancements in large language models (LLMs) have demonstrated their disruptive potential in scientific discovery. These models have efficiently tackled combinatorial optimization problems, often surpassing traditional heuristics. 1 , 2 LLMs have also shown success in various chemistry and materials science tasks, such as predicting molecular properties and chemical reaction yields, 3 and autonomously searching for chemicals. 4 , 5 Our research aligns with efforts to develop &#8220;autonomous scientists&#8221; inspired by scientific collaboration. In scientific investigation, given a question, human researchers apply deductive and inductive reasoning to drive predictions and draw conclusions. 6 However, such traditional scientific reasoning is often limited by human bias and cognitive capacity. 6 Researchers may exhibit confirmation bias, favoring data that supports their preconceptions, struggle to process the vast amount of existing literature, leading to incomplete reviews and overlooked insights, and have difficulty in managing multiple factors and identifying subtle patterns. LLMs can mitigate these limitations by assisting human scientific reasoning, as evidenced in several prior studies. 7 , 8 , 9 Recent reasoning-oriented LLMs, such as OpenAI o1 and DeepSeek V3, 10 have achieved human-level performance on closed-form questions where the answer is already known, but remain underexplored for open-ended biomedical hypothesis generation due to the unclear reasoning and answers. While data-driven machine learning models work well when abundant data exists, our focus is on data-scarce research areas, which are more common in real world scientific discovery. Recent work by Qi et al. has demonstrated that LLMs can generate novel and validated biomedical hypotheses. 11 , 12 However, their approach requires background knowledge extracted from existing literature as input to generate hypotheses, which may not be available in data-scarce domains. Building on this motivation, we chose a specific biomedical question that requires deep domain knowledge and critical reasoning without definite answers to test LLM&#8217;s capacity beyond memorization: identifying effective drug combinations for in vivo experiments in complex systemic diseases. We chose to focus on Alzheimer&#8217;s disease (AD), a complex neurodegenerative condition where multiple disease etiologies entangle together; thus, a comprehensive consideration of multiple underlying mechanisms is critical when developing therapeutics. Drug combination therapy is to use of two or more therapeutic agents to treat a single disease, to achieve a more effective treatment outcome than what could be achieved with a single drug. This approach is particularly prevalent in the treatment of complex diseases such as diabetes and metabolic syndrome, cardiovascular disease, cancer, and others, 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 but no successes in AD due to the multifactorial nature of the disease. Until now, only Donepezil + Memantine has been FDA-approved for AD. Developing effective combinatorial therapies faces significant challenges, particularly in selecting the right therapeutic agents (drugs) and relevant in vivo models. Researchers make specific predictions about the potential efficacy of various combinations from general principles and known mechanisms of action (deductive reasoning). The complexity arises from the factorial growth in the number of possible combinations (therapeutic agent 1 &#8727; therapeutic agent 2 &#8727; in vivo model), making manual evaluation impractical. Only a few studies have explored the use of LLMs for such combinatorial search problems. A recent study 9 introduced CancerGPT, an LLM-based model designed for predicting drug pair synergy in rare tissues with limited data. It demonstrates the capability of LLMs to handle complex biological inference tasks. However, CancerGPT primarily focuses on fine-tuning LLMs using high-throughput in vitro experimental data, which is not available for most complex diseases. In contrast, we propose a versatile and generalizable LLM-based framework, named Co mbinatorial A lzheimer&#8217;s disease T herapeutic E fficacy D ecision (Coated-LLM), designed specifically to generate biomedical hypotheses even in data-scarce settings. Our innovative approach, inspired by collective human scientific reasoning, employs multiple specialized LLM agents ( Researcher , Reviewer s , Moderator) to generate biomedical hypotheses. Critically, our framework identified a novel combinational therapy, m266 antibody with Gypenoside XVII, that experimentally demonstrated superior inhibition of amyloid beta aggregation compared to individual treatments alone, highlighting a previously unexplored synergistic combinatorial therapy in AD. Comprehensive computational evaluation alongside these in vitro validations demonstrates that Coated-LLM effectively identifies potent therapeutic combinations, significantly augmenting human capabilities for scientific discovery in complex diseases. Results Model summary We create an AI model f that automates scientific reasoning to generate hypotheses on efficacious combinatorial therapy for in vivo experiments ( Algorithm 1 ; Figure 1 ). Inspired by human scientific collaboration, our framework consists of multiple LLM agents playing different roles: Researcher , Reviewer s , and Moderator . The R esearcher generates a series of reasoning steps to propose a prediction on the efficacy of combinatorial therapeutic agents. Multiple Reviewers review and criticize the quality of the prediction generated by the Researcher and offer feedback. Finally, the Moderator integrates Researcher &#8217;s proposed prediction and Reviewers &#8217; feedback to suggest a more valid prediction. Algorithm 1 Framework for Drug Combination Efficacy in Alzheimer's Disease Input: Dataset D = {(t1, t2, m)}, external knowledge B_q Output: Predictions A_Q for test set Split D into training set D_train and test set D_test # Phase I - Warm Up: &#160; learning_examples=[] &#160; learning_question_embeddings = [] &#160; for each (t1, t2, m) in D_train &#160; &#160; q = transform_into_question(t1, t2, m) &#160; &#160; B_q = search_external_knowledge(t1, t2) &#160; &#160; E_q = generate_learning_question_embedding(q) &#160; &#160; (C_q, A_q) = LLM_ Researcher (instruction, q, B_q) &#160; &#160; if correct(A_q) then &#160; &#160; &#160; learning_examples.append((q, C_q, A_q)) &#160; &#160; &#160;learning_question_embeddings.append(E_q) # Phase II - Inference: &#160; for each (t1, t2, m) in D_test &#160; &#160; Q = transform_into_question(t1, t2, m) &#160; &#160; B_Q = search_external_knowledge(t1, t2) &#160; &#160; E_Q = generate_testing_question_embedding(Q) &#160; &#160; top_k_examples = select_top_k_similar_question(E_Q, learning_question_embeddings, learning_examples, k=5) &#160; &#160; input = concatenate(top_k_examples, Q, B_Q) &#160; &#160; hypotheses = [] &#160; &#160; Repeat 5 times do &#160; &#160; &#160; &#160;(C_Q, A_Q) = LLM_ Researcher (instruction, input) &#160; &#160; &#160; hypotheses.append((C_Q, A_Q)) &#160; &#160; A_Q*= majority_vote([A_Q for _, A_Q in hypotheses]) &#160; &#160; C_Q*=select_longest_CoT([C_Q for C_Q, A_Q in hypotheses if A_Q==A_Q*]) # Phase III - Revision &#160; for each (t1, t2, m) in D_test &#160; &#160; Q = transform into_question(t1, t2, m) &#160; &#160; F_Q = LLM_ Reviewers (instruction, C_Q*, A_Q*) &#160; &#160; hypotheses = [] &#160; &#160; Repeat 5 times &#160; &#160; &#160; (C_Q, A_Q) = LLM_ Moderator (instruction, Q, F_Q, C_Q*, A_Q*) &#160; &#160; &#160; hypotheses.append((C_Q, A_Q)) &#160; &#160; &#160; A_Q* = majority_vote([A_Q for _, A_Q in hypotheses])) &#160; &#160; &#160; Return A_Q* Figure 1 Study overview (A) Traditional approach: Given the vast number of possible drug combinations, human experts rely on general principles and known mechanisms of action to manually select potential candidates. The top-score combinations are then subjected to in vitro experiments to evaluate their efficacy. (B) Coated-LLM workflow: Coated-LLM is a structured framework that was inspired by collective scientific discovery processes to generate hypotheses on efficacious combinatorial therapy. For a target drug combination, Researcher learns from its top five relevant questions from learning examples, generates a series of reasoning steps to propose a prediction on the efficacy, and gets a consistent prediction. Multiple Reviewers then provide feedback, and the Moderator integrates consistency prediction from the Researcher and feedback from the Reviewer s to generate the final consensus prediction. (C) In vitro Experiments: Coated-LLM selects the most promising drug combinations from false-positive augmented data for in vitro efficacy testing. Throughout all the communication among LLM agents, we prompt the LLM agents to utilize various in-context learning techniques such as integrating external biomedical knowledge as retrieval augmented generation (RAG), 21 few-shot learning, 22 self-generated chain-of-thoughts (CoT), 23 and/or tree of thoughts (ToT), 24 and self-consistency. 25 Data collection and augmentation Following literature mining, we identified 242 articles reporting 250 drug combinations with positive efficacy and 30 with negative efficacy ( Figure 2 A). In comparison, even the rare tissue subsets in the CancerGPT study (e.g., 352 samples for soft tissue, 1,190 for stomach) 9 are significantly larger. This highlights the relative scarcity of AD-related combination data. In addition, our literature collection showed severe positive bias. To address it, we performed data augmentation. As a result, we have a total of 530 combinations (250 combinations with positive efficacy; 30 combinations with negative efficacy, augmented by 250 combinations with noisy, non-positive efficacy). Figure 2 B summarizes the top five most frequently mentioned terms across therapeutic agents, animal models, and pathways. Figure 2 Distribution of drug combinations and efficacy in the literature (A) Data collection from literature. The process began with an initial pool of articles from the AlzPED, followed by additional searches conducted in PubMed. Articles were screened and excluded based on predefined criteria. The final selected literature included articles that reported drug combinations with positive or negative efficacy. (B) Top 5 frequent terms in therapeutic agents, animal models, and pathways. Model development In the warmup phase, the Researcher correctly predicted 231 training combinations (134 combinations with positive efficacy; 97 combinations with non-positive efficacy), which became learning examples for the inference phase. Following, 1 we used k = 5 examples for the few-shot learning, achieving a high average similarity score between test questions and selected five examples (0.919 &#177; 0.017), while also maintaining contextual diversity among selected examples. Further, to demonstrate that the selected questions were more relevant to the test questions than other questions from the learning examples in the inference phase, we calculated the mean cosine distance between the test question embeddings and the learning examples' embeddings. The mean top five average cosine distance was 0.08 (variance: 0.0002), while the mean overall average cosine distance was 0.13 (variance: 0.0003). Figure S2 provided a visual representation of similarities between the target combination and learning examples. In the revision phase, among 156 testing combinations, 129 demonstrated more than 80% consistency across 5 rounds, with 83 of them achieving 100% consistency. Coated-LLM achieved significant accuracy in predicting drug combination efficacy The Coated-LLM framework significantly surpassed the traditional network-based approach (no data-driven machine learning models available) in predicting the efficacy of drug combinations. Specifically, on a test set with 156 drug combinations, Coated-LLM achieved an accuracy of 0.74, precision of 0.71, recall of 0.80, and an F1-score of 0.75, with an average confidence of 0.87 and ECE of 0.17. Table 1 presents the contingency table for Coated-LLM&#8217;s predictions and examples of misclassifications. In comparison, the traditional network-based model yielded substantially lower performance metrics, with an accuracy of 0.52, precision of 0.46, recall of 0.16, and an F1 score of 0.24 ( Table S1 ). Our model demonstrates superior flexibility by effectively accommodating therapeutic agents beyond conventional pharmaceuticals, such as membrane-free stem cell extracts, for which traditional gene-target data are often unavailable. This significant accuracy, even without data-driven model training, underscores the ability of Coated-LLM to identify effective combinatorial therapies in AD in a scalable way. Table 1 Contingency table of prediction outcomes with examples using Coated-LLM Therapeutic agent 1 Therapeutic agent 2 Animal model Predicted efficacy Actual efficacy (reference) True positive ( n = 61) Lycopene Vitamin E Tau P301L Positive Positive 26 Donepezil Fluoroethylnormemantine Swiss OF-1 mice Positive Positive 27 False positive ( n = 25) Cholesterol Homocysteine Sprague-Dawley rats Positive Non-positive 28 m266 Gypenoside XVII 3xTg-AD transgenic mice Positive Non-positive (Augmented data) False negative ( n = 1 5 ) Gallic Acid Sodium Arsenite Male rats Non-positive Positive 29 AMD3100 L-Lactate 3xTg Non-positive Positive 30 True negative ( n = 5 5 ) Atorvastatin Farnesol C57BL/6 Non-positive Non-positive 31 Galantamine Mecamylamine ICR Non-positive Non-positive 32 Retrospective in vitro validation To evaluate the generalizability of Coated-LLM and address data leakage concerns, we conducted in vitro validation with an independent private dataset. This dataset comprises eleven drug combinations and in vitro efficacy in cell lines, and includes nine non-positive and two positive efficacies, skewed toward negative efficacy. Although our model is developed for predicting in vivo efficacy, we assume that the model for in vivo efficacy can also capture in vitro efficacy. Despite the increased difficulty due to realistic distribution skew, Coated-LLM achieved an accuracy of 0.82 ( Table 2 ), whereas the baseline network-based model achieved 0.27 ( Table S2 ). Table 2 presents the contingency table for Coated-LLM&#8217;s predictions and examples of misclassifications in the external validation. The precision, recall, and F1-score for our model were each 0.50, reflecting the challenging nature of the task but still outperforming the baseline. Table 2 Contingency table of prediction outcomes for the external data with examples Therapeutic agent 1 Therapeutic agent 2 Model Predicted efficacy Actual efficacy True positive ( n = 1) Galantamine Caffeine HT22 Mouse Hippocampal Neuronal Cell Line Positive Positive False positive ( n = 1) Donepezil Salicylic HT22 Mouse Hippocampal Neuronal Cell Line Positive Non-positive False negative ( n = 1) Galantamine Mifepristone HT22 Mouse Hippocampal Neuronal Cell Line Non-positive Positive True negative ( n = 8) Galantamine Diclofenac HT22 Mouse Hippocampal Neuronal Cell Line Non-positive Non-positive Rivastigmine Lithium HT22 Mouse Hippocampal Neuronal Cell Line Non-positive Non-positive Experimentally validated drug combinations demonstrate our model&#8217;s practical utility To demonstrate practical utility, we prospectively tested the top-ranked drug combinations predicted by Coated-LLM through in vitro amyloid beta aggregation experiments. We selected the top three most promising drug combinations from false positives for experimental validation ( Table 3 ). Across a total of 24 experimental conditions, we assessed individual drugs, their combinations, and controls using two independent assays. Among these, Gypenoside alone resulted in approximately 50% inhibition of A&#946;42 aggregation ( Figure 3 ). The m266 alone, an anti-amyloid beta monoclonal antibody, did not show significant inhibition. Remarkably, Gypenoside combined with m266 exhibited even greater inhibition of A&#946;42 aggregation. The synergy between Gypenoside and m266 not only validates the predictive capabilities of Coated-LLM but also highlights its potential to uncover novel and effective combinatorial therapies that could otherwise remain unexplored. Table 3 Selected drug combination candidates Therapeutic agent 1 Therapeutic agent 2 Researcher&#8217;s reasoning M266, an anti-amyloid beta (A&#946;) monoclonal antibody Gypenoside XVII, a bioactive compound derived from Gynostemma pentaphyllum m266 directly targets and clears &#946;-amyloid plaques. Gypenoside XVII activates ER&#946;, which can reduce plaque production, enhance plaque clearance, and provide neuroprotection. Gypenoside XVII could potentiate m266&#8217;s plaque-clearing effect by enhancing clearance through ER&#946; activation. Acamprosate, an NMDA receptor modulator Melatonin, a biogenic amine Acamprosate reduces glutamate-induced neurotoxicity. Melatonin has neuroprotective properties, including anti-amyloid, antioxidant, and anti-inflammatory effects. Melatonin increases GABA receptor expression, which could potentially enhance the GABAergic effects of Acamprosate. Both drugs have anti-inflammatory properties, which could provide an additive neuroprotective effect. Memantine, an NMDA receptor antagonist Atorvastatin, an HMG-CoA reductase inhibitor Memantine blocks NMDA receptors, reducing excitotoxicity and slowing disease progression. Atorvastatin provides neuroprotection, reduces beta-amyloid peptide production, enhances cerebral blood flow, and modulates the immune response. Memantine&#8217;s reduction of excitotoxicity could be enhanced by Atorvastatin&#8217;s neuroprotective effects. Atorvastatin&#8217;s ability to reduce beta-amyloid peptide production could further slow disease progression alongside Memantine&#8217;s effects. Figure 3 Inhibitory effects of therapeutic agents on amyloid beta aggregation (A) The aggregation profiles of amyloid beta, both in the absence and presence of various combinations of compounds, are depicted. Error bars represent the standard error of the mean (SEM). (B) The percentage of aggregation is presented to better illustrate the effect of the different compounds. Ablation study highlights key contributing factors Aiming to understand the contributions of each component within our model, we conducted an ablation study ( Figure 4 ). The ablation study begins with a zero-shot GPT-4 model, which serves as the baseline. In this setting, GPT-4 leverages its pre-learned knowledge to make predictions. Introducing dynamic few-shot 1 results in a slight performance decrease, likely due to probable mislabeled augmented combinations used as negative few-shot examples. Despite this performance decrease, this augmentation strategy still remains necessary to mitigate reporting bias. Augmented data helps balance the number of examples with positive and negative results in our learning examples, preventing the dynamic few-shot learning examples from being overoptimistic due to the reporting bias (always being positive efficacy), which would otherwise lead to biased predictions, skewing predictions toward more frequently observed positive combinations. For comparison, we conducted the same ablation study under a non-augmentation setting (no negative augmented data), in which the data contains only 250 positive and 30 negative combinations ( Figure S1 ) to assess the impact of data availability and class balance on model performance. With this unbalanced data, we observed a clear accuracy improvement thanks to the dynamic few-shot strategy. However, the models were overly optimistic, and the predicted results were biased toward positive efficacy. Figure 4 Visual illustration of Coated-LLM components and additive contributions to the performance Coated-LLM combines kNN-based five-shot dynamic learning example selection, external pathway knowledge, self-consistency ( n = 5), Reviewer s , and Moderator . Subsequently, applying a RAG, we integrated external biomedical knowledge on pathways to address knowledge gaps in GPT-4&#8217;s pre-trained model and led to significant improvements in accuracy (+17%), precision (+13%), recall (+40%), and F1-score (+23%). By implementing self-consistency via an ensemble strategy, we increased the accuracy rate by 6%, precision by 4%, recall by 3%, and F1-score by 4%. Finally, incorporating feedback from the Reviewer s and Moderator further improved the model&#8217;s predictions, correcting potential errors to achieve the highest accuracy (+5%) and precision (+9%). However, we observed a decrease in recall due to Reviewer s and Moderator , suggesting that two LLM agents in the revision phase favor reducing false positives over false negatives, analogous to the fact that (human) reviewers are more skeptical than (human) researchers. Despite the decrease in the recall, we kept the Reviewer s and Moderator because, in real in vivo experiments, our model is used to retrieve a top ranked list of probable positive combinations, thus high precision is more important than high recall. Discussion In this study, we introduced Coated-LLM, an innovative AI-driven framework that leverages multiple specialized LLM agents to systematically generate, evaluate, and revise biomedical hypotheses for identifying efficacious combinatorial therapies for AD. Our framework demonstrated robust predictive accuracy through both internal cross-validation and a retrospective in vitro dataset, underscoring its potential for real-world application. In addition, experimental validation further reinforced the practical utility of our framework. Computational analysis showed that for a single testing drug combination, Coated-LLM required approximately 4 minutes and $0.95 in total. Specifically, the fully equipped Researcher (incorporating dynamic few-shots, external knowledge integration, and self-consistency) required approximately 15,000 tokens, 90 seconds of processing time, and $0.53 in computational costs. The R eviewer s consumed approximately 1,600 tokens, 27 seconds, and $0.07, and Moderator , executing five self-consistent runs, utilized approximately 11,000 tokens, 100 seconds, and $0.35. This level of resource usage suggests that Coated-LLM is a viable solution for scalable application in academic and industrial environments, thereby enhancing the traditional drug discovery process by uncovering therapeutically valuable combinations that might remain unexplored due to the complexity, cost, and limitations of conventional experimental approaches. In addition, Coated-LLM provides a generalizable insight for developing an LLM framework. The following lessons were derived from our study. &#8226; Interaction of Multi-Agent LLM: In our multi-agent LLM framework, Reviewer s and Moderator operate in a manner similar to human peer reviewers and journal editors, prioritizing rigor and skepticism over inclusivity, avoiding false positives over false negatives. Given that our model is designed to discover the most probable positive drug combinations for in vivo experiments, where false positives carry high experimental costs, favoring precision over recall is a necessary trade-off. &#8226; Implications of Dynamic Few-Shot Learning: Dynamic few-shot learning plays a crucial role in enhancing the predictive capabilities of LLMs by leveraging examples that are most similar to the target data. Our findings reveal that the inclusion of high-quality real learning examples significantly enhances the accuracy of predictions compared to the zero-shot strategy. In contrast, the use of augmented learning examples does not yield a similar increase in accuracy, showing the importance of high-quality examples. &#8226; Importance of a balanced set of learning examples: Given that the initial dataset from literature mining contains 250 positive combinations (89.28%), the predominance of the positive class in the dynamic few-shot learning examples can introduce bias, potentially leading to skewed predictions toward more frequently observed positive combinations. Our data augmentation strategy balanced the distribution of positive and non-positive learning examples. Although this approach resulted in a slight performance decrease, it remains essential for reducing bias and preventing overly optimistic predictions. In addition, we analyzed the failure modes of Coated-LLM&#8217;s reasoning when evaluated against experimental ground-truth. In the case of estradiol and continuous progesterone combination, our model incorrectly assumed that continuous progesterone would reduce amyloid pathology and improve cognition. However, experimental evidence demonstrated that while continuous progesterone reduced tau hyperphosphorylation, it failed to lower amyloid levels and in fact antagonized the beneficial cognitive effects of estradiol, which was distinct from the effects observed with cyclic progesterone. 33 Similarly, for the combination of cholesterol and homocysteine, our model predicted therapeutic benefit by jointly targeting two pathological mechanisms. In contrast, empirical findings showed only the partial reduction of inflammation without recovery of cognitive function. 28 In the case of BACE1 haploinsufficiency and neprilysin overexpression combination, our model emphasized dual targeting of amyloid production and clearance. However, experimental results revealed no additional benefit beyond neprilysin overexpression alone, which was sufficient to abolish amyloid deposition and rescue memory, thus leaving no room for further improvement. 34 These failure cases showed common reasoning errors of Coated-LLM, such as the overgeneralization of prior knowledge and a lack of sensitivity to ceiling effects. In all, Coated-LLM presents a transformative opportunity for drug discovery across AD and other multifactorial conditions by reducing reliance on extensive experimental screening. Its ability to effectively predict drug combination efficacy from limited or scarce data holds promise for accelerating therapeutic innovation, optimizing resource allocation, and substantially decreasing costs associated with drug development. Limitations of the study While we have obtained promising results, our study has several limitations. One of the primary limitations is the underrepresentation of negative combinations within the dataset obtained through literature mining, introducing unavoidable bias toward positive outcomes during the model&#8217;s inference phase. While the data augmentation approach helped balance this bias, it is important to acknowledge that these augmented combinations could be false negatives. Furthermore, determining the true efficacy or non-efficacy of such combinations ultimately requires experimental validation, and expert review alone may not fully resolve such uncertainty. As a result, a certain degree of mislabeling may exist and could potentially lead to incorrect predictions. However, given that truly efficacious drug combinations are relatively rare in the real world, the likelihood of falsely labeling an efficacious pair as negative is relatively low. In addition, our experts manually reviewed a representative subset of augmented combinations and concluded that the theoretical conclusion of non-positive remains justifiable without the empirical testing of these combinations. Based on these considerations, we regarded the trade-off acceptable in order to improve the class balance of the learning examples. The use of LLMs in biomedical hypothesis generation may raise ethical concerns, particularly regarding potential hallucinated reasoning or misleading predictions that may influence human decisions. 35 , 36 However, recent studies have shown that multi-agent collaborative frameworks can effectively reduce hallucinations in LLM outputs, 37 , 38 , 39 which aligns with the design of our Coated-LLM that incorporates Reviewer s and Moderator agents. While our framework is designed to generate hypotheses and predictions, we acknowledge that the outputs require rigorous reasoning checks and experimental validation before clinical consideration. In addition, false positive predictions could lead to costly experimental investigations. Our multi-agent system with Reviewers and Moderator specifically addresses this concern by prioritizing precision over recall to minimize false positives. It is important to acknowledge that the retrospective in vitro validation is limited by the small, private dataset, whereas our model was trained on in vivo experimental outcomes. Despite this difference in modality, the in vitro dataset remained within the same disease context (Alzheimer&#8217;s Disease), allowing a within-domain but cross-modality assessment, since both in vitro and in vivo assays ultimately aim to evaluate whether drug combinations can slow or treat Alzheimer&#8217;s Disease. In addition, the inclusion of the cross-validation test set provides a complementary evaluation, supporting the generalizability of our findings. Given that the initial learning examples contain only 97 non-positive efficacy combinations out of 231 (41.9%), such bias presents a considerable challenge for our model in accurately predicting non-positive outcomes. Despite these challenges, our model achieved a significant accuracy rate, demonstrating its effectiveness in generating hypotheses for synergistic drug combinations with minimal historical data. While only three top-ranked combinations were selected for experimental validation, this selection reflects our prioritization of disease-modifying mechanisms over symptomatic targets or herbal compounds. We acknowledge that expanding validation to a broader range of predicted candidates would strengthen the robustness and generalizability of our findings. Nevertheless, we interpret these results as preliminary, and further validation on larger in vivo datasets, additional experimental assays, and applications to other disease domains is needed to substantiate broader claims of generalizability. Resource availability Lead contact Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Yejin Kim ( yejin.kim@uth.tmc.edu ). Materials availability This study did not generate new unique reagents. Data and code availability &#8226; The data generated from literature mining and drug hit AD genes for this study can be accessed via the following link: https://github.com/QidiXu96/Coated-LLM . 40 &#8226; The code for Researcher , Reviewer s , and Moderator can be found at https://github.com/QidiXu96/Coated-LLM . 40 Acknowledgments YK is supported in part by 10.13039/100000002 National Institutes of Health under award number R01AG082721 , R01AG066749 , and R01AG084637 . Author contributions Concept and design: QX and YK.; data access and analysis: Y.K. model development: Q.X. and Y.K. interpretation: C.S., M.S., Q.X., and Y.K.; draft article: C.S., M.S., Q.X., Y.K., and X.L. in vitro experiments design: C.S., M.S., and X.J. All authors contributed to editing the article, approved the final article, and accepted the responsibility to submit it for publication. Declaration of interests No competing interest to declare. STAR&#9733;Methods Key resources table REAGENT or RESOURCE SOURCE IDENTIFIER Data Literature mining and data augmentation This paper https://github.com/QidiXu96/Coated-LLM 40 Software and algorithms OpenAI API Python package https://openai.com/api/ Claude API Python package https://www.anthropic.com/api Method details Problem formulation Our objective is to predict whether a combination of therapeutic agents t 1 and t 2 have a positive efficacy y when tested in an in vivo model m . That is, we aim to develop a model f such that y = f ( x ), where x is a triplet of ( t 1 , t 2 , m ). Here, t 1 , t 2 , and m are not only drawn from a finite set, but can be a new or investigational therapeutic agent (e.g. &#8220; Membrane-free stem cell extract&#8221; ) or in vivo model (e.g. &#8220; Rats induced with AD using aluminum chloride &#8221;), which are not registered with a formal identifier, thus best described as a natural text. We convert the structured input ( t 1 , t 2 , m ) into a natural text question Q following. 9 For example, we convert the combination (&#8220;Galantamine&#8221;, &#8220;Nicotine&#8221;, &#8220;ICR mice&#8221;) into &#8216; Decide if the combination of Galantamine and Nicotine is effective or not to treat ICR mice model in theory.&#8217; In some previous studies, 9 the effectiveness of combinations of therapeutic agents was measured as synergy. However, the synergy quantification requires dose-dependent inhibition profiling, which is not available in an in vivo model. As most in vivo experiments only report efficacy (without formal calculation of synergy, toxicity, or dose&#8211;response relationships), our focus is also on efficacy (rather than synergy). Rather than a specific efficacy measurement, we focused on a broad sentiment as an efficacy measurement (positive or not). While this binary labeling simplifies the underlying pharmacological nuances, it makes more transferable to different studies. Data collection We collected scientific articles that report the efficacy of therapeutic agent combinations on AD in vivo models. We first utilized the Alzheimer&#8217;s Disease Preclinical Efficacy Database (AlzPED), 41 a data resource dedicated to the preclinical efficacy studies of candidate therapeutics for Alzheimer's Disease. Among the 1,463 articles in AlzPED, we manually reviewed and selected 39 articles that experimented with multiple therapeutic agents. We further searched for more related articles based on the 39 selected articles. Specific search queries are available in Data S1 . We extracted 376 additional articles meeting the query from PubMed. 10 out of 39 (25.64%) AlzPED articles were searchable from the query. We then extracted therapeutic agents, in vivo models, and their efficacy from the abstract of the selected articles. We excluded articles in which drugs were used to induce AD or suppress mechanisms for mechanistic study. Among the 376 articles, 199 articles reported positive efficacy, and 3 reported mixed or partial effects, 16 reported negative efficacy. All others are not relevant. Data augmentation Our initial dataset showed severe imbalance toward positive efficacy, as researchers tend to publish positive results more than negative ones. In addition to combinations reporting non-positive efficacy in the literature, we created plausible samples with unknown efficacy (unlabeled data), and used them as non-positive samples with noise in both the warm-up phase and inference phase. The non-positive samples were created by randomly replacing either one of the drugs or an in vivo model from the positive efficacy combinations, with the replacement therapeutic agents or models selected from those commonly used in AD research from AlzPED dataset. This constraint ensures that the augmented combinations remain within the AD therapeutic domain, preventing obvious non-positive cases that would result from replacing AD drugs with treatments for unrelated conditions like obesity. For example, given an efficacious combination ( Acamprosate, Baclofen, mThy1-hAPP751 (TASD41) ), we created a non-positive combination by replacing Baclofen. As a result, we have ( Acamprosate, Melatonin, mThy1-hAPP751 (TASD41) ). This random replacement strategy generated combinations that were statistically unlikely to be efficacious, as the probability of two randomly selected therapeutic agents showing efficacy is extremely low by nature, thereby not only balance the dataset, but also creating more truly novel cases that couldn&#8217;t have been memorized during LLMs&#8217; pre-training. To further address potential data leakage concerns, we had an independent private dataset (see retrospective in vitro validation ) that was never published, ensuring our model&#8217;s predictions were not simply regurgitations of previously seen information. Coated-LLM Warm-up phase Overview In the warm-up phase, Researcher generates answers to training questions and compares them with ground truth answers. The correctly generated answers are used as learning examples in the next inference phase. For this purpose, we split the data into 70% training and 30% testing sets, which are used to derive learning examples in the warm-up phase and actual inference in the next phase, respectively. This training set is not for actual training nor fine-tuning LLMs but for learning examples. We set aside a higher proportion for training to ensure Researcher can be exposed to diverse learning examples. Chain-of-thoughts (CoT) To improve the reasoning ability of Researcher , we applied a chain-of-thought (CoT) 23 prompting strategy by incorporating the instruction: &#8220;Take a deep breath and work on this problem step-by-step.&#8221;. 42 This approach encourages Researcher to decompose complex drug combination efficacious task into a series of intermediate steps, such as identifying drug targets and mechanisms of action, analyzing biological pathways, evaluating multi-pathway targeting, before reaching a final conclusion. (Prompt at Data S2 List S1.1). Retrieval augmented generation (RAG) To make Researcher answers a question q in the training set more intelligently, we allowed it to use external biomedical knowledge. Since LLMs generate responses based on patterns learned during training, which are inherently limited by the data they were pre-trained on. However, information on therapeutic agents is vast and continuously expanding. To bridge this gap, we provided external biomedical knowledge to enhance Researcher &#8217;s responses through retrieval-augmented generation (RAG), 21 which complements static LLM parameters with up-to-date and dynamic information. 21 We retrieved and provided specific external information B q on therapeutic agents t 1 , t 2 . We used the Comparative Toxicogenomics Database (CTDbase), 43 a knowledge database encompassing 88,144,004 relationships in chemicals, genes, pathways, and diseases. We particularly focused on the pathway information that the therapeutic agents t 1 , t 2 targets. Only pathways with a corrected p-value below 0.01 are incorporated as external knowledge. Of the total combinations, 129 had pathway information for both drugs available from CTDbase, and 235 had information for only one of the two drugs. For example, for the therapeutic agent Galantamine, we provided molecular pathway information such as &#8220; Galantamine has several pathway information, such as cholinergic synapse, transmission across chemical synapses, highly calcium permeable postsynaptic nicotinic acetylcholine receptors, &#8230;, and peptide hormone metabolism. &#8221;. Note that we also tried to incorporate a list of targeting genes as external knowledge, and it did not provide high-quality answers due to its high sparsity. Also note that the in vivo model information, such as one available in AlzForum, 44 marginally increased the generation quality while consuming many tokens. Based on the targeting pathway information B q , we prompt Researcher to generate a hypothesis. This hypothesis consists of a series of CoT reasoning ( C q ) and a final binary answer A q (Output example at Data S2 List S1.2). We only focused on the correct answers and its corresponding reasoning as learning examples and filtered out C q if the answer A q is different from the ground truth efficacy label y . This simple filtering has greatly decreased the low-quality chain-of-throughs examples. 1 We used GPT-4 for Researcher . To encourage Researcher to be skeptical, we added the statement &#8216;It is rare for combinations of two drugs to be efficacious and synergistic in real world&#8217; into the prompt (Prompt at Data S2 List S1.1). Inference phase Overview Using the learning examples from the warm-up phase, Researcher generates hypotheses to the questions in the testing set. In the inference phase, Researcher leverages the learning examples (dynamic few-shot learning) and external biomedical knowledge (RAG), following the same methodology as in the warm-up phase. Dynamic Few-shot When asked scientific questions, human researchers look for similar previously that were answered previously and performed inductive reasoning. So does Researcher by leveraging dynamic few-shot learning. 1 Few-shot learning 22 is one of the most effective in-context learning methods to guide LLMs to learn the patterns from a few demonstration examples and to generate similar outcomes like the examples. Here, it is critical to provide examples that are relevant to the question of interest. 1 However, in our application on AD combinatorial therapy discovery, the therapeutic and their associated biological mechanisms are very diverse, making randomly selected examples insufficient for pattern learning. For example, the question Q from ( &#8216;Galantamine &#8217;, &#8216;Nicotine &#8217;, &#8216; ICR mice &#8217;) is more similar to one from ( &#8216;Galantamine&#8217;, Memantine&#8217;, &#8216;ICR mice &#8217;) than one from ( &#8216;Scyllo-inositol&#8217;, &#8216;neotrofin&#8217;, &#8216;TgCRND8&#8217; ). Thus, we selected the most similar question q in the learning examples and its associated reasoning C q for inductive reasoning in the inference phase. We derived textual embedding E Q of the question Q of interest and E q of the question q in the learning examples using OpenAI&#8217;s text-embedding-ada-002. 1 We then calculated cosine similarity &lt; E q , E Q &gt;/(&#8214; E q &#8214; &#183; &#8214; E Q &#8214;)to identify the top five similar questions q with the highest similarity. So, the prompt consisted of similar learning examples ( C q , A q ), interest question Q , and external biomedical knowledge B Q . Note that we guided LLMs to have a series of CoT reasoning not only by simply encouraging LLM to &#8220; think step by step, &#8221; but by providing the exact CoT demonstration C q in the learning example from dynamic few-shot learning. See the full prompt and the output examples at Data S2 List S2.1 &amp; S2.2. Self-consistency via ensemble To increase the reliability of LLM&#8217;s prediction, we generated the response ( C Q , A Q ) multiple times. We aggregated them by obtaining consensus prediction A Q &#8727; via majority vote and selecting the most detailed (thus longest) chain of thought C Q &#8727; if its paired answer A q is the same as the majority. This ensemble technique can minimize the risk of incorrect prediction by cross-verifying multiple outputs. 25 Revision phase Evaluate The theoretical inductive reasoning inevitably carries uncertainty. It is critical to independently evaluate the validity of hypotheses and revise accordingly. After we obtain the hypothesis on efficacy A Q &#8727; and reasoning C Q &#8727; in the inference phase, Reviewers need to critically evaluate whether the hypothesis is logical and reasonable. This review process should be independent, thus we used another LLM with comparable performance to Researcher (GPT-4), Claude-3-opus, to enhance the independence of the reviewing process. Tree-of-thoughts (ToT) Reviewers should have diverse perspectives than Researcher to critically evaluate Researcher &#8217;s hypothesis and identify potential pitfalls that Researcher could not spot. Thus we encouraged Reviewers to have multiple perspectives and discuss different branches of thoughts via tree-of-thoughts (ToT) reasoning. 24 This approach explores different possibilities and then converges on the most optimal solution. We prompted Reviewers by instructing, &#8220; Imagine three different experts who are in therapy development for Alzheimer&#8217;s disease, are tasked with critically reviewing the reasoning&#8230; &#8221; (Prompt and output example at Data S2 List S3.1 &amp; S3.2). Revise Once Reviewers finish the discussion and provide feedback F Q , Moderator aggregated the Reviewers&#8217; feedback and Researcher &#8217;s hypothesis to obtain the final decision. Moderator took input of Q , C Q &#8727; , A Q &#8727; F Q and deduced the final revised reasoning C Q &#8727; and answer A Q &#8727; . See the full prompt and output example at Data S2 List S4.1 &amp; S4.2. Evaluation We evaluate whether the prediction of Coated-LLM is accurate by comparing the binary prediction (i.e., positive vs. non-positive) with the ground-truth label. We reported accuracy, precision, recall, and F1. We also quantified our model confidence based on self-consistency, defined as the proportion of repeated Moderator runs that agreed with the majority prediction. To assess how well this confidence aligns with actual prediction correctness, we calculated the Expected Calibration Error (ECE), which is defined as the average absolute difference between the model's confidence and accuracy across binned confidence intervals. We first evaluated the accuracy via cross-validation using the test set and via retrospective in vitro validation using in-house private data. This in vitro dataset has 11 drug combinations, all of which were entirely unseen in our main dataset, including literature mining and data augmentation. Of these, 9 combinations are labeled as non-positive efficacy. Furthermore, during the evaluation of the in vitro data, we augmented the initial learning examples from the warm-up phase by incorporating combinations that were correctly predicted during the revision phase. After augmenting the learning examples, we had 347 combinations (195 combinations with positive efficacy; 152 showing non-positive efficacy) serving as learning examples for predicting efficacy on our private data. We conducted an ablation study to understand the relative contributions of each component in our model. We iteratively introduced each component and measured the performance differences. Since these components are not statistically independent, 1 , 2 we should consider the performance differences as the components' relative contributions. Baseline Due to the lack of sufficient data, data-driven machine learning models are not appropriate or available. Instead, we developed a rule-based baseline model to predict the efficacy of drug combinations. We utilize complementary exposure patterns, 45 stating that drug combination is therapeutically effective if the targets of the therapeutic agents hit the disease module without overlap ( Methods S1 ). The target genes of the therapeutic agents were collated from multiple sources, including Drug Target Commons, PubChem, and CTDbase, 43 , 46 , 47 whereas AD-related genes were derived from Agora&#8217;s nominated gene list. 48 Within the test set, target gene information was unobtainable for 76 therapeutic agent (34.3%). Additionally, 76 therapeutic agent combinations (48.72%), can not be evaluated using the baseline model due to the absence of necessary target gene information. Selecting drug combination for in vitro experiments We had created augmented non-positive samples. These augmented data that were predicted to be positive (i.e., false positive) may suggest intriguing hypotheses. Although these combinations were unknown thus marked as non-positive, we hypothesize that they may, in fact, be efficacious combinations that have not yet been empirically tested. Therefore, we selected the top most promising drug combinations out of them and tested for its in vitro efficacy. Human experts carefully examined the 25 false positives. We selected drug combinations if the drug combination co-inhibit functionally complementary pathways according to Researcher&#8217;s reasoning. Recent studies in network pharmacology indicate that synergy occurs when drug targets are functionally complementary but do not share direct interactions, reducing the risk of redundant inhibition, such as the combination of Lenalidomide and BET inhibitors (e.g., I-BET-762) in bortezomib-resistant mantle cell lymphoma, 49 Geldanamycin + Tofacitinib in myeloproliferative neoplasm cells, 49 co-inhibition of NOX4-derived ROS and NOS-derived NO in ischemic stroke. 50 In addition, we excluded combinations if they only target symptom relief (e.g., targeting neurotransmitters) as we are more interested in disease-modifying therapies. We excluded drug combinations if they contain natural products (e.g., Herbal medicine like Huperzine A). In-vitro amyloid beta 1&#8211;42 aggregation assay Misfolding, aggregation, and the progressive accumulation of amyloid beta (A&#946;) and Tau proteins in the brain are hallmark events in AD. To evaluate the efficacy of selected drug combinations, we performed an A&#946;42 aggregation assay. A&#946; peptide (A&#946;42) was synthesized at the W. M. Keck Facility at Yale University using solid-phase N-tert-butyloxycarbonyl chemistry and purified via reverse-phase high-performance liquid chromatography (HPLC). The purified A&#946;42 was lyophilized and stored at -80&#176;C until use. To ensure seed-free preparations, the lyophilized A&#946;42 powder was dissolved in a high pH solution (10 mM NaOH) and filtered through a 30-kDa cutoff filter to eliminate residual aggregates. For the aggregation assay, 200 &#956;l of seed-free A&#946;42 at a concentration of 2 &#956;M was prepared in aggregation buffer (0.1 M Tris-Cl pH 7.4, 500 mM NaCl, and 5 &#956;M Thioflavin T). The assay was conducted either with A&#946;42 alone as a control or in the presence of individual therapeutic agents (Gypenoside, Acamprosate, Melatonin, Memantine, Atorvastatin), drug combinations (each at a final concentration of 100 &#956;M), or an amyloid beta-specific antibody (m266, 5 &#956;g/ml). The mixtures were placed in a 96-well plate and incubated at 25 &#176;C for 150 hours, with intermittent shaking at 500 rpm. Aggregation was monitored periodically by measuring Thioflavin T (ThT) fluorescence intensity using a Gemini-XS microplate spectrofluorometer (Molecular Devices, Sunnyvale, CA), with excitation and emission wavelengths set at 435 nm and 485 nm, respectively. Differences in aggregation were quantified by calculating percent aggregation, using the maximum fluorescence (Fmax) of the A&#946;42-alone control as a reference. Quantification and statistical analysis Statistical analysis in our study was primarily descriptive and focused on evaluating prediction performance of large language models. All analyses were performed using Python (v3.11.1). LLM-based outputs were generated using OpenAI GPT-4 and Claude-3-Opus-20240229 APIs. Embeddings were produced using the OpenAI text-embedding-ada-002 API. Evaluation metrics included accuracy, precision, recall, and F1 score for binary classification of drug combination efficacy. Cosine similarity was used for k-nearest neighbor (where k = 5) retrieval of learning examples, which were used in dynamic few-shot prompting. Performance metrics reflected model behavior across test instances, not biological replicates. References 1 Nori H. Lee Y.T. Zhang S. Carignan D. Edgar R. Fusi N. King N. Larson J. Li Y. Liu W. Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine Preprint at arXiv 2023 10.48550/arXiv.2311.16452 2 Romera-Paredes B. Barekatain M. Novikov A. Balog M. Kumar M.P. Dupont E. Ruiz F.J.R. Ellenberg J.S. Wang P. Fawzi O. Mathematical discoveries from program search with large language models Nature 625 2024 468 475 38096900 10.1038/s41586-023-06924-6 PMC10794145 3 Jablonka K.M. Schwaller P. Ortega-Guerrero A. Smit B. Leveraging large language models for predictive chemistry Nat. Mach. Intell. 6 2024 161 169 4 Boiko D.A. MacKnight R. Kline B. Gomes G. Autonomous chemical research with large language models Nature 624 2023 570 578 38123806 10.1038/s41586-023-06792-0 PMC10733136 5 M Bran A. Cox S. Schilter O. Baldassari C. White A.D. Schwaller P.A. Augmenting large language models with chemistry tools Nat. Mach. Intell. 6 2024 525 535 38799228 10.1038/s42256-024-00832-8 PMC11116106 6 Holyoak K.J. Morrison R.G. The Cambridge Handbook of Thinking and Reasoning 2005 Cambridge University Press 7 Kalyanpur A. Saravanakumar K.K. Barres V. Chu-Carroll J. Melville D. Ferrucci D. LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic Preprint at arXiv 2024 10.48550/arXiv.2406.17663 8 Haji F. Bethany M. Tabar M. Chiang J. Rios A. Najafirad P. Improving LLM reasoning with multi-agent Tree-of-Thought Validator agent Preprint at arXiv 2024 10.48550/arXiv.2409.11527 9 Li T. Shetty S. Kamath A. Jaiswal A. Jiang X. Ding Y. Kim Y. CancerGPT for few shot drug pair synergy prediction using large pretrained language models npj Digit. Med. 7 2024 40 38374445 10.1038/s41746-024-01024-9 PMC10876664 10 Liu A. Feng B. Xue B. Wang B. Wu B. Lu C. Zhao C. Deng C. Zhang C. Ruan C. DeepSeek-V3 Technical Report Preprint at arXiv 2024 10.48550/arXiv.2412.19437 11 Qi B. Zhang K. Li H. Tian K. Zeng S. Chen Z. Zhou B. Large language models as biomedical hypothesis generators: A comprehensive evaluation Preprint at arXiv 2024 10.48550/arXiv.2311.05965 12 Qi B. Zhang K. Li H. Tian K. Zeng S. Chen Z. Zhou B. Large Language Models are zero shot hypothesis proposers Preprint at arXiv 2023 10.48550/arXiv.2311.05965 13 Sun X. Vilar S. Tatonetti N.P. High-throughput methods for combinatorial drug discovery Sci. Transl. Med. 5 2013 205rv1 10.1126/scitranslmed.3006667 24089409 14 Celebi R. Bear Don&#8217;t Walk O. 4th Movva R. Alpsoy S. Dumontier M. In-silico Prediction of Synergistic Anti-Cancer Drug Combinations Using Multi-omics Data Sci. Rep. 9 2019 8949 31222109 10.1038/s41598-019-45236-6 PMC6586895 15 Preuer K. Lewis R.P.I. Hochreiter S. Bender A. Bulusu K.C. Klambauer G. DeepSynergy: predicting anti-cancer drug synergy with Deep Learning Bioinformatics 34 2018 1538 1546 29253077 10.1093/bioinformatics/btx806 PMC5925774 16 Huang L. Li F. Sheng J. Xia X. Ma J. Zhan M. Wong S.T.C. DrugComboRanker: drug combination discovery based on target network analysis Bioinformatics 30 2014 i228 i236 24931988 10.1093/bioinformatics/btu278 PMC4058933 17 Bansal M. Yang J. Karan C. Menden M.P. Costello J.C. Tang H. Xiao G. Li Y. Allen J. Zhong R. A community computational challenge to predict the activity of pairs of compounds Nat. Biotechnol. 32 2014 1213 1222 25419740 10.1038/nbt.3052 PMC4399794 18 Zhao X.-M. Iskar M. Zeller G. Kuhn M. van Noort V. Bork P. Prediction of drug combinations by integrating molecular and pharmacological data PLoS Comput. Biol. 7 2011 e1002323 10.1371/journal.pcbi.1002323 PMC3248384 22219721 19 Chen G. Tsoi A. Xu H. Zheng W.J. Predict effective drug combination by deep belief network and ontology fingerprints Preprint at J. Biomed. Inform. 85 2018 149 154 10.1016/j.jbi.2018.07.024 30081101 20 Tang J. Gautam P. Gupta A. He L. Timonen S. Akimov Y. Wang W. Szwajda A. Jaiswal A. Turei D. Network pharmacology modeling identifies synergistic Aurora B and ZAK interaction in triple-negative breast cancer NPJ Syst. Biol. Appl. 5 2019 20 31312514 10.1038/s41540-019-0098-z PMC6614366 21 Lewis P. Perez E. Piktus A. Petroni F. Karpukhin V. Goyal N. K&#252;ttler H. Lewis M. Yih W. Rockt&#228;schel T. Retrieval-augmented generation for knowledge-intensive NLP tasks Preprint at arXiv 2020 10.48550/arXiv.2005.11401 22 Brown T.B. Mann B. Ryder N. Subbiah M. Kaplan J. Dhariwal P. Neelakantan A. Shyam P. Sastry G. Askell A. Language Models are Few-Shot Learners Preprint at arXiv 2020 10.48550/ARXIV.2005.14165 23 Wei J. Wang X. Schuurmans D. Bosma M. Ichter B. Xia F. Chi E. Le Q. Zhou D. Chain-of-thought prompting elicits reasoning in large language models Preprint at arXiv 2022 10.48550/ARXIV.2201.11903 24 Yao S. Yu D. Zhao J. Tree of thoughts: Deliberate problem solving with large language models Preprint at arXiv 2023 10.48550/arXiv.2305.10601 25 Wang X. Self-consistency improves chain of thought reasoning in language models Preprint at arXiv 2022 10.48550/arXiv.2203.11171 26 Yu L. Wang W. Pang W. Xiao Z. Jiang Y. Hong Y. Dietary lycopene supplementation improves cognitive performances in tau transgenic mice expressing P301L mutation via inhibiting oxidative stress and tau hyperphosphorylation J. Alzheimers Dis. 57 2017 475 482 28269786 10.3233/JAD-161216 27 Freyssin A. Carles A. Guehairia S. Rubinstenn G. Maurice T. Fluoroethylnormemantine (FENM) shows synergistic protection in combination with a sigma-1 receptor agonist in a mouse model of Alzheimer&#8217;s disease Neuropharmacology 242 2024 109733 10.1016/j.neuropharm.2023.109733 37844867 28 Pirchl M. Ullrich C. Sperner-Unterweger B. Humpel C. Homocysteine has anti-inflammatory properties in a hypercholesterolemic rat model in vivo Mol. Cell. Neurosci. 49 2012 456 463 22425561 10.1016/j.mcn.2012.03.001 PMC3359503 29 Samad N. Jabeen S. Imran I. Zulfiqar I. Bilal K. Protective effect of gallic acid against arsenic-induced anxiety-/depression- like behaviors and memory impairment in male rats Metab. Brain Dis. 34 2019 1091 1102 31119507 10.1007/s11011-019-00432-1 30 Gavriel Y. Rabinovich-Nikitin I. Ezra A. Barbiro B. Solomon B. Subcutaneous administration of AMD3100 into mice models of Alzheimer&#8217;s disease ameliorated cognitive impairment, reduced neuroinflammation, and improved pathophysiological markers J. Alzheimers. Dis. 78 2020 653 671 33016905 10.3233/JAD-200506 31 Zhao L. Chen T. Wang C. Li G. Zhi W. Yin J. Wan Q. Chen L. Atorvastatin in improvement of cognitive impairments caused by amyloid &#946; in mice: involvement of inflammatory reaction BMC Neurol. 16 2016 18 26846170 10.1186/s12883-016-0533-3 PMC4743318 32 Wang D. Noda Y. Zhou Y. Mouri A. Mizoguchi H. Nitta A. Chen W. Nabeshima T. The allosteric potentiation of nicotinic acetylcholine receptors by galantamine ameliorates the cognitive dysfunction in beta amyloid25-35 i.c.v.-injected mice: involvement of dopaminergic systems Neuropsychopharmacology 32 2007 1261 1271 17133263 10.1038/sj.npp.1301256 33 Carroll J.C. Rosario E.R. Villamagna A. Pike C.J. Continuous and cyclic progesterone differentially interact with estradiol in the regulation of Alzheimer-like pathology in female 3xTransgenic-Alzheimer&#8217;s disease mice Endocrinology 151 2010 2713 2722 20410196 10.1210/en.2009-1487 PMC2875823 34 Devi L. Ohno M. A combination Alzheimer&#8217;s therapy targeting BACE1 and neprilysin in 5XFAD transgenic mice Mol. Brain 8 2015 19 25884928 10.1186/s13041-015-0110-5 PMC4397831 35 Deng Z. Ma W. Han Q.L. Zhou W. Zhu X. Wen S. Xiang Y. Exploring DeepSeek: A survey on advances, applications, challenges and future directions IEEE/CAA J. Autom. Sin. 12 2025 872 893 36 Zhou W. The security of using large language models: A survey with emphasis on ChatGPT IEEE/CAA J. Autom. Sin. 12 2025 1 26 37 Deng Z. AI agents under threat: A survey of key security challenges and future pathways Preprint at arXiv 2024 10.48550/arXiv.2406.02630 38 Chen D. Wang H. Huo Y. Li Y. Zhang H. GameGPT: Multi-agent collaborative framework for game development Preprint at arXiv 2023 10.48550/arXiv.2310.08067 39 Du Y. Li S. Torralba A. Tenenbaum J.B. Mordatch I. Improving factuality and reasoning in language models through multiagent debate Preprint at arXiv 2023 10.48550/arXiv.2305.14325 40 Xu Q. Coated-LLM. Github https://github.com/QidiXu96/Coated-LLM 41 AlzPED. https://alzped.nia.nih.gov/ . 42 Yang C. Large Language Models as Optimizers Preprint at arXiv 2023 10.48550/ARXIV.2309.03409 43 Davis A.P. Wiegers T.C. Johnson R.J. Sciaky D. Wiegers J. Mattingly C.J. Comparative Toxicogenomics Database (CTD): update 2023 Nucleic Acids Res. 51 2023 D1257 D1262 36169237 10.1093/nar/gkac833 PMC9825590 44 Research Models. https://www.alzforum.org/research-models . 45 Cheng F. Kov&#225;cs I.A. Barab&#225;si A.-L. Network-based prediction of drug combinations Nat. Commun. 10 2019 1197 30867426 10.1038/s41467-019-09186-x PMC6416394 46 Tang J. Tanoli Z.U.R. Ravikumar B. Alam Z. Rebane A. V&#228;h&#228;-Koskela M. Peddinti G. van Adrichem A.J. Wakkinen J. Jaiswal A. Drug Target Commons: A community effort to build a consensus knowledge base for drug-target interactions Cell Chem. Biol. 25 2018 224 229.e2 29276046 10.1016/j.chembiol.2017.11.009 PMC5814751 47 PubMed. https://pubmed.ncbi.nlm.nih.gov/ . 48 Agora. https://agora.adknowledgeportal.org/ . 49 Unsal-Beyge S. Tuncbag N. Functional stratification of cancer drugs through integrated network similarity NPJ Syst. Biol. Appl. 8 2022 11 35440787 10.1038/s41540-022-00219-8 PMC9018743 50 Casas A.I. Hassan A.A. Larsen S.J. Gomez-Rangel V. Elbatreek M. Kleikers P.W.M. Guney E. Egea J. L&#243;pez M.G. Baumbach J. Schmidt H.H.H.W. From single drug targets to synergistic network pharmacology in ischemic stroke Proc. Natl. Acad. Sci. USA 116 2019 7129 7136 30894481 10.1073/pnas.1820799116 PMC6452748 Supplemental information Document S1. Figures S1 and S2, Tables S1 and S2, Methods S1, and Data S1 and S2 Supplemental information can be found online at https://doi.org/10.1016/j.isci.2025.113984 ."
}