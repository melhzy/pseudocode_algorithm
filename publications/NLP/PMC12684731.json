{
  "pmcid": "PMC12684731",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:14.359501",
  "metadata": {
    "journal_title": "Briefings in Bioinformatics",
    "journal_nlm_ta": "Brief Bioinform",
    "journal_iso_abbrev": "Brief Bioinform",
    "journal": "Briefings in Bioinformatics",
    "pmcid": "PMC12684731",
    "pmid": "41359543",
    "doi": "10.1093/bib/bbaf642",
    "title": "iDNA-DAPHA: a generic framework for methylation prediction via domain-adaptive pretraining and hierarchical attention",
    "year": "2025",
    "month": "12",
    "day": "08",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "08"
    },
    "authors": [
      "Wang Wenjun",
      "Tan Wen",
      "Lai Lvlong",
      "Lu Youjun",
      "Wu Qingyao"
    ],
    "abstract": "Abstract Accurately identifying DNA methylation is essential for understanding complex regulatory networks and disease mechanisms. However, the dynamic nature of methylation and species differences make prediction challenging. Existing deep learning methods often overlook the potential of shared features across diverse species’ methylation sequences and rely solely on token-to-token attention when modelling long-range dependencies, limiting the model’s representation capabilities. To address these limitations, we propose iDNA-DAPHA, an accurate and generic two-stage deep learning framework that leverages domain-adaptive pretraining (DAP) incorporating feature alignment to learn common features across various types of methylation sequences from multiple species, followed by fine-tuning to capture task-specific features. The framework further introduces hierarchical attention (HA) to enhance its representational power. Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods across seventeen benchmark datasets covering three representative DNA methylation types. Ablation studies validate the effectiveness and contributions of DAP and HA. Furthermore, visualization-based analyses reveal that the model can capture conserved sequence patterns and learn discriminative representations. We believe that iDNA-DAPHA will serve as a valuable framework for methylation prediction, especially in scenarios with limited training samples for specific methylation types in certain species.",
    "keywords": [
      "DNA methylation",
      "deep learning",
      "domain-adaptive pretraining",
      "hierarchical attention"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Brief Bioinform</journal-id><journal-id journal-id-type=\"iso-abbrev\">Brief Bioinform</journal-id><journal-id journal-id-type=\"pmc-domain-id\">721</journal-id><journal-id journal-id-type=\"pmc-domain\">bib</journal-id><journal-id journal-id-type=\"publisher-id\">bib</journal-id><journal-title-group><journal-title>Briefings in Bioinformatics</journal-title></journal-title-group><issn pub-type=\"ppub\">1467-5463</issn><issn pub-type=\"epub\">1477-4054</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12684731</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12684731.1</article-id><article-id pub-id-type=\"pmcaid\">12684731</article-id><article-id pub-id-type=\"pmcaiid\">12684731</article-id><article-id pub-id-type=\"pmid\">41359543</article-id><article-id pub-id-type=\"doi\">10.1093/bib/bbaf642</article-id><article-id pub-id-type=\"publisher-id\">bbaf642</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Problem Solving Protocol</subject></subj-group><subj-group subj-group-type=\"category-taxonomy-collection\"><subject>AcademicSubjects/SCI01060</subject></subj-group></article-categories><title-group><article-title>iDNA-DAPHA: a generic framework for methylation prediction via domain-adaptive pretraining and hierarchical attention</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"W\">Wenjun</given-names></name><aff>\n<institution>School of Software Engineering</institution>, South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, <country country=\"CN\">China</country></aff><aff>\n<institution>School of Data Science and Information Engineering</institution>, Guizhou Minzu University, Huaxi University Town, Huaxi District, Guiyang, Guizhou 550025, <country country=\"CN\">China</country></aff></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Tan</surname><given-names initials=\"W\">Wen</given-names></name><aff>\n<institution>School of Software Engineering</institution>, South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, <country country=\"CN\">China</country></aff></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Lai</surname><given-names initials=\"L\">Lvlong</given-names></name><aff>\n<institution>School of Computer Science</institution>, Guangdong Polytechnic Normal University, No. 293, West Zhongshan Avenue, Tianhe District, Guangzhou, Guangdong 510665, <country country=\"CN\">China</country></aff></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names initials=\"Y\">Youjun</given-names></name><aff>\n<institution>School of Data Science and Information Engineering</institution>, Guizhou Minzu University, Huaxi University Town, Huaxi District, Guiyang, Guizhou 550025, <country country=\"CN\">China</country></aff></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"Q\">Qingyao</given-names></name><aff>\n<institution>School of Software Engineering</institution>, South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, <country country=\"CN\">China</country></aff><aff>\n<institution>Peng Cheng Laboratory</institution>, No. 6001 Shahexi Road, Nanshan District, Shenzhen, Guangdong 518055, <country country=\"CN\">China</country></aff><xref rid=\"cor1\" ref-type=\"corresp\"/></contrib></contrib-group><author-notes><corresp id=\"cor1\">Corresponding author. E-mail: <email>qyw@scut.edu.cn</email></corresp></author-notes><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"epub\" iso-8601-date=\"2025-12-08\"><day>08</day><month>12</month><year>2025</year></pub-date><volume>26</volume><issue>6</issue><issue-id pub-id-type=\"pmc-issue-id\">499914</issue-id><elocation-id>bbaf642</elocation-id><history><date date-type=\"received\"><day>01</day><month>6</month><year>2025</year></date><date date-type=\"rev-recd\"><day>13</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>05</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>08</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 14:25:12.950\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbynclicense\">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc/4.0/\">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"bbaf642.pdf\"/><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"bbaf642.pdf\"/><abstract><title>Abstract</title><p>Accurately identifying DNA methylation is essential for understanding complex regulatory networks and disease mechanisms. However, the dynamic nature of methylation and species differences make prediction challenging. Existing deep learning methods often overlook the potential of shared features across diverse species&#8217; methylation sequences and rely solely on token-to-token attention when modelling long-range dependencies, limiting the model&#8217;s representation capabilities. To address these limitations, we propose iDNA-DAPHA, an accurate and generic two-stage deep learning framework that leverages domain-adaptive pretraining (DAP) incorporating feature alignment to learn common features across various types of methylation sequences from multiple species, followed by fine-tuning to capture task-specific features. The framework further introduces hierarchical attention (HA) to enhance its representational power. Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods across seventeen benchmark datasets covering three representative DNA methylation types. Ablation studies validate the effectiveness and contributions of DAP and HA. Furthermore, visualization-based analyses reveal that the model can capture conserved sequence patterns and learn discriminative representations. We believe that iDNA-DAPHA will serve as a valuable framework for methylation prediction, especially in scenarios with limited training samples for specific methylation types in certain species.</p></abstract><kwd-group><kwd>DNA methylation</kwd><kwd>deep learning</kwd><kwd>domain-adaptive pretraining</kwd><kwd>hierarchical attention</kwd></kwd-group><funding-group><award-group award-type=\"grant\"><funding-source><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type=\"DOI\">10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id>62272172</award-id><award-id>62266012</award-id></award-group><award-group award-type=\"grant\"><funding-source><institution-wrap><institution>Zhuhai Science and Technology Plan Project</institution></institution-wrap></funding-source><award-id>2320004002758</award-id></award-group><award-group award-type=\"grant\"><funding-source><institution-wrap><institution>Fundamental Research Funds for the Central Universities</institution><institution-id institution-id-type=\"DOI\">10.13039/501100012226</institution-id></institution-wrap></funding-source><award-id>2025ZYGXZR095</award-id></award-group><award-group award-type=\"grant\"><funding-source><institution-wrap><institution>High-Level Innovative Talent Project of Guizhou Province</institution></institution-wrap></funding-source><award-id>QKHPTRC-GCC2023027</award-id></award-group></funding-group><counts><page-count count=\"13\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"sec1\"><title>Introduction</title><p>DNA methylation, an epigenetic mechanism influencing gene expression, plays an important role in various medical disorders [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>, <xref rid=\"ref2\" ref-type=\"bibr\">2</xref>]. Aberrant changes in DNA methylation are a significant factor in the onset and progression of diseases, including cancer [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>]. Currently, three representative DNA methylation types&#8212;N4-Methylcytosine (4mC), 5-Hydroxymethylcytosine (5hmC), and N6-Methyladenine (6mA)&#8212;have been identified across species [<xref rid=\"ref4\" ref-type=\"bibr\">4&#8211;6</xref>]. These types are associated with different functional mechanisms. 4mC, involved in the restriction&#8211;modification system, corrects DNA replication errors and controls DNA replication and cell cycle [<xref rid=\"ref7\" ref-type=\"bibr\">7</xref>]. 5hmC, linked to neural system development and tumorigenesis, aids in identifying novel therapeutic targets and strategies for cancer treatment [<xref rid=\"ref8\" ref-type=\"bibr\">8</xref>]. 6mA, a epigenetic mark in prokaryotes and eukaryotes [<xref rid=\"ref9\" ref-type=\"bibr\">9</xref>], regulates various cellular processes, including gene transcription, chromosome replication, and cell defense [<xref rid=\"ref10\" ref-type=\"bibr\">10</xref>]. Therefore, identifying the above DNA methylations is crucial for analysing and revealing their functional mechanisms.</p><p>As environmental degradation increasingly affects gene regulation and expression across ecosystems, identifying DNA methylation has also become an important and pressing task for understanding biological development and preventing diseases [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>]. Environmental factors, ageing, and other influences cause dynamic changes in DNA methylation, posing challenges for accurate prediction [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]. Traditional wet-lab techniques, such as reduced-representation bisulfite sequencing [<xref rid=\"ref13\" ref-type=\"bibr\">13</xref>] and whole-genome bisulfite sequencing [<xref rid=\"ref14\" ref-type=\"bibr\">14</xref>], are costly and time-consuming. Besides this, bisulfite sequencing using short-read techniques has drawbacks such as low positional efficiency and uneven genome coverage, leading to suboptimal sequencing quality. Thus, computational prediction of DNA methylation serves as a good choice. This approach not only minimizes costs but also furnishes valuable guidance for experimental studies on epigenetic modifications [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>].</p><p>In the past few years, machine learning and deep learning approaches have proven effective in predicting DNA methylation [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>]. Methods like DNA4mC-LIP [<xref rid=\"ref15\" ref-type=\"bibr\">15</xref>] and DeepTorrent [<xref rid=\"ref16\" ref-type=\"bibr\">16</xref>] target 4mC methylation, while Deep5hmC [<xref rid=\"ref17\" ref-type=\"bibr\">17</xref>] and iRhm5CNN [<xref rid=\"ref18\" ref-type=\"bibr\">18</xref>] focus on 5hmC methylation, and i6mA-Pred [<xref rid=\"ref19\" ref-type=\"bibr\">19</xref>], SNNRice6mA [<xref rid=\"ref20\" ref-type=\"bibr\">20</xref>], and SDM6A [<xref rid=\"ref21\" ref-type=\"bibr\">21</xref>] address 6mA methylation. However, these models are often specific to particular modifications or species, limiting generalization. Hence, there is an urgent need for a universal approach supporting prediction of 4mC, 5hmC, and 6mA methylation across species. iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>] is the first machine learning generic predictor but relies on manually designed features, requiring extensive prior knowledge, and resulting in limited adaptability across different methylation prediction tasks. To tackle this issue, deep learning-based generic predictors like iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>], iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>], MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>], and StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>] have been proposed. They utilize BERT [<xref rid=\"ref22\" ref-type=\"bibr\">22</xref>] and its variants to learn distinguishable features for identifying methylation. Each of these methods follows the pretraining&#8211;fine-tuning paradigm, as depicted in <xref rid=\"f1\" ref-type=\"fig\">Fig. 1A</xref>, adapting to the specific task and achieving improved performance.</p><fig position=\"float\" id=\"f1\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Comparison of (A) previous deep learning-based methods for methylation prediction and (B) our proposed approach.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"bbaf642f1.jpg\"><alt-text>Alt text: A two-part flowchart comparing previous deep learning-based methylation prediction methods (a) with our proposed method (b). Panel (a) shows DNABERT general-purpose pretraining followed by task-specific fine-tuning, using only token-to-token attention. Panel (b) introduces a methylation DAP stage using multiple methylation datasets after general-purpose pretraining. This new stage enhances the model with HA and feature alignment before proceeding to task-specific fine-tuning.</alt-text></graphic></fig><p>Since these generic predictors, built upon BERT and its variants, undergo general-purpose pretraining on large amounts of unlabelled sequences, they are capable of obtaining comprehensive contextual representations, thereby improving overall model performance. However, these approaches often overlook the potential of shared characteristics across diverse species&#8217; methylation data. It is important to understand common features and evolutionary trends in DNA methylation. Models can benefit from these shared features, improving their ability to generalize DNA methylation patterns across different species. With this in mind, our objective is to guide the model in learning general features across various DNA methylation tasks, enabling it to implicitly capture cross-species commonalities and provide informative feature representations that facilitate downstream fine-tuning for accurate methylation identification. We adopt a domain-adaptive pretraining (DAP) approach that leverages data from multiple methylation tasks, shares a unified feature extractor, and incorporates a methylation task classification loss. To encourage the model to capture common features across tasks, we perform feature alignment using a gradient reversal layer during the backpropagation phase. The layer keeps the input unchanged during forward propagation but reverses the gradient by multiplying it with a negative scalar during backpropagation. This adversarial process forces the model&#8217;s feature extraction module to maximize the task classification loss, making the feature distributions of different tasks indistinguishable and facilitating the learning of shared features relevant to methylation.</p><p>Based on this, we propose a generic two-stage methylation prediction framework termed iDNA-DAPHA, as illustrated in <xref rid=\"f1\" ref-type=\"fig\">Fig. 1B</xref>. We first initialize the model with general-purpose pretraining weights. Then, in the first stage, we employ DAP incorporating feature alignment to learn common features among various types of methylation sequences from multiple species. In the second stage, we conduct fine-tuning to capture discriminative features within the specific methylation prediction task (designated methylation types in specified species). In addition, existing sequence-based generic predictors only consider token-to-token attention at a single granularity when modelling long-range dependencies. They overlook group-to-group attention, making it difficult to understand broader inter-group correlations and obtain higher-level feature representation power. Therefore, to enhance representational capabilities, our iDNA-DAPHA network introduces a hierarchical attention (HA)mechanism that combines both token-to-token and group-to-group attention, enabling the model to capture both fine-grained inter-token and broader inter-group correlations.</p><p>Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods across 17 benchmark datasets, with particularly significant improvements in scenarios involving limited training samples for specific methylation types in certain species. We further conduct ablation studies to validate the effectiveness and contributions of DAP and HA. Moreover, visualization-based interpretability analyses reveal that the model can capture conserved sequence patterns and learn more discriminative representations for methylation prediction.</p></sec><sec id=\"sec2\"><title>Related work</title><sec id=\"sec2a\"><title>Domain-adaptive pretraining</title><p>DAP has emerged as a crucial technique to enhance the performance of pretrained models in specialized domains. This approach involves fine-tuning pretrained models on domain data to adapt them to the target domain&#8217;s tasks. Recent studies have emphasized the importance of adapting pretrained models to diverse domains to unlock their full potential across various applications. Lee <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref23\" ref-type=\"bibr\">23</xref>] proposed BioBERT, the first domain-specific BERT-based model pretrained on biomedical corpora. Gururangan <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref24\" ref-type=\"bibr\">24</xref>] explored the benefits of continued pretraining language models and found that DAP leads to large gains in task performance. Tai <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref25\" ref-type=\"bibr\">25</xref>] pretrained exBERT with biomedical articles and domain-specific vocabulary to extend the pretrained BERT from a generic domain to a specific domain. Van der Putten <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref26\" ref-type=\"bibr\">26</xref>] used several Barrett-specific datasets closer to the target domain in a multi-stage transfer learning strategy for improved detection and localization of Barrett&#8217;s neoplasia. Koto <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref27\" ref-type=\"bibr\">27</xref>] present INDOBERTWEET, the first large-scale pretrained model for Indonesian Twitter, domain-adaptively trained through the expansion of a monolingually trained Indonesian BERT with additional domain-specific vocabulary. Ji <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref28\" ref-type=\"bibr\">28</xref>] developed two pretrained language models, i.e. MentalBERT and MentalRoBERTa, and demonstrated that language representations pretrained in the specific downstream domain enhance the performance of mental health detection tasks. Zhang <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref29\" ref-type=\"bibr\">29</xref>] studied domain-specific pretraining on biomedical image-text data and proposed BiomedCLIP for biomedical vision-language processing. Ji <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref30\" ref-type=\"bibr\">30</xref>] conducted domain-specific continued pretraining and released MentalXLNet and MentalLongformer for effectively modelling long sequences in the field of mental health. Xu <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref31\" ref-type=\"bibr\">31</xref>] introduce multi-stage domain pretraining for the Textbook Question Answering task, conducting unsupervised post-pretraining with the span mask strategy and supervised pre-fine-tuning to overcome the domain gaps of general language model. Zhai <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref32\" ref-type=\"bibr\">32</xref>] develop a psychology-adapted language model through DAP on social media texts with psychological lexicon integration, enhancing performance in psychological text analysis. Zeng <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref33\" ref-type=\"bibr\">33</xref>] propose ESM-DBP, a domain-adaptive protein language model trained on 170 264 DNA-binding protein sequences, which improves feature representation and prediction performance for DNA&#8211;protein interactions. Although the above works on DAP have been successful, there has been no work exploring adaptive pretraining in the methylation domain. Therefore, this paper proposes a DAP method using feature alignment to improve the performance of methylation prediction tasks.</p></sec><sec id=\"sec2b\"><title>Computational approaches for DNA methylation prediction</title><p>In recent years, machine learning and deep learning have made significant strides in predicting DNA methylation. For 4mC methylation identification, methods such as the ensemble learning-based DNA4mC-LIP [<xref rid=\"ref15\" ref-type=\"bibr\">15</xref>] and the deep learning-based DeepTorrent [<xref rid=\"ref16\" ref-type=\"bibr\">16</xref>] have been proposed. For 5hmC methylation detection, models such as Deep5hmC [<xref rid=\"ref17\" ref-type=\"bibr\">17</xref>] adopt a multimodal design, while iRhm5CNN [<xref rid=\"ref18\" ref-type=\"bibr\">18</xref>] utilizes a convolutional neural network. For 6mA methylation prediction, various methods have been developed, including i6mA-Pred [<xref rid=\"ref19\" ref-type=\"bibr\">19</xref>], which employs a support vector machine; SNNRice6mA [<xref rid=\"ref20\" ref-type=\"bibr\">20</xref>], which builds a lightweight CNN; and SDM6A [<xref rid=\"ref21\" ref-type=\"bibr\">21</xref>], which explores different feature encodings and integrates multiple single models. These predictors have propelled the identification of DNA methylation in various contexts, but they focus on a specific type of modification, with some even limited to one specific species. They are difficult to generalize to other methylation types and species. Therefore, there is an urgent need to develop a universal method that supports the prediction of three representative types of methylation, 4mC, 5hmC, and 6mA, applicable to different species.</p><p>iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>] is the first machine learning generic predictor for detecting various methylations of different species. However, this method uses manual features, limiting adaptability. To address this, several deep learning-based generic predictors of different methylations across different species have been proposed. iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>] employs BERT to adaptively learn distinguishable features for identifying DNA methylation in multiple species. iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>] uses the DNABERT model pretrained with large-scale genomic sequences and introduces a dual-scale processing strategy to effectively capture discernible methylation information across various scales. MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>] integrates five kinds of widely used BERT models to collectively predict the DNA methylation status. StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>] utilizes the pretraining model DNABERT and develops a feature correction module to enhance the model&#8217;s stability. Each of these models is adapted to the specific task through the pretraining&#8211;fine-tuning paradigm. These deep learning methods based on BERT and its variants achieved better performance over previous approaches. However, these predictors tend to overlook the potential of shared characteristics relevant to methylation prediction across species. Meanwhile, the field is evolving toward increasingly diverse methodological directions targeting distinct biological objectives. For instance, iResNetDM [<xref rid=\"ref34\" ref-type=\"bibr\">34</xref>], which integrates Residual Networks (ResNet) with self-attention mechanisms, distinguishes among four types of DNA methylation modifications and captures relationships between different types of modifications. MethylProphet [<xref rid=\"ref35\" ref-type=\"bibr\">35</xref>], on the other hand, developed a novel encoding scheme that integrates gene expression profiles with DNA sequence context to infer the whole-genome DNA methylation landscape. Methodologically, techniques from related bioinformatics fields also offer valuable insights, such as the joint masking and self-supervised strategies employed by JMSS-MMA [<xref rid=\"ref36\" ref-type=\"bibr\">36</xref>] to reduce data noise for inferring small molecule-miRNA associations. Returning to the challenge of generic methylation prediction across different species and types, there remains a need for a systematic framework that captures the shared representations of DNA methylation to enhance generalization across diverse species. Moreover, these BERT-based universal predictors only use single-granularity token-to-token attention, neglecting group-to-group attention. This limits their ability to understand broader inter-group correlations and obtain higher-level feature representation capabilities. Based on the above considerations, this paper proposes a generic framework, iDNA-DAPHA, for methylation prediction. The framework leverages DAP to learn common features relevant to methylation and incorporates HA, combining token-to-token and group-to-group attention, to further enhance representation capabilities.</p></sec></sec><sec id=\"sec3\"><title>Materials and methods</title><sec id=\"sec3a\"><title>Benchmark dataset collection</title><p>In this study, we utilized the same benchmark dataset collection as employed by previous generic predictors, such as iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>], MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>], and StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>], etc., facilitating fair comparisons. This benchmark collection provides a comprehensive set of DNA methylation data across various species, encompassing three representative DNA methylation types: 4mC, 5hmC, and 6mA, and covering a total of 17 distinct datasets. Among these datasets, the 4 4mC datasets encompass sequences from C.equisetifolia, F.vesca, S.cerevisiae, and Ts.SUP5-1. The 2 5hmC datasets include sequences from H.sapiens and M.musculus. The 11 6mA datasets comprise sequences from A.thaliana, C.elegans, C.equisetifolia, D.melanogaster, F.vesca, H.sapiens, R.chinensis, S.cerevisiae, Ts.SUP5-1, T.thermophile and Xoc.BLS256. To ensure dataset quality, stringent selection criteria are used. Positive samples were selected based on experimental verification, with sequences centred on methylation sites and stringent quality thresholds applied. Negative samples were carefully curated to include sequences not proven to be methylated by experiments, balanced with positive samples to mitigate the impact of skewed class distributions on model learning. Moreover, to avoid redundancy and homology bias, sequences exhibiting &gt;80% sequence similarity were excluded using the CD-HIT program. Each dataset in the benchmark collection is randomly divided into training and testing sets by a ratio of 1:1, ensuring independence between the two subsets. The statistics of training and testing sets for all 17 datasets are listed in <xref rid=\"sup1\" ref-type=\"supplementary-material\">Supplementary Table S1</xref>.</p></sec><sec id=\"sec3b\"><title>The framework of the proposed iDNA-DAPHA</title><p>Our method employs a two-stage framework for methylation prediction. In the first stage, we implement DAP to capture common features among different types of methylation sequences from multiple species. The workflow is illustrated in <xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref>. We use the dual-stream network as the shared feature extraction module (B), processing input sequences from 17 distinct methylation datasets (A). Each sequence is tokenized using both 3-mer and 6-mer encoding and is fed separately into the subnetworks (B-comp) of the upstream and downstream branches, following the dual-scale sequence processing strategy [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]. The features extracted from both branches are then concatenated. The fused results are passed to a feature alignment module (C) and a methylation prediction module (D). The feature alignment module, comprising a gradient reversal layer and fully connected layers, aligns features across diverse methylation prediction tasks by reversing the gradient direction under the task classifier&#8217;s discrimination during training. In parallel, the methylation prediction module, composed of fully connected layers, predicts the methylation status of each input sequence. Additionally, the two subnetworks within the dual-stream network incorporate an HA mechanism to capture both fine-grained inter-token and broader inter-group correlations, enhancing feature representation.</p><fig position=\"float\" id=\"f2\" orientation=\"portrait\"><label>Figure 2</label><caption><p>DAP for methylation prediction, comprising four parts: (A) DAP data, (B) dual-stream feature extraction with HA (B-comp), (C) feature alignment via gradient reversal under a task classifier, and (D) methylation prediction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"bbaf642f2.jpg\"><alt-text>Alt text: A diagram illustrating the architecture for DAP in methylation prediction, consisting of four parts. Part A (DAP data): shows input DNA sequences from multiple species and methylation types. Part B (feature extraction): a dual-stream network processes 3-mer and 6-mer tokens separately using B-comp modules with HA. The features from both streams are then combined. Part C (feature alignment): the combined features are aligned across tasks using a gradient reversal layer and a task classifier to extract shared features. Part D (methylation prediction): a methylation classifier predicts methylation states from the learnt representations. Insets show the B-comp mechanism (token-to-token and group-to-group attention), the gradient reversal, and the classifier structure.</alt-text></graphic></fig><p>In the second stage, we perform task-specific fine-tuning to extract and refine discriminative features tailored to the target methylation prediction task (i.e. a designated methylation type in a specified species). The workflow in this stage remains similar to the first stage, with the primary difference being the exclusion of the feature alignment module (C). Furthermore, fine-tuning utilizes only the target task dataset, unlike the DAP stage, which leverages multiple distinct methylation datasets.</p></sec><sec id=\"sec3c\"><title>Methylation domain-adaptive pretraining</title><p>Due to the significant performance gains observed in many NLP tasks through the pretraining&#8211;fine-tuning paradigm, various methods for methylation prediction tasks have also adopted this strategy [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>, <xref rid=\"ref3\" ref-type=\"bibr\">3</xref>, <xref rid=\"ref11\" ref-type=\"bibr\">11</xref>, <xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]. They utilize BERT pretrained on the general-domain corpus or DNABERT pretrained on full human genome sequences, which are then directly fine-tuned for target methylation prediction task. However, these approaches overlook the potential of shared features across diverse species&#8217; methylation sequences, which is crucial for effective representation learning in methylation sequence classification. Moreover, this generic pretraining strategy typically requires a large amount of target task data for fine-tuning to achieve satisfactory performance. For that downstream task&#8217;s methylation dataset with few samples, such as 4mC_C.equisetifolia and 6mA_R.chinensis, fine-tuning alone often yields suboptimal results. Therefore, in this paper, we introduce a DAP method to address these limitations.</p><sec id=\"sec3c1\"><title>Domain-adaptive pretraining data</title><p>Since DAP typically requires large amounts of domain data, we merged the training sets from 17 different methylation datasets in the benchmark collection, as shown in <xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref>A, each of which corresponds to a methylation prediction task involving a specific species and methylation type. These data cover three representative types of DNA methylation sites (4mC, 5hmC, and 6mA) across 12 genomes, including both eukaryotes and prokaryotes. In total, there are 125 488 methylated sequences and an equal number of unmethylated sequences, with sequences from the same dataset assigned the same task label during pretraining. These DAP data contain extensive sequence information regarding various types of methylation sites while also considering the diversity of genomic sources. It facilitates the pretraining of language models tailored to methylation prediction tasks, enhancing their performance and generalization in this domain.</p></sec><sec id=\"sec3c2\"><title>Feature extraction module</title><p>Since the genome sequence is regarded as the language for transmitting genetic information within and between cells, many bioinformatic researchers have utilized word embeddings to represent biological sequences. They treat DNA sequences as &#8216;sentences&#8217;, using the k-mers technique to consider consecutive k bases as &#8216;words&#8217;. In this paper, we build on the insights from Jin <italic toggle=\"yes\">et&#160;al.</italic> [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>], using 3-mers and 6-mers to represent DNA sequences and employing a dual-stream architecture shared across all datasets to process these two scales of tokenized sequences for feature extraction, as shown in <xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref>B. Specifically, for a sequence <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation1\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$S$\\end{document}</tex-math></inline-formula> of length <italic toggle=\"yes\">n</italic>, its k-mers tokenized representation is denoted as </p><disp-formula id=\"deqn01\"><label>(1)</label><tex-math notation=\"LaTeX\" id=\"DmEquation1\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*}&amp; S_{k-mers} = [s_{1}, s_{2},..., s_{n-k+1}],\\end{align*}\\end{document}</tex-math></disp-formula><p>where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation2\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$s_{i}$\\end{document}</tex-math></inline-formula> represents consecutive k bases starting from the <italic toggle=\"yes\">i</italic>th base. We feed the 3-mers represented sequence into the upstream subnetwork and the 6-mers represented sequence into the downstream subnetwork. The features extracted from both subnetworks are then concatenated for fusion. This process of feature extraction can be represented as </p><disp-formula id=\"deqn02\"><label>(2)</label><tex-math notation=\"LaTeX\" id=\"DmEquation2\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*}&amp; F(S) = [B_{up}(S_{3-mers}), B_{down}(S_{6-mers})],\\end{align*}\\end{document}</tex-math></disp-formula><p>where [<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation3\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\cdot $\\end{document}</tex-math></inline-formula>,<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation4\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\cdot $\\end{document}</tex-math></inline-formula>] denotes the concatenation operation, both <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation5\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$B_{up}$\\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation6\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$B_{down}$\\end{document}</tex-math></inline-formula> are DNABERT models incorporating HA, initialized with pretrained DNABERT on full human genome data tokenized with 3-mers and 6-mers, respectively.</p></sec><sec id=\"sec3c3\"><title>Feature alignment module</title><p>In various methylation prediction tasks, there often exist latent general features that can facilitate accurate methylation identification. To extract such shared features, we perform feature alignment (<xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref>C) to make the feature distributions across different tasks as similar as possible. Specifically, we leverage the loss of a methylation task classifier to indirectly estimate the distributional discrepancies among tasks, under the assumption that this classifier can be trained to optimally distinguish between features from different tasks. By maximizing the loss of this classifier, we encourage the underlying feature representations to become indistinguishable across tasks, thus promoting the learning of common features.</p><p>During training, we assume that the input sequence <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation7\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$x$\\end{document}</tex-math></inline-formula> is first mapped to a feature vector <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation8\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathbf{f}$\\end{document}</tex-math></inline-formula> by the feature extraction module <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation9\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$M_{f}$\\end{document}</tex-math></inline-formula>. We represent the parameters of this module as <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation10\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\theta _{f}$\\end{document}</tex-math></inline-formula>, such that <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation11\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathbf{f}=M_{f}(x;\\theta _{f})$\\end{document}</tex-math></inline-formula>. The resulting feature vector <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation12\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathbf{f}$\\end{document}</tex-math></inline-formula> is then passed to the methylation task classifier <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation13\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$M_{t}$\\end{document}</tex-math></inline-formula> to predict the task label <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation14\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$t$\\end{document}</tex-math></inline-formula>, with the classifier&#8217;s parameters denoted as <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation15\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\theta _{t}$\\end{document}</tex-math></inline-formula>, and the corresponding task classification loss is denoted as <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation16\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathcal{L}_{fa}$\\end{document}</tex-math></inline-formula>. General features across different methylation tasks are learnt by updating these parameters as follows: </p><disp-formula id=\"deqn03\"><label>(3)</label><tex-math notation=\"LaTeX\" id=\"DmEquation3\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; \\theta_{f}\\quad\\longleftarrow\\quad\\theta_{f}-\\mu\\left(-\\lambda\\frac{\\partial \\mathcal{L}_{fa}^{i}}{\\partial\\theta_{f}}\\right), \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn04\"><label>(4)</label><tex-math notation=\"LaTeX\" id=\"DmEquation4\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; \\theta_{t}\\quad\\longleftarrow\\quad\\theta_{t}-\\mu\\frac{\\partial \\mathcal{L}_{fa}^{i}}{\\partial\\theta_{t}}, \\end{align*}\\end{document}</tex-math></disp-formula><p>\nwhere <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation17\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mu $\\end{document}</tex-math></inline-formula> is the learning rate, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation18\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\lambda $\\end{document}</tex-math></inline-formula> is a scaling factor that determines the relative influence of the task classification loss on the feature extractor. Updates (<xref rid=\"deqn03\" ref-type=\"disp-formula\">3</xref>) and (<xref rid=\"deqn04\" ref-type=\"disp-formula\">4</xref>) closely resemble stochastic gradient descent updates, with the key distinction being the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation19\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$-\\lambda $\\end{document}</tex-math></inline-formula> factor in (<xref rid=\"deqn03\" ref-type=\"disp-formula\">3</xref>).</p><p>Specifically, the negative factor <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation20\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$-\\lambda $\\end{document}</tex-math></inline-formula> reverses the gradient direction during backpropagation, thereby introducing an adversarial interaction between the feature extractor and the task classifier. While the task classifier minimizes its loss to better distinguish features from different tasks, the feature extractor&#8212;receiving the reversed gradient multiplied by <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation21\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$-\\lambda $\\end{document}</tex-math></inline-formula>&#8212;updates its parameters in the opposite direction, effectively maximizing the task classification loss. This adversarial process drives the feature extractor to learn shared representations that generalize better across methylation tasks. Without this reversal, the extractor would instead cooperate with the task classifier and capture task-specific features, reducing the model&#8217;s generalization ability.</p><p>Thus, we introduce a gradient reversal layer before the methylation task classifier, allowing the feature extraction module to learn common features among different methylation tasks. The gradient reversal layer contains no parameters, except for the meta-parameter <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation22\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\lambda $\\end{document}</tex-math></inline-formula>, which is not updated via backpropagation [<xref rid=\"ref37\" ref-type=\"bibr\">37</xref>]. During forward propagation, this layer acts as an identity transformation, passing the features through unchanged. During backward propagation, it takes gradients from subsequent layers, multiplies them by <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation23\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$-\\lambda $\\end{document}</tex-math></inline-formula>, and passes them to the previous layer. This effectively reverses the optimization direction of the task classifier&#8217;s loss with respect to the feature extraction module. Formally, its forward and backward propagation behaviours are defined by the following two equations: </p><disp-formula id=\"deqn05\"><label>(5)</label><tex-math notation=\"LaTeX\" id=\"DmEquation5\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; G_\\lambda(\\mathbf{x})=\\mathbf{x}, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn06\"><label>(6)</label><tex-math notation=\"LaTeX\" id=\"DmEquation6\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; \\frac{dG_\\lambda}{d\\mathbf{x}}=-\\lambda\\mathbf{E}, \\end{align*}\\end{document}</tex-math></disp-formula><p>where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation24\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathbf{E}$\\end{document}</tex-math></inline-formula> is an identity matrix. Through the gradient reversal layer, we can optimize the following loss function for methylation task classification while encouraging the feature extraction module to learn shared features across tasks: </p><disp-formula id=\"deqn07\"><label>(7)</label><tex-math notation=\"LaTeX\" id=\"DmEquation7\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*}&amp; \\mathcal{L}_{fa} =-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C}t_{i,c}\\ln (\\hat{t}_{i,c}).\\end{align*}\\end{document}</tex-math></disp-formula><p>Here, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation25\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$t_{i,c}$\\end{document}</tex-math></inline-formula> denotes the ground-truth task label of sample <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation26\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$i$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation27\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\hat{t}_{i,c}$\\end{document}</tex-math></inline-formula> represents the predicted probability that sample <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation28\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$i$\\end{document}</tex-math></inline-formula> belongs to task <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation29\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$c$\\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation30\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$N$\\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation31\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$C$\\end{document}</tex-math></inline-formula> denote the batch size and the number of tasks, respectively. Equation (<xref rid=\"deqn07\" ref-type=\"disp-formula\">7</xref>) is thus the cross-entropy loss for task discrimination. When combined with the gradient reversal layer, the feature extractor is adversarially trained to confuse the task classifier, thereby learning shared representations across different methylation tasks.</p></sec><sec id=\"sec3c4\"><title>Methylation prediction module</title><p>As shown in <xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref>D, we employ a linear layer followed by a methylation classifier to guide the model adapt for distinguishing whether sequences undergo methylation. To achieve this, we utilize the following cross-entropy loss function: </p><disp-formula id=\"deqn08\"><label>(8)</label><tex-math notation=\"LaTeX\" id=\"DmEquation8\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*}&amp; \\mathcal{L}_{mp}=-\\frac{1}{N}\\sum_{i=1}^{N}y_{i}\\ln \\hat{y}_{i}.\\end{align*}\\end{document}</tex-math></disp-formula><p>Where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation32\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$y_{i}$\\end{document}</tex-math></inline-formula> represents the one-hot label vector, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation33\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\hat{y}_{i}$\\end{document}</tex-math></inline-formula> denotes the predicted probability distribution for sample <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation34\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$i$\\end{document}</tex-math></inline-formula>. By minimizing this loss function, the model learns better feature representations, allowing it to map the inputs to the correct output labels effectively. Finally, the entire domain pretraining framework is optimized by jointly minimizing the task classification loss <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation35\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathcal{L}_{fa}$\\end{document}</tex-math></inline-formula> and the methylation prediction loss <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation36\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathcal{L}_{mp}$\\end{document}</tex-math></inline-formula> in an end-to-end manner: </p><disp-formula id=\"deqn09\"><label>(9)</label><tex-math notation=\"LaTeX\" id=\"DmEquation9\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*}&amp; \\mathcal{L}_{total}=\\mathcal{L}_{fa}+\\mathcal{L}_{mp}.\\end{align*}\\end{document}</tex-math></disp-formula></sec></sec><sec id=\"sec3h\"><title>Hierarchical attention</title><p>DNABERT utilizes the original Query-Key-Value (<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation37\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation38\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation39\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula>) computation [<xref rid=\"ref38\" ref-type=\"bibr\">38</xref>], which has shown effectiveness in processing sequential information [<xref rid=\"ref39\" ref-type=\"bibr\">39</xref>]. However, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation40\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation41\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation42\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> self-attention has a limitation where it only captures the correlation between individual tokens, overlooking interrelations among token groups (i.e. neighborhoods) [<xref rid=\"ref40\" ref-type=\"bibr\">40</xref>]. The limitation has been inadvertently overlooked because the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation43\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation44\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation45\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> computation seems sufficient for mapping input to output, with each output entry attending to every input entry. In this paper, we introduce an advanced HA mechanism, as shown in <xref rid=\"f2\" ref-type=\"fig\">Fig. 2</xref> (B-comp), by incorporating group-to-group attention to overcome the limitation of traditional <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation46\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation47\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation48\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> self-attention. In our framework, &#8216;groups&#8217; are dynamic, higher-level units obtained by applying a learnable aggregation over local neighborhoods of the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation49\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation50\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation51\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> vectors. They encode the collective characteristics of token neighborhoods by aggregating their corresponding <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation52\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation53\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation54\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> representations. Inter-group relationships denote the contextual dependencies among these aggregated, group-level representations. The group-to-group attention then applies self-attention over these dynamically formed groups to capture correlations among them. This two-level attention mechanism, consisting of token-level attention and group-level attention, enables the model to not only capture fine-grained correlations between individual tokens but also model broader correlations between groups within each transformer encoder layer, thereby enhancing the overall representational capability.</p><p>In the existing self-attention mechanism of DNABERT, each input token is mapped to <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation55\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation56\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation57\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> via three learnable linear projections. The correlation between every pair of tokens is computed using <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation58\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\frac{QK^{T}}{\\sqrt{d}}$\\end{document}</tex-math></inline-formula> (where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation59\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$d$\\end{document}</tex-math></inline-formula> is the dimension of the query and key), and applying the softmax function to these correlations generates the attention map A. The multiplication <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation60\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$A \\cdot V$\\end{document}</tex-math></inline-formula> linearly recombines tokens based on the attention weights at each position. However, this approach has the limitation of considering only correlations among individual tokens (i.e. individual patterns), overlooking the correlations among token groups (i.e. group patterns). To address this limitation, we extend the original self-attention mechanism by integrating both individual and group patterns within each encoder layer of DNABERT. We dynamically aggregate local neighborhoods of tokens into groups through a learnable operation, and model the contextual dependencies among these formed groups. This is achieved by generating group proxies in Query, Key, and Value and executing the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation61\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation62\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation63\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula> computation using these proxies. Specifically, we use <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation64\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$X$\\end{document}</tex-math></inline-formula> to represent <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation65\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation66\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K$\\end{document}</tex-math></inline-formula>, or <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation67\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V$\\end{document}</tex-math></inline-formula>, and generate group proxies <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation68\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q^{\\prime}$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation69\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K^{\\prime}$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation70\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V^{\\prime}$\\end{document}</tex-math></inline-formula> through an aggregation operation <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation71\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\text{Agg}(X)$\\end{document}</tex-math></inline-formula>. This operation employs a sliding-window strategy and is consistently implemented using depthwise convolution with a fixed grouping size (kernel size k = 3), chosen to effectively capture local contextual dependencies. The group attention calculation is then performed on these group proxies (<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation72\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Q^{\\prime}$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation73\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$K^{\\prime}$\\end{document}</tex-math></inline-formula>-<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation74\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$V^{\\prime}$\\end{document}</tex-math></inline-formula>), following the same procedure as self-attention, to generate the output. Finally, the outputs from both the traditional self-attention and the group attention branches are fused through linear mapping layers, followed by normalization and activation. This design constitutes an HA mechanism that captures both fine-grained token-level interactions and coarse-grained group-level dependencies. The process described above can be expressed as follows: </p><disp-formula id=\"deqn10\"><label>(10)</label><tex-math notation=\"LaTeX\" id=\"DmEquation10\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; Attn(Q,K,V)=softmax\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right)V, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn11\"><label>(11)</label><tex-math notation=\"LaTeX\" id=\"DmEquation11\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; \\begin{aligned} Attn(Q^{\\prime},K^{\\prime},V^{\\prime}) &amp;= Attn(Agg(Q),Agg(K),Agg(V)) \\\\ &amp;= softmax\\left(\\frac{Agg(Q)Agg(K)^{T}}{\\sqrt{d_{k}}}\\right)Agg(V), \\end{aligned} \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn12\"><label>(12)</label><tex-math notation=\"LaTeX\" id=\"DmEquation12\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; \\mathit{HA}\\text{-}Attn(Q,K,V)=F_{\\mathit{linear}}[Attn(Q,K,V),Attn(Q^{\\prime},K^{\\prime},V^{\\prime})], \\end{align*}\\end{document}</tex-math></disp-formula><p>where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation75\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Attn(\\cdot ,\\cdot ,\\cdot )$\\end{document}</tex-math></inline-formula> denotes the original self-attention output calculated from the Q, K, and V matrices, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation76\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Agg(\\cdot )$\\end{document}</tex-math></inline-formula> indicates the aggregation operation utilizing depth-wise convolution. The operator <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation77\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$[\\cdot ,\\cdot ]$\\end{document}</tex-math></inline-formula> signifies concatenation. <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation78\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathit{HA}\\text{-}Attn(\\cdot ,\\cdot ,\\cdot )$\\end{document}</tex-math></inline-formula> represents HA, which fuses traditional self-attention and group attention. In our framework, we replace the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation79\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$Attn$\\end{document}</tex-math></inline-formula> with <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation80\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\mathit{HA}\\text{-}Attn$\\end{document}</tex-math></inline-formula> to capture the correlations among not only individual tokens but also group proxies, enhancing the model&#8217;s representational capabilities.</p></sec><sec id=\"sec3i\"><title>Performance metrics</title><p>To ensure fair comparisons with existing methods, we adopt widely recognized performance evaluation metrics, including accuracy (ACC), Matthews&#8217; correlation coefficient (MCC), sensitivity (SN), specificity (SP), and area under the receiver operating characteristic curve (AUC). These metrics are defined as follows: </p><disp-formula id=\"deqn13\"><label>(13)</label><tex-math notation=\"LaTeX\" id=\"DmEquation13\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; ACC = \\frac{TP+TN}{TP+FN+TN+FP}, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn14\"><label>(14)</label><tex-math notation=\"LaTeX\" id=\"DmEquation14\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt[]{(TP + FP) (TP + FN) (TN + FP) (TN + FN)}}, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn15\"><label>(15)</label><tex-math notation=\"LaTeX\" id=\"DmEquation15\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; SN = \\frac{TP}{TP + FN}, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn16\"><label>(16)</label><tex-math notation=\"LaTeX\" id=\"DmEquation16\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; SP = \\frac{TN}{TN + FP}, \\end{align*}\\end{document}</tex-math></disp-formula><disp-formula id=\"deqn17\"><label>(17)</label><tex-math notation=\"LaTeX\" id=\"DmEquation17\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n\\begin{align*} &amp; AUC=\\frac{\\sum_{{\\text{i}\\in \\text{pos}}}{\\text{rank}_{\\text{i}}}-\\frac{{\\text{n}_{\\text{pos}}}({\\text{n}_{\\text{pos}}}+1)}{2}}{{\\text{n}_{\\text{pos}}\\text{n}_{\\text{neg}}}}, \\end{align*}\\end{document}</tex-math></disp-formula><p>where <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation81\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$TP$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation82\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$TN$\\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation83\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$FN$\\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation84\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$FP$\\end{document}</tex-math></inline-formula> denote the numbers of true positives, true negatives, false negatives, and false positives, respectively. <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation85\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$rank_{i}$\\end{document}</tex-math></inline-formula> stands for the rank of the <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation86\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$i$\\end{document}</tex-math></inline-formula>th positive sample when all predicted scores for the positive class are sorted in ascending order. <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation87\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$n_{pos}$\\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation88\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$n_{neg}$\\end{document}</tex-math></inline-formula> are the total numbers of positive and negative samples, respectively; ACC, MCC, and AUC assess the predictor&#8217;s overall performance from different perspectives, where higher values indicate better predictive capability. SN and SP represent the proportions of correctly identified methylated and unmethylated samples, respectively.</p></sec></sec><sec id=\"sec4\"><title>Results and discussion</title><p>In this section, we present extensive experiments to validate the effectiveness of our approach. First, we compare the performance of iDNA-DAPHA with existing state-of-the-art methods across 17 benchmark datasets. Second, we conduct ablation studies to evaluate the individual contributions of DAP and HA. Third, we interpret iDNA-DAPHA through motif similarity analysis and attention visualization, showing the model&#8217;s ability to capture conserved sequence patterns. Finally, we demonstrate that DAP helps the model learn discriminative representations for methylation prediction.</p><sec id=\"sec4a\"><title>Performance comparison with existing methods</title><p>In this study, to evaluate the effectiveness of our method, we performed DAP to obtain the shared pretrained weights, which were then independently fine-tuned on each methylation dataset (D1&#8211;D17). The resulting models were subsequently compared with existing state-of-the-art methods. As shown in <xref rid=\"TB1\" ref-type=\"table\">Table 1</xref>, iDNA-DAPHA consistently achieved superior performance in terms of ACC, outperforming all other methods across every dataset. For MCC, iDNA-DAPHA outperformed all competing methods on 16 of the 17 datasets and achieved performance comparable to the best-performing model on the remaining one (D15). This indicates that iDNA-DAPHA is not only accurate but also reliable in distinguishing between methylated and unmethylated sites. Additionally, iDNA-DAPHA achieved the highest results on 13 out of 17 datasets in terms of AUC, with results on the remaining 4 datasets (D7, D9, D15, and D16) comparable to the best. Notably, in datasets D1 and D13, both of which have the smallest training samples, iDNA-DAPHA showed significant improvements over the second-best method, achieving enhancements of 3.82% and 1.82% in ACC, and 9.27% and 4.01% in MCC, respectively. Besides the global performance metrics (ACC and MCC) and ranking capability evaluation (AUC), additional class-specific metrics&#8212;SN and SP&#8212;are available in <xref rid=\"sup2\" ref-type=\"supplementary-material\">Supplementary Table S2</xref>.</p><table-wrap position=\"float\" id=\"TB1\" orientation=\"portrait\"><label>Table 1</label><caption><p>Performance comparison between the proposed iDNA-DAPHA model and existing methods on 17 methylation datasets (D1&#8211;D17) evaluated by ACC, MCC, and AUC</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col span=\"2\" align=\"left\"/><col span=\"3\" align=\"left\"/><col span=\"4\" align=\"left\"/><col span=\"5\" align=\"left\"/><col span=\"6\" align=\"left\"/><col span=\"7\" align=\"left\"/><col span=\"8\" align=\"left\"/><col span=\"9\" align=\"left\"/><col span=\"10\" align=\"left\"/><col span=\"11\" align=\"left\"/><col span=\"12\" align=\"left\"/><col span=\"13\" align=\"left\"/><col span=\"14\" align=\"left\"/><col span=\"15\" align=\"left\"/><col span=\"16\" align=\"left\"/><col span=\"17\" align=\"left\"/><col span=\"18\" align=\"left\"/><col span=\"19\" align=\"left\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Indicator</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Models</bold>\n</th><th colspan=\"4\" align=\"left\" rowspan=\"1\">\n<bold>4mC</bold>\n</th><th colspan=\"2\" align=\"left\" rowspan=\"1\">\n<bold>5hmC</bold>\n</th><th colspan=\"11\" align=\"left\" rowspan=\"1\">\n<bold>6mA</bold>\n</th></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\"/><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D1</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D2</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D3</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D4</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D5</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D6</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D7</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D8</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D9</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D10</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D11</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D12</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D13</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D14</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D15</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D16</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>D17</bold>\n</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">ACC</td><td rowspan=\"1\" colspan=\"1\">iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.7109</td><td rowspan=\"1\" colspan=\"1\">0.8239</td><td rowspan=\"1\" colspan=\"1\">0.7042</td><td rowspan=\"1\" colspan=\"1\">0.7115</td><td rowspan=\"1\" colspan=\"1\">0.9475</td><td rowspan=\"1\" colspan=\"1\">0.9679</td><td rowspan=\"1\" colspan=\"1\">0.8377</td><td rowspan=\"1\" colspan=\"1\">0.8557</td><td rowspan=\"1\" colspan=\"1\">0.7113</td><td rowspan=\"1\" colspan=\"1\">0.8962</td><td rowspan=\"1\" colspan=\"1\">0.9226</td><td rowspan=\"1\" colspan=\"1\">0.8842</td><td rowspan=\"1\" colspan=\"1\">0.8545</td><td rowspan=\"1\" colspan=\"1\">0.7855</td><td rowspan=\"1\" colspan=\"1\">0.8563</td><td rowspan=\"1\" colspan=\"1\">0.7342</td><td rowspan=\"1\" colspan=\"1\">0.8451</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.8251</td><td rowspan=\"1\" colspan=\"1\">0.842</td><td rowspan=\"1\" colspan=\"1\">0.7027</td><td rowspan=\"1\" colspan=\"1\">0.7383</td><td rowspan=\"1\" colspan=\"1\">0.9492</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9685</bold>\n</td><td rowspan=\"1\" colspan=\"1\">0.8538</td><td rowspan=\"1\" colspan=\"1\">0.8903</td><td rowspan=\"1\" colspan=\"1\">0.7328</td><td rowspan=\"1\" colspan=\"1\">0.9122</td><td rowspan=\"1\" colspan=\"1\">0.9268</td><td rowspan=\"1\" colspan=\"1\">0.898</td><td rowspan=\"1\" colspan=\"1\">0.8261</td><td rowspan=\"1\" colspan=\"1\">0.8011</td><td rowspan=\"1\" colspan=\"1\">0.874</td><td rowspan=\"1\" colspan=\"1\">0.7738</td><td rowspan=\"1\" colspan=\"1\">0.8694</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8579</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8524</td><td rowspan=\"1\" colspan=\"1\">0.723</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7434</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9501</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9679</td><td rowspan=\"1\" colspan=\"1\">0.8603</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9138</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7399</td><td rowspan=\"1\" colspan=\"1\">0.9228</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9413</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9104</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8629</td><td rowspan=\"1\" colspan=\"1\">0.8278</td><td rowspan=\"1\" colspan=\"1\">0.8804</td><td rowspan=\"1\" colspan=\"1\">0.7771</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8817</underline>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.8333</td><td rowspan=\"1\" colspan=\"1\">0.8522</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7376</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7380</td><td rowspan=\"1\" colspan=\"1\">0.9484</td><td rowspan=\"1\" colspan=\"1\">0.9649</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8649</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9131</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7590</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9276</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9407</td><td rowspan=\"1\" colspan=\"1\">0.9077</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9164</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8325</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8840</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7895</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8742</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.8531</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8529</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7104</td><td rowspan=\"1\" colspan=\"1\">0.7429</td><td rowspan=\"1\" colspan=\"1\">0.9492</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9679</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8607</td><td rowspan=\"1\" colspan=\"1\">0.9086</td><td rowspan=\"1\" colspan=\"1\">0.7445</td><td rowspan=\"1\" colspan=\"1\">0.9231</td><td rowspan=\"1\" colspan=\"1\">0.9388</td><td rowspan=\"1\" colspan=\"1\">0.9072</td><td rowspan=\"1\" colspan=\"1\">0.8177</td><td rowspan=\"1\" colspan=\"1\">0.8270</td><td rowspan=\"1\" colspan=\"1\">0.8821</td><td rowspan=\"1\" colspan=\"1\">0.7680</td><td rowspan=\"1\" colspan=\"1\">0.8774</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8907</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8567</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7396</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7476</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9509</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9685</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8655</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9216</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7603</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9283</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9449</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9132</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9331</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8457</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8846</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7978</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8941</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">MCC</td><td rowspan=\"1\" colspan=\"1\">iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.4220</td><td rowspan=\"1\" colspan=\"1\">0.648</td><td rowspan=\"1\" colspan=\"1\">0.4080</td><td rowspan=\"1\" colspan=\"1\">0.4230</td><td rowspan=\"1\" colspan=\"1\">0.8970</td><td rowspan=\"1\" colspan=\"1\">0.9360</td><td rowspan=\"1\" colspan=\"1\">0.6760</td><td rowspan=\"1\" colspan=\"1\">0.7120</td><td rowspan=\"1\" colspan=\"1\">0.4230</td><td rowspan=\"1\" colspan=\"1\">0.7920</td><td rowspan=\"1\" colspan=\"1\">0.8460</td><td rowspan=\"1\" colspan=\"1\">0.7690</td><td rowspan=\"1\" colspan=\"1\">0.7100</td><td rowspan=\"1\" colspan=\"1\">0.5720</td><td rowspan=\"1\" colspan=\"1\">0.7280</td><td rowspan=\"1\" colspan=\"1\">0.4680</td><td rowspan=\"1\" colspan=\"1\">0.6910</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.6517</td><td rowspan=\"1\" colspan=\"1\">0.6842</td><td rowspan=\"1\" colspan=\"1\">0.4064</td><td rowspan=\"1\" colspan=\"1\">0.4768</td><td rowspan=\"1\" colspan=\"1\">0.9009</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9369</bold>\n</td><td rowspan=\"1\" colspan=\"1\">0.7088</td><td rowspan=\"1\" colspan=\"1\">0.7808</td><td rowspan=\"1\" colspan=\"1\">0.4673</td><td rowspan=\"1\" colspan=\"1\">0.8244</td><td rowspan=\"1\" colspan=\"1\">0.8244</td><td rowspan=\"1\" colspan=\"1\">0.796</td><td rowspan=\"1\" colspan=\"1\">0.6525</td><td rowspan=\"1\" colspan=\"1\">0.6096</td><td rowspan=\"1\" colspan=\"1\">0.754</td><td rowspan=\"1\" colspan=\"1\">0.5512</td><td rowspan=\"1\" colspan=\"1\">0.7394</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7162</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7047</td><td rowspan=\"1\" colspan=\"1\">0.447</td><td rowspan=\"1\" colspan=\"1\">0.4868</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9022</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9358</td><td rowspan=\"1\" colspan=\"1\">0.7223</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8279</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.4843</td><td rowspan=\"1\" colspan=\"1\">0.8457</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8827</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8209</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7271</td><td rowspan=\"1\" colspan=\"1\">0.6569</td><td rowspan=\"1\" colspan=\"1\">0.7671</td><td rowspan=\"1\" colspan=\"1\">0.5543</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7634</underline>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.6684</td><td rowspan=\"1\" colspan=\"1\">0.7051</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.4772</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.4771</td><td rowspan=\"1\" colspan=\"1\">0.8984</td><td rowspan=\"1\" colspan=\"1\">0.9299</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7307</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8263</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.5199</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8552</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8815</td><td rowspan=\"1\" colspan=\"1\">0.8156</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8328</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.6688</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7773</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.5803</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7491</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.7061</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7062</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.4227</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.4865</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9003</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9359</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7215</td><td rowspan=\"1\" colspan=\"1\">0.8178</td><td rowspan=\"1\" colspan=\"1\">0.4944</td><td rowspan=\"1\" colspan=\"1\">0.8462</td><td rowspan=\"1\" colspan=\"1\">0.8778</td><td rowspan=\"1\" colspan=\"1\">0.8146</td><td rowspan=\"1\" colspan=\"1\">0.6354</td><td rowspan=\"1\" colspan=\"1\">0.6542</td><td rowspan=\"1\" colspan=\"1\">0.7720</td><td rowspan=\"1\" colspan=\"1\">0.5378</td><td rowspan=\"1\" colspan=\"1\">0.7558</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7826</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7135</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.4838</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.4962</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9035</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9369</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7315</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8435</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.5214</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8567</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8898</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8270</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8662</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.6924</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7770</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.5956</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7882</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">AUC</td><td rowspan=\"1\" colspan=\"1\">iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.7800</td><td rowspan=\"1\" colspan=\"1\">0.9000</td><td rowspan=\"1\" colspan=\"1\">0.7710</td><td rowspan=\"1\" colspan=\"1\">0.7800</td><td rowspan=\"1\" colspan=\"1\">0.9600</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9840</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9110</td><td rowspan=\"1\" colspan=\"1\">0.9350</td><td rowspan=\"1\" colspan=\"1\">0.7790</td><td rowspan=\"1\" colspan=\"1\">0.9560</td><td rowspan=\"1\" colspan=\"1\">0.9770</td><td rowspan=\"1\" colspan=\"1\">0.9500</td><td rowspan=\"1\" colspan=\"1\">0.9240</td><td rowspan=\"1\" colspan=\"1\">0.8680</td><td rowspan=\"1\" colspan=\"1\">0.9220</td><td rowspan=\"1\" colspan=\"1\">0.8130</td><td rowspan=\"1\" colspan=\"1\">0.9210</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABT [<xref rid=\"ref3\" ref-type=\"bibr\">3</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.8555</td><td rowspan=\"1\" colspan=\"1\">0.9070</td><td rowspan=\"1\" colspan=\"1\">0.7537</td><td rowspan=\"1\" colspan=\"1\">0.8057</td><td rowspan=\"1\" colspan=\"1\">0.9553</td><td rowspan=\"1\" colspan=\"1\">0.9757</td><td rowspan=\"1\" colspan=\"1\">0.9184</td><td rowspan=\"1\" colspan=\"1\">0.9433</td><td rowspan=\"1\" colspan=\"1\">0.7902</td><td rowspan=\"1\" colspan=\"1\">0.9544</td><td rowspan=\"1\" colspan=\"1\">0.9544</td><td rowspan=\"1\" colspan=\"1\">0.9510</td><td rowspan=\"1\" colspan=\"1\">0.8789</td><td rowspan=\"1\" colspan=\"1\">0.8709</td><td rowspan=\"1\" colspan=\"1\">0.9310</td><td rowspan=\"1\" colspan=\"1\">0.8361</td><td rowspan=\"1\" colspan=\"1\">0.9261</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.9089</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9285</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.7897</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8213</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9677</td><td rowspan=\"1\" colspan=\"1\">0.9796</td><td rowspan=\"1\" colspan=\"1\">0.9349</td><td rowspan=\"1\" colspan=\"1\">0.9682</td><td rowspan=\"1\" colspan=\"1\">0.8098</td><td rowspan=\"1\" colspan=\"1\">0.9713</td><td rowspan=\"1\" colspan=\"1\">0.9804</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9695</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9280</td><td rowspan=\"1\" colspan=\"1\">0.9062</td><td rowspan=\"1\" colspan=\"1\">0.9355</td><td rowspan=\"1\" colspan=\"1\">0.8500</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9506</underline>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">MuLan-Methyl [<xref rid=\"ref1\" ref-type=\"bibr\">1</xref>]</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9108</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9256</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8064</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.8149</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9680</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9817</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9378</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9684</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8350</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9730</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9821</underline>\n</td><td rowspan=\"1\" colspan=\"1\">0.9687</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9654</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9082</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9467</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8669</bold>\n</td><td rowspan=\"1\" colspan=\"1\">0.9446</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>]</td><td rowspan=\"1\" colspan=\"1\">0.8958</td><td rowspan=\"1\" colspan=\"1\">0.9283</td><td rowspan=\"1\" colspan=\"1\">0.7758</td><td rowspan=\"1\" colspan=\"1\">0.8187</td><td rowspan=\"1\" colspan=\"1\">0.9672</td><td rowspan=\"1\" colspan=\"1\">0.9810</td><td rowspan=\"1\" colspan=\"1\">0.9342</td><td rowspan=\"1\" colspan=\"1\">0.9657</td><td rowspan=\"1\" colspan=\"1\">0.8156</td><td rowspan=\"1\" colspan=\"1\">0.9713</td><td rowspan=\"1\" colspan=\"1\">0.9813</td><td rowspan=\"1\" colspan=\"1\">0.9686</td><td rowspan=\"1\" colspan=\"1\">0.8806</td><td rowspan=\"1\" colspan=\"1\">0.9053</td><td rowspan=\"1\" colspan=\"1\">0.9437</td><td rowspan=\"1\" colspan=\"1\">0.8454</td><td rowspan=\"1\" colspan=\"1\">0.9485</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9291</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9291</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8112</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8219</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9708</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9841</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9365</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9714</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8311</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9735</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9835</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9705</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9682</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9100</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9460</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8654</underline>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9541</bold>\n</td></tr></tbody></table><table-wrap-foot><p> Bold values indicate the optimal performance among the compared methods, while underlined values represent the second-best performance.</p></table-wrap-foot></table-wrap><p>Overall, these results demonstrate that iDNA-DAPHA outperforms existing methods across a broad spectrum of datasets, confirming its suitability for methylation identification, especially in scenarios with limited training samples for specific methylation types in certain species. The consistently superior performance indicates the reliability and robustness of iDNA-DAPHA, making it a powerful tool for methylation prediction.</p></sec><sec id=\"sec4b\"><title>Ablation studies</title><sec id=\"sec4b1\"><title>Ablation of domain-adaptive pretraining and hierarchical attention</title><p>In this subsection, we present the results of ablation studies to evaluate the effectiveness of DAP and HA. Ablation experiments were conducted on three randomly selected datasets representing different types of methylation: 4mC_C.equisetifolia, 5hmC_H.sapiens, and 6mA_S.cerevisiae. We first compared four main variants of the model (with results summarized in <xref rid=\"TB2\" ref-type=\"table\">Table 2</xref>): the baseline model (BL), the baseline with domain-adaptive pretraining (BL+DAP), the baseline with hierarchical attention (BL+HA), and our proposed method (iDNA-DAPHA), which integrates both components. The baseline model (BL) provided a solid foundation across these datasets. For instance, it achieved the highest accuracy of 0.9428 on the 5hmC_H.sapiens dataset, while even its lowest accuracy, observed on the 6mA_S.cerevisiae dataset, remained at a respectable 0.8106. Nevertheless, when BL+DAP, which integrates DAP, was used, there were consistent and notable improvements in performance metrics (ACC, MCC, and AUC), with average increases of 2.81% in ACC, 5.62% in MCC, and 2.06% in AUC. This trend underscores the effectiveness of DAP in enhancing the model&#8217;s ability to adapt to methylation sequences. Furthermore, BL+HA, which incorporates HA into the baseline, also resulted in performance improvements, albeit to a lesser extent than those observed with domain-adaptive pretraining (BL+DAP). The BL+HA variant showed average increases of 0.72% in ACC, 1.44% in MCC, and 0.46% in AUC. Finally, our proposed method, iDNA-DAPHA, which combines both DAP and HA, consistently outperformed other variants, with average increases of 3.35% in ACC, 6.71% in MCC, and 2.30% in AUC. Notably, on the 4mC_C.equisetifolia dataset, iDNA-DAPHA exhibited the highest improvement, with an increase of 5.74% in accuracy, 11.51% in MCC, and 3.63% in AUC.</p><table-wrap position=\"float\" id=\"TB2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Ablation analysis of our model across three species and methylation types, comparing the baseline (BL), BL with DAP (BL+DAP), BL with HA (BL+HA), the full iDNA-DAPHA model, and the variant without GRL</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col span=\"2\" align=\"left\"/><col span=\"3\" align=\"left\"/><col span=\"4\" align=\"left\"/><col span=\"5\" align=\"left\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Dataset</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Methods</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>ACC</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>MCC</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>AUC</bold>\n</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">4mC C.equisetifolia</td><td rowspan=\"1\" colspan=\"1\">BL</td><td rowspan=\"1\" colspan=\"1\">0.8333</td><td rowspan=\"1\" colspan=\"1\">0.6675</td><td rowspan=\"1\" colspan=\"1\">0.8928</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+DAP</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8798</underline> (+4.65%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.7597</underline> (+9.22%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9238</underline> (+3.10%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+HA</td><td rowspan=\"1\" colspan=\"1\">0.8443 (+1.10%)</td><td rowspan=\"1\" colspan=\"1\">0.6890 (+2.15%)</td><td rowspan=\"1\" colspan=\"1\">0.8978 (+0.50%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-DAPHA w/o GRL</td><td rowspan=\"1\" colspan=\"1\">0.8689 (+3.56%)</td><td rowspan=\"1\" colspan=\"1\">0.7413 (+7.38%)</td><td rowspan=\"1\" colspan=\"1\">0.9193 (+2.65%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8907</bold> (+<bold>5.74%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.7826</bold> (+<bold>11.51%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9291</bold> (+<bold>3.63%</bold>)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">5hmC H.sapiens</td><td rowspan=\"1\" colspan=\"1\">BL</td><td rowspan=\"1\" colspan=\"1\">0.9428</td><td rowspan=\"1\" colspan=\"1\">0.8873</td><td rowspan=\"1\" colspan=\"1\">0.9633</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+DAP</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9497</underline> (+0.69%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9014</underline> (+1.41%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9696</underline> (+0.63%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+HA</td><td rowspan=\"1\" colspan=\"1\">0.9480 (+0.52%)</td><td rowspan=\"1\" colspan=\"1\">0.8978 (+1.05%)</td><td rowspan=\"1\" colspan=\"1\">0.9671 (+0.38%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-DAPHA w/o GRL</td><td rowspan=\"1\" colspan=\"1\">0.9488 (+0.6%)</td><td rowspan=\"1\" colspan=\"1\">0.8990 (+1.17%)</td><td rowspan=\"1\" colspan=\"1\">0.9679 (+0.46%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9509</bold> (+<bold>0.81%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9035</bold> (+<bold>1.62%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9708</bold> (+<bold>0.75%</bold>)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">6mA S.cerevisiae</td><td rowspan=\"1\" colspan=\"1\">BL</td><td rowspan=\"1\" colspan=\"1\">0.8106</td><td rowspan=\"1\" colspan=\"1\">0.6223</td><td rowspan=\"1\" colspan=\"1\">0.8847</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+DAP</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.8415</underline> (+3.09%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.6845</underline> (+6.22%)</td><td rowspan=\"1\" colspan=\"1\">\n<underline>0.9093</underline> (+2.46%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">BL+HA</td><td rowspan=\"1\" colspan=\"1\">0.8159 (+0.53%)</td><td rowspan=\"1\" colspan=\"1\">0.6334 (+1.11%)</td><td rowspan=\"1\" colspan=\"1\">0.8897 (+0.50%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">iDNA-DAPHA w/o GRL</td><td rowspan=\"1\" colspan=\"1\">0.8394 (+2.88%)</td><td rowspan=\"1\" colspan=\"1\">0.6829 (+6.06%)</td><td rowspan=\"1\" colspan=\"1\">0.9050 (+2.03%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">\n<bold>iDNA-DAPHA</bold>\n</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.8457</bold> (+<bold>3.51%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.6924</bold> (+<bold>7.01%</bold>)</td><td rowspan=\"1\" colspan=\"1\">\n<bold>0.9100</bold> (+<bold>2.53%</bold>)</td></tr></tbody></table><table-wrap-foot><p>Bold values indicate the optimal performance among the compared methods, while underlined values represent the second-best performance.</p></table-wrap-foot></table-wrap><p>In addition, to investigate the role of the gradient reversal layer (GRL) in DAP, we further introduced an additional variant, &#8216;iDNA-DAPHA w/o GRL,&#8217; which performs DAP without the GRL while still incorporating the HA mechanism. Intriguingly, the results in <xref rid=\"TB2\" ref-type=\"table\">Table 2</xref> reveal that this variant not only performs worse than iDNA-DAPHA but also that this architecturally more complex model consistently underperforms the comparatively simpler BL+DAP model across all three datasets. This observation demonstrates that the GRL plays a crucial role in feature alignment during DAP, promoting the learning of shared features across methylation tasks.</p><p>Overall, these ablation studies show that both DAP and HA contribute to improved model performance, with DAP having a more pronounced effect. Our iDNA-DAPHA method integrates both techniques, achieving superior generalization and prediction accuracy. These results indicate that incorporating DAP and HA is an effective strategy for enhancing performance across diverse methylation datasets.</p></sec><sec id=\"sec4b2\"><title>Evaluation of dual-scale versus four-scale k-mer representations</title><p>Our model employs a dual-scale k-mer representation strategy, integrating features from both 3-mers and 6-mers. This choice follows the effective dual-scale information processing strategy proposed in the previous method iDNA-ABF [<xref rid=\"ref12\" ref-type=\"bibr\">12</xref>], where this combination was reported to achieve the highest average accuracy among various single-scale and dual-scale configurations. To evaluate this decision against a more complex alternative, we conducted a comparative analysis with a four-scale strategy (3-, 4-, 5-, and 6-mers), similar to that used in StableDNAm [<xref rid=\"ref11\" ref-type=\"bibr\">11</xref>]. As shown in <xref rid=\"TB3\" ref-type=\"table\">Table 3</xref>, this four-scale strategy yields slight improvements in predictive accuracy across three benchmark datasets, while significantly increasing computational demands (see <xref rid=\"sup3\" ref-type=\"supplementary-material\">Supplementary Table S3</xref>). Specifically, it requires four parallel DNA-BERT models (DNAbert-3, -4, -5, and -6), resulting in nearly double the number of parameters, training time, inference time, and GPU memory usage.</p><table-wrap position=\"float\" id=\"TB3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Comparison of predictive performance between dual-scale (3- and 6-mers) and four-scale (3-, 4-, 5-, and 6-mers) k-mer representation strategies</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col span=\"2\" align=\"left\"/><col span=\"3\" align=\"left\"/><col span=\"4\" align=\"left\"/><col span=\"5\" align=\"left\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Dataset</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>K-mers strategy</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>ACC</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>MCC</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>AUC</bold>\n</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">4mC C.equisetifolia</td><td rowspan=\"1\" colspan=\"1\">dual-scale</td><td rowspan=\"1\" colspan=\"1\">0.8907</td><td rowspan=\"1\" colspan=\"1\">0.7826</td><td rowspan=\"1\" colspan=\"1\">0.9291</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">four-scale</td><td rowspan=\"1\" colspan=\"1\">0.8962 (+0.55%)</td><td rowspan=\"1\" colspan=\"1\">0.7931 (+1.05%)</td><td rowspan=\"1\" colspan=\"1\">0.9339 (+0.48%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">5hmC H.sapiens</td><td rowspan=\"1\" colspan=\"1\">dual-scale</td><td rowspan=\"1\" colspan=\"1\">0.9509</td><td rowspan=\"1\" colspan=\"1\">0.9035</td><td rowspan=\"1\" colspan=\"1\">0.9708</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">four-scale</td><td rowspan=\"1\" colspan=\"1\">0.9509 (+0%)</td><td rowspan=\"1\" colspan=\"1\">0.9040 (+0.05%)</td><td rowspan=\"1\" colspan=\"1\">0.9733 (+0.25%)</td></tr><tr><td rowspan=\"1\" colspan=\"1\">6mA S.cerevisiae</td><td rowspan=\"1\" colspan=\"1\">dual-scale</td><td rowspan=\"1\" colspan=\"1\">0.8457</td><td rowspan=\"1\" colspan=\"1\">0.6924</td><td rowspan=\"1\" colspan=\"1\">0.9100</td></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">four-scale</td><td rowspan=\"1\" colspan=\"1\">0.8457 (+0%)</td><td rowspan=\"1\" colspan=\"1\">0.6929 (+0.05%)</td><td rowspan=\"1\" colspan=\"1\">0.9120 (+0.2%)</td></tr></tbody></table></table-wrap><p>Therefore, the dual-scale (3-mer and 6-mer) representation provides a practical balance between predictive performance and computational efficiency, and we retain it as the default configuration for our model. In addition, to facilitate reproducibility and flexible application, we have made both the dual-scale and four-scale domain-adaptive pretrained weights and the corresponding model code publicly available, allowing users to fine-tune either model depending on their performance and efficiency requirements.</p></sec><sec id=\"sec4b3\"><title>Computational efficiency analysis</title><p>To evaluate the computational efficiency of iDNA-DAPHA, we quantitatively compared it with the baseline model under the same hardware and software environment. <xref rid=\"TB4\" ref-type=\"table\">Table 4</xref> summarizes the detailed benchmark results obtained from the 4mC_C.equisetifolia dataset as a representative example. Compared with the baseline, iDNA-DAPHA introduces slightly more parameters and moderate increases in training time and GPU memory. The increase in parameter count mainly arises from the HA module, whereas the gradient reversal layer used in DAP adds no additional learnable parameters. The reported parameter counts correspond to the fine-tuned models actually used for inference.</p><table-wrap position=\"float\" id=\"TB4\" orientation=\"portrait\"><label>Table 4</label><caption><p>Computational efficiency comparison of iDNA-DAPHA and the baseline</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col span=\"2\" align=\"left\"/><col span=\"3\" align=\"left\"/><col span=\"4\" align=\"left\"/><col span=\"5\" align=\"left\"/><col span=\"6\" align=\"left\"/><col span=\"7\" align=\"left\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Model</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Params (M)</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Training time/Step (ms)</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Inference time/Step (ms)</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Peak Mem (Train, MB)</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Peak Mem (Infer, MB)</bold>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Pretrain time (Total)</bold>\n</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">Baseline</td><td rowspan=\"1\" colspan=\"1\">175.32</td><td rowspan=\"1\" colspan=\"1\">78.7</td><td rowspan=\"1\" colspan=\"1\">23.2</td><td rowspan=\"1\" colspan=\"1\">3541.12</td><td rowspan=\"1\" colspan=\"1\">691.29</td><td rowspan=\"1\" colspan=\"1\">&#8211;</td></tr><tr><td rowspan=\"1\" colspan=\"1\">iDNA-DAPHA</td><td rowspan=\"1\" colspan=\"1\">217.86</td><td rowspan=\"1\" colspan=\"1\">117.2</td><td rowspan=\"1\" colspan=\"1\">29.3</td><td rowspan=\"1\" colspan=\"1\">4774.75</td><td rowspan=\"1\" colspan=\"1\">862.23</td><td rowspan=\"1\" colspan=\"1\">1.72h (39160 steps, bs=32)</td></tr></tbody></table></table-wrap><fig position=\"float\" id=\"f3\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Interpretable illustrations of motifs learnt by iDNA-DAPHA across three species and methylation types (4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256), displaying attention heatmaps (left panel) where visual intensity denotes greater importance, alongside a comparison with STREME motifs (right panel) using P-values to indicate statistical significance and motif similarity.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"bbaf642f3.jpg\"><alt-text>Alt text: Interpretable illustrations of motifs learnt by iDNA-DAPHA across three species and methylation types: 4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256. The figure is organized in three horizontal rows, each showing two main components. Left Panel: Attention heatmaps generated by iDNA-DAPHA for 3-mer and 6-mer sequences, where darker blue indicates high-importance regions (e.g. &#8216;TCTGC&#8217;, &#8216;AGGGG&#8217;, and &#8216;TTGAG&#8217;). Right Panel: Comparing motifs identified by iDNA-DAPHA and the STREME tool. Low Tomtom P-values (3.57e-03 for 4mC_F.vesca, 2.44e-02 for 5hmC_H.sapiens, and 5.18e-03 for 6mA_Xoc BLS256) show the high similarity between the paired motifs.</alt-text></graphic></fig><p>In absolute terms, iDNA-DAPHA remains lightweight (<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation89\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\approx $\\end{document}</tex-math></inline-formula>218 M parameters, &lt;4.8 GB peak GPU memory during training). Despite a nearly 49% increase in training time, the additional one-time pretraining phase was completed in a modest 1.72 h. Similarly, while the per-step inference time increases by <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation90\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\sim $\\end{document}</tex-math></inline-formula>26%, the final per-step latency (<inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation91\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$\\sim $\\end{document}</tex-math></inline-formula>29 ms) remains sufficiently low for practical deployment. In exchange for these manageable costs, the model delivers significant performance gains, with average performance increases of 3.35% in ACC, 6.71% in MCC, and 2.30% in AUC (see <xref rid=\"TB2\" ref-type=\"table\">Table 2</xref>). Overall, these results demonstrate that iDNA-DAPHA achieves a favorable trade-off between efficiency and accuracy.</p></sec></sec><sec id=\"sec4f\"><title>Interpreting iDNA-DAPHA through motif similarity analysis and attention visualization</title><p>To provide insight into the interpretability of iDNA-DAPHA, we analyse the similarity between motifs extracted by our method and those identified by the established motif-finding tool STREME. We employ iDNA-DAPHA&#8217;s attention mechanism on sequences from three randomly selected methylation datasets to identify key regions crucial for accurate methylation prediction and extract motifs from these attention-highlighted regions. The regions focused on by the model correspond to sequence areas with high attention scores, indicating their significance in the prediction process. As shown in <xref rid=\"f3\" ref-type=\"fig\">Fig. 3</xref> (right), the motifs identified by our method (highlighted within the light blue boxes) exhibit a high degree of similarity to those found by STREME on the three species datasets (4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256). This similarity is quantitatively supported by low <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation92\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$P$\\end{document}</tex-math></inline-formula>-values calculated using the TOMTOM tool: 4mC_F.vesca <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation93\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$P$\\end{document}</tex-math></inline-formula>-value = 3.57e-03, 5hmC_H.sapiens <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation94\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$P$\\end{document}</tex-math></inline-formula>-value = 2.44e-02, and 6mA_Xoc BLS256 <inline-formula><tex-math notation=\"LaTeX\" id=\"ImEquation95\">\\documentclass[12pt]{minimal}\n\\usepackage{amsmath}\n\\usepackage{wasysym}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{amsbsy}\n\\usepackage{upgreek}\n\\usepackage{mathrsfs}\n\\setlength{\\oddsidemargin}{-69pt}\n\\begin{document}\n$P$\\end{document}</tex-math></inline-formula>-value = 5.18e-03. The consistent matches between iDNA-DAPHA and STREME motifs reflect the reliability of our method in identifying conserved sequence patterns.</p><fig position=\"float\" id=\"f4\" orientation=\"portrait\"><label>Figure 4</label><caption><p>t-SNE visualizations comparing the learnt feature representations of general-purpose (left) and domain-adaptive (right) pretraining across three methylation datasets (4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256) demonstrate that DAP yields distinct and compact clusters, in contrast to the dispersed and overlapping clusters observed in general-purpose pretraining.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"bbaf642f4.jpg\"><alt-text>Alt text: t-SNE visualizations comparing learnt feature representations from general-purpose pretraining versus DAP across three DNA methylation datasets: 4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256. The plots in the left column (general-purpose pretraining) show dispersed and overlapping clusters of methylated and unmethylated sequences, whereas the plots in the right column (DAP) show more compact and clearly separated clusters, illustrating improved discriminative representations for methylation prediction.</alt-text></graphic></fig><p>Furthermore, to enhance intuitive understanding, we visualize attention score distributions for three randomly selected sequences from the respective species. The high-attention regions are aligned with the motifs identified in each dataset, as depicted in <xref rid=\"f3\" ref-type=\"fig\">Fig. 3</xref> (left). This alignment not only highlights the regions focused on by our method but also demonstrates their correspondence to statistically significant motifs, further enhancing the interpretability of the model.</p></sec><sec id=\"sec4g\"><title>Impact of domain-adaptive pretraining on DNA methylation prediction</title><p>Given that DAP showed a more notable performance improvement than HA in the ablation studies, we also conduct an interpretability analysis through visualization to explore its impact on DNA methylation prediction. Specifically, we compare DAP with general-purpose pretraining. We randomly selected three methylation datasets from species representing different DNA methylation types and visualized the learnt representation distribution using t-SNE. <xref rid=\"f4\" ref-type=\"fig\">Fig. 4</xref> illustrates the differences in feature space by models with general-purpose pretraining versus those with methylation DAP. The left-side plots, derived from model with general-purpose pretraining, display more dispersed and overlapping clusters across the three datasets (4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256), suggesting less coherent groupings and reduced discrimination between methylated and unmethylated sequences. In contrast, the right-side plots, obtained from model with methylation DAP, exhibit more compact and well-separated clusters, reflecting enhanced discriminative representations for methylation prediction.</p><p>This comparison demonstrates that DAP leads to more discriminative representations for DNA methylation data, improving the model&#8217;s ability to distinguish between methylated and unmethylated sequences. The enhanced clustering evident in the visualizations underscores DAP&#8217;s utility, ultimately yielding more accurate predictions in target downstream tasks.</p></sec></sec><sec id=\"sec5\"><title>Conclusion</title><p>In this paper, we present iDNA-DAPHA, an accurate and generic two-stage computational framework for DNA methylation prediction. The framework leverages DAP incorporating feature alignment to learn general features across various types of methylation sequences from multiple species, followed by fine-tuning to capture task-specific features. Moreover, we introduce an HA mechanism into the model architecture to improve its performance. Extensive experiments across 17 benchmark datasets demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods, and ablation studies validate the effectiveness of DAP and HA. As aberrant DNA methylation is widely recognized to be associated with many diseases and serves as an important indicator in epigenetic diagnostics, iDNA-DAPHA&#8217;s methylation prediction capability across species and methylation types, even with limited training samples, may contribute to studies on disease biomarker discovery and epigenetic drug target identification. In future work, we plan to develop a user-friendly web server to facilitate the application of our framework by biologists. Additionally, we will explore integrating broader methylation data during pretraining to further enhance the model&#8217;s ability to adapt across species and methylation types.</p><boxed-text id=\"box01\" position=\"float\" orientation=\"portrait\"><sec id=\"sec24a\"><title>Key Points</title><list list-type=\"bullet\"><list-item id=\"item1\"><p>We propose iDNA-DAPHA, an accurate and generic two-stage framework that leverages domain-adaptive pretraining (DAP) incorporating feature alignment to learn common features across various types of methylation sequences from multiple species.</p></list-item><list-item id=\"item2\"><p>Our framework introduces a hierarchical attention (HA) mechanism that combines token-to-token attention and group-to-group attention to capture both fine-grained inter-token and broader inter-group correlations.</p></list-item><list-item id=\"item3\"><p>Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods on benchmark datasets, especially in scenarios with limited training samples for specific methylation types in certain species. Ablation studies validate the effectiveness and contributions of DAP and HA. Furthermore, visualization-based interpretability analyses reveal that the model can learn conserved sequence patterns and more discriminative representations for methylation prediction.</p></list-item></list></sec></boxed-text></sec><sec sec-type=\"supplementary-material\"><title>Supplementary Material</title><supplementary-material id=\"sup1\" position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><label>Supplementary_table_s1_bbaf642</label><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"supplementary_table_s1_bbaf642.xlsx\" position=\"float\" orientation=\"portrait\"/></supplementary-material><supplementary-material id=\"sup2\" position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><label>Supplementary_table_s2_bbaf642</label><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"supplementary_table_s2_bbaf642.xlsx\" position=\"float\" orientation=\"portrait\"/></supplementary-material><supplementary-material id=\"sup3\" position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><label>Supplementary_table_s3_bbaf642</label><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"supplementary_table_s3_bbaf642.xlsx\" position=\"float\" orientation=\"portrait\"/></supplementary-material></sec></body><back><sec sec-type=\"COI-statement\" id=\"sec50b\"><title>Conflict of interest</title><p>The authors declare no conflict of interest.</p></sec><sec id=\"sec50c\"><title>Funding</title><p>This work was supported by National Natural Science Foundation of China (NSFC) 62272172 and 62266012, Zhuhai Science and Technology Plan Project (2320004002758), Fundamental Research Funds for the Central Universities 2025ZYGXZR095, and the High-Level Innovative Talent Project of Guizhou Province (Grant no. QKHPTRC-GCC2023027).</p></sec><sec sec-type=\"data-availability\" id=\"sec50d\"><title>Data availability</title><p>The model code, domain-adaptive pretraining weight, and fine-tuned weights on all benchmark datasets are freely available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/WenjunWang-SCUT/iDNA-DAPHA\" ext-link-type=\"uri\">https://github.com/WenjunWang-SCUT/iDNA-DAPHA</ext-link> to facilitate reproducibility and benefit the research community. All datasets used in this study are publicly available and were obtained from the previously published study iDNA-MS [<xref rid=\"ref4\" ref-type=\"bibr\">4</xref>].</p></sec><ref-list id=\"bib1\"><title>References</title><ref id=\"ref1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zeng</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Gautam</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Huson</surname> &#160;<given-names>DH</given-names></string-name></person-group>. <article-title>MuLan-methyl&#8212;Multiple transformer-based language models for accurate DNA methylation prediction</article-title>. <source><italic toggle=\"yes\">GigaScience</italic></source> &#160;<year>2023</year>;<volume>12</volume>:<fpage>giad054</fpage>. <pub-id pub-id-type=\"doi\">10.1093/gigascience/giad054</pub-id><pub-id pub-id-type=\"pmcid\">PMC10367125</pub-id><pub-id pub-id-type=\"pmid\">37489753</pub-id></mixed-citation></ref><ref id=\"ref2\"><label>2</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zhou</surname> &#160;<given-names>D</given-names></string-name>, <string-name name-style=\"western\"><surname>Robertson</surname> &#160;<given-names>KD</given-names></string-name></person-group>. <article-title>Chapter 24&#8212;role of DNA methylation in genome stability</article-title>. In <string-name name-style=\"western\"><given-names>Igor</given-names> &#160;<surname>Kovalchuk</surname></string-name> and <string-name name-style=\"western\"><given-names>Olga</given-names> &#160;<surname>Kovalchuk</surname></string-name>, (eds), <source>Genome Stability</source>, pages <fpage>409</fpage>&#8211;<lpage>24</lpage>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Academic Press</publisher-name>, <year>2016</year>. <pub-id pub-id-type=\"doi\">10.1016/B978-0-12-803309-8.00024-0</pub-id>.</mixed-citation></ref><ref id=\"ref3\"><label>3</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Yingying</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>He</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Jin</surname> &#160;<given-names>J</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2021</year>;<volume>37</volume>:<fpage>4603</fpage>&#8211;<lpage>10</lpage>.<pub-id pub-id-type=\"pmid\">34601568</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/btab677</pub-id></mixed-citation></ref><ref id=\"ref4\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Lv</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Dao</surname> &#160;<given-names>F-Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname> &#160;<given-names>D</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes</article-title>. <source><italic toggle=\"yes\">Iscience</italic></source> &#160;<year>2020</year>;<volume>23</volume>:<fpage>100991</fpage>. <pub-id pub-id-type=\"doi\">10.1016/j.isci.2020.100991</pub-id><pub-id pub-id-type=\"pmid\">32240948</pub-id><pub-id pub-id-type=\"pmcid\">PMC7115099</pub-id></mixed-citation></ref><ref id=\"ref5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Lulu</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Han</surname> &#160;<given-names>S</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Jump-seq: genome-wide capture and amplification of 5-hydroxymethylcytosine sites</article-title>. <source><italic toggle=\"yes\">J Am Chem Soc</italic></source> &#160;<year>2019</year>;<volume>141</volume>:<fpage>8694</fpage>&#8211;<lpage>7</lpage>.<pub-id pub-id-type=\"pmid\">31117646</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1021/jacs.9b02512</pub-id><pub-id pub-id-type=\"pmcid\">PMC7061342</pub-id></mixed-citation></ref><ref id=\"ref6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>O&#8217;Brown</surname> &#160;<given-names>ZK</given-names></string-name>, <string-name name-style=\"western\"><surname>Boulias</surname> &#160;<given-names>K</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname> &#160;<given-names>J</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Sources of artifact in measurements of 6mA and 4mC abundance in eukaryotic genomic DNA</article-title>. <source><italic toggle=\"yes\">BMC Genomics</italic></source> &#160;<year>2019</year>;<volume>20</volume>:<fpage>1</fpage>&#8211;<lpage>15</lpage>. <pub-id pub-id-type=\"doi\">10.1186/s12864-019-5754-6</pub-id><pub-id pub-id-type=\"pmid\">31159718</pub-id><pub-id pub-id-type=\"pmcid\">PMC6547475</pub-id></mixed-citation></ref><ref id=\"ref7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zhao</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname> &#160;<given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname> &#160;<given-names>F</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Accurate prediction of DNA N4-methylcytosine sites via boost-learning various types of sequence features</article-title>. <source><italic toggle=\"yes\">BMC Genomics</italic></source> &#160;<year>2020</year>;<volume>21</volume>:<fpage>1</fpage>&#8211;<lpage>11</lpage>. <pub-id pub-id-type=\"doi\">10.1186/s12864-020-07033-8</pub-id><pub-id pub-id-type=\"pmcid\">PMC7488740</pub-id><pub-id pub-id-type=\"pmid\">32917152</pub-id></mixed-citation></ref><ref id=\"ref8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Thomson</surname> &#160;<given-names>JP</given-names></string-name>, <string-name name-style=\"western\"><surname>Meehan</surname> &#160;<given-names>RR</given-names></string-name></person-group>. <article-title>The application of genome-wide 5-hydroxymethylcytosine studies in cancer research</article-title>. <source><italic toggle=\"yes\">Epigenomics</italic></source> &#160;<year>2017</year>;<volume>9</volume>:<fpage>77</fpage>&#8211;<lpage>91</lpage>. <pub-id pub-id-type=\"doi\">10.2217/epi-2016-0122</pub-id><pub-id pub-id-type=\"pmid\">27936926</pub-id></mixed-citation></ref><ref id=\"ref9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Heyn</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Esteller</surname> &#160;<given-names>M</given-names></string-name></person-group>. <article-title>An adenine code for DNA: a second life for N6-methyladenine</article-title>. <source><italic toggle=\"yes\">Cell</italic></source> &#160;<year>2015</year>;<volume>161</volume>:<fpage>710</fpage>&#8211;<lpage>3</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.cell.2015.04.021</pub-id><pub-id pub-id-type=\"pmid\">25936836</pub-id></mixed-citation></ref><ref id=\"ref10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Ye</surname> &#160;<given-names>F</given-names></string-name>, <string-name name-style=\"western\"><surname>Luo</surname> &#160;<given-names>G-Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname> &#160;<given-names>K</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>N6-methyldeoxyadenosine marks active transcription start sites in chlamydomonas</article-title>. <source><italic toggle=\"yes\">Cell</italic></source> &#160;<year>2015</year>;<volume>161</volume>:<fpage>879</fpage>&#8211;<lpage>92</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.cell.2015.04.010</pub-id><pub-id pub-id-type=\"pmid\">25936837</pub-id><pub-id pub-id-type=\"pmcid\">PMC4427561</pub-id></mixed-citation></ref><ref id=\"ref11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zhuo</surname> &#160;<given-names>L</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname> &#160;<given-names>R</given-names></string-name>, <string-name name-style=\"western\"><surname>Xiangzheng</surname> &#160;<given-names>F</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>StableDNAm: towards a stable and efficient model for predicting DNA methylation based on adaptive feature correction learning</article-title>. <source><italic toggle=\"yes\">BMC Genomics</italic></source> &#160;<year>2023</year>;<volume>24</volume>:<fpage>742</fpage>. <pub-id pub-id-type=\"doi\">10.1186/s12864-023-09802-7</pub-id><pub-id pub-id-type=\"pmid\">38053026</pub-id><pub-id pub-id-type=\"pmcid\">PMC10698904</pub-id></mixed-citation></ref><ref id=\"ref12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Jin</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Yingying</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname> &#160;<given-names>R</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations</article-title>. <source><italic toggle=\"yes\">Genome Biol</italic></source> &#160;<year>2022</year>;<volume>23</volume>:<fpage>1</fpage>&#8211;<lpage>23</lpage>.<pub-id pub-id-type=\"pmid\">36253864</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s13059-022-02780-1</pub-id><pub-id pub-id-type=\"pmcid\">PMC9575223</pub-id></mixed-citation></ref><ref id=\"ref13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Meissner</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Gnirke</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Bell</surname> &#160;<given-names>GW</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Reduced representation bisulfite sequencing for comparative high-resolution DNA methylation analysis</article-title>. <source><italic toggle=\"yes\">Nucleic Acids Res</italic></source> &#160;<year>2005</year>;<volume>33</volume>:<fpage>5868</fpage>&#8211;<lpage>77</lpage>. <pub-id pub-id-type=\"doi\">10.1093/nar/gki901</pub-id><pub-id pub-id-type=\"pmid\">16224102</pub-id><pub-id pub-id-type=\"pmcid\">PMC1258174</pub-id></mixed-citation></ref><ref id=\"ref14\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Lister</surname> &#160;<given-names>R</given-names></string-name>, <string-name name-style=\"western\"><surname>Pelizzola</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Dowen</surname> &#160;<given-names>RH</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Human DNA methylomes at base resolution show widespread epigenomic differences</article-title>. <source><italic toggle=\"yes\">Nature</italic></source> &#160;<year>2009</year>;<volume>462</volume>:<fpage>315</fpage>&#8211;<lpage>22</lpage>. <pub-id pub-id-type=\"doi\">10.1038/nature08514</pub-id><pub-id pub-id-type=\"pmid\">19829295</pub-id><pub-id pub-id-type=\"pmcid\">PMC2857523</pub-id></mixed-citation></ref><ref id=\"ref15\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Tang</surname> &#160;<given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Kang</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Yuan</surname> &#160;<given-names>J</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>DNA4mC-LIP: a linear integration method to identify N4-methylcytosine site in multiple species</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2020</year>;<volume>36</volume>:<fpage>3327</fpage>&#8211;<lpage>35</lpage>. <pub-id pub-id-type=\"doi\">10.1093/bioinformatics/btaa143</pub-id><pub-id pub-id-type=\"pmid\">32108866</pub-id></mixed-citation></ref><ref id=\"ref16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Liu</surname> &#160;<given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname> &#160;<given-names>Y</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>DeepTorrent: a deep learning-based approach for predicting DNA N4-methylcytosine sites</article-title>. <source><italic toggle=\"yes\">Brief Bioinform</italic></source> &#160;<year>2021</year>;<volume>22</volume>:<fpage>bbaa124</fpage>.<pub-id pub-id-type=\"pmid\">32608476</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bib/bbaa124</pub-id><pub-id pub-id-type=\"pmcid\">PMC8599298</pub-id></mixed-citation></ref><ref id=\"ref17\"><label>17</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Ma</surname> &#160;<given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Thela</surname> &#160;<given-names>SR</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhao</surname> &#160;<given-names>F</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Deep5hmC: predicting genome-wide 5-hydroxymethylcytosine landscape via a multimodal deep learning model</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2024</year>;<volume>40</volume>:<fpage>btae528</fpage>.<pub-id pub-id-type=\"pmid\">39196755</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/btae528</pub-id><pub-id pub-id-type=\"pmcid\">PMC11379467</pub-id></mixed-citation></ref><ref id=\"ref18\"><label>18</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Ali</surname> &#160;<given-names>SD</given-names></string-name>, <string-name name-style=\"western\"><surname>Kim</surname> &#160;<given-names>JH</given-names></string-name>, <string-name name-style=\"western\"><surname>Tayara</surname> &#160;<given-names>H</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Prediction of RNA 5-hydroxymethylcytosine modifications using deep learning</article-title>. <source><italic toggle=\"yes\">IEEE Access</italic></source> &#160;<year>2021</year>;<volume>9</volume>:<fpage>8491</fpage>&#8211;<lpage>6</lpage>. <pub-id pub-id-type=\"doi\">10.1109/ACCESS.2021.3049146</pub-id></mixed-citation></ref><ref id=\"ref19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Chen</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Lv</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Nie</surname> &#160;<given-names>F</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>i6mA-Pred: identifying DNA N6-methyladenine sites in the rice genome</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2019</year>;<volume>35</volume>:<fpage>2796</fpage>&#8211;<lpage>800</lpage>. <pub-id pub-id-type=\"doi\">10.1093/bioinformatics/btz015</pub-id><pub-id pub-id-type=\"pmid\">30624619</pub-id></mixed-citation></ref><ref id=\"ref20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Haitao</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Dai</surname> &#160;<given-names>Z</given-names></string-name></person-group>. <article-title>SNNRice6mA: a deep learning method for predicting DNA N6-methyladenine sites in rice genome</article-title>. <source><italic toggle=\"yes\">Front Genet</italic></source> &#160;<year>2019</year>;<volume>10</volume>:<fpage>1071</fpage>.<pub-id pub-id-type=\"pmid\">31681441</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fgene.2019.01071</pub-id><pub-id pub-id-type=\"pmcid\">PMC6797597</pub-id></mixed-citation></ref><ref id=\"ref21\"><label>21</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Basith</surname> &#160;<given-names>S</given-names></string-name>, <string-name name-style=\"western\"><surname>Manavalan</surname> &#160;<given-names>B</given-names></string-name>, <string-name name-style=\"western\"><surname>Shin</surname> &#160;<given-names>TH</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>SDM6A: a web-based integrative machine-learning framework for predicting 6mA sites in the rice genome</article-title>. <source><italic toggle=\"yes\">Mol Therapy-Nucleic Acids</italic></source> &#160;<year>2019</year>;<volume>18</volume>:<fpage>131</fpage>&#8211;<lpage>41</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.omtn.2019.08.011</pub-id><pub-id pub-id-type=\"pmcid\">PMC6796762</pub-id><pub-id pub-id-type=\"pmid\">31542696</pub-id></mixed-citation></ref><ref id=\"ref22\"><label>22</label><mixed-citation publication-type=\"other\">Devlin J, Chang M-W, Lee K. <etal>et&#160;al.</etal> BERT: pre-training of deep bidirectional transformers for language understanding. In: <italic toggle=\"yes\">Proceedings of NAACL-HLT</italic>, pp. 4171&#8211;86, Minneapolis, Minnesota: Association for Computational Linguistics, 2019.</mixed-citation></ref><ref id=\"ref23\"><label>23</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Lee</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Yoon</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Kim</surname> &#160;<given-names>S</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2020</year>;<volume>36</volume>:<fpage>1234</fpage>&#8211;<lpage>40</lpage>. <pub-id pub-id-type=\"doi\">10.1093/bioinformatics/btz682</pub-id><pub-id pub-id-type=\"pmid\">31501885</pub-id><pub-id pub-id-type=\"pmcid\">PMC7703786</pub-id></mixed-citation></ref><ref id=\"ref24\"><label>24</label><mixed-citation publication-type=\"other\">Gururangan S, Marasovi&#263; A, Swayamdipta S. <etal>et&#160;al.</etal> Don&#8217;t stop pretraining: adapt language models to domains and tasks. In: <italic toggle=\"yes\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</italic>, pp. 8342&#8211;60, Online. Association for Computational Linguistics, 2020.</mixed-citation></ref><ref id=\"ref25\"><label>25</label><mixed-citation publication-type=\"journal\">Tai W, Kung HT, Dong X. <etal>et&#160;al.</etal> exBERT: extending pre-trained models with domain-specific vocabulary under constrained training resources. In: <italic toggle=\"yes\">Findings of the Association for Computational Linguistics: EMNLP 2020</italic>; pp. 1433&#8211;9, Online. Association for Computational Linguistics, 2020.</mixed-citation></ref><ref id=\"ref26\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>van der Putten</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>de Groof</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Struyvenberg</surname> &#160;<given-names>M</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Multi-stage domain-specific pretraining for improved detection and localization of barrett&#8217;s neoplasia: a comprehensive clinically validated study</article-title>. <source><italic toggle=\"yes\">Artif Intell Med</italic></source> &#160;<year>2020</year>;<volume>107</volume>:<fpage>101914</fpage>. <pub-id pub-id-type=\"doi\">10.1016/j.artmed.2020.101914</pub-id><pub-id pub-id-type=\"pmid\">32828453</pub-id></mixed-citation></ref><ref id=\"ref27\"><label>27</label><mixed-citation publication-type=\"other\">Koto F, Lau JH, Baldwin T. IndoBERTweet: a pretrained language model for Indonesian Twitter with effective domain-specific vocabulary initialization. In: <italic toggle=\"yes\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</italic>, pp. 10660&#8211;8, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021.</mixed-citation></ref><ref id=\"ref28\"><label>28</label><mixed-citation publication-type=\"other\">Ji S, Zhang T, Ansari L. <etal>et&#160;al.</etal> MentalBERT: publicly available pretrained language models for mental healthcare. In: <italic toggle=\"yes\">Proceedings of the Thirteenth Language Resources and Evaluation Conference</italic>, pp. 7184&#8211;90, Marseille, France: European Language Resources Association, 2022.</mixed-citation></ref><ref id=\"ref29\"><label>29</label><mixed-citation publication-type=\"other\">Zhang S, Xu Y, Usuyama N. <etal>et&#160;al.</etal> BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. Available from: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2303.00915\" ext-link-type=\"uri\">https://arxiv.org/abs/2303.00915</ext-link> (29 November 2025, date last accessed).</mixed-citation></ref><ref id=\"ref30\"><label>30</label><mixed-citation publication-type=\"other\">Ji S, Zhang T, Yang K. <etal>et&#160;al.</etal> Domain-specific continued pretraining of language models for capturing long context in mental health. arXiv preprint arXiv:2304.10447. 2023. Available from: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2304.10447\" ext-link-type=\"uri\">https://arxiv.org/abs/2304.10447</ext-link> (29 November 2025, date last accessed).</mixed-citation></ref><ref id=\"ref31\"><label>31</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Fangzhi</surname> &#160;<given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Lin</surname> &#160;<given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname> &#160;<given-names>J</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>MoCA: incorporating domain pretraining and cross attention for textbook question answering</article-title>. <source><italic toggle=\"yes\">Pattern Recogn</italic></source> &#160;<year>2023</year>;<volume>140</volume>:<fpage>109588</fpage>.</mixed-citation></ref><ref id=\"ref32\"><label>32</label><mixed-citation publication-type=\"book\">Zhai W, Qi H, Zhao Q. <etal>et&#160;al.</etal> Chinese MentalBERT: domain-adaptive pre-training on social media for Chinese mental health text analysis. In: <italic toggle=\"yes\">Findings of the Association for Computational Linguistics: ACL 2024</italic>, pp. 10574&#8211;85, Bangkok, Thailand: Association for Computational Linguistics, 2024.</mixed-citation></ref><ref id=\"ref33\"><label>33</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zeng</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Dou</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Pan</surname> &#160;<given-names>L</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Improving prediction performance of general protein language model by domain-adaptive pretraining on DNA-binding protein</article-title>. <source><italic toggle=\"yes\">Nat Commun</italic></source> &#160;<year>2024</year>;<volume>15</volume>:<fpage>7838</fpage>. <pub-id pub-id-type=\"doi\">10.1038/s41467-024-52293-7</pub-id><pub-id pub-id-type=\"pmid\">39244557</pub-id><pub-id pub-id-type=\"pmcid\">PMC11380688</pub-id></mixed-citation></ref><ref id=\"ref34\"><label>34</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Yang</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Shao</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Matsuda</surname> &#160;<given-names>Y</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>iResNetDM: an interpretable deep learning approach for four types of DNA methylation modification prediction</article-title>. <source><italic toggle=\"yes\">Comput Struct Biotechnol J</italic></source> &#160;<year>2024</year>;<volume>23</volume>:<fpage>4214</fpage>&#8211;<lpage>21</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.csbj.2024.11.006</pub-id><pub-id pub-id-type=\"pmid\">39650332</pub-id><pub-id pub-id-type=\"pmcid\">PMC11621598</pub-id></mixed-citation></ref><ref id=\"ref35\"><label>35</label><mixed-citation publication-type=\"other\">Huang X, Liu Q, Zhao Y. <etal>et&#160;al.</etal> MethylProphet: a generalized gene-contextual model for inferring whole-genome DNA methylation landscape. <italic toggle=\"yes\">bioRxiv</italic>, 2025. Available from: <pub-id pub-id-type=\"doi\">10.1101/2025.02.05.636730</pub-id> [29 November 2025, date last accessed).</mixed-citation></ref><ref id=\"ref36\"><label>36</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Zhou</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhuo</surname> &#160;<given-names>L</given-names></string-name>, <string-name name-style=\"western\"><surname>Xiangzheng</surname> &#160;<given-names>F</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>Joint masking and self-supervised strategies for inferring small molecule-miRNA associations</article-title>. <source><italic toggle=\"yes\">Mol Therapy Nucleic Acids</italic></source> &#160;<year>2024</year>;<volume>35</volume>:<fpage>102103</fpage>. <pub-id pub-id-type=\"doi\">10.1016/j.omtn.2023.102103</pub-id><pub-id pub-id-type=\"pmcid\">PMC10794920</pub-id><pub-id pub-id-type=\"pmid\">38261851</pub-id></mixed-citation></ref><ref id=\"ref37\"><label>37</label><mixed-citation publication-type=\"book\">Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation. In: <italic toggle=\"yes\">Proceedings of the 32nd International Conference on Machine Learning</italic>, pp. 1180&#8211;9, Lille, France: International Machine Learning Society, 2015.</mixed-citation></ref><ref id=\"ref38\"><label>38</label><mixed-citation publication-type=\"journal\">Vaswani A, Shazeer N, Parmar N. et&#160;al. Attention is all you need. In: <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>, pp. 6000&#8211;10, Long Beach, California, USA: Curran Associates, Inc., 2017.</mixed-citation></ref><ref id=\"ref39\"><label>39</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Ji</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhou</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname> &#160;<given-names>H</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source><italic toggle=\"yes\">Bioinformatics</italic></source> &#160;<year>2021</year>;<volume>37</volume>:<fpage>2112</fpage>&#8211;<lpage>20</lpage>. <pub-id pub-id-type=\"doi\">10.1093/bioinformatics/btab083</pub-id><pub-id pub-id-type=\"pmid\">33538820</pub-id><pub-id pub-id-type=\"pmcid\">PMC11025658</pub-id></mixed-citation></ref><ref id=\"ref40\"><label>40</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><string-name name-style=\"western\"><surname>Huang</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Mao</surname> &#160;<given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Deng</surname> &#160;<given-names>J</given-names></string-name>. <etal>et&#160;al.</etal></person-group> &#160;<article-title>EMGANet: edge-aware multi-scale group-mix attention network for breast cancer ultrasound image segmentation</article-title>. <source><italic toggle=\"yes\">IEEE J Biomed Health Inform</italic></source> &#160;<year>2025</year>;<volume>29</volume>:<fpage>5631</fpage>&#8211;<lpage>41</lpage>. <pub-id pub-id-type=\"doi\">10.1109/JBHI.2025.3546345</pub-id><pub-id pub-id-type=\"pmid\">40031552</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Brief Bioinform Brief Bioinform 721 bib bib Briefings in Bioinformatics 1467-5463 1477-4054 Oxford University Press PMC12684731 PMC12684731.1 12684731 12684731 41359543 10.1093/bib/bbaf642 bbaf642 1 Problem Solving Protocol AcademicSubjects/SCI01060 iDNA-DAPHA: a generic framework for methylation prediction via domain-adaptive pretraining and hierarchical attention Wang Wenjun School of Software Engineering , South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, China School of Data Science and Information Engineering , Guizhou Minzu University, Huaxi University Town, Huaxi District, Guiyang, Guizhou 550025, China Tan Wen School of Software Engineering , South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, China Lai Lvlong School of Computer Science , Guangdong Polytechnic Normal University, No. 293, West Zhongshan Avenue, Tianhe District, Guangzhou, Guangdong 510665, China Lu Youjun School of Data Science and Information Engineering , Guizhou Minzu University, Huaxi University Town, Huaxi District, Guiyang, Guizhou 550025, China Wu Qingyao School of Software Engineering , South China University of Technology, Guangzhou Higher Education Mega Centre, Panyu District, Guangzhou, Guangdong 510006, China Peng Cheng Laboratory , No. 6001 Shahexi Road, Nanshan District, Shenzhen, Guangdong 518055, China Corresponding author. E-mail: qyw@scut.edu.cn 11 2025 08 12 2025 26 6 499914 bbaf642 01 6 2025 13 10 2025 05 11 2025 08 12 2025 09 12 2025 09 12 2025 &#169; The Author(s) 2025. Published by Oxford University Press. 2025 https://creativecommons.org/licenses/by-nc/4.0/ This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License ( https://creativecommons.org/licenses/by-nc/4.0/ ), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com Abstract Accurately identifying DNA methylation is essential for understanding complex regulatory networks and disease mechanisms. However, the dynamic nature of methylation and species differences make prediction challenging. Existing deep learning methods often overlook the potential of shared features across diverse species&#8217; methylation sequences and rely solely on token-to-token attention when modelling long-range dependencies, limiting the model&#8217;s representation capabilities. To address these limitations, we propose iDNA-DAPHA, an accurate and generic two-stage deep learning framework that leverages domain-adaptive pretraining (DAP) incorporating feature alignment to learn common features across various types of methylation sequences from multiple species, followed by fine-tuning to capture task-specific features. The framework further introduces hierarchical attention (HA) to enhance its representational power. Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods across seventeen benchmark datasets covering three representative DNA methylation types. Ablation studies validate the effectiveness and contributions of DAP and HA. Furthermore, visualization-based analyses reveal that the model can capture conserved sequence patterns and learn discriminative representations. We believe that iDNA-DAPHA will serve as a valuable framework for methylation prediction, especially in scenarios with limited training samples for specific methylation types in certain species. DNA methylation deep learning domain-adaptive pretraining hierarchical attention National Natural Science Foundation of China 10.13039/501100001809 62272172 62266012 Zhuhai Science and Technology Plan Project 2320004002758 Fundamental Research Funds for the Central Universities 10.13039/501100012226 2025ZYGXZR095 High-Level Innovative Talent Project of Guizhou Province QKHPTRC-GCC2023027 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Introduction DNA methylation, an epigenetic mechanism influencing gene expression, plays an important role in various medical disorders [ 1 , 2 ]. Aberrant changes in DNA methylation are a significant factor in the onset and progression of diseases, including cancer [ 3 ]. Currently, three representative DNA methylation types&#8212;N4-Methylcytosine (4mC), 5-Hydroxymethylcytosine (5hmC), and N6-Methyladenine (6mA)&#8212;have been identified across species [ 4&#8211;6 ]. These types are associated with different functional mechanisms. 4mC, involved in the restriction&#8211;modification system, corrects DNA replication errors and controls DNA replication and cell cycle [ 7 ]. 5hmC, linked to neural system development and tumorigenesis, aids in identifying novel therapeutic targets and strategies for cancer treatment [ 8 ]. 6mA, a epigenetic mark in prokaryotes and eukaryotes [ 9 ], regulates various cellular processes, including gene transcription, chromosome replication, and cell defense [ 10 ]. Therefore, identifying the above DNA methylations is crucial for analysing and revealing their functional mechanisms. As environmental degradation increasingly affects gene regulation and expression across ecosystems, identifying DNA methylation has also become an important and pressing task for understanding biological development and preventing diseases [ 11 ]. Environmental factors, ageing, and other influences cause dynamic changes in DNA methylation, posing challenges for accurate prediction [ 12 ]. Traditional wet-lab techniques, such as reduced-representation bisulfite sequencing [ 13 ] and whole-genome bisulfite sequencing [ 14 ], are costly and time-consuming. Besides this, bisulfite sequencing using short-read techniques has drawbacks such as low positional efficiency and uneven genome coverage, leading to suboptimal sequencing quality. Thus, computational prediction of DNA methylation serves as a good choice. This approach not only minimizes costs but also furnishes valuable guidance for experimental studies on epigenetic modifications [ 4 ]. In the past few years, machine learning and deep learning approaches have proven effective in predicting DNA methylation [ 3 ]. Methods like DNA4mC-LIP [ 15 ] and DeepTorrent [ 16 ] target 4mC methylation, while Deep5hmC [ 17 ] and iRhm5CNN [ 18 ] focus on 5hmC methylation, and i6mA-Pred [ 19 ], SNNRice6mA [ 20 ], and SDM6A [ 21 ] address 6mA methylation. However, these models are often specific to particular modifications or species, limiting generalization. Hence, there is an urgent need for a universal approach supporting prediction of 4mC, 5hmC, and 6mA methylation across species. iDNA-MS [ 4 ] is the first machine learning generic predictor but relies on manually designed features, requiring extensive prior knowledge, and resulting in limited adaptability across different methylation prediction tasks. To tackle this issue, deep learning-based generic predictors like iDNA-ABT [ 3 ], iDNA-ABF [ 12 ], MuLan-Methyl [ 1 ], and StableDNAm [ 11 ] have been proposed. They utilize BERT [ 22 ] and its variants to learn distinguishable features for identifying methylation. Each of these methods follows the pretraining&#8211;fine-tuning paradigm, as depicted in Fig. 1A , adapting to the specific task and achieving improved performance. Figure 1 Comparison of (A) previous deep learning-based methods for methylation prediction and (B) our proposed approach. Alt text: A two-part flowchart comparing previous deep learning-based methylation prediction methods (a) with our proposed method (b). Panel (a) shows DNABERT general-purpose pretraining followed by task-specific fine-tuning, using only token-to-token attention. Panel (b) introduces a methylation DAP stage using multiple methylation datasets after general-purpose pretraining. This new stage enhances the model with HA and feature alignment before proceeding to task-specific fine-tuning. Since these generic predictors, built upon BERT and its variants, undergo general-purpose pretraining on large amounts of unlabelled sequences, they are capable of obtaining comprehensive contextual representations, thereby improving overall model performance. However, these approaches often overlook the potential of shared characteristics across diverse species&#8217; methylation data. It is important to understand common features and evolutionary trends in DNA methylation. Models can benefit from these shared features, improving their ability to generalize DNA methylation patterns across different species. With this in mind, our objective is to guide the model in learning general features across various DNA methylation tasks, enabling it to implicitly capture cross-species commonalities and provide informative feature representations that facilitate downstream fine-tuning for accurate methylation identification. We adopt a domain-adaptive pretraining (DAP) approach that leverages data from multiple methylation tasks, shares a unified feature extractor, and incorporates a methylation task classification loss. To encourage the model to capture common features across tasks, we perform feature alignment using a gradient reversal layer during the backpropagation phase. The layer keeps the input unchanged during forward propagation but reverses the gradient by multiplying it with a negative scalar during backpropagation. This adversarial process forces the model&#8217;s feature extraction module to maximize the task classification loss, making the feature distributions of different tasks indistinguishable and facilitating the learning of shared features relevant to methylation. Based on this, we propose a generic two-stage methylation prediction framework termed iDNA-DAPHA, as illustrated in Fig. 1B . We first initialize the model with general-purpose pretraining weights. Then, in the first stage, we employ DAP incorporating feature alignment to learn common features among various types of methylation sequences from multiple species. In the second stage, we conduct fine-tuning to capture discriminative features within the specific methylation prediction task (designated methylation types in specified species). In addition, existing sequence-based generic predictors only consider token-to-token attention at a single granularity when modelling long-range dependencies. They overlook group-to-group attention, making it difficult to understand broader inter-group correlations and obtain higher-level feature representation power. Therefore, to enhance representational capabilities, our iDNA-DAPHA network introduces a hierarchical attention (HA)mechanism that combines both token-to-token and group-to-group attention, enabling the model to capture both fine-grained inter-token and broader inter-group correlations. Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods across 17 benchmark datasets, with particularly significant improvements in scenarios involving limited training samples for specific methylation types in certain species. We further conduct ablation studies to validate the effectiveness and contributions of DAP and HA. Moreover, visualization-based interpretability analyses reveal that the model can capture conserved sequence patterns and learn more discriminative representations for methylation prediction. Related work Domain-adaptive pretraining DAP has emerged as a crucial technique to enhance the performance of pretrained models in specialized domains. This approach involves fine-tuning pretrained models on domain data to adapt them to the target domain&#8217;s tasks. Recent studies have emphasized the importance of adapting pretrained models to diverse domains to unlock their full potential across various applications. Lee et&#160;al. [ 23 ] proposed BioBERT, the first domain-specific BERT-based model pretrained on biomedical corpora. Gururangan et&#160;al. [ 24 ] explored the benefits of continued pretraining language models and found that DAP leads to large gains in task performance. Tai et&#160;al. [ 25 ] pretrained exBERT with biomedical articles and domain-specific vocabulary to extend the pretrained BERT from a generic domain to a specific domain. Van der Putten et&#160;al. [ 26 ] used several Barrett-specific datasets closer to the target domain in a multi-stage transfer learning strategy for improved detection and localization of Barrett&#8217;s neoplasia. Koto et&#160;al. [ 27 ] present INDOBERTWEET, the first large-scale pretrained model for Indonesian Twitter, domain-adaptively trained through the expansion of a monolingually trained Indonesian BERT with additional domain-specific vocabulary. Ji et&#160;al. [ 28 ] developed two pretrained language models, i.e. MentalBERT and MentalRoBERTa, and demonstrated that language representations pretrained in the specific downstream domain enhance the performance of mental health detection tasks. Zhang et&#160;al. [ 29 ] studied domain-specific pretraining on biomedical image-text data and proposed BiomedCLIP for biomedical vision-language processing. Ji et&#160;al. [ 30 ] conducted domain-specific continued pretraining and released MentalXLNet and MentalLongformer for effectively modelling long sequences in the field of mental health. Xu et&#160;al. [ 31 ] introduce multi-stage domain pretraining for the Textbook Question Answering task, conducting unsupervised post-pretraining with the span mask strategy and supervised pre-fine-tuning to overcome the domain gaps of general language model. Zhai et&#160;al. [ 32 ] develop a psychology-adapted language model through DAP on social media texts with psychological lexicon integration, enhancing performance in psychological text analysis. Zeng et&#160;al. [ 33 ] propose ESM-DBP, a domain-adaptive protein language model trained on 170 264 DNA-binding protein sequences, which improves feature representation and prediction performance for DNA&#8211;protein interactions. Although the above works on DAP have been successful, there has been no work exploring adaptive pretraining in the methylation domain. Therefore, this paper proposes a DAP method using feature alignment to improve the performance of methylation prediction tasks. Computational approaches for DNA methylation prediction In recent years, machine learning and deep learning have made significant strides in predicting DNA methylation. For 4mC methylation identification, methods such as the ensemble learning-based DNA4mC-LIP [ 15 ] and the deep learning-based DeepTorrent [ 16 ] have been proposed. For 5hmC methylation detection, models such as Deep5hmC [ 17 ] adopt a multimodal design, while iRhm5CNN [ 18 ] utilizes a convolutional neural network. For 6mA methylation prediction, various methods have been developed, including i6mA-Pred [ 19 ], which employs a support vector machine; SNNRice6mA [ 20 ], which builds a lightweight CNN; and SDM6A [ 21 ], which explores different feature encodings and integrates multiple single models. These predictors have propelled the identification of DNA methylation in various contexts, but they focus on a specific type of modification, with some even limited to one specific species. They are difficult to generalize to other methylation types and species. Therefore, there is an urgent need to develop a universal method that supports the prediction of three representative types of methylation, 4mC, 5hmC, and 6mA, applicable to different species. iDNA-MS [ 4 ] is the first machine learning generic predictor for detecting various methylations of different species. However, this method uses manual features, limiting adaptability. To address this, several deep learning-based generic predictors of different methylations across different species have been proposed. iDNA-ABT [ 3 ] employs BERT to adaptively learn distinguishable features for identifying DNA methylation in multiple species. iDNA-ABF [ 12 ] uses the DNABERT model pretrained with large-scale genomic sequences and introduces a dual-scale processing strategy to effectively capture discernible methylation information across various scales. MuLan-Methyl [ 1 ] integrates five kinds of widely used BERT models to collectively predict the DNA methylation status. StableDNAm [ 11 ] utilizes the pretraining model DNABERT and develops a feature correction module to enhance the model&#8217;s stability. Each of these models is adapted to the specific task through the pretraining&#8211;fine-tuning paradigm. These deep learning methods based on BERT and its variants achieved better performance over previous approaches. However, these predictors tend to overlook the potential of shared characteristics relevant to methylation prediction across species. Meanwhile, the field is evolving toward increasingly diverse methodological directions targeting distinct biological objectives. For instance, iResNetDM [ 34 ], which integrates Residual Networks (ResNet) with self-attention mechanisms, distinguishes among four types of DNA methylation modifications and captures relationships between different types of modifications. MethylProphet [ 35 ], on the other hand, developed a novel encoding scheme that integrates gene expression profiles with DNA sequence context to infer the whole-genome DNA methylation landscape. Methodologically, techniques from related bioinformatics fields also offer valuable insights, such as the joint masking and self-supervised strategies employed by JMSS-MMA [ 36 ] to reduce data noise for inferring small molecule-miRNA associations. Returning to the challenge of generic methylation prediction across different species and types, there remains a need for a systematic framework that captures the shared representations of DNA methylation to enhance generalization across diverse species. Moreover, these BERT-based universal predictors only use single-granularity token-to-token attention, neglecting group-to-group attention. This limits their ability to understand broader inter-group correlations and obtain higher-level feature representation capabilities. Based on the above considerations, this paper proposes a generic framework, iDNA-DAPHA, for methylation prediction. The framework leverages DAP to learn common features relevant to methylation and incorporates HA, combining token-to-token and group-to-group attention, to further enhance representation capabilities. Materials and methods Benchmark dataset collection In this study, we utilized the same benchmark dataset collection as employed by previous generic predictors, such as iDNA-ABT [ 3 ], MuLan-Methyl [ 1 ], and StableDNAm [ 11 ], etc., facilitating fair comparisons. This benchmark collection provides a comprehensive set of DNA methylation data across various species, encompassing three representative DNA methylation types: 4mC, 5hmC, and 6mA, and covering a total of 17 distinct datasets. Among these datasets, the 4 4mC datasets encompass sequences from C.equisetifolia, F.vesca, S.cerevisiae, and Ts.SUP5-1. The 2 5hmC datasets include sequences from H.sapiens and M.musculus. The 11 6mA datasets comprise sequences from A.thaliana, C.elegans, C.equisetifolia, D.melanogaster, F.vesca, H.sapiens, R.chinensis, S.cerevisiae, Ts.SUP5-1, T.thermophile and Xoc.BLS256. To ensure dataset quality, stringent selection criteria are used. Positive samples were selected based on experimental verification, with sequences centred on methylation sites and stringent quality thresholds applied. Negative samples were carefully curated to include sequences not proven to be methylated by experiments, balanced with positive samples to mitigate the impact of skewed class distributions on model learning. Moreover, to avoid redundancy and homology bias, sequences exhibiting &gt;80% sequence similarity were excluded using the CD-HIT program. Each dataset in the benchmark collection is randomly divided into training and testing sets by a ratio of 1:1, ensuring independence between the two subsets. The statistics of training and testing sets for all 17 datasets are listed in Supplementary Table S1 . The framework of the proposed iDNA-DAPHA Our method employs a two-stage framework for methylation prediction. In the first stage, we implement DAP to capture common features among different types of methylation sequences from multiple species. The workflow is illustrated in Fig. 2 . We use the dual-stream network as the shared feature extraction module (B), processing input sequences from 17 distinct methylation datasets (A). Each sequence is tokenized using both 3-mer and 6-mer encoding and is fed separately into the subnetworks (B-comp) of the upstream and downstream branches, following the dual-scale sequence processing strategy [ 12 ]. The features extracted from both branches are then concatenated. The fused results are passed to a feature alignment module (C) and a methylation prediction module (D). The feature alignment module, comprising a gradient reversal layer and fully connected layers, aligns features across diverse methylation prediction tasks by reversing the gradient direction under the task classifier&#8217;s discrimination during training. In parallel, the methylation prediction module, composed of fully connected layers, predicts the methylation status of each input sequence. Additionally, the two subnetworks within the dual-stream network incorporate an HA mechanism to capture both fine-grained inter-token and broader inter-group correlations, enhancing feature representation. Figure 2 DAP for methylation prediction, comprising four parts: (A) DAP data, (B) dual-stream feature extraction with HA (B-comp), (C) feature alignment via gradient reversal under a task classifier, and (D) methylation prediction. Alt text: A diagram illustrating the architecture for DAP in methylation prediction, consisting of four parts. Part A (DAP data): shows input DNA sequences from multiple species and methylation types. Part B (feature extraction): a dual-stream network processes 3-mer and 6-mer tokens separately using B-comp modules with HA. The features from both streams are then combined. Part C (feature alignment): the combined features are aligned across tasks using a gradient reversal layer and a task classifier to extract shared features. Part D (methylation prediction): a methylation classifier predicts methylation states from the learnt representations. Insets show the B-comp mechanism (token-to-token and group-to-group attention), the gradient reversal, and the classifier structure. In the second stage, we perform task-specific fine-tuning to extract and refine discriminative features tailored to the target methylation prediction task (i.e. a designated methylation type in a specified species). The workflow in this stage remains similar to the first stage, with the primary difference being the exclusion of the feature alignment module (C). Furthermore, fine-tuning utilizes only the target task dataset, unlike the DAP stage, which leverages multiple distinct methylation datasets. Methylation domain-adaptive pretraining Due to the significant performance gains observed in many NLP tasks through the pretraining&#8211;fine-tuning paradigm, various methods for methylation prediction tasks have also adopted this strategy [ 1 , 3 , 11 , 12 ]. They utilize BERT pretrained on the general-domain corpus or DNABERT pretrained on full human genome sequences, which are then directly fine-tuned for target methylation prediction task. However, these approaches overlook the potential of shared features across diverse species&#8217; methylation sequences, which is crucial for effective representation learning in methylation sequence classification. Moreover, this generic pretraining strategy typically requires a large amount of target task data for fine-tuning to achieve satisfactory performance. For that downstream task&#8217;s methylation dataset with few samples, such as 4mC_C.equisetifolia and 6mA_R.chinensis, fine-tuning alone often yields suboptimal results. Therefore, in this paper, we introduce a DAP method to address these limitations. Domain-adaptive pretraining data Since DAP typically requires large amounts of domain data, we merged the training sets from 17 different methylation datasets in the benchmark collection, as shown in Fig. 2 A, each of which corresponds to a methylation prediction task involving a specific species and methylation type. These data cover three representative types of DNA methylation sites (4mC, 5hmC, and 6mA) across 12 genomes, including both eukaryotes and prokaryotes. In total, there are 125 488 methylated sequences and an equal number of unmethylated sequences, with sequences from the same dataset assigned the same task label during pretraining. These DAP data contain extensive sequence information regarding various types of methylation sites while also considering the diversity of genomic sources. It facilitates the pretraining of language models tailored to methylation prediction tasks, enhancing their performance and generalization in this domain. Feature extraction module Since the genome sequence is regarded as the language for transmitting genetic information within and between cells, many bioinformatic researchers have utilized word embeddings to represent biological sequences. They treat DNA sequences as &#8216;sentences&#8217;, using the k-mers technique to consider consecutive k bases as &#8216;words&#8217;. In this paper, we build on the insights from Jin et&#160;al. [ 12 ], using 3-mers and 6-mers to represent DNA sequences and employing a dual-stream architecture shared across all datasets to process these two scales of tokenized sequences for feature extraction, as shown in Fig. 2 B. Specifically, for a sequence \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $S$\\end{document} of length n , its k-mers tokenized representation is denoted as (1) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*}&amp; S_{k-mers} = [s_{1}, s_{2},..., s_{n-k+1}],\\end{align*}\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $s_{i}$\\end{document} represents consecutive k bases starting from the i th base. We feed the 3-mers represented sequence into the upstream subnetwork and the 6-mers represented sequence into the downstream subnetwork. The features extracted from both subnetworks are then concatenated for fusion. This process of feature extraction can be represented as (2) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*}&amp; F(S) = [B_{up}(S_{3-mers}), B_{down}(S_{6-mers})],\\end{align*}\\end{document} where [ \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\cdot $\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\cdot $\\end{document} ] denotes the concatenation operation, both \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $B_{up}$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $B_{down}$\\end{document} are DNABERT models incorporating HA, initialized with pretrained DNABERT on full human genome data tokenized with 3-mers and 6-mers, respectively. Feature alignment module In various methylation prediction tasks, there often exist latent general features that can facilitate accurate methylation identification. To extract such shared features, we perform feature alignment ( Fig. 2 C) to make the feature distributions across different tasks as similar as possible. Specifically, we leverage the loss of a methylation task classifier to indirectly estimate the distributional discrepancies among tasks, under the assumption that this classifier can be trained to optimally distinguish between features from different tasks. By maximizing the loss of this classifier, we encourage the underlying feature representations to become indistinguishable across tasks, thus promoting the learning of common features. During training, we assume that the input sequence \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $x$\\end{document} is first mapped to a feature vector \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathbf{f}$\\end{document} by the feature extraction module \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $M_{f}$\\end{document} . We represent the parameters of this module as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\theta _{f}$\\end{document} , such that \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathbf{f}=M_{f}(x;\\theta _{f})$\\end{document} . The resulting feature vector \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathbf{f}$\\end{document} is then passed to the methylation task classifier \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $M_{t}$\\end{document} to predict the task label \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $t$\\end{document} , with the classifier&#8217;s parameters denoted as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\theta _{t}$\\end{document} , and the corresponding task classification loss is denoted as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathcal{L}_{fa}$\\end{document} . General features across different methylation tasks are learnt by updating these parameters as follows: (3) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; \\theta_{f}\\quad\\longleftarrow\\quad\\theta_{f}-\\mu\\left(-\\lambda\\frac{\\partial \\mathcal{L}_{fa}^{i}}{\\partial\\theta_{f}}\\right), \\end{align*}\\end{document} (4) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; \\theta_{t}\\quad\\longleftarrow\\quad\\theta_{t}-\\mu\\frac{\\partial \\mathcal{L}_{fa}^{i}}{\\partial\\theta_{t}}, \\end{align*}\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mu $\\end{document} is the learning rate, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\lambda $\\end{document} is a scaling factor that determines the relative influence of the task classification loss on the feature extractor. Updates ( 3 ) and ( 4 ) closely resemble stochastic gradient descent updates, with the key distinction being the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $-\\lambda $\\end{document} factor in ( 3 ). Specifically, the negative factor \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $-\\lambda $\\end{document} reverses the gradient direction during backpropagation, thereby introducing an adversarial interaction between the feature extractor and the task classifier. While the task classifier minimizes its loss to better distinguish features from different tasks, the feature extractor&#8212;receiving the reversed gradient multiplied by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $-\\lambda $\\end{document} &#8212;updates its parameters in the opposite direction, effectively maximizing the task classification loss. This adversarial process drives the feature extractor to learn shared representations that generalize better across methylation tasks. Without this reversal, the extractor would instead cooperate with the task classifier and capture task-specific features, reducing the model&#8217;s generalization ability. Thus, we introduce a gradient reversal layer before the methylation task classifier, allowing the feature extraction module to learn common features among different methylation tasks. The gradient reversal layer contains no parameters, except for the meta-parameter \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\lambda $\\end{document} , which is not updated via backpropagation [ 37 ]. During forward propagation, this layer acts as an identity transformation, passing the features through unchanged. During backward propagation, it takes gradients from subsequent layers, multiplies them by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $-\\lambda $\\end{document} , and passes them to the previous layer. This effectively reverses the optimization direction of the task classifier&#8217;s loss with respect to the feature extraction module. Formally, its forward and backward propagation behaviours are defined by the following two equations: (5) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; G_\\lambda(\\mathbf{x})=\\mathbf{x}, \\end{align*}\\end{document} (6) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; \\frac{dG_\\lambda}{d\\mathbf{x}}=-\\lambda\\mathbf{E}, \\end{align*}\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathbf{E}$\\end{document} is an identity matrix. Through the gradient reversal layer, we can optimize the following loss function for methylation task classification while encouraging the feature extraction module to learn shared features across tasks: (7) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*}&amp; \\mathcal{L}_{fa} =-\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C}t_{i,c}\\ln (\\hat{t}_{i,c}).\\end{align*}\\end{document} Here, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $t_{i,c}$\\end{document} denotes the ground-truth task label of sample \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $i$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\hat{t}_{i,c}$\\end{document} represents the predicted probability that sample \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $i$\\end{document} belongs to task \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $c$\\end{document} , where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $N$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $C$\\end{document} denote the batch size and the number of tasks, respectively. Equation ( 7 ) is thus the cross-entropy loss for task discrimination. When combined with the gradient reversal layer, the feature extractor is adversarially trained to confuse the task classifier, thereby learning shared representations across different methylation tasks. Methylation prediction module As shown in Fig. 2 D, we employ a linear layer followed by a methylation classifier to guide the model adapt for distinguishing whether sequences undergo methylation. To achieve this, we utilize the following cross-entropy loss function: (8) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*}&amp; \\mathcal{L}_{mp}=-\\frac{1}{N}\\sum_{i=1}^{N}y_{i}\\ln \\hat{y}_{i}.\\end{align*}\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $y_{i}$\\end{document} represents the one-hot label vector, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\hat{y}_{i}$\\end{document} denotes the predicted probability distribution for sample \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $i$\\end{document} . By minimizing this loss function, the model learns better feature representations, allowing it to map the inputs to the correct output labels effectively. Finally, the entire domain pretraining framework is optimized by jointly minimizing the task classification loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathcal{L}_{fa}$\\end{document} and the methylation prediction loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathcal{L}_{mp}$\\end{document} in an end-to-end manner: (9) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*}&amp; \\mathcal{L}_{total}=\\mathcal{L}_{fa}+\\mathcal{L}_{mp}.\\end{align*}\\end{document} Hierarchical attention DNABERT utilizes the original Query-Key-Value ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} ) computation [ 38 ], which has shown effectiveness in processing sequential information [ 39 ]. However, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} self-attention has a limitation where it only captures the correlation between individual tokens, overlooking interrelations among token groups (i.e. neighborhoods) [ 40 ]. The limitation has been inadvertently overlooked because the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} computation seems sufficient for mapping input to output, with each output entry attending to every input entry. In this paper, we introduce an advanced HA mechanism, as shown in Fig. 2 (B-comp), by incorporating group-to-group attention to overcome the limitation of traditional \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} self-attention. In our framework, &#8216;groups&#8217; are dynamic, higher-level units obtained by applying a learnable aggregation over local neighborhoods of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} vectors. They encode the collective characteristics of token neighborhoods by aggregating their corresponding \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} representations. Inter-group relationships denote the contextual dependencies among these aggregated, group-level representations. The group-to-group attention then applies self-attention over these dynamically formed groups to capture correlations among them. This two-level attention mechanism, consisting of token-level attention and group-level attention, enables the model to not only capture fine-grained correlations between individual tokens but also model broader correlations between groups within each transformer encoder layer, thereby enhancing the overall representational capability. In the existing self-attention mechanism of DNABERT, each input token is mapped to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} via three learnable linear projections. The correlation between every pair of tokens is computed using \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\frac{QK^{T}}{\\sqrt{d}}$\\end{document} (where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $d$\\end{document} is the dimension of the query and key), and applying the softmax function to these correlations generates the attention map A. The multiplication \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $A \\cdot V$\\end{document} linearly recombines tokens based on the attention weights at each position. However, this approach has the limitation of considering only correlations among individual tokens (i.e. individual patterns), overlooking the correlations among token groups (i.e. group patterns). To address this limitation, we extend the original self-attention mechanism by integrating both individual and group patterns within each encoder layer of DNABERT. We dynamically aggregate local neighborhoods of tokens into groups through a learnable operation, and model the contextual dependencies among these formed groups. This is achieved by generating group proxies in Query, Key, and Value and executing the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} computation using these proxies. Specifically, we use \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $X$\\end{document} to represent \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K$\\end{document} , or \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V$\\end{document} , and generate group proxies \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q^{\\prime}$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K^{\\prime}$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V^{\\prime}$\\end{document} through an aggregation operation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\text{Agg}(X)$\\end{document} . This operation employs a sliding-window strategy and is consistently implemented using depthwise convolution with a fixed grouping size (kernel size k = 3), chosen to effectively capture local contextual dependencies. The group attention calculation is then performed on these group proxies ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Q^{\\prime}$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $K^{\\prime}$\\end{document} - \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $V^{\\prime}$\\end{document} ), following the same procedure as self-attention, to generate the output. Finally, the outputs from both the traditional self-attention and the group attention branches are fused through linear mapping layers, followed by normalization and activation. This design constitutes an HA mechanism that captures both fine-grained token-level interactions and coarse-grained group-level dependencies. The process described above can be expressed as follows: (10) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; Attn(Q,K,V)=softmax\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right)V, \\end{align*}\\end{document} (11) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; \\begin{aligned} Attn(Q^{\\prime},K^{\\prime},V^{\\prime}) &amp;= Attn(Agg(Q),Agg(K),Agg(V)) \\\\ &amp;= softmax\\left(\\frac{Agg(Q)Agg(K)^{T}}{\\sqrt{d_{k}}}\\right)Agg(V), \\end{aligned} \\end{align*}\\end{document} (12) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; \\mathit{HA}\\text{-}Attn(Q,K,V)=F_{\\mathit{linear}}[Attn(Q,K,V),Attn(Q^{\\prime},K^{\\prime},V^{\\prime})], \\end{align*}\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Attn(\\cdot ,\\cdot ,\\cdot )$\\end{document} denotes the original self-attention output calculated from the Q, K, and V matrices, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Agg(\\cdot )$\\end{document} indicates the aggregation operation utilizing depth-wise convolution. The operator \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $[\\cdot ,\\cdot ]$\\end{document} signifies concatenation. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathit{HA}\\text{-}Attn(\\cdot ,\\cdot ,\\cdot )$\\end{document} represents HA, which fuses traditional self-attention and group attention. In our framework, we replace the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $Attn$\\end{document} with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\mathit{HA}\\text{-}Attn$\\end{document} to capture the correlations among not only individual tokens but also group proxies, enhancing the model&#8217;s representational capabilities. Performance metrics To ensure fair comparisons with existing methods, we adopt widely recognized performance evaluation metrics, including accuracy (ACC), Matthews&#8217; correlation coefficient (MCC), sensitivity (SN), specificity (SP), and area under the receiver operating characteristic curve (AUC). These metrics are defined as follows: (13) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; ACC = \\frac{TP+TN}{TP+FN+TN+FP}, \\end{align*}\\end{document} (14) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt[]{(TP + FP) (TP + FN) (TN + FP) (TN + FN)}}, \\end{align*}\\end{document} (15) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; SN = \\frac{TP}{TP + FN}, \\end{align*}\\end{document} (16) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; SP = \\frac{TN}{TN + FP}, \\end{align*}\\end{document} (17) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} \\begin{align*} &amp; AUC=\\frac{\\sum_{{\\text{i}\\in \\text{pos}}}{\\text{rank}_{\\text{i}}}-\\frac{{\\text{n}_{\\text{pos}}}({\\text{n}_{\\text{pos}}}+1)}{2}}{{\\text{n}_{\\text{pos}}\\text{n}_{\\text{neg}}}}, \\end{align*}\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $TP$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $TN$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $FN$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $FP$\\end{document} denote the numbers of true positives, true negatives, false negatives, and false positives, respectively. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $rank_{i}$\\end{document} stands for the rank of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $i$\\end{document} th positive sample when all predicted scores for the positive class are sorted in ascending order. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $n_{pos}$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $n_{neg}$\\end{document} are the total numbers of positive and negative samples, respectively; ACC, MCC, and AUC assess the predictor&#8217;s overall performance from different perspectives, where higher values indicate better predictive capability. SN and SP represent the proportions of correctly identified methylated and unmethylated samples, respectively. Results and discussion In this section, we present extensive experiments to validate the effectiveness of our approach. First, we compare the performance of iDNA-DAPHA with existing state-of-the-art methods across 17 benchmark datasets. Second, we conduct ablation studies to evaluate the individual contributions of DAP and HA. Third, we interpret iDNA-DAPHA through motif similarity analysis and attention visualization, showing the model&#8217;s ability to capture conserved sequence patterns. Finally, we demonstrate that DAP helps the model learn discriminative representations for methylation prediction. Performance comparison with existing methods In this study, to evaluate the effectiveness of our method, we performed DAP to obtain the shared pretrained weights, which were then independently fine-tuned on each methylation dataset (D1&#8211;D17). The resulting models were subsequently compared with existing state-of-the-art methods. As shown in Table 1 , iDNA-DAPHA consistently achieved superior performance in terms of ACC, outperforming all other methods across every dataset. For MCC, iDNA-DAPHA outperformed all competing methods on 16 of the 17 datasets and achieved performance comparable to the best-performing model on the remaining one (D15). This indicates that iDNA-DAPHA is not only accurate but also reliable in distinguishing between methylated and unmethylated sites. Additionally, iDNA-DAPHA achieved the highest results on 13 out of 17 datasets in terms of AUC, with results on the remaining 4 datasets (D7, D9, D15, and D16) comparable to the best. Notably, in datasets D1 and D13, both of which have the smallest training samples, iDNA-DAPHA showed significant improvements over the second-best method, achieving enhancements of 3.82% and 1.82% in ACC, and 9.27% and 4.01% in MCC, respectively. Besides the global performance metrics (ACC and MCC) and ranking capability evaluation (AUC), additional class-specific metrics&#8212;SN and SP&#8212;are available in Supplementary Table S2 . Table 1 Performance comparison between the proposed iDNA-DAPHA model and existing methods on 17 methylation datasets (D1&#8211;D17) evaluated by ACC, MCC, and AUC Indicator Models 4mC 5hmC 6mA D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 ACC iDNA-MS [ 4 ] 0.7109 0.8239 0.7042 0.7115 0.9475 0.9679 0.8377 0.8557 0.7113 0.8962 0.9226 0.8842 0.8545 0.7855 0.8563 0.7342 0.8451 iDNA-ABT [ 3 ] 0.8251 0.842 0.7027 0.7383 0.9492 0.9685 0.8538 0.8903 0.7328 0.9122 0.9268 0.898 0.8261 0.8011 0.874 0.7738 0.8694 iDNA-ABF [ 12 ] 0.8579 0.8524 0.723 0.7434 0.9501 0.9679 0.8603 0.9138 0.7399 0.9228 0.9413 0.9104 0.8629 0.8278 0.8804 0.7771 0.8817 MuLan-Methyl [ 1 ] 0.8333 0.8522 0.7376 0.7380 0.9484 0.9649 0.8649 0.9131 0.7590 0.9276 0.9407 0.9077 0.9164 0.8325 0.8840 0.7895 0.8742 StableDNAm [ 11 ] 0.8531 0.8529 0.7104 0.7429 0.9492 0.9679 0.8607 0.9086 0.7445 0.9231 0.9388 0.9072 0.8177 0.8270 0.8821 0.7680 0.8774 iDNA-DAPHA 0.8907 0.8567 0.7396 0.7476 0.9509 0.9685 0.8655 0.9216 0.7603 0.9283 0.9449 0.9132 0.9331 0.8457 0.8846 0.7978 0.8941 MCC iDNA-MS [ 4 ] 0.4220 0.648 0.4080 0.4230 0.8970 0.9360 0.6760 0.7120 0.4230 0.7920 0.8460 0.7690 0.7100 0.5720 0.7280 0.4680 0.6910 iDNA-ABT [ 3 ] 0.6517 0.6842 0.4064 0.4768 0.9009 0.9369 0.7088 0.7808 0.4673 0.8244 0.8244 0.796 0.6525 0.6096 0.754 0.5512 0.7394 iDNA-ABF [ 12 ] 0.7162 0.7047 0.447 0.4868 0.9022 0.9358 0.7223 0.8279 0.4843 0.8457 0.8827 0.8209 0.7271 0.6569 0.7671 0.5543 0.7634 MuLan-Methyl [ 1 ] 0.6684 0.7051 0.4772 0.4771 0.8984 0.9299 0.7307 0.8263 0.5199 0.8552 0.8815 0.8156 0.8328 0.6688 0.7773 0.5803 0.7491 StableDNAm [ 11 ] 0.7061 0.7062 0.4227 0.4865 0.9003 0.9359 0.7215 0.8178 0.4944 0.8462 0.8778 0.8146 0.6354 0.6542 0.7720 0.5378 0.7558 iDNA-DAPHA 0.7826 0.7135 0.4838 0.4962 0.9035 0.9369 0.7315 0.8435 0.5214 0.8567 0.8898 0.8270 0.8662 0.6924 0.7770 0.5956 0.7882 AUC iDNA-MS [ 4 ] 0.7800 0.9000 0.7710 0.7800 0.9600 0.9840 0.9110 0.9350 0.7790 0.9560 0.9770 0.9500 0.9240 0.8680 0.9220 0.8130 0.9210 iDNA-ABT [ 3 ] 0.8555 0.9070 0.7537 0.8057 0.9553 0.9757 0.9184 0.9433 0.7902 0.9544 0.9544 0.9510 0.8789 0.8709 0.9310 0.8361 0.9261 iDNA-ABF [ 12 ] 0.9089 0.9285 0.7897 0.8213 0.9677 0.9796 0.9349 0.9682 0.8098 0.9713 0.9804 0.9695 0.9280 0.9062 0.9355 0.8500 0.9506 MuLan-Methyl [ 1 ] 0.9108 0.9256 0.8064 0.8149 0.9680 0.9817 0.9378 0.9684 0.8350 0.9730 0.9821 0.9687 0.9654 0.9082 0.9467 0.8669 0.9446 StableDNAm [ 11 ] 0.8958 0.9283 0.7758 0.8187 0.9672 0.9810 0.9342 0.9657 0.8156 0.9713 0.9813 0.9686 0.8806 0.9053 0.9437 0.8454 0.9485 iDNA-DAPHA 0.9291 0.9291 0.8112 0.8219 0.9708 0.9841 0.9365 0.9714 0.8311 0.9735 0.9835 0.9705 0.9682 0.9100 0.9460 0.8654 0.9541 Bold values indicate the optimal performance among the compared methods, while underlined values represent the second-best performance. Overall, these results demonstrate that iDNA-DAPHA outperforms existing methods across a broad spectrum of datasets, confirming its suitability for methylation identification, especially in scenarios with limited training samples for specific methylation types in certain species. The consistently superior performance indicates the reliability and robustness of iDNA-DAPHA, making it a powerful tool for methylation prediction. Ablation studies Ablation of domain-adaptive pretraining and hierarchical attention In this subsection, we present the results of ablation studies to evaluate the effectiveness of DAP and HA. Ablation experiments were conducted on three randomly selected datasets representing different types of methylation: 4mC_C.equisetifolia, 5hmC_H.sapiens, and 6mA_S.cerevisiae. We first compared four main variants of the model (with results summarized in Table 2 ): the baseline model (BL), the baseline with domain-adaptive pretraining (BL+DAP), the baseline with hierarchical attention (BL+HA), and our proposed method (iDNA-DAPHA), which integrates both components. The baseline model (BL) provided a solid foundation across these datasets. For instance, it achieved the highest accuracy of 0.9428 on the 5hmC_H.sapiens dataset, while even its lowest accuracy, observed on the 6mA_S.cerevisiae dataset, remained at a respectable 0.8106. Nevertheless, when BL+DAP, which integrates DAP, was used, there were consistent and notable improvements in performance metrics (ACC, MCC, and AUC), with average increases of 2.81% in ACC, 5.62% in MCC, and 2.06% in AUC. This trend underscores the effectiveness of DAP in enhancing the model&#8217;s ability to adapt to methylation sequences. Furthermore, BL+HA, which incorporates HA into the baseline, also resulted in performance improvements, albeit to a lesser extent than those observed with domain-adaptive pretraining (BL+DAP). The BL+HA variant showed average increases of 0.72% in ACC, 1.44% in MCC, and 0.46% in AUC. Finally, our proposed method, iDNA-DAPHA, which combines both DAP and HA, consistently outperformed other variants, with average increases of 3.35% in ACC, 6.71% in MCC, and 2.30% in AUC. Notably, on the 4mC_C.equisetifolia dataset, iDNA-DAPHA exhibited the highest improvement, with an increase of 5.74% in accuracy, 11.51% in MCC, and 3.63% in AUC. Table 2 Ablation analysis of our model across three species and methylation types, comparing the baseline (BL), BL with DAP (BL+DAP), BL with HA (BL+HA), the full iDNA-DAPHA model, and the variant without GRL Dataset Methods ACC MCC AUC 4mC C.equisetifolia BL 0.8333 0.6675 0.8928 BL+DAP 0.8798 (+4.65%) 0.7597 (+9.22%) 0.9238 (+3.10%) BL+HA 0.8443 (+1.10%) 0.6890 (+2.15%) 0.8978 (+0.50%) iDNA-DAPHA w/o GRL 0.8689 (+3.56%) 0.7413 (+7.38%) 0.9193 (+2.65%) iDNA-DAPHA 0.8907 (+ 5.74% ) 0.7826 (+ 11.51% ) 0.9291 (+ 3.63% ) 5hmC H.sapiens BL 0.9428 0.8873 0.9633 BL+DAP 0.9497 (+0.69%) 0.9014 (+1.41%) 0.9696 (+0.63%) BL+HA 0.9480 (+0.52%) 0.8978 (+1.05%) 0.9671 (+0.38%) iDNA-DAPHA w/o GRL 0.9488 (+0.6%) 0.8990 (+1.17%) 0.9679 (+0.46%) iDNA-DAPHA 0.9509 (+ 0.81% ) 0.9035 (+ 1.62% ) 0.9708 (+ 0.75% ) 6mA S.cerevisiae BL 0.8106 0.6223 0.8847 BL+DAP 0.8415 (+3.09%) 0.6845 (+6.22%) 0.9093 (+2.46%) BL+HA 0.8159 (+0.53%) 0.6334 (+1.11%) 0.8897 (+0.50%) iDNA-DAPHA w/o GRL 0.8394 (+2.88%) 0.6829 (+6.06%) 0.9050 (+2.03%) iDNA-DAPHA 0.8457 (+ 3.51% ) 0.6924 (+ 7.01% ) 0.9100 (+ 2.53% ) Bold values indicate the optimal performance among the compared methods, while underlined values represent the second-best performance. In addition, to investigate the role of the gradient reversal layer (GRL) in DAP, we further introduced an additional variant, &#8216;iDNA-DAPHA w/o GRL,&#8217; which performs DAP without the GRL while still incorporating the HA mechanism. Intriguingly, the results in Table 2 reveal that this variant not only performs worse than iDNA-DAPHA but also that this architecturally more complex model consistently underperforms the comparatively simpler BL+DAP model across all three datasets. This observation demonstrates that the GRL plays a crucial role in feature alignment during DAP, promoting the learning of shared features across methylation tasks. Overall, these ablation studies show that both DAP and HA contribute to improved model performance, with DAP having a more pronounced effect. Our iDNA-DAPHA method integrates both techniques, achieving superior generalization and prediction accuracy. These results indicate that incorporating DAP and HA is an effective strategy for enhancing performance across diverse methylation datasets. Evaluation of dual-scale versus four-scale k-mer representations Our model employs a dual-scale k-mer representation strategy, integrating features from both 3-mers and 6-mers. This choice follows the effective dual-scale information processing strategy proposed in the previous method iDNA-ABF [ 12 ], where this combination was reported to achieve the highest average accuracy among various single-scale and dual-scale configurations. To evaluate this decision against a more complex alternative, we conducted a comparative analysis with a four-scale strategy (3-, 4-, 5-, and 6-mers), similar to that used in StableDNAm [ 11 ]. As shown in Table 3 , this four-scale strategy yields slight improvements in predictive accuracy across three benchmark datasets, while significantly increasing computational demands (see Supplementary Table S3 ). Specifically, it requires four parallel DNA-BERT models (DNAbert-3, -4, -5, and -6), resulting in nearly double the number of parameters, training time, inference time, and GPU memory usage. Table 3 Comparison of predictive performance between dual-scale (3- and 6-mers) and four-scale (3-, 4-, 5-, and 6-mers) k-mer representation strategies Dataset K-mers strategy ACC MCC AUC 4mC C.equisetifolia dual-scale 0.8907 0.7826 0.9291 four-scale 0.8962 (+0.55%) 0.7931 (+1.05%) 0.9339 (+0.48%) 5hmC H.sapiens dual-scale 0.9509 0.9035 0.9708 four-scale 0.9509 (+0%) 0.9040 (+0.05%) 0.9733 (+0.25%) 6mA S.cerevisiae dual-scale 0.8457 0.6924 0.9100 four-scale 0.8457 (+0%) 0.6929 (+0.05%) 0.9120 (+0.2%) Therefore, the dual-scale (3-mer and 6-mer) representation provides a practical balance between predictive performance and computational efficiency, and we retain it as the default configuration for our model. In addition, to facilitate reproducibility and flexible application, we have made both the dual-scale and four-scale domain-adaptive pretrained weights and the corresponding model code publicly available, allowing users to fine-tune either model depending on their performance and efficiency requirements. Computational efficiency analysis To evaluate the computational efficiency of iDNA-DAPHA, we quantitatively compared it with the baseline model under the same hardware and software environment. Table 4 summarizes the detailed benchmark results obtained from the 4mC_C.equisetifolia dataset as a representative example. Compared with the baseline, iDNA-DAPHA introduces slightly more parameters and moderate increases in training time and GPU memory. The increase in parameter count mainly arises from the HA module, whereas the gradient reversal layer used in DAP adds no additional learnable parameters. The reported parameter counts correspond to the fine-tuned models actually used for inference. Table 4 Computational efficiency comparison of iDNA-DAPHA and the baseline Model Params (M) Training time/Step (ms) Inference time/Step (ms) Peak Mem (Train, MB) Peak Mem (Infer, MB) Pretrain time (Total) Baseline 175.32 78.7 23.2 3541.12 691.29 &#8211; iDNA-DAPHA 217.86 117.2 29.3 4774.75 862.23 1.72h (39160 steps, bs=32) Figure 3 Interpretable illustrations of motifs learnt by iDNA-DAPHA across three species and methylation types (4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256), displaying attention heatmaps (left panel) where visual intensity denotes greater importance, alongside a comparison with STREME motifs (right panel) using P-values to indicate statistical significance and motif similarity. Alt text: Interpretable illustrations of motifs learnt by iDNA-DAPHA across three species and methylation types: 4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256. The figure is organized in three horizontal rows, each showing two main components. Left Panel: Attention heatmaps generated by iDNA-DAPHA for 3-mer and 6-mer sequences, where darker blue indicates high-importance regions (e.g. &#8216;TCTGC&#8217;, &#8216;AGGGG&#8217;, and &#8216;TTGAG&#8217;). Right Panel: Comparing motifs identified by iDNA-DAPHA and the STREME tool. Low Tomtom P-values (3.57e-03 for 4mC_F.vesca, 2.44e-02 for 5hmC_H.sapiens, and 5.18e-03 for 6mA_Xoc BLS256) show the high similarity between the paired motifs. In absolute terms, iDNA-DAPHA remains lightweight ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\approx $\\end{document} 218 M parameters, &lt;4.8 GB peak GPU memory during training). Despite a nearly 49% increase in training time, the additional one-time pretraining phase was completed in a modest 1.72 h. Similarly, while the per-step inference time increases by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\sim $\\end{document} 26%, the final per-step latency ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\sim $\\end{document} 29 ms) remains sufficiently low for practical deployment. In exchange for these manageable costs, the model delivers significant performance gains, with average performance increases of 3.35% in ACC, 6.71% in MCC, and 2.30% in AUC (see Table 2 ). Overall, these results demonstrate that iDNA-DAPHA achieves a favorable trade-off between efficiency and accuracy. Interpreting iDNA-DAPHA through motif similarity analysis and attention visualization To provide insight into the interpretability of iDNA-DAPHA, we analyse the similarity between motifs extracted by our method and those identified by the established motif-finding tool STREME. We employ iDNA-DAPHA&#8217;s attention mechanism on sequences from three randomly selected methylation datasets to identify key regions crucial for accurate methylation prediction and extract motifs from these attention-highlighted regions. The regions focused on by the model correspond to sequence areas with high attention scores, indicating their significance in the prediction process. As shown in Fig. 3 (right), the motifs identified by our method (highlighted within the light blue boxes) exhibit a high degree of similarity to those found by STREME on the three species datasets (4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256). This similarity is quantitatively supported by low \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $P$\\end{document} -values calculated using the TOMTOM tool: 4mC_F.vesca \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $P$\\end{document} -value = 3.57e-03, 5hmC_H.sapiens \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $P$\\end{document} -value = 2.44e-02, and 6mA_Xoc BLS256 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $P$\\end{document} -value = 5.18e-03. The consistent matches between iDNA-DAPHA and STREME motifs reflect the reliability of our method in identifying conserved sequence patterns. Figure 4 t-SNE visualizations comparing the learnt feature representations of general-purpose (left) and domain-adaptive (right) pretraining across three methylation datasets (4mC in F.vesca, 5hmC in H.sapiens, and 6mA in Xoc BLS256) demonstrate that DAP yields distinct and compact clusters, in contrast to the dispersed and overlapping clusters observed in general-purpose pretraining. Alt text: t-SNE visualizations comparing learnt feature representations from general-purpose pretraining versus DAP across three DNA methylation datasets: 4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256. The plots in the left column (general-purpose pretraining) show dispersed and overlapping clusters of methylated and unmethylated sequences, whereas the plots in the right column (DAP) show more compact and clearly separated clusters, illustrating improved discriminative representations for methylation prediction. Furthermore, to enhance intuitive understanding, we visualize attention score distributions for three randomly selected sequences from the respective species. The high-attention regions are aligned with the motifs identified in each dataset, as depicted in Fig. 3 (left). This alignment not only highlights the regions focused on by our method but also demonstrates their correspondence to statistically significant motifs, further enhancing the interpretability of the model. Impact of domain-adaptive pretraining on DNA methylation prediction Given that DAP showed a more notable performance improvement than HA in the ablation studies, we also conduct an interpretability analysis through visualization to explore its impact on DNA methylation prediction. Specifically, we compare DAP with general-purpose pretraining. We randomly selected three methylation datasets from species representing different DNA methylation types and visualized the learnt representation distribution using t-SNE. Fig. 4 illustrates the differences in feature space by models with general-purpose pretraining versus those with methylation DAP. The left-side plots, derived from model with general-purpose pretraining, display more dispersed and overlapping clusters across the three datasets (4mC_F.vesca, 5hmC_H.sapiens, and 6mA_Xoc BLS256), suggesting less coherent groupings and reduced discrimination between methylated and unmethylated sequences. In contrast, the right-side plots, obtained from model with methylation DAP, exhibit more compact and well-separated clusters, reflecting enhanced discriminative representations for methylation prediction. This comparison demonstrates that DAP leads to more discriminative representations for DNA methylation data, improving the model&#8217;s ability to distinguish between methylated and unmethylated sequences. The enhanced clustering evident in the visualizations underscores DAP&#8217;s utility, ultimately yielding more accurate predictions in target downstream tasks. Conclusion In this paper, we present iDNA-DAPHA, an accurate and generic two-stage computational framework for DNA methylation prediction. The framework leverages DAP incorporating feature alignment to learn general features across various types of methylation sequences from multiple species, followed by fine-tuning to capture task-specific features. Moreover, we introduce an HA mechanism into the model architecture to improve its performance. Extensive experiments across 17 benchmark datasets demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods, and ablation studies validate the effectiveness of DAP and HA. As aberrant DNA methylation is widely recognized to be associated with many diseases and serves as an important indicator in epigenetic diagnostics, iDNA-DAPHA&#8217;s methylation prediction capability across species and methylation types, even with limited training samples, may contribute to studies on disease biomarker discovery and epigenetic drug target identification. In future work, we plan to develop a user-friendly web server to facilitate the application of our framework by biologists. Additionally, we will explore integrating broader methylation data during pretraining to further enhance the model&#8217;s ability to adapt across species and methylation types. Key Points We propose iDNA-DAPHA, an accurate and generic two-stage framework that leverages domain-adaptive pretraining (DAP) incorporating feature alignment to learn common features across various types of methylation sequences from multiple species. Our framework introduces a hierarchical attention (HA) mechanism that combines token-to-token attention and group-to-group attention to capture both fine-grained inter-token and broader inter-group correlations. Experimental results demonstrate that iDNA-DAPHA performs better than existing state-of-the-art methods on benchmark datasets, especially in scenarios with limited training samples for specific methylation types in certain species. Ablation studies validate the effectiveness and contributions of DAP and HA. Furthermore, visualization-based interpretability analyses reveal that the model can learn conserved sequence patterns and more discriminative representations for methylation prediction. Supplementary Material Supplementary_table_s1_bbaf642 Supplementary_table_s2_bbaf642 Supplementary_table_s3_bbaf642 Conflict of interest The authors declare no conflict of interest. Funding This work was supported by National Natural Science Foundation of China (NSFC) 62272172 and 62266012, Zhuhai Science and Technology Plan Project (2320004002758), Fundamental Research Funds for the Central Universities 2025ZYGXZR095, and the High-Level Innovative Talent Project of Guizhou Province (Grant no. QKHPTRC-GCC2023027). Data availability The model code, domain-adaptive pretraining weight, and fine-tuned weights on all benchmark datasets are freely available at https://github.com/WenjunWang-SCUT/iDNA-DAPHA to facilitate reproducibility and benefit the research community. All datasets used in this study are publicly available and were obtained from the previously published study iDNA-MS [ 4 ]. References 1 Zeng &#160; W , Gautam &#160; A , Huson &#160; DH . MuLan-methyl&#8212;Multiple transformer-based language models for accurate DNA methylation prediction . GigaScience &#160; 2023 ; 12 : giad054 . 10.1093/gigascience/giad054 PMC10367125 37489753 2 Zhou &#160; D , Robertson &#160; KD . Chapter 24&#8212;role of DNA methylation in genome stability . In Igor &#160; Kovalchuk and Olga &#160; Kovalchuk , (eds), Genome Stability , pages 409 &#8211; 24 . Boston : Academic Press , 2016 . 10.1016/B978-0-12-803309-8.00024-0 . 3 Yingying &#160; Y , He &#160; W , Jin &#160; J . et&#160;al. &#160; iDNA-ABT: advanced deep learning model for detecting DNA methylation with adaptive features and transductive information maximization . Bioinformatics &#160; 2021 ; 37 : 4603 &#8211; 10 . 34601568 10.1093/bioinformatics/btab677 4 Lv &#160; H , Dao &#160; F-Y , Zhang &#160; D . et&#160;al. &#160; iDNA-MS: an integrated computational tool for detecting DNA modification sites in multiple genomes . Iscience &#160; 2020 ; 23 : 100991 . 10.1016/j.isci.2020.100991 32240948 PMC7115099 5 Lulu &#160; H , Liu &#160; Y , Han &#160; S . et&#160;al. &#160; Jump-seq: genome-wide capture and amplification of 5-hydroxymethylcytosine sites . J Am Chem Soc &#160; 2019 ; 141 : 8694 &#8211; 7 . 31117646 10.1021/jacs.9b02512 PMC7061342 6 O&#8217;Brown &#160; ZK , Boulias &#160; K , Wang &#160; J . et&#160;al. &#160; Sources of artifact in measurements of 6mA and 4mC abundance in eukaryotic genomic DNA . BMC Genomics &#160; 2019 ; 20 : 1 &#8211; 15 . 10.1186/s12864-019-5754-6 31159718 PMC6547475 7 Zhao &#160; Z , Zhang &#160; X , Chen &#160; F . et&#160;al. &#160; Accurate prediction of DNA N4-methylcytosine sites via boost-learning various types of sequence features . BMC Genomics &#160; 2020 ; 21 : 1 &#8211; 11 . 10.1186/s12864-020-07033-8 PMC7488740 32917152 8 Thomson &#160; JP , Meehan &#160; RR . The application of genome-wide 5-hydroxymethylcytosine studies in cancer research . Epigenomics &#160; 2017 ; 9 : 77 &#8211; 91 . 10.2217/epi-2016-0122 27936926 9 Heyn &#160; H , Esteller &#160; M . An adenine code for DNA: a second life for N6-methyladenine . Cell &#160; 2015 ; 161 : 710 &#8211; 3 . 10.1016/j.cell.2015.04.021 25936836 10 Ye &#160; F , Luo &#160; G-Z , Chen &#160; K . et&#160;al. &#160; N6-methyldeoxyadenosine marks active transcription start sites in chlamydomonas . Cell &#160; 2015 ; 161 : 879 &#8211; 92 . 10.1016/j.cell.2015.04.010 25936837 PMC4427561 11 Zhuo &#160; L , Wang &#160; R , Xiangzheng &#160; F . et&#160;al. &#160; StableDNAm: towards a stable and efficient model for predicting DNA methylation based on adaptive feature correction learning . BMC Genomics &#160; 2023 ; 24 : 742 . 10.1186/s12864-023-09802-7 38053026 PMC10698904 12 Jin &#160; J , Yingying &#160; Y , Wang &#160; R . et&#160;al. &#160; iDNA-ABF: multi-scale deep biological language learning model for the interpretable prediction of DNA methylations . Genome Biol &#160; 2022 ; 23 : 1 &#8211; 23 . 36253864 10.1186/s13059-022-02780-1 PMC9575223 13 Meissner &#160; A , Gnirke &#160; A , Bell &#160; GW . et&#160;al. &#160; Reduced representation bisulfite sequencing for comparative high-resolution DNA methylation analysis . Nucleic Acids Res &#160; 2005 ; 33 : 5868 &#8211; 77 . 10.1093/nar/gki901 16224102 PMC1258174 14 Lister &#160; R , Pelizzola &#160; M , Dowen &#160; RH . et&#160;al. &#160; Human DNA methylomes at base resolution show widespread epigenomic differences . Nature &#160; 2009 ; 462 : 315 &#8211; 22 . 10.1038/nature08514 19829295 PMC2857523 15 Tang &#160; Q , Kang &#160; J , Yuan &#160; J . et&#160;al. &#160; DNA4mC-LIP: a linear integration method to identify N4-methylcytosine site in multiple species . Bioinformatics &#160; 2020 ; 36 : 3327 &#8211; 35 . 10.1093/bioinformatics/btaa143 32108866 16 Liu &#160; Q , Chen &#160; J , Wang &#160; Y . et&#160;al. &#160; DeepTorrent: a deep learning-based approach for predicting DNA N4-methylcytosine sites . Brief Bioinform &#160; 2021 ; 22 : bbaa124 . 32608476 10.1093/bib/bbaa124 PMC8599298 17 Ma &#160; X , Thela &#160; SR , Zhao &#160; F . et&#160;al. &#160; Deep5hmC: predicting genome-wide 5-hydroxymethylcytosine landscape via a multimodal deep learning model . Bioinformatics &#160; 2024 ; 40 : btae528 . 39196755 10.1093/bioinformatics/btae528 PMC11379467 18 Ali &#160; SD , Kim &#160; JH , Tayara &#160; H . et&#160;al. &#160; Prediction of RNA 5-hydroxymethylcytosine modifications using deep learning . IEEE Access &#160; 2021 ; 9 : 8491 &#8211; 6 . 10.1109/ACCESS.2021.3049146 19 Chen &#160; W , Lv &#160; H , Nie &#160; F . et&#160;al. &#160; i6mA-Pred: identifying DNA N6-methyladenine sites in the rice genome . Bioinformatics &#160; 2019 ; 35 : 2796 &#8211; 800 . 10.1093/bioinformatics/btz015 30624619 20 Haitao &#160; Y , Dai &#160; Z . SNNRice6mA: a deep learning method for predicting DNA N6-methyladenine sites in rice genome . Front Genet &#160; 2019 ; 10 : 1071 . 31681441 10.3389/fgene.2019.01071 PMC6797597 21 Basith &#160; S , Manavalan &#160; B , Shin &#160; TH . et&#160;al. &#160; SDM6A: a web-based integrative machine-learning framework for predicting 6mA sites in the rice genome . Mol Therapy-Nucleic Acids &#160; 2019 ; 18 : 131 &#8211; 41 . 10.1016/j.omtn.2019.08.011 PMC6796762 31542696 22 Devlin J, Chang M-W, Lee K. et&#160;al. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of NAACL-HLT , pp. 4171&#8211;86, Minneapolis, Minnesota: Association for Computational Linguistics, 2019. 23 Lee &#160; J , Yoon &#160; W , Kim &#160; S . et&#160;al. &#160; BioBERT: a pre-trained biomedical language representation model for biomedical text mining . Bioinformatics &#160; 2020 ; 36 : 1234 &#8211; 40 . 10.1093/bioinformatics/btz682 31501885 PMC7703786 24 Gururangan S, Marasovi&#263; A, Swayamdipta S. et&#160;al. Don&#8217;t stop pretraining: adapt language models to domains and tasks. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 8342&#8211;60, Online. Association for Computational Linguistics, 2020. 25 Tai W, Kung HT, Dong X. et&#160;al. exBERT: extending pre-trained models with domain-specific vocabulary under constrained training resources. In: Findings of the Association for Computational Linguistics: EMNLP 2020 ; pp. 1433&#8211;9, Online. Association for Computational Linguistics, 2020. 26 van der Putten &#160; J , de Groof &#160; J , Struyvenberg &#160; M . et&#160;al. &#160; Multi-stage domain-specific pretraining for improved detection and localization of barrett&#8217;s neoplasia: a comprehensive clinically validated study . Artif Intell Med &#160; 2020 ; 107 : 101914 . 10.1016/j.artmed.2020.101914 32828453 27 Koto F, Lau JH, Baldwin T. IndoBERTweet: a pretrained language model for Indonesian Twitter with effective domain-specific vocabulary initialization. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 10660&#8211;8, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021. 28 Ji S, Zhang T, Ansari L. et&#160;al. MentalBERT: publicly available pretrained language models for mental healthcare. In: Proceedings of the Thirteenth Language Resources and Evaluation Conference , pp. 7184&#8211;90, Marseille, France: European Language Resources Association, 2022. 29 Zhang S, Xu Y, Usuyama N. et&#160;al. BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. Available from: https://arxiv.org/abs/2303.00915 (29 November 2025, date last accessed). 30 Ji S, Zhang T, Yang K. et&#160;al. Domain-specific continued pretraining of language models for capturing long context in mental health. arXiv preprint arXiv:2304.10447. 2023. Available from: https://arxiv.org/abs/2304.10447 (29 November 2025, date last accessed). 31 Fangzhi &#160; X , Lin &#160; Q , Liu &#160; J . et&#160;al. &#160; MoCA: incorporating domain pretraining and cross attention for textbook question answering . Pattern Recogn &#160; 2023 ; 140 : 109588 . 32 Zhai W, Qi H, Zhao Q. et&#160;al. Chinese MentalBERT: domain-adaptive pre-training on social media for Chinese mental health text analysis. In: Findings of the Association for Computational Linguistics: ACL 2024 , pp. 10574&#8211;85, Bangkok, Thailand: Association for Computational Linguistics, 2024. 33 Zeng &#160; W , Dou &#160; Y , Pan &#160; L . et&#160;al. &#160; Improving prediction performance of general protein language model by domain-adaptive pretraining on DNA-binding protein . Nat Commun &#160; 2024 ; 15 : 7838 . 10.1038/s41467-024-52293-7 39244557 PMC11380688 34 Yang &#160; Z , Shao &#160; W , Matsuda &#160; Y . et&#160;al. &#160; iResNetDM: an interpretable deep learning approach for four types of DNA methylation modification prediction . Comput Struct Biotechnol J &#160; 2024 ; 23 : 4214 &#8211; 21 . 10.1016/j.csbj.2024.11.006 39650332 PMC11621598 35 Huang X, Liu Q, Zhao Y. et&#160;al. MethylProphet: a generalized gene-contextual model for inferring whole-genome DNA methylation landscape. bioRxiv , 2025. Available from: 10.1101/2025.02.05.636730 [29 November 2025, date last accessed). 36 Zhou &#160; Z , Zhuo &#160; L , Xiangzheng &#160; F . et&#160;al. &#160; Joint masking and self-supervised strategies for inferring small molecule-miRNA associations . Mol Therapy Nucleic Acids &#160; 2024 ; 35 : 102103 . 10.1016/j.omtn.2023.102103 PMC10794920 38261851 37 Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation. In: Proceedings of the 32nd International Conference on Machine Learning , pp. 1180&#8211;9, Lille, France: International Machine Learning Society, 2015. 38 Vaswani A, Shazeer N, Parmar N. et&#160;al. Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems , pp. 6000&#8211;10, Long Beach, California, USA: Curran Associates, Inc., 2017. 39 Ji &#160; Y , Zhou &#160; Z , Liu &#160; H . et&#160;al. &#160; DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome . Bioinformatics &#160; 2021 ; 37 : 2112 &#8211; 20 . 10.1093/bioinformatics/btab083 33538820 PMC11025658 40 Huang &#160; J , Mao &#160; Y , Deng &#160; J . et&#160;al. &#160; EMGANet: edge-aware multi-scale group-mix attention network for breast cancer ultrasound image segmentation . IEEE J Biomed Health Inform &#160; 2025 ; 29 : 5631 &#8211; 41 . 10.1109/JBHI.2025.3546345 40031552"
}