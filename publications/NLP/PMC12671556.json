{
  "pmcid": "PMC12671556",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:25.047670",
  "metadata": {
    "journal_title": "Journal of machine learning research : JMLR",
    "journal_nlm_ta": "J Mach Learn Res",
    "journal_iso_abbrev": "J Mach Learn Res",
    "journal": "Journal of machine learning research : JMLR",
    "pmcid": "PMC12671556",
    "pmid": "41341524",
    "title": "Efficient and Robust Semi-supervised Estimation of Average Treatment Effect with Partially Annotated Treatment and Response",
    "authors": [
      "Hou Jue",
      "Mukherjee Rajarshi",
      "Cai Tianxi"
    ],
    "abstract": "A notable challenge of leveraging Electronic Health Records (EHR) for treatment effect assessment is the lack of precise information on important clinical variables, including the treatment received and the response. Both treatment information and response cannot be accurately captured by readily available EHR features in many studies and require labor-intensive manual chart review to precisely annotate, which limits the number of available gold standard labels on these key variables. We considered average treatment effect (ATE) estimation when 1) exact treatment and outcome variables are only observed together in a small labeled subset and 2) noisy surrogates of treatment and outcome, such as relevant prescription and diagnosis codes, along with potential confounders are observed for all subjects. We derived the efficient influence function for ATE and used it to construct a semi-supervised multiple machine learning (SMMAL) estimator. We justified that our SMMAL ATE estimator is semi-parametric efficient with B-spline regression under low-dimensional smooth models. We developed the adaptive sparsity/model doubly robust estimation under high-dimensional logistic propensity score and outcome regression models. Results from simulation studies demonstrated the validity of our SMMAL method and its superiority over supervised and unsupervised benchmarks. We applied SMMAL to the assessment of targeted therapies for metastatic colorectal cancer in comparison to chemotherapy.",
    "keywords": [
      "semi-parametric efficiency",
      "double robustness",
      "high-dimensional regression",
      "semi-supervised learning"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">J Mach Learn Res</journal-id><journal-id journal-id-type=\"iso-abbrev\">J Mach Learn Res</journal-id><journal-id journal-id-type=\"pmc-domain-id\">319</journal-id><journal-id journal-id-type=\"pmc-domain\">nihpa</journal-id><journal-title-group><journal-title>Journal of machine learning research : JMLR</journal-title></journal-title-group><issn pub-type=\"ppub\">1532-4435</issn><issn pub-type=\"epub\">1533-7928</issn><custom-meta-group><custom-meta><meta-name>pmc-is-collection-domain</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-collection-title</meta-name><meta-value>NIHPA Author Manuscripts</meta-value></custom-meta></custom-meta-group></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12671556</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12671556.1</article-id><article-id pub-id-type=\"pmcaid\">12671556</article-id><article-id pub-id-type=\"pmcaiid\">12671556</article-id><article-id pub-id-type=\"manuscript-id\">NIHMS2084485</article-id><article-id pub-id-type=\"pmid\">41341524</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHMS2084485</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHPA2084485</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Efficient and Robust Semi-supervised Estimation of Average Treatment Effect with Partially Annotated Treatment and Response</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Hou</surname><given-names initials=\"J\">Jue</given-names></name><aff id=\"A1\">Division of Biostatistics, University of Minnesota School of Public Health, Minneapolis, MN 55455, USA.</aff></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Mukherjee</surname><given-names initials=\"R\">Rajarshi</given-names></name><aff id=\"A2\">Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02120, USA</aff></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Cai</surname><given-names initials=\"T\">Tianxi</given-names></name><aff id=\"A3\">Department of Biostatistics, Harvard T.H. Chan School of Public Health, Department of Biomedical Informatics, Harvard Medical School, Boston, MA 02120, USA</aff></contrib></contrib-group><author-notes><corresp id=\"CR1\"><email>HOU00123@UMN.EDU</email></corresp></author-notes><pub-date pub-type=\"ppub\"><year>2025</year></pub-date><volume>26</volume><issue-id pub-id-type=\"pmc-issue-id\">501654</issue-id><elocation-id>40</elocation-id><pub-history><event event-type=\"nihms-submitted\"><date><day>25</day><month>05</month><year>2025</year></date></event><event event-type=\"pmc-release\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 05:25:15.070\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>License: CC-BY 4.0, see <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>. Attribution requirements are provided at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://jmlr.org/papers/v26/23-1587.html\">http://jmlr.org/papers/v26/23-1587.html</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"nihms-2084485.pdf\"/><abstract id=\"ABS1\"><p id=\"P1\">A notable challenge of leveraging Electronic Health Records (EHR) for treatment effect assessment is the lack of precise information on important clinical variables, including the treatment received and the response. Both treatment information and response cannot be accurately captured by readily available EHR features in many studies and require labor-intensive manual chart review to precisely annotate, which limits the number of available gold standard labels on these key variables. We considered average treatment effect (ATE) estimation when 1) exact treatment and outcome variables are only observed together in a small labeled subset and 2) noisy surrogates of treatment and outcome, such as relevant prescription and diagnosis codes, along with potential confounders are observed for all subjects. We derived the efficient influence function for ATE and used it to construct a semi-supervised multiple machine learning (SMMAL) estimator. We justified that our SMMAL ATE estimator is semi-parametric efficient with B-spline regression under low-dimensional smooth models. We developed the adaptive sparsity/model doubly robust estimation under high-dimensional logistic propensity score and outcome regression models. Results from simulation studies demonstrated the validity of our SMMAL method and its superiority over supervised and unsupervised benchmarks. We applied SMMAL to the assessment of targeted therapies for metastatic colorectal cancer in comparison to chemotherapy.</p></abstract><kwd-group><kwd>semi-parametric efficiency</kwd><kwd>double robustness</kwd><kwd>high-dimensional regression</kwd><kwd>semi-supervised learning</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"S1\"><label>1</label><title>Introduction</title><p id=\"P2\">The 21st Century Cures Act and the Prescription Drug User Fee Act VII have shone a spotlight on the use of real-world evidence, generated from real-world data, to support regulatory-decision making on drug effectiveness. Large scale electronic health records (EHRs) data are being increasingly used for creating the real-world evidence on treatment effectiveness or efficacy (<xref rid=\"R16\" ref-type=\"bibr\">Franklin et al., 2021</xref>). In addition to the observational nature, another notable challenge in leveraging EHR for treatment effect assessment lies in the lack of readily available data for key clinical variables, including the treatment being investigated and the outcome of interest. Response variables such as disease progression may not be well represented by readily available EHR features (<xref rid=\"R2\" ref-type=\"bibr\">Bartlett et al., 2019</xref>). Treatment information can be partially captured but not always accurately reflected by procedure codes or medication prescription codes. New therapies may not be well coded in the introduction stage immediately after regulatory approval, and treatment initiation may be later than prescription date due to external factors such as insurance approval delay. For example in a real-world evidence study comparing chemotherapies and targeted therapies as first-line treatment for metastatic colorectal cancer, we discovered based on chart-review of 100 patients by a medical expert that 1) the progression-free-survival (PFS) outcomes were poorly structured in EHRs without clear indicators for progression or complete mortality data, and 2) the medication codes or natural language processing (NLP) identified mentions in notes could not accurately capture the use of targeted therapies (see <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>).</p><p id=\"P3\">Although it is possible to improve treatment or response definition by combining multiple EHR features through rule based or machine learning algorithms, these EHR derived features are at best good &#8220;surrogates&#8221; for approximating the true treatment or response information at patient level. Compared to the classic definition of surrogate, the notion of surrogate in retrospective EHR studies shares the availability trait but differs in the temporal order and causal pathway. In the advanced stage cancer trials or prospective studies, the progress-free-survival is often used as surrogate for overall survival because progression sometimes can be captured at an earlier time. In EHR studies, however, researchers do not have readily available progression data <inline-formula><mml:math id=\"M17\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> unless they perform the labor intensive manual chart review (<xref rid=\"R17\" ref-type=\"bibr\">Griffith et al., 2019</xref>), so it is natural to borrow information from the documentations about progression <inline-formula><mml:math id=\"M18\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> like occurrence of diagnosis codes about secondary malignant neoplasm or NLP identified mention of metastasis at distant parts of body. These documentations <inline-formula><mml:math id=\"M19\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> are considered as &#8220;surrogates&#8221; because 1) they can partially indicate progression, and 2) they are accessible earlier during the research process. Since the documentations <inline-formula><mml:math id=\"M20\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> were recorded according to the true progression status <inline-formula><mml:math id=\"M21\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>, it is more reasonable to consider the true progression status temporally preceded and casually affected the documentation, <inline-formula><mml:math id=\"M22\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> in a causal diagram. Directly using these surrogates as true treatment and outcome which would potentially induce bias in the subsequent analysis (<xref rid=\"R3\" ref-type=\"bibr\">Beaulieu-Jones et al., 2020</xref>). On the other hand, annotating exact treatment and response variables via manual chart review by domain expert is resource intensive, leading to limited sample size for gold standard labels on these key data. It is thus of great practical significance to leverage both the small number of gold standard labels and the vast unlabeled data to derive unbiased and efficient inference about the average treatment effect (ATE), fundamentally a nested problem with both missing data and causal inference components. When the labeling proportion is too small for standard (missing data) positivity assumption, the setting is often referred to as the semi-supervised learning (SSL).</p><p id=\"P4\">Additional challenges arise from the high dimensionality of potential confounders. Unlike traditional cohort studies with a pre-specified number of clinical variables, EHRs provide rich data on a broader range and larger number of confounding factors (<xref rid=\"R21\" ref-type=\"bibr\">Hou et al., 2021c</xref>). Furthermore, multiple EHR features may be necessary to represent one specific clinical variable, further amplifying the dimensionality of features necessary to capture the underlying confounding factors. The complexity of the models from the high-dimensionality also increases the risk of model mis-specification for the propensity score (PS) and the outcome regression (OR). To the best of our knowledge, no method currently exists to estimate ATE under the SSL setting when both the treatment group, denoted by <inline-formula><mml:math id=\"M23\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>, and the response, denoted by <inline-formula><mml:math id=\"M24\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>, are only observed in a small subset of the full data. We focus on the missing data patterns resulting from the lack of readily available data on the exact clinical information like treatment <inline-formula><mml:math id=\"M25\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and outcomes <inline-formula><mml:math id=\"M26\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>. For small subset, manual annotations can be created to recover the exact <inline-formula><mml:math id=\"M27\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M28\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>, but researchers have to rely on scalable yet imperfect computational tools to extract treatment outcome information over the majority of the vast EHR cohort, producing the surrogates <inline-formula><mml:math id=\"M29\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> for <inline-formula><mml:math id=\"M30\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M31\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>. For conciseness, we refer to this specific SSL setting as <italic toggle=\"yes\">double missing</italic> SSL. In this paper, we address the methodology gap by proposing Semi-supervised <bold>M</bold>ultiple <bold>MA</bold>chine <bold>L</bold>earning (SMMAL) estimators for ATE that leverage both the fully observed surrogates for <inline-formula><mml:math id=\"M32\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M33\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>, denoted by <inline-formula><mml:math id=\"M34\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula>, and the partially observed gold standard labels on <inline-formula><mml:math id=\"M35\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M36\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>.</p><p id=\"P5\">Under the supervised setting where both <inline-formula><mml:math id=\"M37\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M38\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> are observed, much progress has been made in recent years on estimation of ATE with confounding adjustment from machine-learning and/or high-dimensional regression. In the low-dimensional setting, the estimation of ATE is a well studied problem including procedures that achieve semi-parametric efficiency and double robustness (<xref rid=\"R35\" ref-type=\"bibr\">Robins et al., 1994</xref>; <xref rid=\"R1\" ref-type=\"bibr\">Bang and Robins, 2005</xref>). Extension to the high-dimensional setting, however, is not straightforward due to the slower convergence rates in the estimated model parameters and the difficulty posed not only by the bias and variance trade-off in the process of regularization but also by the inherent information theoretic barriers to obtaining fast enough estimation rates in high dimensional problems. Similar challenges arise when incorporating more flexible machine-learning models to overcome model mis-specifications. Following intuitions parallel to the low-dimensional setting, flexible approaches for confounder adjustments have been proposed via modeling of PS and OR, including <inline-formula><mml:math id=\"M39\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> regularized regression (<xref rid=\"R14\" ref-type=\"bibr\">Farrell, 2015</xref>), neural network (<xref rid=\"R15\" ref-type=\"bibr\">Farrell et al., 2021</xref>), and a general machine learning framework (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>). Several methods accommodated the high dimensional confounder and achieved statistical inference on ATE based on consistent estimation for PS and OR, which translated to proper model specification and sparsity for high-dimensional regressions (<xref rid=\"R6\" ref-type=\"bibr\">Belloni et al., 2013</xref>; <xref rid=\"R30\" ref-type=\"bibr\">Liu et al., 2021</xref>; <xref rid=\"R19\" ref-type=\"bibr\">Hou et al., 2021a</xref>; <xref rid=\"R5\" ref-type=\"bibr\">Belloni et al., 2017</xref>, e.g.). <xref rid=\"R40\" ref-type=\"bibr\">Tan (2020)</xref> proposed a calibrated estimation that leads to valid inference for the average treatment effect even if one of the high-dimensional logistic PS or linear OR model is mis-specified. <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref> formalized the concept of double robustness in high-dimensional setting by defining the sparsity double robustness and model double robustness properties; and also generalized the idea of <xref rid=\"R40\" ref-type=\"bibr\">Tan (2020)</xref> to a wide range of PS and OR models. For data with sample size <inline-formula><mml:math id=\"M40\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> and dimension of covariate <inline-formula><mml:math id=\"M41\" display=\"inline\"><mml:mi>p</mml:mi></mml:math></inline-formula>, <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref> defined the sparsity double robustness as producing <inline-formula><mml:math id=\"M42\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-asymptotic normal estimator for ATE from consistently estimated PS and OR models as long as the <italic toggle=\"yes\">product</italic> of sparsities for PS and OR models grow slower than <inline-formula><mml:math id=\"M43\" display=\"inline\"><mml:mi>n</mml:mi><mml:mspace width=\"0.5em\"/><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. The model double robustness further allows the estimation of either PS or OR model to be inconsistent while still achieving <inline-formula><mml:math id=\"M44\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-asymptotic normal estimation of ATE. <xref rid=\"R7\" ref-type=\"bibr\">Bradic et al. (2019)</xref> established a sharper sparsity double robustness property of the calibrated estimation. Unlike the two-model approaches (PS and OR) listed above, <xref rid=\"R46\" ref-type=\"bibr\">Wang and Shah (2020)</xref> considered a single model approach in which they debiased the regularized PS model in the inverse probability of treatment weighting estimator to achieve <inline-formula><mml:math id=\"M45\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-inference.</p><p id=\"P6\">Semi-supervised estimation for ATE is less studied. Existing literatures focused almost entirely on the setting where <inline-formula><mml:math id=\"M46\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> is observed for patients in the small labeled set of size <inline-formula><mml:math id=\"M47\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> while <inline-formula><mml:math id=\"M48\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and surrogates/proxies of <inline-formula><mml:math id=\"M49\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> along with confounders <inline-formula><mml:math id=\"M50\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula> are observed for all subjects of size <inline-formula><mml:math id=\"M51\" display=\"inline\"><mml:mi>N</mml:mi></mml:math></inline-formula>. The semi-supervised learning (SSL) setting refers to the missing data proportion <inline-formula><mml:math id=\"M52\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> for <inline-formula><mml:math id=\"M53\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> tending to 1 along an asymptotic sequence where both number of labels and total sample size tend to infinity, <inline-formula><mml:math id=\"M54\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>. The SSL setting is distinguished from classical missing data problems as the standard (missing data) positivity assumption on observation rate is violated. SSL estimators for the ATE have been proposed by <xref rid=\"R9\" ref-type=\"bibr\">Cheng et al. (2021)</xref> when <inline-formula><mml:math id=\"M55\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> is missing-completely-at-random (MCAR) and by <xref rid=\"R50\" ref-type=\"bibr\">Zhang et al. (2023)</xref> and <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao (2024)</xref> when <inline-formula><mml:math id=\"M56\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> is missing-at-random (MAR). However, these methods cannot be easily adapted to the setting where both <inline-formula><mml:math id=\"M57\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M58\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> are missing. The missingness in <inline-formula><mml:math id=\"M59\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> is fundamentally different from the missingness in <inline-formula><mml:math id=\"M60\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> since treatment is an internal node in the causal pathway &#8220;confounder <inline-formula><mml:math id=\"M61\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8594;</mml:mo><mml:mtext>treatment</mml:mtext><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8594;</mml:mo><mml:mtext>outcome</mml:mtext><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8594;</mml:mo><mml:mtext>surrogates</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> &#8221; (<xref rid=\"F1\" ref-type=\"fig\">Figure 1</xref>), introducing technical challenges on the projection by conditional expectation in the semi-parametric analysis.</p><p id=\"P7\">In this paper, we propose an efficient and robust SSL estimator for ATE when both <inline-formula><mml:math id=\"M62\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M63\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> are only observed for a small labeled subset but the confounders <inline-formula><mml:math id=\"M64\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula> and surrogates <inline-formula><mml:math id=\"M65\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> for <inline-formula><mml:math id=\"M66\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M67\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> are observed for all <inline-formula><mml:math id=\"M68\" display=\"inline\"><mml:mi>N</mml:mi></mml:math></inline-formula> patients. We derived the SMMAL estimator by first deriving the efficient influence function for the ATE under this double missing SSL setting and then constructing a cross-fitted multiple machine learning estimator. We subsequently provided a formal characterization of semi-parametric efficiency under the double missing SSL setting with the SMMAL estimator coupled to B-spline regressions over low-dimensional space. We also designed a doubly robust estimator with a two-layer cross-fitted calibrated estimation for high-dimensional logistic PS and OR models. Via cross-fitting and a truncation in initial OR/PS predictions, we relaxed the sparsity assumptions in the initial estimation for PS and OR, previously required for <inline-formula><mml:math id=\"M69\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-inference of ATE (<xref rid=\"R40\" ref-type=\"bibr\">Tan, 2020</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>). We further showed that our doubly robust SMMAL estimator attains 1) the rate double robustness when both PS and OR models are correct and 2) model double robustness when one of them is correct, as defined by <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref>. The SMMAL estimator also does not require correct specifications of the imputation models for <inline-formula><mml:math id=\"M70\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id=\"M71\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> for proper inference under MCAR assumption. We summarize our key contributions herein:</p><list list-type=\"order\" id=\"L2\"><list-item><p id=\"P8\">We formalized the efficient estimation under a general SSL setting (including specifically the double missing SSL setting) with a decaying observation rate that violates the classical (missing data) positivity assumption. Our theory justified the efficiency claims of existing works and can provide benchmark for future work in this direction. A discussion regarding the subtleties and challenges involved in this formalization and subsequent analyses can be found in Remark 6.</p></list-item><list-item><p id=\"P9\">We laid out a general approach for efficient SSL with a complex missing data structure. Our general approach is particularly convenient when the missing data and dependence patterns render typical projection approach difficult for the semi-parametric theory. A explanation of the challenge from missing treatment under double missing SSL setting can be found in Remark 4, and the general framework is given in <xref rid=\"S16\" ref-type=\"sec\">Section 7</xref>.</p></list-item><list-item><p id=\"P10\">We made progress in statistical inference on ATE based on the doubly robust estimation with high-dimensional confounders achieving the sparsity/model double robustness. We generalized to the SSL setting the techniques in the existing literatures on calibrated estimation of PS and OR models so that the final ATE estimator has weak dependence on estimation of these models, characterized by small derivatives, also known as the Neyman Orthogonality (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>). Using a truncation of initial model prediction, we removed the sparsity requirement in initial estimation. In addition, we demonstrated that the SSL estimation derived from our modified semi-parametric theory contributed to robustness toward estimation of the imputation models. The comparison with related work can be found in Remark 9.</p></list-item></list><p id=\"P11\">The paper is organized as follows. In <xref rid=\"S2\" ref-type=\"sec\">Section 2</xref>, we introduce our causal inference structure under the double missing SSL setting along with the notations. In <xref rid=\"S3\" ref-type=\"sec\">Section 3</xref>, we first present the efficient influence function, followed by the multiple machine learning estimator and the model multiply robust estimator derived from the efficient influence function. In <xref rid=\"S7\" ref-type=\"sec\">Section 4</xref>, we state the theoretical guarantees of the <inline-formula><mml:math id=\"M72\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-inference on the ATE from our methods, whose proofs are provided in the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>. We also provide the semi-parametric efficiency lower bound for average treatment effect under double missing SSL setting in low-dimensional space. In <xref rid=\"S11\" ref-type=\"sec\">Section 5</xref>, we assess the finite sample performance of our SSL methods and compare them to supervised benchmarks. In <xref rid=\"S15\" ref-type=\"sec\">Section 6</xref>, we apply SMMAL to the real-world evidence study on targeted therapies for metastatic colorectal cancer in comparison with chemotherapy. In <xref rid=\"S16\" ref-type=\"sec\">Section 7</xref>, we offer the efficiency lower bound for general low-dimensional parameters under broader SSL settings with flexible missing data components. In <xref rid=\"S17\" ref-type=\"sec\">Section 8</xref>, we conclude with a brief discussion.</p></sec><sec id=\"S2\"><label>2</label><title>Setting and notation</title><p id=\"P12\">For the <inline-formula><mml:math id=\"M73\" display=\"inline\"><mml:mi>i</mml:mi></mml:math></inline-formula>-th observation in a study of <inline-formula><mml:math id=\"M74\" display=\"inline\"><mml:mi>N</mml:mi></mml:math></inline-formula> subjects, <inline-formula><mml:math id=\"M75\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:math></inline-formula> denotes the outcome variable, <inline-formula><mml:math id=\"M76\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> denotes the treatment group indicator, <inline-formula><mml:math id=\"M77\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula> indicates whether <inline-formula><mml:math id=\"M78\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> is annotated, <inline-formula><mml:math id=\"M79\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> denotes the surrogates for <inline-formula><mml:math id=\"M80\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M81\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M82\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> denotes the vector of potential confounders including 1 as the first element. We use the notations without the subscript indices <inline-formula><mml:math id=\"M83\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula> to denote the generic versions of these random variables. In EHR studies, routine documentations on treatments and outcomes in the form of digital codes and mentions in narrative notes are often prone to errors (<xref rid=\"R49\" ref-type=\"bibr\">Zhang et al., 2019</xref>) and hence can only serve as surrogates <inline-formula><mml:math id=\"M84\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula>. To ascertain <inline-formula><mml:math id=\"M85\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M86\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>, researchers may design the sampling scheme for a representative labeled subset, <inline-formula><mml:math id=\"M87\" display=\"inline\"><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:mo>]</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, over which the exact data <inline-formula><mml:math id=\"M88\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> are annotated by medical experts, where <inline-formula><mml:math id=\"M89\" display=\"inline\"><mml:mo>[</mml:mo><mml:mi>N</mml:mi><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>. For those with <inline-formula><mml:math id=\"M90\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, exact values of the pair <inline-formula><mml:math id=\"M91\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> are not ascertained, creating the joint missingness of <inline-formula><mml:math id=\"M92\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. The observed data consist of <inline-formula><mml:math id=\"M93\" display=\"inline\"><mml:mi>N</mml:mi></mml:math></inline-formula> independent and identically distributed (i.i.d.) random vectors, <inline-formula><mml:math id=\"M94\" display=\"inline\"><mml:mi>&#119967;</mml:mi><mml:mo>=</mml:mo><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, where <inline-formula><mml:math id=\"M95\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p id=\"P13\">We assume the MCAR mechanism for the sampling process with\n<disp-formula id=\"FD1\"><label>(1)</label><mml:math id=\"M96\" display=\"block\"><mml:mi>R</mml:mi><mml:mo>&#10987;</mml:mo><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>\nand the number of labelled sample is <inline-formula><mml:math id=\"M97\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with the proportion of labeled observation being <inline-formula><mml:math id=\"M98\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>(</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> with <inline-formula><mml:math id=\"M99\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> as <inline-formula><mml:math id=\"M100\" display=\"inline\"><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula> while the expected number of labels also grow asymptotically to infinity <inline-formula><mml:math id=\"M101\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>. Under MCAR formulation, the size of labeled subset <inline-formula><mml:math id=\"M102\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> is a random variable asymptotically equivalent to <inline-formula><mml:math id=\"M103\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:math></inline-formula>, as <inline-formula><mml:math id=\"M104\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. We use a simplified notation &#8220;<inline-formula><mml:math id=\"M105\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8781;</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> &#8221;, e.g. <inline-formula><mml:math id=\"M106\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>&#8781;</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:math></inline-formula>, to describe the equivalence in stochastic order, <inline-formula><mml:math id=\"M107\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id=\"M108\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. To better reflect the dependence on labeled set and compare with supervised benchmarks, we use <inline-formula><mml:math id=\"M109\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> instead of <inline-formula><mml:math id=\"M110\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:math></inline-formula> when describing the asymptotic orders. As the exception, we use <inline-formula><mml:math id=\"M111\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:math></inline-formula> in the derivation of efficiency lower bound. Extension to MAR is plausible through modeling and estimating the missing data pattern <inline-formula><mml:math id=\"M112\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> under classical semi-parametric theory, but a few technical and practical challenges exist as we listed in the <xref rid=\"S17\" ref-type=\"sec\">Section 8</xref>. Thorough investigation of the MCAR setting would already provide methodological guidance to the rapidly growing real-world evidence studies in which random subsets are selected for gold-standard validation of intervention and outcome data (<xref rid=\"R23\" ref-type=\"bibr\">Hou et al., 2023</xref>). Our MCAR formulation, as opposed to the two sample formulations in existing statistical semi-supervised learning literatures (<xref rid=\"R8\" ref-type=\"bibr\">Chakrabortty et al., 2019</xref>; <xref rid=\"R50\" ref-type=\"bibr\">Zhang et al., 2023</xref>; <xref rid=\"R20\" ref-type=\"bibr\">Hou et al., 2021b</xref>), connects better with existing literature on semiparametric estimation and missing data. The results under MCAR, with minor modification in theoretical derivations, are largely applicable to the other similar formulation like first <inline-formula><mml:math id=\"M113\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> samples <inline-formula><mml:math id=\"M114\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> or sampling without replacement for a deterministic sequence <inline-formula><mml:math id=\"M115\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula>.</p><p id=\"P14\">To properly define the causally interpretable ATE, we adopt the typical counterfactual outcome framework and its standard assumptions (<xref rid=\"R24\" ref-type=\"bibr\">Imbens and Rubin, 2015</xref>; <xref rid=\"R18\" ref-type=\"bibr\">Hernan and Robins, 2023</xref>). Let <inline-formula><mml:math id=\"M116\" display=\"inline\"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> be the counterfactual outcome with treatment set as <inline-formula><mml:math id=\"M117\" display=\"inline\"><mml:mi>a</mml:mi></mml:math></inline-formula>, for <inline-formula><mml:math id=\"M118\" display=\"inline\"><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>}</mml:mo></mml:math></inline-formula>. The ATE is defined as\n<disp-formula id=\"FD2\"><label>(2)</label><mml:math id=\"M119\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mspace width=\"thickmathspace\"/><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe make the following standard assumptions regarding the triplet <inline-formula><mml:math id=\"M120\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>,</p><p id=\"P15\"><bold>Assumption 1</bold> (<italic toggle=\"yes\">a</italic>) <italic toggle=\"yes\">Consistency:</italic>\n<inline-formula><mml:math id=\"M121\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>;</p><p id=\"P16\">(<italic toggle=\"yes\">b</italic>) <italic toggle=\"yes\">(Causal inference) Positivity of treatment assignment</italic>: <inline-formula><mml:math id=\"M122\" display=\"inline\"><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>&#8804;</mml:mo><mml:mi mathvariant=\"double-struck\">P</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8804;</mml:mo></mml:math></inline-formula>\n<inline-formula><mml:math id=\"M123\" display=\"inline\"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">almost surely for an absolute constant</italic>\n<inline-formula><mml:math id=\"M124\" display=\"inline\"><mml:mi>M</mml:mi><mml:mo>&lt;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>;</p><p id=\"P17\">(<italic toggle=\"yes\">c</italic>) <italic toggle=\"yes\">Ignorability</italic>: <inline-formula><mml:math id=\"M125\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>&#10987;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>.</p><p id=\"P18\">The (causal inference) positivity in Assumption 1 is imposed on the treatment assignment <inline-formula><mml:math id=\"M126\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>, which should be distinguished from the (missing data) positivity regarding the observation indicator <inline-formula><mml:math id=\"M127\" display=\"inline\"><mml:mi>R</mml:mi></mml:math></inline-formula>. Under the Assumption 1, the ATE can be alternatively expressed as\n<disp-formula id=\"FD3\"><label>(3)</label><mml:math id=\"M128\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mspace width=\"thickmathspace\"/><mml:mo>{</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>}</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>\nIn the motivating EHR studies, <inline-formula><mml:math id=\"M129\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the documentations and retrospective data curation of <inline-formula><mml:math id=\"M130\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, such as the presence of diagnosis code in follow-up for outcomes and medication codes at baseline for treatments, that are conceivably determined by the underlying truth <inline-formula><mml:math id=\"M131\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>. In <xref rid=\"F1\" ref-type=\"fig\">Figure 1</xref>, we present a setting such that the surrogates can be classified into those for <inline-formula><mml:math id=\"M132\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and those for <inline-formula><mml:math id=\"M133\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. The causal identification (<xref rid=\"FD3\" ref-type=\"disp-formula\">3</xref>) still holds with the introduction of additional variable <inline-formula><mml:math id=\"M134\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Sometimes <inline-formula><mml:math id=\"M135\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> may contain colliders that are affected by both treatment <inline-formula><mml:math id=\"M136\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and outcome <inline-formula><mml:math id=\"M137\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8592;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, e.g. increased code counts from frequent healthcare visits as part of intense treatment or caused by poor outcome. Adjustment of colliders would distort causal relationship <inline-formula><mml:math id=\"M138\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and should be excluded from causal identification (<xref rid=\"R18\" ref-type=\"bibr\">Hernan and Robins, 2023</xref>, Chapter 6.4). Throughout the paper, we assume MCAR (<xref rid=\"FD1\" ref-type=\"disp-formula\">1</xref>) and Assumption 1.</p><p id=\"P19\"><bold>Remark 1</bold>\n<italic toggle=\"yes\">We herein summarize the setting of our study</italic>.</p><list list-type=\"bullet\" id=\"L4\"><list-item><p id=\"P20\">Over a small randomly sampled labeled subset, we can causally identify ATE with outcome <inline-formula><mml:math id=\"M139\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, binary treatment <inline-formula><mml:math id=\"M140\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and confounders <inline-formula><mml:math id=\"M141\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> under standard consistency, (causal inference) positivity and ignorability assumptions.</p></list-item><list-item><p id=\"P21\">We seek to robustly enhance the efficiency of estimating ATE by incorporating the large unlabeled data containing confounders <inline-formula><mml:math id=\"M142\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and surrogates <inline-formula><mml:math id=\"M143\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> without stringent model assumptions on <inline-formula><mml:math id=\"M144\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The method should adaptively achieve\n<list list-type=\"bullet\" id=\"L6\"><list-item><p id=\"P22\">better efficiency if <inline-formula><mml:math id=\"M145\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can effectively inform <inline-formula><mml:math id=\"M146\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P23\">the same property as the ATE estimated from labeled subset if <inline-formula><mml:math id=\"M147\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> cannot inform <inline-formula><mml:math id=\"M148\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item></list></p></list-item></list></sec><sec id=\"S3\"><label>3</label><title>SMMAL Estimation</title><p id=\"P24\">We start by presenting in <xref rid=\"S4\" ref-type=\"sec\">Section 3.1</xref> the efficient influence function under the double missing SSL setting without assuming known model for unlabeled data. Deviating from the classical missing data setting, the derived efficient influence functions under double missing SSL setting have diverging variances, which requires a formal justification of its connection with efficiency lower bound in <xref rid=\"S9\" ref-type=\"sec\">Sections 4.2</xref>. Our approach is hence distinguished from existing SSL literatures (<xref rid=\"R9\" ref-type=\"bibr\">Cheng et al., 2021</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>) that considered a simplified theoretical formulation to define the efficient influence function assuming known model for large unlabeled data. Then, we discuss the estimation of ATE with different ways of estimating the nuisance models involved in the efficient influence function in <xref rid=\"S5\" ref-type=\"sec\">Section 3.2</xref> for low-dimensional <inline-formula><mml:math id=\"M149\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula> and in <xref rid=\"S6\" ref-type=\"sec\">Section 3.3</xref> for high-dimensional <inline-formula><mml:math id=\"M150\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>. As the standard tool to control over-fitting from using estimated models in subsequent estimation procedures (<xref rid=\"R29\" ref-type=\"bibr\">Lin and Ying, 1994</xref>; <xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>; <xref rid=\"R33\" ref-type=\"bibr\">Newey and Robins, 2018</xref>; <xref rid=\"R20\" ref-type=\"bibr\">Hou et al., 2021b</xref>), cross-fitting is adopted for both settings, where we split the data into <inline-formula><mml:math id=\"M151\" display=\"inline\"><mml:mi>K</mml:mi></mml:math></inline-formula> (e.g. <inline-formula><mml:math id=\"M152\" display=\"inline\"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula>) folds of approximately equal size. For <inline-formula><mml:math id=\"M153\" display=\"inline\"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:math></inline-formula>, we let <inline-formula><mml:math id=\"M154\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denote the index set for the <inline-formula><mml:math id=\"M155\" display=\"inline\"><mml:mi>k</mml:mi></mml:math></inline-formula>th fold of the data with size <inline-formula><mml:math id=\"M156\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and let <inline-formula><mml:math id=\"M157\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo><mml:mo>&#8726;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id=\"M158\" display=\"inline\"><mml:mo>|</mml:mo><mml:mi>&#8464;</mml:mi><mml:mo>|</mml:mo></mml:math></inline-formula> denotes the cardinality of <inline-formula><mml:math id=\"M159\" display=\"inline\"><mml:mi>&#8464;</mml:mi></mml:math></inline-formula>. Here, we do not split the folds separately for labeled and the unlabeled data because the label indicator <inline-formula><mml:math id=\"M160\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is random under the MCAR formulation (<xref rid=\"FD1\" ref-type=\"disp-formula\">1</xref>).</p><sec id=\"S4\"><label>3.1</label><title>The efficient influence function</title><p id=\"P25\">We define the following nuisance models:\n<disp-formula id=\"FD4\"><mml:math id=\"M161\" display=\"block\"><mml:mtable><mml:mtr><mml:mtd columnalign=\"left\"><mml:mtext>PS</mml:mtext><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign=\"left\"><mml:mi mathvariant=\"double-struck\">P</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#960;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>OR</mml:mtext><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign=\"left\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#956;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Imputations</mml:mtext><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign=\"left\"><mml:mi mathvariant=\"double-struck\">P</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#928;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd><mml:mtd/><mml:mtd columnalign=\"left\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>\nWe use the subscript star to indicate the true models, <inline-formula><mml:math id=\"M162\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>. Starting from the efficient influence function with complete (cmp) observation of treatment and outcome (<xref rid=\"R35\" ref-type=\"bibr\">Robins et al., 1994</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>),\n<disp-formula id=\"FD5\"><mml:math id=\"M163\" display=\"block\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>\nwe produced the efficient influence function through the following mapping\n<disp-formula id=\"FD6\"><label>(4)</label><mml:math id=\"M164\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></disp-formula>\n<disp-formula id=\"FD7\"><label>(5)</label><mml:math id=\"M165\" display=\"block\"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:mtext>I</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>\nIn the formula (<xref rid=\"FD6\" ref-type=\"disp-formula\">4</xref>) that produces <inline-formula><mml:math id=\"M166\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> from <inline-formula><mml:math id=\"M167\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the maximal information on ATE from the unlabeled data with a known imputation model, and the second term is the price for training the best imputation model over the labeled data. We provide the rigorous justification of this procedure in <xref rid=\"S7\" ref-type=\"sec\">Section 4</xref>.</p><p id=\"P26\">The efficient influence function in the missing data context is usually derived by projecting an arbitrary initial influence function to the nuisance tangent space (<xref rid=\"R41\" ref-type=\"bibr\">Tsiatis, 2007</xref>). The approach has been applied to the SSL setting with missing outcome by first deriving the efficient influence function under missing data setting and then setting <inline-formula><mml:math id=\"M168\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>N</mml:mi><mml:mo>&#8781;</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> for the SSL setting with very large unlabeled data (<xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>). No formal justification of efficiency has been given in exiting literatures under the semi-supervised setting with <inline-formula><mml:math id=\"M169\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> yet <inline-formula><mml:math id=\"M170\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. Moreover, such standard procedure for deriving efficient influence function under missing data or causal inference settings is usually specific for the assumed dependence structure among variables, reflected by the correspondent chain-rule decomposition of nuisance model tangent space (<xref rid=\"R35\" ref-type=\"bibr\">Robins et al., 1994</xref>; <xref rid=\"R41\" ref-type=\"bibr\">Tsiatis, 2007</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>; <xref rid=\"R9\" ref-type=\"bibr\">Cheng et al., 2021</xref>). For estimating of ATE under SSL setting, existing formulation focused on the surrogates <inline-formula><mml:math id=\"M171\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> that are defined as short-term markers for long-term outcomes <inline-formula><mml:math id=\"M172\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>, represented by the <inline-formula><mml:math id=\"M173\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>Y</mml:mi></mml:math></inline-formula> dependence pattern in causal diagram (<xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>; <xref rid=\"R9\" ref-type=\"bibr\">Cheng et al., 2021</xref>). The generalization to other types of surrogates <inline-formula><mml:math id=\"M174\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> is currently absent. For example, surrogates <inline-formula><mml:math id=\"M175\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> in our motivating EHR studies were imperfect documentations for treatment and outcome variables <inline-formula><mml:math id=\"M176\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, represented by the <inline-formula><mml:math id=\"M177\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> dependence pattern (<xref rid=\"F1\" ref-type=\"fig\">Figure 1</xref>). The shift from <inline-formula><mml:math id=\"M178\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>Y</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id=\"M179\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> also creates the technical challenges in deriving projections to nuisance model tangent space defined according to the dependence pattern (see <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section E2</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>).</p><p id=\"P27\">While our derivation of efficient <inline-formula><mml:math id=\"M180\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> also involved projecting an inefficient <inline-formula><mml:math id=\"M181\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, a common approach among existing literatures (<xref rid=\"R35\" ref-type=\"bibr\">Robins et al., 1994</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>), our approach did not impose stringent assumptions on surrogates <inline-formula><mml:math id=\"M182\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula>. To provide a general theoretical basis consistent across various surrogate mechanism, we established the connection between efficiency lower bound of complete data setting and that of double missing SSL setting through asymptotic local minimax result similar to <xref rid=\"R4\" ref-type=\"bibr\">Begun et al. (1983)</xref> in <xref rid=\"S9\" ref-type=\"sec\">Section 4.2</xref>. Our efficiency lower bound justified projecting complete data efficient influence function <inline-formula><mml:math id=\"M183\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> to derive the SSL efficient influence function <inline-formula><mml:math id=\"M184\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. We further generalized the efficiency theory to other parameters with missing data under SSL setting in <xref rid=\"S16\" ref-type=\"sec\">Section 7</xref>. Our alternative justification only requires 1) the target ATE parameter <inline-formula><mml:math id=\"M185\" display=\"inline\"><mml:mi>&#916;</mml:mi></mml:math></inline-formula> can be identified by <inline-formula><mml:math id=\"M186\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> through <inline-formula><mml:math id=\"M187\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and 2) <inline-formula><mml:math id=\"M188\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> can provide information on (<inline-formula><mml:math id=\"M189\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:math></inline-formula>) when they are not observed over the unlabeled set. Hence, our framework covered a broad range of surrogate mechanism including both the setting considered by <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao (2024)</xref> and the causal diagram in <xref rid=\"F1\" ref-type=\"fig\">Figure 1</xref>.</p></sec><sec id=\"S5\"><label>3.2</label><title>SMMAL Procedure</title><p id=\"P28\">Inspired by the double machine learning estimation (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>) based on <inline-formula><mml:math id=\"M190\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, we propose the following SMMAL estimator for ATE:</p><list list-type=\"order\" id=\"L8\"><list-item><p id=\"P29\">For each labelled fold <inline-formula><mml:math id=\"M191\" display=\"inline\"><mml:mi>k</mml:mi></mml:math></inline-formula>, we estimate the nuisance models by the out-of-fold data <inline-formula><mml:math id=\"M192\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, obtaining <inline-formula><mml:math id=\"M193\" display=\"inline\"><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P30\">Construct the estimated influence functions\n<disp-formula id=\"FD8\"><mml:math id=\"M194\" display=\"block\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext mathvariant=\"italic\">ik</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula>\nand estimate the ATE by\n<disp-formula id=\"FD9\"><label>(6)</label><mml:math id=\"M195\" display=\"block\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext mathvariant=\"italic\">ik</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P32\">Estimate the asymptotic variance of <inline-formula><mml:math id=\"M196\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> by\n<disp-formula id=\"FD10\"><label>(7)</label><mml:math id=\"M197\" display=\"block\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext mathvariant=\"italic\">ik</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item></list><p id=\"P33\">Here we considered the <inline-formula><mml:math id=\"M198\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula> standardized estimation error <inline-formula><mml:math id=\"M199\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> instead of the <inline-formula><mml:math id=\"M200\" display=\"inline\"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></inline-formula> standardized estimation error <inline-formula><mml:math id=\"M201\" display=\"inline\"><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> because the latter is diverging at <inline-formula><mml:math id=\"M202\" display=\"inline\"><mml:msqrt><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt><mml:mo>&#8781;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> rate due to the unbounded variance of <inline-formula><mml:math id=\"M203\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as <inline-formula><mml:math id=\"M204\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. The <inline-formula><mml:math id=\"M205\" display=\"inline\"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math></inline-formula> confidence interval for ATE can be constructed with <inline-formula><mml:math id=\"M206\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M207\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>,\n<disp-formula id=\"FD11\"><mml:math id=\"M208\" display=\"block\"><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfenced></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"M209\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id=\"M210\" display=\"inline\"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> quantile of standard normal distribution.</p><p id=\"P34\">Similar to existing results in double machine learning literature, any estimators for the nuisance models with suitable rates of consistency can be used in our proposal as well. For low-dimensional <inline-formula><mml:math id=\"M211\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula> and smooth nuisance models, we can choose B-spline regression with proper order and degrees. Precise discussions on these rates, related conditions for general estimators and relevant smoothness classes for B-spline regression are listed in <xref rid=\"S9\" ref-type=\"sec\">Section 4.2</xref>.</p></sec><sec id=\"S6\"><label>3.3</label><title>Doubly Robust SMMAL Construction in high-dimensions</title><p id=\"P35\">We next discuss a specific construction of the SMMAL estimator when the dimensions <inline-formula><mml:math id=\"M212\" display=\"inline\"><mml:mi>p</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M213\" display=\"inline\"><mml:mi>q</mml:mi></mml:math></inline-formula> grow with <inline-formula><mml:math id=\"M214\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M215\" display=\"inline\"><mml:mi>p</mml:mi></mml:math></inline-formula> may be larger than <inline-formula><mml:math id=\"M216\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula>. In real-world evidence studies using EHRs, confounding adjustment often involves selection of the few determinants of treatment and risk factors for outcomes from a large number of candidate variables (<xref rid=\"R21\" ref-type=\"bibr\">Hou et al., 2021c</xref>, <xref rid=\"R22\" ref-type=\"bibr\">2022</xref>). We focus on the binary <inline-formula><mml:math id=\"M217\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and put the high-dimensional logistic regression models with link function <inline-formula><mml:math id=\"M218\" display=\"inline\"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math></inline-formula> on the nuisance models\n<disp-formula id=\"FD12\"><label>(8)</label><mml:math id=\"M219\" display=\"block\"><mml:mi>&#960;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#956;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>;</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mi>&#928;</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe denote the derivatives of the link <inline-formula><mml:math id=\"M220\" display=\"inline\"><mml:mi>g</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id=\"M221\" display=\"inline\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#729;</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and the corresponding loss function as <inline-formula><mml:math id=\"M222\" display=\"inline\"><mml:mo>&#8467;</mml:mo><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>log</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:math></inline-formula>. Other types of generalized linear models for OR model <inline-formula><mml:math id=\"M223\" display=\"inline\"><mml:mi>&#956;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> may also be considered and derived similarly. To enhance the robustness against model mis-specification in <inline-formula><mml:math id=\"M224\" display=\"inline\"><mml:mi>&#960;</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M225\" display=\"inline\"><mml:mi>&#956;</mml:mi></mml:math></inline-formula>, we propose a bias-reducing calibration after an initial estimation (<xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>). We added another layer of cross-fitting to reduce the overfitting bias when using initial estimators in the bias-reducing calibration. Compare with the general SMMAL algorithm in <xref rid=\"S5\" ref-type=\"sec\">Section 3.2</xref>, the generic estimation process for nuisance models (Step 1 in <xref rid=\"S5\" ref-type=\"sec\">Section 3.2</xref>) is expanded into the Step 1&#8211;4 of the following SMMAL algorithm for high-dimensional logistic regression. To ensure that estimated PS and OR are bounded away from zero and one, we propose to truncate linear predictors according to a predetermined constant <inline-formula><mml:math id=\"M226\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula> corresponding to a reasonable range for PS and OR probabilities, e.g. <inline-formula><mml:math id=\"M227\" display=\"inline\"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>2.2</mml:mn></mml:math></inline-formula> for range [0.1, 0.9]. Our algorithm for doubly robust SMMAL estimator <inline-formula><mml:math id=\"M228\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> has the following steps:</p><list list-type=\"order\" id=\"L12\"><list-item><p id=\"P36\">For each labelled fold <inline-formula><mml:math id=\"M229\" display=\"inline\"><mml:mi>k</mml:mi></mml:math></inline-formula>, we estimate the imputation models by the Lasso over out-of-fold data <inline-formula><mml:math id=\"M230\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>,\n<disp-formula id=\"FD13\"><label>(9)</label><mml:math id=\"M231\" display=\"block\"><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width=\"2.5em\"/><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#950;</mml:mi></mml:mrow></mml:msub><mml:mo>&#8781;</mml:mo><mml:msqrt><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#950;</mml:mi></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#950;</mml:mi></mml:mrow></mml:msub><mml:mo>&#8781;</mml:mo><mml:msqrt><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt><mml:mo>;</mml:mo></mml:math></disp-formula>\nChoice of the imputation method is flexible (See Remark 10).</p></list-item><list-item><p id=\"P38\">For each labelled fold pair <inline-formula><mml:math id=\"M232\" display=\"inline\"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, we estimate the initial PS and OR models by the Lasso over out-of-two-folds data <inline-formula><mml:math id=\"M233\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#8746;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>,\n<disp-formula id=\"FD14\"><label>(10)</label><mml:math id=\"M234\" display=\"block\"><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>\nwith <inline-formula><mml:math id=\"M235\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mtext>a</mml:mtext><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8781;</mml:mo><mml:msqrt><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P40\">Define the truncation at <inline-formula><mml:math id=\"M236\" display=\"inline\"><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>sign</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mtext>min</mml:mtext><mml:mo>{</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:math></inline-formula>, and its composition with functions <inline-formula><mml:math id=\"M237\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#729;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#729;</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id=\"M238\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mtext>xp</mml:mtext></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mo>(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>. For each labelled fold <inline-formula><mml:math id=\"M239\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, we construct the calibrated losses,\n<disp-formula id=\"FD15\"><label>(11)</label><mml:math id=\"M240\" display=\"block\"><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#729;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>exp</mml:mtext></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>\nand estimate the PS and OR models by cross-fitting within out-of-fold data <inline-formula><mml:math id=\"M241\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>,\n<disp-formula id=\"FD16\"><label>(12)</label><mml:math id=\"M242\" display=\"block\"><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8800;</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8800;</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:mrow><mml:mi>I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder></mml:mrow><mml:mi>I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>\nwith <inline-formula><mml:math id=\"M243\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>&#8781;</mml:mo><mml:msqrt><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>.</p></list-item><list-item><p id=\"P43\">Construct the nuisance model estimators:\n<disp-formula id=\"FD17\"><label>(13)</label><mml:math id=\"M244\" display=\"block\"><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P44\">Estimate the ATE by sending (<xref rid=\"FD17\" ref-type=\"disp-formula\">13</xref>) to (<xref rid=\"FD9\" ref-type=\"disp-formula\">6</xref>), producing <inline-formula><mml:math id=\"M245\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p id=\"P45\">Estimate the variance by sending (<xref rid=\"FD17\" ref-type=\"disp-formula\">13</xref>) and <inline-formula><mml:math id=\"M246\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> to (<xref rid=\"FD10\" ref-type=\"disp-formula\">7</xref>), producing <inline-formula><mml:math id=\"M247\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item></list><p id=\"P46\">The <inline-formula><mml:math id=\"M248\" display=\"inline\"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math></inline-formula> confidence interval for ATE can be constructed with <inline-formula><mml:math id=\"M249\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M250\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>,\n<disp-formula id=\"FD18\"><mml:math id=\"M251\" display=\"block\"><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:msqrt></mml:mrow></mml:mfenced></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"M252\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119989;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the <inline-formula><mml:math id=\"M253\" display=\"inline\"><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> quantile of standard normal distribution.</p><p id=\"P47\">The calibrated losses (<xref rid=\"FD15\" ref-type=\"disp-formula\">11</xref>) aim to estimate OR and PS models by approximately solving the equations of the partial derivatives of <inline-formula><mml:math id=\"M254\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> with respect to PS and OR models being zero (<xref rid=\"R40\" ref-type=\"bibr\">Tan, 2020</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>). The correctly specified model will be recovered as it can be identified by the same equation. Even with mis-specified model, <inline-formula><mml:math id=\"M255\" display=\"inline\"><mml:mi>&#916;</mml:mi></mml:math></inline-formula> will be insensitive to estimation errors in OR and PS models, guaranteed by the small partial derivatives. The property is referred to as the <italic toggle=\"yes\">Neyman orthogonality</italic> (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>), which produces <inline-formula><mml:math id=\"M256\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula> asymptotic normal estimator with sub <inline-formula><mml:math id=\"M257\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula> rate nuisance model estimations. We didn&#8217;t use imputations to improve estimation of OR and PS models because there is no asymptotic efficiency gain due to Neyman orthogonality but potential risk of introducing bias.</p><p id=\"P48\">To control the overfitting bias from the sequential estimation process with 3 steps <inline-formula><mml:math id=\"M258\" display=\"inline\"><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>a,init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8594;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8594;</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, we propose the two-level cross-fitting for learning ATE in (<xref rid=\"FD14\" ref-type=\"disp-formula\">10</xref>) and (<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>), previously considered for semi-supervised learning of high-dimensional regression in (<xref rid=\"R20\" ref-type=\"bibr\">Hou et al., 2021b</xref>). The two-level cross-fitting has the advantage of having larger training set for each Lasso (using <inline-formula><mml:math id=\"M259\" display=\"inline\"><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> folds) compared to the averaging after data splitting (using <inline-formula><mml:math id=\"M260\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> folds) in <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref>. If we choose <inline-formula><mml:math id=\"M261\" display=\"inline\"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>, we are able to use at least 80% data while the data splitting in <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref> may only use 45% data. Larger training sample typically allows the choice of smaller penalty factor thus reducing the bias. Taking averaging after data splitting, however, cannot reduce bias.</p><p id=\"P49\">The truncation <inline-formula><mml:math id=\"M262\" display=\"inline\"><mml:mi>&#964;</mml:mi></mml:math></inline-formula> at <inline-formula><mml:math id=\"M263\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula> in (<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>) secured the (causal inference) positivity property of the initially estimated models with no compromise in estimation accuracy. Truncation of PS has been commonly invoked in practice when (causal inference) positivity holds in principle but is violated practically by estimated PS (<xref rid=\"R34\" ref-type=\"bibr\">Petersen et al., 2012</xref>; <xref rid=\"R25\" ref-type=\"bibr\">Ju et al., 2019</xref>). Our method generalized the truncation to OR prediction for binary outcome and identified a novel theoretical property of relaxing sparsity requirement for initial Lasso with the truncation. When the initial estimated model <inline-formula><mml:math id=\"M264\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is consistent for true models satisfying the (causal inference) positivity conditions, i. e. <inline-formula><mml:math id=\"M265\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> such that <inline-formula><mml:math id=\"M266\" display=\"inline\"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8804;</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>, truncation at <inline-formula><mml:math id=\"M267\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula> brings the <inline-formula><mml:math id=\"M268\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> closer to <inline-formula><mml:math id=\"M269\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> (See Lemma A20 in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>). Otherwise, the truncation always ensure <inline-formula><mml:math id=\"M270\" display=\"inline\"><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mtext>xp</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8804;</mml:mo><mml:msub><mml:mrow><mml:mtext>exp</mml:mtext></mml:mrow><mml:mrow><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mtext>exp</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Then, estimating calibrated OR coefficients <inline-formula><mml:math id=\"M271\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is a weighted <inline-formula><mml:math id=\"M272\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> penalized regression with bound weights independent of the responses, which we have shown to be consistent under mild assumptions (see <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section D1</xref> in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>). Same argument applies to truncation of <inline-formula><mml:math id=\"M273\" display=\"inline\"><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. Besides numerical stability, we can remove the sparsity condition associated with the initial estimator of the mis-specified model.</p></sec></sec><sec id=\"S7\"><label>4</label><title>Theoretical Properties of the SMMAL</title><p id=\"P50\">We established the <inline-formula><mml:math id=\"M274\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-consistency of <inline-formula><mml:math id=\"M275\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and the honest asymptotic coverage of the confidence intervals with consistent estimation of PS and OR models in <xref rid=\"S8\" ref-type=\"sec\">Section 4.1</xref>. In <xref rid=\"S9\" ref-type=\"sec\">Section 4.2</xref>, we derived the asymptotic distribution of the SMMAL estimator <inline-formula><mml:math id=\"M276\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and the subsequent matching lower bound to show its semi-parametric efficiency in the low-dimensional <inline-formula><mml:math id=\"M277\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula> case while using B -spline series estimators for nuisance regression models. For high-dimensional sub-Gaussian <inline-formula><mml:math id=\"M278\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M279\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula> and sparse nuisance models, we demonstrated in <xref rid=\"S10\" ref-type=\"sec\">Section 4.3</xref> that <inline-formula><mml:math id=\"M280\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is <italic toggle=\"yes\">adaptively</italic> sparsity/model doubly robust with sparse nuisance models (<xref rid=\"R36\" ref-type=\"bibr\">Rotnitzky et al., 2020</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>): sparsity doubly robust when both OR and PS are correctly specified; model doubly robust when one of OR or PS is correctly specified.</p><sec id=\"S8\"><label>4.1</label><title><inline-formula><mml:math id=\"M281\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-inference</title><p id=\"P51\">We require the following assumptions for nuisance models and the machine-learning estimators. We denote the true propensity score as <inline-formula><mml:math id=\"M282\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> and outcome regression as <inline-formula><mml:math id=\"M283\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. As we do not require consistency of imputation models, we denote <inline-formula><mml:math id=\"M284\" display=\"inline\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id=\"M285\" display=\"inline\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover></mml:math></inline-formula> as the potentially biased asymptotic limits of the estimated imputation models.</p><p id=\"P52\"><bold>Assumption 2</bold>\n<italic toggle=\"yes\">For a fixed constant</italic>\n<inline-formula><mml:math id=\"M286\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">we assume</italic></p><list list-type=\"alpha-lower\" id=\"L16\"><list-item><p id=\"P53\"><italic toggle=\"yes\">(Bounded response) almost surely</italic>\n<inline-formula><mml:math id=\"M287\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P54\"><italic toggle=\"yes\">(Causal Inference Positivity) almost surely</italic>\n<inline-formula><mml:math id=\"M288\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P55\"><italic toggle=\"yes\">(Bounded estimators) almost surely</italic>\n<disp-formula id=\"FD19\"><mml:math id=\"M289\" display=\"block\"><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:mspace width=\"thickmathspace\"/><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mspace width=\"thickmathspace\"/><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:munder><mml:mtext>max</mml:mtext><mml:mspace width=\"thickmathspace\"/><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P56\"><italic toggle=\"yes\">(Rate of estimation)</italic>\n<disp-formula id=\"FD20\"><mml:math id=\"M290\" display=\"block\"><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">for some</italic>\n<inline-formula><mml:math id=\"M291\" display=\"inline\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M292\" display=\"inline\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover></mml:math></inline-formula>\n<italic toggle=\"yes\">satisfying</italic>\n<inline-formula><mml:math id=\"M293\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:mtext>max</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">where for two models</italic>\n<inline-formula><mml:math id=\"M294\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M295\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, <italic toggle=\"yes\">we define</italic>\n<disp-formula id=\"FD21\"><label>(14)</label><mml:math id=\"M296\" display=\"block\"><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>{</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mspace width=\"thickmathspace\"/><mml:msqrt><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:msqrt><mml:mo>.</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">Here we use the</italic>\n<inline-formula><mml:math id=\"M297\" display=\"inline\"><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>-<italic toggle=\"yes\">norm notation because the mean squared error (MSE) thus defined correspond to the</italic>\n<inline-formula><mml:math id=\"M298\" display=\"inline\"><mml:msub><mml:mrow><mml:mo>&#8467;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>-<italic toggle=\"yes\">estimation error for model coefficients under parametric models</italic>.</p></list-item><list-item><p id=\"P59\"><italic toggle=\"yes\">(Stable variance)</italic>\n<disp-formula id=\"FD22\"><mml:math id=\"M299\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"\" separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:mi>Y</mml:mi><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mfenced open=\"\" close=\"]\" separators=\"|\"><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mi mathvariant=\"normal\">&#8254;</mml:mi></mml:mover><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item></list><p id=\"P60\">We established the validity and asymptotic distribution of <inline-formula><mml:math id=\"M300\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> in the following theorem.</p><p id=\"P61\"><bold>Theorem 2</bold>\n<italic toggle=\"yes\">Under Assumption 2</italic>,\n<disp-formula id=\"FD23\"><mml:math id=\"M301\" display=\"block\"><mml:msqrt><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8669;</mml:mo><mml:mi>N</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">where</italic> &#8220; <inline-formula><mml:math id=\"M302\" display=\"inline\"><mml:mo>&#8669;</mml:mo></mml:math></inline-formula> &#8221; <italic toggle=\"yes\">denotes convergence in distribution</italic>.</p><p id=\"P62\">Assumption 2 a guarantees the boundedness of all nuisance models. When <inline-formula><mml:math id=\"M303\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> is binary, the models <inline-formula><mml:math id=\"M304\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M305\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> are all bounded by one. Assumption 2b is equivalent to the standard (causal inference) positivity condition <inline-formula><mml:math id=\"M306\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula> as in Assumption 1b. Assumption 2c can be guaranteed by truncation of nuisance model estimators at <inline-formula><mml:math id=\"M307\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>, which would not compromise the estimation accuracy under Assumptions 2a and 2b. Assumption 2e ensures the proper scaling of the asymptotic variance of <inline-formula><mml:math id=\"M308\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. As noted following (<xref rid=\"FD10\" ref-type=\"disp-formula\">7</xref>), the term with the <inline-formula><mml:math id=\"M309\" display=\"inline\"><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> factor from labeled data in <inline-formula><mml:math id=\"M310\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> dominates its variance if <inline-formula><mml:math id=\"M311\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. The rate condition for the PS and OR models in Assumption 2d matches those for the double machine-learning estimator proposed in <xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al. (2018)</xref> if applied to the complete data subset of size <inline-formula><mml:math id=\"M312\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula>. Under MCAR by design, the missing data mechanism is known a priori, which we utilized to accommodate the mis-specified imputation models estimated at an arbitrarily slow rate.</p><p id=\"P63\"><bold>Remark 3</bold>\n<italic toggle=\"yes\">Compared to existing work on semi-supervised estimation of ATE</italic> (<xref rid=\"R9\" ref-type=\"bibr\">Cheng et al., 2021</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>) <italic toggle=\"yes\">approximating the</italic>\n<inline-formula><mml:math id=\"M313\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>\n<italic toggle=\"yes\">setting by the</italic>\n<inline-formula><mml:math id=\"M314\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>\n<italic toggle=\"yes\">setting, our SMMAL incorporates additionally the uncertainty from large yet finite unlabeled data through</italic> (<xref rid=\"FD7\" ref-type=\"disp-formula\">5</xref>)&#8211;(<xref rid=\"FD10\" ref-type=\"disp-formula\">7</xref>). <italic toggle=\"yes\">As the result, the inference from SMMAL has two methodological advantages. First, by harmonizing the</italic>\n<inline-formula><mml:math id=\"M315\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>&#8810;</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M316\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>&#8781;</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">settings, users may use the same SMMAL procedure without choosing from two setting-specific approaches</italic> (<xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>). <italic toggle=\"yes\">Especially, it seems implausible to decide the asymptotic limit of</italic>\n<inline-formula><mml:math id=\"M317\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">by a single realization of the data. Second, the uncertainty of</italic>\n<inline-formula><mml:math id=\"M318\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">consists of the uncertainty from labeled data and the uncertainty from large but finite unlabeled data</italic>,\n<disp-formula id=\"FD24\"><mml:math id=\"M319\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder accentunder=\"false\"><mml:mrow><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">from</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">labeled</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">set</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder accentunder=\"false\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">from</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">labeled</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext mathvariant=\"italic\">set</mml:mtext></mml:mrow></mml:munder><mml:mo>.</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">Unlike existing work</italic> (<xref rid=\"R9\" ref-type=\"bibr\">Cheng et al., 2021</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>) <italic toggle=\"yes\">that only considered</italic>\n<inline-formula><mml:math id=\"M320\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">from labeled set, our SMMAL variance estimation captures both</italic>\n<inline-formula><mml:math id=\"M321\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M322\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">by involving estimated influence functions</italic>\n<inline-formula><mml:math id=\"M323\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext mathvariant=\"italic\">ik</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">for all observations so that SMMAL is expected to have less issues in underestimation of uncertainty particularly from unlabeled data with a moderately small</italic>\n<inline-formula><mml:math id=\"M324\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">in practice</italic>.</p></sec><sec id=\"S9\"><label>4.2</label><title>Semi-parametric efficiency with low-dimensional confounder</title><p id=\"P64\">We next formally establish the semi-parametric efficiency lower bound under the double missing SSL setting. Consider the non-parametric model for <inline-formula><mml:math id=\"M325\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>\n<disp-formula id=\"FD25\"><mml:math id=\"M326\" display=\"block\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open=\"{\" close=\"\" separators=\"|\"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>:</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mfenced open=\"\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>f</mml:mi><mml:mspace width=\"0.5em\"/><mml:mtext>is</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>density</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>over</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mi>&#119986;</mml:mi><mml:mo>&#8855;</mml:mo><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:mi>&#119988;</mml:mi><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:mtext>and</mml:mtext><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy=\"true\">&#8747;</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>&#119988;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>\nfor some measures <inline-formula><mml:math id=\"M327\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id=\"M328\" display=\"inline\"><mml:mi>&#119988;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id=\"M329\" display=\"inline\"><mml:mi>&#119986;</mml:mi></mml:math></inline-formula> and\n<disp-formula id=\"FD26\"><mml:math id=\"M330\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"M331\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#119964;</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the counting/Dirac measure over the set <inline-formula><mml:math id=\"M332\" display=\"inline\"><mml:mi>&#119964;</mml:mi></mml:math></inline-formula>. Elements in <inline-formula><mml:math id=\"M333\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> can be indexed by the density <inline-formula><mml:math id=\"M334\" display=\"inline\"><mml:mi>f</mml:mi></mml:math></inline-formula>, and we denote the true density as <inline-formula><mml:math id=\"M335\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and the true model <inline-formula><mml:math id=\"M336\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p id=\"P65\"><bold>Remark 4</bold>\n<italic toggle=\"yes\">In existing work on ATE</italic> (<xref rid=\"R35\" ref-type=\"bibr\">Robins et al., 1994</xref>; <xref rid=\"R26\" ref-type=\"bibr\">Kallus and Mao, 2024</xref>), <italic toggle=\"yes\">the model</italic>\n<inline-formula><mml:math id=\"M337\" display=\"inline\"><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">provides no information on the</italic>\n<inline-formula><mml:math id=\"M338\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">and thus would not be included in the nuisance tangent space. In our setting, however, the surrogates</italic>\n<inline-formula><mml:math id=\"M339\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">induced a correlation between subspaces corresponding to</italic>\n<inline-formula><mml:math id=\"M340\" display=\"inline\"><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M341\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">in the nuisance tangent space, which indicates that</italic>\n<inline-formula><mml:math id=\"M342\" display=\"inline\"><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">provides information on the</italic>\n<inline-formula><mml:math id=\"M343\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">through the unlabelled data. As the result, the geometry of the model tangent space is more complex, and the projection can no longer be obtained through simple conditional expectation. See</italic>\n<xref rid=\"SD1\" ref-type=\"supplementary-material\">Section E2</xref>\n<italic toggle=\"yes\">of the</italic>\n<xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>\n<italic toggle=\"yes\">for details</italic>.</p><p id=\"P66\">We denote the total variation norm as <inline-formula><mml:math id=\"M344\" display=\"inline\"><mml:mo>&#8214;</mml:mo><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mtext>TV</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. In the following theorem, we establish the semi-parametric efficiency lower bound for <inline-formula><mml:math id=\"M345\" display=\"inline\"><mml:mi>&#916;</mml:mi></mml:math></inline-formula> under <inline-formula><mml:math id=\"M346\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> in the form of a local minimax theorem obtained in the spirit of <xref rid=\"R4\" ref-type=\"bibr\">Begun et al. (1983)</xref>.</p><p id=\"P67\"><bold>Theorem 5</bold>\n<italic toggle=\"yes\">Under Assumptions 2a, 2c and 2e, we have</italic>\n<disp-formula id=\"FD27\"><mml:math id=\"M347\" display=\"block\"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext><mml:mo>&#8198;</mml:mo><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext><mml:mo>&#8198;</mml:mo><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>TV</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>c</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">&#8747;</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:msup><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8719;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant=\"italic\">RY</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">RA</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula>\n<bold>Remark 6</bold>\n<italic toggle=\"yes\">Theorem 5 offers one example that the semi-parametric efficiency bound (SEB) derived under the classical missing data setting can be generalized to the double missing SSL setting with</italic>\n<inline-formula><mml:math id=\"M348\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>\n<italic toggle=\"yes\">while</italic>\n<inline-formula><mml:math id=\"M349\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>. <italic toggle=\"yes\">Later in</italic>\n<xref rid=\"S16\" ref-type=\"sec\">Section 7</xref>, <italic toggle=\"yes\">we present Theorem 13 for general SSL setting (including specifically the double missing SSL setting). Previous attempts to formalize semi-parametric efficiency in a the SSL settings have assumed that the entire distribution of</italic>\n<inline-formula><mml:math id=\"M350\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">is known, i.e</italic>. <inline-formula><mml:math id=\"M351\" display=\"inline\"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M352\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. <italic toggle=\"yes\">Under the simplified SSL setting with</italic>\n<inline-formula><mml:math id=\"M353\" display=\"inline\"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">the SEB can be derived by straightforward applications of standard results in classical semiparametric literature &#8211; see e.g.</italic>\n<xref rid=\"R44\" ref-type=\"bibr\">van der Vaart (1998)</xref>. <italic toggle=\"yes\">Indeed, another possible consideration for choosing this simplified formulation version is the ambiguity of defining regular estimators without (missing data) positivity assumption and thereby formalizing efficiency through the calibration of the best regular estimator. We bypassed this conceptual difficulty by providing the alternative characterization based on local asymptotic minimax theory &#8211; which may operate on all possible estimators instead of restricting to the class of regular procedures</italic>.</p><p id=\"P68\">Utilizing the correlation structure induced by the projection\n<disp-formula id=\"FD28\"><mml:math id=\"M354\" display=\"block\"><mml:mtext>Cov</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mtext>Cov</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>\nwe obtain the limiting lower bound in Theorem 5 when <inline-formula><mml:math id=\"M355\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> that matches the asymptotic variance of the labeled data component in <inline-formula><mml:math id=\"M356\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>,\n<disp-formula id=\"FD29\"><mml:math id=\"M357\" display=\"block\"><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant=\"italic\">RY</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">RA</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:munder accentunder=\"false\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:munder accentunder=\"false\"><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mrow><mml:munder accentunder=\"false\"><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mo>&#9183;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mspace linebreak=\"newline\"/><mml:mo>=</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nFrom the representation above, we showed that the efficiency gain from the unlabeled data with surrogates is given by the variance of the <inline-formula><mml:math id=\"M358\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> explained by the surrogates and confounders. The efficiency gain based on semi-parametric efficiency theory typically requires consistent estimation of nuisance models. Under mis-specified imputation models, there is no general guarantee on efficiency gain. In Discussion (<xref rid=\"S17\" ref-type=\"sec\">Section 8</xref>), we offered efficient linear combination as the backup plan when quality of estimated nuisance models is in doubt.</p><p id=\"P69\">The key idea of the proof is to construct the two-dimensional least favorable perturbation in an asymmetric neighborhood with different size in two directions. The first direction is proportional to <inline-formula><mml:math id=\"M359\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and of <inline-formula><mml:math id=\"M360\" display=\"inline\"><mml:mtext>size</mml:mtext><mml:mo>&#8781;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:mo>&#8781;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>, which reflects the level of the information on <inline-formula><mml:math id=\"M361\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> from the labels and should naturally scale with the number of expected labels. The second direction is proportional to <inline-formula><mml:math id=\"M362\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and of <inline-formula><mml:math id=\"M363\" display=\"inline\"><mml:mtext>size</mml:mtext><mml:mo>&#8781;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math></inline-formula>, which reflects the level of the information on <inline-formula><mml:math id=\"M364\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> from the unlabelled data and should scale with the total sample size. The design of the different scales ensured the tightness of log-likelihood ratio between the perturbed and the true models, which would otherwise be degenerating or diverging.</p><p id=\"P70\">We next show that the lower bound is attained under low-dimensional smoothness class models for the nuisance functions and can be operationalized by feeding B-spline regressions to <inline-formula><mml:math id=\"M365\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. Suppose the confounders and surrogates are bounded continuous variables of fixed dimension, <inline-formula><mml:math id=\"M366\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>d</mml:mi><mml:mo>&#8781;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. We measure the smoothness of the models by <inline-formula><mml:math id=\"M367\" display=\"inline\"><mml:mi>&#8459;</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> the H&#246;lder class defined in Definition A22, <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section E</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>.</p><p id=\"P71\"><bold>Assumption 3</bold>\n<italic toggle=\"yes\">For a fixed constant</italic>\n<inline-formula><mml:math id=\"M368\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">we assume</italic></p><list list-type=\"alpha-lower\" id=\"L20\"><list-item><p id=\"P72\"><italic toggle=\"yes\">(Bounded density) the density functions for</italic>\n<inline-formula><mml:math id=\"M369\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M370\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M371\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, <italic toggle=\"yes\">are bounded and bounded from zero</italic>,\n<disp-formula id=\"FD30\"><mml:math id=\"M372\" display=\"block\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mo>&#8704;</mml:mo><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mo>&#8704;</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P73\"><italic toggle=\"yes\">(Smooth models) the smoothness of the nuisance models observe</italic>\n<disp-formula id=\"FD31\"><mml:math id=\"M373\" display=\"block\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">for</italic>\n<inline-formula><mml:math id=\"M374\" display=\"inline\"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:math></inline-formula>.</p></list-item></list><p id=\"P75\"><bold>Corollary 7</bold>\n<italic toggle=\"yes\">Under Assumptions 2a, 2b, 2e and 3, we may choose B-spline regressions with order</italic>\n<disp-formula id=\"FD32\"><mml:math id=\"M375\" display=\"block\"><mml:mi>&#954;</mml:mi><mml:mo>&#8805;</mml:mo><mml:mtext>max</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">degrees</italic>\n<disp-formula id=\"FD33\"><mml:math id=\"M376\" display=\"block\"><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>&#960;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:mi>&#956;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mi>&#928;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#8459;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></disp-formula>\n<italic toggle=\"yes\">and truncation at</italic>\n<inline-formula><mml:math id=\"M377\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">for</italic>\n<inline-formula><mml:math id=\"M378\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">to achieve</italic>\n<disp-formula id=\"FD34\"><mml:math id=\"M379\" display=\"block\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant=\"italic\">RY</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">RA</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:msqrt><mml:mo>&#8669;</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>\nCorollary 7 is special case of Theorem 2 under smooth models estimated by standard non-parametric estimation. By Corollary 7, the asymptotic MSE of <inline-formula><mml:math id=\"M380\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id=\"M381\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>&#8781;</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula>, matching the lower bound established in Theorem 5. Therefore, we have justified the semi-parametric efficiency of <inline-formula><mml:math id=\"M382\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. At the same time, the lower bound in Theorem 5 is the sharp semi-parametric efficiency bound for <inline-formula><mml:math id=\"M383\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> under double missing SSL setting <inline-formula><mml:math id=\"M384\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>.</p></sec><sec id=\"S10\"><label>4.3</label><title>Doubly robustness with high-dimensional confounder</title><p id=\"P76\">To describe the sparsity/model double robustness of <inline-formula><mml:math id=\"M385\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>, we define the asymptotic limits for Lasso estimators in (<xref rid=\"FD13\" ref-type=\"disp-formula\">9</xref>)&#8211;(<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>) under potentially mis-specified models.\n<disp-formula id=\"FD35\"><label>(15)</label><mml:math id=\"M386\" display=\"block\"><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mo>&#729;</mml:mo></mml:mover><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mo>,</mml:mo><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mtext>xp</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8467;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>\nWe use <inline-formula><mml:math id=\"M387\" display=\"inline\"><mml:mo>&#8214;</mml:mo><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to denote the sparsity of a vector and <inline-formula><mml:math id=\"M388\" display=\"inline\"><mml:mo>&#8214;</mml:mo><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#968;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> denote the sub-Gaussian norm for random variables or vectors. The sparsities of coefficients for OR <inline-formula><mml:math id=\"M389\" display=\"inline\"><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and PS <inline-formula><mml:math id=\"M390\" display=\"inline\"><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> models reflect the numbers of true determinants for the treatment and outcomes, including the true confounders that must be adjusted for. The detailed definition is given in Definition A23, <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section E</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>.</p><p id=\"P77\"><bold>Assumption 4</bold>\n<italic toggle=\"yes\">For constant</italic>\n<inline-formula><mml:math id=\"M391\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">independent of dimensions</italic>\n<inline-formula><mml:math id=\"M392\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:math></inline-formula>,</p><list list-type=\"alpha-lower\" id=\"L24\"><list-item><p id=\"P78\"><italic toggle=\"yes\">(Sub-Gaussian and bounded covariates) the vector of confounders and surrogates is sub-Gaussian,</italic>\n<inline-formula><mml:math id=\"M393\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold\">v</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#968;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">and coordinate-wisely bounded</italic>\n<inline-formula><mml:math id=\"M394\" display=\"inline\"><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">almost surely</italic>;</p></list-item><list-item><p id=\"P79\"><italic toggle=\"yes\">(Identifiability) the variance of</italic>\n<inline-formula><mml:math id=\"M395\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">is invertible</italic>\n<inline-formula><mml:math id=\"M396\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold\">v</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mtext>Var</mml:mtext><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mi mathvariant=\"bold\">v</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P80\"><italic toggle=\"yes\">(Causal Inference positivity) the true propensity scores and the asymptotic predictions of all models are bounded away from zero and one, almost surely</italic>,\n<disp-formula id=\"FD36\"><mml:math id=\"M397\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mtext>max</mml:mtext><mml:mspace width=\"thickmathspace\"/><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P81\"><italic toggle=\"yes\">and one of the following:</italic>\n<list list-type=\"roman-lower\" id=\"L28\"><list-item><p id=\"P82\"><italic toggle=\"yes\">(PS correct) the propensity model is correct</italic>, <inline-formula><mml:math id=\"M398\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>\n<italic toggle=\"yes\">and the dimensions satisfy</italic>\n<disp-formula id=\"FD37\"><label>(16)</label><mml:math id=\"M399\" display=\"block\"><mml:mfrac><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P83\"><italic toggle=\"yes\">(OR correct) the OR model is correct</italic>, <inline-formula><mml:math id=\"M400\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>\n<italic toggle=\"yes\">and the dimensions satisfy</italic>\n<disp-formula id=\"FD38\"><label>(17)</label><mml:math id=\"M401\" display=\"block\"><mml:mfrac><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0, 1</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item><list-item><p id=\"P84\"><italic toggle=\"yes\">(both correct) both models are correct</italic>, <inline-formula><mml:math id=\"M402\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula><mml:math id=\"M403\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8739;</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo></mml:math></inline-formula>\n<inline-formula><mml:math id=\"M404\" display=\"inline\"><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>\n<italic toggle=\"yes\">and the dimensions satisfy</italic>\n<disp-formula id=\"FD39\"><label>(18)</label><mml:math id=\"M405\" display=\"block\"><mml:mfrac><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#958;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"false\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#950;</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width=\"thickmathspace\"/><mml:mtext>log</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>;</mml:mo></mml:math></disp-formula></p></list-item></list></p></list-item></list><p id=\"P85\"><bold>Theorem 8</bold>\n<italic toggle=\"yes\">Under Assumption 4,</italic>\n<inline-formula><mml:math id=\"M406\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">converges in distribution to a normal random variable at</italic>\n<inline-formula><mml:math id=\"M407\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula>-<italic toggle=\"yes\">rate</italic>,\n<disp-formula id=\"FD40\"><mml:math id=\"M408\" display=\"block\"><mml:msqrt><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:msqrt><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8669;</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0, 1</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>\n<italic toggle=\"yes\">where</italic> &#8220; <inline-formula><mml:math id=\"M409\" display=\"inline\"><mml:mo>&#8669;</mml:mo></mml:math></inline-formula> &#8221; <italic toggle=\"yes\">denotes convergence in the distribution</italic>.</p><p id=\"P86\">Besides the double robustness toward PS and OR, <inline-formula><mml:math id=\"M410\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is additionally robust to the imputation models. Similar to the general Theorem 2, we utilized the known missing data mechanism under MCAR by design to allow model mis-specifications on the imputation.</p><p id=\"P87\"><bold>Remark 9</bold>\n<italic toggle=\"yes\">Regarding the PS model and OR model estimated over the labeled data of size</italic>\n<inline-formula><mml:math id=\"M411\" display=\"inline\"><mml:mi>n</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">our</italic>\n<inline-formula><mml:math id=\"M412\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">is both rate doubly robust</italic>\n<xref rid=\"R36\" ref-type=\"bibr\">Rotnitzky et al. (2020)</xref>\n<italic toggle=\"yes\">and model doubly robust</italic> (<xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>). <italic toggle=\"yes\">When both models are correct, the dimension condition</italic> (<xref rid=\"FD39\" ref-type=\"disp-formula\">18</xref>) <italic toggle=\"yes\">for the PS model and OR model in Assumption 4d-iii satisfies the condition for rate doubly robust, i.e. each sparsity obeying</italic>\n<inline-formula><mml:math id=\"M413\" display=\"inline\"><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>\n<italic toggle=\"yes\">and their product satisfying</italic>\n<inline-formula><mml:math id=\"M414\" display=\"inline\"><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#945;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#946;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#8810;</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. <italic toggle=\"yes\">In the case of only one model is correct, our</italic>\n<inline-formula><mml:math id=\"M415\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">can still provide <inline-formula><mml:math id=\"M416\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></inline-formula></italic>-<italic toggle=\"yes\">inference, thus being model doubly robust. By the truncation</italic>\n<inline-formula><mml:math id=\"M417\" display=\"inline\"><mml:mi>&#964;</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">in</italic> (<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>), <italic toggle=\"yes\">we are able to completely remove the sparsity requirement of the mis-specified initial model under the (causal inference) positivity condition of Assumption 4c. The general framework of</italic>\n<xref rid=\"R37\" ref-type=\"bibr\">Smucler et al. (2019)</xref>\n<italic toggle=\"yes\">would require all models in</italic> (<xref rid=\"FD35\" ref-type=\"disp-formula\">15</xref>) <italic toggle=\"yes\">being sparse</italic>.</p><p id=\"P88\"><bold>Remark 10</bold>\n<italic toggle=\"yes\">As the correct model specification is only required for OR or PS in Assumption 4d, the validity of Theorem 8 does not rely on the consistency of imputation models based on the MCAR missing data mechanism by design. Therefore, the choice on imputation methods</italic> (<xref rid=\"FD13\" ref-type=\"disp-formula\">9</xref>) <italic toggle=\"yes\">can be flexible. If preliminary evidence suggests that certain element in</italic>\n<inline-formula><mml:math id=\"M418\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">contains the most information such as</italic>\n<inline-formula><mml:math id=\"M419\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">for</italic>\n<inline-formula><mml:math id=\"M420\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">and</italic>\n<inline-formula><mml:math id=\"M421\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">for</italic>\n<inline-formula><mml:math id=\"M422\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>, <italic toggle=\"yes\">we can remove penalty for the associated coefficients or simply run the low-dimensional regressions</italic>\n<inline-formula><mml:math id=\"M423\" display=\"inline\"><mml:mi>A</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M424\" display=\"inline\"><mml:mi>Y</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">S</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p id=\"P89\"><bold>Remark 11</bold>\n<italic toggle=\"yes\">We presented the theory according to the exact sparsity in Assumption 4d-iii for two considerations. First, the exact sparsity has a clear interpretation that classifies the covariates into relevant signals and irrelevant noises, about which domain experts may have a preliminary evaluation in applications. Second, the exact sparsity facilitates direct comparison with many related literatures have used exact sparsity to measure the local efficiency or robustness of their proposed methods</italic> (<xref rid=\"R14\" ref-type=\"bibr\">Farrell, 2015</xref>; <xref rid=\"R40\" ref-type=\"bibr\">Tan, 2020</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>; <xref rid=\"R50\" ref-type=\"bibr\">Zhang et al., 2023</xref>). <italic toggle=\"yes\">The exact sparsity in Assumption 4d-iii can be substituted by other conditions that produce the appropriate estimation rate in the more general Assumption 2d. For example, estimation rates of</italic>\n<inline-formula><mml:math id=\"M425\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">penalized high-dimensional generalized linear models have been established for approximately sparse models</italic> (<xref rid=\"R32\" ref-type=\"bibr\">Negahban et al., 2012</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>).</p></sec></sec><sec id=\"S11\"><label>5</label><title>Simulation</title><p id=\"P90\">We conducted extensive simulation studies to evaluate the finite sample performance of the SMMAL methods. Throughout the simulations, we set the total sample size <inline-formula><mml:math id=\"M426\" display=\"inline\"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10000</mml:mn></mml:math></inline-formula>, the number of labels <inline-formula><mml:math id=\"M427\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula>, the number of repeats as 1000, <inline-formula><mml:math id=\"M428\" display=\"inline\"><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> with one surrogate <inline-formula><mml:math id=\"M429\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id=\"M430\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and another <inline-formula><mml:math id=\"M431\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id=\"M432\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>. We focused on the situation that <inline-formula><mml:math id=\"M433\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> is also binary. Let <inline-formula><mml:math id=\"M434\" display=\"inline\"><mml:mi>&#934;</mml:mi></mml:math></inline-formula> be cumulative distribution function for standard normal distribution. The surrogates for binary <inline-formula><mml:math id=\"M435\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M436\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> were generated from mixture Beta distribution of the form:\n<disp-formula id=\"FD41\"><mml:math id=\"M437\" display=\"block\"><mml:mtable columnalign=\"right\"><mml:mtr><mml:mtd/><mml:mtd columnalign=\"left\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Low-dimensional</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>model</mml:mtext><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign=\"left\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign=\"left\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>High-dimensional</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>model</mml:mtext><mml:mo>:</mml:mo></mml:mtd><mml:mtd columnalign=\"left\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#934;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#934;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign=\"left\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#934;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext mathvariant=\"italic\">Beta</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#934;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>\nThe mixture Beta distribution mimicked the outputs from phenotyping algorithms, which typically take value between zero and one (<xref rid=\"R28\" ref-type=\"bibr\">Liao et al., 2019</xref>). We considered a list of values for <inline-formula><mml:math id=\"M438\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M439\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref rid=\"T1\" ref-type=\"table\">Table 1</xref>), corresponding to different level of prediction accuracy measured by area-under-curve (AUC) of the receiver operating characteristic (ROC). Five values were considered for <inline-formula><mml:math id=\"M440\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M441\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, creating 25 two-way combinations for each simulation setting.</p><p id=\"P91\">We considered two scenarios for generating the data, the low-dimensional smooth model and high-dimensional logistic regression.</p><sec id=\"S12\"><title>Low-dimensional smooth model</title><p id=\"P92\">We generated the one dimensional <inline-formula><mml:math id=\"M442\" display=\"inline\"><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:math></inline-formula> from Uniform (0,1) and set the PS and OR to be the following smooth models (<xref rid=\"F2\" ref-type=\"fig\">Figure 2</xref>):\n<disp-formula id=\"FD42\"><mml:math id=\"M443\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1.2</mml:mn><mml:mo>/</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>3</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1.2</mml:mn><mml:mo>/</mml:mo><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mn>3</mml:mn><mml:mo>-</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe used tensor product first order B-spline (piece-wise linear splines) regression to estimate the nuisance models. The splines were constructed from <inline-formula><mml:math id=\"M645\" display=\"inline\"><mml:mtext mathvariant=\"italic\">bs</mml:mtext></mml:math></inline-formula> function of the <italic toggle=\"yes\">splines</italic> R package. The degrees were selected by 10 fold cross-validation among integers less than <inline-formula><mml:math id=\"M444\" display=\"inline\"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>&#8776;</mml:mo><mml:mn>22</mml:mn></mml:math></inline-formula> according to the out-of-fold entropy. Using the cross-fitted nuisance models from B-spline regression with <inline-formula><mml:math id=\"M445\" display=\"inline\"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>, we obtained point and interval estimates for the ATE based on <inline-formula><mml:math id=\"M446\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M447\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. As the benchmark, we also estimated the ATE using the labeled data only by the double machine learning method (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>).</p></sec><sec id=\"S13\"><title>High-dimensional logistic regression</title><p id=\"P93\">We generated the high-dimensional <inline-formula><mml:math id=\"M448\" display=\"inline\"><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> with <inline-formula><mml:math id=\"M449\" display=\"inline\"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula> from the multivariate Gaussian distribution with auto-regressive correlation structure:\n<disp-formula id=\"FD43\"><mml:math id=\"M450\" display=\"block\"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mover><mml:mrow><mml:mo>~</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>d</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mover><mml:mi>N</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0, 1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"0.5em\"/><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msqrt><mml:mn>0.75</mml:mn></mml:msqrt><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe generated <inline-formula><mml:math id=\"M451\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M452\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> from the high-dimensional logistic regression models\n<disp-formula id=\"FD44\"><mml:math id=\"M453\" display=\"block\"><mml:mtext>PS</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>Linear</mml:mtext><mml:mspace width=\"thickmathspace\"/><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mtext>PS</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>Interaction</mml:mtext><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mtext>OR</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>Linear</mml:mtext><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace linebreak=\"newline\"/><mml:mtext>OR</mml:mtext><mml:mspace width=\"0.5em\"/><mml:mtext>Interaction</mml:mtext><mml:mo>:</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced open=\"{\" close=\"\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mfenced open=\"\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>&#215;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace linebreak=\"newline\"/><mml:msub><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced open=\"{\" close=\"\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.25</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mfenced open=\"\" close=\"}\" separators=\"|\"><mml:mrow><mml:mo>&#215;</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.0625</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.125</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nAs signal strength is known to impact variable selection in theory and practice (<xref rid=\"R13\" ref-type=\"bibr\">Fan and Peng, 2004</xref>; <xref rid=\"R12\" ref-type=\"bibr\">Fan and Lv, 2010</xref>), we set up the coefficients in the models to reflect different level of signal strength: 0.5-strong, 0.25-moderately strong, 0.125-moderately weak, 0.0625-weak. For PS/OR models with second order interactions, we still fitted high-dimensional logistic regression without interactions, creating the mis-specification scenarios. We considered 3 combinations corresponding to the three settings of Assumption 4d: correct models (PS Linear + OR Linear); mis-specified PS (PS Interaction + OR Linear); mis-specified OR (PS Linear + OR Interaction). We set the number of the folds as 10 and fitted the imputations (<xref rid=\"FD13\" ref-type=\"disp-formula\">9</xref>) and initial estimators (<xref rid=\"FD14\" ref-type=\"disp-formula\">10</xref>) using <italic toggle=\"yes\">glmnet</italic> from R-package <italic toggle=\"yes\">glmnet</italic>. We fitted the calibrated estimators (<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>) using rcal from from R-package <italic toggle=\"yes\">rcal</italic>. The penalty parameters were selected by 10-fold cross validation with out-of-fold entropy. Using the cross-fitted nuisance models, we estimated the ATE using <inline-formula><mml:math id=\"M454\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and construct the 95% confidence interval based on the variance estimator <inline-formula><mml:math id=\"M455\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#119985;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. As the benchmark, we also estimated the ATE by the model doubly robust estimation (<xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>) using (1) the labeled data alone; (2) the dichotomized surrogates defined by\n<disp-formula id=\"FD45\"><mml:math id=\"M456\" display=\"block\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width=\"thickmathspace\"/><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe refer to the two benchmarks as supervised learning (SL) and unsupervised learning (UL).</p></sec><sec id=\"S14\"><title>Results</title><p id=\"P94\">Results generally followed a consistent pattern across low-d and high-d settings. Comparison between settings, however, is not meaningful due to the completely different data generating processes. In <xref rid=\"F3\" ref-type=\"fig\">Figure 3</xref>, we visualized the relative efficiency of our semi-supervised <inline-formula><mml:math id=\"M457\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> compared to their supervised benchmarks. In general, our semi-supervised approaches gained efficiency from the unlabeled data whose magnitude was increasing with the minimal prediction accuracy of the two surrogates. With good imputation (AUC .95) from both surrogates, the relative efficiency was about 1.32&#8211;1.64 across all settings. With great imputation (AUC .99) from both surrogates, the relative efficiency was about 2.23&#8211;2.89 across all settings. The result quantified the benefit from improving the quality of surrogates in terms of relative increase in labels. Since the algorithms to curate surrogates are often portable to other studies sharing the variables, effort put into high-quality labels is more cost-effective compared to the brutal expansion in labeling. The detailed simulation results containing the bias, standard deviation, average standard error, coverage of 95% confidence interval for our semi-supervised <inline-formula><mml:math id=\"M458\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> along with those for the supervised benchmarks were presented in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Tables A4</xref>&#8211;<xref rid=\"SD1\" ref-type=\"supplementary-material\">A7</xref> in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section A</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>. Our semi-supervised <inline-formula><mml:math id=\"M459\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>DR</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> achieved reasonably honest inference with coverage of 95% confidence interval close to the nominal level. In <xref rid=\"F4\" ref-type=\"fig\">Figure 4</xref>, we visualized the coverage of 95% confidence intervals by unsupervised learning. Using the dichotomized surrogates as if they were the true treatment and outcome led to under coverage of the confidence intervals even for nearly perfect surrogates, and the under coverage exacerbated with poorer surrogates. The detailed summaries on the bias, standard deviation and coverage of 95% confidence interval for the unsupervised benchmark were presented in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Table A8</xref> in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section A</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>.</p></sec></sec><sec id=\"S15\"><label>6</label><title>Real-world evidence on targeted cancer therapy</title><p id=\"P95\">We applied the proposed SMMAL method to EHR data from Mass General Brigham healthcare to generate real-world evidence (RWE) on treatment effect of targeted therapy for metastatic colorectal cancer in comparison with conventional chemotherapy. Over the past two decades, a total of 9 targeted therapies have been approved for the treatment of colorectal cancer (<xref rid=\"R47\" ref-type=\"bibr\">Xie et al., 2020</xref>), the 4th most prevalent and lethal cancer (<xref rid=\"R43\" ref-type=\"bibr\">U.S. Cancer Statistics Working Group, 2022</xref>). While the targeted therapies have been reported as advantageous compared to conventional chemotherapy in clinical trials within specific trial populations, their effectiveness in real-world patient population has not been fully established. With increasing availability of EHR data, it is now plausible to generate RWE on targeted cancer therapy with respect to their efficacy in improving progression free survival via causal modeling treating EHR data as an observational cohort. Unfortunately, such a modeling task is highly challenging with EHR data due to the lack of readily available precise information on both treatments patient received and progression free survival. To overcome this challenge, we manually annotated treatment-response information for 100 randomly selected patients. We derived several potential surrogates for both <inline-formula><mml:math id=\"M460\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M461\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> from codified and narrative EHR data, which have varying degree of accuracy as shown in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>. Our goal was to leverage both the labeled observations on <inline-formula><mml:math id=\"M462\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M463\" display=\"inline\"><mml:mi>S</mml:mi></mml:math></inline-formula> as well as the larger set of unlabeled EHR data to infer about ATE for targeted therapy based on SMMAL.</p><p id=\"P96\">The full study cohort consisted of <inline-formula><mml:math id=\"M464\" display=\"inline\"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>4147</mml:mn></mml:math></inline-formula> colorectal cancer patients who have available cancer stage information extracted via a natural language process tool (<xref rid=\"R48\" ref-type=\"bibr\">Yuan et al., 2021</xref>) and received chemotherapy and/or targeted therapy. We grouped therapies into chemotherapy alone and targeted therapy which includes those treated with any of the 9 treatments: Bevacizumab, Cetuximab, Ipilimumab, Regorafenib, Pembrolizumab, Nivolumab, and Tipiracil. We set the outcome as 1-year progression free survival, a binary outcome defined as: 1 &#8211; exit in terminal condition (death/terminal care) or development of new metastasis site with 1-year from the treatment initiation; 0 &#8211; otherwise. As the standard quality control (<xref rid=\"R23\" ref-type=\"bibr\">Hou et al., 2023</xref>), an abstractor randomly sampled <inline-formula><mml:math id=\"M465\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math></inline-formula> from the study cohort and annotated the gold-standard labels for prescription of targeted medication, terminal condition and new metastasis site by manually reviewing those patients&#8217; EHR. The treatment <inline-formula><mml:math id=\"M466\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M467\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> outcome were defined based on annotations over the labeled set, creating the MCAR data. We reported the treatment and outcome labels as well as their EHR proxies in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Table A9</xref> of <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials Section B</xref>, where we also described the construction of the reasonably good surrogates shown in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>.</p><p id=\"P97\">We extracted a comprehensive list of potential confounders (<xref rid=\"T3\" ref-type=\"table\">Table 3</xref>). From EHR near the colorectal cancer diagnosis date, we used location specific colorectal cancer diagnosis code to identify the initial tumor location and natural language process tool (<xref rid=\"R48\" ref-type=\"bibr\">Yuan et al., 2021</xref>) to extract the initial stage. We also extracted the code for secondary malignancy at lymph node and other distant organs. From EHR between cancer diagnosis and subsequent metastasis, we extracted the codes for common procedures (chemotherapy, radiotherapy, colon biopsy and colon rescission). From EHR near the metastasis date, we used location specific secondary malignancy code to identify the initial metastasis site(s). We also adjusted for the time gap between diagnosis and metastasis, healthcare utilization before metastasis or one year before metastasis measured by days with diagnosis codes and the high-dimensional general health status consisting of diagnosis code counts grouped by the PheWAS catalog (<xref rid=\"R22\" ref-type=\"bibr\">Hou et al., 2022</xref>). The targeted therapy arm was associated with factors for poor prognosis including higher proportion of stage IV at diagnosis (81% vs 58%), higher proportion of likely liver metastasis (57% or 67% vs 34%). After merging rare levels for cancer characteristics at initial diagnosis (tumor location, cancer stage) and deleting features with fewer than 10 occurrence in labeled subset, we obtained the <inline-formula><mml:math id=\"M468\" display=\"inline\"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:math></inline-formula> potential confounders.</p><p id=\"P98\">We applied the doubly robust SMMAL in high-dimensions described in <xref rid=\"S6\" ref-type=\"sec\">Section 3.3</xref>. Besides the crude analysis, we ran two benchmark analyses, the double machine learning (DML) (<xref rid=\"R10\" ref-type=\"bibr\">Chernozhukov et al., 2018</xref>) using initial estimators (<xref rid=\"FD14\" ref-type=\"disp-formula\">10</xref>) and the calibrated estimation (Cal) (<xref rid=\"R40\" ref-type=\"bibr\">Tan, 2020</xref>; <xref rid=\"R37\" ref-type=\"bibr\">Smucler et al., 2019</xref>) using the calibrated estimators (<xref rid=\"FD16\" ref-type=\"disp-formula\">12</xref>). Both supervised learning (SL) using labeled data only and the unsupervised learning (UL) deriving treatment and outcome from the dichotomized surrogates by matching observed prevalence in labeled data were considered. The number of fold was set as <inline-formula><mml:math id=\"M469\" display=\"inline\"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula>, and the penalties factors were selected by the minimal cross-validated entropy. In <xref rid=\"F5\" ref-type=\"fig\">Figure 5</xref>, we displayed the point estimation and the 95 % confidence interval. The confounder adjusted analysis results suggested that on average, targeted therapy had comparable efficacy compared to traditional chemotherapy. Compared to the SL crude analysis which indicated worse outcomes for targeted therapy, our SMMAL accounted for substantial confounding caused by association between target therapy and factors indicating poor prognosis. Except for the crude analysis that did not adjust for any confounding, our SMMAL had the shortest confidence interval, achieving 1.88 relative efficiency with respect to SL DML and 1.35 relative efficiency with respect to the SL cal. The results from UL methods were questionable as we observed a significant deviation of the UL crude estimation from the SL crude estimation, indicating substantial bias from imperfect data. Coupled with the short confidence intervals, researcher should take caution in the risk of misleading conclusions from the UL methods.</p></sec><sec id=\"S16\"><label>7</label><title>General Efficiency Lower Bound</title><p id=\"P99\">While the paper focused on the method for ATE under double missing SSL setting, we established the theoretical efficient lower bound for general parameter and broader missing data pattern in this section. We considered a generic model for data <inline-formula><mml:math id=\"M470\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> with always observed <inline-formula><mml:math id=\"M471\" display=\"inline\"><mml:mi mathvariant=\"bold\">W</mml:mi></mml:math></inline-formula> and MCAR <inline-formula><mml:math id=\"M472\" display=\"inline\"><mml:mi mathvariant=\"bold\">Z</mml:mi></mml:math></inline-formula>. Specifically, consider\n<disp-formula id=\"FD46\"><mml:math id=\"M473\" display=\"block\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open=\"{\" close=\"\" separators=\"|\"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:msub><mml:mo stretchy=\"true\">&#8747;</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>&#119989;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>:</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mfenced open=\"\" close=\"}\" separators=\"|\"><mml:mrow><mml:mi>f</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>\nfor a complete data model class <inline-formula><mml:math id=\"M474\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id=\"M475\" display=\"inline\"><mml:mi>&#119989;</mml:mi><mml:mo>&#8855;</mml:mo><mml:mi>&#119986;</mml:mi></mml:math></inline-formula> and measures <inline-formula><mml:math id=\"M476\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id=\"M477\" display=\"inline\"><mml:mi>&#119989;</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> over <inline-formula><mml:math id=\"M478\" display=\"inline\"><mml:mi>&#119986;</mml:mi></mml:math></inline-formula> and\n<disp-formula id=\"FD47\"><mml:math id=\"M479\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#948;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>&#957;</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nLet <inline-formula><mml:math id=\"M480\" display=\"inline\"><mml:mi>&#8459;</mml:mi></mml:math></inline-formula> be the nuisance tangent space of <inline-formula><mml:math id=\"M481\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> at the true model <inline-formula><mml:math id=\"M482\" display=\"inline\"><mml:mi>d</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> with <inline-formula><mml:math id=\"M483\" display=\"inline\"><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula>. Suppose <inline-formula><mml:math id=\"M484\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the efficient influence function for parameter <inline-formula><mml:math id=\"M485\" display=\"inline\"><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:math></inline-formula> under <inline-formula><mml:math id=\"M486\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. Here we use a different notation <inline-formula><mml:math id=\"M487\" display=\"inline\"><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:math></inline-formula> for general parameter under missing data components to distinguish from the <inline-formula><mml:math id=\"M488\" display=\"inline\"><mml:mi>&#981;</mml:mi></mml:math></inline-formula> used specifically for ATE under double missing SSL setting. Our theory was established under the following basic assumptions.</p><p id=\"P100\"><bold>Assumption 5</bold>\n<italic toggle=\"yes\">For absolute constant</italic>\n<inline-formula><mml:math id=\"M489\" display=\"inline\"><mml:mi>M</mml:mi></mml:math></inline-formula>,</p><list list-type=\"alpha-lower\" id=\"L34\"><list-item><p id=\"P101\"><inline-formula><mml:math id=\"M490\" display=\"inline\"><mml:mo>(</mml:mo><mml:mtext mathvariant=\"italic\">MCAR</mml:mtext><mml:mo>)</mml:mo><mml:mi>R</mml:mi><mml:mo>&#10987;</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P102\"><italic toggle=\"yes\">(Informative labels)</italic>\n<inline-formula><mml:math id=\"M491\" display=\"inline\"><mml:msub><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold\">v</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width=\"0.5em\"/><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mtext>Var</mml:mtext><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi mathvariant=\"bold\">v</mml:mi><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P103\"><italic toggle=\"yes\">(Model flexibility)</italic>\n<inline-formula><mml:math id=\"M492\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"double-struck\">E</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:mi>&#8459;</mml:mi></mml:math></inline-formula>;</p></list-item><list-item><p id=\"P104\"><italic toggle=\"yes\">(Bounded influence function)</italic>\n<inline-formula><mml:math id=\"M493\" display=\"inline\"><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>M</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">almost surely</italic>.</p></list-item></list><p id=\"P105\">We derived the SSL efficient influence function by the following proposition.</p><p id=\"P106\"><bold>Proposition 12</bold>\n<italic toggle=\"yes\">Let</italic>\n<inline-formula><mml:math id=\"M494\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>\n<italic toggle=\"yes\">be the efficient influence function for parameter <inline-formula><mml:math id=\"M495\" display=\"inline\"><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:math></inline-formula> under complete data model</italic>\n<inline-formula><mml:math id=\"M496\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. <italic toggle=\"yes\">Under Assumptions 5a and 5c, the efficient influence function for</italic>\n<inline-formula><mml:math id=\"M497\" display=\"inline\"><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">under SSL model</italic>\n<inline-formula><mml:math id=\"M498\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>\n<italic toggle=\"yes\">is</italic>\n<disp-formula id=\"FD48\"><label>(19)</label><mml:math id=\"M499\" display=\"block\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nThe influence function <inline-formula><mml:math id=\"M500\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> leads to a semi-parametric efficiency lower bound.</p><p id=\"P107\"><bold>Theorem 13</bold>\n<italic toggle=\"yes\">Under Assumptions 5a-5d, we have the minimax semi-parametric efficiency for SSL of</italic>\n<inline-formula><mml:math id=\"M501\" display=\"inline\"><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:math></inline-formula>\n<italic toggle=\"yes\">under</italic>\n<inline-formula><mml:math id=\"M502\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#119982;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>,\n<disp-formula id=\"FD49\"><mml:math id=\"M503\" display=\"block\"><mml:munder><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">a</mml:mi><mml:mo>:</mml:mo><mml:mo>&#8214;</mml:mo><mml:mi mathvariant=\"bold\">a</mml:mi><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext><mml:mo>&#8198;</mml:mo><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext><mml:mo>&#8198;</mml:mo><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"normal\">&#8734;</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced open=\"&#x2016;\" close=\"&#x2016;\" separators=\"|\"><mml:mrow><mml:mi>f</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>TV</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8804;</mml:mo><mml:mi>c</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">&#8747;</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:msup><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#952;</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8719;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"double-struck\">P</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">&#8868;</mml:mi></mml:mrow></mml:msup><mml:mtext>Var</mml:mtext><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">&#968;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mi mathvariant=\"bold\">a</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#8805;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula>\nWe offered the proof of Theorem 13 in <xref rid=\"SD1\" ref-type=\"supplementary-material\">Section C6</xref> of the <xref rid=\"SD1\" ref-type=\"supplementary-material\">Supplementary Materials</xref>. Upper bound would depend on the context. Like Corollary 7, the bound can be attained if non-parametric estimation of nuisance models admit sufficiently fast rate of consistency, which has been thoroughly studied under classical low-dimensional settings by <xref rid=\"R38\" ref-type=\"bibr\">Stone (1977</xref>, <xref rid=\"R39\" ref-type=\"bibr\">1982)</xref>. While we focus on <inline-formula><mml:math id=\"M504\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8594;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id=\"M505\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>&#8810;</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> setting, the theory also applies to classical setting with <inline-formula><mml:math id=\"M506\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>M</mml:mi><mml:mo>]</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id=\"M507\" display=\"inline\"><mml:mi>n</mml:mi><mml:mo>&#8781;</mml:mo><mml:mi>N</mml:mi></mml:math></inline-formula> setting.</p></sec><sec id=\"S17\"><label>8</label><title>Discussion</title><p id=\"P108\">Motivated by the increasing interest of generating real-world evidence on treatment effect with big yet noisy EHR data, we proposed a robust and efficient semi-supervised estimator for ATE under the double missing SSL setting. The SMMAL estimator gained efficiency by leveraging the large unlabelled data containing noisy yet predictive surrogates for <inline-formula><mml:math id=\"M508\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M509\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> with almost no additional requirement than those needed for the supervised analysis using the labeled set alone. We established semi-parametric efficiency bound for the ATE estimator under the low dimensional confounder setting and constructed a doubly robust SMMAL estimator for the high dimensional confounder setting.</p><p id=\"P109\">Unlike the MCAR setting, the missing data propensity score <inline-formula><mml:math id=\"M510\" display=\"inline\"><mml:mi mathvariant=\"double-struck\">P</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> must be modeled and estimated. We conjecture that the efficient influence function under MAR may take the form\n<disp-formula id=\"FD50\"><mml:math id=\"M511\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>MAR</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mtext mathvariant=\"italic\">RY</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">RA</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced><mml:mspace linebreak=\"newline\"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant=\"double-struck\">E</mml:mi><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nThe estimation of the decaying <inline-formula><mml:math id=\"M512\" display=\"inline\"><mml:mi>&#961;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> has been studied in <xref rid=\"R50\" ref-type=\"bibr\">Zhang et al. (2023)</xref>. When all nuisance models, <inline-formula><mml:math id=\"M513\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#928;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>&#961;</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, are consistently estimated at suitable rates, the efficiency lower bound should be attained under ideal conditions. However, extension of the SMMAL with high-dimensional regressions to MAR setting would require a more sophisticated calibration procedure for all 5 models <inline-formula><mml:math id=\"M514\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#960;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#928;</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>&#961;</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, as the potential bias from mis-specified <inline-formula><mml:math id=\"M515\" display=\"inline\"><mml:mi>&#961;</mml:mi></mml:math></inline-formula> now may impact the orthogonality of ATE estimator toward all 4 other estimated models. Moreover, caution must be taken when making MAR assumption for treatment and outcome data from linked observational data such as a disease registry. Enrollment in registry led by pioneering clinical experts may systematically impact the treatment pattern and care quality, which would put the MAR assumption in doubt.</p><p id=\"P110\">The classical semi-parametric efficiency theory relies on the correct modeling and estimation of the nuisance models. When some nuisance models cannot be consistently estimated, there is no universal efficiency guarantee for estimation procedures derived from semi-parametric efficiency theory. To ensure efficiency improvement when both the supervised estimator\n<disp-formula id=\"FD51\"><mml:math id=\"M516\" display=\"block\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>&#8464;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak=\"newline\"/><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mfenced separators=\"|\"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>\nand the SMMAL estimator <inline-formula><mml:math id=\"M517\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> are consistent and asymptotically normal, we may consider the linear ensemble\n<disp-formula id=\"FD52\"><mml:math id=\"M518\" display=\"block\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>comb</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SL</mml:mtext></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>\nSuppose the influence functions for <inline-formula><mml:math id=\"M519\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SMMAL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M520\" display=\"inline\"><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#916;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>SL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> are <inline-formula><mml:math id=\"M521\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M522\" display=\"inline\"><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. The optimal linear ensemble is given by\n<disp-formula id=\"FD53\"><mml:math id=\"M523\" display=\"block\"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mtext>Cov</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>Var</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mtext>Cov</mml:mtext><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>SSL</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mrow><mml:mi>&#981;</mml:mi></mml:mrow><mml:mrow><mml:mtext>cmp</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>&#961;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>\nwhich can be estimated by the empirical variances and covariance of estimated influence functions constructed with estimated nuisance models <inline-formula><mml:math id=\"M524\" display=\"inline\"><mml:mo>(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#960;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>&#928;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math></inline-formula>.</p><p id=\"P111\">Our doubly robust estimation can be generalized to other models if the calibrated estimation for the model is available. For example, we can directly adopt the estimators from <xref rid=\"R40\" ref-type=\"bibr\">Tan (2020)</xref> for linear outcome model. The calibrated estimation is, however, limited to M-estimator in high-dimensional regression due to the paucity of works on Z-estimators in high-dimensional setting. It would be interesting to study if the Z-estimator approach (<xref rid=\"R45\" ref-type=\"bibr\">Vermeulen and Vansteelandt, 2015</xref>) can be generalized to high-dimensional setting.</p></sec><sec sec-type=\"supplementary-material\" id=\"SM1\"><title>Supplementary Material</title><supplementary-material id=\"SD1\" position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><label>1</label><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"NIHMS2084485-supplement-1.pdf\" id=\"d67e19978\" position=\"anchor\" orientation=\"portrait\"/></supplementary-material></sec></body><back><ack id=\"S18\"><title>Acknowledgment</title><p id=\"P112\">This publication is partially supported by the Food and Drug Administration (FDA) of the U.S. Department of Health and Human Services (HHS) as part of a financial assistance award [FAIN] totaling $367,807 with 50 percent funded by FDA/HHS. The project is also supported by grants R01 LM013614 and R01 AR080193 from the National Institute of Health. The contents are those of the authors and do not necessarily represent the official views of, nor an endorsement, by FDA/HHS, or the U.S. Government.</p></ack><ref-list><title>References</title><ref id=\"R1\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Bang</surname><given-names>Heejung</given-names></name> and <name name-style=\"western\"><surname>Robins</surname><given-names>James M.</given-names></name>. <article-title>Doubly robust estimation in missing data and causal inference models</article-title>. <source>Biometrics</source>, <volume>61</volume>(<issue>4</issue>):<fpage>962</fpage>&#8211;<lpage>973</lpage>, <year>2005</year>. doi: <pub-id pub-id-type=\"doi\">10.1111/j.1541-0420.2005.00377.x</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x</ext-link>.<pub-id pub-id-type=\"pmid\">16401269</pub-id></mixed-citation></ref><ref id=\"R2\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Bartlett</surname><given-names>Victoria L.</given-names></name>, <name name-style=\"western\"><surname>Dhruva</surname><given-names>Sanket S.</given-names></name>, <name name-style=\"western\"><surname>Shah</surname><given-names>Nilay D.</given-names></name>, <name name-style=\"western\"><surname>Ryan</surname><given-names>Patrick</given-names></name>, and <name name-style=\"western\"><surname>Ross</surname><given-names>Joseph S.</given-names></name>. <article-title>Feasibility of Using Real-World Data to Replicate Clinical Trial Evidence</article-title>. <source>JAMA Network Open</source>, <volume>2</volume>(<issue>10</issue>):<fpage>e1912869</fpage>&#8211;<lpage>e1912869</lpage>, <month>10</month><year>2019</year>. ISSN 2574&#8211;3805. doi: <pub-id pub-id-type=\"doi\">10.1001/jamanetworkopen.2019.12869</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1001/jamanetworkopen.2019.12869\" ext-link-type=\"uri\">https://doi.org/10.1001/jamanetworkopen.2019.12869</ext-link>.<pub-id pub-id-type=\"pmid\">31596493</pub-id><pub-id pub-id-type=\"pmcid\">PMC6802419</pub-id></mixed-citation></ref><ref id=\"R3\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Beaulieu-Jones</surname><given-names>Brett K.</given-names></name>, <name name-style=\"western\"><surname>Finlayson</surname><given-names>Samuel G.</given-names></name>, <name name-style=\"western\"><surname>Yuan</surname><given-names>William</given-names></name>, <name name-style=\"western\"><surname>Altman</surname><given-names>Russ B.</given-names></name>, <name name-style=\"western\"><surname>Kohane</surname><given-names>Isaac S.</given-names></name>, <name name-style=\"western\"><surname>Prasad</surname><given-names>Vinay</given-names></name>, and <name name-style=\"western\"><surname>Yu</surname><given-names>Kun-Hsing</given-names></name>. <article-title>Examining the use of real-world evidence in the regulatory process</article-title>. <source>Clinical Pharmacology &amp; Therapeutics</source>, <volume>107</volume>(<issue>4</issue>):<fpage>843</fpage>&#8211;<lpage>852</lpage>, <year>2020</year>. doi: <pub-id pub-id-type=\"doi\">10.1002/cpt.1658</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1002/cpt.1658\" ext-link-type=\"uri\">https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1002/cpt.1658</ext-link>.<pub-id pub-id-type=\"pmid\">31562770</pub-id><pub-id pub-id-type=\"pmcid\">PMC7093234</pub-id></mixed-citation></ref><ref id=\"R4\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Begun</surname><given-names>Janet M.</given-names></name>, <name name-style=\"western\"><surname>Hall</surname><given-names>WJ</given-names></name>, <name name-style=\"western\"><surname>Huang</surname><given-names>Wei-Min</given-names></name>, and <name name-style=\"western\"><surname>Wellner</surname><given-names>Jon A.</given-names></name>. <article-title>Information and Asymptotic Efficiency in Parametric-Nonparametric Models</article-title>. <source>The Annals of Statistics</source>, <volume>11</volume>(<issue>2</issue>): <fpage>432</fpage>&#8211;<lpage>452</lpage>, <year>1983</year>. doi: <pub-id pub-id-type=\"doi\">10.1214/aos/1176346151</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1214/aos/1176346151\" ext-link-type=\"uri\">https://doi.org/10.1214/aos/1176346151</ext-link>.</mixed-citation></ref><ref id=\"R5\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Belloni</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Chernozhukov</surname><given-names>V</given-names></name>, <name name-style=\"western\"><surname>Fern&#225;ndez-Val</surname><given-names>I</given-names></name>, and <name name-style=\"western\"><surname>Hansen</surname><given-names>C</given-names></name>. <article-title>Program evaluation and causal inference with high-dimensional data</article-title>. <source>Econometrica</source>, <volume>85</volume>(<issue>1</issue>):<fpage>233</fpage>&#8211;<lpage>298</lpage>, <year>2017</year>. doi: <pub-id pub-id-type=\"doi\">10.3982/ECTA12723</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723</ext-link>.</mixed-citation></ref><ref id=\"R6\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Belloni</surname><given-names>Alexandre</given-names></name>, <name name-style=\"western\"><surname>Chernozhukov</surname><given-names>Victor</given-names></name>, and <name name-style=\"western\"><surname>Hansen</surname><given-names>Christian</given-names></name>. <article-title>Inference on Treatment Effects after Selection among High-Dimensional Controls&#8224;</article-title>. <source>The Review of Economic Studies</source>, <volume>81</volume>(<issue>2</issue>):<fpage>608</fpage>&#8211;<lpage>650</lpage>, <month>11</month><year>2013</year>. ISSN 0034&#8211;6527. doi: <pub-id pub-id-type=\"doi\">10.1093/restud/rdt044</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1093/restud/rdt044\" ext-link-type=\"uri\">https://doi.org/10.1093/restud/rdt044</ext-link>.</mixed-citation></ref><ref id=\"R7\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Bradic</surname><given-names>Jelena</given-names></name>, <name name-style=\"western\"><surname>Wager</surname><given-names>Stefan</given-names></name>, and <name name-style=\"western\"><surname>Zhu</surname><given-names>Yinchu</given-names></name>. <article-title>Sparsity Double Robust Inference of Average Treatment Effects</article-title>. <source>arXiv</source>, page 1905.00744, <year>2019</year>.</mixed-citation></ref><ref id=\"R8\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Chakrabortty</surname><given-names>Abhishek</given-names></name>, <name name-style=\"western\"><surname>Lu</surname><given-names>Jiarui</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>T. Tony</given-names></name>, and <name name-style=\"western\"><surname>Li</surname><given-names>Hongzhe</given-names></name>. <article-title>High dimensional m-estimation with missing outcomes: A semi-parametric framework</article-title>. <source>arXiv</source>, page 1911.11345, <year>2019</year>.</mixed-citation></ref><ref id=\"R9\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Cheng</surname><given-names>David</given-names></name>, <name name-style=\"western\"><surname>Ananthakrishnan</surname><given-names>Ashwin N.</given-names></name>, and <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>. <article-title>Robust and efficient semi-supervised estimation of average treatment effects with application to electronic health records data</article-title>. <source>Biometrics</source>, <volume>77</volume>(<issue>2</issue>):<fpage>413</fpage>&#8211;<lpage>423</lpage>, <year>2021</year>. doi: <pub-id pub-id-type=\"doi\">10.1111/biom.13298</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13298\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13298</ext-link>.<pub-id pub-id-type=\"pmid\">32413171</pub-id><pub-id pub-id-type=\"pmcid\">PMC7758040</pub-id></mixed-citation></ref><ref id=\"R10\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Chernozhukov</surname><given-names>Victor</given-names></name>, <name name-style=\"western\"><surname>Chetverikov</surname><given-names>Denis</given-names></name>, <name name-style=\"western\"><surname>Demirer</surname><given-names>Mert</given-names></name>, <name name-style=\"western\"><surname>Duflo</surname><given-names>Esther</given-names></name>, <name name-style=\"western\"><surname>Hansen</surname><given-names>Christian</given-names></name>, <name name-style=\"western\"><surname>Newey</surname><given-names>Whitney</given-names></name>, and <name name-style=\"western\"><surname>Robins</surname><given-names>James</given-names></name>. <article-title>Double/debiased machine learning for treatment and structural parameters</article-title>. <source>The Econometrics Journal</source>, <volume>21</volume>:<fpage>C1</fpage>&#8211;<lpage>C68</lpage>, <year>2018</year>.</mixed-citation></ref><ref id=\"R11\"><mixed-citation publication-type=\"webpage\"><name name-style=\"western\"><surname>Duchi</surname><given-names>John</given-names></name>. <source>A few notes on contiguity, asymptotics, and local asymptotic normality</source>, <month>March</month><year>2021</year>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://web.stanford.edu/class/stats300b/Notes/contiguity-and-asymptotics.pdf\" ext-link-type=\"uri\">https://web.stanford.edu/class/stats300b/Notes/contiguity-and-asymptotics.pdf</ext-link>.</mixed-citation></ref><ref id=\"R12\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Fan</surname><given-names>Jianqing</given-names></name> and <name name-style=\"western\"><surname>Lv</surname><given-names>Jinchi</given-names></name>. <article-title>A selective overview of variable selection in high dimensional feature space</article-title>. <source>Statistica Sinica</source>, <volume>20</volume>(<issue>1</issue>):<fpage>101</fpage>&#8211;<lpage>148</lpage>, <year>2010</year>.<pub-id pub-id-type=\"pmid\">21572976</pub-id><pub-id pub-id-type=\"pmcid\">PMC3092303</pub-id></mixed-citation></ref><ref id=\"R13\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Fan</surname><given-names>Jianqing</given-names></name> and <name name-style=\"western\"><surname>Peng</surname><given-names>Heng</given-names></name>. <article-title>Nonconcave penalized likelihood with a diverging number of parameters</article-title>. <source>The Annals of Statistics</source>, <volume>32</volume>(<issue>3</issue>):<fpage>928</fpage>&#8211;<lpage>961</lpage>, <year>2004</year>. doi: <pub-id pub-id-type=\"doi\">10.1214/009053604000000256</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1214/009053604000000256\" ext-link-type=\"uri\">https://doi.org/10.1214/009053604000000256</ext-link>.</mixed-citation></ref><ref id=\"R14\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Farrell</surname><given-names>Max H.</given-names></name>. <article-title>Robust inference on average treatment effects with possibly more covariates than observations</article-title>. <source>Journal of Econometrics</source>, <volume>189</volume>:<fpage>1</fpage>&#8211;<lpage>23</lpage>, <year>2015</year>.</mixed-citation></ref><ref id=\"R15\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Farrell</surname><given-names>Max H.</given-names></name>, <name name-style=\"western\"><surname>Liang</surname><given-names>Tengyuan</given-names></name>, and <name name-style=\"western\"><surname>Misra</surname><given-names>Sanjog</given-names></name>. <article-title>Deep neural networks for estimation and inference</article-title>. <source>Econometrica</source>, <volume>89</volume>(<issue>1</issue>):<fpage>181</fpage>&#8211;<lpage>213</lpage>, <year>2021</year>. doi: <pub-id pub-id-type=\"doi\">10.3982/ECTA16901</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901</ext-link>.</mixed-citation></ref><ref id=\"R16\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Franklin</surname><given-names>Jessica M.</given-names></name>, <name name-style=\"western\"><surname>Liaw</surname><given-names>Kai-Li</given-names></name>, <name name-style=\"western\"><surname>Iyasu</surname><given-names>Solomon</given-names></name>, <name name-style=\"western\"><surname>Critchlow</surname><given-names>Cathy W.</given-names></name>, and <name name-style=\"western\"><surname>Dreyer</surname><given-names>Nancy A.</given-names></name>. <article-title>Real-world evidence to support regulatory decision making: New or expanded medical product indications</article-title>. <source>Pharmacoepidemiology and Drug Safety</source>, <volume>30</volume>(<issue>6</issue>):<fpage>685</fpage>&#8211;<lpage>693</lpage>, <year>2021</year>. doi: <pub-id pub-id-type=\"doi\">10.1002/pds.5222</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.5222\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.5222</ext-link>.<pub-id pub-id-type=\"pmid\">33675248</pub-id></mixed-citation></ref><ref id=\"R17\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Griffith</surname><given-names>Sandra D.</given-names></name>, <name name-style=\"western\"><surname>Tucker</surname><given-names>Melisa</given-names></name>, <name name-style=\"western\"><surname>Bowser</surname><given-names>Bryan</given-names></name>, <name name-style=\"western\"><surname>Calkins</surname><given-names>Geoffrey</given-names></name>, <name name-style=\"western\"><surname>Chang</surname><given-names>Che-hsu (Joe)</given-names></name>, <name name-style=\"western\"><surname>Guardino</surname><given-names>Ellie</given-names></name>, <name name-style=\"western\"><surname>Khozin</surname><given-names>Sean</given-names></name>, <name name-style=\"western\"><surname>Kraut</surname><given-names>Josh</given-names></name>, <name name-style=\"western\"><surname>You</surname><given-names>Paul</given-names></name>, <name name-style=\"western\"><surname>Schrag</surname><given-names>Deb</given-names></name>, and <name name-style=\"western\"><surname>Miksad</surname><given-names>Rebecca A.</given-names></name>. <article-title>Generating real-world tumor burden endpoints from electronic health record data: Comparison of recist, radiology-anchored, and clinician-anchored approaches for abstracting real-world progression in non-small cell lung cancer</article-title>. <source>Advances in Therapy</source>, <volume>36</volume>(<issue>8</issue>):<fpage>2122</fpage>&#8211;<lpage>2136</lpage>, <month>Aug</month><year>2019</year>. ISSN 1865&#8211;8652. doi: <pub-id pub-id-type=\"doi\">10.1007/s12325-019-00970-1</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1007/s12325-019-00970-1\" ext-link-type=\"uri\">https://doi.org/10.1007/s12325-019-00970-1</ext-link>.<pub-id pub-id-type=\"pmid\">31140124</pub-id><pub-id pub-id-type=\"pmcid\">PMC6822856</pub-id></mixed-citation></ref><ref id=\"R18\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Hernan</surname><given-names>MA</given-names></name> and <name name-style=\"western\"><surname>Robins</surname><given-names>JM</given-names></name>. <part-title>Causal Inference</part-title>. <source>Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probab</source>. <publisher-name>Taylor &amp; Francis</publisher-name>, <year>2023</year>. ISBN 9781420076165. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://books.google.com/books?id=_KnHIAAACAAJ\" ext-link-type=\"uri\">https://books.google.com/books?id=_KnHIAAACAAJ</ext-link>.</mixed-citation></ref><ref id=\"R19\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hou</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Bradic</surname><given-names>J</given-names></name>, and <name name-style=\"western\"><surname>Xu</surname><given-names>R</given-names></name>. <article-title>Treatment effect estimation under additive hazards models with high-dimensional confounding</article-title>. <source>Journal of the American Statistical Association</source>, page In press, <year>2021a</year>.</mixed-citation></ref><ref id=\"R20\"><mixed-citation publication-type=\"other\"><name name-style=\"western\"><surname>Hou</surname><given-names>Jue</given-names></name>, <name name-style=\"western\"><surname>Guo</surname><given-names>Zijian</given-names></name>, and <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>. <source>Surrogate assisted semi-supervised inference for high dimensional risk prediction</source>, <year>2021b</year>.</mixed-citation></ref><ref id=\"R21\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hou</surname><given-names>Jue</given-names></name>, <name name-style=\"western\"><surname>Kim</surname><given-names>Nicole</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianrun</given-names></name>, <name name-style=\"western\"><surname>Dahal</surname><given-names>Kumar</given-names></name>, <name name-style=\"western\"><surname>Weiner</surname><given-names>Howard</given-names></name>, <name name-style=\"western\"><surname>Chitnis</surname><given-names>Tanuja</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>, and <name name-style=\"western\"><surname>Xia</surname><given-names>Zongqi</given-names></name>. <article-title>Comparison of dimethyl fumarate vs fingolimod and rituximab vsnatalizumab for treatment of multiple sclerosis</article-title>. <source>JAMA Network Open</source>, page To appear, <year>2021c</year>.</mixed-citation></ref><ref id=\"R22\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hou</surname><given-names>Jue</given-names></name>, <name name-style=\"western\"><surname>Zhao</surname><given-names>Rachel</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianrun</given-names></name>, <name name-style=\"western\"><surname>Beaulieu-Jones</surname><given-names>Brett</given-names></name>, <name name-style=\"western\"><surname>Seyok</surname><given-names>Thany</given-names></name>, <name name-style=\"western\"><surname>Dahal</surname><given-names>Kumar</given-names></name>, <name name-style=\"western\"><surname>Yuan</surname><given-names>Qianyu</given-names></name>, <name name-style=\"western\"><surname>Xiong</surname><given-names>Xin</given-names></name>, <name name-style=\"western\"><surname>Bonzel</surname><given-names>Clara-Lea</given-names></name>, <name name-style=\"western\"><surname>Fox</surname><given-names>Claire</given-names></name>, <name name-style=\"western\"><surname>Christiani</surname><given-names>David C.</given-names></name>, <name name-style=\"western\"><surname>Jemielita</surname><given-names>Thomas</given-names></name>, <name name-style=\"western\"><surname>Liao</surname><given-names>Katherine P.</given-names></name>, <name name-style=\"western\"><surname>Liaw</surname><given-names>Kai-Li</given-names></name>, and <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>. <article-title>Temporal Trends in Clinical Evidence of 5-Year Survival Within Electronic Health Records Among Patients With Early-Stage Colon Cancer Managed With Laparoscopy-Assisted Colectomy vs Open Colectomy</article-title>. <source>JAMA Network Open</source>, <volume>5</volume>(<issue>6</issue>):<fpage>e2218371</fpage>&#8211;<lpage>e2218371</lpage>, <month>06</month><year>2022</year>. ISSN 2574&#8211;3805. doi: <pub-id pub-id-type=\"doi\">10.1001/jamanetworkopen.2022.18371</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1001/jamanetworkopen.2022.18371\" ext-link-type=\"uri\">https://doi.org/10.1001/jamanetworkopen.2022.18371</ext-link>.<pub-id pub-id-type=\"pmid\">35737384</pub-id><pub-id pub-id-type=\"pmcid\">PMC9227003</pub-id></mixed-citation></ref><ref id=\"R23\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hou</surname><given-names>Jue</given-names></name>, <name name-style=\"western\"><surname>Zhao</surname><given-names>Rachel</given-names></name>, <name name-style=\"western\"><surname>Gronsbell</surname><given-names>Jessica</given-names></name>, <name name-style=\"western\"><surname>Lin</surname><given-names>Yucong</given-names></name>, <name name-style=\"western\"><surname>Bonzel</surname><given-names>Clara-Lea</given-names></name>, <etal/><article-title>Generate analysis-ready data for real-world evidence: Tutorial for harnessing electronic health records with advanced informatic technologies</article-title>. <source>Journal of Medical Internet Research</source>, <volume>25</volume>: <fpage>e45662</fpage>, <year>2023</year>.<pub-id pub-id-type=\"pmid\">37227772</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.2196/45662</pub-id><pub-id pub-id-type=\"pmcid\">PMC10251230</pub-id></mixed-citation></ref><ref id=\"R24\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Imbens</surname><given-names>Guido W.</given-names></name> and <name name-style=\"western\"><surname>Rubin</surname><given-names>Donald B.</given-names></name>. <source>Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>2015</year>. doi: <pub-id pub-id-type=\"doi\">10.1017/CBO9781139025751</pub-id>.</mixed-citation></ref><ref id=\"R25\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Ju</surname><given-names>Cheng</given-names></name>, <name name-style=\"western\"><surname>Schwab</surname><given-names>Joshua</given-names></name>, and <name name-style=\"western\"><surname>van der Laan</surname><given-names>Mark J</given-names></name>. <article-title>On adaptive propensity score truncation in causal inference</article-title>. <source>Statistical Methods in Medical Research</source>, <volume>28</volume>(<issue>6</issue>):<fpage>1741</fpage>&#8211;<lpage>1760</lpage>, <year>2019</year>. doi: <pub-id pub-id-type=\"doi\">10.1177/0962280218774817</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1177/0962280218774817\" ext-link-type=\"uri\">https://doi.org/10.1177/0962280218774817</ext-link>.<pub-id pub-id-type=\"pmid\">29991330</pub-id></mixed-citation></ref><ref id=\"R26\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Kallus</surname><given-names>Nathan</given-names></name> and <name name-style=\"western\"><surname>Mao</surname><given-names>Xiaojie</given-names></name>. <article-title>On the role of surrogates in the efficient estimation of treatment effects with limited outcome data</article-title>. <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source>, page <fpage>qkae099</fpage>, <month>10</month><year>2024</year>. ISSN 1369&#8211;7412. doi: <pub-id pub-id-type=\"doi\">10.1093/jrsssb/qkae099</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1093/jrsssb/qkae099\" ext-link-type=\"uri\">https://doi.org/10.1093/jrsssb/qkae099</ext-link>.</mixed-citation></ref><ref id=\"R27\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Cam</surname><given-names>Lucien Le</given-names></name> and <name name-style=\"western\"><surname>Yang</surname><given-names>Grace Lo</given-names></name>. <source>Contiguity - Hellinger Transforms</source>, pages <fpage>34</fpage>&#8211;<lpage>49</lpage>. <publisher-name>Springer New York</publisher-name>, <publisher-loc>New York, NY</publisher-loc>, <year>2000</year>. ISBN 978&#8211;1-4612&#8211;1166-2. doi: <pub-id pub-id-type=\"doi\">10.1007/978-1-4612-1166-2_3</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1007/978-1-4612-1166-2_3\" ext-link-type=\"uri\">https://doi.org/10.1007/978-1-4612-1166-2_3</ext-link>.</mixed-citation></ref><ref id=\"R28\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Liao</surname><given-names>Katherine P</given-names></name>, <name name-style=\"western\"><surname>Sun</surname><given-names>Jiehuan</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianrun A</given-names></name>, <name name-style=\"western\"><surname>Link</surname><given-names>Nicholas</given-names></name>, <name name-style=\"western\"><surname>Hong</surname><given-names>Chuan</given-names></name>, <name name-style=\"western\"><surname>Huang</surname><given-names>Jie</given-names></name>, <name name-style=\"western\"><surname>Huffman</surname><given-names>Jennifer E</given-names></name>, <name name-style=\"western\"><surname>Gronsbell</surname><given-names>Jessica</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>Yichi</given-names></name>, <name name-style=\"western\"><surname>Ho</surname><given-names>Yuk-Lam</given-names></name>, <etal/><article-title>High-throughput multimodal automated phenotyping (map) with application to phewas</article-title>. <source>Journal of the American Medical Informatics Association</source>, <volume>26</volume>(<issue>11</issue>):<fpage>1255</fpage>&#8211;<lpage>1262</lpage>, <year>2019</year>.<pub-id pub-id-type=\"pmid\">31613361</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/jamia/ocz066</pub-id><pub-id pub-id-type=\"pmcid\">PMC6798574</pub-id></mixed-citation></ref><ref id=\"R29\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Lin</surname><given-names>Dan-Yu</given-names></name> and <name name-style=\"western\"><surname>Ying</surname><given-names>Zhiliang</given-names></name>. <article-title>Semiparametric analysis of the additive risk model</article-title>. <source>Biometrika</source>, <volume>81</volume>(<issue>1</issue>):<fpage>61</fpage>&#8211;<lpage>71</lpage>, <year>1994</year>.</mixed-citation></ref><ref id=\"R30\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Liu</surname><given-names>Molei</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>Yi</given-names></name>, and <name name-style=\"western\"><surname>Zhou</surname><given-names>Doudou</given-names></name>. <article-title>Double/debiased machine learning for logistic partially linear model</article-title>. <source>The Econometrics Journal</source>, <volume>24</volume>(<issue>3</issue>):<fpage>559</fpage>&#8211;<lpage>588</lpage>, <month>06</month><year>2021</year>. ISSN 1368&#8211;4221. doi: <pub-id pub-id-type=\"doi\">10.1093/ectj/utab019</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1093/ectj/utab019\" ext-link-type=\"uri\">https://doi.org/10.1093/ectj/utab019</ext-link>.<pub-id pub-id-type=\"pmid\">38223304</pub-id><pub-id pub-id-type=\"pmcid\">PMC10786638</pub-id></mixed-citation></ref><ref id=\"R31\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Negahban</surname><given-names>Sahand</given-names></name>, <name name-style=\"western\"><surname>Ravikumar</surname><given-names>Pradeep</given-names></name>, <name name-style=\"western\"><surname>Wainwright</surname><given-names>Martin J.</given-names></name>, and <name name-style=\"western\"><surname>Yu</surname><given-names>Bin</given-names></name>. <part-title>A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers</part-title>. <source>Technical Report 797</source>, <publisher-name>University of California Berkeley, Department of Statistics</publisher-name>, <year>2010</year>.</mixed-citation></ref><ref id=\"R32\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Negahban</surname><given-names>Sahand N.</given-names></name>, <name name-style=\"western\"><surname>Ravikumar</surname><given-names>Pradeep</given-names></name>, <name name-style=\"western\"><surname>Wainwright</surname><given-names>Martin J.</given-names></name>, and <name name-style=\"western\"><surname>Yu</surname><given-names>Bin</given-names></name>. <article-title>A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers</article-title>. <source>Statistical Science</source>, <volume>27</volume>(<issue>4</issue>):<fpage>538</fpage>&#8211;<lpage>557</lpage>, 2021/12/17/2012. ISSN 08834237. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://www.jstor.org/stable/41714783\" ext-link-type=\"uri\">http://www.jstor.org/stable/41714783</ext-link>. Full publication date: <month>November</month><year>2012</year>.</mixed-citation></ref><ref id=\"R33\"><mixed-citation publication-type=\"other\"><name name-style=\"western\"><surname>Newey</surname><given-names>Whitney K.</given-names></name> and <name name-style=\"western\"><surname>Robins</surname><given-names>James R.</given-names></name>. <source>Cross-fitting and fast remainder rates for semi-parametric estimation</source>, <year>2018</year>.</mixed-citation></ref><ref id=\"R34\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Petersen</surname><given-names>Maya L</given-names></name>, <name name-style=\"western\"><surname>Porter</surname><given-names>Kristin E</given-names></name>, <name name-style=\"western\"><surname>Gruber</surname><given-names>Susan</given-names></name>, <name name-style=\"western\"><surname>Wang</surname><given-names>Yue</given-names></name>, and <name name-style=\"western\"><surname>van der Laan</surname><given-names>Mark J</given-names></name>. <article-title>Diagnosing and responding to violations in the positivity assumption</article-title>. <source>Statistical Methods in Medical Research</source>, <volume>21</volume>(<issue>1</issue>):<fpage>31</fpage>&#8211;<lpage>54</lpage>, <year>2012</year>. doi: <pub-id pub-id-type=\"doi\">10.1177/0962280210386207</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1177/0962280210386207\" ext-link-type=\"uri\">https://doi.org/10.1177/0962280210386207</ext-link>.<pub-id pub-id-type=\"pmid\">21030422</pub-id><pub-id pub-id-type=\"pmcid\">PMC4107929</pub-id></mixed-citation></ref><ref id=\"R35\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Robins</surname><given-names>James M.</given-names></name>, <name name-style=\"western\"><surname>Rotnitzky</surname><given-names>Andrea</given-names></name>, and <name name-style=\"western\"><surname>Zhao</surname><given-names>Lue Ping</given-names></name>. <article-title>Estimation of regression coefficients when some regressors are not always observed</article-title>. <source>Journal of the American Statistical Association</source>, <volume>89</volume>(<issue>427</issue>):<fpage>846</fpage>&#8211;<lpage>866</lpage>, <year>1994</year>. ISSN 01621459. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://www.jstor.org/stable/2290910\" ext-link-type=\"uri\">http://www.jstor.org/stable/2290910</ext-link>.</mixed-citation></ref><ref id=\"R36\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Rotnitzky</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Smucler</surname><given-names>E</given-names></name>, and <name name-style=\"western\"><surname>Robins</surname><given-names>JM</given-names></name>. <article-title>Characterization of parameters with a mixed bias property</article-title>. <source>Biometrika</source>, <volume>108</volume>(<issue>1</issue>):<fpage>231</fpage>&#8211;<lpage>238</lpage>, <month>08</month><year>2020</year>. ISSN 0006&#8211;3444. doi: <pub-id pub-id-type=\"doi\">10.1093/biomet/asaa054</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1093/biomet/asaa054\" ext-link-type=\"uri\">https://doi.org/10.1093/biomet/asaa054</ext-link>.</mixed-citation></ref><ref id=\"R37\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Smucler</surname><given-names>Ezequiel</given-names></name>, <name name-style=\"western\"><surname>Rotnitzky</surname><given-names>Andrea</given-names></name>, and <name name-style=\"western\"><surname>Robins</surname><given-names>James M.</given-names></name>. <article-title>A unifying approach for doubly-robust &#8467;1 regularized estimation of causal contrasts</article-title>. <source>arXiv</source> e-prints, art. arXiv:1904.03737, <month>Apr</month><year>2019</year>.</mixed-citation></ref><ref id=\"R38\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Stone</surname><given-names>Charles J.</given-names></name>. <article-title>Consistent Nonparametric Regression</article-title>. <source>The Annals of Statistics</source>, <volume>5</volume>(<issue>4</issue>): <fpage>595</fpage>&#8211;<lpage>620</lpage>, <year>1977</year>. doi: <pub-id pub-id-type=\"doi\">10.1214/aos/1176343886</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1214/aos/1176343886\" ext-link-type=\"uri\">https://doi.org/10.1214/aos/1176343886</ext-link>.</mixed-citation></ref><ref id=\"R39\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Stone</surname><given-names>Charles J.</given-names></name>. <article-title>Optimal Global Rates of Convergence for Nonparametric Regression</article-title>. <source>The Annals of Statistics</source>, <volume>10</volume>(<issue>4</issue>):<fpage>1040</fpage>&#8211;<lpage>1053</lpage>, <year>1982</year>. doi: <pub-id pub-id-type=\"doi\">10.1214/aos/1176345969</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1214/aos/1176345969\" ext-link-type=\"uri\">https://doi.org/10.1214/aos/1176345969</ext-link>.</mixed-citation></ref><ref id=\"R40\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Tan</surname><given-names>Zhiqiang</given-names></name>. <article-title>Model-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data</article-title>. <source>The Annals of Statistics</source>, <volume>48</volume>(<issue>2</issue>):<fpage>811</fpage>&#8211;<lpage>837</lpage>, <year>2020</year>. doi: <pub-id pub-id-type=\"doi\">10.1214/19-AOS1824</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1214/19-AOS1824\" ext-link-type=\"uri\">https://doi.org/10.1214/19-AOS1824</ext-link>.</mixed-citation></ref><ref id=\"R41\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Tsiatis</surname><given-names>A</given-names></name>. <part-title>Semiparametric Theory and Missing Data</part-title>. <source>Springer Series in Statistics</source>. <publisher-name>Springer</publisher-name><publisher-loc>New York</publisher-loc>, <year>2007</year>. ISBN 9780387373454. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://books.google.com/books?id=xqZFi2EMB40C\" ext-link-type=\"uri\">https://books.google.com/books?id=xqZFi2EMB40C</ext-link>.</mixed-citation></ref><ref id=\"R42\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>Tsybakov</surname><given-names>AB</given-names></name>. <source>Introduction to Nonparametric Estimation</source>. <publisher-name>Springer</publisher-name><publisher-loc>New York, NY</publisher-loc>, <year>2009</year>. doi: <pub-id pub-id-type=\"doi\">10.1007/b13794</pub-id>.</mixed-citation></ref><ref id=\"R43\"><mixed-citation publication-type=\"other\"><collab>U.S. Cancer Statistics Working Group</collab>. <source>U.s. cancer statistics data visualizations tool, based on 2021 submission data (1999&#8211;2019): U.s. department of health and human services, centers for disease control and prevention and national cancer institute</source>, <month>June</month><year>2022</year>.</mixed-citation></ref><ref id=\"R44\"><mixed-citation publication-type=\"book\"><name name-style=\"western\"><surname>van der Vaart</surname><given-names>AW</given-names></name>. <article-title>Asymptotic Statistics</article-title>. <source>Cambridge Series in Statistical and Probabilistic Mathematics</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>1998</year>. doi: <pub-id pub-id-type=\"doi\">10.1017/CBO9780511802256</pub-id>.</mixed-citation></ref><ref id=\"R45\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Vermeulen</surname><given-names>Karel</given-names></name> and <name name-style=\"western\"><surname>Vansteelandt</surname><given-names>Stijn</given-names></name>. <article-title>Bias-reduced doubly robust estimation</article-title>. <source>Journal of the American Statistical Association</source>, <volume>110</volume>(<issue>511</issue>):<fpage>1024</fpage>&#8211;<lpage>1036</lpage>, <year>2015</year>. doi: <pub-id pub-id-type=\"doi\">10.1080/01621459.2014.958155</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1080/01621459.2014.958155\" ext-link-type=\"uri\">https://doi.org/10.1080/01621459.2014.958155</ext-link>.</mixed-citation></ref><ref id=\"R46\"><mixed-citation publication-type=\"other\"><name name-style=\"western\"><surname>Wang</surname><given-names>Yuhao</given-names></name> and <name name-style=\"western\"><surname>Shah</surname><given-names>Rajen D.</given-names></name>. <source>Debiased inverse propensity score weighting for estimation of average treatment effects with high-dimensional confounders</source>, <year>2020</year>.</mixed-citation></ref><ref id=\"R47\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Xie</surname><given-names>Yuan-Hong</given-names></name>, <name name-style=\"western\"><surname>Chen</surname><given-names>Ying-Xuan</given-names></name>, and <name name-style=\"western\"><surname>Fang</surname><given-names>Jing-Yuan</given-names></name>. <article-title>Comprehensive review of targeted therapy for colorectal cancer</article-title>. <source>Signal Transduction and Targeted Therapy</source>, <volume>5</volume>(<issue>1</issue>):<fpage>22</fpage>, <month>Mar</month><year>2020</year>. ISSN 2059&#8211;3635. doi: <pub-id pub-id-type=\"doi\">10.1038/s41392-020-0116-z</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1038/s41392-020-0116-z\" ext-link-type=\"uri\">https://doi.org/10.1038/s41392-020-0116-z</ext-link>.<pub-id pub-id-type=\"pmid\">32296018</pub-id><pub-id pub-id-type=\"pmcid\">PMC7082344</pub-id></mixed-citation></ref><ref id=\"R48\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Yuan</surname><given-names>Qianyu</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianrun</given-names></name>, <name name-style=\"western\"><surname>Hong</surname><given-names>Chuan</given-names></name>, <name name-style=\"western\"><surname>Du</surname><given-names>Mulong</given-names></name>, <name name-style=\"western\"><surname>Johnson</surname><given-names>Bruce E.</given-names></name>, <name name-style=\"western\"><surname>Lanuti</surname><given-names>Michael</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>, and <name name-style=\"western\"><surname>Christiani</surname><given-names>David C.</given-names></name>. <article-title>Performance of a Machine Learning Algorithm Using Electronic Health Record Data to Identify and Estimate Survival in a Longitudinal Cohort of Patients With Lung Cancer</article-title>. <source>JAMA Network Open</source>, <volume>4</volume>(<issue>7</issue>):<fpage>e2114723</fpage>&#8211;<lpage>e2114723</lpage>, <month>07</month><year>2021</year>. ISSN 2574&#8211;3805. doi: <pub-id pub-id-type=\"doi\">10.1001/jamanetworkopen.2021.14723</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1001/jamanetworkopen.2021.14723\" ext-link-type=\"uri\">https://doi.org/10.1001/jamanetworkopen.2021.14723</ext-link>.<pub-id pub-id-type=\"pmid\">34232304</pub-id><pub-id pub-id-type=\"pmcid\">PMC8264641</pub-id></mixed-citation></ref><ref id=\"R49\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Yinchi</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianrun</given-names></name>, <name name-style=\"western\"><surname>Yu</surname><given-names>Sheng</given-names></name>, <name name-style=\"western\"><surname>Cho</surname><given-names>Kelly</given-names></name>, <name name-style=\"western\"><surname>Hong</surname><given-names>Chuan</given-names></name>, <name name-style=\"western\"><surname>Sun</surname><given-names>Jiehuan</given-names></name>, <name name-style=\"western\"><surname>Huang</surname><given-names>Jie</given-names></name>, <name name-style=\"western\"><surname>Ho</surname><given-names>Yuk-Lam</given-names></name>, <name name-style=\"western\"><surname>Ananthakrishnan</surname><given-names>Ashwin N.</given-names></name>, <name name-style=\"western\"><surname>Xia</surname><given-names>Zonggi</given-names></name>, <name name-style=\"western\"><surname>Shaw</surname><given-names>Stanley Y.</given-names></name>, <name name-style=\"western\"><surname>Gainer</surname><given-names>Vivian</given-names></name>, <name name-style=\"western\"><surname>Castro</surname><given-names>Victor</given-names></name>, <name name-style=\"western\"><surname>Link</surname><given-names>Nicholas</given-names></name>, <name name-style=\"western\"><surname>Honerlaw</surname><given-names>Jacqueline</given-names></name>, <name name-style=\"western\"><surname>Huang</surname><given-names>Selena</given-names></name>, <name name-style=\"western\"><surname>Gagnon</surname><given-names>David</given-names></name>, <name name-style=\"western\"><surname>Karlson</surname><given-names>Elizabeth W.</given-names></name>, <name name-style=\"western\"><surname>Plenge</surname><given-names>Robert M.</given-names></name>, <name name-style=\"western\"><surname>Szolovits</surname><given-names>Peter</given-names></name>, <name name-style=\"western\"><surname>Savova</surname><given-names>Guergana</given-names></name>, <name name-style=\"western\"><surname>O&#8217;Donnell</surname><given-names>Christopher</given-names></name>, <name name-style=\"western\"><surname>Murphy</surname><given-names>Shawn N.</given-names></name>, <name name-style=\"western\"><surname>Gaziano</surname><given-names>J. Michael</given-names></name>, <name name-style=\"western\"><surname>Kohane</surname><given-names>Isaac</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>Tianxi</given-names></name>, and <name name-style=\"western\"><surname>Liao</surname><given-names>Katherine P.</given-names></name>. <article-title>High-throughput phenotyping with electronic medical record data using a common semi-supervised approach (phecap)</article-title>. <source>Nature Protocal</source>, page To appear, <year>2019</year>.</mixed-citation></ref><ref id=\"R50\"><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Yuqian</given-names></name>, <name name-style=\"western\"><surname>Chakrabortty</surname><given-names>Abhishek</given-names></name>, and <name name-style=\"western\"><surname>Bradic</surname><given-names>Jelena</given-names></name>. <article-title>Double robust semi-supervised inference for the mean: selection bias under MAR labeling with decaying overlap</article-title>. <source>Information and Inference: A Journal of the IMA</source>, <volume>12</volume>(<issue>3</issue>):<fpage>2066</fpage>&#8211;<lpage>2159</lpage>, <month>07</month><year>2023</year>. ISSN 2049&#8211;8772. doi: <pub-id pub-id-type=\"doi\">10.1093/imaiai/iaad021</pub-id>. URL <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1093/imaiai/iaad021\" ext-link-type=\"uri\">https://doi.org/10.1093/imaiai/iaad021</ext-link>.</mixed-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"F1\" orientation=\"portrait\"><label>Figure 1:</label><caption><p id=\"P113\">Causal diagrams of double missing SSL setting with missing treatment and outcome. The surrogates <inline-formula><mml:math id=\"M1\" display=\"inline\"><mml:mi mathvariant=\"bold\">S</mml:mi></mml:math></inline-formula> represent the imprecise documentation of <inline-formula><mml:math id=\"M2\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M3\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula>, which should be predictive for <inline-formula><mml:math id=\"M4\" display=\"inline\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"M5\" display=\"inline\"><mml:mi>Y</mml:mi></mml:math></inline-formula> but not affecting the causal identification based on perfect data <inline-formula><mml:math id=\"M6\" display=\"inline\"><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2084485-f0001.jpg\"/></fig><fig position=\"float\" id=\"F2\" orientation=\"portrait\"><label>Figure 2:</label><caption><p id=\"P114\">Visualized simulation settings. Left-the models for PS and OR under the low-dimensional setting. Right-the mixture Beta distribution for surrogates at different level of prediction accuracy (AUC 0.8, 0.9, 0.95, 0.99, 0.999) at the median covariate (<inline-formula><mml:math id=\"M7\" display=\"inline\"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> under low-dimensional smooth model and <inline-formula><mml:math id=\"M8\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> under high-dimensional logistic regression).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2084485-f0002.jpg\"/></fig><fig position=\"float\" id=\"F3\" orientation=\"portrait\"><label>Figure 3:</label><caption><p id=\"P115\">Heat map for relative efficiency of the SMMAL compared to the benchmark supervised learning in all four simulation settings. Deeper red indicates larger advantage of the semi-supervised estimation. We set relative efficiency one as white in all plots, but the scale varies between low-dimensional setting and high-dimensional settings.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2084485-f0003.jpg\"/></fig><fig position=\"float\" id=\"F4\" orientation=\"portrait\"><label>Figure 4:</label><caption><p id=\"P116\">Heat map for coverage of 95% confidence intervals by unsupervised learning. White marks 0.95 coverage rate. Orange marks 0.8 coverage rate. Deeper red indicates poorer coverage rate by unsupervised learning.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2084485-f0004.jpg\"/></fig><fig position=\"float\" id=\"F5\" orientation=\"portrait\"><label>Figure 5:</label><caption><p id=\"P117\">Point estimate and 95% confidence interval of average risk difference from crude, Double Machine-Learning (DML), calibrated (Cal) and SMMAL analyses. Supervised learning (SL) benchmark analysed only uses the labeled data. Unsupervised learning (UL) benchmark analyses used dichotomized surrogates by matching prevalence observed in labeled data. The RE value indicated the SMMAL&#8217;s relative efficiency in comparison with the two supervised benchmark methods (ratio of estimated variances).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2084485-f0005.jpg\"/></fig><table-wrap position=\"float\" id=\"T1\" orientation=\"portrait\"><label>Table 1:</label><caption><p id=\"P118\">List of parameters used in the mixture Beta distribution for the surrogates.</p></caption><table frame=\"box\" rules=\"rows\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Setting</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">OK</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Reasonable</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Good</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Great</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Perfect</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AUC</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.95</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.99</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.999</td></tr><tr><td colspan=\"6\" align=\"left\" valign=\"middle\" rowspan=\"1\">Low-dimensional smooth model</td></tr><tr style=\"border-bottom: hidden\"><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M9\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.99</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.86</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.49</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M10\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.70</td></tr><tr><td colspan=\"6\" align=\"left\" valign=\"middle\" rowspan=\"1\">High-dimensional logistic regression</td></tr><tr style=\"border-bottom: hidden\"><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M11\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.99</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.64</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M12\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.51</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.89</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.55</td></tr><tr><td colspan=\"6\" align=\"left\" valign=\"middle\" rowspan=\"1\">High-dimensional regression: mis-specified PS</td></tr><tr style=\"border-bottom: hidden\"><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M13\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.55</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M14\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.93</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.52</td></tr><tr><td colspan=\"6\" align=\"left\" valign=\"middle\" rowspan=\"1\">High-dimensional regression: mis-specified OR</td></tr><tr style=\"border-bottom: hidden\"><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M15\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.55</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"M16\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.93</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.52</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"T2\" orientation=\"portrait\"><label>Table 2:</label><caption><p id=\"P119\"><bold>Accuracy of extracted EHR feature counts</bold> for targeted therapy and 1-year progression (defined as new metastasis site) free survival from EHR valided over 100 patients reviewed by abstractor. False positive rates (FPR) and false negative rates (FNR) were calculated by the dichotomized extractions: Benchmark features &#8211; count &gt; 0; <bold>Engineered features</bold> &#8211; classification by the quantiles matching prevalence in gold-standard labels. Area under reception operating curve (AUC) were calculated using count/score as predictor (death encoded as a very large value 1000).</p></caption><table frame=\"box\" rules=\"none\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Surrogate</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">FPR</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">FNR</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AUC</th></tr></thead><tbody><tr><td colspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-top: solid 1px;border-bottom: solid 1px\" rowspan=\"1\">Targeted Therapy</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Medication Code</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.44</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.17</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.60</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>Mention in Note</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.35<xref rid=\"TFN1\" ref-type=\"table-fn\">*</xref></td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.10<xref rid=\"TFN1\" ref-type=\"table-fn\">*</xref></td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>0.93</bold>\n</td></tr><tr><td colspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-top: solid 1px;border-bottom: solid 1px\" rowspan=\"1\">1-year Progression Free Survival</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Death Registry</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.02</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.43</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Death &amp; New Site Code</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.34</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.20</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.84</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Death &amp; New Site in Note</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.31</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.20</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.85</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>Terminal-Progression Score</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.31<xref rid=\"TFN1\" ref-type=\"table-fn\">*</xref></td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.10<xref rid=\"TFN1\" ref-type=\"table-fn\">*</xref></td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>0.93</bold>\n</td></tr></tbody></table><table-wrap-foot><fn id=\"TFN1\"><p id=\"P120\">Straightforward rule based extraction (indicated by *) failed to capture treatment and response.</p></fn><fn id=\"TFN2\"><p id=\"P121\">Two surrogates in <bold>bold font</bold> were chosen for SMMAL for their <bold>reasonably good AUC</bold>.</p></fn></table-wrap-foot></table-wrap><table-wrap position=\"float\" id=\"T3\" orientation=\"portrait\"><label>Table 3:</label><caption><p id=\"P122\">Baseline characteristics of full study cohort and two arms in the labeled subset. The format is &#8220;count (percentage %)&#8221; for binary/categorical variables and &#8220;mean (standard deviation)&#8221; for numerical variables.</p></caption><table frame=\"box\" rules=\"none\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Full data</th><th colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Labeled set</th></tr><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Chemotherapy</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Targeted Therapy</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Size</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4147</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">79</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">21</td></tr><tr><td colspan=\"4\" align=\"left\" valign=\"middle\" style=\"border-top: solid 1px\" rowspan=\"1\">\n<bold>Demographics</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Age at Metastasis</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.5 (13.8)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.2 (11.4)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.7 (15.9)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Female</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1926 (46%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">46 (58%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">11 (52%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">White</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3470 (84%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68 (86%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">20 (95%)</td></tr><tr><td colspan=\"4\" align=\"left\" valign=\"middle\" style=\"border-top: solid 1px\" rowspan=\"1\">\n<bold>Cancer characteristics at diagnosis</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Left Colon Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">221 (5%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5 (6%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Right Colon Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">890 (21%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">23 (29%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8 (38%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Transverse Colon Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">420 (10%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9 (11%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1 (5%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Sigmoid Colon Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2092 (50%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44 (56%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9 (43%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Rectum Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2002 (48%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">43 (54%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5 (24%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Metastasis Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2674 (64%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">50 (63%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">16 (76%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Lymph Node Tumor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">364 (9%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10 (13%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stage I</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">55 (1%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stage II</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">159 (4%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3 (4%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stage II</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">992 (24%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">22 (28%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2 (10%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stage IV</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2546 (61%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">46 (58%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">17 (81%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stage Missing</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">395 (10%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8 (10%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2 (10%)</td></tr><tr><td colspan=\"4\" align=\"left\" valign=\"middle\" style=\"border-top: solid 1px\" rowspan=\"1\">\n<bold>Cancer characteristics at metastasis</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Year since Diagnosis</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7 (2)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.5 (1)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8 (1.7)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Lung Metastasis Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">646 (16%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10 (13%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8 (38%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Liver Metastasis Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1694 (41%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">27 (34%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">14 (67%)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Liver Metastasis in Note</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1422 (34%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">27 (34%)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">12 (57%)</td></tr><tr><td colspan=\"4\" align=\"left\" valign=\"middle\" style=\"border-top: solid 1px\" rowspan=\"1\">\n<bold>Treatments between diagnosis and metastasis</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Chemotherapy Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.4 (5.3)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.3 (3.7)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Radiotherapy Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.1 (35.1)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.5 (30.4)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">7.2 (29.1)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Colon Biopsy Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6 (1.7)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.5 (1.5)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0 (0)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Colon Rescission Code</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.4 (0.8)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.3 (0.7)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.2 (0.5)</td></tr><tr><td colspan=\"4\" align=\"left\" valign=\"middle\" style=\"border-top: solid 1px\" rowspan=\"1\">\n<bold>Healthcare utilization</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Before Metastasis</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">29.7 (59.3)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">36.2 (74.5)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">13 (21.2)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">One Year Before Metastasis</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9.8 (15.4)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.2 (14.7)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4.2 (8.1)</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc J Mach Learn Res J Mach Learn Res 319 nihpa Journal of machine learning research : JMLR 1532-4435 1533-7928 pmc-is-collection-domain yes pmc-collection-title NIHPA Author Manuscripts PMC12671556 PMC12671556.1 12671556 12671556 NIHMS2084485 41341524 NIHMS2084485 NIHPA2084485 1 Article Efficient and Robust Semi-supervised Estimation of Average Treatment Effect with Partially Annotated Treatment and Response Hou Jue Division of Biostatistics, University of Minnesota School of Public Health, Minneapolis, MN 55455, USA. Mukherjee Rajarshi Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02120, USA Cai Tianxi Department of Biostatistics, Harvard T.H. Chan School of Public Health, Department of Biomedical Informatics, Harvard Medical School, Boston, MA 02120, USA HOU00123@UMN.EDU 2025 26 501654 40 25 05 2025 03 12 2025 03 12 2025 05 12 2025 https://creativecommons.org/licenses/by/4.0/ License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided at http://jmlr.org/papers/v26/23-1587.html . A notable challenge of leveraging Electronic Health Records (EHR) for treatment effect assessment is the lack of precise information on important clinical variables, including the treatment received and the response. Both treatment information and response cannot be accurately captured by readily available EHR features in many studies and require labor-intensive manual chart review to precisely annotate, which limits the number of available gold standard labels on these key variables. We considered average treatment effect (ATE) estimation when 1) exact treatment and outcome variables are only observed together in a small labeled subset and 2) noisy surrogates of treatment and outcome, such as relevant prescription and diagnosis codes, along with potential confounders are observed for all subjects. We derived the efficient influence function for ATE and used it to construct a semi-supervised multiple machine learning (SMMAL) estimator. We justified that our SMMAL ATE estimator is semi-parametric efficient with B-spline regression under low-dimensional smooth models. We developed the adaptive sparsity/model doubly robust estimation under high-dimensional logistic propensity score and outcome regression models. Results from simulation studies demonstrated the validity of our SMMAL method and its superiority over supervised and unsupervised benchmarks. We applied SMMAL to the assessment of targeted therapies for metastatic colorectal cancer in comparison to chemotherapy. semi-parametric efficiency double robustness high-dimensional regression semi-supervised learning pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript yes pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1 Introduction The 21st Century Cures Act and the Prescription Drug User Fee Act VII have shone a spotlight on the use of real-world evidence, generated from real-world data, to support regulatory-decision making on drug effectiveness. Large scale electronic health records (EHRs) data are being increasingly used for creating the real-world evidence on treatment effectiveness or efficacy ( Franklin et al., 2021 ). In addition to the observational nature, another notable challenge in leveraging EHR for treatment effect assessment lies in the lack of readily available data for key clinical variables, including the treatment being investigated and the outcome of interest. Response variables such as disease progression may not be well represented by readily available EHR features ( Bartlett et al., 2019 ). Treatment information can be partially captured but not always accurately reflected by procedure codes or medication prescription codes. New therapies may not be well coded in the introduction stage immediately after regulatory approval, and treatment initiation may be later than prescription date due to external factors such as insurance approval delay. For example in a real-world evidence study comparing chemotherapies and targeted therapies as first-line treatment for metastatic colorectal cancer, we discovered based on chart-review of 100 patients by a medical expert that 1) the progression-free-survival (PFS) outcomes were poorly structured in EHRs without clear indicators for progression or complete mortality data, and 2) the medication codes or natural language processing (NLP) identified mentions in notes could not accurately capture the use of targeted therapies (see Table 2 ). Although it is possible to improve treatment or response definition by combining multiple EHR features through rule based or machine learning algorithms, these EHR derived features are at best good &#8220;surrogates&#8221; for approximating the true treatment or response information at patient level. Compared to the classic definition of surrogate, the notion of surrogate in retrospective EHR studies shares the availability trait but differs in the temporal order and causal pathway. In the advanced stage cancer trials or prospective studies, the progress-free-survival is often used as surrogate for overall survival because progression sometimes can be captured at an earlier time. In EHR studies, however, researchers do not have readily available progression data Y unless they perform the labor intensive manual chart review ( Griffith et al., 2019 ), so it is natural to borrow information from the documentations about progression S like occurrence of diagnosis codes about secondary malignant neoplasm or NLP identified mention of metastasis at distant parts of body. These documentations S are considered as &#8220;surrogates&#8221; because 1) they can partially indicate progression, and 2) they are accessible earlier during the research process. Since the documentations S were recorded according to the true progression status Y , it is more reasonable to consider the true progression status temporally preceded and casually affected the documentation, Y &#8594; S in a causal diagram. Directly using these surrogates as true treatment and outcome which would potentially induce bias in the subsequent analysis ( Beaulieu-Jones et al., 2020 ). On the other hand, annotating exact treatment and response variables via manual chart review by domain expert is resource intensive, leading to limited sample size for gold standard labels on these key data. It is thus of great practical significance to leverage both the small number of gold standard labels and the vast unlabeled data to derive unbiased and efficient inference about the average treatment effect (ATE), fundamentally a nested problem with both missing data and causal inference components. When the labeling proportion is too small for standard (missing data) positivity assumption, the setting is often referred to as the semi-supervised learning (SSL). Additional challenges arise from the high dimensionality of potential confounders. Unlike traditional cohort studies with a pre-specified number of clinical variables, EHRs provide rich data on a broader range and larger number of confounding factors ( Hou et al., 2021c ). Furthermore, multiple EHR features may be necessary to represent one specific clinical variable, further amplifying the dimensionality of features necessary to capture the underlying confounding factors. The complexity of the models from the high-dimensionality also increases the risk of model mis-specification for the propensity score (PS) and the outcome regression (OR). To the best of our knowledge, no method currently exists to estimate ATE under the SSL setting when both the treatment group, denoted by A , and the response, denoted by Y , are only observed in a small subset of the full data. We focus on the missing data patterns resulting from the lack of readily available data on the exact clinical information like treatment A and outcomes Y . For small subset, manual annotations can be created to recover the exact Y and A , but researchers have to rely on scalable yet imperfect computational tools to extract treatment outcome information over the majority of the vast EHR cohort, producing the surrogates S for Y and A . For conciseness, we refer to this specific SSL setting as double missing SSL. In this paper, we address the methodology gap by proposing Semi-supervised M ultiple MA chine L earning (SMMAL) estimators for ATE that leverage both the fully observed surrogates for Y and A , denoted by S , and the partially observed gold standard labels on Y and A . Under the supervised setting where both A and Y are observed, much progress has been made in recent years on estimation of ATE with confounding adjustment from machine-learning and/or high-dimensional regression. In the low-dimensional setting, the estimation of ATE is a well studied problem including procedures that achieve semi-parametric efficiency and double robustness ( Robins et al., 1994 ; Bang and Robins, 2005 ). Extension to the high-dimensional setting, however, is not straightforward due to the slower convergence rates in the estimated model parameters and the difficulty posed not only by the bias and variance trade-off in the process of regularization but also by the inherent information theoretic barriers to obtaining fast enough estimation rates in high dimensional problems. Similar challenges arise when incorporating more flexible machine-learning models to overcome model mis-specifications. Following intuitions parallel to the low-dimensional setting, flexible approaches for confounder adjustments have been proposed via modeling of PS and OR, including L 1 regularized regression ( Farrell, 2015 ), neural network ( Farrell et al., 2021 ), and a general machine learning framework ( Chernozhukov et al., 2018 ). Several methods accommodated the high dimensional confounder and achieved statistical inference on ATE based on consistent estimation for PS and OR, which translated to proper model specification and sparsity for high-dimensional regressions ( Belloni et al., 2013 ; Liu et al., 2021 ; Hou et al., 2021a ; Belloni et al., 2017 , e.g.). Tan (2020) proposed a calibrated estimation that leads to valid inference for the average treatment effect even if one of the high-dimensional logistic PS or linear OR model is mis-specified. Smucler et al. (2019) formalized the concept of double robustness in high-dimensional setting by defining the sparsity double robustness and model double robustness properties; and also generalized the idea of Tan (2020) to a wide range of PS and OR models. For data with sample size n and dimension of covariate p , Smucler et al. (2019) defined the sparsity double robustness as producing n -asymptotic normal estimator for ATE from consistently estimated PS and OR models as long as the product of sparsities for PS and OR models grow slower than n log ( p ) . The model double robustness further allows the estimation of either PS or OR model to be inconsistent while still achieving n -asymptotic normal estimation of ATE. Bradic et al. (2019) established a sharper sparsity double robustness property of the calibrated estimation. Unlike the two-model approaches (PS and OR) listed above, Wang and Shah (2020) considered a single model approach in which they debiased the regularized PS model in the inverse probability of treatment weighting estimator to achieve n -inference. Semi-supervised estimation for ATE is less studied. Existing literatures focused almost entirely on the setting where Y is observed for patients in the small labeled set of size n while A and surrogates/proxies of Y along with confounders X are observed for all subjects of size N . The semi-supervised learning (SSL) setting refers to the missing data proportion ( N - n ) / N for Y tending to 1 along an asymptotic sequence where both number of labels and total sample size tend to infinity, n , N &#8594; &#8734; . The SSL setting is distinguished from classical missing data problems as the standard (missing data) positivity assumption on observation rate is violated. SSL estimators for the ATE have been proposed by Cheng et al. (2021) when Y is missing-completely-at-random (MCAR) and by Zhang et al. (2023) and Kallus and Mao (2024) when Y is missing-at-random (MAR). However, these methods cannot be easily adapted to the setting where both Y and A are missing. The missingness in A is fundamentally different from the missingness in Y since treatment is an internal node in the causal pathway &#8220;confounder X &#8594; treatment ( A ) &#8594; outcome ( Y ) &#8594; surrogates ( S ) &#8221; ( Figure 1 ), introducing technical challenges on the projection by conditional expectation in the semi-parametric analysis. In this paper, we propose an efficient and robust SSL estimator for ATE when both Y and A are only observed for a small labeled subset but the confounders X and surrogates S for Y and A are observed for all N patients. We derived the SMMAL estimator by first deriving the efficient influence function for the ATE under this double missing SSL setting and then constructing a cross-fitted multiple machine learning estimator. We subsequently provided a formal characterization of semi-parametric efficiency under the double missing SSL setting with the SMMAL estimator coupled to B-spline regressions over low-dimensional space. We also designed a doubly robust estimator with a two-layer cross-fitted calibrated estimation for high-dimensional logistic PS and OR models. Via cross-fitting and a truncation in initial OR/PS predictions, we relaxed the sparsity assumptions in the initial estimation for PS and OR, previously required for n -inference of ATE ( Tan, 2020 ; Smucler et al., 2019 ). We further showed that our doubly robust SMMAL estimator attains 1) the rate double robustness when both PS and OR models are correct and 2) model double robustness when one of them is correct, as defined by Smucler et al. (2019) . The SMMAL estimator also does not require correct specifications of the imputation models for A or Y for proper inference under MCAR assumption. We summarize our key contributions herein: We formalized the efficient estimation under a general SSL setting (including specifically the double missing SSL setting) with a decaying observation rate that violates the classical (missing data) positivity assumption. Our theory justified the efficiency claims of existing works and can provide benchmark for future work in this direction. A discussion regarding the subtleties and challenges involved in this formalization and subsequent analyses can be found in Remark 6. We laid out a general approach for efficient SSL with a complex missing data structure. Our general approach is particularly convenient when the missing data and dependence patterns render typical projection approach difficult for the semi-parametric theory. A explanation of the challenge from missing treatment under double missing SSL setting can be found in Remark 4, and the general framework is given in Section 7 . We made progress in statistical inference on ATE based on the doubly robust estimation with high-dimensional confounders achieving the sparsity/model double robustness. We generalized to the SSL setting the techniques in the existing literatures on calibrated estimation of PS and OR models so that the final ATE estimator has weak dependence on estimation of these models, characterized by small derivatives, also known as the Neyman Orthogonality ( Chernozhukov et al., 2018 ). Using a truncation of initial model prediction, we removed the sparsity requirement in initial estimation. In addition, we demonstrated that the SSL estimation derived from our modified semi-parametric theory contributed to robustness toward estimation of the imputation models. The comparison with related work can be found in Remark 9. The paper is organized as follows. In Section 2 , we introduce our causal inference structure under the double missing SSL setting along with the notations. In Section 3 , we first present the efficient influence function, followed by the multiple machine learning estimator and the model multiply robust estimator derived from the efficient influence function. In Section 4 , we state the theoretical guarantees of the n -inference on the ATE from our methods, whose proofs are provided in the Supplementary Materials . We also provide the semi-parametric efficiency lower bound for average treatment effect under double missing SSL setting in low-dimensional space. In Section 5 , we assess the finite sample performance of our SSL methods and compare them to supervised benchmarks. In Section 6 , we apply SMMAL to the real-world evidence study on targeted therapies for metastatic colorectal cancer in comparison with chemotherapy. In Section 7 , we offer the efficiency lower bound for general low-dimensional parameters under broader SSL settings with flexible missing data components. In Section 8 , we conclude with a brief discussion. 2 Setting and notation For the i -th observation in a study of N subjects, Y i &#8712; R denotes the outcome variable, A i &#8712; { 0, 1 } denotes the treatment group indicator, R i &#8712; { 0, 1 } indicates whether ( Y i , A i ) is annotated, S i &#8712; R q denotes the surrogates for Y i and A i , and X i &#8712; R p + 1 denotes the vector of potential confounders including 1 as the first element. We use the notations without the subscript indices Y , A , R , S , X to denote the generic versions of these random variables. In EHR studies, routine documentations on treatments and outcomes in the form of digital codes and mentions in narrative notes are often prone to errors ( Zhang et al., 2019 ) and hence can only serve as surrogates S . To ascertain Y and A , researchers may design the sampling scheme for a representative labeled subset, i &#8712; [ N ] : R i = 1 , over which the exact data Y i , A i are annotated by medical experts, where [ N ] = { 1 , &#8230; , N } . For those with R i = 0 , exact values of the pair ( Y i , A i ) are not ascertained, creating the joint missingness of ( Y i , A i ) . The observed data consist of N independent and identically distributed (i.i.d.) random vectors, &#119967; = D i = R i , R i Y i , R i A i , W i &#8868; &#8868; , i = 1 , &#8230; , N , where W i = X i &#8868; , S i &#8868; &#8868; . We assume the MCAR mechanism for the sampling process with (1) R &#10987; ( Y , A , X , S ) , and the number of labelled sample is n = &#8721; i = 1 N R i with the proportion of labeled observation being &#961; N = E ( R ) &#8712; ( 0, 1 ) with &#961; N &#8594; 0 as N &#8594; &#8734; while the expected number of labels also grow asymptotically to infinity &#961; N N &#8594; &#8734; . Under MCAR formulation, the size of labeled subset n is a random variable asymptotically equivalent to &#961; N N , as n / &#961; N N = 1 + o p ( 1 ) . We use a simplified notation &#8220; V N &#8781; U N &#8221;, e.g. n &#8781; &#961; N N , to describe the equivalence in stochastic order, V N / U N = O p ( 1 ) and U N / V N = O p ( 1 ) . To better reflect the dependence on labeled set and compare with supervised benchmarks, we use n instead of &#961; N N when describing the asymptotic orders. As the exception, we use &#961; N N in the derivation of efficiency lower bound. Extension to MAR is plausible through modeling and estimating the missing data pattern P ( R = 1 &#8739; W ) under classical semi-parametric theory, but a few technical and practical challenges exist as we listed in the Section 8 . Thorough investigation of the MCAR setting would already provide methodological guidance to the rapidly growing real-world evidence studies in which random subsets are selected for gold-standard validation of intervention and outcome data ( Hou et al., 2023 ). Our MCAR formulation, as opposed to the two sample formulations in existing statistical semi-supervised learning literatures ( Chakrabortty et al., 2019 ; Zhang et al., 2023 ; Hou et al., 2021b ), connects better with existing literature on semiparametric estimation and missing data. The results under MCAR, with minor modification in theoretical derivations, are largely applicable to the other similar formulation like first n samples R i = I ( i &#8804; n ) or sampling without replacement for a deterministic sequence n . To properly define the causally interpretable ATE, we adopt the typical counterfactual outcome framework and its standard assumptions ( Imbens and Rubin, 2015 ; Hernan and Robins, 2023 ). Let Y ( a ) be the counterfactual outcome with treatment set as a , for a &#8712; { 0, 1 } . The ATE is defined as (2) &#916; * = E Y ( 1 ) - Y ( 0 ) . We make the following standard assumptions regarding the triplet ( Y , A , X ) , Assumption 1 ( a ) Consistency: Y = Y ( A ) ; ( b ) (Causal inference) Positivity of treatment assignment : 1 / M &#8804; P ( A = 1 &#8739; X ) &#8804; 1 - 1 / M almost surely for an absolute constant M &lt; &#8734; ; ( c ) Ignorability : Y ( 1 ) , Y ( 0 ) &#10987; A &#8739; X . The (causal inference) positivity in Assumption 1 is imposed on the treatment assignment A , which should be distinguished from the (missing data) positivity regarding the observation indicator R . Under the Assumption 1, the ATE can be alternatively expressed as (3) &#916; * = E { E ( Y &#8739; X , A = 1 ) - E ( Y &#8739; X , A = 0 ) } . In the motivating EHR studies, S i represents the documentations and retrospective data curation of ( Y i , A i ) , such as the presence of diagnosis code in follow-up for outcomes and medication codes at baseline for treatments, that are conceivably determined by the underlying truth ( Y i , A i ) . In Figure 1 , we present a setting such that the surrogates can be classified into those for A i and those for Y i , S i = S i , a &#8868; , S i , y &#8868; &#8868; . The causal identification ( 3 ) still holds with the introduction of additional variable S i . Sometimes S i may contain colliders that are affected by both treatment A i and outcome Y i , A i &#8594; S i &#8592; Y i , e.g. increased code counts from frequent healthcare visits as part of intense treatment or caused by poor outcome. Adjustment of colliders would distort causal relationship A i &#8594; Y i and should be excluded from causal identification ( Hernan and Robins, 2023 , Chapter 6.4). Throughout the paper, we assume MCAR ( 1 ) and Assumption 1. Remark 1 We herein summarize the setting of our study . Over a small randomly sampled labeled subset, we can causally identify ATE with outcome Y i , binary treatment A i and confounders X i under standard consistency, (causal inference) positivity and ignorability assumptions. We seek to robustly enhance the efficiency of estimating ATE by incorporating the large unlabeled data containing confounders X i and surrogates S i without stringent model assumptions on S i . The method should adaptively achieve better efficiency if S i &#8739; X i can effectively inform Y i , A i &#8739; X i ; the same property as the ATE estimated from labeled subset if S i &#8739; X i cannot inform Y i , A i &#8739; X i . 3 SMMAL Estimation We start by presenting in Section 3.1 the efficient influence function under the double missing SSL setting without assuming known model for unlabeled data. Deviating from the classical missing data setting, the derived efficient influence functions under double missing SSL setting have diverging variances, which requires a formal justification of its connection with efficiency lower bound in Sections 4.2 . Our approach is hence distinguished from existing SSL literatures ( Cheng et al., 2021 ; Kallus and Mao, 2024 ) that considered a simplified theoretical formulation to define the efficient influence function assuming known model for large unlabeled data. Then, we discuss the estimation of ATE with different ways of estimating the nuisance models involved in the efficient influence function in Section 3.2 for low-dimensional X and in Section 3.3 for high-dimensional X . As the standard tool to control over-fitting from using estimated models in subsequent estimation procedures ( Lin and Ying, 1994 ; Chernozhukov et al., 2018 ; Newey and Robins, 2018 ; Hou et al., 2021b ), cross-fitting is adopted for both settings, where we split the data into K (e.g. K = 5 ) folds of approximately equal size. For k = 1 , &#8230; , K , we let &#8464; k denote the index set for the k th fold of the data with size N k = &#8464; k and let &#8464; k c = { 1 , &#8230; , N } &#8726; &#8464; k , where | &#8464; | denotes the cardinality of &#8464; . Here, we do not split the folds separately for labeled and the unlabeled data because the label indicator R i is random under the MCAR formulation ( 1 ). 3.1 The efficient influence function We define the following nuisance models: PS : P A = a &#8739; X = &#960; a , X , OR : E Y &#8739; A = a , X = &#956; a , X , Imputations : P A = a &#8739; W = &#928; a , W , E Y &#8739; A = a , W = m a , W . We use the subscript star to indicate the true models, &#960; * , &#956; * , &#928; * , m * . Starting from the efficient influence function with complete (cmp) observation of treatment and outcome ( Robins et al., 1994 ; Kallus and Mao, 2024 ), &#981; cmp Y , A , X = &#956; * 1 , X - &#956; * 0 , X + I A = 1 &#960; * 1 , X Y - &#956; * 1 , X - I A = 0 &#960; * 0 , X Y - &#956; * 0 , X - &#916; * , we produced the efficient influence function through the following mapping (4) &#981; SSL ( R Y , R A , W , R ) = E &#981; cmp ( Y , A , X ) &#8739; W + R &#961; N &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W (5) = &#956; * 1 , X + &#928; * 1 , W &#960; * 1 , X m * 1 , W - &#956; * 1 , X - &#956; * 0 , X - &#928; * 0 , W &#960; * 0 , X m * 0 , W - &#956; * 0 , X - &#916; * + R I A = 1 Y - I A = 1 &#956; * 1 , X - &#928; * 1 , W m * 1 , W + &#928; * 1 , W &#956; * 1 , X &#961; N &#960; * 1 , X - R I A = 0 Y - I A = 0 &#956; * 0 , X - &#928; * 0 , W m * 0 , W + &#928; * 0 , W &#956; * 0 , X &#961; N &#960; * 0 , X . In the formula ( 4 ) that produces &#981; SSL from &#981; cmp , E &#981; cmp ( Y , A , X ) &#8739; W is the maximal information on ATE from the unlabeled data with a known imputation model, and the second term is the price for training the best imputation model over the labeled data. We provide the rigorous justification of this procedure in Section 4 . The efficient influence function in the missing data context is usually derived by projecting an arbitrary initial influence function to the nuisance tangent space ( Tsiatis, 2007 ). The approach has been applied to the SSL setting with missing outcome by first deriving the efficient influence function under missing data setting and then setting n / N &#8781; &#961; N = 0 for the SSL setting with very large unlabeled data ( Kallus and Mao, 2024 ). No formal justification of efficiency has been given in exiting literatures under the semi-supervised setting with &#961; N &#8594; 0 yet &#961; N &gt; 0 . Moreover, such standard procedure for deriving efficient influence function under missing data or causal inference settings is usually specific for the assumed dependence structure among variables, reflected by the correspondent chain-rule decomposition of nuisance model tangent space ( Robins et al., 1994 ; Tsiatis, 2007 ; Kallus and Mao, 2024 ; Cheng et al., 2021 ). For estimating of ATE under SSL setting, existing formulation focused on the surrogates S that are defined as short-term markers for long-term outcomes Y , represented by the S &#8594; Y dependence pattern in causal diagram ( Kallus and Mao, 2024 ; Cheng et al., 2021 ). The generalization to other types of surrogates S is currently absent. For example, surrogates S in our motivating EHR studies were imperfect documentations for treatment and outcome variables ( A , Y ) , represented by the ( A , Y ) &#8594; S dependence pattern ( Figure 1 ). The shift from S &#8594; Y to ( A , Y ) &#8594; S also creates the technical challenges in deriving projections to nuisance model tangent space defined according to the dependence pattern (see Section E2 of the Supplementary Materials ). While our derivation of efficient &#981; SSL also involved projecting an inefficient R / &#961; N &#981; cmp , a common approach among existing literatures ( Robins et al., 1994 ; Kallus and Mao, 2024 ), our approach did not impose stringent assumptions on surrogates S . To provide a general theoretical basis consistent across various surrogate mechanism, we established the connection between efficiency lower bound of complete data setting and that of double missing SSL setting through asymptotic local minimax result similar to Begun et al. (1983) in Section 4.2 . Our efficiency lower bound justified projecting complete data efficient influence function &#981; cmp to derive the SSL efficient influence function &#981; SSL . We further generalized the efficiency theory to other parameters with missing data under SSL setting in Section 7 . Our alternative justification only requires 1) the target ATE parameter &#916; can be identified by ( Y , A , X ) through &#981; cmp and 2) S can provide information on ( Y , A ) when they are not observed over the unlabeled set. Hence, our framework covered a broad range of surrogate mechanism including both the setting considered by Kallus and Mao (2024) and the causal diagram in Figure 1 . 3.2 SMMAL Procedure Inspired by the double machine learning estimation ( Chernozhukov et al., 2018 ) based on &#981; cmp , we propose the following SMMAL estimator for ATE: For each labelled fold k , we estimate the nuisance models by the out-of-fold data &#8464; k c , obtaining &#960; ^ ( k ) , &#956; ^ ( k ) , &#928; ^ ( k ) , m ^ ( k ) ; Construct the estimated influence functions &#119985; ^ ik = &#956; ^ ( k ) 1 , X i + &#928; ^ ( k ) 1 , W i &#960; ^ ( k ) 1 , X i m ^ ( k ) 1 , W i - &#956; ^ ( k ) 1 , X i - &#956; ^ ( k ) 0 , X i - &#928; ^ ( k ) 0 , W i &#960; ^ ( k ) 0 , X i m ^ ( k ) 0 , W i - &#956; ^ ( k ) 0 , X i + R i A i Y i - A i &#956; ^ ( k ) 1 , X i &#961; N &#960; ^ ( k ) 1 , X i - R i 1 - A i Y i - 1 - A i &#956; ^ ( k ) 0 , X i &#961; N &#960; ^ ( k ) 0 , X i - R i &#928; ^ ( k ) 1 , W i m ^ ( k ) 1 , W i - &#928; ^ ( k ) 1 , W i &#956; ^ ( k ) 1 , X i &#961; N &#960; ^ ( k ) 1 , X i + R i &#928; ^ ( k ) 0 , W i m ^ ( k ) 0 , W i - &#928; ^ ( k ) 0 , W i &#956; ^ ( k ) 0 , X i &#961; N &#960; ^ ( k ) 0 , X i . and estimate the ATE by (6) &#916; ^ SMMAL = 1 N &#8721; k = 1 K &#8721; i &#8712; &#8464; k &#119985; ^ ik . Estimate the asymptotic variance of n &#916; ^ SMMAL - &#916; * by (7) &#119985; ^ SMMAL = &#961; N N &#8721; k = 1 K &#8721; i &#8712; &#8464; k &#119985; ^ ik - &#916; ^ SMMAL 2 . Here we considered the n standardized estimation error n &#916; ^ SMMAL - &#916; * instead of the N standardized estimation error N &#916; ^ SMMAL - &#916; * because the latter is diverging at N / n &#8781; &#961; N - 1 / 2 rate due to the unbounded variance of R i / &#961; N as &#961; N &#8594; 0 . The ( 1 - &#945; ) &#215; 100 % confidence interval for ATE can be constructed with &#916; ^ SMMAL and &#119985; ^ SMMAL , &#916; ^ SMMAL - &#119989; &#945; / 2 &#119985; ^ SMMAL / n , &#916; ^ SMMAL + &#119989; &#945; / 2 &#119985; ^ SMMAL / n where &#119989; &#945; / 2 is the 1 - &#945; / 2 quantile of standard normal distribution. Similar to existing results in double machine learning literature, any estimators for the nuisance models with suitable rates of consistency can be used in our proposal as well. For low-dimensional W and smooth nuisance models, we can choose B-spline regression with proper order and degrees. Precise discussions on these rates, related conditions for general estimators and relevant smoothness classes for B-spline regression are listed in Section 4.2 . 3.3 Doubly Robust SMMAL Construction in high-dimensions We next discuss a specific construction of the SMMAL estimator when the dimensions p and q grow with n and p may be larger than n . In real-world evidence studies using EHRs, confounding adjustment often involves selection of the few determinants of treatment and risk factors for outcomes from a large number of candidate variables ( Hou et al., 2021c , 2022 ). We focus on the binary Y and put the high-dimensional logistic regression models with link function g ( x ) = 1 / 1 + e - x on the nuisance models (8) &#960; 1 , X = g &#945; &#8868; X ; &#956; a , X = g &#946; a &#8868; X , a = 0, 1 ; &#928; ( 1 , W ) = g &#958; &#8868; W ; m ( a , W ) = g &#950; a &#8868; W , a = 0, 1 . We denote the derivatives of the link g as g &#729; ( x ) = e x / 1 + e x 2 and the corresponding loss function as &#8467; ( y , x ) = log 1 + e x - y x . Other types of generalized linear models for OR model &#956; ( a , X ) may also be considered and derived similarly. To enhance the robustness against model mis-specification in &#960; and &#956; , we propose a bias-reducing calibration after an initial estimation ( Smucler et al., 2019 ). We added another layer of cross-fitting to reduce the overfitting bias when using initial estimators in the bias-reducing calibration. Compare with the general SMMAL algorithm in Section 3.2 , the generic estimation process for nuisance models (Step 1 in Section 3.2 ) is expanded into the Step 1&#8211;4 of the following SMMAL algorithm for high-dimensional logistic regression. To ensure that estimated PS and OR are bounded away from zero and one, we propose to truncate linear predictors according to a predetermined constant M corresponding to a reasonable range for PS and OR probabilities, e.g. M = 2.2 for range [0.1, 0.9]. Our algorithm for doubly robust SMMAL estimator &#916; ^ DR has the following steps: For each labelled fold k , we estimate the imputation models by the Lasso over out-of-fold data &#8464; k c , (9) &#958; ^ ( k ) = argmin &#958; &#8712; R p + q + 1 &#8721; i &#8712; &#8464; k c R i &#8467; A i , &#958; &#8868; W i &#8721; i &#8712; &#8464; k c R i + &#955; &#951; &#8214; &#958; &#8214; 1 , &#955; &#950; &#8781; log ( p + q ) / n , &#950; ^ a ( k ) = argmin &#950; &#8712; R p + q + 1 &#8721; i &#8712; &#8464; k c I A i = a R i &#8467; Y i , &#950; &#8868; W i &#8721; i &#8712; &#8464; k c I A i = a R i + &#955; &#950; &#8214; &#950; &#8214; 1 , &#955; &#950; &#8781; log ( p + q ) / n ; Choice of the imputation method is flexible (See Remark 10). For each labelled fold pair ( k 1 , k 2 ) , we estimate the initial PS and OR models by the Lasso over out-of-two-folds data &#8464; k 1 , k 2 c = &#8464; k 1 &#8746; &#8464; k 2 c , (10) &#945; ^ init k 1 , k 2 = argmin &#945; &#8712; R p + 1 &#8721; i &#8712; &#8464; k 1 , k 2 c R i &#8467; A i , &#945; &#8868; X i &#8721; i &#8712; &#8464; k 1 , k 2 c R i + &#955; &#945; , init &#8214; &#945; &#8214; 1 , &#946; ^ a , init k 1 , k 2 = argmin &#946; &#8712; R p + 1 &#8721; i &#8712; &#8464; k 1 , k 2 c I A i = a R i &#8467; Y i , &#946; &#8868; X i &#8721; i &#8712; &#8464; k 1 , k 2 c I A i = a R i + &#955; &#946; , a , init &#8214; &#946; &#8214; 1 , with &#955; &#945; , init , &#955; &#946; , a , init &#8781; log ( p ) / n ; Define the truncation at 2 M , &#964; x = sign x min { | x | , 2 M } , and its composition with functions g &#729; &#964; ( x ) = g &#729; ( &#964; ( x ) ) and e xp &#964; ( x ) = exp ( &#964; ( x ) ) . For each labelled fold k 1 , we construct the calibrated losses, (11) &#8467; &#945; , a A , &#945; &#8868; X ; &#946; = g &#729; &#964; X &#8868; &#946; ( a - A ) &#945; &#8868; X + I ( A = a ) e ( - 1 ) a &#945; &#8868; X , &#8467; &#946; , a Y , &#946; &#8868; X ; &#945; = exp &#964; ( - 1 ) a &#945; &#8868; X &#8467; Y i , &#946; &#8868; X i , and estimate the PS and OR models by cross-fitting within out-of-fold data &#8464; k 1 c , (12) &#945; ^ a k 1 = argmin &#945; &#8712; R p + 1 &#8721; k 2 &#8800; k 1 &#8721; i &#8712; &#8464; k 2 R i n &#8467; &#945; , a A , &#945; &#8868; X i ; &#946; ^ a , init k 1 , k 2 + &#955; &#945; , a &#8214; &#945; &#8214; 1 , &#946; ^ a k 1 = argmin &#946; &#8712; R p + 1 &#8721; k 2 &#8800; k 1 &#8721; i &#8712; &#8464; k 2 I A i = a R i &#8467; &#946; , a Y i , &#946; &#8868; X i ; &#945; ^ init k 1 , k 2 &#8721; i &#8712; &#8464; k 1 c I A i = a R i + &#955; &#946; , a &#8214; &#946; &#8214; 1 , with &#955; &#945; , a , &#955; &#946; , a &#8781; log ( p ) / n . Construct the nuisance model estimators: (13) &#960; ^ ( k ) 1 , X i = g &#964; X i &#8868; &#945; ^ 1 ( k ) , &#960; ^ ( k ) 0 , X i = g &#964; - X i &#8868; &#945; ^ 0 ( k ) , &#956; ^ ( k ) a , X i = g X i &#8868; &#946; ^ a ( k ) , &#928; ^ ( k ) a , W i = g W i &#8868; &#958; ^ ( k ) , m ^ ( k ) a , W i = g W i &#8868; &#950; ^ a ( k ) ; Estimate the ATE by sending ( 13 ) to ( 6 ), producing &#916; ^ DR . Estimate the variance by sending ( 13 ) and &#916; ^ DR to ( 7 ), producing &#119985; ^ DR . The ( 1 - &#945; ) &#215; 100 % confidence interval for ATE can be constructed with &#916; ^ DR and &#119985; ^ DR , &#916; ^ DR - &#119989; &#945; / 2 &#119985; ^ DR / n , &#916; ^ DR + &#119989; &#945; / 2 &#119985; ^ DR / n where &#119989; &#945; / 2 is the 1 - &#945; / 2 quantile of standard normal distribution. The calibrated losses ( 11 ) aim to estimate OR and PS models by approximately solving the equations of the partial derivatives of &#916; ^ DR with respect to PS and OR models being zero ( Tan, 2020 ; Smucler et al., 2019 ). The correctly specified model will be recovered as it can be identified by the same equation. Even with mis-specified model, &#916; will be insensitive to estimation errors in OR and PS models, guaranteed by the small partial derivatives. The property is referred to as the Neyman orthogonality ( Chernozhukov et al., 2018 ), which produces n asymptotic normal estimator with sub n rate nuisance model estimations. We didn&#8217;t use imputations to improve estimation of OR and PS models because there is no asymptotic efficiency gain due to Neyman orthogonality but potential risk of introducing bias. To control the overfitting bias from the sequential estimation process with 3 steps &#945; ^ init , &#946; ^ a,init &#8594; &#945; ^ a , &#946; ^ a &#8594; &#916; ^ DR , we propose the two-level cross-fitting for learning ATE in ( 10 ) and ( 12 ), previously considered for semi-supervised learning of high-dimensional regression in ( Hou et al., 2021b ). The two-level cross-fitting has the advantage of having larger training set for each Lasso (using k - 2 folds) compared to the averaging after data splitting (using ( k - 1 ) / 2 folds) in Smucler et al. (2019) . If we choose K = 10 , we are able to use at least 80% data while the data splitting in Smucler et al. (2019) may only use 45% data. Larger training sample typically allows the choice of smaller penalty factor thus reducing the bias. Taking averaging after data splitting, however, cannot reduce bias. The truncation &#964; at M in ( 12 ) secured the (causal inference) positivity property of the initially estimated models with no compromise in estimation accuracy. Truncation of PS has been commonly invoked in practice when (causal inference) positivity holds in principle but is violated practically by estimated PS ( Petersen et al., 2012 ; Ju et al., 2019 ). Our method generalized the truncation to OR prediction for binary outcome and identified a novel theoretical property of relaxing sparsity requirement for initial Lasso with the truncation. When the initial estimated model X i &#8868; &#945; ^ init is consistent for true models satisfying the (causal inference) positivity conditions, i. e. X i &#8868; &#945; * such that 0 &lt; g ( - M ) &#8804; g X i &#8868; &#945; * &#8804; g ( M ) &lt; &#8734; , truncation at M brings the X i &#8868; &#945; ^ init closer to X i &#8868; &#945; * (See Lemma A20 in Supplementary Materials ). Otherwise, the truncation always ensure e xp ( - 2 M ) &#8804; exp &#964; - X i &#8868; &#945; ^ init &#8804; exp ( 2 M ) . Then, estimating calibrated OR coefficients &#946; ^ a k 1 is a weighted L 1 penalized regression with bound weights independent of the responses, which we have shown to be consistent under mild assumptions (see Section D1 in Supplementary Materials ). Same argument applies to truncation of X i &#8868; &#946; ^ a , init . Besides numerical stability, we can remove the sparsity condition associated with the initial estimator of the mis-specified model. 4 Theoretical Properties of the SMMAL We established the n -consistency of &#916; ^ SMMAL and the honest asymptotic coverage of the confidence intervals with consistent estimation of PS and OR models in Section 4.1 . In Section 4.2 , we derived the asymptotic distribution of the SMMAL estimator &#916; ^ SMMAL and the subsequent matching lower bound to show its semi-parametric efficiency in the low-dimensional W case while using B -spline series estimators for nuisance regression models. For high-dimensional sub-Gaussian X and W and sparse nuisance models, we demonstrated in Section 4.3 that &#916; ^ DR is adaptively sparsity/model doubly robust with sparse nuisance models ( Rotnitzky et al., 2020 ; Smucler et al., 2019 ): sparsity doubly robust when both OR and PS are correctly specified; model doubly robust when one of OR or PS is correctly specified. 4.1 n -inference We require the following assumptions for nuisance models and the machine-learning estimators. We denote the true propensity score as &#960; * ( X ) = E ( A &#8739; X ) and outcome regression as &#956; * ( a , X ) = E ( Y &#8739; X , A = a ) . As we do not require consistency of imputation models, we denote &#928; &#8254; and m &#8254; as the potentially biased asymptotic limits of the estimated imputation models. Assumption 2 For a fixed constant M , we assume (Bounded response) almost surely sup i = 1 , &#8230; , N Y i &#8804; M ; (Causal Inference Positivity) almost surely sup i = 1 , &#8230; , N sup a = 0, 1 1 / &#960; * a , X i &#8804; M ; (Bounded estimators) almost surely sup k = 1 , &#8230; , K sup i &#8712; &#8464; k sup a = 0, 1 max 1 / &#960; ^ ( k ) a , X i , &#956; ^ ( k ) a , X i , &#928; ^ ( k ) a , W i , m ^ ( k ) a , W i &#8804; M ; (Rate of estimation) sup k = 1 , &#8230; , K &#960; ^ ( k ) - &#960; * 2 + &#956; ^ ( k ) - &#956; * 2 + &#928; ^ ( k ) - &#928; &#8254; 2 + m ^ ( k ) - m &#8254; 2 + n &#960; ^ ( k ) - &#960; * 2 &#956; ^ ( k ) - &#956; * 2 = o p ( 1 ) for some &#928; &#8254; and m &#8254; satisfying sup i = 1 , &#8230; , N sup a = 0, 1 max &#928; &#8254; a , W i , m &#8254; a , W i &#8804; M , where for two models h 1 ( a , W ) and h 2 ( a , W ) , we define (14) h 1 - h 2 2 = max a &#8712; { 0, 1 } E h 1 ( a , W ) - h 2 ( a , W ) 2 . Here we use the &#8467; 2 - norm notation because the mean squared error (MSE) thus defined correspond to the &#8467; 2 - estimation error for model coefficients under parametric models . (Stable variance) &#119985; * = Var A Y - A &#956; * ( 1 , X ) &#960; * ( 1 , X ) - ( 1 - A ) Y - ( 1 - A ) &#956; * ( 0 , X ) &#960; * ( 0 , X ) - &#928; &#8254; ( 1 , W ) m &#8254; ( 1 , W ) - &#928; &#8254; ( 1 , W ) &#956; * ( 1 , X ) &#960; * ( 1 , X ) + &#928; &#8254; ( 0 , W ) m &#8254; ( 0 , W ) - &#928; &#8254; ( 0 , W ) &#956; * ( 0 , W ) &#960; * ( 0 , X ) &#8712; [ 1 / M , M ] . We established the validity and asymptotic distribution of &#916; ^ SMMAL in the following theorem. Theorem 2 Under Assumption 2 , n / &#119985; ^ SMMAL &#916; ^ SMMAL - &#916; * &#8669; N 0, 1 , where &#8220; &#8669; &#8221; denotes convergence in distribution . Assumption 2 a guarantees the boundedness of all nuisance models. When Y is binary, the models &#960; * , &#956; * , &#928; * and m * are all bounded by one. Assumption 2b is equivalent to the standard (causal inference) positivity condition &#960; * 1 , X i &#8712; [ 1 / M , 1 - 1 / M ] as in Assumption 1b. Assumption 2c can be guaranteed by truncation of nuisance model estimators at M , which would not compromise the estimation accuracy under Assumptions 2a and 2b. Assumption 2e ensures the proper scaling of the asymptotic variance of &#916; ^ SMMAL . As noted following ( 7 ), the term with the R / &#961; N factor from labeled data in &#981; SSL dominates its variance if &#961; N &#8594; 0 . The rate condition for the PS and OR models in Assumption 2d matches those for the double machine-learning estimator proposed in Chernozhukov et al. (2018) if applied to the complete data subset of size n . Under MCAR by design, the missing data mechanism is known a priori, which we utilized to accommodate the mis-specified imputation models estimated at an arbitrarily slow rate. Remark 3 Compared to existing work on semi-supervised estimation of ATE ( Cheng et al., 2021 ; Kallus and Mao, 2024 ) approximating the &#961; N &#8594; 0 setting by the &#961; N = 0 setting, our SMMAL incorporates additionally the uncertainty from large yet finite unlabeled data through ( 5 )&#8211;( 7 ). As the result, the inference from SMMAL has two methodological advantages. First, by harmonizing the n &#8810; N and n &#8781; N settings, users may use the same SMMAL procedure without choosing from two setting-specific approaches ( Kallus and Mao, 2024 ). Especially, it seems implausible to decide the asymptotic limit of &#961; N by a single realization of the data. Second, the uncertainty of &#916; ^ SMMAL consists of the uncertainty from labeled data and the uncertainty from large but finite unlabeled data , &#119985; SMMAL = Var &#981; cmp ( Y , A , X ) - &#981; cmp ( Y , A , X ) &#8739; W &#9183; &#119985; L from labeled set + &#961; N Var &#981; cmp ( Y , A , X ) &#8739; W &#9183; &#119985; U from labeled set . Unlike existing work ( Cheng et al., 2021 ; Kallus and Mao, 2024 ) that only considered &#119985; L from labeled set, our SMMAL variance estimation captures both &#119985; L and &#119985; U by involving estimated influence functions &#119985; ^ ik for all observations so that SMMAL is expected to have less issues in underestimation of uncertainty particularly from unlabeled data with a moderately small &#961; N in practice . 4.2 Semi-parametric efficiency with low-dimensional confounder We next formally establish the semi-parametric efficiency lower bound under the double missing SSL setting. Consider the non-parametric model for ( W , R A , R Y , R ) &#119982; SSL = d P f w , a , y , r = &#961; N f w , a , y r 1 - &#961; N f w w 1 - r d &#957; SSL w , a , y , r : f is density over &#119986; &#8855; 0, 1 &#8855; &#119988; , and f w w = &#8721; a &#8712; 0, 1 &#8747; y &#8712; &#119988; f w , a , y d &#957; y y for some measures &#957; y over &#119988; , &#957; w over &#119986; and &#957; SSL ( w , a , y , r ) = &#957; w &#215; &#948; { 0, 1 } &#215; &#957; y ( w , a , y ) &#215; &#948; 1 ( r ) + &#957; w ( w ) &#215; &#948; 0 ( r ) where &#948; &#119964; is the counting/Dirac measure over the set &#119964; . Elements in &#119982; SSL can be indexed by the density f , and we denote the true density as f * and the true model P f * . Remark 4 In existing work on ATE ( Robins et al., 1994 ; Kallus and Mao, 2024 ), the model A &#8739; X provides no information on the Y &#8739; A , X , and thus would not be included in the nuisance tangent space. In our setting, however, the surrogates S induced a correlation between subspaces corresponding to A &#8739; X and Y &#8739; A , X in the nuisance tangent space, which indicates that A &#8739; X provides information on the Y &#8739; A , X through the unlabelled data. As the result, the geometry of the model tangent space is more complex, and the projection can no longer be obtained through simple conditional expectation. See Section E2 of the Supplementary Materials for details . We denote the total variation norm as &#8214; &#183; &#8214; TV . In the following theorem, we establish the semi-parametric efficiency lower bound for &#916; under &#119982; SSL in the form of a local minimax theorem obtained in the spirit of Begun et al. (1983) . Theorem 5 Under Assumptions 2a, 2c and 2e, we have lim &#8198; inf c &#8594; &#8734; lim &#8198; inf N &#8594; &#8734; inf &#916; ^ sup f - f * TV &#8804; c / &#961; N N &#8747; N &#916; ^ - &#916; * 2 d &#8719; i = 1 N P f w i , a i , y i , r i Var &#981; SSL ( RY , RA , W , R ) &#8805; 1 . Remark 6 Theorem 5 offers one example that the semi-parametric efficiency bound (SEB) derived under the classical missing data setting can be generalized to the double missing SSL setting with &#961; N &#8594; 0 while &#961; N N &#8594; &#8734; . Later in Section 7 , we present Theorem 13 for general SSL setting (including specifically the double missing SSL setting). Previous attempts to formalize semi-parametric efficiency in a the SSL settings have assumed that the entire distribution of W is known, i.e . N = &#8734; and &#961; N = 0 . Under the simplified SSL setting with N = &#8734; , the SEB can be derived by straightforward applications of standard results in classical semiparametric literature &#8211; see e.g. van der Vaart (1998) . Indeed, another possible consideration for choosing this simplified formulation version is the ambiguity of defining regular estimators without (missing data) positivity assumption and thereby formalizing efficiency through the calibration of the best regular estimator. We bypassed this conceptual difficulty by providing the alternative characterization based on local asymptotic minimax theory &#8211; which may operate on all possible estimators instead of restricting to the class of regular procedures . Utilizing the correlation structure induced by the projection Cov E &#981; cmp ( Y , A , X ) &#8739; W , R / &#961; N &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W = 0 , Cov &#981; cmp ( Y , A , X ) , E &#981; cmp ( Y , A , X ) &#8739; W = Var E &#981; cmp ( Y , A , X ) &#8739; W , we obtain the limiting lower bound in Theorem 5 when &#961; N &#8594; 0 that matches the asymptotic variance of the labeled data component in &#981; SSL , lim &#961; N &#8594; 0 &#961; N Var &#981; SSL ( RY , RA , W , R ) = lim &#961; N &#8594; 0 &#961; N Var E &#981; cmp ( Y , A , X ) &#8739; W + R / &#961; N &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W = lim &#961; N &#8594; 0 &#961; N Var E &#981; cmp ( Y , A , X ) &#8739; W &#9183; &#8594; 0 + &#961; N Var R / &#961; N &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W + 2 &#961; N Cov E &#981; cmp ( Y , A , X ) &#8739; W , R / &#961; N &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W &#9183; = 0 = Var &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W = Var &#981; cmp ( Y , A , X ) + Var E &#981; cmp ( Y , A , X ) &#8739; W - 2 Cov &#981; cmp ( Y , A , X ) , E &#981; cmp ( Y , A , X ) &#8739; W &#9183; = Var E &#981; cmp ( Y , A , X ) &#8739; W = Var &#981; cmp ( Y , A , X ) - Var E &#981; cmp Y , A , X &#8739; W . From the representation above, we showed that the efficiency gain from the unlabeled data with surrogates is given by the variance of the &#981; cmp explained by the surrogates and confounders. The efficiency gain based on semi-parametric efficiency theory typically requires consistent estimation of nuisance models. Under mis-specified imputation models, there is no general guarantee on efficiency gain. In Discussion ( Section 8 ), we offered efficient linear combination as the backup plan when quality of estimated nuisance models is in doubt. The key idea of the proof is to construct the two-dimensional least favorable perturbation in an asymmetric neighborhood with different size in two directions. The first direction is proportional to &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W and of size &#8781; 1 / N &#961; N &#8781; 1 / n , which reflects the level of the information on &#916; * from the labels and should naturally scale with the number of expected labels. The second direction is proportional to E &#981; cmp ( Y , A , X ) &#8739; W and of size &#8781; 1 / N , which reflects the level of the information on &#916; * from the unlabelled data and should scale with the total sample size. The design of the different scales ensured the tightness of log-likelihood ratio between the perturbed and the true models, which would otherwise be degenerating or diverging. We next show that the lower bound is attained under low-dimensional smoothness class models for the nuisance functions and can be operationalized by feeding B-spline regressions to &#916; ^ SMMAL . Suppose the confounders and surrogates are bounded continuous variables of fixed dimension, W &#8712; [ - M , M ] p + q , p + q &lt; d &#8781; 1 . We measure the smoothness of the models by &#8459; ( f ( &#183; ) ) the H&#246;lder class defined in Definition A22, Section E of the Supplementary Materials . Assumption 3 For a fixed constant M , we assume (Bounded density) the density functions for X and W , f X ( x ) and f W ( w ) , are bounded and bounded from zero , f X ( x ) &#8712; [ 1 / M , M ] , &#8704; x &#8712; [ - M , M ] p , f W ( w ) &#8712; [ 1 / M , M ] , &#8704; w &#8712; [ - M , M ] p + q ; (Smooth models) the smoothness of the nuisance models observe 1 1 + &#8459; &#960; * ( a , &#183; ) / p + 1 1 + &#8459; &#956; * ( a , &#183; ) / p &lt; 1 , &#8459; &#928; * ( a , &#183; ) &gt; 0 , &#8459; m * ( a , &#183; ) &gt; 0 , for a = 0, 1 . Corollary 7 Under Assumptions 2a, 2b, 2e and 3, we may choose B-spline regressions with order &#954; &#8805; max &#8459; &#960; * ( a , &#183; ) , &#8459; &#956; * ( a , &#183; ) , &#8459; &#928; * ( a , &#183; ) , &#8459; m * ( a , &#183; ) : a = 0, 1 - 1 , degrees 1 / &#960; ( a , &#183; ) : n 1 1 + &#8459; &#960; * ( a , &#183; ) / p , &#956; ( a , &#183; ) : n 1 1 + &#8459; &#956; * ( a , &#183; ) / p , &#928; ( a , &#183; ) : n 1 1 + &#8459; &#928; * ( a , &#183; ) / ( p + q ) , m ( a , &#183; ) : n 1 1 + &#8459; m * ( a , &#183; ) / ( p + q ) and truncation at M for &#916; ^ SMMAL to achieve n &#916; ^ SMMAL - &#916; * / &#961; N Var &#981; SSL ( RY , RA , W , R ) &#8669; N ( 0, 1 ) . Corollary 7 is special case of Theorem 2 under smooth models estimated by standard non-parametric estimation. By Corollary 7, the asymptotic MSE of &#916; ^ SMMAL is &#961; N Var &#981; SSL / n &#8781; Var &#981; SSL / N , matching the lower bound established in Theorem 5. Therefore, we have justified the semi-parametric efficiency of &#916; ^ SMMAL . At the same time, the lower bound in Theorem 5 is the sharp semi-parametric efficiency bound for &#916; * under double missing SSL setting &#119982; SSL . 4.3 Doubly robustness with high-dimensional confounder To describe the sparsity/model double robustness of &#916; ^ DR , we define the asymptotic limits for Lasso estimators in ( 9 )&#8211;( 12 ) under potentially mis-specified models. (15) &#958; &#175; = argmin &#958; &#8712; R p + q + 1 E &#8467; A i , &#958; &#8868; W i , &#950; &#175; a = argmin &#950; &#8712; R p + q + 1 E I A i = a &#8467; Y i , &#950; &#8868; W i , &#945; &#175; init = argmin &#945; &#8712; R p + 1 E &#8467; A i , &#945; &#8868; X i , &#946; &#175; a , init = argmin &#946; &#8712; R p + 1 E I A i = a &#8467; Y i , &#946; &#8868; X i , &#945; &#175; a = argmin &#945; &#8712; R p + 1 E g &#729; X i &#8868; &#946; &#175; a , init a - A i &#945; &#8868; X i + I A i = a e ( - 1 ) a &#945; &#8868; X i , &#946; &#175; a = argmin &#946; &#8712; R p + 1 E e xp ( - 1 ) a X i &#8868; &#945; &#175; init I A i = a &#8467; Y i , &#946; &#8868; X i , We use &#8214; &#183; &#8214; 0 to denote the sparsity of a vector and &#8214; &#183; &#8214; &#968; 2 denote the sub-Gaussian norm for random variables or vectors. The sparsities of coefficients for OR &#946; a 0 and PS &#8214; &#945; &#8214; 0 models reflect the numbers of true determinants for the treatment and outcomes, including the true confounders that must be adjusted for. The detailed definition is given in Definition A23, Section E of the Supplementary Materials . Assumption 4 For constant M independent of dimensions n , N , p , q , (Sub-Gaussian and bounded covariates) the vector of confounders and surrogates is sub-Gaussian, sup &#8214; v &#8214; 2 = 1 v &#8868; W &#968; 2 &#8804; M , and coordinate-wisely bounded &#8214; W &#8214; &#8734; &#8804; M almost surely ; (Identifiability) the variance of W is invertible inf &#8214; v &#8214; 2 = 1 v &#8868; Var ( W ) v &#8805; 1 / M ; (Causal Inference positivity) the true propensity scores and the asymptotic predictions of all models are bounded away from zero and one, almost surely , &#960; * ( a , X ) &#8712; [ 1 / M , 1 - 1 / M ] , max &#958; &#175; &#8868; W , &#950; &#175; a &#8868; W , &#945; &#175; a &#8868; X , &#946; &#175; a &#8868; X : a = 0, 1 &#8804; M ; and one of the following: (PS correct) the propensity model is correct , E ( A &#8739; X ) = g &#945; * &#8868; X and the dimensions satisfy (16) &#946; &#175; 1 0 + &#946; &#175; 0 0 log ( p ) + &#8214; &#958; &#175; &#8214; 0 + &#950; &#175; 1 0 + &#950; &#175; 0 0 log ( p + q ) n + &#945; * 0 &#945; * 0 + &#946; &#175; 1 0 + &#946; &#175; 0 0 log ( p ) 2 / n = o p ( 1 ) ; (OR correct) the OR model is correct , E ( Y &#8739; A = a , X ) = g &#946; * , a &#8868; X and the dimensions satisfy (17) &#945; &#175; 1 0 + &#945; &#175; 0 0 log ( p ) + &#8214; &#958; &#175; &#8214; 0 + &#950; &#175; 1 0 + &#950; &#175; 0 0 log ( p + q ) n + &#8721; a = 0, 1 &#946; * , a 0 &#946; * , a 0 + &#945; &#175; a 0 log ( p ) 2 / n = o p ( 1 ) ; (both correct) both models are correct , E ( A &#8739; X ) = g &#945; * &#8868; X and E ( Y &#8739; A = a , X ) = g &#946; * , a &#8868; X and the dimensions satisfy (18) &#945; * 0 + &#946; * , 1 0 + &#946; * , 0 0 log ( p ) + &#8214; &#958; &#175; &#8214; 0 + &#950; &#175; 1 0 + &#950; &#175; 0 0 log ( p + q ) n + &#945; * 0 &#946; * , 1 0 + &#946; * , 0 0 log ( p ) 2 / n = o p ( 1 ) ; Theorem 8 Under Assumption 4, &#916; ^ DR converges in distribution to a normal random variable at n - rate , n / &#119985; ^ DR &#916; ^ DR - &#916; * &#8669; N ( 0, 1 ) , where &#8220; &#8669; &#8221; denotes convergence in the distribution . Besides the double robustness toward PS and OR, &#916; ^ DR is additionally robust to the imputation models. Similar to the general Theorem 2, we utilized the known missing data mechanism under MCAR by design to allow model mis-specifications on the imputation. Remark 9 Regarding the PS model and OR model estimated over the labeled data of size n , our &#916; ^ DR is both rate doubly robust Rotnitzky et al. (2020) and model doubly robust ( Smucler et al., 2019 ). When both models are correct, the dimension condition ( 18 ) for the PS model and OR model in Assumption 4d-iii satisfies the condition for rate doubly robust, i.e. each sparsity obeying &#945; * 0 &#8810; n / log ( p ) , &#946; * , a 0 &#8810; n / log ( p ) and their product satisfying &#945; * 0 &#946; * , a 0 &#8810; n / log ( p ) 2 . In the case of only one model is correct, our &#916; ^ DR can still provide n - inference, thus being model doubly robust. By the truncation &#964; in ( 12 ), we are able to completely remove the sparsity requirement of the mis-specified initial model under the (causal inference) positivity condition of Assumption 4c. The general framework of Smucler et al. (2019) would require all models in ( 15 ) being sparse . Remark 10 As the correct model specification is only required for OR or PS in Assumption 4d, the validity of Theorem 8 does not rely on the consistency of imputation models based on the MCAR missing data mechanism by design. Therefore, the choice on imputation methods ( 9 ) can be flexible. If preliminary evidence suggests that certain element in S contains the most information such as S a for A and S y for Y , we can remove penalty for the associated coefficients or simply run the low-dimensional regressions A ~ S a and Y ~ S y . Remark 11 We presented the theory according to the exact sparsity in Assumption 4d-iii for two considerations. First, the exact sparsity has a clear interpretation that classifies the covariates into relevant signals and irrelevant noises, about which domain experts may have a preliminary evaluation in applications. Second, the exact sparsity facilitates direct comparison with many related literatures have used exact sparsity to measure the local efficiency or robustness of their proposed methods ( Farrell, 2015 ; Tan, 2020 ; Smucler et al., 2019 ; Zhang et al., 2023 ). The exact sparsity in Assumption 4d-iii can be substituted by other conditions that produce the appropriate estimation rate in the more general Assumption 2d. For example, estimation rates of L 1 penalized high-dimensional generalized linear models have been established for approximately sparse models ( Negahban et al., 2012 ; Smucler et al., 2019 ). 5 Simulation We conducted extensive simulation studies to evaluate the finite sample performance of the SMMAL methods. Throughout the simulations, we set the total sample size N = 10000 , the number of labels n = 500 , the number of repeats as 1000, q = 2 with one surrogate S A for A and another S Y for Y . We focused on the situation that Y is also binary. Let &#934; be cumulative distribution function for standard normal distribution. The surrogates for binary A and Y were generated from mixture Beta distribution of the form: S A = A S A , 1 + 1 - A S A , 0 , S Y = Y S Y , 1 + 1 - Y S Y , 0 , Low-dimensional model : S A , 1 ~ Beta &#945; A + X , 1 , S A , 0 ~ Beta 1 , &#945; A + X , S Y , 1 ~ Beta &#945; Y + X , 1 , S Y , 0 ~ Beta 1 , &#945; Y + X ; High-dimensional model : S A , 1 ~ Beta &#945; A + &#934; X 1 , 1 , S A , 0 ~ Beta 1 , &#945; A + &#934; X 1 , S Y , 1 ~ Beta &#945; Y + &#934; X 1 , 1 , S Y , 0 ~ Beta 1 , &#945; Y + &#934; X 1 . The mixture Beta distribution mimicked the outputs from phenotyping algorithms, which typically take value between zero and one ( Liao et al., 2019 ). We considered a list of values for &#945; A and &#945; Y ( Table 1 ), corresponding to different level of prediction accuracy measured by area-under-curve (AUC) of the receiver operating characteristic (ROC). Five values were considered for &#945; A and &#945; Y , creating 25 two-way combinations for each simulation setting. We considered two scenarios for generating the data, the low-dimensional smooth model and high-dimensional logistic regression. Low-dimensional smooth model We generated the one dimensional X &#8712; R from Uniform (0,1) and set the PS and OR to be the following smooth models ( Figure 2 ): &#960; * ( 1 , X ) = &#956; * ( 1 , X ) = 1 - 1.2 / 3 - X 2 , &#956; * ( 0 , X ) = 1 - 1.2 / 3 - ( 1 - X ) 2 . We used tensor product first order B-spline (piece-wise linear splines) regression to estimate the nuisance models. The splines were constructed from bs function of the splines R package. The degrees were selected by 10 fold cross-validation among integers less than n &#8776; 22 according to the out-of-fold entropy. Using the cross-fitted nuisance models from B-spline regression with K = 10 , we obtained point and interval estimates for the ATE based on &#916; ^ SMMAL and &#119985; ^ SMMAL . As the benchmark, we also estimated the ATE using the labeled data only by the double machine learning method ( Chernozhukov et al., 2018 ). High-dimensional logistic regression We generated the high-dimensional X &#8712; R p with p = 500 from the multivariate Gaussian distribution with auto-regressive correlation structure: U 1 , &#8230; , U p ~ i . i . d . N 0, 1 , X 1 = U 1 , X j = 0.5 X j - 1 + 0.75 U j . We generated A and Y from the high-dimensional logistic regression models PS Linear : &#960; * ( 1 , X ) = g 0.5 X 1 + 0.25 X 2 + 0.125 X 3 ; PS Interaction : &#960; * ( 1 , X ) = g 0.5 X 1 + 0.25 X 2 + 0.125 X 3 1 + 0.0625 X 1 + 0.125 X 2 - 0.5 X 3 ; OR Linear : &#956; * ( 1 , X ) = g 0.1 + 0.25 X 1 + 0.125 X 2 + 0.0625 X 3 , &#956; * ( 0 , X ) = g - 0.1 - 0.25 X 1 - 0.125 X 2 - 0.0625 X 3 ; OR Interaction : &#956; * ( 1 , X ) = g 0.1 + 0.25 X 1 + 0.125 X 2 + 0.0625 X 3 &#215; 1 + 0.0625 X 1 + 0.125 X 2 - 0.5 X 3 , &#956; * ( 0 , X ) = g - 0.1 - 0.25 X 1 - 0.125 X 2 - 0.0625 X 3 &#215; 1 + 0.0625 X 1 + 0.125 X 2 - 0.5 X 3 . As signal strength is known to impact variable selection in theory and practice ( Fan and Peng, 2004 ; Fan and Lv, 2010 ), we set up the coefficients in the models to reflect different level of signal strength: 0.5-strong, 0.25-moderately strong, 0.125-moderately weak, 0.0625-weak. For PS/OR models with second order interactions, we still fitted high-dimensional logistic regression without interactions, creating the mis-specification scenarios. We considered 3 combinations corresponding to the three settings of Assumption 4d: correct models (PS Linear + OR Linear); mis-specified PS (PS Interaction + OR Linear); mis-specified OR (PS Linear + OR Interaction). We set the number of the folds as 10 and fitted the imputations ( 9 ) and initial estimators ( 10 ) using glmnet from R-package glmnet . We fitted the calibrated estimators ( 12 ) using rcal from from R-package rcal . The penalty parameters were selected by 10-fold cross validation with out-of-fold entropy. Using the cross-fitted nuisance models, we estimated the ATE using &#916; ^ DR and construct the 95% confidence interval based on the variance estimator &#119985; ^ DR . As the benchmark, we also estimated the ATE by the model doubly robust estimation ( Smucler et al., 2019 ) using (1) the labeled data alone; (2) the dichotomized surrogates defined by Y ~ i = I S Y , i &#8805; 1 - n - 1 &#8721; i = 1 N R i Y i , A ~ i = I S A , i &#8805; 1 - n - 1 &#8721; i = 1 N R i A i . We refer to the two benchmarks as supervised learning (SL) and unsupervised learning (UL). Results Results generally followed a consistent pattern across low-d and high-d settings. Comparison between settings, however, is not meaningful due to the completely different data generating processes. In Figure 3 , we visualized the relative efficiency of our semi-supervised &#916; ^ SMMAL , &#916; ^ DR compared to their supervised benchmarks. In general, our semi-supervised approaches gained efficiency from the unlabeled data whose magnitude was increasing with the minimal prediction accuracy of the two surrogates. With good imputation (AUC .95) from both surrogates, the relative efficiency was about 1.32&#8211;1.64 across all settings. With great imputation (AUC .99) from both surrogates, the relative efficiency was about 2.23&#8211;2.89 across all settings. The result quantified the benefit from improving the quality of surrogates in terms of relative increase in labels. Since the algorithms to curate surrogates are often portable to other studies sharing the variables, effort put into high-quality labels is more cost-effective compared to the brutal expansion in labeling. The detailed simulation results containing the bias, standard deviation, average standard error, coverage of 95% confidence interval for our semi-supervised &#916; ^ SMMAL , &#916; ^ DR along with those for the supervised benchmarks were presented in Tables A4 &#8211; A7 in Section A of the Supplementary Materials . Our semi-supervised &#916; ^ SMMAL , &#916; ^ DR achieved reasonably honest inference with coverage of 95% confidence interval close to the nominal level. In Figure 4 , we visualized the coverage of 95% confidence intervals by unsupervised learning. Using the dichotomized surrogates as if they were the true treatment and outcome led to under coverage of the confidence intervals even for nearly perfect surrogates, and the under coverage exacerbated with poorer surrogates. The detailed summaries on the bias, standard deviation and coverage of 95% confidence interval for the unsupervised benchmark were presented in Table A8 in Section A of the Supplementary Materials . 6 Real-world evidence on targeted cancer therapy We applied the proposed SMMAL method to EHR data from Mass General Brigham healthcare to generate real-world evidence (RWE) on treatment effect of targeted therapy for metastatic colorectal cancer in comparison with conventional chemotherapy. Over the past two decades, a total of 9 targeted therapies have been approved for the treatment of colorectal cancer ( Xie et al., 2020 ), the 4th most prevalent and lethal cancer ( U.S. Cancer Statistics Working Group, 2022 ). While the targeted therapies have been reported as advantageous compared to conventional chemotherapy in clinical trials within specific trial populations, their effectiveness in real-world patient population has not been fully established. With increasing availability of EHR data, it is now plausible to generate RWE on targeted cancer therapy with respect to their efficacy in improving progression free survival via causal modeling treating EHR data as an observational cohort. Unfortunately, such a modeling task is highly challenging with EHR data due to the lack of readily available precise information on both treatments patient received and progression free survival. To overcome this challenge, we manually annotated treatment-response information for 100 randomly selected patients. We derived several potential surrogates for both S A and S Y from codified and narrative EHR data, which have varying degree of accuracy as shown in Table 2 . Our goal was to leverage both the labeled observations on Y and S as well as the larger set of unlabeled EHR data to infer about ATE for targeted therapy based on SMMAL. The full study cohort consisted of N = 4147 colorectal cancer patients who have available cancer stage information extracted via a natural language process tool ( Yuan et al., 2021 ) and received chemotherapy and/or targeted therapy. We grouped therapies into chemotherapy alone and targeted therapy which includes those treated with any of the 9 treatments: Bevacizumab, Cetuximab, Ipilimumab, Regorafenib, Pembrolizumab, Nivolumab, and Tipiracil. We set the outcome as 1-year progression free survival, a binary outcome defined as: 1 &#8211; exit in terminal condition (death/terminal care) or development of new metastasis site with 1-year from the treatment initiation; 0 &#8211; otherwise. As the standard quality control ( Hou et al., 2023 ), an abstractor randomly sampled n = 100 from the study cohort and annotated the gold-standard labels for prescription of targeted medication, terminal condition and new metastasis site by manually reviewing those patients&#8217; EHR. The treatment A and Y outcome were defined based on annotations over the labeled set, creating the MCAR data. We reported the treatment and outcome labels as well as their EHR proxies in Table A9 of Supplementary Materials Section B , where we also described the construction of the reasonably good surrogates shown in Table 2 . We extracted a comprehensive list of potential confounders ( Table 3 ). From EHR near the colorectal cancer diagnosis date, we used location specific colorectal cancer diagnosis code to identify the initial tumor location and natural language process tool ( Yuan et al., 2021 ) to extract the initial stage. We also extracted the code for secondary malignancy at lymph node and other distant organs. From EHR between cancer diagnosis and subsequent metastasis, we extracted the codes for common procedures (chemotherapy, radiotherapy, colon biopsy and colon rescission). From EHR near the metastasis date, we used location specific secondary malignancy code to identify the initial metastasis site(s). We also adjusted for the time gap between diagnosis and metastasis, healthcare utilization before metastasis or one year before metastasis measured by days with diagnosis codes and the high-dimensional general health status consisting of diagnosis code counts grouped by the PheWAS catalog ( Hou et al., 2022 ). The targeted therapy arm was associated with factors for poor prognosis including higher proportion of stage IV at diagnosis (81% vs 58%), higher proportion of likely liver metastasis (57% or 67% vs 34%). After merging rare levels for cancer characteristics at initial diagnosis (tumor location, cancer stage) and deleting features with fewer than 10 occurrence in labeled subset, we obtained the p = 55 potential confounders. We applied the doubly robust SMMAL in high-dimensions described in Section 3.3 . Besides the crude analysis, we ran two benchmark analyses, the double machine learning (DML) ( Chernozhukov et al., 2018 ) using initial estimators ( 10 ) and the calibrated estimation (Cal) ( Tan, 2020 ; Smucler et al., 2019 ) using the calibrated estimators ( 12 ). Both supervised learning (SL) using labeled data only and the unsupervised learning (UL) deriving treatment and outcome from the dichotomized surrogates by matching observed prevalence in labeled data were considered. The number of fold was set as K = 5 , and the penalties factors were selected by the minimal cross-validated entropy. In Figure 5 , we displayed the point estimation and the 95 % confidence interval. The confounder adjusted analysis results suggested that on average, targeted therapy had comparable efficacy compared to traditional chemotherapy. Compared to the SL crude analysis which indicated worse outcomes for targeted therapy, our SMMAL accounted for substantial confounding caused by association between target therapy and factors indicating poor prognosis. Except for the crude analysis that did not adjust for any confounding, our SMMAL had the shortest confidence interval, achieving 1.88 relative efficiency with respect to SL DML and 1.35 relative efficiency with respect to the SL cal. The results from UL methods were questionable as we observed a significant deviation of the UL crude estimation from the SL crude estimation, indicating substantial bias from imperfect data. Coupled with the short confidence intervals, researcher should take caution in the risk of misleading conclusions from the UL methods. 7 General Efficiency Lower Bound While the paper focused on the method for ATE under double missing SSL setting, we established the theoretical efficient lower bound for general parameter and broader missing data pattern in this section. We considered a generic model for data ( R , R Z , W ) with always observed W and MCAR Z . Specifically, consider &#119982; SSL = d P f r , z , w , r = &#961; N f z , w r 1 - &#961; N &#8747; z &#8712; &#119989; f z , w d &#957; z z 1 - r d &#957; SSL r , z , w : f z , w d &#957; cmp z , w &#8712; &#119982; cmp for a complete data model class &#119982; cmp over &#119989; &#8855; &#119986; and measures &#957; z over &#119989; , &#957; w over &#119986; and &#957; cmp = &#957; z &#215; &#957; w , &#957; SSL r , z , w = &#948; 1 r &#215; &#957; cmp z , w + &#948; 0 r &#215; &#957; w w . Let &#8459; be the nuisance tangent space of &#119982; SSL at the true model d P f * with f = f * . Suppose &#968; cmp ( Z , W ) is the efficient influence function for parameter &#952; under &#119982; cmp . Here we use a different notation &#968; for general parameter under missing data components to distinguish from the &#981; used specifically for ATE under double missing SSL setting. Our theory was established under the following basic assumptions. Assumption 5 For absolute constant M , ( MCAR ) R &#10987; ( Z , W ) ; (Informative labels) inf &#8214; v &#8214; 2 = 1 v &#8868; Var &#968; cmp ( Z , W ) - E &#968; cmp ( Z , W ) &#8739; W v &#8805; 1 / M ; (Model flexibility) E * &#968; cmp ( Z , W ) &#8739; W &#8712; &#8459; ; (Bounded influence function) &#968; cmp ( Z , W ) 2 &#8804; M almost surely . We derived the SSL efficient influence function by the following proposition. Proposition 12 Let &#968; cmp ( Z , W ) be the efficient influence function for parameter &#952; under complete data model &#119982; cmp . Under Assumptions 5a and 5c, the efficient influence function for &#952; under SSL model &#119982; SSL is (19) &#968; SSL R , Z , W = R &#961; N &#968; cmp Z , W - E &#968; cmp Z , W &#8739; W + E &#968; cmp Z , W &#8739; W . The influence function &#968; SSL leads to a semi-parametric efficiency lower bound. Theorem 13 Under Assumptions 5a-5d, we have the minimax semi-parametric efficiency for SSL of &#952; under &#119982; SSL , inf a : &#8214; a &#8214; 2 = 1 lim &#8198; inf c &#8594; &#8734; lim &#8198; inf N &#8594; &#8734; inf &#952; ^ sup f - f * TV &#8804; c / &#961; N N &#8747; N a &#8868; &#952; ^ - &#952; * 2 d &#8719; i = 1 N P f z i , w i , r i a &#8868; Var &#968; SSL ( R , Z , W ) a &#8805; 1 . We offered the proof of Theorem 13 in Section C6 of the Supplementary Materials . Upper bound would depend on the context. Like Corollary 7, the bound can be attained if non-parametric estimation of nuisance models admit sufficiently fast rate of consistency, which has been thoroughly studied under classical low-dimensional settings by Stone (1977 , 1982) . While we focus on &#961; N &#8594; 0 and n &#8810; N setting, the theory also applies to classical setting with &#961; N &#8712; [ 1 / M , 1 - 1 / M ] and n &#8781; N setting. 8 Discussion Motivated by the increasing interest of generating real-world evidence on treatment effect with big yet noisy EHR data, we proposed a robust and efficient semi-supervised estimator for ATE under the double missing SSL setting. The SMMAL estimator gained efficiency by leveraging the large unlabelled data containing noisy yet predictive surrogates for Y and A with almost no additional requirement than those needed for the supervised analysis using the labeled set alone. We established semi-parametric efficiency bound for the ATE estimator under the low dimensional confounder setting and constructed a doubly robust SMMAL estimator for the high dimensional confounder setting. Unlike the MCAR setting, the missing data propensity score P ( R = 1 &#8739; W ) = &#961; ( W ) must be modeled and estimated. We conjecture that the efficient influence function under MAR may take the form &#981; MAR ( RY , RA , W , R ) = E &#981; cmp ( Y , A , X ) &#8739; W + R &#961; ( W ) &#981; cmp ( Y , A , X ) - E &#981; cmp ( Y , A , X ) &#8739; W . The estimation of the decaying &#961; ( W ) has been studied in Zhang et al. (2023) . When all nuisance models, ( &#956; , &#960; , &#928; , m , &#961; ) , are consistently estimated at suitable rates, the efficiency lower bound should be attained under ideal conditions. However, extension of the SMMAL with high-dimensional regressions to MAR setting would require a more sophisticated calibration procedure for all 5 models ( &#956; , &#960; , &#928; , m , &#961; ) , as the potential bias from mis-specified &#961; now may impact the orthogonality of ATE estimator toward all 4 other estimated models. Moreover, caution must be taken when making MAR assumption for treatment and outcome data from linked observational data such as a disease registry. Enrollment in registry led by pioneering clinical experts may systematically impact the treatment pattern and care quality, which would put the MAR assumption in doubt. The classical semi-parametric efficiency theory relies on the correct modeling and estimation of the nuisance models. When some nuisance models cannot be consistently estimated, there is no universal efficiency guarantee for estimation procedures derived from semi-parametric efficiency theory. To ensure efficiency improvement when both the supervised estimator &#916; ^ SL = 1 N &#8721; k = 1 K &#8721; i &#8712; &#8464; k R i &#961; N &#956; ^ k 1 , X i + A i &#960; ^ k 1 , X i Y i - &#956; ^ k 1 , X i - R i &#961; N &#956; ^ k 0 , X i + 1 - A i &#960; ^ k 0 , X i Y i - &#956; ^ k 0 , X i and the SMMAL estimator &#916; ^ SMMAL are consistent and asymptotically normal, we may consider the linear ensemble &#916; ^ comb = &#916; ^ SMMAL + b &#916; ^ SL - &#916; ^ SMMAL . Suppose the influence functions for &#916; ^ SMMAL and &#916; ^ SL are &#981; SSL and R &#981; cmp / &#961; N , respectively. The optimal linear ensemble is given by b opt = Var &#981; SSL - Cov &#981; SSL , R &#981; cmp / &#961; N Var &#981; SSL + Var R &#981; cmp / &#961; N - 2 Cov &#981; SSL , R &#981; cmp / &#961; N , which can be estimated by the empirical variances and covariance of estimated influence functions constructed with estimated nuisance models ( &#956; ^ , &#960; ^ , m ^ , &#928; ^ ) . Our doubly robust estimation can be generalized to other models if the calibrated estimation for the model is available. For example, we can directly adopt the estimators from Tan (2020) for linear outcome model. The calibrated estimation is, however, limited to M-estimator in high-dimensional regression due to the paucity of works on Z-estimators in high-dimensional setting. It would be interesting to study if the Z-estimator approach ( Vermeulen and Vansteelandt, 2015 ) can be generalized to high-dimensional setting. Supplementary Material 1 Acknowledgment This publication is partially supported by the Food and Drug Administration (FDA) of the U.S. Department of Health and Human Services (HHS) as part of a financial assistance award [FAIN] totaling $367,807 with 50 percent funded by FDA/HHS. The project is also supported by grants R01 LM013614 and R01 AR080193 from the National Institute of Health. The contents are those of the authors and do not necessarily represent the official views of, nor an endorsement, by FDA/HHS, or the U.S. Government. References Bang Heejung and Robins James M. . Doubly robust estimation in missing data and causal inference models . Biometrics , 61 ( 4 ): 962 &#8211; 973 , 2005 . doi: 10.1111/j.1541-0420.2005.00377.x . URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x . 16401269 Bartlett Victoria L. , Dhruva Sanket S. , Shah Nilay D. , Ryan Patrick , and Ross Joseph S. . Feasibility of Using Real-World Data to Replicate Clinical Trial Evidence . JAMA Network Open , 2 ( 10 ): e1912869 &#8211; e1912869 , 10 2019 . ISSN 2574&#8211;3805. doi: 10.1001/jamanetworkopen.2019.12869 . URL https://doi.org/10.1001/jamanetworkopen.2019.12869 . 31596493 PMC6802419 Beaulieu-Jones Brett K. , Finlayson Samuel G. , Yuan William , Altman Russ B. , Kohane Isaac S. , Prasad Vinay , and Yu Kun-Hsing . Examining the use of real-world evidence in the regulatory process . Clinical Pharmacology &amp; Therapeutics , 107 ( 4 ): 843 &#8211; 852 , 2020 . doi: 10.1002/cpt.1658 . URL https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1002/cpt.1658 . 31562770 PMC7093234 Begun Janet M. , Hall WJ , Huang Wei-Min , and Wellner Jon A. . Information and Asymptotic Efficiency in Parametric-Nonparametric Models . The Annals of Statistics , 11 ( 2 ): 432 &#8211; 452 , 1983 . doi: 10.1214/aos/1176346151 . URL https://doi.org/10.1214/aos/1176346151 . Belloni A , Chernozhukov V , Fern&#225;ndez-Val I , and Hansen C . Program evaluation and causal inference with high-dimensional data . Econometrica , 85 ( 1 ): 233 &#8211; 298 , 2017 . doi: 10.3982/ECTA12723 . URL https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA12723 . Belloni Alexandre , Chernozhukov Victor , and Hansen Christian . Inference on Treatment Effects after Selection among High-Dimensional Controls&#8224; . The Review of Economic Studies , 81 ( 2 ): 608 &#8211; 650 , 11 2013 . ISSN 0034&#8211;6527. doi: 10.1093/restud/rdt044 . URL https://doi.org/10.1093/restud/rdt044 . Bradic Jelena , Wager Stefan , and Zhu Yinchu . Sparsity Double Robust Inference of Average Treatment Effects . arXiv , page 1905.00744, 2019 . Chakrabortty Abhishek , Lu Jiarui , Cai T. Tony , and Li Hongzhe . High dimensional m-estimation with missing outcomes: A semi-parametric framework . arXiv , page 1911.11345, 2019 . Cheng David , Ananthakrishnan Ashwin N. , and Cai Tianxi . Robust and efficient semi-supervised estimation of average treatment effects with application to electronic health records data . Biometrics , 77 ( 2 ): 413 &#8211; 423 , 2021 . doi: 10.1111/biom.13298 . URL https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13298 . 32413171 PMC7758040 Chernozhukov Victor , Chetverikov Denis , Demirer Mert , Duflo Esther , Hansen Christian , Newey Whitney , and Robins James . Double/debiased machine learning for treatment and structural parameters . The Econometrics Journal , 21 : C1 &#8211; C68 , 2018 . Duchi John . A few notes on contiguity, asymptotics, and local asymptotic normality , March 2021 . URL https://web.stanford.edu/class/stats300b/Notes/contiguity-and-asymptotics.pdf . Fan Jianqing and Lv Jinchi . A selective overview of variable selection in high dimensional feature space . Statistica Sinica , 20 ( 1 ): 101 &#8211; 148 , 2010 . 21572976 PMC3092303 Fan Jianqing and Peng Heng . Nonconcave penalized likelihood with a diverging number of parameters . The Annals of Statistics , 32 ( 3 ): 928 &#8211; 961 , 2004 . doi: 10.1214/009053604000000256 . URL https://doi.org/10.1214/009053604000000256 . Farrell Max H. . Robust inference on average treatment effects with possibly more covariates than observations . Journal of Econometrics , 189 : 1 &#8211; 23 , 2015 . Farrell Max H. , Liang Tengyuan , and Misra Sanjog . Deep neural networks for estimation and inference . Econometrica , 89 ( 1 ): 181 &#8211; 213 , 2021 . doi: 10.3982/ECTA16901 . URL https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901 . Franklin Jessica M. , Liaw Kai-Li , Iyasu Solomon , Critchlow Cathy W. , and Dreyer Nancy A. . Real-world evidence to support regulatory decision making: New or expanded medical product indications . Pharmacoepidemiology and Drug Safety , 30 ( 6 ): 685 &#8211; 693 , 2021 . doi: 10.1002/pds.5222 . URL https://onlinelibrary.wiley.com/doi/abs/10.1002/pds.5222 . 33675248 Griffith Sandra D. , Tucker Melisa , Bowser Bryan , Calkins Geoffrey , Chang Che-hsu (Joe) , Guardino Ellie , Khozin Sean , Kraut Josh , You Paul , Schrag Deb , and Miksad Rebecca A. . Generating real-world tumor burden endpoints from electronic health record data: Comparison of recist, radiology-anchored, and clinician-anchored approaches for abstracting real-world progression in non-small cell lung cancer . Advances in Therapy , 36 ( 8 ): 2122 &#8211; 2136 , Aug 2019 . ISSN 1865&#8211;8652. doi: 10.1007/s12325-019-00970-1 . URL https://doi.org/10.1007/s12325-019-00970-1 . 31140124 PMC6822856 Hernan MA and Robins JM . Causal Inference . Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probab . Taylor &amp; Francis , 2023 . ISBN 9781420076165. URL https://books.google.com/books?id=_KnHIAAACAAJ . Hou J , Bradic J , and Xu R . Treatment effect estimation under additive hazards models with high-dimensional confounding . Journal of the American Statistical Association , page In press, 2021a . Hou Jue , Guo Zijian , and Cai Tianxi . Surrogate assisted semi-supervised inference for high dimensional risk prediction , 2021b . Hou Jue , Kim Nicole , Cai Tianrun , Dahal Kumar , Weiner Howard , Chitnis Tanuja , Cai Tianxi , and Xia Zongqi . Comparison of dimethyl fumarate vs fingolimod and rituximab vsnatalizumab for treatment of multiple sclerosis . JAMA Network Open , page To appear, 2021c . Hou Jue , Zhao Rachel , Cai Tianrun , Beaulieu-Jones Brett , Seyok Thany , Dahal Kumar , Yuan Qianyu , Xiong Xin , Bonzel Clara-Lea , Fox Claire , Christiani David C. , Jemielita Thomas , Liao Katherine P. , Liaw Kai-Li , and Cai Tianxi . Temporal Trends in Clinical Evidence of 5-Year Survival Within Electronic Health Records Among Patients With Early-Stage Colon Cancer Managed With Laparoscopy-Assisted Colectomy vs Open Colectomy . JAMA Network Open , 5 ( 6 ): e2218371 &#8211; e2218371 , 06 2022 . ISSN 2574&#8211;3805. doi: 10.1001/jamanetworkopen.2022.18371 . URL https://doi.org/10.1001/jamanetworkopen.2022.18371 . 35737384 PMC9227003 Hou Jue , Zhao Rachel , Gronsbell Jessica , Lin Yucong , Bonzel Clara-Lea , Generate analysis-ready data for real-world evidence: Tutorial for harnessing electronic health records with advanced informatic technologies . Journal of Medical Internet Research , 25 : e45662 , 2023 . 37227772 10.2196/45662 PMC10251230 Imbens Guido W. and Rubin Donald B. . Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction . Cambridge University Press , 2015 . doi: 10.1017/CBO9781139025751 . Ju Cheng , Schwab Joshua , and van der Laan Mark J . On adaptive propensity score truncation in causal inference . Statistical Methods in Medical Research , 28 ( 6 ): 1741 &#8211; 1760 , 2019 . doi: 10.1177/0962280218774817 . URL https://doi.org/10.1177/0962280218774817 . 29991330 Kallus Nathan and Mao Xiaojie . On the role of surrogates in the efficient estimation of treatment effects with limited outcome data . Journal of the Royal Statistical Society Series B: Statistical Methodology , page qkae099 , 10 2024 . ISSN 1369&#8211;7412. doi: 10.1093/jrsssb/qkae099 . URL https://doi.org/10.1093/jrsssb/qkae099 . Cam Lucien Le and Yang Grace Lo . Contiguity - Hellinger Transforms , pages 34 &#8211; 49 . Springer New York , New York, NY , 2000 . ISBN 978&#8211;1-4612&#8211;1166-2. doi: 10.1007/978-1-4612-1166-2_3 . URL https://doi.org/10.1007/978-1-4612-1166-2_3 . Liao Katherine P , Sun Jiehuan , Cai Tianrun A , Link Nicholas , Hong Chuan , Huang Jie , Huffman Jennifer E , Gronsbell Jessica , Zhang Yichi , Ho Yuk-Lam , High-throughput multimodal automated phenotyping (map) with application to phewas . Journal of the American Medical Informatics Association , 26 ( 11 ): 1255 &#8211; 1262 , 2019 . 31613361 10.1093/jamia/ocz066 PMC6798574 Lin Dan-Yu and Ying Zhiliang . Semiparametric analysis of the additive risk model . Biometrika , 81 ( 1 ): 61 &#8211; 71 , 1994 . Liu Molei , Zhang Yi , and Zhou Doudou . Double/debiased machine learning for logistic partially linear model . The Econometrics Journal , 24 ( 3 ): 559 &#8211; 588 , 06 2021 . ISSN 1368&#8211;4221. doi: 10.1093/ectj/utab019 . URL https://doi.org/10.1093/ectj/utab019 . 38223304 PMC10786638 Negahban Sahand , Ravikumar Pradeep , Wainwright Martin J. , and Yu Bin . A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers . Technical Report 797 , University of California Berkeley, Department of Statistics , 2010 . Negahban Sahand N. , Ravikumar Pradeep , Wainwright Martin J. , and Yu Bin . A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers . Statistical Science , 27 ( 4 ): 538 &#8211; 557 , 2021/12/17/2012. ISSN 08834237. URL http://www.jstor.org/stable/41714783 . Full publication date: November 2012 . Newey Whitney K. and Robins James R. . Cross-fitting and fast remainder rates for semi-parametric estimation , 2018 . Petersen Maya L , Porter Kristin E , Gruber Susan , Wang Yue , and van der Laan Mark J . Diagnosing and responding to violations in the positivity assumption . Statistical Methods in Medical Research , 21 ( 1 ): 31 &#8211; 54 , 2012 . doi: 10.1177/0962280210386207 . URL https://doi.org/10.1177/0962280210386207 . 21030422 PMC4107929 Robins James M. , Rotnitzky Andrea , and Zhao Lue Ping . Estimation of regression coefficients when some regressors are not always observed . Journal of the American Statistical Association , 89 ( 427 ): 846 &#8211; 866 , 1994 . ISSN 01621459. URL http://www.jstor.org/stable/2290910 . Rotnitzky A , Smucler E , and Robins JM . Characterization of parameters with a mixed bias property . Biometrika , 108 ( 1 ): 231 &#8211; 238 , 08 2020 . ISSN 0006&#8211;3444. doi: 10.1093/biomet/asaa054 . URL https://doi.org/10.1093/biomet/asaa054 . Smucler Ezequiel , Rotnitzky Andrea , and Robins James M. . A unifying approach for doubly-robust &#8467;1 regularized estimation of causal contrasts . arXiv e-prints, art. arXiv:1904.03737, Apr 2019 . Stone Charles J. . Consistent Nonparametric Regression . The Annals of Statistics , 5 ( 4 ): 595 &#8211; 620 , 1977 . doi: 10.1214/aos/1176343886 . URL https://doi.org/10.1214/aos/1176343886 . Stone Charles J. . Optimal Global Rates of Convergence for Nonparametric Regression . The Annals of Statistics , 10 ( 4 ): 1040 &#8211; 1053 , 1982 . doi: 10.1214/aos/1176345969 . URL https://doi.org/10.1214/aos/1176345969 . Tan Zhiqiang . Model-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data . The Annals of Statistics , 48 ( 2 ): 811 &#8211; 837 , 2020 . doi: 10.1214/19-AOS1824 . URL https://doi.org/10.1214/19-AOS1824 . Tsiatis A . Semiparametric Theory and Missing Data . Springer Series in Statistics . Springer New York , 2007 . ISBN 9780387373454. URL https://books.google.com/books?id=xqZFi2EMB40C . Tsybakov AB . Introduction to Nonparametric Estimation . Springer New York, NY , 2009 . doi: 10.1007/b13794 . U.S. Cancer Statistics Working Group . U.s. cancer statistics data visualizations tool, based on 2021 submission data (1999&#8211;2019): U.s. department of health and human services, centers for disease control and prevention and national cancer institute , June 2022 . van der Vaart AW . Asymptotic Statistics . Cambridge Series in Statistical and Probabilistic Mathematics . Cambridge University Press , 1998 . doi: 10.1017/CBO9780511802256 . Vermeulen Karel and Vansteelandt Stijn . Bias-reduced doubly robust estimation . Journal of the American Statistical Association , 110 ( 511 ): 1024 &#8211; 1036 , 2015 . doi: 10.1080/01621459.2014.958155 . URL https://doi.org/10.1080/01621459.2014.958155 . Wang Yuhao and Shah Rajen D. . Debiased inverse propensity score weighting for estimation of average treatment effects with high-dimensional confounders , 2020 . Xie Yuan-Hong , Chen Ying-Xuan , and Fang Jing-Yuan . Comprehensive review of targeted therapy for colorectal cancer . Signal Transduction and Targeted Therapy , 5 ( 1 ): 22 , Mar 2020 . ISSN 2059&#8211;3635. doi: 10.1038/s41392-020-0116-z . URL https://doi.org/10.1038/s41392-020-0116-z . 32296018 PMC7082344 Yuan Qianyu , Cai Tianrun , Hong Chuan , Du Mulong , Johnson Bruce E. , Lanuti Michael , Cai Tianxi , and Christiani David C. . Performance of a Machine Learning Algorithm Using Electronic Health Record Data to Identify and Estimate Survival in a Longitudinal Cohort of Patients With Lung Cancer . JAMA Network Open , 4 ( 7 ): e2114723 &#8211; e2114723 , 07 2021 . ISSN 2574&#8211;3805. doi: 10.1001/jamanetworkopen.2021.14723 . URL https://doi.org/10.1001/jamanetworkopen.2021.14723 . 34232304 PMC8264641 Zhang Yinchi , Cai Tianrun , Yu Sheng , Cho Kelly , Hong Chuan , Sun Jiehuan , Huang Jie , Ho Yuk-Lam , Ananthakrishnan Ashwin N. , Xia Zonggi , Shaw Stanley Y. , Gainer Vivian , Castro Victor , Link Nicholas , Honerlaw Jacqueline , Huang Selena , Gagnon David , Karlson Elizabeth W. , Plenge Robert M. , Szolovits Peter , Savova Guergana , O&#8217;Donnell Christopher , Murphy Shawn N. , Gaziano J. Michael , Kohane Isaac , Cai Tianxi , and Liao Katherine P. . High-throughput phenotyping with electronic medical record data using a common semi-supervised approach (phecap) . Nature Protocal , page To appear, 2019 . Zhang Yuqian , Chakrabortty Abhishek , and Bradic Jelena . Double robust semi-supervised inference for the mean: selection bias under MAR labeling with decaying overlap . Information and Inference: A Journal of the IMA , 12 ( 3 ): 2066 &#8211; 2159 , 07 2023 . ISSN 2049&#8211;8772. doi: 10.1093/imaiai/iaad021 . URL https://doi.org/10.1093/imaiai/iaad021 . Figure 1: Causal diagrams of double missing SSL setting with missing treatment and outcome. The surrogates S represent the imprecise documentation of A and Y , which should be predictive for A and Y but not affecting the causal identification based on perfect data ( Y , A , X ) . Figure 2: Visualized simulation settings. Left-the models for PS and OR under the low-dimensional setting. Right-the mixture Beta distribution for surrogates at different level of prediction accuracy (AUC 0.8, 0.9, 0.95, 0.99, 0.999) at the median covariate ( X = 0.5 under low-dimensional smooth model and X 1 = 0 under high-dimensional logistic regression). Figure 3: Heat map for relative efficiency of the SMMAL compared to the benchmark supervised learning in all four simulation settings. Deeper red indicates larger advantage of the semi-supervised estimation. We set relative efficiency one as white in all plots, but the scale varies between low-dimensional setting and high-dimensional settings. Figure 4: Heat map for coverage of 95% confidence intervals by unsupervised learning. White marks 0.95 coverage rate. Orange marks 0.8 coverage rate. Deeper red indicates poorer coverage rate by unsupervised learning. Figure 5: Point estimate and 95% confidence interval of average risk difference from crude, Double Machine-Learning (DML), calibrated (Cal) and SMMAL analyses. Supervised learning (SL) benchmark analysed only uses the labeled data. Unsupervised learning (UL) benchmark analyses used dichotomized surrogates by matching prevalence observed in labeled data. The RE value indicated the SMMAL&#8217;s relative efficiency in comparison with the two supervised benchmark methods (ratio of estimated variances). Table 1: List of parameters used in the mixture Beta distribution for the surrogates. Setting OK Reasonable Good Great Perfect AUC 0.80 0.90 0.95 0.99 0.999 Low-dimensional smooth model &#945; A 1.39 1.99 2.54 3.86 5.49 &#945; Y 1.39 1.96 2.57 3.80 5.70 High-dimensional logistic regression &#945; A 1.36 1.99 2.54 3.80 5.64 &#945; Y 1.33 1.96 2.51 3.89 5.55 High-dimensional regression: mis-specified PS &#945; A 1.36 1.96 2.54 3.80 5.55 &#945; Y 1.39 1.93 2.54 3.74 5.52 High-dimensional regression: mis-specified OR &#945; A 1.36 1.96 2.54 3.80 5.55 &#945; Y 1.39 1.93 2.54 3.74 5.52 Table 2: Accuracy of extracted EHR feature counts for targeted therapy and 1-year progression (defined as new metastasis site) free survival from EHR valided over 100 patients reviewed by abstractor. False positive rates (FPR) and false negative rates (FNR) were calculated by the dichotomized extractions: Benchmark features &#8211; count &gt; 0; Engineered features &#8211; classification by the quantiles matching prevalence in gold-standard labels. Area under reception operating curve (AUC) were calculated using count/score as predictor (death encoded as a very large value 1000). Surrogate FPR FNR AUC Targeted Therapy Medication Code 0.44 0.17 0.60 Mention in Note 0.35 * 0.10 * 0.93 1-year Progression Free Survival Death Registry 0.02 0.43 &#8211; Death &amp; New Site Code 0.34 0.20 0.84 Death &amp; New Site in Note 0.31 0.20 0.85 Terminal-Progression Score 0.31 * 0.10 * 0.93 Straightforward rule based extraction (indicated by *) failed to capture treatment and response. Two surrogates in bold font were chosen for SMMAL for their reasonably good AUC . Table 3: Baseline characteristics of full study cohort and two arms in the labeled subset. The format is &#8220;count (percentage %)&#8221; for binary/categorical variables and &#8220;mean (standard deviation)&#8221; for numerical variables. Full data Labeled set Chemotherapy Targeted Therapy Size 4147 79 21 Demographics Age at Metastasis 62.5 (13.8) 65.2 (11.4) 62.7 (15.9) Female 1926 (46%) 46 (58%) 11 (52%) White 3470 (84%) 68 (86%) 20 (95%) Cancer characteristics at diagnosis Left Colon Tumor 221 (5%) 5 (6%) 0 (0%) Right Colon Tumor 890 (21%) 23 (29%) 8 (38%) Transverse Colon Tumor 420 (10%) 9 (11%) 1 (5%) Sigmoid Colon Tumor 2092 (50%) 44 (56%) 9 (43%) Rectum Tumor 2002 (48%) 43 (54%) 5 (24%) Metastasis Code 2674 (64%) 50 (63%) 16 (76%) Lymph Node Tumor 364 (9%) 10 (13%) 0 (0%) Stage I 55 (1%) 0 (0%) 0 (0%) Stage II 159 (4%) 3 (4%) 0 (0%) Stage II 992 (24%) 22 (28%) 2 (10%) Stage IV 2546 (61%) 46 (58%) 17 (81%) Stage Missing 395 (10%) 8 (10%) 2 (10%) Cancer characteristics at metastasis Year since Diagnosis 0.7 (2) 0.5 (1) 0.8 (1.7) Lung Metastasis Code 646 (16%) 10 (13%) 8 (38%) Liver Metastasis Code 1694 (41%) 27 (34%) 14 (67%) Liver Metastasis in Note 1422 (34%) 27 (34%) 12 (57%) Treatments between diagnosis and metastasis Chemotherapy Code 1.4 (5.3) 1.3 (3.7) 0 (0) Radiotherapy Code 10.1 (35.1) 10.5 (30.4) 7.2 (29.1) Colon Biopsy Code 0.6 (1.7) 0.5 (1.5) 0 (0) Colon Rescission Code 0.4 (0.8) 0.3 (0.7) 0.2 (0.5) Healthcare utilization Before Metastasis 29.7 (59.3) 36.2 (74.5) 13 (21.2) One Year Before Metastasis 9.8 (15.4) 10.2 (14.7) 4.2 (8.1)"
}