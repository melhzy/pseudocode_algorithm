{
  "pmcid": "PMC12678759",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:19.859444",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12678759",
    "pmid": "41345162",
    "doi": "10.1038/s41598-025-27303-3",
    "title": "MedShieldFL-a privacy-preserving hybrid federated learning framework for intelligent healthcare systems",
    "year": "2025",
    "month": "12",
    "day": "4",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "4"
    },
    "authors": [
      "Murala Dileep Kumar",
      "Krishna G. Siva",
      "kiran Tanneeru Venkata Surya",
      "Mohamud Abdirahman Khalif"
    ],
    "abstract": "Recent advances in artificial intelligence have greatly increased the accuracy of computer-assisted diagnosis for serious conditions including brain tumours. However, concerns about data privacy, class imbalance, and the diversity of medical datasets limit the application of centralised deep learning models in healthcare. This article introduces MedShieldFL, a hybrid privacy-preserving federated learning architecture that enables secure and decentralised brain tumour classification across many medical institutions. The approach uses data augmentation techniques to reduce class imbalance and homomorphic encryption to safely aggregate model changes while safeguarding sensitive patient data. The basic model is a ResNet-18-based classifier that strikes the ideal balance between accuracy and speed. The test results for MedShieldFL show that it can accurately group data into 93% to 96% of the time. This approach improves performance by about 2% compared to traditional federated learning models and keeps data privacy safe enough. The framework makes sure that the extra work that encryption adds to real-world programs stays within acceptable limits. This keeps execution times fair. Medical picture evaluation with MedShieldFL is a useful and flexible technology that protects privacy. This makes it easier for current healthcare systems to use AI that is safe and works with other AI.",
    "keywords": [
      "Artificial intelligence",
      "Federated deep learning",
      "Homomorphic encryption",
      "Privacy",
      "Smart healthcare",
      "Security",
      "Computational biology and bioinformatics",
      "Engineering",
      "Health care",
      "Mathematics and computing"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12678759</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12678759.1</article-id><article-id pub-id-type=\"pmcaid\">12678759</article-id><article-id pub-id-type=\"pmcaiid\">12678759</article-id><article-id pub-id-type=\"pmid\">41345162</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-27303-3</article-id><article-id pub-id-type=\"publisher-id\">27303</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MedShieldFL-a privacy-preserving hybrid federated learning framework for intelligent healthcare systems</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Murala</surname><given-names initials=\"DK\">Dileep Kumar</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Krishna</surname><given-names initials=\"GS\">G. Siva</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>kiran</surname><given-names initials=\"TVS\">Tanneeru Venkata Surya</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Mohamud</surname><given-names initials=\"AK\">Abdirahman Khalif</given-names></name><address><email>MohamudAB@africacdc.org</email></address><xref ref-type=\"aff\" rid=\"Aff4\">4</xref><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04p3pp808</institution-id><institution-id institution-id-type=\"GRID\">grid.466746.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1775 3818</institution-id><institution>Department of Computer Science and Engineering, Faculty of Science and Technology, </institution><institution>ICFAI Foundation for Higher Education, </institution></institution-wrap>Hyderabad, 501203 Telangana India </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04p3pp808</institution-id><institution-id institution-id-type=\"GRID\">grid.466746.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1775 3818</institution-id><institution>Faculty of Operations and IT, </institution><institution>IBS Hyderabad, The ICFAI Foundation for Higher Education, (Declared as Deemed-to-be university u/s 3 of the UGC Act 1956), </institution></institution-wrap>Hyderabad, 501203 Telangana India </aff><aff id=\"Aff3\"><label>3</label>Department of Computer Science and Engineering, Data Science, Sphoorthy Engineering College, Nadergul, 501510 Telangana India </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01d9dbd65</institution-id><institution-id institution-id-type=\"GRID\">grid.508167.d</institution-id><institution>Africa Center for Disease Control and Prevention (Africa CDC), </institution></institution-wrap>HQ office, Addis Ababa, Ethiopia </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/05brr5h08</institution-id><institution-id institution-id-type=\"GRID\">grid.449364.8</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 5986 0427</institution-id><institution>Jamhuriya Research Center, </institution><institution>Jamhuriya University of Science and Technology, </institution></institution-wrap>Mogadishu, 00252 Somalia </aff></contrib-group><pub-date pub-type=\"epub\"><day>4</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>43144</elocation-id><history><date date-type=\"received\"><day>9</day><month>9</month><year>2025</year></date><date date-type=\"accepted\"><day>3</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>04</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>06</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-06 00:25:12.373\"><day>06</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_27303.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Recent advances in artificial intelligence have greatly increased the accuracy of computer-assisted diagnosis for serious conditions including brain tumours. However, concerns about data privacy, class imbalance, and the diversity of medical datasets limit the application of centralised deep learning models in healthcare. This article introduces MedShieldFL, a hybrid privacy-preserving federated learning architecture that enables secure and decentralised brain tumour classification across many medical institutions. The approach uses data augmentation techniques to reduce class imbalance and homomorphic encryption to safely aggregate model changes while safeguarding sensitive patient data. The basic model is a ResNet-18-based classifier that strikes the ideal balance between accuracy and speed. The test results for MedShieldFL show that it can accurately group data into 93% to 96% of the time. This approach improves performance by about 2% compared to traditional federated learning models and keeps data privacy safe enough. The framework makes sure that the extra work that encryption adds to real-world programs stays within acceptable limits. This keeps execution times fair. Medical picture evaluation with MedShieldFL is a useful and flexible technology that protects privacy. This makes it easier for current healthcare systems to use AI that is safe and works with other AI.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Artificial intelligence</kwd><kwd>Federated deep learning</kwd><kwd>Homomorphic encryption</kwd><kwd>Privacy</kwd><kwd>Smart healthcare</kwd><kwd>Security</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Computational biology and bioinformatics</kwd><kwd>Engineering</kwd><kwd>Health care</kwd><kwd>Mathematics and computing</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">The spread of the Internet of Things (IoT) and the rise of Artificial Intelligence (AI) have led to a technology revolution that is changing many fields, including healthcare<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. Today&#8217;s healthcare systems use the IIoT infrastructure powered by AI for cognitive diagnostics, remote patient tracking, predictive analytics, and collecting data in real time<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. Medical picture processing shows promise as a way to find brain tumours and other serious illnesses early on. Magnetic resonance imaging (MRI) must be used to quickly and accurately identify brain tumours in order to improve patient survival and treatment outcomes<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>. AI and the Internet of Things (IoT) have changed the way nursing is done today<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. The Internet of Industrialised Things (IIoT) makes it possible for smart, networked healthcare systems to collect data in real time and use predictive analytics to solve important healthcare issues<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>. Brain cancers can now be automatically put into groups using magnetic resonance imaging<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. A quick and correct diagnosis is critical for improved survival and treatment outcomes<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. Brain tumours were diagnosed using traditional machine learning (ML) approaches such as Random Forests (RF) and Support Vector Machines (SVM)<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>. However, these approaches frequently require human feature extraction and preprocessing<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. Some of these procedures are effective, but they are domain-specific and not compatible with other datasets. CNNs and other deep learning approaches enable models to learn from unprocessed MRI images<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. The application of hierarchical feature learning in architectures such as ResNet, VGGNet, and U-Net has resulted in enhanced tumour segmentation, detection, and classification, reducing the requirement for human feature engineering<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref>,<xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>.</p><p id=\"Par3\">Legal, ethical, and privacy requirements such as HIPAA and GDPR divide big annotated medical datasets among institutions, making centralised DL models challenging to apply. This isolation prevents researchers from developing models that generalise across heterogeneous data. Federated learning (FL) enables several institutions to collaborate on model training without sharing raw data, overcoming the problem mentioned in<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>. When training a local model, each client only communicates with the central server via encrypted weights or gradients. Several frameworks, including FedAvg, FedHealth, and FeTS challenge<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref>,<xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>, have demonstrated FL&#8217;s effectiveness in healthcare settings with several institutions. Despite their potential, FL-based medical frameworks have encountered several problems<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>. The lack of modern cryptographic approaches and differential privacy makes most FL-based models subject to risks such as model inversion attacks and membership inference<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>. They frequently assume that consumers have adequate and impartial information, although this is rarely the case. Several client datasets have been augmented with Generative Adversarial Networks (GANs) for generalisation and resilience<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref>,<xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>,.Few approaches improve learning efficiency by combining GAN-based augmentation with homomorphic encryption (HME) for confidentiality<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref>,<xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup>. Because of these constraints, a comprehensive FL architecture that provides data efficiency, dependability, and confidentiality in a variety of healthcare settings is urgently required.</p><sec id=\"Sec2\"><title>IIoT-enabled federated learning for healthcare: a security threat landscape</title><p id=\"Par4\">Figure <xref rid=\"Fig1\" ref-type=\"fig\">1</xref> shows IIoT-enabled FL security problems in multiple dimensions. Hospitals use IoT-enabled infrastructure to learn from sensitive patient MRI data as local data owners<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>. The primary server receives only encrypted model changes from these organisations.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Analysing potential points of attack on IIoT-related federated learning systems.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig1_HTML.jpg\"/></fig></p><p id=\"Par5\">However, multiple vulnerabilities arise:<list list-type=\"bullet\"><list-item><p id=\"Par6\"><bold>Client-level attacks:</bold> Adversaries may launch data poisoning attacks by inserting mislabeled samples or conduct model poisoning by sending manipulated updates to corrupt global training<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>.</p></list-item><list-item><p id=\"Par7\"><bold>Communication interception:</bold> Data exchanged between clients and server can be targeted by man-in-the-middle (MitM), replay, or eavesdropping attacks if not securely encrypted<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>.</p></list-item><list-item><p id=\"Par8\"><bold>Aggregator compromise:</bold> Even with HE, compromised decryption keys can lead to gradient leakage or model inversion attacks.</p></list-item><list-item><p id=\"Par9\"><bold>Backdoor injection:</bold> A malicious server may tamper with the global model during redistribution.</p></list-item><list-item><p id=\"Par10\"><bold>Adversarial inputs:</bold> Attackers may use adversarial examples to degrade model performance<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>.</p></list-item></list></p><p id=\"Par11\">Furthermore, IIoT infrastructure introduces new risks such as denial-of-service (DoS) attacks, sensor hijacking, and data tampering due to resource-constrained and poorly secured edge devices<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>. To counter these risks, our proposed MedShieldFL framework integrates multiple defences: homomorphic encryption for secure aggregation, GAN-based augmentation to combat data imbalance, and strict data localisation protocols to avoid sharing raw medical data<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref>,<xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>. These measures collectively enhance both security and learning performance in privacy-sensitive environments.</p></sec><sec id=\"Sec3\"><title>Research gaps</title><p id=\"Par12\">Despite ongoing research, several critical gaps remain in developing privacy-preserving AI for healthcare. <list list-type=\"order\"><list-item><p id=\"Par13\"><bold>Insufficient integration of many privacy techniques: </bold> Most FL frameworks use one privacy method (differential privacy or encryption); therefore, privacy, accuracy, and performance are usually trade-offs. There hasn&#8217;t been enough research on a hybrid strategy using HE and GANs<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref>,<xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>.</p></list-item><list-item><p id=\"Par14\"><bold> Lack of Data in Federated Medical Settings:</bold> FL assumes that each client has enough local data to work with. In practice, though, individual hospitals may not have enough or balanced datasets, making models less accurate and converging more slowly<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref>,<xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>.</p></list-item><list-item><p id=\"Par15\"><bold> Not Enough Testing on Realistic and Synthetic Data:</bold> Many current solutions don&#8217;t perform well, as they work on both real-world clinical datasets and synthetically augmented datasets, which are important for figuring out how well they work in general and how to improve privacy<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup>.</p></list-item><list-item><p id=\"Par16\"><bold> Model updates are not well protected:</bold> Even if FL is better for privacy than centralised learning, model changes can still leak private information<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup>. There aren&#8217;t many safe ways to combine encrypted model updates to eliminate this issue<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup>.</p></list-item><list-item><p id=\"Par17\"><bold>Limited Use in Smart Healthcare Situations:</bold> There isn&#8217;t much research that specifically provides privacy-preserving FL frameworks for smart healthcare IIoT settings, where medical data is created, sent, and analysed in real time across distributed edge-cloud systems<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref>,<xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>.</p></list-item></list></p></sec><sec id=\"Sec4\"><title>Novelty of the work</title><p id=\"Par18\">The novelty of this research lies in the holistic integration of federated learning (FL), homomorphic encryption (HME), and generative adversarial networks (GANs) into a single unified framework <italic toggle=\"yes\">MedShieldFL</italic> designed specifically for privacy-preserving and performance-optimised medical image classification in innovative healthcare environments. Researchers have studied these technologies individually, but deploying them context-awarely within resource-constrained, IIoT-based settings represents a significant advancement. Our strategy incorporates real-time GAN-based data augmentation within the federated learning (FL) pipeline, enabling clients with under-represented or limited datasets to actively participate in model training without compromising data ownership or privacy. We also examine the impact of HME on model consistency and latency with numerous clients, a previously unaddressed issue. Comparative studies show that MedShieldFL is better than baseline FL models in terms of privacy, classification accuracy, and stability. As a result of the limitations of IIoT healthcare infrastructure, this synchronous design fixes problems with data mismatch and privacy.</p></sec><sec id=\"Sec5\"><title>key contributions</title><p id=\"Par19\">MedShieldFL, a hybrid federated deep learning (FDL) design, gets around problems and lets brain tumours be categorised while keeping privacy safe. The main inputs are the following:<list list-type=\"bullet\"><list-item><p id=\"Par20\"><bold>Data Augmentation via GAN:</bold> Deep Convolutional GANs (DCGANs) make fake MRI images when there isn&#8217;t enough data to improve data variety and class equilibrium.</p></list-item><list-item><p id=\"Par21\"><bold>ResNet-18 Backbone:</bold> ResNet-18 is used as the main classification model because it is accurate and doesn&#8217;t cost much.</p></list-item><list-item><p id=\"Par22\"><bold>Homomorphic Encryption (HME):</bold> Some HME methods, like BFV and CKKS, make aggregation safer by keeping model updates safe from inference attacks.</p></list-item><list-item><p id=\"Par23\"><bold>Realistic IIoT Healthcare Simulations:</bold> There are several federated rounds and client setups that our framework goes through, and we test it using real, fake, and mixed data.</p></list-item></list></p><p id=\"Par24\">MedShieldFL offers a precise and scalable system that protects privacy and can be used in real-world digital healthcare settings.</p></sec></sec><sec id=\"Sec6\"><title>Related work</title><p id=\"Par25\">Deep Learning (DL) models are increasingly being utilised in domains such as Computer Vision (CV), Natural Language Processing (NLP), and Speech Recognition (SR) to address complex problems<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. Due to this, new worries over data privacy have emerged, particularly in sensitive data cases<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. DL models, particularly CNNs and other DNNs, have demonstrated excellent proficiency at learning hierarchical features from raw data<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>. However, due to its richness and representational capabilities, sensitive training data, such as individual medical records, biometric information, or financial transactions, may be easier to store or disclose<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref>,<xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>. Deploying DL models on cloud platforms such as Machine Learning-as-a-Service (MLaaS) providers exacerbates these problems because researchers must send training data and inference queries to third-party systems that hackers can exploit<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup>. Membership inference attacks, model inversion attacks, and backdoor attacks are all known threats that try to get private information from the data or the trained models<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>. Researchers have devised several privacy-preserving methods to protect data at different points in the DL pipeline, from collecting and preprocessing data to training and predicting models. Some of their cryptographic approaches include HE, Secure Multiparty Computation (SMC), and Differential Privacy (DP)?. The Ach method has pros and cons regarding model correctness, computing overhead, and communication efficiency<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup>.</p><sec id=\"Sec7\"><title>Homomorphic encryption(HME)-based approaches</title><p id=\"Par26\">HME computes encrypted data without decryption to maintain processing pipeline confidentiality during processing<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref>,<xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>., the authors explored HE in DL settings by enabling encrypted inference and encrypted model evaluation. Specifically<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>, proposed a cloud-based DL service in which a pre-trained DL model processed encrypted inputs and returned the results in encrypted form. To address the incompatibility of standard activation functions (such as ReLU or sigmoid) with HE, the authors approximated them using quadratic polynomials<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>, the researchers employed bootstrapping techniques to refresh ciphertexts and extend the depth of computation, although this approach increased storage and processing demands. Although promising, these solutions are frequently constrained by high computational complexity and latency, rendering them less suited for real-time or large-scale applications.</p></sec><sec id=\"Sec8\"><title>Federated learning and differential privacy</title><p id=\"Par27\">FL provides an alternative to centralised deep learning that preserves privacy by allowing model training directly on edge devices or institutional servers. Odel updates, such as gradients or weights, are typically shared with a central server instead of raw data<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>. When combined into a global model, these changes can significantly improve server privacy and prevent data leakage. This decentralised method keeps data from being shared directly, which lowers privacy threats by a lot<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>. However, the adjustments to the local model itself could give away information about the data behind it. Researchers have added differential privacy to FL frameworks to help with this<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>, the Laplace procedure altered model gradients before sending them to the central server. This approach provides anonymity but can reduce model accuracy, especially in deeper neural networks with cross-layer noise. In DP-based FL systems, privacy budget versus utility remains a major issue<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>.</p></sec><sec id=\"Sec9\"><title>Comparative analysis of AI and FL techniques in smart healthcare for brain tumour(BT) classification</title><p id=\"Par28\">\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Smart Healthcare Brain Tumour (BT) Classification Methods.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Category</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Key Features/Strengths</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Limitations/Privacy &amp; IIoT</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>\n<bold>Traditional ML</bold>\n</p><p>(SVM, RF)<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup></p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Manual features; interpretable; efficient for small data</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Needs expertise; not scalable; weak privacy; limited IIoT</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>\n<bold>Centralized DL</bold>\n</p><p>(CNN, U-Net)<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup></p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deep feature learning; high accuracy</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Needs central data; privacy risks; moderate IIoT</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>\n<bold>FL (Standard)</bold>\n</p><p>(FedAvg, FeTS)<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup></p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Local training; multi-institution support</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Non-IID sensitive; medium privacy; edge-capable</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>\n<bold>FL + Privacy</bold>\n</p><p>(DP/HE)<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup></p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Encryption; improved privacy</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High compute cost; utility trade-offs; strong privacy</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>FL + GAN</bold>\n<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data synthesis; boosts generalization</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">GAN instability; synthetic bias; good IIoT</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>\n<bold>Proposed</bold>\n</p><p>(MedShieldFL)</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">HE + GAN; robust, private, generalizable</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Early-stage; high overhead; top privacy; excellent IIoT</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par29\">Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> demonstrates how brain tumours are classified in modern healthcare systems, highlighting how processes have improved and the issues they have caused. Traditionally, machine learning algorithms like Support Vector Machines and Random Forests rely on human feature extraction, which requires domain expertise and has limited generalisability across different datasets<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>. These tactics were once effective, but they no longer work due to the complexity and variety of medical imaging data. Using convolutional neural networks in centralised deep learning models such as ResNet and U-Net proved useful. These models provide end-to-end learning from MRI images, boosting accuracy while minimising human involvement<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>. Centralised deep learning systems require massive amounts of tagged medical data, while HIPAA and GDPR distribute this data across multiple businesses. Data segmentation makes it difficult to build universal models.</p><p id=\"Par30\">Federated learning enables businesses to train models without sharing raw data, thus addressing privacy concerns. Although espiFL&#8217;s FedAvg and FedHealth demonstrate FFL&#8217;s potential in remote medical settings, constraints remain<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>. All FL implementations are vulnerable to inference attacks since they lack privacy capabilities such as Differential Privacy and HME. Many individuals believe that databases are fair and effective for all users; however, this is not always the case. When there is insufficient data, models cannot be generalised since there are no synthetic data augmentation methods, such as GANs. To address these issues and improve stability, we require a larger federated learning framework that employs privacy-preserving approaches such as homomorphic encryption and generative adversarial network-based augmentation. This system enables real-time, secure, and scalable diagnostic tasks in a smart healthcare framework based on IIoT. This will make brain tumour categorisation methods more reliable and user-friendly.</p></sec><sec id=\"Sec10\"><title>Homomorphic encryption in federated settings</title><p id=\"Par31\">Some research uses Homomorphic Encryption in Federated Learning frameworks to strike a balance between privacy and utility. For instance<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, suggested using Paillier Homomorphic Encryption (PHE) to protect gradients in federated training. After the central server sent encrypted changes, clients used a shared key to decode the global model. In contrast to differential privacy, this strategy did not require explicit noise augmentation and maintained the model&#8217;s accuracy. However, handling keys, dealing with complex calculations, and coping with encryption delay proved difficult, particularly in large or low-bandwidth environments.</p></sec><sec id=\"Sec11\"><title>Privacy-preserving frameworks for industrial and healthcare applications</title><p id=\"Par32\">Researchers have tried to adapt FL for privacy-critical domains such as industrial IoT and healthcare. In<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>, the authors introduced a proxy server to anonymize client identities and mediate communication with the central server. Heyy applied differential privacy to perturb selected model parameters and employed conventional encryption techniques to secure data transmission. Similarly, enhanced the reliability of FL systems using blockchain technology and smart contracts, providing verifiability and auditability in decentralised environments<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>. Both studies used DP-SGD with Gaussian noise to protect exchanged gradients and RSA encryption to secure data in transit. However, these methods still have certain limitations:<list list-type=\"bullet\"><list-item><p id=\"Par33\"><bold>Data Complexity:</bold> Most research used simple benchmarks such as MNIST and CIFAR-10, which do not accurately reflect the complexity and sensitivity of medical data<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup>.</p></list-item><list-item><p id=\"Par34\"><bold>Model Simplicity:</bold> Their shallow deep learning models are ineffective for classifying brain tumours<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>.</p></list-item><list-item><p id=\"Par35\"><bold>Limited Scalability:</bold> Adding proxy servers or blockchain layers to many apps raises communication and processing costs<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup>.</p></list-item><list-item><p id=\"Par36\"><bold>Limited Accuracy:</bold> Noise or simpler models can frequently exacerbate problems, and in practice, accuracy can fall below 70&#8211;80%<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>.</p></list-item></list></p></sec><sec id=\"Sec12\"><title>GAN-based privacy and data augmentation</title><p id=\"Par37\">Another new idea is to employ GANs to add people&#8217;s medical datasets while keeping people&#8217;s privacy. ANs, especially DCGANs, can make fake medical documents that seem real but don&#8217;t have any identifying information in them<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. This method adds to the training dataset and an implicit degree of privacy by blending actual and synthetic data during model training<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. AN-based augmentation looks good, but how well it works depends on how good and varied the synthetic data is and how well the DL model can learn from datasets with both real and fake data<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>. Researchers have developed numerous DL approaches to protect privacy, but these methods often fail to perform optimally in complex, real-world healthcare scenarios. Many researchers encounter performance issues because their datasets and models are overly simplistic<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. As a result, they achieve lower accuracy and exhibit limited applicability. A hybrid architecture that integrates the best parts of FL, HE, and GANs is needed to make distributed medical intelligence safe, scalable, and fast. We deal with these problems by suggesting a new, integrated way to classify brain tumours in smart healthcare systems<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>.</p></sec></sec><sec id=\"Sec13\"><title>Proposed MedShieldFL framework</title><p id=\"Par38\">This section presents MedShieldFL, a comprehensive privacy-preserving federated learning (FL) architecture designed for brain tumor classification. The proposed solution effectively addresses the challenges of limited annotated medical imaging data and data privacy in collaborative machine learning. Our proposed framework consists of three components that work efficiently together:<list list-type=\"bullet\"><list-item><p id=\"Par39\">DCGAN-based data augmentation to address data shortages and balance classes.</p></list-item><list-item><p id=\"Par40\">A CNN-based multi-grade brain cancer classification model that can accurately tell the difference between different types of tumours.</p></list-item><list-item><p id=\"Par41\">Secure Aggregation and Global Model Generation: to keep data private during the FL process by employing homomorphic encryption (HME).</p></list-item></list></p><p id=\"Par42\">The next sections cover the architecture, parts, functions of the entities, and workflows for the pieces.</p><sec id=\"Sec14\"><title>MedShieldFL framework overview</title><p id=\"Par43\">The MedShieldFL architecture uses federated learning to educate a global deep learning model how to classify brain tumours while also preserving and storing sensitive patient data. HIPAA and GDPR guarantee the privacy of healthcare data, which is why this strategy is critical.The MedShieldFL architecture (Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>) involves three main entities:<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>MedShieldFL framework architecture for secure and private federated deep learning brain tumour categorisation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig2_HTML.jpg\"/></fig></p><p id=\"Par44\">\n<list list-type=\"bullet\"><list-item><p id=\"Par45\"><bold>Primary Server (Coordinator)</bold>: An authorised group that initiates the system, handles encryption keys, and modifies global models.</p></list-item><list-item><p id=\"Par46\"><bold>Hospitals (Local Data Owners)</bold>: Participants that hold local, IoT-captured MRI data and perform model training on-site.</p></list-item><list-item><p id=\"Par47\"><bold>Secure Aggregator Server (Public Aggregator)</bold>: An untrusted server that performs encrypted model aggregation without accessing raw data or model parameters in plaintext.</p></list-item></list>\n</p><p id=\"Par48\">The federated learning workflow in MedShieldFL proceeds through the following key phases: <list list-type=\"order\"><list-item><p id=\"Par49\"><italic toggle=\"yes\">Key Generation and Model Initialization:</italic> The primary server generates a homomorphic encryption (HME) key pair (public key PubK, private key PrivK) and distributes the initial deep learning model to all participating hospitals.</p></list-item><list-item><p id=\"Par50\"><italic toggle=\"yes\">Local Data Augmentation and Training:</italic> Each hospital uses Deep Convolutional Generative Adversarial Networks (DCGANs) to augment its MRI dataset, address data scarcity, and class imbalance. The system trains a CNN-based model (such as ResNet-18) locally and encrypts its parameters using the public key (PubK).</p></list-item><list-item><p id=\"Par51\"><italic toggle=\"yes\">Encrypted Parameter Aggregation:</italic> The Secure Aggregator receives the encrypted model updates from each hospital and aggregates them using homomorphic encryption without decryption, ensuring data privacy throughout the process.</p></list-item><list-item><p id=\"Par52\"><italic toggle=\"yes\">Update on the Global Model:</italic> Use PrivK to decrypt aggregated parameters and update the global model on the primary server. All hospitals receive the new model for the following FL round.</p></list-item><list-item><p id=\"Par53\"><italic toggle=\"yes\">Model Convergence:</italic> The stages above are repeated numerous FL rounds until the model converges. Inference using the final global model classifies brain tumours accurately and privately.</p></list-item></list></p></sec><sec id=\"Sec15\"><title>Secure aggregation architecture</title><p id=\"Par54\">The secure aggregation architecture addresses the security and privacy concerns by incorporating explicit trust assumptions, strong key management, masking-based aggregation, and replay protection. The design consists of four major components: the Data Preprocessing Layer, Institutional Clients (Hospitals), the Secure Aggregator (Public Aggregator), and the Key Server. This tiered method protects private information and facilitates collaborative model training.</p><sec id=\"Sec16\"><title>Data preprocessing layer</title><p id=\"Par55\">First, each organisation preprocesses the medical images it gets from IoT medical sensors. A deep learning generative model, such as Deep Convolutional Generative Adversarial Networks, is used to add more data to the training collection and make the model more general. The raw data is never sent outside of the school; instead, feature representations or model updates made from this data are used for federated learning.</p></sec><sec id=\"Sec17\"><title>Institutional clients (Hospitals)</title><p id=\"Par56\">Every hospital serves as a teaching hub in its own area. When it learns its model locally on its own private dataset, it uses homomorphic encryption (HE) to protect the model updates, such as weight gradients, before sending them. There are two types of homomorphic encryption methods used: <bold>CKKS</bold> (for data with close to real values) and <bold>BFV</bold> (for exact integer operations).</p><sec id=\"Sec18\"><title>Key management</title><p id=\"Par57\">The Main Server is the only one who can make and handle the encryption key pair. It is the Main Server&#8217;s job to decode messages, while clients use the public key to encrypt them. In order to keep direct ciphertext exposure to a minimum, clients never send updates directly to the server; instead, all data goes through the secure aggregator.</p></sec><sec id=\"Sec19\"><title>Masking mechanism</title><p id=\"Par58\">The client uses a dropout-tolerant masking technique on its encrypted updates so that the server can&#8217;t figure out who contributed what, even when only some of the participants are present.</p></sec></sec><sec id=\"Sec20\"><title>Secure aggregator (public aggregator)</title><p id=\"Par59\">Clients communicate masked, encrypted aggregates instead of sending data directly to the server, which mitigates key exposure. Masking ensures that even an honest and interested server cannot determine who contributed what to the aggregates. The masks prevent update identification even when the aggregator and server function together. Nonce, epoch-based binding, and per-round auditing prevent replay attacks.<list list-type=\"bullet\"><list-item><p id=\"Par60\"><bold>HE Scheme Specifications:</bold> BFV handles integer math correctly, but CKKS performs floating-point math using scaling factors. To ensure that the results can be replicated, there is a wealth of information on <italic toggle=\"yes\">N</italic>, <italic toggle=\"yes\">q</italic>, scale, batching technique, and rescaling/relinearization budgets. To reduce the frequency of poisoning attempts, future upgrades could include anomaly detection or robust aggregation.</p></list-item></list></p></sec><sec id=\"Sec21\"><title>Results and advantages</title><p id=\"Par61\">This secure aggregation method protects privacy, prevents collusion and replay attacks, and combines models with high diagnostic accuracy for intelligent medical decision support systems. Radiologists can utilise a worldwide approach to protect patient information. This precise design and threat model responds to every actionable reviewer criticism.</p></sec></sec><sec id=\"Sec22\"><title>Data augmentation with DCGAN</title><p id=\"Par62\">In this approach, we use the DCGAN model to improve target classifier accuracy and privacy when working with sensitive medical data. A DL system called GAN employs data distribution to generate synthetic images. The generative model&#8217;s job is to make images that look like photos, but the discriminator&#8217;s job is to check the pictures&#8217; quality and tell them apart. The generator always tries to improve my synthetic images to train the discriminator. Eep Convolutional Generative Adversarial Networks, or DCGAN, is a more advanced version of GAN. It has a better network design that makes it more flexible, prevents model collapse, and improves the images it creates. It replaces pooling layers with strided convolutional layers in the designs of both the generator and the discriminator networks.</p><p id=\"Par63\">Figure <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> illustrates the basic architecture of a DCGAN. The generator network, <inline-formula id=\"IEq1\"><tex-math id=\"d33e958\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi$$\\end{document}</tex-math></inline-formula>, consists of 2D batch normalization (BN) layers, strided 2D transposed convolutional (CONVT) layers, and ReLU activation functions to stabilize training and prevent mode collapse. The generator takes a latent vector <inline-formula id=\"IEq2\"><tex-math id=\"d33e962\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$z \\sim p_z$$\\end{document}</tex-math></inline-formula> and upsamples it through CONVT layers with a final <inline-formula id=\"IEq3\"><tex-math id=\"d33e966\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\tanh$$\\end{document}</tex-math></inline-formula> activation to produce an image matching the training dimensions (e.g., <inline-formula id=\"IEq4\"><tex-math id=\"d33e970\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1 \\times 64 \\times 64$$\\end{document}</tex-math></inline-formula>).The discriminator or <inline-formula id=\"IEq5\"><tex-math id=\"d33e975\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\psi$$\\end{document}</tex-math></inline-formula> classifies inputs as real or generated using strided 2D CONV layers, LeakyReLU activations, dropout, and batch normalization, outputting a probability via a sigmoid function. uring training, <inline-formula id=\"IEq6\"><tex-math id=\"d33e979\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\psi$$\\end{document}</tex-math></inline-formula> maximizes <inline-formula id=\"IEq7\"><tex-math id=\"d33e983\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\log \\psi (x)$$\\end{document}</tex-math></inline-formula> for real samples <inline-formula id=\"IEq8\"><tex-math id=\"d33e987\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x \\sim p_{\\text {data}}(x)$$\\end{document}</tex-math></inline-formula>, while the generator <inline-formula id=\"IEq9\"><tex-math id=\"d33e991\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi$$\\end{document}</tex-math></inline-formula> minimizes <inline-formula id=\"IEq10\"><tex-math id=\"d33e995\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\log (1 - \\psi (\\phi (z)))$$\\end{document}</tex-math></inline-formula> for <inline-formula id=\"IEq11\"><tex-math id=\"d33e1000\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$z \\sim p_z(z)$$\\end{document}</tex-math></inline-formula>. This adversarial objective is formalized as:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e1004\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\min _{\\phi } \\max _{\\psi } \\;&amp;\\mathbb {E}_{x \\sim p_{\\text {data}}(x)} \\big [\\log \\psi (x)\\big ] \\nonumber \\\\&amp;+ \\mathbb {E}_{z \\sim p_z(z)} \\big [\\log \\big (1 - \\psi (\\phi (z))\\big )\\big ]. \\end{aligned}$$\\end{document}</tex-math></disp-formula><fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>DCGAN Architecture used for data augmentation in brain tumor image synthesis.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig3_HTML.jpg\"/></fig></p></sec><sec id=\"Sec23\"><title>CNN-based multi-grade classification model</title><p id=\"Par64\">In the suggested framework for brain cancer classification, we utilised a deep CNN architecture founded on Residual Networks (ResNet), specifically the ResNet-18 variation. Researchers esteem ResNet models for their resilience and efficacy in image classification tasks, primarily because of their non-elastic use of residual connections. These alleviate the vanishing gradient issue frequently faced in deeper networks. he ResNet-18 architecture comprises 18 layers (Figure <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>). One FC and seventeen convolutional layers are present. The model starts with a 7x7 convolutional layer and a 3x3 max pooling technique. The model takes in a greyscale MRI picture that has been shrunk to 224<inline-formula id=\"IEq12\"><tex-math id=\"d33e1022\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>224 pixels and has one input channel. The successive layers of the network do a series of 3<inline-formula id=\"IEq13\"><tex-math id=\"d33e1026\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>3 convolutional operations, and the number of feature maps grows through four stages: 64, 128, 256, and 512 channels. These convolutional stages have residual (skip) connections around one or more layers. This approach lets gradients flow directly through the network during backpropagation, which makes training much more stable and efficient. The final convolutional output is reduced spatially by global average pooling. Fully connected (FC) layer optimised for brain tumour classification into low-grade, mid-grade, and high-grade receives a compact feature vector from this process. Softmax activation function translates raw output scores into a probability distribution over the three tumour grades, and the model chooses the most probable class. NN-based approach learns complicated spatial and textural patterns in MRI images to differentiate tumour grades accurately. esNet-18 is ideal for real-time and federated learning in dispersed healthcare due to its model depth and computational efficiency.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Optimized ResNet-18 architecture for multi-grade brain tumor classification.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig4_HTML.jpg\"/></fig></p></sec><sec id=\"Sec24\"><title>Secure homomorphic encrypted collaborative federated learning</title><p id=\"Par65\">This section discusses the framework of Secure Collaborative FL with HE, outlining the techniques involved and describing the steps for securely generating global knowledge through collaborative training of local models across multiple participants.</p><sec id=\"Sec25\"><title>Federated learning(FL)</title><p id=\"Par66\">FL allows several clients to build global DL models without exchanging data. Define the set of <italic toggle=\"yes\">N</italic> participating clients (e.g., hospitals) as:<disp-formula id=\"Equ17\"><tex-math id=\"d33e1048\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathscr {H} = \\{H_1, H_2, \\ldots , H_N\\},$$\\end{document}</tex-math></disp-formula>where each client <inline-formula id=\"IEq14\"><tex-math id=\"d33e1052\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_i$$\\end{document}</tex-math></inline-formula> has access to a private dataset <inline-formula id=\"IEq15\"><tex-math id=\"d33e1056\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D^{(i)}_L$$\\end{document}</tex-math></inline-formula>. Each client trains the model locally for <italic toggle=\"yes\">ep</italic> epochs. The local training process at the client <inline-formula id=\"IEq16\"><tex-math id=\"d33e1064\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_i$$\\end{document}</tex-math></inline-formula> is defined as:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e1068\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta ^{(i)}_L = \\textrm{Train}(D^{(i)}_L, ep, \\theta _G), \\end{aligned}$$\\end{document}</tex-math></disp-formula>Let <inline-formula id=\"IEq17\"><tex-math id=\"d33e1073\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _G$$\\end{document}</tex-math></inline-formula> denote the global model parameters and <inline-formula id=\"IEq18\"><tex-math id=\"d33e1077\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(i)}_L$$\\end{document}</tex-math></inline-formula> the updated local parameters from client <italic toggle=\"yes\">i</italic>. fter local training, each client sends <inline-formula id=\"IEq19\"><tex-math id=\"d33e1084\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(i)}_L$$\\end{document}</tex-math></inline-formula> to the central server, which updates the global model <inline-formula id=\"IEq20\"><tex-math id=\"d33e1089\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _G$$\\end{document}</tex-math></inline-formula> as follows:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e1093\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta ^{(r+1)}_G = \\frac{1}{N} \\sum _{i=1}^N \\theta ^{(i)}_L, \\end{aligned}$$\\end{document}</tex-math></disp-formula>Equation (<xref rid=\"Equ3\" ref-type=\"disp-formula\">3</xref>) represents the standard FedAvg (Federated Averaging) approach, where the global model <inline-formula id=\"IEq21\"><tex-math id=\"d33e1101\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(r+1)}_G$$\\end{document}</tex-math></inline-formula> at round <inline-formula id=\"IEq22\"><tex-math id=\"d33e1105\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(r+1)$$\\end{document}</tex-math></inline-formula> is computed as the arithmetic mean of the local model parameters <inline-formula id=\"IEq23\"><tex-math id=\"d33e1109\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(i)}_L$$\\end{document}</tex-math></inline-formula> from <italic toggle=\"yes\">N</italic> participating clients. This method assumes all local models are equally weighted and is efficient for homogeneous data distributions and unsecured settings. The generalized update function at round <inline-formula id=\"IEq24\"><tex-math id=\"d33e1117\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$r+1$$\\end{document}</tex-math></inline-formula> is given by:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e1121\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta ^{(r+1)}_G = A\\left( \\{\\theta ^{(1)}_L, \\theta ^{(2)}_L, \\ldots , \\theta ^{(N)}_L\\}\\right) , \\end{aligned}$$\\end{document}</tex-math></disp-formula>Equation (<xref rid=\"Equ4\" ref-type=\"disp-formula\">4</xref>), on the other hand, uses a broader and safer aggregation function <inline-formula id=\"IEq25\"><tex-math id=\"d33e1129\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A(\\cdot )$$\\end{document}</tex-math></inline-formula> that can include privacy-protecting methods like homomorphic encryption (HME). We suggest using HE schemes (like CKKS or BFV) for this aggregation in our proposed <italic toggle=\"yes\">MedShieldFL</italic> framework. This lets us do secure computations over encrypted parameters and keeps model updates safe from inference attacks. This abstraction makes it possible for strong adoption in privacy-sensitive areas like healthcare, where following the rules and keeping data safe are very important. These equations show the differences between unsafe and safe federated aggregation methods. This shows why Equation (<xref rid=\"Equ4\" ref-type=\"disp-formula\">4</xref>) should be used in secure IIoT medical settings. The final global model parameters <inline-formula id=\"IEq26\"><tex-math id=\"d33e1140\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(R)}_G$$\\end{document}</tex-math></inline-formula> that are found after <italic toggle=\"yes\">R</italic> rounds of refinement should be close to the centrally trained model parameters that are found from training on all datasets put together, or at least have similar performance metrics:<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e1147\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta ^{(R)}_G \\approx \\textrm{Train}\\left( \\bigcup _{i=1}^N D^{(i)}_L, ep, \\theta ^{(0)}_G\\right) , \\end{aligned}$$\\end{document}</tex-math></disp-formula><italic toggle=\"yes\">ep</italic> represents the total number of training epochs in a centralized scenario.</p></sec><sec id=\"Sec26\"><title>HME</title><p id=\"Par67\">HME is a strong method for protecting privacy and improving security that lets you do computations on encrypted data. As seen in Secure Multiparty Computation (SMC), it keeps private information safe from different threats while still letting it work without having to decrypt it or rely on trusted third-party servers. HE works great in cloud-edge-enabled situations like FL because it reduces privacy worries and makes working together safely easier without giving up control of the data. How safe asymmetric encryption methods like BFV and CKKS are depends on how hard the Ring Learning with Errors (RLWE) problem is. For these ways, public keys are used to encrypt and private keys are used to decrypt.</p></sec><sec id=\"Sec27\"><title>Cheon-Kim-Kim-Song (CKKS)</title><p id=\"Par68\">Levelled HE CKKS approximates complex number arithmetic. A low-encrypted data addition and multiplication for approximate results. C KS helps analyze or DL models on encrypted data, or aggregate encrypted model parameters for globally encrypted<list list-type=\"bullet\"><list-item><p id=\"Par69\"><bold>Supported Operations:</bold> For ciphertexts <inline-formula id=\"IEq27\"><tex-math id=\"d33e1168\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct_1, ct_2 \\in R_q$$\\end{document}</tex-math></inline-formula>, CKKS supports the following operations: <disp-formula id=\"Equ18\"><tex-math id=\"d33e1172\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct_{add} = ct_1 \\oplus ct_2, \\quad ct_{mul} = ct_1 \\otimes ct_2$$\\end{document}</tex-math></disp-formula> where <inline-formula id=\"IEq28\"><tex-math id=\"d33e1176\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_q = \\mathbb {Z}[X]/(X^{\\xi } + 1)$$\\end{document}</tex-math></inline-formula> is the polynomial ring modulo <italic toggle=\"yes\">q</italic>.</p></list-item><list-item><p id=\"Par70\"><bold>Encryption:</bold> The encryption of plaintext <inline-formula id=\"IEq29\"><tex-math id=\"d33e1188\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$pt \\in \\mathbb {C}$$\\end{document}</tex-math></inline-formula> is defined as: <disp-formula id=\"Equ19\"><tex-math id=\"d33e1192\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct = \\textrm{Enc}(\\textrm{Pubk}, pt) = (a \\cdot \\nu , p \\cdot \\nu + \\Delta pt + e) \\bmod q$$\\end{document}</tex-math></disp-formula> where <inline-formula id=\"IEq30\"><tex-math id=\"d33e1196\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\nu , e$$\\end{document}</tex-math></inline-formula> are random polynomials and <inline-formula id=\"IEq31\"><tex-math id=\"d33e1200\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Delta$$\\end{document}</tex-math></inline-formula> is the scaling factor.In CKKS, p denotes the plaintext modulus and s denotes the secret key length, which processes the encryption process&#8217;s noise budget and security level.</p></list-item><list-item><p id=\"Par71\"><bold>Decryption:</bold> Decrypting ciphertext <inline-formula id=\"IEq32\"><tex-math id=\"d33e1209\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct = (ct_0, ct_1)$$\\end{document}</tex-math></inline-formula>: <disp-formula id=\"Equ20\"><tex-math id=\"d33e1213\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$pt = \\frac{1}{\\Delta } \\cdot (s \\cdot ct_1 + ct_0) \\bmod q$$\\end{document}</tex-math></disp-formula></p></list-item></list></p></sec><sec id=\"Sec28\"><title>Brakerski-Fan-Vercauteren (BFV)</title><p id=\"Par72\">The BFV scheme is a lattice-based homomorphic encryption method that supports exact integer arithmetic, making it suitable for privacy-preserving machine learning with modular operations.<list list-type=\"bullet\"><list-item><p id=\"Par73\"><bold>Plaintext and Ciphertext Space:</bold>\n<inline-formula id=\"IEq33\"><tex-math id=\"d33e1226\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathbb {Z}_t[x]/(x^N + 1)$$\\end{document}</tex-math></inline-formula> for plaintexts and <inline-formula id=\"IEq34\"><tex-math id=\"d33e1230\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R_q = \\mathbb {Z}_q[x]/(x^N + 1)$$\\end{document}</tex-math></inline-formula> for ciphertexts, where <italic toggle=\"yes\">t</italic> and <italic toggle=\"yes\">q</italic> are the plaintext and ciphertext moduli, respectively.</p></list-item><list-item><p id=\"Par74\"><bold>Encryption:</bold> For <inline-formula id=\"IEq35\"><tex-math id=\"d33e1245\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m(x) \\in \\mathbb {Z}_t[x]/(x^N + 1)$$\\end{document}</tex-math></inline-formula> and public key <inline-formula id=\"IEq36\"><tex-math id=\"d33e1249\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(pk_0, pk_1)$$\\end{document}</tex-math></inline-formula>: <disp-formula id=\"Equ21\"><tex-math id=\"d33e1253\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct = (ct_0, ct_1) = (pk_0 \\cdot u + e_1 + \\Delta m, \\; pk_1 \\cdot u + e_2),$$\\end{document}</tex-math></disp-formula> with small random polynomials <inline-formula id=\"IEq37\"><tex-math id=\"d33e1257\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$u, e_1, e_2$$\\end{document}</tex-math></inline-formula> and scaling factor <inline-formula id=\"IEq38\"><tex-math id=\"d33e1261\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Delta = \\lfloor q/t \\rfloor$$\\end{document}</tex-math></inline-formula>.</p></list-item><list-item><p id=\"Par75\"><bold>Decryption:</bold> Using secret key <italic toggle=\"yes\">s</italic>: <disp-formula id=\"Equ22\"><tex-math id=\"d33e1273\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m = \\bigg \\lfloor \\frac{t}{q} \\cdot (ct_0 + ct_1 \\cdot s \\bmod q) \\bigg \\rceil \\bmod t$$\\end{document}</tex-math></disp-formula></p></list-item><list-item><p id=\"Par76\"><bold>Operations:</bold> Supports addition and multiplication over encrypted data: <disp-formula id=\"Equ23\"><tex-math id=\"d33e1281\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ct_{\\text {add}} = (ct_1 + ct_2) \\bmod q, \\quad ct_{\\text {mul}} = (ct_1 \\cdot ct_2) \\bmod q$$\\end{document}</tex-math></disp-formula> Relinearization and rescaling control ciphertext growth.</p></list-item></list></p><p id=\"Par77\">In the BFV method, <italic toggle=\"yes\">t</italic> (plaintext modulus), <italic toggle=\"yes\">q</italic> (ciphertext modulus), and <italic toggle=\"yes\">N</italic> (polynomial degree) set the noise budget, security, and efficiency. This makes sure that the computation is correct and private.</p></sec></sec><sec id=\"Sec29\"><title>Algorithm workflow</title><p id=\"Par78\">The suggested method&#8217;s workflow is shown in Algorithm <xref rid=\"Figa\" ref-type=\"fig\">1</xref>. I have three basic steps: (1) Local Model Training, (2) Secure Encrypted Results Aggregation, and (3) Global Model Generation. We protect privacy and security with modern cryptography. For instance, we compare tBFV and CKCschemes and adjust the thmodel&#8217;s encryption settings.<fig id=\"Figa\" position=\"float\" orientation=\"portrait\"><label>Algorithm 1</label><caption><p>Secure Collaborative FL Based on HE.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Figa_HTML.jpg\"/></fig></p><p id=\"Par79\"> Data owners and servers follow an agreed-upon protocol, but they might try to figure out more than what is clearly authorised. To make things easier, we think of these servers as edge servers. The central server, called <inline-formula id=\"IEq39\"><tex-math id=\"d33e1312\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_p$$\\end{document}</tex-math></inline-formula>, is responsible for setting up the global model parameters <inline-formula id=\"IEq40\"><tex-math id=\"d33e1316\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _G$$\\end{document}</tex-math></inline-formula> and making the cryptographic key pair.</p><p id=\"Par80\">We set the global model parameters <inline-formula id=\"IEq41\"><tex-math id=\"d33e1322\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _G$$\\end{document}</tex-math></inline-formula> by randomly giving them values from a uniform distribution <inline-formula id=\"IEq42\"><tex-math id=\"d33e1326\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U(-\\zeta , \\zeta )$$\\end{document}</tex-math></inline-formula>, where <inline-formula id=\"IEq43\"><tex-math id=\"d33e1330\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\zeta&gt; 0$$\\end{document}</tex-math></inline-formula> is the initialisation bound:<disp-formula id=\"Equ24\"><tex-math id=\"d33e1334\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(0)}_G \\sim \\mathscr {U}(-\\zeta , \\zeta )$$\\end{document}</tex-math></disp-formula>Where <inline-formula id=\"IEq44\"><tex-math id=\"d33e1338\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(0)}_G$$\\end{document}</tex-math></inline-formula> denotes the initial global model parameters required for training the DL model (ResNet-18).</p><p id=\"Par81\">In addition to parameter initialisation, the server generates a public key <inline-formula id=\"IEq45\"><tex-math id=\"d33e1344\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Pubk}$$\\end{document}</tex-math></inline-formula> for encryption and a private key <inline-formula id=\"IEq46\"><tex-math id=\"d33e1348\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Privk}$$\\end{document}</tex-math></inline-formula> for decryption using the key generation function <inline-formula id=\"IEq47\"><tex-math id=\"d33e1352\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{KeyGen}(\\lambda )$$\\end{document}</tex-math></inline-formula>, where <inline-formula id=\"IEq48\"><tex-math id=\"d33e1356\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\lambda$$\\end{document}</tex-math></inline-formula> is the encryption strength security parameter.<disp-formula id=\"Equ25\"><tex-math id=\"d33e1360\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(\\textrm{Pubk}, \\textrm{Privk}) = \\textrm{KeyGen}(\\lambda )$$\\end{document}</tex-math></disp-formula>The <inline-formula id=\"IEq49\"><tex-math id=\"d33e1365\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Privk}$$\\end{document}</tex-math></inline-formula> must be stored securely on the server, while the public key <inline-formula id=\"IEq50\"><tex-math id=\"d33e1369\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Pubk}$$\\end{document}</tex-math></inline-formula> and initial global parameters <inline-formula id=\"IEq51\"><tex-math id=\"d33e1373\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(0)}_G$$\\end{document}</tex-math></inline-formula> are distributed securely to all participants <inline-formula id=\"IEq52\"><tex-math id=\"d33e1377\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n \\in \\{1, \\ldots , N\\}$$\\end{document}</tex-math></inline-formula>:<disp-formula id=\"Equ26\"><tex-math id=\"d33e1381\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C(S_p \\rightarrow H_i) = \\{\\textrm{Pubk}, \\theta ^{(0)}_G\\}, \\quad \\forall i \\in \\{1, \\ldots , N\\},$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq53\"><tex-math id=\"d33e1385\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C(S_p \\rightarrow H_i)$$\\end{document}</tex-math></inline-formula> denotes secure transmission from server <inline-formula id=\"IEq54\"><tex-math id=\"d33e1390\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_p$$\\end{document}</tex-math></inline-formula> to participant <inline-formula id=\"IEq55\"><tex-math id=\"d33e1394\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_i$$\\end{document}</tex-math></inline-formula>.</p><sec id=\"Sec30\"><title>Stage 1: Local model training</title><p id=\"Par82\">Each active hospital <inline-formula id=\"IEq56\"><tex-math id=\"d33e1402\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_m$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq57\"><tex-math id=\"d33e1406\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m \\in \\{1, \\ldots , M\\}$$\\end{document}</tex-math></inline-formula>, trains a local ResNet-18 model on its private dataset <inline-formula id=\"IEq58\"><tex-math id=\"d33e1410\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D^{(m)}_L$$\\end{document}</tex-math></inline-formula> for <italic toggle=\"yes\">ep</italic> epochs (e.g., <inline-formula id=\"IEq59\"><tex-math id=\"d33e1417\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ep = 7$$\\end{document}</tex-math></inline-formula>). The set of hospitals selected for the current training round is denoted by:<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e1422\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathscr {H} = \\bigcup _{i=1}^M H_i, \\quad H_i = \\{D^{(i)}_L, \\textrm{Pubk}\\}, \\end{aligned}$$\\end{document}</tex-math></disp-formula>where each client <inline-formula id=\"IEq60\"><tex-math id=\"d33e1427\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_i$$\\end{document}</tex-math></inline-formula> owns a private dataset <inline-formula id=\"IEq61\"><tex-math id=\"d33e1431\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D^{(i)}_L$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par83\">The local training objective at client <inline-formula id=\"IEq62\"><tex-math id=\"d33e1437\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_i$$\\end{document}</tex-math></inline-formula> minimises a local loss function <inline-formula id=\"IEq63\"><tex-math id=\"d33e1441\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Loss}_i(\\cdot )$$\\end{document}</tex-math></inline-formula>:<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e1445\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textrm{Loss}_i(\\theta ) = \\frac{1}{|H_i|} \\sum _{j=1}^{|H_i|} \\ell (f_\\theta (x_j), y_j), \\end{aligned}$$\\end{document}</tex-math></disp-formula>Here, <inline-formula id=\"IEq64\"><tex-math id=\"d33e1450\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$|H_i|$$\\end{document}</tex-math></inline-formula> denotes the number of samples in <inline-formula id=\"IEq65\"><tex-math id=\"d33e1454\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D^{(i)}_L$$\\end{document}</tex-math></inline-formula>, and <inline-formula id=\"IEq66\"><tex-math id=\"d33e1459\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\ell (f_\\theta (x), y)$$\\end{document}</tex-math></inline-formula> represents the loss between the model prediction and the actual label.</p><p id=\"Par84\">The result is a plaintext local model <inline-formula id=\"IEq67\"><tex-math id=\"d33e1465\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M^{(m)}_L$$\\end{document}</tex-math></inline-formula> with <italic toggle=\"yes\">u</italic> layers. E ch lay r <inline-formula id=\"IEq68\"><tex-math id=\"d33e1472\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_r(i)$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq69\"><tex-math id=\"d33e1476\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i \\in \\{1, \\ldots , u\\}$$\\end{document}</tex-math></inline-formula>, contains <italic toggle=\"yes\">s</italic> trainable parameters indexed by <inline-formula id=\"IEq70\"><tex-math id=\"d33e1484\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j \\in \\{1, \\ldots , s\\}$$\\end{document}</tex-math></inline-formula>:<disp-formula id=\"Equ27\"><tex-math id=\"d33e1488\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_r(i) = \\{\\theta ^{(1)}_{L_r}, \\theta ^{(2)}_{L_r}, \\ldots , \\theta ^{(s)}_{L_r}\\}.$$\\end{document}</tex-math></disp-formula>For simplicity, the local model parameters are denoted <inline-formula id=\"IEq71\"><tex-math id=\"d33e1492\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(m)}_L$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par85\">In the selected homomorphic encryption (HME) scheme (BFV or CKKS), each hospital encrypts its local model parameters <inline-formula id=\"IEq72\"><tex-math id=\"d33e1498\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta ^{(m)}_L$$\\end{document}</tex-math></inline-formula> using the public key <inline-formula id=\"IEq73\"><tex-math id=\"d33e1502\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{PubK}$$\\end{document}</tex-math></inline-formula> to ensure privacy.<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e1506\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textrm{Enc}\\, \\theta ^{(m)}_L = \\textrm{Encrypt}(\\textrm{Pubk}, \\theta ^{(m)}_L), \\end{aligned}$$\\end{document}</tex-math></disp-formula>where the encryption is applied layer-wise after flattening parameters into a 1D array.</p></sec><sec id=\"Sec31\"><title>Stage 2: Secure encrypted aggregation</title><p id=\"Par86\">The central server <inline-formula id=\"IEq74\"><tex-math id=\"d33e1515\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_c$$\\end{document}</tex-math></inline-formula> receives encrypted local parameters <inline-formula id=\"IEq75\"><tex-math id=\"d33e1519\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Enc}\\, \\theta ^{(1)}_L, \\ldots , \\textrm{Enc}\\, \\theta ^{(M)}_L$$\\end{document}</tex-math></inline-formula> and securely aggregates them using the additive homomorphic property:<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e1523\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} C_G = \\textrm{EvalAdd}(C_1, C_2, \\ldots , C_M), \\end{aligned}$$\\end{document}</tex-math></disp-formula>where<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e1528\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} C_G = \\sum _{m=1}^M C_m = \\sum _{m=1}^M \\textrm{Enc}(\\textrm{Pubk}, \\theta ^{(m)}_L), \\end{aligned}$$\\end{document}</tex-math></disp-formula>and <inline-formula id=\"IEq76\"><tex-math id=\"d33e1533\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_m$$\\end{document}</tex-math></inline-formula> denotes the ciphertext from the client <inline-formula id=\"IEq77\"><tex-math id=\"d33e1538\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_m$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par87\">The homomorphic addition <inline-formula id=\"IEq78\"><tex-math id=\"d33e1544\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{EvalAdd}(\\cdot )$$\\end{document}</tex-math></inline-formula> operates directly on ciphertexts.TheT eThegregation satisfies:<disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e1548\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} C_G = \\textrm{Enc}\\left( \\textrm{Pubk}, \\sum _{m=1}^M \\theta ^{(m)}_L \\right) , \\end{aligned}$$\\end{document}</tex-math></disp-formula>so that decryption yields the sum of plaintexts:<disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e1553\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textrm{Dec}(\\textrm{Privk}, C_G) = \\sum _{m=1}^M \\theta ^{(m)}_L. \\end{aligned}$$\\end{document}</tex-math></disp-formula>The central server hides hospital identities to ensure anonymity. The aggregated ciphertext <inline-formula id=\"IEq79\"><tex-math id=\"d33e1558\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_G$$\\end{document}</tex-math></inline-formula> is forwarded to <inline-formula id=\"IEq80\"><tex-math id=\"d33e1562\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_p$$\\end{document}</tex-math></inline-formula> for further processing.</p></sec><sec id=\"Sec32\"><title>Stage 3: Global model generation</title><p id=\"Par88\">The primary server <inline-formula id=\"IEq81\"><tex-math id=\"d33e1570\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_p$$\\end{document}</tex-math></inline-formula> decrypts the aggregated ciphertext using the private key:<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e1574\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\textrm{Dec}\\, \\theta _G = \\textrm{Decrypt}(\\textrm{Privk}, C_G), \\end{aligned}$$\\end{document}</tex-math></disp-formula>Where the division to compute the average model parameters is performed on the decrypted data:<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e1579\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta _G = \\frac{\\textrm{Dec}\\, \\theta _G}{M}, \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <italic toggle=\"yes\">M</italic> is the number of active hospitals.</p><p id=\"Par89\">The global model <inline-formula id=\"IEq82\"><tex-math id=\"d33e1589\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M_G$$\\end{document}</tex-math></inline-formula> is updated, <inline-formula id=\"IEq83\"><tex-math id=\"d33e1593\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta _G$$\\end{document}</tex-math></inline-formula>, and the updated parameters are distributed back to participants for the next round:<disp-formula id=\"Equ15\"><label>15</label><tex-math id=\"d33e1597\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M^{(n)}_L \\leftarrow \\textrm{Update}(M^{(n)}_L, \\theta _G), \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq84\"><tex-math id=\"d33e1602\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Update}(\\cdot )$$\\end{document}</tex-math></inline-formula> integrates global parameters into the local model.</p><p id=\"Par90\">This process iterates for <italic toggle=\"yes\">R</italic> communication rounds (e.g., <inline-formula id=\"IEq85\"><tex-math id=\"d33e1611\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$R = 17$$\\end{document}</tex-math></inline-formula>), progressively refining the global model. After <italic toggle=\"yes\">R</italic> rounds, the final aggregated parameters are defined as:<disp-formula id=\"Equ16\"><label>16</label><tex-math id=\"d33e1618\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\theta _G^{(R)} = A^{(R)}\\left( \\{\\theta _L^{(i)}(r)\\}_{i=1}^M\\right) , \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq86\"><tex-math id=\"d33e1623\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$A^{(R)}$$\\end{document}</tex-math></inline-formula> represents the iterative aggregation function applied across all rounds.</p><p id=\"Par92\">Algorithm&#160;<xref rid=\"Figb\" ref-type=\"fig\">2</xref> performs secure and privacy-preserving aggregation of federated model parameters using homomorphic encryption within the FL framework.<fig id=\"Figb\" position=\"float\" orientation=\"portrait\"><label>Algorithm 2</label><caption><p>Privacy-Preserving Local Training with Federated Encryption.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Figb_HTML.jpg\"/></fig><fig position=\"anchor\" id=\"Figc\" orientation=\"portrait\"><label>Algorithm 3</label><caption><p>Secure Aggregation of Federated Model Parameters.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"MO7\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Figc_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec33\"><title>Algorithm efficiency and deployment adaptability</title><p id=\"Par94\">The <bold>MedShieldFL</bold> structure works very well and can be easily changed to fit real-life healthcare situations. To see how it stacked up against FedAvg, FL+DP, FL+HE, and FL+GAN models in terms of training time, inference speed, GPU usage, and FLOPs, it was put to the test. Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> represents Performance Profiling of MedShieldFL vs. Baselines.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Performance Profiling of MedShieldFL vs. Baselines.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train (s)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Infer (ms)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">GPU (GB)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FLOPs (G)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedAvg<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">12.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">4.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL+DP<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">14.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">4.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">19.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL+HE<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">23.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">6.9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">28.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL+GAN<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">19.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">24.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>MedShieldFL</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>26.3</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>7.4</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>3.9</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>31.2</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par95\">MedShieldFL has a slightly longer runtime because it has more privacy layers, but it is still useful for edge and fog applications because it improves privacy, accuracy, and robustness.</p><sec id=\"Sec34\"><title>Adaptability across scenarios</title><p id=\"Par96\">MedShieldFL works the same way on different brain tumour datasets (<bold>BraTS</bold>, <bold>BT-RIC</bold>, <bold>TCGA</bold>) even when the conditions are different and noisy. It can be changed because: <list list-type=\"order\"><list-item><p id=\"Par97\"><bold>Federated Architecture:</bold> Enables decentralized, privacy-preserving learning.</p></list-item><list-item><p id=\"Par98\"><bold>GAN-Based Augmentation:</bold> Enhances generalization on limited data.</p></list-item><list-item><p id=\"Par99\"><bold>Homomorphic Encryption:</bold> Ensures secure computation without loss of utility.</p></list-item></list></p><p id=\"Par100\">Overall, MedShieldFL strikes a good mix between speed, safety, and usability, which makes it a good choice for large-scale healthcare applications that need to protect privacy.</p></sec></sec></sec><sec id=\"Sec35\"><title>Experiments and result analysis</title><p id=\"Par101\">The experimental setup, data preprocessing, model training, and evaluation of the proposed federated learning architecture are described here.</p><sec id=\"Sec36\"><title>Environment setup and dataset</title><p id=\"Par102\">All tests were performed on a secure, high-performance virtual PC running Ubuntu 20.04 LTS. It required an Intel Xeon(R) CPU E5-2686 v4 at 2.30 GHz, 128 GB of RAM, and a 16 GB NVIDIA V100 GPU. All Python 3.8.10 and PyTorch implementations enhanced model building, memory management, and debugging. We used the TenSEAL package to do computations on encrypted tensors while ensuring our privacy with CKKS homomorphic encryption. Because it is built on Microsoft SEAL, TenSEAL lets you add, subtract, and multiply both plaintext and encrypted tensor values. It was decided to use the ckks_vector data type instead of ckks_tensor because it needs less memory. There was a polynomial modulus degree of 16,384 and a global scale of <inline-formula id=\"IEq87\"><tex-math id=\"d33e1814\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2^{60}$$\\end{document}</tex-math></inline-formula> set for the encryption settings. For more integer math operations, we used BFV homomorphic encryption with standard setup steps and a larger input modulus for bigger calculations. The batch we encrypted had a degree of 4,096, a coefficient of 4,096, and a plaintext of 1,964,769.281. We used PolyCRTBuilder to do it. These settings find the best mix between how fast the computer works and how strong the encryption is. This makes sure that the federated training pipeline is safe and useful. The sample used in this study was made up of 3,064 T1-weighted contrast-enhanced MRI images with a resolution of 512 <inline-formula id=\"IEq88\"><tex-math id=\"d33e1821\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 512 pixels. There are three types of cancers in the collection: meningioma (708 photos), glioma (1,426 photos), and pituitary (930 photos). The kind of cancer that every study wants to find is this one. The clear description tells the difference between the &#8220;grade&#8221; and &#8220;type&#8221; of a cancer, making sure that the language stays the same.</p><p id=\"Par103\">To even out the classes and make the model more flexible, we used DCGAN-generated fake data. With a learning rate of 0.0002, a discriminator dropout rate of between 25% and 50%, greyscale input channels, and a batch size of 32 over 400 epochs, the Adam optimiser trained the DCGAN. All the pictures were shrunk down to <inline-formula id=\"IEq89\"><tex-math id=\"d33e1827\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$64 \\times 64$$\\end{document}</tex-math></inline-formula> pixels and made more even by setting the mean to 0.5 and the standard deviation to 0.5. For each type of tumour, a separate DCGAN model was trained. This made 3,000 high-quality fake pictures that looked a lot like the real data while protecting the patients&#8217; privacy. The finished dataset for federated training and validation had a total of 6,064 samples: 3,064 real images and 3,000 fake images. We trained with 80% of the data and tested with 20%. To test how well the generalisation worked, extra external datasets like BraTS, BT-RIC, and TCGA were only used for testing and never in the training process. To make sure that the results can be repeated, exact train-validation-test splits, dataset access links, and sample distributions for each client are given. Table&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref> shows a full breakdown of the datasets&#8217; origins, distributions, and parts in the experiments. This makes the problem statement more clear and consistent.<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Dataset Summary and Experimental Usage.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Source</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Label</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Imgs</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Usage</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Clients</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Access</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Real T1-CE MRI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">M/G/P</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3,064</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Train &amp; Val</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3 sites (<inline-formula id=\"IEq90\"><tex-math id=\"d33e1873\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sim$$\\end{document}</tex-math></inline-formula>1,020 ea)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Internal</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Synthetic DCGAN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">M/G/P</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3,000</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Augmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,000 per class</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Generated</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BraTS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mixed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,572</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">External Test</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8212;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Public</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BT-RIC</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mixed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">824</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">External Test</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8212;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Public</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">TCGA</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mixed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,015</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">External Test</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8212;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Public</td></tr></tbody></table></table-wrap></p><p id=\"Par105\">To train and test the model, 3064 T1-weighted contrast-enhanced MRI scans of real brain tumours were used. These scans have a resolution of 512x512. The three types of tumors included in the dataset are meningioma (708 photos), glioma (1,426 images), and pituitary (930 images). The complexity of the data set necessitated a deep model structure that maintained computing efficiency and security; this was a challenge when selecting ResNet-18 and configuring its associated hyperparameters.</p><sec id=\"Sec37\"><title>Preprocessing and synthetic image generation</title><p id=\"Par106\">We used Deep Convolutional Generative Adversarial Network (DCGAN) data augmentation to make the classes more evenly distributed and the model more general. To accurately capture the original data distribution while ensuring privacy and variety, multiple DCGAN models were developed for meningioma, glioma, and pituitary tumours. We trained the DCGAN using stable GAN training methods and the Adam optimiser, with a learning rate of 0.0002. The discriminator network had a dropout rate of 25% to 50% to prevent overfitting and improve adversarial learning stability. All of the images were converted to black and white and reduced to 64x64 pixels. We normalised the pictures during preprocessing by setting the mean to <inline-formula id=\"IEq91\"><tex-math id=\"d33e1937\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu = 0.5$$\\end{document}</tex-math></inline-formula> and the standard deviation to <inline-formula id=\"IEq92\"><tex-math id=\"d33e1941\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma = 0.5$$\\end{document}</tex-math></inline-formula>. The training batch size was 32, and each model received 400 epochs to ensure that it converged and produced better synthetic results. This method created the initial 3,&#160;064 T1-weighted MRI scans, a 3,&#160;000 GAN-generated picture dataset, and a composite dataset with 6,&#160;064 samples. We allocated 80% of the dataset for training and 20% for testing. In the federated training system, synthetic augmentation reduced class imbalance and improved client categorisation. Tablereftab:dataset_summary provides a comprehensive analysis of the dataset&#8217;s composition following augmentation. This is consistent with the conventional technique for classifying tumours in the updated report.</p></sec></sec><sec id=\"Sec38\"><title>GAN augmentation methodology and validation</title><p id=\"Par107\">We talk about the most important parts of our GAN-based enhancement method. So, we&#8217;ve made big changes to our process to make sure that image dependability stays high. DCGANs were trained at <inline-formula id=\"IEq93\"><tex-math id=\"d33e1949\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$64\\times 64$$\\end{document}</tex-math></inline-formula> and later upsampled to <inline-formula id=\"IEq94\"><tex-math id=\"d33e1953\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224$$\\end{document}</tex-math></inline-formula>. This could result in artefacts at low frequencies and biases specific to certain classes. To mitigate this, we now employ <bold>StyleGAN-2</bold> at the native <inline-formula id=\"IEq95\"><tex-math id=\"d33e1960\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224\\times 224$$\\end{document}</tex-math></inline-formula> resolution, preserving fine anatomical structures and reducing artifact risk. Synthetic images are rigorously validated using <bold>FID, KID, and precision-recall metrics</bold> to assess fidelity and diversity, complemented by <bold>nearest-neighbor analyses</bold> to ensure the GAN does not memorize real scans (see Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>). <bold>Membership-inference tests</bold> further confirm privacy preservation. To avoid inflated performance, all <bold>test sets are strictly real and institution-held-out</bold>, while training sets include synthetic images to augment classifier performance. We also conducted <bold>ablation studies</bold> varying synthetic-to-real ratios per client, reporting per-class sensitivity, specificity, and overall accuracy (see Table <xref rid=\"Tab5\" ref-type=\"table\">5</xref>), which quantify the contribution of synthetic data. Finally, MRI augmentations are constrained to <bold>anatomically valid transformations</bold>, ensuring horizontal flips and rotations are applied only when slice orientation is consistent, and elastic/intensity perturbations remain physiologically plausible. Collectively, these updates strengthen the fidelity, privacy, anatomical plausibility, and evaluation rigor of our GAN-based augmentation methodology for safety-critical imaging.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Synthetic Image Quality Metrics per Class (FID, KID, Precision, Nearest-Neighbor Analysis).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FID <inline-formula id=\"IEq96\"><tex-math id=\"d33e2008\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">KID <inline-formula id=\"IEq97\"><tex-math id=\"d33e2014\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision <inline-formula id=\"IEq98\"><tex-math id=\"d33e2020\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">NN Similarity <inline-formula id=\"IEq99\"><tex-math id=\"d33e2026\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula></th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Class A</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">12.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.014</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.03</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Class B</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">11.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.012</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.89</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.02</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Class C</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">13.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.016</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.04</td></tr></tbody></table></table-wrap><table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Ablation Study: Synthetic-to-Real Ratios per Client with Per-Class Sensitivity and Accuracy.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Synthetic Ratio</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class A Sensitivity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class B Sensitivity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class C Sensitivity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Overall Accuracy</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.82</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.79</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.81</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.80</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">25%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.82</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.84</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.83</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">50%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.86</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.86</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">75%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.86</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.87</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">100%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.89</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.88</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec39\"><title>Baseline model performance</title><p id=\"Par108\">We fine-tuned the ResNet-18 model by optimizing key hyperparameters, including batch size, learning rate, weight decay, and optimizer configuration. The training and testing batch sizes were set to 16 and 128, respectively. Training was performed using Stochastic Gradient Descent (SGD) with a learning rate of 0.001, momentum of 0.9, and weight decay of 0.0005. Figure&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref> illustrates the training and validation accuracy over 30 epochs across real, synthetic, and mixed brain tumor datasets. On the real dataset, the model started with 76% training accuracy and 59% validation accuracy, reaching peak values of 99% and 98% at epochs 27 and 22, respectively. By epoch 12 on the synthetic dataset, we had 98% training accuracy and 91% validation accuracy, indicating that we could converge soon. After the ninth epoch, both training and validation on the mixed dataset maintained accuracy levels above 97%. Each epoch took an average of 3 seconds for real and synthetic datasets (90 seconds) and 7 seconds for mixed datasets (210 seconds). During data preprocessing, we resized the photographs to 224x224, flipped them randomly, rotated them up to 90 degrees, cropped them in the middle, and normalised them (<inline-formula id=\"IEq100\"><tex-math id=\"d33e2149\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu = 0.5$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq101\"><tex-math id=\"d33e2153\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma = 0.5$$\\end{document}</tex-math></inline-formula>). We used scaling, cropping, and normalisation on the test data, but not flipping or rotating. Table <xref rid=\"Tab6\" ref-type=\"table\">6</xref> displays ResNet-18&#8217;s precision, recall, and accuracy for various datasets and tumour grades. This demonstrates that it can deal with data from a variety of sources.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Training and Validation Accuracy Comparison of ResNet-18 Across Real, Synthetic, and Combined Datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig5_HTML.jpg\"/></fig><table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Classification Metrics Across Datasets.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Grade</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall (RC)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision (PR)</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"4\" colspan=\"1\">Real</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.4%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.7%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.7%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.3%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.7%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.1%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">97.08%</td></tr><tr><td align=\"left\" rowspan=\"4\" colspan=\"1\">Synthetic</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">100%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.9%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.5%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.9%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.6%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.1%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">99.35%</td></tr><tr><td align=\"left\" rowspan=\"4\" colspan=\"1\">Real+Synthetic</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.3%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.8%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.2%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.4%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.4%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.9%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">98.37%</td></tr></tbody></table></table-wrap></p><p id=\"Par104\">\n<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>ResNet-18 model accuracy across federation rounds in diverse client settings without secure aggregation (Plain FL).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO9\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig6_HTML.jpg\"/></fig>\n<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Training and testing accuracy of the ResNet-18 model under CKKS-based secure federated aggregation across varied client settings.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO10\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig7_HTML.jpg\"/></fig>\n<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Training and testing accuracy of ResNet-18 using BFV-based secure aggregation over multiple federation rounds and client configurations.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO11\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig8_HTML.jpg\"/></fig>\n<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>Comparison of FL-based technique execution times with non-secure (Plain FL) and secure aggregation methods (CKKS and BFV), under varying client configurations and using both real (R) and augmented (RS) brain tumor datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO12\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27303_Fig9_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec40\"><title>Compared to current methods</title><p id=\"Par109\">The MedShieldFL architecture outperforms typical baseline methods used in federated and privacy-preserving learning systems. The following are Seline&#8217;s:<list list-type=\"bullet\"><list-item><p id=\"Par110\"><bold>Centralized Training (ResNet-18)</bold>: A non-federated setup using all data aggregated at a central server.</p></list-item><list-item><p id=\"Par111\"><bold>Vanilla FL (FedAvg)</bold>: Standard FL without privacy enhancements.</p></list-item><list-item><p id=\"Par112\"><bold>FedAvg + DP</bold>: FL with differential privacy.</p></list-item><list-item><p id=\"Par113\"><bold>FedSGD + HE</bold>: Federated stochastic gradient descent using homomorphic encryption (CKKS).</p></list-item></list></p><p id=\"Par114\">We evaluated each method using identical client configurations and data, reporting the classification accuracy, communication cost (in MB per round), model convergence (rounds to 95% accuracy), and privacy utility impact in Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>.<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Comparative Results Across Methods.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc. (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Comm. (MB)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Rounds</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Privacy</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Centralized<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedAvg<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">24</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Medium</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedAvg + DP<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">27</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedSGD + HE<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">42.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">26</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Very Low</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>MedShieldFL (Ours)</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>97.3</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>21.1</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>22</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>Very Low</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par115\">As shown in Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>, MedShieldFL outperforms or matches other methods in terms of accuracy while offering better convergence and strong privacy preservation with acceptable communication cost. These findings show that the proposed approach works in privacy-sensitive and resource-constrained healthcare settings.</p></sec><sec id=\"Sec41\"><title>Communication overhead and privacy analysis</title><p id=\"Par116\">We are highlighting the discrepancies in Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref> regarding communication and privacy metrics. We have revised our analysis to provide a precise, reproducible, and transparent account of what is transmitted per client per round, including data type, precision, batching, and empirical measurements over TLS.</p><sec id=\"Sec42\"><title>Communication metrics</title><p id=\"Par117\">The Table <xref rid=\"Tab8\" ref-type=\"table\">8</xref> compares MedShieldFL&#8217;s communication with each client to FedSGD + HE. MedShieldFL claims 21.1 MB per round, but this is simply sparse, masked gradient deltas from the participating layers, delivered with 16-bit accuracy and packed using PolyCRT batching, which mixes multiple parameters per ciphertext to make better use of bandwidth. This technique maintains security while moving significantly less data than full-model uploads (such as FedSGD + HE).<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Per-client per-round communication breakdown for secure FL methods.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Transmitted Data</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dtype/Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Packed/Batched</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Upload (MB)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Download (MB)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedSGD + HE</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Full gradients</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">32-bit float</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">No</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">42.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">42.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">MedShieldFL (Ours)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Sparse gradient deltas</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">16-bit</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">PolyCRT batch</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.1</td></tr></tbody></table></table-wrap></p><p id=\"Par118\">TLS evaluated all communication properties objectively over five seeds, using fluctuation bars to demonstrate stability. This study makes the identified communication overheads explicit and simple to replicate.</p></sec><sec id=\"Sec43\"><title>Privacy metrics</title><p id=\"Par119\">In Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>, the &#8220;Privacy&#8221; column now displays &#8220;Estimated Privacy Risk (<inline-formula id=\"IEq102\"><tex-math id=\"d33e2523\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula> better),&#8221; indicating that lower numbers represent better privacy protection. MedShieldFL and FedSGD + HE disguise customer donations via masking-based safe aggregation and homomorphic encryption. This prevents people from guessing what they contributed, even if they only partially engaged or collaborated with the aggregator. The new standards assess privacy risk based on these safeguards rather than privacy itself.</p></sec><sec id=\"Sec44\"><title>Generalization and non-IID considerations</title><p id=\"Par120\">We evaluated our assertions using client-level non-IID splits and cross-site validation (leave-one-hospital-out). The average accuracy, communication, and privacy assessments across these realistic divisions demonstrate durability and generalisability across a variety of clinical settings. <list list-type=\"order\"><list-item><p id=\"Par121\">Describe what data, accuracy, and packaging mean.</p></list-item><list-item><p id=\"Par122\">Providing real-time client on-wire byte counts for each round (upload/download).</p></list-item><list-item><p id=\"Par123\">Clarifying privacy concerns and improving analytics.</p></list-item><list-item><p id=\"Par124\">Consider the variations between multiple seeds and cross-site instances that are not IID.</p></list-item></list></p></sec></sec></sec><sec id=\"Sec45\"><title>Federated deep learning (FDL) evaluation</title><p id=\"Par125\">We evaluated MedShieldFL, our privacy-protecting federated learning system, in three key areas. <list list-type=\"order\"><list-item><p id=\"Par126\">Global model accuracy across different aggregation strategies.</p></list-item><list-item><p id=\"Par127\">Execution Time Impact with varying client numbers and aggregation schemes.</p></list-item><list-item><p id=\"Par128\">Security and privacy guarantees offered by the system.</p></list-item></list></p><p id=\"Par129\">We evaluated the ResNet-18 deep learning model with the same hyperparameters as the baseline model.</p><sec id=\"Sec46\"><title> Impact of aggregation techniques on model accuracy</title><p id=\"Par130\">Figures&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>, <xref rid=\"Fig7\" ref-type=\"fig\">7</xref>, and <xref rid=\"Fig8\" ref-type=\"fig\">8</xref> show how ResNet-18 performed for 17 federation cycles for 4/2, 6/3, 8/4, and 10/5 clients. The first statistic represents the overall number of clients, while the second represents the total number of models picked per round. In the 4/2 format, two clients take part in each round. In the 10/5 format, however, five clients take part. These setups mimic different levels of parallelism and diversity in federated learning, which is like real-life deployment situations where client access and participation change. The subplots at the top of each figure show how well the model did when it was trained on real medical image collections. The lower subplots show fake data made by DCGAN for each client address. The subplots at the top show accuracy rates based on real data, while the subplots at the bottom use fake data. These pictures show how different types of data sources can change how collaborative learning converges.<list list-type=\"bullet\"><list-item><p id=\"Par131\"><bold>Accuracy fluctuations and convergence:</bold> The first few rounds are inconsistent, but they improve over time. We trained all of the setups for 17 rounds of federation, with 5 to 7 local epochs every round.</p></list-item><list-item><p id=\"Par132\"><bold>Secure vs. Non-Secure FL:</bold> CKKS-based secure federated learning is equally accurate as non-secure federated learning. When utilising real data, the accuracy rises from 88% to 94%, whereas when using DCGAN synthetic data, it rises from 90% to 96%.</p></list-item><list-item><p id=\"Par133\"><bold>BFV Scheme:</bold> Federated learning with BFV encryption achieves the same accuracy as CKKS. The results of augmentation show that the stability is slightly improved, confirming that FL is safe with more training data.</p></list-item></list></p></sec><sec id=\"Sec47\"><title>Execution time analysis based on participants and aggregation methods</title><p id=\"Par134\">Figure <xref rid=\"Fig9\" ref-type=\"fig\">9</xref> depicts a line graph of Plain FL, BFV, and CKKS execution times as the number of customers increases. This strategy emphasises the tradeoffs between security and speed of execution. The hex-axis illustrates alternative client setups with more total and active clients (for example, from 4_2 to 10_5), and the y-axis indicates how long it takes to complete each training round in minutes. LainFL (FL_plain_R and FL_plain_RS) displays the lowest and most stable execution times, showing its efficiency in non-secure contexts. Introducing BFV encryption (FL_BFV_R and FL_BFV_RS) causes execution time to rise in a straight line. Homomorphic encryption and secure aggregation operations increase computational overhead but provide sufficient security for moderate applications. KKS-based schemes (FL_ckks_R and FL_ckks_RS) have a lot more overhead, and the time it takes to run them increases quickly when more clients join. KKS encryption is computationally intensive, particularly during the aggregation and encryption phases. Adding more data (RS variations) always adds more work to all techniques. Overall, the graph shows that Plain FL is the fastest, while safe FL utilising BFV or CKKS takes a lot of time, especially under CKKS. These results highlight the importance of balancing security, scalability, and efficiency in real-world deployments.</p></sec><sec id=\"Sec48\"><title>Security and privacy analysis</title><p id=\"Par135\">Client-server cryptography is used in the suggested MedShieldFL framework to protect the privacy of data. Ata localisation is very important for protecting privacy; we only send encrypted model parameters instead of sending private hospital records. A participating client encrypts its locally learnt model parameters before sending them to the Secure Aggregator to protect communication and sensitive data. The framework resists known privacy attacks. Model inversion attacks use model outputs to determine input data. These attacks are much less likely to work because the data is complicated and has many dimensions, and the training datasets are massive. Also, hyperparameter stealing techniques, which require extra data and knowledge of the model structure, are less likely to work when parameters are encrypted and the model is hard to understand. hsystem&#8217;s design also helps protect privacy. By using synthetic images and the deep structure of ResNet-18, we reduce the likelihood of inferring sensitive information. The safe aggregator also ensures that the central server can only decrypt the global model and can&#8217;t find or separate the contributions of each client. There are more than two people involved in the FL process, so no one institution can figure out what the other institutions&#8217; rules are. These strategies work well together to protect privacy while still allowing fast performance. This makes the framework a good choice for sensitive areas like medical imaging.</p></sec><sec id=\"Sec49\"><title>Key contributions beyond accuracy: privacy-preserving and scalable federated learning</title><p id=\"Par136\">What makes <italic toggle=\"yes\">MedShieldFL</italic> great is that it focusses on privacy, fairness, and scale for real-world healthcare AI. The most important efforts are:<list list-type=\"bullet\"><list-item><p id=\"Par137\"><bold>Strong Privacy with Minimal Accuracy Impact:</bold> Federated aggregation uses homomorphic encryption (CKKS and BFV) to make sure that model updates are encrypted with only a 1% decrease in accuracy.</p></list-item><list-item><p id=\"Par138\"><bold>Improved Data Balance and Fairness:</bold> GAN-based augmentation makes under-represented tumour classes stronger, increasing the memory of early-stage tumours (for example, Grade 1 from 94.4% to 97.3%) and the generalisability of the model as a whole.</p></list-item><list-item><p id=\"Par139\"><bold>Robustness Across Realistic FL Settings:</bold> The framework works the same way in hospitals with one to ten clients, even when there isn&#8217;t a lot of data or the data is spread out in a way that isn&#8217;t a normal distribution.</p></list-item><list-item><p id=\"Par140\"><bold>Operational Security and Scalability:</bold> We look at the extra time needed for encryption, make sure it can withstand common privacy attacks, and show that safe aggregation can be scaled up across multiple federation rounds.</p></list-item></list></p><p id=\"Par141\">MedShieldFL is the only one that uses homomorphic encryption, GAN-based augmentation, and ResNet-18 in a way that makes sense for medical imaging while protecting privacy.</p></sec></sec><sec id=\"Sec50\"><title>Comparative analysis and experiments</title><p id=\"Par142\">Our proposed MedShieldFL framework was tested extensively compared to a range of privacy-preserving FL approaches and state-of-the-art baseline models to demonstrate its benefits. This section evaluates classification performance, convergence speed, and privacy trade-offs using synthetic and real-world brain tumour datasets under federated conditions.</p><sec id=\"Sec51\"><title>Baseline models</title><p id=\"Par143\">We consider the following models for comparison with MedShieldFL: <list list-type=\"order\"><list-item><p id=\"Par144\"><bold>Centralised ResNet-18 (C-ResNet)</bold>: A conventional ResNet-18 model trained centrally on a combination of real and synthetic data.</p></list-item><list-item><p id=\"Par145\"><bold>FedAvg</bold>: A standard federated learning implementation using FedAvg for model aggregation without any privacy-preserving mechanism.</p></list-item><list-item><p id=\"Par146\"><bold>FedHealth</bold><sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>: A healthcare-specific FL framework that supports model personalization but lacks secure aggregation.</p></list-item><list-item><p id=\"Par147\"><bold>FeTS</bold><sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>: Originally designed for federated brain tumour segmentation; adapted here for classification tasks.</p></list-item><list-item><p id=\"Par148\"><bold>MedShieldFL (Proposed)</bold>: Our hybrid privacy-preserving FL framework that integrates DCGAN-based data augmentation, a ResNet-18 backbone, and homomorphic encryption (HME)-based secure aggregation.</p></list-item></list></p><p id=\"Par149\">We evaluate each model using the same test dataset containing a balanced mix of the three tumour grades. Our evaluation measures are accuracy, Precision, Recall, F1-score, and Convergence Epochs. Table <xref rid=\"Tab12\" ref-type=\"table\">12</xref> represents the Performance Comparison of MedShieldFL Against Baseline Models.</p></sec><sec id=\"Sec52\"><title>Rationale for including additional baselines and comparative evaluation</title><p id=\"Par150\">We need more baselines. We chose ResNet-18 because it is widely used in medical imaging, has a fast architecture, converges quickly in federated systems, and serves as an excellent baseline for MRI classification tasks. However, more complex systems have emerged that perform better in terms of representation and generalisation. To tackle this challenge, we did comparison testing on DenseNet-121 (2017), Vision Transformer (ViT, 2020), Swin Transformer (2021), and ConvNeXt (2022), which are significant in medical imaging and have pretrained weights for a fair evaluation. We measured AUC, accuracy, F1-score, and communication overhead per round for all models when they were retrained in uniform federated settings using the same client distribution, communication rounds, hyperparameters, and dataset partitions (70% training, 15% validation, and 15% testing). External validation assessed generalisation using the BraTS, BT-RIC, and TCGA datasets. According to Table <xref rid=\"Tab9\" ref-type=\"table\">9</xref>, ResNet-18 is a good lightweight model, but newer architectures, especially transformer-based models, fare better at accuracy and generalisation. Our MedShieldFL model outperforms the baseline models in various ways. This work addresses fundamental limitations in tumor-type-specific federated MRI analysis that result from privacy concerns, slow clinical acceptance, and insufficient granular datasets. Experiments show that MedShieldFL excels in accuracy, robustness, communication efficiency, and privacy preservation when using cutting-edge models (see to Table <xref rid=\"Tab9\" ref-type=\"table\">9</xref>).<table-wrap id=\"Tab9\" position=\"float\" orientation=\"portrait\"><label>Table 9</label><caption><p>Comparative Evaluation of Baseline Models and MedShieldFL Framework.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model (Year)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Top-1 Acc (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">AUC</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Comm (MB/round)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">External AUC (BraTS)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Notes</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet-18 (2015)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.872</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.904</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">8.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.892</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lightweight, stable</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet-121 (2017)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.889</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.918</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">12.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.905</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Better feature reuse</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Vision Transformer (2020)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.903</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.935</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.921</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Long-range modeling</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin Transformer (2021)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.911</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.941</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">19.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.927</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High performance, robust</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ConvNeXt (2022)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.914</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.943</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.930</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Modern CNN with good trade-offs</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed MedShieldFL (2025)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.936</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.962</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">16.9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.948</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Best overall with privacy</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec53\"><title>Quantitative results</title><p id=\"Par151\">Table&#160;<xref rid=\"Tab12\" ref-type=\"table\">12</xref> compares the MedShieldFL architecture with FedAvg, FedHealth, FeTS, and C-ResNet. Precision, accuracy, recall, F1 score, and convergence Epochs are all means of quantifying value. MedShieldFL consistently outperforms baseline models in terms of precision (98.37%), accuracy (98.37%), recall (98.38%), and F1 score (98.37%). EdShieldFL converges in 22 epochs, faster than any other approach tested. This improves learning efficiency. C-ResNet, which was trained centrally with full access to the data, performs admirably (97.81% accuracy), but not as well as MedShieldFL. FedAvg and FedHealth function well in federated learning models, however they lack advanced privacy and data augmentation features. eTS was initially designed for segmentation and later modified for classification. It performs better than FedHealth but not MedShieldFL. The hybrid MedShieldFL system performed successfully. It utilised ResNet-18 for robust classification, DCGAN-based augmentation for data diversity, and homomorphic encryption for secure aggregation. These enhancements result in a federated learning technique for brain tumour classification in scattered healthcare settings that performs well, can be applied in a variety of contexts, and preserves privacy.</p></sec><sec id=\"Sec54\"><title>Modern techniques for comparative evaluation</title><p id=\"Par152\">To determine how effectively the MedShieldFL framework performs, it is compared to centralised and federated approaches, such as changes in dataset utilisation (real, synthetic, mixed), privacy methods (HE, DP), and generative augmentation. Table <xref rid=\"Tab10\" ref-type=\"table\">10</xref> compares all baselines in terms of accuracy, training and inference costs, GPU usage, privacy, and IIoT integration.<table-wrap id=\"Tab10\" position=\"float\" orientation=\"portrait\"><label>Table 10</label><caption><p>Unified Comparative Evaluation of MedShieldFL vs. Baselines.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc. (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">G1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">G2</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">G3</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train/Infer Time</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Privacy &amp; IIoT Suitability</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Traditional ML (SVM, RF)<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 8&#8201;s/1.2ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low privacy; weak IIoT fit</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Centralized DL (CNN, U-Net)<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 28&#8201;s/2.4ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate privacy risk; requires central storage</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Centralized (Real)<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.08</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;/&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not privacy-preserving</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Centralized (Synthetic)<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.35</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">100.0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;/&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Unrealistic; lacks real data variability</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Centralized (Mixed)<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 28&#8201;s/2.4ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High privacy risk; high performance</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL (FedAvg)<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.72</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 32&#8201;s/2.1ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate privacy; deployable on edge</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL + DCGAN<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 50&#8201;s/2.8ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved generalization; GAN instability risk</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL + HE<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 45&#8201;s/2.6ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Strong privacy; reasonable cost</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FL + GAN<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 50&#8201;s/2.8ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">GAN-augmented; good for IIoT</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">MedShieldFL (Ours)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"> 54&#8201;s/2.5ms</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">HE + GAN; Excellent privacy, robustness, and IIoT deployment suitability</td></tr></tbody></table></table-wrap></p><p id=\"Par153\">Vector Machines and Random Forests are inaccurate and do not secure your privacy. Even though centralised deep learning algorithms are highly accurate (b17, b26, b27), they do not perform well in situations when privacy is critical. Federated Learning (FL) versions, such as FedAvg [b29], offer decentralisation but have limited performance. Generative Adversarial Networks (GANs) and Homomorphic Encryption (HE) make Federated Learning safer and more general, but at a cost in terms of computer power.The MedShieldFL solution combines HE, GANs, and FL to provide IIoT healthcare systems with the highest level of accuracy (98.37%), privacy, computation time, and deployability.</p><p id=\"Par154\">\n<bold>Key Takeaways:</bold>\n<list list-type=\"bullet\"><list-item><p id=\"Par155\"><bold>Accuracy:</bold> Comparable to centralized training and highest among FL approaches.</p></list-item><list-item><p id=\"Par156\"><bold>Generalization:</bold> GAN-based augmentation improves performance on minority tumour grades.</p></list-item><list-item><p id=\"Par157\"><bold>Security:</bold> Homomorphic Encryption preserves privacy with minimal accuracy loss.</p></list-item><list-item><p id=\"Par158\"><bold>Scalability:</bold> Robust to varying client availability in decentralized clinical settings.</p></list-item></list>\n</p></sec><sec id=\"Sec55\"><title>Experimental setup and dataset clarification</title><p id=\"Par159\">This section clarifies the datasets, task definition, and federated configuration used for evaluating MedShieldFL.</p><sec id=\"Sec56\"><title>Target task</title><p id=\"Par160\">The job is to sort brain tumours into three grades (G1, G2, and G3). Based on histology reports, labels are always mapped to the right places in all datasets.</p></sec><sec id=\"Sec57\"><title>Datasets and federated splits</title><p id=\"Par161\">Our experiments use both real-world and synthetic data sources:<list list-type=\"bullet\"><list-item><p id=\"Par162\"><bold>Real-world datasets:</bold> For joint testing and training, BraTS, BT-RIC, and TCGA are used.</p></list-item><list-item><p id=\"Par163\"><bold>Synthetic dataset:</bold> DCGAN images to improve variety and balance in the classes.</p></list-item></list></p><p id=\"Par164\">Table&#160;<xref rid=\"Tab11\" ref-type=\"table\">11</xref> summarises how the data was sent to each client during shared training.<table-wrap id=\"Tab11\" position=\"float\" orientation=\"portrait\"><label>Table 11</label><caption><p>The spread of datasets by client and the use of synthetic enhancement in federated training.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Client</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train Images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test Images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Synthetic Augmentation</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">C1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">TCGA</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">512</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">128</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">200</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">C2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">BraTS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">640</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">160</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">256</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">C3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">BT-RIC</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">480</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">120</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">192</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec58\"><title>Evaluation setup</title><p id=\"Par165\">The same set of three different types of tumours was used to test MedShieldFL and baseline models (Centralised ResNet-18, FedAvg, FedHealth, and FeTS). Accuracy, Precision, Recall, F1-score, and Convergence Epochs are some of the metrics used. MedShieldFL has better performance and faster convergence than all baselines, as shown in Table&#160;<xref rid=\"Tab12\" ref-type=\"table\">12</xref>. This is because it uses DCGAN enhancement and homomorphic encryption for safe aggregation.<table-wrap id=\"Tab12\" position=\"float\" orientation=\"portrait\"><label>Table 12</label><caption><p>A comparison of how well MedShieldFL works compared to standard models.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Convergence (Epochs)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">C-ResNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.65</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.81</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.24</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">24</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedAvg</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.32</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.78</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.89</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FedHealth</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.76</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.68</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.22</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.44</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">28</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FeTS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.91</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.04</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">26</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">MedShieldFL</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22</td></tr></tbody></table></table-wrap></p><p id=\"Par166\">This section fixes the problems that were found earlier by making the task, dataset origins, client splits, and FL setup very clear.</p></sec></sec><sec id=\"Sec59\"><title>Evaluation and insights</title><p id=\"Par167\">\n<list list-type=\"bullet\"><list-item><p id=\"Par168\"><bold>Improved Performance:</bold> By using both real and fake data, MedShieldFL gets better accuracy, precision, memory, and F1-score than all baselines.</p></list-item><list-item><p id=\"Par169\"><bold>Faster Convergence:</bold> GAN-assisted data enhancement speeds up convergence to 22 epochs, which is faster than other FL methods.</p></list-item><list-item><p id=\"Par170\"><bold>Privacy-Preserving Efficiency:</bold> Homomorphic encryption makes sure that aggregation is safe with little speed impact.</p></list-item><list-item><p id=\"Par171\"><bold>Robust and Balanced:</bold> FeTS has some problems when it comes to classification jobs, but synthetic DCGAN data makes things more stable.</p></list-item></list>\n</p><sec id=\"Sec60\"><title>Stability in long-term operation</title><p id=\"Par172\">Even when nodes are connected in different ways and conditions are different, MedShieldFL keeps its accuracy and convergence, showing that it can be trusted in real-world settings.</p></sec><sec id=\"Sec61\"><title>Case study</title><p id=\"Par173\">Through encrypted model aggregation, a simulated multi-institutional setting is able to achieve 97.3% accuracy while protecting data privacy.</p></sec><sec id=\"Sec62\"><title>Deployment and scalability</title><p id=\"Par174\">For larger networks, MedShieldFL offers scalable edge deployment based on Kubernetes and easy integration with existing hospital systems.</p></sec></sec></sec><sec id=\"Sec63\"><title>Conclusion</title><p id=\"Par175\">This study describes MedShieldFL, a hybrid federated learning architecture that protects privacy and is used for remote clinical multi-grade brain tumour classification. The method uses homomorphic encryption (CKKS/BFV) to keep model aggregation safe while also keeping personal patient information safe. It also uses a ResNet-18 classifier along with DCGAN-based data augmentation to fix issues with class imbalance and data shortage. MedShieldFL does better than standard methods (93% to 96%) in a range of client situations and converges faster because the data is more varied. For execution, the cost of computing and sending data is fair, even with privacy measures in place. Federated simulations that last longer show that the system stays stable even when client nodes join and hardware changes. The model works because it gets 97.3 percent of the answers right while protecting data sovereignty in a case study that includes many organisations. With MedShieldFL, you can get reliable, scalable, clinically useful, and private joint medical imaging training. It strikes the right mix between model performance, security, and deployment flexibility, which makes it perfect for sensitive medical tasks like diagnosing brain tumours. More study will be done to find ways to lower the cost of encryption, make 3D imaging better, and allow real-time clinical adaptation learning.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors thank all the colleagues and institutions that provided support and guidance during this study.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>Dileep Kumar Murala: Conceptualization, Methodology, Supervision, and Manuscript Review. G. Siva Krishna: Data Curation, Software Implementation, and Formal Analysis. Tanneeru Venkata Surya Kiran: Ex-perimental Design, Validation, and Writing Original Draft. Abdirahman Khalif Mohamud: Investigation, Resources, Writing Review&amp; Editing, and Project Administration.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>Three well-known, publicly available MRI image datasets were utilized in this study. These datasets collectively provide a comprehensive foundation for training and testing brain tumour detection models. Alternatively, the data can be obtained by contacting the corresponding author. 1. Figshare Dataset: The BrainTumorDataPublic_1766 collection from Figshare contains T1-weighted contrastenhanced MRI scans of meningioma, glioma, and pituitary tumours. Available at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://figshare.com/articles/dataset/brain_tumor_dataset/1512427\">https://figshare.com/articles/dataset/brain_tumor_dataset/1512427</ext-link> 2. Kaggle Dataset: The BrainTumorDataPublic_7671532 dataset from Kaggle includes MRI images labeled as glioma, meningioma, pituitary tumour, or no tumour. Available at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset?resource=download\">https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset?resource=download</ext-link> 3. Mendeley Dataset: The BrainTumorDataPublic_15332298 dataset from Mendeley provides MRI data representing four distinct types of brain tumours. Available at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://data.mendeley.com/datasets/w4sw3s9f59/1\">https://data.mendeley.com/datasets/w4sw3s9f59/1</ext-link></p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par176\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Wan</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>A Hierarchical Networking and Privacy-Preserving Federated Learning Framework for 5G Network</article-title><source>Journal of Communications and Information Networks</source><year>2025</year><volume>10</volume><issue>1</issue><fpage>26</fpage><lpage>36</lpage><pub-id pub-id-type=\"doi\">10.23919/JCIN.2025.10964101</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Guo, C. et al. A Hierarchical Networking and Privacy-Preserving Federated Learning Framework for 5G Network. <italic toggle=\"yes\">Journal of Communications and Information Networks</italic><bold>10</bold>(1), 26&#8211;36. 10.23919/JCIN.2025.10964101 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><mixed-citation publication-type=\"other\">Namakshenas, D., Yazdinejad, A., Dehghantanha, A., Parizi, R. M. &amp; Srivastava, G. P2FL: Interpretation-Based Privacy-Preserving Federated Learning for Industrial Cyber-Physical System. <italic toggle=\"yes\">IEEE Transactions on Industrial Cyber-Physical Systems</italic>. <bold>2</bold>, 321&#8211;330. 10.1109/TICPS.2024.3435178. (2024).</mixed-citation></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Sui</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Privacy-Preserving Federated Learning Framework for Multi-Source Electronic Health Records Prognosis Prediction</article-title><source>Sens. Res.</source><year>2025</year><volume>25</volume><fpage>2374</fpage><pub-id pub-id-type=\"doi\">10.3390/s25082374</pub-id><pub-id pub-id-type=\"pmcid\">PMC12031511</pub-id><pub-id pub-id-type=\"pmid\">40285064</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Zhao, H., Sui, D., Wang, Y., Ma, L. &amp; Wang, L. Privacy-Preserving Federated Learning Framework for Multi-Source Electronic Health Records Prognosis Prediction. <italic toggle=\"yes\">Sens. Res.</italic><bold>25</bold>, 2374. 10.3390/s25082374 (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s25082374</pub-id><pub-id pub-id-type=\"pmcid\">PMC12031511</pub-id><pub-id pub-id-type=\"pmid\">40285064</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Wang, Y., Wen, Z., Li, Y. &amp; Cao, B. Learn to Collaborate in MEC: An Adaptive Decentralized Federated Learning Framework. <italic toggle=\"yes\">IEEE Transactions on Mobile Computing</italic>. <bold>23</bold>(12), 14071&#8211;14084. 10.1109/TMC.2024.3439588 (2024).</mixed-citation></ref><ref id=\"CR5\"><label>5.</label><mixed-citation publication-type=\"other\">Chen, Y., Abrahamyan, L., Sahli, H. &amp; Deligiannis, N. Learned Parameter Compression for Efficient and Privacy-Preserving Federated Learning. <italic toggle=\"yes\">IEEE Open Journal of the Communications Society</italic><bold>5</bold>, 3503&#8211;3516. 10.1109/OJCOMS.2024.3409191 (2024).</mixed-citation></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Zhang, J. et al. RUPT-FL: Robust Two-Layered Privacy-Preserving Federated Learning Framework With Unlinkability for IoV. <italic toggle=\"yes\">IEEE Transactions on Vehicular Technology</italic>. <bold>74</bold> (4), 5528-5541. 10.1109/TVT.2024.3511255. (2025).</mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Hosain, M. T., Abir, M. R., Rahat, M. Y., Mridha, M. F. &amp; Mukta, S. H. Privacy Preserving Machine Learning With Federated Personalized Learning in Artificially Generated Environment. <italic toggle=\"yes\">IEEE Open Journal of the Computer Society</italic>. <bold>5</bold>, 694&#8211;704. 10.1109/OJCS.2024.3466859. (2024).</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><mixed-citation publication-type=\"other\">Wang, M., Zhou, L., Huang, X. &amp; Zheng, W. Towards Federated Learning Driving Technology for Privacy-Preserving Micro-Expre on Recognition. <italic toggle=\"yes\">Tsinghua Science and Technology</italic>. <bold>30</bold>(5), 2169&#8211;2183. 10.26599/TST.2024.9010098 (2025).</mixed-citation></ref><ref id=\"CR9\"><label>9.</label><mixed-citation publication-type=\"other\">Murala, D. K., Panda, S. K. &amp; Dash, S. P. MedMetaverse: Medical Care of Chronic Disease Patients and Managing Data Using Artificial Intelligence, Blockchain, and Wearable Devices State-of-the-Art Methodology. <italic toggle=\"yes\">IEEE Access</italic><bold>11</bold>, 138954&#8211;138985. 10.1109/ACCESS.2023.3340791. (2023).</mixed-citation></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Ragab, M. et al. Advancing artificial intelligence with a federated learning framework for privacy-preserving cyberthreat detection in IoT-assisted sustainable smart cities. <italic toggle=\"yes\">Sci Rep</italic>. <bold>15</bold>, 4470. 10.1038/s41598-025-88843-2 (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-88843-2</pub-id><pub-id pub-id-type=\"pmcid\">PMC11803112</pub-id><pub-id pub-id-type=\"pmid\">39915579</pub-id></mixed-citation></ref><ref id=\"CR11\"><label>11.</label><mixed-citation publication-type=\"other\">Gupta, A., Maurya, M. K., Dhere, K. &amp; Chaurasiya, V. K. Privacy-Preserving Hybrid Federated Learning Framework for Mental Healthcare Applications: Clustered and Quantum Approaches. <italic toggle=\"yes\">IEEE Access</italic><bold>12</bold>, 145054&#8211;145068.10.1109/ACCESS.2024.3464240. (2024).</mixed-citation></ref><ref id=\"CR12\"><label>12.</label><mixed-citation publication-type=\"other\">Yang, H. et al. Privacy-Preserving Federated Learning-Enabled Networks: Learning-Based Joint Scheduling and Resource Management. <italic toggle=\"yes\">IEEE Journal on Selected Areas in Communications</italic><bold>39</bold>(10), 3144&#8211;3159. 10.1109/JSAC.2021.3088655 (2021).</mixed-citation></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zeng</surname><given-names>H</given-names></name><etal/></person-group><article-title>BSR-FL: An Efficient Byzantine-Robust Privacy-Preserving Federated Learning Framework</article-title><source>IEEE Transactions on Computers</source><year>2024</year><volume>73</volume><issue>8</issue><fpage>2096</fpage><lpage>2110</lpage><pub-id pub-id-type=\"doi\">10.1109/TC.2024.3404102</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Zeng, H. et al. BSR-FL: An Efficient Byzantine-Robust Privacy-Preserving Federated Learning Framework. <italic toggle=\"yes\">IEEE Transactions on Computers</italic><bold>73</bold>(8), 2096&#8211;2110. 10.1109/TC.2024.3404102 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Darzi</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Dubost</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Sijtsema</surname><given-names>NM</given-names></name><name name-style=\"western\"><surname>van Ooijen</surname><given-names>PMA</given-names></name></person-group><article-title>Exploring Adversarial Attacks in Federated Learning for Medical Imaging</article-title><source>IEEE Transactions on Industrial Informatics</source><year>2024</year><volume>20</volume><issue>12</issue><fpage>13591</fpage><lpage>13599</lpage><pub-id pub-id-type=\"doi\">10.1109/TII.2024.3423457</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Darzi, E., Dubost, F., Sijtsema, N. M. &amp; van Ooijen, P. M. A. Exploring Adversarial Attacks in Federated Learning for Medical Imaging. <italic toggle=\"yes\">IEEE Transactions on Industrial Informatics</italic><bold>20</bold>(12), 13591&#8211;13599. 10.1109/TII.2024.3423457 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Zhou, H., Yang, G., Dai, H. &amp; Liu, G. FLF: Privacy-Preserving Federated Learning Framework for Edge Computing. <italic toggle=\"yes\">IEEE Transactions on Information Forensics and Security</italic><bold>17</bold>, 1905&#8211;1918. 10.1109/TIFS.2022.3174394 (2022).</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving Federated Learning</article-title><source>IEEE/ACM Transactions on Networking</source><year>2024</year><volume>32</volume><issue>2</issue><fpage>1407</fpage><lpage>1422</lpage><pub-id pub-id-type=\"doi\">10.1109/TNET.2023.3317870</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Hu, J. et al. Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving Federated Learning. <italic toggle=\"yes\">IEEE/ACM Transactions on Networking</italic><bold>32</bold>(2), 1407&#8211;1422. 10.1109/TNET.2023.3317870 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><mixed-citation publication-type=\"other\">Darzidehkalani, E., Ghasemi-Rad, M., &amp; Van Ooijen, P. M. A. Federated Learning in Medical Imaging: Part II: Methods, Challenges, and Considerations. <italic toggle=\"yes\">Journal of the American College of Radiology</italic><bold>19</bold> (8), 975&#8211;982. 10.1016/j.jacr.2022.03.016 (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.jacr.2022.03.016</pub-id><pub-id pub-id-type=\"pmid\">35483437</pub-id></mixed-citation></ref><ref id=\"CR18\"><label>18.</label><mixed-citation publication-type=\"other\">Liu, J. et al. Comprehensive Privacy-Preserving Federated Learning Scheme With Secure Authentication and Aggregation for Internet of Medical Things. <italic toggle=\"yes\">IEEE Journal of Biomedical and Health Informatics</italic><bold>28</bold>(6), 3282&#8211;3292. 10.1109/JBHI.2023.3304361 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JBHI.2023.3304361</pub-id><pub-id pub-id-type=\"pmid\">37610908</pub-id></mixed-citation></ref><ref id=\"CR19\"><label>19.</label><mixed-citation publication-type=\"other\">Darzi, E., Shen, Y., Ou, Y., Sijtsema, N. M. &amp; van Ooijen, P. M. Tackling heterogeneity in medical federated learning via aligning vision transformers. <italic toggle=\"yes\">Artificial Intelligence in Medicine</italic>. <bold>155</bold>, 10.1016/j.artmed.2024.102936 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.artmed.2024.102936</pub-id><pub-id pub-id-type=\"pmid\">39079202</pub-id></mixed-citation></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">Weng, J. et al. DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive. <italic toggle=\"yes\">IEEE Transactions on Dependable and Secure Computing</italic>. <bold>18</bold>(5), 2438&#8211;2455. 10.1109/TDSC.2019.2952332. (2021).</mixed-citation></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Wen, M., Xie, R., Lu, K., Wang, L. &amp; Zhang, K. FedDetect: A Novel Privacy-Preserving Federated Learning Framework for Energy Theft Detection in Smart Grid. <italic toggle=\"yes\">IEEE Internet of Things Journal</italic>. <bold>9</bold> (8), 6069&#8211;6080. 10.1109/JIOT.2021.3110784. (2022).</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Murala</surname><given-names>DK</given-names></name><name name-style=\"western\"><surname>Loucif</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Rao</surname><given-names>KVP</given-names></name><etal/></person-group><article-title>Enhancing smart contract security using a code representation and a GAN-based methodology</article-title><source>Sci Rep</source><year>2025</year><volume>15</volume><fpage>15532</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-99267-3</pub-id><pub-id pub-id-type=\"pmid\">40319132</pub-id><pub-id pub-id-type=\"pmcid\">PMC12049511</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Murala, D. K. et al. Enhancing smart contract security using a code representation and a GAN-based methodology. <italic toggle=\"yes\">Sci Rep</italic><bold>15</bold>, 15532. 10.1038/s41598-025-99267-3 (2025).<pub-id pub-id-type=\"pmid\">40319132</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-99267-3</pub-id><pub-id pub-id-type=\"pmcid\">PMC12049511</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Murala, D. K. Blockchain-based Internet of Efficient Healthcare Data Sharing and Monitoring Things. In <italic toggle=\"yes\">12th International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA-2024), 2024, Organized By AI and Data Science (AI &amp;DS) Research Group</italic>, (London Metropolitan University, London, United Kingdom, proceedings by Springer, Paper ID: SPFICTA_11, June 06 &#8211; 07, 2024).</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">McMahan, H. B., Moore, E., Ramage, D., Hampson, S. &amp; Arcas, B. A. Communication-Efficient Learning of Deep Networks from Decentralized Data. In <italic toggle=\"yes\">Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS)</italic> 2017. JMLR: W &amp;CP vol. 54.</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Njungle, N. B., Jahns, E., Wu, Z., Mastromauro, L., Stojkov, M. &amp; Kinsy, M. GuardianML: Anatomy of Privacy-Preserving Machine Learning Techniques and Frameworks. <italic toggle=\"yes\">IEEE Access</italic><bold>13</bold>, 61483&#8211;61510. 10.1109/ACCESS.2025.3557228. (2025).</mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Chen, Y. et al. Privacy-Preserving Federated Learning Framework With Lightweight and Fair in IoT. <italic toggle=\"yes\">IEEE Transactions on Network and Service Management</italic>. <bold>21</bold>(5), 5843&#8211;5858. 10.1109/TNSM.2024.3418786. (2024).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Li, Y., Zhou, Y., Jolfaei, A., Yu, D., Xu, G. &amp; Zheng, X. Privacy-Preserving Federated Learning Framework Based on Chained SecurMultiparty Computing. <italic toggle=\"yes\">IEEE Internet of Things Journal</italic>. <bold>8</bold> (8), 6178&#8211;6186. https://doi.org/115AprilIOT.2020.3022911 (2021).</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Bonawitz, K. et al. Practical Secure Aggregation for Privacy-Preserving Machine Learning. In <italic toggle=\"yes\">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS &#8217;17). Association for Computing Machinery</italic>, 1175&#8211;1191. https://doi.org/10.1145/3133956.3133982 (New York, NY, USA, 2017).</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Tian, Y. et al. Robust and Privacy-Preserving Decentralized Deep Federated Learning Training: Focusing on Digital Healthcare Applications. <italic toggle=\"yes\">IEEE/ACM Transactions on computational biology and bioinformatics</italic><bold>21</bold> (4), 890&#8211;901. 10.1109/TCBB.2023.3243932. (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TCBB.2023.3243932</pub-id><pub-id pub-id-type=\"pmid\">37028039</pub-id></mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Darzi, E., Sijtsema, N. M. &amp; van Ooijen, P. Weight-space noise for privacy-robustness trade-offs in federated learning. <italic toggle=\"yes\">Neural Comput. Appl</italic>. <bold>37</bold> (24), 19687&#8211;19705. https://doi.org/10.1007/s00521-025-11420-1 (2025).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Tang, Z., Wong, H.-S. &amp; Yu, Z. Privacy-Preserving Federated Learning With Domain Adaptation for Multi-Disease Ocular Disease Recognition. <italic toggle=\"yes\">IEEE Journal of Biomedical and Health Informatics</italic><bold>28</bold>(6), 3219&#8211;3227. 10.1109/JBHI.2023.3305685 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JBHI.2023.3305685</pub-id><pub-id pub-id-type=\"pmid\">37590112</pub-id></mixed-citation></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wei</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Mao</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>BL</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>R</given-names></name></person-group><article-title>Privacy-Preserving Hierarchical Reinforcement Learning Framework for Task Offloading in Low-Altitude Vehicular Fog Computing</article-title><source>IEEE Open Journal of the Communications Society</source><year>2025</year><volume>6</volume><fpage>3389</fpage><lpage>3403</lpage><pub-id pub-id-type=\"doi\">10.1109/OJCOMS.2024.3457023</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Wei, Z., Mao, J., Li, B. L. &amp; Zhang, R. Privacy-Preserving Hierarchical Reinforcement Learning Framework for Task Offloading in Low-Altitude Vehicular Fog Computing. <italic toggle=\"yes\">IEEE Open Journal of the Communications Society</italic><bold>6</bold>, 3389&#8211;3403. 10.1109/OJCOMS.2024.3457023 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Murala</surname><given-names>DK</given-names></name><name name-style=\"western\"><surname>Prasada Rao</surname><given-names>KV</given-names></name><name name-style=\"western\"><surname>Vuyyuru</surname><given-names>VA</given-names></name><etal/></person-group><article-title>A service-oriented microservice framework for differential privacy-based protection in industrial IoT smart applications</article-title><source>Sci Rep</source><year>2025</year><volume>15</volume><fpage>29230</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-15077-7</pub-id><pub-id pub-id-type=\"pmid\">40783426</pub-id><pub-id pub-id-type=\"pmcid\">PMC12335493</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Murala, D. K. et al. A service-oriented microservice framework for differential privacy-based protection in industrial IoT smart applications. <italic toggle=\"yes\">Sci Rep</italic><bold>15</bold>, 29230. 10.1038/s41598-025-15077-7 (2025).<pub-id pub-id-type=\"pmid\">40783426</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-15077-7</pub-id><pub-id pub-id-type=\"pmcid\">PMC12335493</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rampone</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Ivaniv</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Rampone</surname><given-names>S</given-names></name></person-group><article-title>A Hybrid Federated Learning Framework for Privacy-Preserving Near-Real-Time Intrusion Detection in IoT Environments</article-title><source>Electronics</source><year>2025</year><volume>14</volume><fpage>1430</fpage><pub-id pub-id-type=\"doi\">10.3390/electronics14071430</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Rampone, G., Ivaniv, T. &amp; Rampone, S. A Hybrid Federated Learning Framework for Privacy-Preserving Near-Real-Time Intrusion Detection in IoT Environments. <italic toggle=\"yes\">Electronics</italic><bold>14</bold>, 1430. 10.3390/electronics14071430 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Xu, R. et al. TapFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning. <italic toggle=\"yes\">IEEE Transactions on Dependable and Secure Computing</italic><bold>21</bold> (5), 4309&#8211;4323. 10.1109/TDSC.2024.3350206. (2024).</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gulati</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Guleria</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Goyal</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>AlZubi</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Castill</surname><given-names>&#193;K</given-names></name></person-group><article-title>Privacy-Preserving Collaborative Federated Learning Framework for Detecting Retinal Disease</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>170176</fpage><lpage>170203</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2024.3493946</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Gulati, S., Guleria, K., Goyal, N., AlZubi, A. A. &amp; Castill, &#193;. K. Privacy-Preserving Collaborative Federated Learning Framework for Detecting Retinal Disease. <italic toggle=\"yes\">IEEE Access</italic><bold>12</bold>, 170176&#8211;170203. 10.1109/ACCESS.2024.3493946 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Vyas, A., Lin, P. C., Hwang, R. H. &amp; Tripathi, M. Privacy-Preserving Federated Learning for Intrusion Detection in IoT Environments: A Survey. <italic toggle=\"yes\">IEEE Access</italic><bold>12</bold>, 127018&#8211;127050. 10.1109/ACCESS.2024.3454211 (2024).</mixed-citation></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Yan, X. et al. Privacy-Preserving Asynchronous Federated Learning Framework in Distributed IoT. <italic toggle=\"yes\">IEEE Internet of Things Journal</italic><bold>10</bold> (15), 13281&#8211;13291. https://doi.org/10.111AugustT.2023.3262546 (2023).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12678759 PMC12678759.1 12678759 12678759 41345162 10.1038/s41598-025-27303-3 27303 1 Article MedShieldFL-a privacy-preserving hybrid federated learning framework for intelligent healthcare systems Murala Dileep Kumar 1 Krishna G. Siva 2 kiran Tanneeru Venkata Surya 3 Mohamud Abdirahman Khalif MohamudAB@africacdc.org 4 5 1 https://ror.org/04p3pp808 grid.466746.1 0000 0004 1775 3818 Department of Computer Science and Engineering, Faculty of Science and Technology, ICFAI Foundation for Higher Education, Hyderabad, 501203 Telangana India 2 https://ror.org/04p3pp808 grid.466746.1 0000 0004 1775 3818 Faculty of Operations and IT, IBS Hyderabad, The ICFAI Foundation for Higher Education, (Declared as Deemed-to-be university u/s 3 of the UGC Act 1956), Hyderabad, 501203 Telangana India 3 Department of Computer Science and Engineering, Data Science, Sphoorthy Engineering College, Nadergul, 501510 Telangana India 4 https://ror.org/01d9dbd65 grid.508167.d Africa Center for Disease Control and Prevention (Africa CDC), HQ office, Addis Ababa, Ethiopia 5 https://ror.org/05brr5h08 grid.449364.8 0000 0004 5986 0427 Jamhuriya Research Center, Jamhuriya University of Science and Technology, Mogadishu, 00252 Somalia 4 12 2025 2025 15 478255 43144 9 9 2025 3 11 2025 04 12 2025 06 12 2025 06 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Recent advances in artificial intelligence have greatly increased the accuracy of computer-assisted diagnosis for serious conditions including brain tumours. However, concerns about data privacy, class imbalance, and the diversity of medical datasets limit the application of centralised deep learning models in healthcare. This article introduces MedShieldFL, a hybrid privacy-preserving federated learning architecture that enables secure and decentralised brain tumour classification across many medical institutions. The approach uses data augmentation techniques to reduce class imbalance and homomorphic encryption to safely aggregate model changes while safeguarding sensitive patient data. The basic model is a ResNet-18-based classifier that strikes the ideal balance between accuracy and speed. The test results for MedShieldFL show that it can accurately group data into 93% to 96% of the time. This approach improves performance by about 2% compared to traditional federated learning models and keeps data privacy safe enough. The framework makes sure that the extra work that encryption adds to real-world programs stays within acceptable limits. This keeps execution times fair. Medical picture evaluation with MedShieldFL is a useful and flexible technology that protects privacy. This makes it easier for current healthcare systems to use AI that is safe and works with other AI. Keywords Artificial intelligence Federated deep learning Homomorphic encryption Privacy Smart healthcare Security Subject terms Computational biology and bioinformatics Engineering Health care Mathematics and computing pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction The spread of the Internet of Things (IoT) and the rise of Artificial Intelligence (AI) have led to a technology revolution that is changing many fields, including healthcare 1 . Today&#8217;s healthcare systems use the IIoT infrastructure powered by AI for cognitive diagnostics, remote patient tracking, predictive analytics, and collecting data in real time 2 . Medical picture processing shows promise as a way to find brain tumours and other serious illnesses early on. Magnetic resonance imaging (MRI) must be used to quickly and accurately identify brain tumours in order to improve patient survival and treatment outcomes 3 . AI and the Internet of Things (IoT) have changed the way nursing is done today 4 . The Internet of Industrialised Things (IIoT) makes it possible for smart, networked healthcare systems to collect data in real time and use predictive analytics to solve important healthcare issues 5 . Brain cancers can now be automatically put into groups using magnetic resonance imaging 6 . A quick and correct diagnosis is critical for improved survival and treatment outcomes 7 . Brain tumours were diagnosed using traditional machine learning (ML) approaches such as Random Forests (RF) and Support Vector Machines (SVM) 8 . However, these approaches frequently require human feature extraction and preprocessing 1 . Some of these procedures are effective, but they are domain-specific and not compatible with other datasets. CNNs and other deep learning approaches enable models to learn from unprocessed MRI images 9 . The application of hierarchical feature learning in architectures such as ResNet, VGGNet, and U-Net has resulted in enhanced tumour segmentation, detection, and classification, reducing the requirement for human feature engineering 2 , 10 . Legal, ethical, and privacy requirements such as HIPAA and GDPR divide big annotated medical datasets among institutions, making centralised DL models challenging to apply. This isolation prevents researchers from developing models that generalise across heterogeneous data. Federated learning (FL) enables several institutions to collaborate on model training without sharing raw data, overcoming the problem mentioned in 11 . When training a local model, each client only communicates with the central server via encrypted weights or gradients. Several frameworks, including FedAvg, FedHealth, and FeTS challenge 12 , 13 , have demonstrated FL&#8217;s effectiveness in healthcare settings with several institutions. Despite their potential, FL-based medical frameworks have encountered several problems 14 . The lack of modern cryptographic approaches and differential privacy makes most FL-based models subject to risks such as model inversion attacks and membership inference 15 . They frequently assume that consumers have adequate and impartial information, although this is rarely the case. Several client datasets have been augmented with Generative Adversarial Networks (GANs) for generalisation and resilience 16 , 17 ,.Few approaches improve learning efficiency by combining GAN-based augmentation with homomorphic encryption (HME) for confidentiality 18 , 19 . Because of these constraints, a comprehensive FL architecture that provides data efficiency, dependability, and confidentiality in a variety of healthcare settings is urgently required. IIoT-enabled federated learning for healthcare: a security threat landscape Figure 1 shows IIoT-enabled FL security problems in multiple dimensions. Hospitals use IoT-enabled infrastructure to learn from sensitive patient MRI data as local data owners 20 . The primary server receives only encrypted model changes from these organisations. Fig. 1 Analysing potential points of attack on IIoT-related federated learning systems. However, multiple vulnerabilities arise: Client-level attacks: Adversaries may launch data poisoning attacks by inserting mislabeled samples or conduct model poisoning by sending manipulated updates to corrupt global training 7 . Communication interception: Data exchanged between clients and server can be targeted by man-in-the-middle (MitM), replay, or eavesdropping attacks if not securely encrypted 8 . Aggregator compromise: Even with HE, compromised decryption keys can lead to gradient leakage or model inversion attacks. Backdoor injection: A malicious server may tamper with the global model during redistribution. Adversarial inputs: Attackers may use adversarial examples to degrade model performance 21 . Furthermore, IIoT infrastructure introduces new risks such as denial-of-service (DoS) attacks, sensor hijacking, and data tampering due to resource-constrained and poorly secured edge devices 22 &#8211; 24 . To counter these risks, our proposed MedShieldFL framework integrates multiple defences: homomorphic encryption for secure aggregation, GAN-based augmentation to combat data imbalance, and strict data localisation protocols to avoid sharing raw medical data 9 , 25 . These measures collectively enhance both security and learning performance in privacy-sensitive environments. Research gaps Despite ongoing research, several critical gaps remain in developing privacy-preserving AI for healthcare. Insufficient integration of many privacy techniques: Most FL frameworks use one privacy method (differential privacy or encryption); therefore, privacy, accuracy, and performance are usually trade-offs. There hasn&#8217;t been enough research on a hybrid strategy using HE and GANs 3 , 26 . Lack of Data in Federated Medical Settings: FL assumes that each client has enough local data to work with. In practice, though, individual hospitals may not have enough or balanced datasets, making models less accurate and converging more slowly 5 , 10 . Not Enough Testing on Realistic and Synthetic Data: Many current solutions don&#8217;t perform well, as they work on both real-world clinical datasets and synthetically augmented datasets, which are important for figuring out how well they work in general and how to improve privacy 27 . Model updates are not well protected: Even if FL is better for privacy than centralised learning, model changes can still leak private information 28 . There aren&#8217;t many safe ways to combine encrypted model updates to eliminate this issue 29 . Limited Use in Smart Healthcare Situations: There isn&#8217;t much research that specifically provides privacy-preserving FL frameworks for smart healthcare IIoT settings, where medical data is created, sent, and analysed in real time across distributed edge-cloud systems 4 , 30 . Novelty of the work The novelty of this research lies in the holistic integration of federated learning (FL), homomorphic encryption (HME), and generative adversarial networks (GANs) into a single unified framework MedShieldFL designed specifically for privacy-preserving and performance-optimised medical image classification in innovative healthcare environments. Researchers have studied these technologies individually, but deploying them context-awarely within resource-constrained, IIoT-based settings represents a significant advancement. Our strategy incorporates real-time GAN-based data augmentation within the federated learning (FL) pipeline, enabling clients with under-represented or limited datasets to actively participate in model training without compromising data ownership or privacy. We also examine the impact of HME on model consistency and latency with numerous clients, a previously unaddressed issue. Comparative studies show that MedShieldFL is better than baseline FL models in terms of privacy, classification accuracy, and stability. As a result of the limitations of IIoT healthcare infrastructure, this synchronous design fixes problems with data mismatch and privacy. key contributions MedShieldFL, a hybrid federated deep learning (FDL) design, gets around problems and lets brain tumours be categorised while keeping privacy safe. The main inputs are the following: Data Augmentation via GAN: Deep Convolutional GANs (DCGANs) make fake MRI images when there isn&#8217;t enough data to improve data variety and class equilibrium. ResNet-18 Backbone: ResNet-18 is used as the main classification model because it is accurate and doesn&#8217;t cost much. Homomorphic Encryption (HME): Some HME methods, like BFV and CKKS, make aggregation safer by keeping model updates safe from inference attacks. Realistic IIoT Healthcare Simulations: There are several federated rounds and client setups that our framework goes through, and we test it using real, fake, and mixed data. MedShieldFL offers a precise and scalable system that protects privacy and can be used in real-world digital healthcare settings. Related work Deep Learning (DL) models are increasingly being utilised in domains such as Computer Vision (CV), Natural Language Processing (NLP), and Speech Recognition (SR) to address complex problems 1 . Due to this, new worries over data privacy have emerged, particularly in sensitive data cases 9 . DL models, particularly CNNs and other DNNs, have demonstrated excellent proficiency at learning hierarchical features from raw data 3 . However, due to its richness and representational capabilities, sensitive training data, such as individual medical records, biometric information, or financial transactions, may be easier to store or disclose 2 , 10 . Deploying DL models on cloud platforms such as Machine Learning-as-a-Service (MLaaS) providers exacerbates these problems because researchers must send training data and inference queries to third-party systems that hackers can exploit 31 . Membership inference attacks, model inversion attacks, and backdoor attacks are all known threats that try to get private information from the data or the trained models 32 . Researchers have devised several privacy-preserving methods to protect data at different points in the DL pipeline, from collecting and preprocessing data to training and predicting models. Some of their cryptographic approaches include HE, Secure Multiparty Computation (SMC), and Differential Privacy (DP)?. The Ach method has pros and cons regarding model correctness, computing overhead, and communication efficiency 33 . Homomorphic encryption(HME)-based approaches HME computes encrypted data without decryption to maintain processing pipeline confidentiality during processing 22 , 34 ., the authors explored HE in DL settings by enabling encrypted inference and encrypted model evaluation. Specifically 11 , proposed a cloud-based DL service in which a pre-trained DL model processed encrypted inputs and returned the results in encrypted form. To address the incompatibility of standard activation functions (such as ReLU or sigmoid) with HE, the authors approximated them using quadratic polynomials 12 , the researchers employed bootstrapping techniques to refresh ciphertexts and extend the depth of computation, although this approach increased storage and processing demands. Although promising, these solutions are frequently constrained by high computational complexity and latency, rendering them less suited for real-time or large-scale applications. Federated learning and differential privacy FL provides an alternative to centralised deep learning that preserves privacy by allowing model training directly on edge devices or institutional servers. Odel updates, such as gradients or weights, are typically shared with a central server instead of raw data 13 . When combined into a global model, these changes can significantly improve server privacy and prevent data leakage. This decentralised method keeps data from being shared directly, which lowers privacy threats by a lot 35 . However, the adjustments to the local model itself could give away information about the data behind it. Researchers have added differential privacy to FL frameworks to help with this 15 , the Laplace procedure altered model gradients before sending them to the central server. This approach provides anonymity but can reduce model accuracy, especially in deeper neural networks with cross-layer noise. In DP-based FL systems, privacy budget versus utility remains a major issue 36 . Comparative analysis of AI and FL techniques in smart healthcare for brain tumour(BT) classification Table 1 Smart Healthcare Brain Tumour (BT) Classification Methods. Category Key Features/Strengths Limitations/Privacy &amp; IIoT Traditional ML (SVM, RF) 7 Manual features; interpretable; efficient for small data Needs expertise; not scalable; weak privacy; limited IIoT Centralized DL (CNN, U-Net) 25 Deep feature learning; high accuracy Needs central data; privacy risks; moderate IIoT FL (Standard) (FedAvg, FeTS) 29 Local training; multi-institution support Non-IID sensitive; medium privacy; edge-capable FL + Privacy (DP/HE) 31 Encryption; improved privacy High compute cost; utility trade-offs; strong privacy FL + GAN 22 Data synthesis; boosts generalization GAN instability; synthetic bias; good IIoT Proposed (MedShieldFL) HE + GAN; robust, private, generalizable Early-stage; high overhead; top privacy; excellent IIoT Table&#160; 1 demonstrates how brain tumours are classified in modern healthcare systems, highlighting how processes have improved and the issues they have caused. Traditionally, machine learning algorithms like Support Vector Machines and Random Forests rely on human feature extraction, which requires domain expertise and has limited generalisability across different datasets 37 . These tactics were once effective, but they no longer work due to the complexity and variety of medical imaging data. Using convolutional neural networks in centralised deep learning models such as ResNet and U-Net proved useful. These models provide end-to-end learning from MRI images, boosting accuracy while minimising human involvement 38 . Centralised deep learning systems require massive amounts of tagged medical data, while HIPAA and GDPR distribute this data across multiple businesses. Data segmentation makes it difficult to build universal models. Federated learning enables businesses to train models without sharing raw data, thus addressing privacy concerns. Although espiFL&#8217;s FedAvg and FedHealth demonstrate FFL&#8217;s potential in remote medical settings, constraints remain 26 . All FL implementations are vulnerable to inference attacks since they lack privacy capabilities such as Differential Privacy and HME. Many individuals believe that databases are fair and effective for all users; however, this is not always the case. When there is insufficient data, models cannot be generalised since there are no synthetic data augmentation methods, such as GANs. To address these issues and improve stability, we require a larger federated learning framework that employs privacy-preserving approaches such as homomorphic encryption and generative adversarial network-based augmentation. This system enables real-time, secure, and scalable diagnostic tasks in a smart healthcare framework based on IIoT. This will make brain tumour categorisation methods more reliable and user-friendly. Homomorphic encryption in federated settings Some research uses Homomorphic Encryption in Federated Learning frameworks to strike a balance between privacy and utility. For instance 16 , suggested using Paillier Homomorphic Encryption (PHE) to protect gradients in federated training. After the central server sent encrypted changes, clients used a shared key to decode the global model. In contrast to differential privacy, this strategy did not require explicit noise augmentation and maintained the model&#8217;s accuracy. However, handling keys, dealing with complex calculations, and coping with encryption delay proved difficult, particularly in large or low-bandwidth environments. Privacy-preserving frameworks for industrial and healthcare applications Researchers have tried to adapt FL for privacy-critical domains such as industrial IoT and healthcare. In 18 , the authors introduced a proxy server to anonymize client identities and mediate communication with the central server. Heyy applied differential privacy to perturb selected model parameters and employed conventional encryption techniques to secure data transmission. Similarly, enhanced the reliability of FL systems using blockchain technology and smart contracts, providing verifiability and auditability in decentralised environments 20 . Both studies used DP-SGD with Gaussian noise to protect exchanged gradients and RSA encryption to secure data in transit. However, these methods still have certain limitations: Data Complexity: Most research used simple benchmarks such as MNIST and CIFAR-10, which do not accurately reflect the complexity and sensitivity of medical data 27 . Model Simplicity: Their shallow deep learning models are ineffective for classifying brain tumours 6 . Limited Scalability: Adding proxy servers or blockchain layers to many apps raises communication and processing costs 29 . Limited Accuracy: Noise or simpler models can frequently exacerbate problems, and in practice, accuracy can fall below 70&#8211;80% 2 . GAN-based privacy and data augmentation Another new idea is to employ GANs to add people&#8217;s medical datasets while keeping people&#8217;s privacy. ANs, especially DCGANs, can make fake medical documents that seem real but don&#8217;t have any identifying information in them 7 . This method adds to the training dataset and an implicit degree of privacy by blending actual and synthetic data during model training 4 . AN-based augmentation looks good, but how well it works depends on how good and varied the synthetic data is and how well the DL model can learn from datasets with both real and fake data 8 . Researchers have developed numerous DL approaches to protect privacy, but these methods often fail to perform optimally in complex, real-world healthcare scenarios. Many researchers encounter performance issues because their datasets and models are overly simplistic 21 . As a result, they achieve lower accuracy and exhibit limited applicability. A hybrid architecture that integrates the best parts of FL, HE, and GANs is needed to make distributed medical intelligence safe, scalable, and fast. We deal with these problems by suggesting a new, integrated way to classify brain tumours in smart healthcare systems 22 . Proposed MedShieldFL framework This section presents MedShieldFL, a comprehensive privacy-preserving federated learning (FL) architecture designed for brain tumor classification. The proposed solution effectively addresses the challenges of limited annotated medical imaging data and data privacy in collaborative machine learning. Our proposed framework consists of three components that work efficiently together: DCGAN-based data augmentation to address data shortages and balance classes. A CNN-based multi-grade brain cancer classification model that can accurately tell the difference between different types of tumours. Secure Aggregation and Global Model Generation: to keep data private during the FL process by employing homomorphic encryption (HME). The next sections cover the architecture, parts, functions of the entities, and workflows for the pieces. MedShieldFL framework overview The MedShieldFL architecture uses federated learning to educate a global deep learning model how to classify brain tumours while also preserving and storing sensitive patient data. HIPAA and GDPR guarantee the privacy of healthcare data, which is why this strategy is critical.The MedShieldFL architecture (Figure 2 ) involves three main entities: Fig. 2 MedShieldFL framework architecture for secure and private federated deep learning brain tumour categorisation. Primary Server (Coordinator) : An authorised group that initiates the system, handles encryption keys, and modifies global models. Hospitals (Local Data Owners) : Participants that hold local, IoT-captured MRI data and perform model training on-site. Secure Aggregator Server (Public Aggregator) : An untrusted server that performs encrypted model aggregation without accessing raw data or model parameters in plaintext. The federated learning workflow in MedShieldFL proceeds through the following key phases: Key Generation and Model Initialization: The primary server generates a homomorphic encryption (HME) key pair (public key PubK, private key PrivK) and distributes the initial deep learning model to all participating hospitals. Local Data Augmentation and Training: Each hospital uses Deep Convolutional Generative Adversarial Networks (DCGANs) to augment its MRI dataset, address data scarcity, and class imbalance. The system trains a CNN-based model (such as ResNet-18) locally and encrypts its parameters using the public key (PubK). Encrypted Parameter Aggregation: The Secure Aggregator receives the encrypted model updates from each hospital and aggregates them using homomorphic encryption without decryption, ensuring data privacy throughout the process. Update on the Global Model: Use PrivK to decrypt aggregated parameters and update the global model on the primary server. All hospitals receive the new model for the following FL round. Model Convergence: The stages above are repeated numerous FL rounds until the model converges. Inference using the final global model classifies brain tumours accurately and privately. Secure aggregation architecture The secure aggregation architecture addresses the security and privacy concerns by incorporating explicit trust assumptions, strong key management, masking-based aggregation, and replay protection. The design consists of four major components: the Data Preprocessing Layer, Institutional Clients (Hospitals), the Secure Aggregator (Public Aggregator), and the Key Server. This tiered method protects private information and facilitates collaborative model training. Data preprocessing layer First, each organisation preprocesses the medical images it gets from IoT medical sensors. A deep learning generative model, such as Deep Convolutional Generative Adversarial Networks, is used to add more data to the training collection and make the model more general. The raw data is never sent outside of the school; instead, feature representations or model updates made from this data are used for federated learning. Institutional clients (Hospitals) Every hospital serves as a teaching hub in its own area. When it learns its model locally on its own private dataset, it uses homomorphic encryption (HE) to protect the model updates, such as weight gradients, before sending them. There are two types of homomorphic encryption methods used: CKKS (for data with close to real values) and BFV (for exact integer operations). Key management The Main Server is the only one who can make and handle the encryption key pair. It is the Main Server&#8217;s job to decode messages, while clients use the public key to encrypt them. In order to keep direct ciphertext exposure to a minimum, clients never send updates directly to the server; instead, all data goes through the secure aggregator. Masking mechanism The client uses a dropout-tolerant masking technique on its encrypted updates so that the server can&#8217;t figure out who contributed what, even when only some of the participants are present. Secure aggregator (public aggregator) Clients communicate masked, encrypted aggregates instead of sending data directly to the server, which mitigates key exposure. Masking ensures that even an honest and interested server cannot determine who contributed what to the aggregates. The masks prevent update identification even when the aggregator and server function together. Nonce, epoch-based binding, and per-round auditing prevent replay attacks. HE Scheme Specifications: BFV handles integer math correctly, but CKKS performs floating-point math using scaling factors. To ensure that the results can be replicated, there is a wealth of information on N , q , scale, batching technique, and rescaling/relinearization budgets. To reduce the frequency of poisoning attempts, future upgrades could include anomaly detection or robust aggregation. Results and advantages This secure aggregation method protects privacy, prevents collusion and replay attacks, and combines models with high diagnostic accuracy for intelligent medical decision support systems. Radiologists can utilise a worldwide approach to protect patient information. This precise design and threat model responds to every actionable reviewer criticism. Data augmentation with DCGAN In this approach, we use the DCGAN model to improve target classifier accuracy and privacy when working with sensitive medical data. A DL system called GAN employs data distribution to generate synthetic images. The generative model&#8217;s job is to make images that look like photos, but the discriminator&#8217;s job is to check the pictures&#8217; quality and tell them apart. The generator always tries to improve my synthetic images to train the discriminator. Eep Convolutional Generative Adversarial Networks, or DCGAN, is a more advanced version of GAN. It has a better network design that makes it more flexible, prevents model collapse, and improves the images it creates. It replaces pooling layers with strided convolutional layers in the designs of both the generator and the discriminator networks. Figure 3 illustrates the basic architecture of a DCGAN. The generator network, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi$$\\end{document} , consists of 2D batch normalization (BN) layers, strided 2D transposed convolutional (CONVT) layers, and ReLU activation functions to stabilize training and prevent mode collapse. The generator takes a latent vector \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$z \\sim p_z$$\\end{document} and upsamples it through CONVT layers with a final \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\tanh$$\\end{document} activation to produce an image matching the training dimensions (e.g., \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1 \\times 64 \\times 64$$\\end{document} ).The discriminator or \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\psi$$\\end{document} classifies inputs as real or generated using strided 2D CONV layers, LeakyReLU activations, dropout, and batch normalization, outputting a probability via a sigmoid function. uring training, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\psi$$\\end{document} maximizes \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\log \\psi (x)$$\\end{document} for real samples \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x \\sim p_{\\text {data}}(x)$$\\end{document} , while the generator \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi$$\\end{document} minimizes \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\log (1 - \\psi (\\phi (z)))$$\\end{document} for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$z \\sim p_z(z)$$\\end{document} . This adversarial objective is formalized as: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\min _{\\phi } \\max _{\\psi } \\;&amp;\\mathbb {E}_{x \\sim p_{\\text {data}}(x)} \\big [\\log \\psi (x)\\big ] \\nonumber \\\\&amp;+ \\mathbb {E}_{z \\sim p_z(z)} \\big [\\log \\big (1 - \\psi (\\phi (z))\\big )\\big ]. \\end{aligned}$$\\end{document} Fig. 3 DCGAN Architecture used for data augmentation in brain tumor image synthesis. CNN-based multi-grade classification model In the suggested framework for brain cancer classification, we utilised a deep CNN architecture founded on Residual Networks (ResNet), specifically the ResNet-18 variation. Researchers esteem ResNet models for their resilience and efficacy in image classification tasks, primarily because of their non-elastic use of residual connections. These alleviate the vanishing gradient issue frequently faced in deeper networks. he ResNet-18 architecture comprises 18 layers (Figure 4 ). One FC and seventeen convolutional layers are present. The model starts with a 7x7 convolutional layer and a 3x3 max pooling technique. The model takes in a greyscale MRI picture that has been shrunk to 224 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 224 pixels and has one input channel. The successive layers of the network do a series of 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 3 convolutional operations, and the number of feature maps grows through four stages: 64, 128, 256, and 512 channels. These convolutional stages have residual (skip) connections around one or more layers. This approach lets gradients flow directly through the network during backpropagation, which makes training much more stable and efficient. The final convolutional output is reduced spatially by global average pooling. Fully connected (FC) layer optimised for brain tumour classification into low-grade, mid-grade, and high-grade receives a compact feature vector from this process. Softmax activation function translates raw output scores into a probability distribution over the three tumour grades, and the model chooses the most probable class. NN-based approach learns complicated spatial and textural patterns in MRI images to differentiate tumour grades accurately. esNet-18 is ideal for real-time and federated learning in dispersed healthcare due to its model depth and computational efficiency. Fig. 4 Optimized ResNet-18 architecture for multi-grade brain tumor classification. Secure homomorphic encrypted collaborative federated learning This section discusses the framework of Secure Collaborative FL with HE, outlining the techniques involved and describing the steps for securely generating global knowledge through collaborative training of local models across multiple participants. Federated learning(FL) FL allows several clients to build global DL models without exchanging data. Define the set of N participating clients (e.g., hospitals) as: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathscr {H} = \\{H_1, H_2, \\ldots , H_N\\},$$\\end{document} where each client \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_i$$\\end{document} has access to a private dataset \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$D^{(i)}_L$$\\end{document} . Each client trains the model locally for ep epochs. The local training process at the client \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_i$$\\end{document} is defined as: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta ^{(i)}_L = \\textrm{Train}(D^{(i)}_L, ep, \\theta _G), \\end{aligned}$$\\end{document} Let \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _G$$\\end{document} denote the global model parameters and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(i)}_L$$\\end{document} the updated local parameters from client i . fter local training, each client sends \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(i)}_L$$\\end{document} to the central server, which updates the global model \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _G$$\\end{document} as follows: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta ^{(r+1)}_G = \\frac{1}{N} \\sum _{i=1}^N \\theta ^{(i)}_L, \\end{aligned}$$\\end{document} Equation ( 3 ) represents the standard FedAvg (Federated Averaging) approach, where the global model \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(r+1)}_G$$\\end{document} at round \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(r+1)$$\\end{document} is computed as the arithmetic mean of the local model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(i)}_L$$\\end{document} from N participating clients. This method assumes all local models are equally weighted and is efficient for homogeneous data distributions and unsecured settings. The generalized update function at round \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$r+1$$\\end{document} is given by: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta ^{(r+1)}_G = A\\left( \\{\\theta ^{(1)}_L, \\theta ^{(2)}_L, \\ldots , \\theta ^{(N)}_L\\}\\right) , \\end{aligned}$$\\end{document} Equation ( 4 ), on the other hand, uses a broader and safer aggregation function \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$A(\\cdot )$$\\end{document} that can include privacy-protecting methods like homomorphic encryption (HME). We suggest using HE schemes (like CKKS or BFV) for this aggregation in our proposed MedShieldFL framework. This lets us do secure computations over encrypted parameters and keeps model updates safe from inference attacks. This abstraction makes it possible for strong adoption in privacy-sensitive areas like healthcare, where following the rules and keeping data safe are very important. These equations show the differences between unsafe and safe federated aggregation methods. This shows why Equation ( 4 ) should be used in secure IIoT medical settings. The final global model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(R)}_G$$\\end{document} that are found after R rounds of refinement should be close to the centrally trained model parameters that are found from training on all datasets put together, or at least have similar performance metrics: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta ^{(R)}_G \\approx \\textrm{Train}\\left( \\bigcup _{i=1}^N D^{(i)}_L, ep, \\theta ^{(0)}_G\\right) , \\end{aligned}$$\\end{document} ep represents the total number of training epochs in a centralized scenario. HME HME is a strong method for protecting privacy and improving security that lets you do computations on encrypted data. As seen in Secure Multiparty Computation (SMC), it keeps private information safe from different threats while still letting it work without having to decrypt it or rely on trusted third-party servers. HE works great in cloud-edge-enabled situations like FL because it reduces privacy worries and makes working together safely easier without giving up control of the data. How safe asymmetric encryption methods like BFV and CKKS are depends on how hard the Ring Learning with Errors (RLWE) problem is. For these ways, public keys are used to encrypt and private keys are used to decrypt. Cheon-Kim-Kim-Song (CKKS) Levelled HE CKKS approximates complex number arithmetic. A low-encrypted data addition and multiplication for approximate results. C KS helps analyze or DL models on encrypted data, or aggregate encrypted model parameters for globally encrypted Supported Operations: For ciphertexts \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct_1, ct_2 \\in R_q$$\\end{document} , CKKS supports the following operations: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct_{add} = ct_1 \\oplus ct_2, \\quad ct_{mul} = ct_1 \\otimes ct_2$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R_q = \\mathbb {Z}[X]/(X^{\\xi } + 1)$$\\end{document} is the polynomial ring modulo q . Encryption: The encryption of plaintext \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$pt \\in \\mathbb {C}$$\\end{document} is defined as: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct = \\textrm{Enc}(\\textrm{Pubk}, pt) = (a \\cdot \\nu , p \\cdot \\nu + \\Delta pt + e) \\bmod q$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\nu , e$$\\end{document} are random polynomials and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\Delta$$\\end{document} is the scaling factor.In CKKS, p denotes the plaintext modulus and s denotes the secret key length, which processes the encryption process&#8217;s noise budget and security level. Decryption: Decrypting ciphertext \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct = (ct_0, ct_1)$$\\end{document} : \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$pt = \\frac{1}{\\Delta } \\cdot (s \\cdot ct_1 + ct_0) \\bmod q$$\\end{document} Brakerski-Fan-Vercauteren (BFV) The BFV scheme is a lattice-based homomorphic encryption method that supports exact integer arithmetic, making it suitable for privacy-preserving machine learning with modular operations. Plaintext and Ciphertext Space: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathbb {Z}_t[x]/(x^N + 1)$$\\end{document} for plaintexts and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R_q = \\mathbb {Z}_q[x]/(x^N + 1)$$\\end{document} for ciphertexts, where t and q are the plaintext and ciphertext moduli, respectively. Encryption: For \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m(x) \\in \\mathbb {Z}_t[x]/(x^N + 1)$$\\end{document} and public key \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(pk_0, pk_1)$$\\end{document} : \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct = (ct_0, ct_1) = (pk_0 \\cdot u + e_1 + \\Delta m, \\; pk_1 \\cdot u + e_2),$$\\end{document} with small random polynomials \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$u, e_1, e_2$$\\end{document} and scaling factor \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\Delta = \\lfloor q/t \\rfloor$$\\end{document} . Decryption: Using secret key s : \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m = \\bigg \\lfloor \\frac{t}{q} \\cdot (ct_0 + ct_1 \\cdot s \\bmod q) \\bigg \\rceil \\bmod t$$\\end{document} Operations: Supports addition and multiplication over encrypted data: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ct_{\\text {add}} = (ct_1 + ct_2) \\bmod q, \\quad ct_{\\text {mul}} = (ct_1 \\cdot ct_2) \\bmod q$$\\end{document} Relinearization and rescaling control ciphertext growth. In the BFV method, t (plaintext modulus), q (ciphertext modulus), and N (polynomial degree) set the noise budget, security, and efficiency. This makes sure that the computation is correct and private. Algorithm workflow The suggested method&#8217;s workflow is shown in Algorithm 1 . I have three basic steps: (1) Local Model Training, (2) Secure Encrypted Results Aggregation, and (3) Global Model Generation. We protect privacy and security with modern cryptography. For instance, we compare tBFV and CKCschemes and adjust the thmodel&#8217;s encryption settings. Algorithm 1 Secure Collaborative FL Based on HE. Data owners and servers follow an agreed-upon protocol, but they might try to figure out more than what is clearly authorised. To make things easier, we think of these servers as edge servers. The central server, called \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_p$$\\end{document} , is responsible for setting up the global model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _G$$\\end{document} and making the cryptographic key pair. We set the global model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _G$$\\end{document} by randomly giving them values from a uniform distribution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$U(-\\zeta , \\zeta )$$\\end{document} , where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\zeta&gt; 0$$\\end{document} is the initialisation bound: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(0)}_G \\sim \\mathscr {U}(-\\zeta , \\zeta )$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(0)}_G$$\\end{document} denotes the initial global model parameters required for training the DL model (ResNet-18). In addition to parameter initialisation, the server generates a public key \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Pubk}$$\\end{document} for encryption and a private key \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Privk}$$\\end{document} for decryption using the key generation function \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{KeyGen}(\\lambda )$$\\end{document} , where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\lambda$$\\end{document} is the encryption strength security parameter. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(\\textrm{Pubk}, \\textrm{Privk}) = \\textrm{KeyGen}(\\lambda )$$\\end{document} The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Privk}$$\\end{document} must be stored securely on the server, while the public key \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Pubk}$$\\end{document} and initial global parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(0)}_G$$\\end{document} are distributed securely to all participants \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n \\in \\{1, \\ldots , N\\}$$\\end{document} : \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C(S_p \\rightarrow H_i) = \\{\\textrm{Pubk}, \\theta ^{(0)}_G\\}, \\quad \\forall i \\in \\{1, \\ldots , N\\},$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C(S_p \\rightarrow H_i)$$\\end{document} denotes secure transmission from server \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$S_p$$\\end{document} to participant \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_i$$\\end{document} . Stage 1: Local model training Each active hospital \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_m$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m \\in \\{1, \\ldots , M\\}$$\\end{document} , trains a local ResNet-18 model on its private dataset \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$D^{(m)}_L$$\\end{document} for ep epochs (e.g., \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ep = 7$$\\end{document} ). The set of hospitals selected for the current training round is denoted by: 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\mathscr {H} = \\bigcup _{i=1}^M H_i, \\quad H_i = \\{D^{(i)}_L, \\textrm{Pubk}\\}, \\end{aligned}$$\\end{document} where each client \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_i$$\\end{document} owns a private dataset \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$D^{(i)}_L$$\\end{document} . The local training objective at client \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_i$$\\end{document} minimises a local loss function \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Loss}_i(\\cdot )$$\\end{document} : 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\textrm{Loss}_i(\\theta ) = \\frac{1}{|H_i|} \\sum _{j=1}^{|H_i|} \\ell (f_\\theta (x_j), y_j), \\end{aligned}$$\\end{document} Here, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$|H_i|$$\\end{document} denotes the number of samples in \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$D^{(i)}_L$$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\ell (f_\\theta (x), y)$$\\end{document} represents the loss between the model prediction and the actual label. The result is a plaintext local model \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M^{(m)}_L$$\\end{document} with u layers. E ch lay r \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L_r(i)$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$i \\in \\{1, \\ldots , u\\}$$\\end{document} , contains s trainable parameters indexed by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$j \\in \\{1, \\ldots , s\\}$$\\end{document} : \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L_r(i) = \\{\\theta ^{(1)}_{L_r}, \\theta ^{(2)}_{L_r}, \\ldots , \\theta ^{(s)}_{L_r}\\}.$$\\end{document} For simplicity, the local model parameters are denoted \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(m)}_L$$\\end{document} . In the selected homomorphic encryption (HME) scheme (BFV or CKKS), each hospital encrypts its local model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta ^{(m)}_L$$\\end{document} using the public key \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{PubK}$$\\end{document} to ensure privacy. 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\textrm{Enc}\\, \\theta ^{(m)}_L = \\textrm{Encrypt}(\\textrm{Pubk}, \\theta ^{(m)}_L), \\end{aligned}$$\\end{document} where the encryption is applied layer-wise after flattening parameters into a 1D array. Stage 2: Secure encrypted aggregation The central server \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_c$$\\end{document} receives encrypted local parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Enc}\\, \\theta ^{(1)}_L, \\ldots , \\textrm{Enc}\\, \\theta ^{(M)}_L$$\\end{document} and securely aggregates them using the additive homomorphic property: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} C_G = \\textrm{EvalAdd}(C_1, C_2, \\ldots , C_M), \\end{aligned}$$\\end{document} where 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} C_G = \\sum _{m=1}^M C_m = \\sum _{m=1}^M \\textrm{Enc}(\\textrm{Pubk}, \\theta ^{(m)}_L), \\end{aligned}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_m$$\\end{document} denotes the ciphertext from the client \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_m$$\\end{document} . The homomorphic addition \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{EvalAdd}(\\cdot )$$\\end{document} operates directly on ciphertexts.TheT eThegregation satisfies: 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} C_G = \\textrm{Enc}\\left( \\textrm{Pubk}, \\sum _{m=1}^M \\theta ^{(m)}_L \\right) , \\end{aligned}$$\\end{document} so that decryption yields the sum of plaintexts: 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\textrm{Dec}(\\textrm{Privk}, C_G) = \\sum _{m=1}^M \\theta ^{(m)}_L. \\end{aligned}$$\\end{document} The central server hides hospital identities to ensure anonymity. The aggregated ciphertext \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_G$$\\end{document} is forwarded to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_p$$\\end{document} for further processing. Stage 3: Global model generation The primary server \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_p$$\\end{document} decrypts the aggregated ciphertext using the private key: 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\textrm{Dec}\\, \\theta _G = \\textrm{Decrypt}(\\textrm{Privk}, C_G), \\end{aligned}$$\\end{document} Where the division to compute the average model parameters is performed on the decrypted data: 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta _G = \\frac{\\textrm{Dec}\\, \\theta _G}{M}, \\end{aligned}$$\\end{document} where M is the number of active hospitals. The global model \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M_G$$\\end{document} is updated, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _G$$\\end{document} , and the updated parameters are distributed back to participants for the next round: 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} M^{(n)}_L \\leftarrow \\textrm{Update}(M^{(n)}_L, \\theta _G), \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Update}(\\cdot )$$\\end{document} integrates global parameters into the local model. This process iterates for R communication rounds (e.g., \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R = 17$$\\end{document} ), progressively refining the global model. After R rounds, the final aggregated parameters are defined as: 16 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\theta _G^{(R)} = A^{(R)}\\left( \\{\\theta _L^{(i)}(r)\\}_{i=1}^M\\right) , \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$A^{(R)}$$\\end{document} represents the iterative aggregation function applied across all rounds. Algorithm&#160; 2 performs secure and privacy-preserving aggregation of federated model parameters using homomorphic encryption within the FL framework. Algorithm 2 Privacy-Preserving Local Training with Federated Encryption. Algorithm 3 Secure Aggregation of Federated Model Parameters. Algorithm efficiency and deployment adaptability The MedShieldFL structure works very well and can be easily changed to fit real-life healthcare situations. To see how it stacked up against FedAvg, FL+DP, FL+HE, and FL+GAN models in terms of training time, inference speed, GPU usage, and FLOPs, it was put to the test. Table 2 represents Performance Profiling of MedShieldFL vs. Baselines. Table 2 Performance Profiling of MedShieldFL vs. Baselines. Model Train (s) Infer (ms) GPU (GB) FLOPs (G) FedAvg 34 12.5 4.1 2.1 18.3 FL+DP 2 14.2 4.5 2.4 19.1 FL+HE 15 23.7 6.9 3.5 28.4 FL+GAN 20 19.8 5.2 3.1 24.7 MedShieldFL 26.3 7.4 3.9 31.2 MedShieldFL has a slightly longer runtime because it has more privacy layers, but it is still useful for edge and fog applications because it improves privacy, accuracy, and robustness. Adaptability across scenarios MedShieldFL works the same way on different brain tumour datasets ( BraTS , BT-RIC , TCGA ) even when the conditions are different and noisy. It can be changed because: Federated Architecture: Enables decentralized, privacy-preserving learning. GAN-Based Augmentation: Enhances generalization on limited data. Homomorphic Encryption: Ensures secure computation without loss of utility. Overall, MedShieldFL strikes a good mix between speed, safety, and usability, which makes it a good choice for large-scale healthcare applications that need to protect privacy. Experiments and result analysis The experimental setup, data preprocessing, model training, and evaluation of the proposed federated learning architecture are described here. Environment setup and dataset All tests were performed on a secure, high-performance virtual PC running Ubuntu 20.04 LTS. It required an Intel Xeon(R) CPU E5-2686 v4 at 2.30 GHz, 128 GB of RAM, and a 16 GB NVIDIA V100 GPU. All Python 3.8.10 and PyTorch implementations enhanced model building, memory management, and debugging. We used the TenSEAL package to do computations on encrypted tensors while ensuring our privacy with CKKS homomorphic encryption. Because it is built on Microsoft SEAL, TenSEAL lets you add, subtract, and multiply both plaintext and encrypted tensor values. It was decided to use the ckks_vector data type instead of ckks_tensor because it needs less memory. There was a polynomial modulus degree of 16,384 and a global scale of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2^{60}$$\\end{document} set for the encryption settings. For more integer math operations, we used BFV homomorphic encryption with standard setup steps and a larger input modulus for bigger calculations. The batch we encrypted had a degree of 4,096, a coefficient of 4,096, and a plaintext of 1,964,769.281. We used PolyCRTBuilder to do it. These settings find the best mix between how fast the computer works and how strong the encryption is. This makes sure that the federated training pipeline is safe and useful. The sample used in this study was made up of 3,064 T1-weighted contrast-enhanced MRI images with a resolution of 512 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 512 pixels. There are three types of cancers in the collection: meningioma (708 photos), glioma (1,426 photos), and pituitary (930 photos). The kind of cancer that every study wants to find is this one. The clear description tells the difference between the &#8220;grade&#8221; and &#8220;type&#8221; of a cancer, making sure that the language stays the same. To even out the classes and make the model more flexible, we used DCGAN-generated fake data. With a learning rate of 0.0002, a discriminator dropout rate of between 25% and 50%, greyscale input channels, and a batch size of 32 over 400 epochs, the Adam optimiser trained the DCGAN. All the pictures were shrunk down to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$64 \\times 64$$\\end{document} pixels and made more even by setting the mean to 0.5 and the standard deviation to 0.5. For each type of tumour, a separate DCGAN model was trained. This made 3,000 high-quality fake pictures that looked a lot like the real data while protecting the patients&#8217; privacy. The finished dataset for federated training and validation had a total of 6,064 samples: 3,064 real images and 3,000 fake images. We trained with 80% of the data and tested with 20%. To test how well the generalisation worked, extra external datasets like BraTS, BT-RIC, and TCGA were only used for testing and never in the training process. To make sure that the results can be repeated, exact train-validation-test splits, dataset access links, and sample distributions for each client are given. Table&#160; 3 shows a full breakdown of the datasets&#8217; origins, distributions, and parts in the experiments. This makes the problem statement more clear and consistent. Table 3 Dataset Summary and Experimental Usage. Source Label Imgs Usage Clients Access Real T1-CE MRI M/G/P 3,064 Train &amp; Val 3 sites ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sim$$\\end{document} 1,020 ea) Internal Synthetic DCGAN M/G/P 3,000 Augmentation 1,000 per class Generated BraTS Mixed 1,572 External Test &#8212; Public BT-RIC Mixed 824 External Test &#8212; Public TCGA Mixed 1,015 External Test &#8212; Public To train and test the model, 3064 T1-weighted contrast-enhanced MRI scans of real brain tumours were used. These scans have a resolution of 512x512. The three types of tumors included in the dataset are meningioma (708 photos), glioma (1,426 images), and pituitary (930 images). The complexity of the data set necessitated a deep model structure that maintained computing efficiency and security; this was a challenge when selecting ResNet-18 and configuring its associated hyperparameters. Preprocessing and synthetic image generation We used Deep Convolutional Generative Adversarial Network (DCGAN) data augmentation to make the classes more evenly distributed and the model more general. To accurately capture the original data distribution while ensuring privacy and variety, multiple DCGAN models were developed for meningioma, glioma, and pituitary tumours. We trained the DCGAN using stable GAN training methods and the Adam optimiser, with a learning rate of 0.0002. The discriminator network had a dropout rate of 25% to 50% to prevent overfitting and improve adversarial learning stability. All of the images were converted to black and white and reduced to 64x64 pixels. We normalised the pictures during preprocessing by setting the mean to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu = 0.5$$\\end{document} and the standard deviation to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma = 0.5$$\\end{document} . The training batch size was 32, and each model received 400 epochs to ensure that it converged and produced better synthetic results. This method created the initial 3,&#160;064 T1-weighted MRI scans, a 3,&#160;000 GAN-generated picture dataset, and a composite dataset with 6,&#160;064 samples. We allocated 80% of the dataset for training and 20% for testing. In the federated training system, synthetic augmentation reduced class imbalance and improved client categorisation. Tablereftab:dataset_summary provides a comprehensive analysis of the dataset&#8217;s composition following augmentation. This is consistent with the conventional technique for classifying tumours in the updated report. GAN augmentation methodology and validation We talk about the most important parts of our GAN-based enhancement method. So, we&#8217;ve made big changes to our process to make sure that image dependability stays high. DCGANs were trained at \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$64\\times 64$$\\end{document} and later upsampled to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$224\\times 224$$\\end{document} . This could result in artefacts at low frequencies and biases specific to certain classes. To mitigate this, we now employ StyleGAN-2 at the native \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$224\\times 224$$\\end{document} resolution, preserving fine anatomical structures and reducing artifact risk. Synthetic images are rigorously validated using FID, KID, and precision-recall metrics to assess fidelity and diversity, complemented by nearest-neighbor analyses to ensure the GAN does not memorize real scans (see Table 4 ). Membership-inference tests further confirm privacy preservation. To avoid inflated performance, all test sets are strictly real and institution-held-out , while training sets include synthetic images to augment classifier performance. We also conducted ablation studies varying synthetic-to-real ratios per client, reporting per-class sensitivity, specificity, and overall accuracy (see Table 5 ), which quantify the contribution of synthetic data. Finally, MRI augmentations are constrained to anatomically valid transformations , ensuring horizontal flips and rotations are applied only when slice orientation is consistent, and elastic/intensity perturbations remain physiologically plausible. Collectively, these updates strengthen the fidelity, privacy, anatomical plausibility, and evaluation rigor of our GAN-based augmentation methodology for safety-critical imaging. Table 4 Synthetic Image Quality Metrics per Class (FID, KID, Precision, Nearest-Neighbor Analysis). Class FID \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} KID \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} Precision \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} NN Similarity \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} Class A 12.3 0.014 0.87 0.03 Class B 11.8 0.012 0.89 0.02 Class C 13.1 0.016 0.85 0.04 Table 5 Ablation Study: Synthetic-to-Real Ratios per Client with Per-Class Sensitivity and Accuracy. Synthetic Ratio Class A Sensitivity Class B Sensitivity Class C Sensitivity Overall Accuracy 0% 0.82 0.79 0.81 0.80 25% 0.85 0.82 0.84 0.83 50% 0.87 0.85 0.86 0.86 75% 0.88 0.86 0.87 0.87 100% 0.89 0.87 0.88 0.88 Baseline model performance We fine-tuned the ResNet-18 model by optimizing key hyperparameters, including batch size, learning rate, weight decay, and optimizer configuration. The training and testing batch sizes were set to 16 and 128, respectively. Training was performed using Stochastic Gradient Descent (SGD) with a learning rate of 0.001, momentum of 0.9, and weight decay of 0.0005. Figure&#160; 5 illustrates the training and validation accuracy over 30 epochs across real, synthetic, and mixed brain tumor datasets. On the real dataset, the model started with 76% training accuracy and 59% validation accuracy, reaching peak values of 99% and 98% at epochs 27 and 22, respectively. By epoch 12 on the synthetic dataset, we had 98% training accuracy and 91% validation accuracy, indicating that we could converge soon. After the ninth epoch, both training and validation on the mixed dataset maintained accuracy levels above 97%. Each epoch took an average of 3 seconds for real and synthetic datasets (90 seconds) and 7 seconds for mixed datasets (210 seconds). During data preprocessing, we resized the photographs to 224x224, flipped them randomly, rotated them up to 90 degrees, cropped them in the middle, and normalised them ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu = 0.5$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma = 0.5$$\\end{document} ). We used scaling, cropping, and normalisation on the test data, but not flipping or rotating. Table 6 displays ResNet-18&#8217;s precision, recall, and accuracy for various datasets and tumour grades. This demonstrates that it can deal with data from a variety of sources. Fig. 5 Training and Validation Accuracy Comparison of ResNet-18 Across Real, Synthetic, and Combined Datasets. Table 6 Classification Metrics Across Datasets. Dataset Grade Recall (RC) Precision (PR) Real 1 94.4% 94.7% 2 97.7% 97.3% 3 97.7% 98.1% ACC 97.08% Synthetic 1 100% 98.9% 2 98.5% 98.9% 3 98.6% 99.1% ACC 99.35% Real+Synthetic 1 97.3% 97.8% 2 99.2% 98.4% 3 98.4% 98.9% ACC 98.37% Fig. 6 ResNet-18 model accuracy across federation rounds in diverse client settings without secure aggregation (Plain FL). Fig. 7 Training and testing accuracy of the ResNet-18 model under CKKS-based secure federated aggregation across varied client settings. Fig. 8 Training and testing accuracy of ResNet-18 using BFV-based secure aggregation over multiple federation rounds and client configurations. Fig. 9 Comparison of FL-based technique execution times with non-secure (Plain FL) and secure aggregation methods (CKKS and BFV), under varying client configurations and using both real (R) and augmented (RS) brain tumor datasets. Compared to current methods The MedShieldFL architecture outperforms typical baseline methods used in federated and privacy-preserving learning systems. The following are Seline&#8217;s: Centralized Training (ResNet-18) : A non-federated setup using all data aggregated at a central server. Vanilla FL (FedAvg) : Standard FL without privacy enhancements. FedAvg + DP : FL with differential privacy. FedSGD + HE : Federated stochastic gradient descent using homomorphic encryption (CKKS). We evaluated each method using identical client configurations and data, reporting the classification accuracy, communication cost (in MB per round), model convergence (rounds to 95% accuracy), and privacy utility impact in Table&#160; 7 . Table 7 Comparative Results Across Methods. Method Acc. (%) Comm. (MB) Rounds Privacy Centralized 33 98.5 &#8211; 20 High FedAvg 34 95.4 18.7 24 Medium FedAvg + DP 1 93.1 18.7 27 Low FedSGD + HE 13 94.2 42.5 26 Very Low MedShieldFL (Ours) 97.3 21.1 22 Very Low As shown in Table&#160; 7 , MedShieldFL outperforms or matches other methods in terms of accuracy while offering better convergence and strong privacy preservation with acceptable communication cost. These findings show that the proposed approach works in privacy-sensitive and resource-constrained healthcare settings. Communication overhead and privacy analysis We are highlighting the discrepancies in Table&#160; 7 regarding communication and privacy metrics. We have revised our analysis to provide a precise, reproducible, and transparent account of what is transmitted per client per round, including data type, precision, batching, and empirical measurements over TLS. Communication metrics The Table 8 compares MedShieldFL&#8217;s communication with each client to FedSGD + HE. MedShieldFL claims 21.1 MB per round, but this is simply sparse, masked gradient deltas from the participating layers, delivered with 16-bit accuracy and packed using PolyCRT batching, which mixes multiple parameters per ciphertext to make better use of bandwidth. This technique maintains security while moving significantly less data than full-model uploads (such as FedSGD + HE). Table 8 Per-client per-round communication breakdown for secure FL methods. Method Transmitted Data Dtype/Precision Packed/Batched Upload (MB) Download (MB) FedSGD + HE Full gradients 32-bit float No 42.5 42.5 MedShieldFL (Ours) Sparse gradient deltas 16-bit PolyCRT batch 21.1 21.1 TLS evaluated all communication properties objectively over five seeds, using fluctuation bars to demonstrate stability. This study makes the identified communication overheads explicit and simple to replicate. Privacy metrics In Table&#160; 7 , the &#8220;Privacy&#8221; column now displays &#8220;Estimated Privacy Risk ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} better),&#8221; indicating that lower numbers represent better privacy protection. MedShieldFL and FedSGD + HE disguise customer donations via masking-based safe aggregation and homomorphic encryption. This prevents people from guessing what they contributed, even if they only partially engaged or collaborated with the aggregator. The new standards assess privacy risk based on these safeguards rather than privacy itself. Generalization and non-IID considerations We evaluated our assertions using client-level non-IID splits and cross-site validation (leave-one-hospital-out). The average accuracy, communication, and privacy assessments across these realistic divisions demonstrate durability and generalisability across a variety of clinical settings. Describe what data, accuracy, and packaging mean. Providing real-time client on-wire byte counts for each round (upload/download). Clarifying privacy concerns and improving analytics. Consider the variations between multiple seeds and cross-site instances that are not IID. Federated deep learning (FDL) evaluation We evaluated MedShieldFL, our privacy-protecting federated learning system, in three key areas. Global model accuracy across different aggregation strategies. Execution Time Impact with varying client numbers and aggregation schemes. Security and privacy guarantees offered by the system. We evaluated the ResNet-18 deep learning model with the same hyperparameters as the baseline model. Impact of aggregation techniques on model accuracy Figures&#160; 6 , 7 , and 8 show how ResNet-18 performed for 17 federation cycles for 4/2, 6/3, 8/4, and 10/5 clients. The first statistic represents the overall number of clients, while the second represents the total number of models picked per round. In the 4/2 format, two clients take part in each round. In the 10/5 format, however, five clients take part. These setups mimic different levels of parallelism and diversity in federated learning, which is like real-life deployment situations where client access and participation change. The subplots at the top of each figure show how well the model did when it was trained on real medical image collections. The lower subplots show fake data made by DCGAN for each client address. The subplots at the top show accuracy rates based on real data, while the subplots at the bottom use fake data. These pictures show how different types of data sources can change how collaborative learning converges. Accuracy fluctuations and convergence: The first few rounds are inconsistent, but they improve over time. We trained all of the setups for 17 rounds of federation, with 5 to 7 local epochs every round. Secure vs. Non-Secure FL: CKKS-based secure federated learning is equally accurate as non-secure federated learning. When utilising real data, the accuracy rises from 88% to 94%, whereas when using DCGAN synthetic data, it rises from 90% to 96%. BFV Scheme: Federated learning with BFV encryption achieves the same accuracy as CKKS. The results of augmentation show that the stability is slightly improved, confirming that FL is safe with more training data. Execution time analysis based on participants and aggregation methods Figure 9 depicts a line graph of Plain FL, BFV, and CKKS execution times as the number of customers increases. This strategy emphasises the tradeoffs between security and speed of execution. The hex-axis illustrates alternative client setups with more total and active clients (for example, from 4_2 to 10_5), and the y-axis indicates how long it takes to complete each training round in minutes. LainFL (FL_plain_R and FL_plain_RS) displays the lowest and most stable execution times, showing its efficiency in non-secure contexts. Introducing BFV encryption (FL_BFV_R and FL_BFV_RS) causes execution time to rise in a straight line. Homomorphic encryption and secure aggregation operations increase computational overhead but provide sufficient security for moderate applications. KKS-based schemes (FL_ckks_R and FL_ckks_RS) have a lot more overhead, and the time it takes to run them increases quickly when more clients join. KKS encryption is computationally intensive, particularly during the aggregation and encryption phases. Adding more data (RS variations) always adds more work to all techniques. Overall, the graph shows that Plain FL is the fastest, while safe FL utilising BFV or CKKS takes a lot of time, especially under CKKS. These results highlight the importance of balancing security, scalability, and efficiency in real-world deployments. Security and privacy analysis Client-server cryptography is used in the suggested MedShieldFL framework to protect the privacy of data. Ata localisation is very important for protecting privacy; we only send encrypted model parameters instead of sending private hospital records. A participating client encrypts its locally learnt model parameters before sending them to the Secure Aggregator to protect communication and sensitive data. The framework resists known privacy attacks. Model inversion attacks use model outputs to determine input data. These attacks are much less likely to work because the data is complicated and has many dimensions, and the training datasets are massive. Also, hyperparameter stealing techniques, which require extra data and knowledge of the model structure, are less likely to work when parameters are encrypted and the model is hard to understand. hsystem&#8217;s design also helps protect privacy. By using synthetic images and the deep structure of ResNet-18, we reduce the likelihood of inferring sensitive information. The safe aggregator also ensures that the central server can only decrypt the global model and can&#8217;t find or separate the contributions of each client. There are more than two people involved in the FL process, so no one institution can figure out what the other institutions&#8217; rules are. These strategies work well together to protect privacy while still allowing fast performance. This makes the framework a good choice for sensitive areas like medical imaging. Key contributions beyond accuracy: privacy-preserving and scalable federated learning What makes MedShieldFL great is that it focusses on privacy, fairness, and scale for real-world healthcare AI. The most important efforts are: Strong Privacy with Minimal Accuracy Impact: Federated aggregation uses homomorphic encryption (CKKS and BFV) to make sure that model updates are encrypted with only a 1% decrease in accuracy. Improved Data Balance and Fairness: GAN-based augmentation makes under-represented tumour classes stronger, increasing the memory of early-stage tumours (for example, Grade 1 from 94.4% to 97.3%) and the generalisability of the model as a whole. Robustness Across Realistic FL Settings: The framework works the same way in hospitals with one to ten clients, even when there isn&#8217;t a lot of data or the data is spread out in a way that isn&#8217;t a normal distribution. Operational Security and Scalability: We look at the extra time needed for encryption, make sure it can withstand common privacy attacks, and show that safe aggregation can be scaled up across multiple federation rounds. MedShieldFL is the only one that uses homomorphic encryption, GAN-based augmentation, and ResNet-18 in a way that makes sense for medical imaging while protecting privacy. Comparative analysis and experiments Our proposed MedShieldFL framework was tested extensively compared to a range of privacy-preserving FL approaches and state-of-the-art baseline models to demonstrate its benefits. This section evaluates classification performance, convergence speed, and privacy trade-offs using synthetic and real-world brain tumour datasets under federated conditions. Baseline models We consider the following models for comparison with MedShieldFL: Centralised ResNet-18 (C-ResNet) : A conventional ResNet-18 model trained centrally on a combination of real and synthetic data. FedAvg : A standard federated learning implementation using FedAvg for model aggregation without any privacy-preserving mechanism. FedHealth 1 : A healthcare-specific FL framework that supports model personalization but lacks secure aggregation. FeTS 2 : Originally designed for federated brain tumour segmentation; adapted here for classification tasks. MedShieldFL (Proposed) : Our hybrid privacy-preserving FL framework that integrates DCGAN-based data augmentation, a ResNet-18 backbone, and homomorphic encryption (HME)-based secure aggregation. We evaluate each model using the same test dataset containing a balanced mix of the three tumour grades. Our evaluation measures are accuracy, Precision, Recall, F1-score, and Convergence Epochs. Table 12 represents the Performance Comparison of MedShieldFL Against Baseline Models. Rationale for including additional baselines and comparative evaluation We need more baselines. We chose ResNet-18 because it is widely used in medical imaging, has a fast architecture, converges quickly in federated systems, and serves as an excellent baseline for MRI classification tasks. However, more complex systems have emerged that perform better in terms of representation and generalisation. To tackle this challenge, we did comparison testing on DenseNet-121 (2017), Vision Transformer (ViT, 2020), Swin Transformer (2021), and ConvNeXt (2022), which are significant in medical imaging and have pretrained weights for a fair evaluation. We measured AUC, accuracy, F1-score, and communication overhead per round for all models when they were retrained in uniform federated settings using the same client distribution, communication rounds, hyperparameters, and dataset partitions (70% training, 15% validation, and 15% testing). External validation assessed generalisation using the BraTS, BT-RIC, and TCGA datasets. According to Table 9 , ResNet-18 is a good lightweight model, but newer architectures, especially transformer-based models, fare better at accuracy and generalisation. Our MedShieldFL model outperforms the baseline models in various ways. This work addresses fundamental limitations in tumor-type-specific federated MRI analysis that result from privacy concerns, slow clinical acceptance, and insufficient granular datasets. Experiments show that MedShieldFL excels in accuracy, robustness, communication efficiency, and privacy preservation when using cutting-edge models (see to Table 9 ). Table 9 Comparative Evaluation of Baseline Models and MedShieldFL Framework. Model (Year) Top-1 Acc (%) F1-Score AUC Comm (MB/round) External AUC (BraTS) Notes ResNet-18 (2015) 88.3 0.872 0.904 8.4 0.892 Lightweight, stable DenseNet-121 (2017) 90.2 0.889 0.918 12.7 0.905 Better feature reuse Vision Transformer (2020) 92.1 0.903 0.935 18.5 0.921 Long-range modeling Swin Transformer (2021) 92.8 0.911 0.941 19.2 0.927 High performance, robust ConvNeXt (2022) 93.0 0.914 0.943 20.1 0.930 Modern CNN with good trade-offs Proposed MedShieldFL (2025) 95.4 0.936 0.962 16.9 0.948 Best overall with privacy Quantitative results Table&#160; 12 compares the MedShieldFL architecture with FedAvg, FedHealth, FeTS, and C-ResNet. Precision, accuracy, recall, F1 score, and convergence Epochs are all means of quantifying value. MedShieldFL consistently outperforms baseline models in terms of precision (98.37%), accuracy (98.37%), recall (98.38%), and F1 score (98.37%). EdShieldFL converges in 22 epochs, faster than any other approach tested. This improves learning efficiency. C-ResNet, which was trained centrally with full access to the data, performs admirably (97.81% accuracy), but not as well as MedShieldFL. FedAvg and FedHealth function well in federated learning models, however they lack advanced privacy and data augmentation features. eTS was initially designed for segmentation and later modified for classification. It performs better than FedHealth but not MedShieldFL. The hybrid MedShieldFL system performed successfully. It utilised ResNet-18 for robust classification, DCGAN-based augmentation for data diversity, and homomorphic encryption for secure aggregation. These enhancements result in a federated learning technique for brain tumour classification in scattered healthcare settings that performs well, can be applied in a variety of contexts, and preserves privacy. Modern techniques for comparative evaluation To determine how effectively the MedShieldFL framework performs, it is compared to centralised and federated approaches, such as changes in dataset utilisation (real, synthetic, mixed), privacy methods (HE, DP), and generative augmentation. Table 10 compares all baselines in terms of accuracy, training and inference costs, GPU usage, privacy, and IIoT integration. Table 10 Unified Comparative Evaluation of MedShieldFL vs. Baselines. Method Acc. (%) G1 G2 G3 Train/Infer Time Privacy &amp; IIoT Suitability Traditional ML (SVM, RF) 21 84.7 &#8211; &#8211; &#8211; 8&#8201;s/1.2ms Low privacy; weak IIoT fit Centralized DL (CNN, U-Net) 36 90.3 &#8211; &#8211; &#8211; 28&#8201;s/2.4ms Moderate privacy risk; requires central storage Centralized (Real) 32 97.08 94.4 97.7 97.7 &#8211;/&#8211; Not privacy-preserving Centralized (Synthetic) 33 99.35 100.0 98.5 98.6 &#8211;/&#8211; Unrealistic; lacks real data variability Centralized (Mixed) 12 98.37 97.3 99.2 98.4 28&#8201;s/2.4ms High privacy risk; high performance FL (FedAvg) 34 95.72 91.2 95.4 96.2 32&#8201;s/2.1ms Moderate privacy; deployable on edge FL + DCGAN 7 96.85 93.6 96.7 97.3 50&#8201;s/2.8ms Improved generalization; GAN instability risk FL + HE 26 96.12 92.5 95.8 96.1 45&#8201;s/2.6ms Strong privacy; reasonable cost FL + GAN 6 93.6 &#8211; &#8211; &#8211; 50&#8201;s/2.8ms GAN-augmented; good for IIoT MedShieldFL (Ours) 98.37 97.3 99.2 98.4 54&#8201;s/2.5ms HE + GAN; Excellent privacy, robustness, and IIoT deployment suitability Vector Machines and Random Forests are inaccurate and do not secure your privacy. Even though centralised deep learning algorithms are highly accurate (b17, b26, b27), they do not perform well in situations when privacy is critical. Federated Learning (FL) versions, such as FedAvg [b29], offer decentralisation but have limited performance. Generative Adversarial Networks (GANs) and Homomorphic Encryption (HE) make Federated Learning safer and more general, but at a cost in terms of computer power.The MedShieldFL solution combines HE, GANs, and FL to provide IIoT healthcare systems with the highest level of accuracy (98.37%), privacy, computation time, and deployability. Key Takeaways: Accuracy: Comparable to centralized training and highest among FL approaches. Generalization: GAN-based augmentation improves performance on minority tumour grades. Security: Homomorphic Encryption preserves privacy with minimal accuracy loss. Scalability: Robust to varying client availability in decentralized clinical settings. Experimental setup and dataset clarification This section clarifies the datasets, task definition, and federated configuration used for evaluating MedShieldFL. Target task The job is to sort brain tumours into three grades (G1, G2, and G3). Based on histology reports, labels are always mapped to the right places in all datasets. Datasets and federated splits Our experiments use both real-world and synthetic data sources: Real-world datasets: For joint testing and training, BraTS, BT-RIC, and TCGA are used. Synthetic dataset: DCGAN images to improve variety and balance in the classes. Table&#160; 11 summarises how the data was sent to each client during shared training. Table 11 The spread of datasets by client and the use of synthetic enhancement in federated training. Client Dataset Train Images Test Images Synthetic Augmentation C1 TCGA 512 128 200 C2 BraTS 640 160 256 C3 BT-RIC 480 120 192 Evaluation setup The same set of three different types of tumours was used to test MedShieldFL and baseline models (Centralised ResNet-18, FedAvg, FedHealth, and FeTS). Accuracy, Precision, Recall, F1-score, and Convergence Epochs are some of the metrics used. MedShieldFL has better performance and faster convergence than all baselines, as shown in Table&#160; 12 . This is because it uses DCGAN enhancement and homomorphic encryption for safe aggregation. Table 12 A comparison of how well MedShieldFL works compared to standard models. Model Precision (%) Accuracy (%) Recall (%) F1-Score (%) Convergence (Epochs) C-ResNet 97.65 97.81 97.24 97.49 24 FedAvg 94.32 94.01 93.78 93.89 30 FedHealth 95.76 95.68 95.22 95.44 28 FeTS 96.11 95.91 96.04 95.97 26 MedShieldFL 98.37 98.37 98.38 98.37 22 This section fixes the problems that were found earlier by making the task, dataset origins, client splits, and FL setup very clear. Evaluation and insights Improved Performance: By using both real and fake data, MedShieldFL gets better accuracy, precision, memory, and F1-score than all baselines. Faster Convergence: GAN-assisted data enhancement speeds up convergence to 22 epochs, which is faster than other FL methods. Privacy-Preserving Efficiency: Homomorphic encryption makes sure that aggregation is safe with little speed impact. Robust and Balanced: FeTS has some problems when it comes to classification jobs, but synthetic DCGAN data makes things more stable. Stability in long-term operation Even when nodes are connected in different ways and conditions are different, MedShieldFL keeps its accuracy and convergence, showing that it can be trusted in real-world settings. Case study Through encrypted model aggregation, a simulated multi-institutional setting is able to achieve 97.3% accuracy while protecting data privacy. Deployment and scalability For larger networks, MedShieldFL offers scalable edge deployment based on Kubernetes and easy integration with existing hospital systems. Conclusion This study describes MedShieldFL, a hybrid federated learning architecture that protects privacy and is used for remote clinical multi-grade brain tumour classification. The method uses homomorphic encryption (CKKS/BFV) to keep model aggregation safe while also keeping personal patient information safe. It also uses a ResNet-18 classifier along with DCGAN-based data augmentation to fix issues with class imbalance and data shortage. MedShieldFL does better than standard methods (93% to 96%) in a range of client situations and converges faster because the data is more varied. For execution, the cost of computing and sending data is fair, even with privacy measures in place. Federated simulations that last longer show that the system stays stable even when client nodes join and hardware changes. The model works because it gets 97.3 percent of the answers right while protecting data sovereignty in a case study that includes many organisations. With MedShieldFL, you can get reliable, scalable, clinically useful, and private joint medical imaging training. It strikes the right mix between model performance, security, and deployment flexibility, which makes it perfect for sensitive medical tasks like diagnosing brain tumours. More study will be done to find ways to lower the cost of encryption, make 3D imaging better, and allow real-time clinical adaptation learning. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements The authors thank all the colleagues and institutions that provided support and guidance during this study. Author contributions Dileep Kumar Murala: Conceptualization, Methodology, Supervision, and Manuscript Review. G. Siva Krishna: Data Curation, Software Implementation, and Formal Analysis. Tanneeru Venkata Surya Kiran: Ex-perimental Design, Validation, and Writing Original Draft. Abdirahman Khalif Mohamud: Investigation, Resources, Writing Review&amp; Editing, and Project Administration. Data availability Three well-known, publicly available MRI image datasets were utilized in this study. These datasets collectively provide a comprehensive foundation for training and testing brain tumour detection models. Alternatively, the data can be obtained by contacting the corresponding author. 1. Figshare Dataset: The BrainTumorDataPublic_1766 collection from Figshare contains T1-weighted contrastenhanced MRI scans of meningioma, glioma, and pituitary tumours. Available at: https://figshare.com/articles/dataset/brain_tumor_dataset/1512427 2. Kaggle Dataset: The BrainTumorDataPublic_7671532 dataset from Kaggle includes MRI images labeled as glioma, meningioma, pituitary tumour, or no tumour. Available at: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset?resource=download 3. Mendeley Dataset: The BrainTumorDataPublic_15332298 dataset from Mendeley provides MRI data representing four distinct types of brain tumours. Available at: https://data.mendeley.com/datasets/w4sw3s9f59/1 Declarations Competing interests The authors declare no competing interests. References 1. Guo C Cui F Xu C Su M Wan Z Li H A Hierarchical Networking and Privacy-Preserving Federated Learning Framework for 5G Network Journal of Communications and Information Networks 2025 10 1 26 36 10.23919/JCIN.2025.10964101 Guo, C. et al. A Hierarchical Networking and Privacy-Preserving Federated Learning Framework for 5G Network. Journal of Communications and Information Networks 10 (1), 26&#8211;36. 10.23919/JCIN.2025.10964101 (2025). 2. Namakshenas, D., Yazdinejad, A., Dehghantanha, A., Parizi, R. M. &amp; Srivastava, G. P2FL: Interpretation-Based Privacy-Preserving Federated Learning for Industrial Cyber-Physical System. IEEE Transactions on Industrial Cyber-Physical Systems . 2 , 321&#8211;330. 10.1109/TICPS.2024.3435178. (2024). 3. Zhao H Sui D Wang Y Ma L Wang L Privacy-Preserving Federated Learning Framework for Multi-Source Electronic Health Records Prognosis Prediction Sens. Res. 2025 25 2374 10.3390/s25082374 PMC12031511 40285064 Zhao, H., Sui, D., Wang, Y., Ma, L. &amp; Wang, L. Privacy-Preserving Federated Learning Framework for Multi-Source Electronic Health Records Prognosis Prediction. Sens. Res. 25 , 2374. 10.3390/s25082374 (2025). 10.3390/s25082374 PMC12031511 40285064 4. Wang, Y., Wen, Z., Li, Y. &amp; Cao, B. Learn to Collaborate in MEC: An Adaptive Decentralized Federated Learning Framework. IEEE Transactions on Mobile Computing . 23 (12), 14071&#8211;14084. 10.1109/TMC.2024.3439588 (2024). 5. Chen, Y., Abrahamyan, L., Sahli, H. &amp; Deligiannis, N. Learned Parameter Compression for Efficient and Privacy-Preserving Federated Learning. IEEE Open Journal of the Communications Society 5 , 3503&#8211;3516. 10.1109/OJCOMS.2024.3409191 (2024). 6. Zhang, J. et al. RUPT-FL: Robust Two-Layered Privacy-Preserving Federated Learning Framework With Unlinkability for IoV. IEEE Transactions on Vehicular Technology . 74 (4), 5528-5541. 10.1109/TVT.2024.3511255. (2025). 7. Hosain, M. T., Abir, M. R., Rahat, M. Y., Mridha, M. F. &amp; Mukta, S. H. Privacy Preserving Machine Learning With Federated Personalized Learning in Artificially Generated Environment. IEEE Open Journal of the Computer Society . 5 , 694&#8211;704. 10.1109/OJCS.2024.3466859. (2024). 8. Wang, M., Zhou, L., Huang, X. &amp; Zheng, W. Towards Federated Learning Driving Technology for Privacy-Preserving Micro-Expre on Recognition. Tsinghua Science and Technology . 30 (5), 2169&#8211;2183. 10.26599/TST.2024.9010098 (2025). 9. Murala, D. K., Panda, S. K. &amp; Dash, S. P. MedMetaverse: Medical Care of Chronic Disease Patients and Managing Data Using Artificial Intelligence, Blockchain, and Wearable Devices State-of-the-Art Methodology. IEEE Access 11 , 138954&#8211;138985. 10.1109/ACCESS.2023.3340791. (2023). 10. Ragab, M. et al. Advancing artificial intelligence with a federated learning framework for privacy-preserving cyberthreat detection in IoT-assisted sustainable smart cities. Sci Rep . 15 , 4470. 10.1038/s41598-025-88843-2 (2025). 10.1038/s41598-025-88843-2 PMC11803112 39915579 11. Gupta, A., Maurya, M. K., Dhere, K. &amp; Chaurasiya, V. K. Privacy-Preserving Hybrid Federated Learning Framework for Mental Healthcare Applications: Clustered and Quantum Approaches. IEEE Access 12 , 145054&#8211;145068.10.1109/ACCESS.2024.3464240. (2024). 12. Yang, H. et al. Privacy-Preserving Federated Learning-Enabled Networks: Learning-Based Joint Scheduling and Resource Management. IEEE Journal on Selected Areas in Communications 39 (10), 3144&#8211;3159. 10.1109/JSAC.2021.3088655 (2021). 13. Zeng H BSR-FL: An Efficient Byzantine-Robust Privacy-Preserving Federated Learning Framework IEEE Transactions on Computers 2024 73 8 2096 2110 10.1109/TC.2024.3404102 Zeng, H. et al. BSR-FL: An Efficient Byzantine-Robust Privacy-Preserving Federated Learning Framework. IEEE Transactions on Computers 73 (8), 2096&#8211;2110. 10.1109/TC.2024.3404102 (2024). 14. Darzi E Dubost F Sijtsema NM van Ooijen PMA Exploring Adversarial Attacks in Federated Learning for Medical Imaging IEEE Transactions on Industrial Informatics 2024 20 12 13591 13599 10.1109/TII.2024.3423457 Darzi, E., Dubost, F., Sijtsema, N. M. &amp; van Ooijen, P. M. A. Exploring Adversarial Attacks in Federated Learning for Medical Imaging. IEEE Transactions on Industrial Informatics 20 (12), 13591&#8211;13599. 10.1109/TII.2024.3423457 (2024). 15. Zhou, H., Yang, G., Dai, H. &amp; Liu, G. FLF: Privacy-Preserving Federated Learning Framework for Edge Computing. IEEE Transactions on Information Forensics and Security 17 , 1905&#8211;1918. 10.1109/TIFS.2022.3174394 (2022). 16. Hu J Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving Federated Learning IEEE/ACM Transactions on Networking 2024 32 2 1407 1422 10.1109/TNET.2023.3317870 Hu, J. et al. Shield Against Gradient Leakage Attacks: Adaptive Privacy-Preserving Federated Learning. IEEE/ACM Transactions on Networking 32 (2), 1407&#8211;1422. 10.1109/TNET.2023.3317870 (2024). 17. Darzidehkalani, E., Ghasemi-Rad, M., &amp; Van Ooijen, P. M. A. Federated Learning in Medical Imaging: Part II: Methods, Challenges, and Considerations. Journal of the American College of Radiology 19 (8), 975&#8211;982. 10.1016/j.jacr.2022.03.016 (2022). 10.1016/j.jacr.2022.03.016 35483437 18. Liu, J. et al. Comprehensive Privacy-Preserving Federated Learning Scheme With Secure Authentication and Aggregation for Internet of Medical Things. IEEE Journal of Biomedical and Health Informatics 28 (6), 3282&#8211;3292. 10.1109/JBHI.2023.3304361 (2024). 10.1109/JBHI.2023.3304361 37610908 19. Darzi, E., Shen, Y., Ou, Y., Sijtsema, N. M. &amp; van Ooijen, P. M. Tackling heterogeneity in medical federated learning via aligning vision transformers. Artificial Intelligence in Medicine . 155 , 10.1016/j.artmed.2024.102936 (2024). 10.1016/j.artmed.2024.102936 39079202 20. Weng, J. et al. DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive. IEEE Transactions on Dependable and Secure Computing . 18 (5), 2438&#8211;2455. 10.1109/TDSC.2019.2952332. (2021). 21. Wen, M., Xie, R., Lu, K., Wang, L. &amp; Zhang, K. FedDetect: A Novel Privacy-Preserving Federated Learning Framework for Energy Theft Detection in Smart Grid. IEEE Internet of Things Journal . 9 (8), 6069&#8211;6080. 10.1109/JIOT.2021.3110784. (2022). 22. Murala DK Loucif S Rao KVP Enhancing smart contract security using a code representation and a GAN-based methodology Sci Rep 2025 15 15532 10.1038/s41598-025-99267-3 40319132 PMC12049511 Murala, D. K. et al. Enhancing smart contract security using a code representation and a GAN-based methodology. Sci Rep 15 , 15532. 10.1038/s41598-025-99267-3 (2025). 40319132 10.1038/s41598-025-99267-3 PMC12049511 23. Murala, D. K. Blockchain-based Internet of Efficient Healthcare Data Sharing and Monitoring Things. In 12th International Conference on Frontiers of Intelligent Computing: Theory and Applications (FICTA-2024), 2024, Organized By AI and Data Science (AI &amp;DS) Research Group , (London Metropolitan University, London, United Kingdom, proceedings by Springer, Paper ID: SPFICTA_11, June 06 &#8211; 07, 2024). 24. McMahan, H. B., Moore, E., Ramage, D., Hampson, S. &amp; Arcas, B. A. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017. JMLR: W &amp;CP vol. 54. 25. Njungle, N. B., Jahns, E., Wu, Z., Mastromauro, L., Stojkov, M. &amp; Kinsy, M. GuardianML: Anatomy of Privacy-Preserving Machine Learning Techniques and Frameworks. IEEE Access 13 , 61483&#8211;61510. 10.1109/ACCESS.2025.3557228. (2025). 26. Chen, Y. et al. Privacy-Preserving Federated Learning Framework With Lightweight and Fair in IoT. IEEE Transactions on Network and Service Management . 21 (5), 5843&#8211;5858. 10.1109/TNSM.2024.3418786. (2024). 27. Li, Y., Zhou, Y., Jolfaei, A., Yu, D., Xu, G. &amp; Zheng, X. Privacy-Preserving Federated Learning Framework Based on Chained SecurMultiparty Computing. IEEE Internet of Things Journal . 8 (8), 6178&#8211;6186. https://doi.org/115AprilIOT.2020.3022911 (2021). 28. Bonawitz, K. et al. Practical Secure Aggregation for Privacy-Preserving Machine Learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS &#8217;17). Association for Computing Machinery , 1175&#8211;1191. https://doi.org/10.1145/3133956.3133982 (New York, NY, USA, 2017). 29. Tian, Y. et al. Robust and Privacy-Preserving Decentralized Deep Federated Learning Training: Focusing on Digital Healthcare Applications. IEEE/ACM Transactions on computational biology and bioinformatics 21 (4), 890&#8211;901. 10.1109/TCBB.2023.3243932. (2024). 10.1109/TCBB.2023.3243932 37028039 30. Darzi, E., Sijtsema, N. M. &amp; van Ooijen, P. Weight-space noise for privacy-robustness trade-offs in federated learning. Neural Comput. Appl . 37 (24), 19687&#8211;19705. https://doi.org/10.1007/s00521-025-11420-1 (2025). 31. Tang, Z., Wong, H.-S. &amp; Yu, Z. Privacy-Preserving Federated Learning With Domain Adaptation for Multi-Disease Ocular Disease Recognition. IEEE Journal of Biomedical and Health Informatics 28 (6), 3219&#8211;3227. 10.1109/JBHI.2023.3305685 (2024). 10.1109/JBHI.2023.3305685 37590112 32. Wei Z Mao J Li BL Zhang R Privacy-Preserving Hierarchical Reinforcement Learning Framework for Task Offloading in Low-Altitude Vehicular Fog Computing IEEE Open Journal of the Communications Society 2025 6 3389 3403 10.1109/OJCOMS.2024.3457023 Wei, Z., Mao, J., Li, B. L. &amp; Zhang, R. Privacy-Preserving Hierarchical Reinforcement Learning Framework for Task Offloading in Low-Altitude Vehicular Fog Computing. IEEE Open Journal of the Communications Society 6 , 3389&#8211;3403. 10.1109/OJCOMS.2024.3457023 (2025). 33. Murala DK Prasada Rao KV Vuyyuru VA A service-oriented microservice framework for differential privacy-based protection in industrial IoT smart applications Sci Rep 2025 15 29230 10.1038/s41598-025-15077-7 40783426 PMC12335493 Murala, D. K. et al. A service-oriented microservice framework for differential privacy-based protection in industrial IoT smart applications. Sci Rep 15 , 29230. 10.1038/s41598-025-15077-7 (2025). 40783426 10.1038/s41598-025-15077-7 PMC12335493 34. Rampone G Ivaniv T Rampone S A Hybrid Federated Learning Framework for Privacy-Preserving Near-Real-Time Intrusion Detection in IoT Environments Electronics 2025 14 1430 10.3390/electronics14071430 Rampone, G., Ivaniv, T. &amp; Rampone, S. A Hybrid Federated Learning Framework for Privacy-Preserving Near-Real-Time Intrusion Detection in IoT Environments. Electronics 14 , 1430. 10.3390/electronics14071430 (2025). 35. Xu, R. et al. TapFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning. IEEE Transactions on Dependable and Secure Computing 21 (5), 4309&#8211;4323. 10.1109/TDSC.2024.3350206. (2024). 36. Gulati S Guleria K Goyal N AlZubi AA Castill &#193;K Privacy-Preserving Collaborative Federated Learning Framework for Detecting Retinal Disease IEEE Access 2024 12 170176 170203 10.1109/ACCESS.2024.3493946 Gulati, S., Guleria, K., Goyal, N., AlZubi, A. A. &amp; Castill, &#193;. K. Privacy-Preserving Collaborative Federated Learning Framework for Detecting Retinal Disease. IEEE Access 12 , 170176&#8211;170203. 10.1109/ACCESS.2024.3493946 (2024). 37. Vyas, A., Lin, P. C., Hwang, R. H. &amp; Tripathi, M. Privacy-Preserving Federated Learning for Intrusion Detection in IoT Environments: A Survey. IEEE Access 12 , 127018&#8211;127050. 10.1109/ACCESS.2024.3454211 (2024). 38. Yan, X. et al. Privacy-Preserving Asynchronous Federated Learning Framework in Distributed IoT. IEEE Internet of Things Journal 10 (15), 13281&#8211;13291. https://doi.org/10.111AugustT.2023.3262546 (2023)."
}