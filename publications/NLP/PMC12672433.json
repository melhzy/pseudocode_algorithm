{
  "pmcid": "PMC12672433",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:24.248920",
  "metadata": {
    "journal_title": "Frontiers in Artificial Intelligence",
    "journal_nlm_ta": "Front Artif Intell",
    "journal_iso_abbrev": "Front Artif Intell",
    "journal": "Frontiers in Artificial Intelligence",
    "pmcid": "PMC12672433",
    "pmid": "41346860",
    "doi": "10.3389/frai.2025.1697169",
    "title": "Advancing engineering research through context-aware and knowledge graph–based retrieval-augmented generation",
    "authors": [
      "Ghosh Soham",
      "Mittal Gaurav"
    ],
    "abstract": "Large language models (LLMs) are powerful in language understanding and content generation but frequently fall short of technical accuracy when they are applied to engineering code, standards, and design documents. To mitigate this, we are seeing the emergence of Retrieval-Augmented Generation (RAG) models that ground outputs of LLMs with information from external trustworthy resources, increasing the factual consistency. However, traditional RAG techniques are limited in the treatment of isolated information (limited to the amount of information in a fixed-size chunk) and are deemed ill-equipped to traverse semantically linked technical information. This study introduces a collection of new and highly deployable RAG-LLMs built on the n8n automation system and specifically designed for engineering domains. Framework effectiveness was tested on a set of prompts developed with the help of practicing electrical engineering professionals and should be read through the framework’s lens for interpretation of national engineering codes, technical standards, and design standards. To mitigate the shortcomings of the conventional retrieval-based chunking methods, a contextual RAG-based approach is employed to align the retrieved content with the query context to improve relevance. Moreover, RAG is adopted to structure knowledge graph retrieval, which can retrieve densely linked concepts from multiple knowledge graphs, thereby promoting more profound semantic understanding in complex technical domains. The study describes the relative benefits of these improvements, points to practical deployment issues, strengths, and weaknesses. All the n8n workflows employed in this study are made available as supplementary materials to facilitate reproducibility and sharing within the engineering research community and practitioners.",
    "keywords": [
      "context-aware information retrieval",
      "RAG architectures",
      "engineering design automation",
      "knowledge graphs",
      "LLM and intelligence"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Front Artif Intell</journal-id><journal-id journal-id-type=\"iso-abbrev\">Front Artif Intell</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3981</journal-id><journal-id journal-id-type=\"pmc-domain\">frontai</journal-id><journal-id journal-id-type=\"publisher-id\">Front. Artif. Intell.</journal-id><journal-title-group><journal-title>Frontiers in Artificial Intelligence</journal-title></journal-title-group><issn pub-type=\"epub\">2624-8212</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12672433</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12672433.1</article-id><article-id pub-id-type=\"pmcaid\">12672433</article-id><article-id pub-id-type=\"pmcaiid\">12672433</article-id><article-id pub-id-type=\"pmid\">41346860</article-id><article-id pub-id-type=\"doi\">10.3389/frai.2025.1697169</article-id><article-version-alternatives><article-version article-version-type=\"pmc-version\">1</article-version><article-version article-version-type=\"Version of Record\" vocab=\"NISO-RP-8-2008\"/></article-version-alternatives><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Advancing engineering research through context-aware and knowledge graph&#8211;based retrieval-augmented generation</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Ghosh</surname><given-names initials=\"S\">Soham</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3191157\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Mittal</surname><given-names initials=\"G\">Gaurav</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">\n<sup>3</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3268362\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role></contrib></contrib-group><aff id=\"aff1\"><label>1</label><institution>Department of Electrical Engineering, Black &amp; Veatch</institution>, <city>Overland Park, KS</city>, <country country=\"us\">United States</country></aff><aff id=\"aff2\"><label>2</label><institution>IEEE IAS/PES Kansas City Section</institution>, <city>Kansas City, Kansas,</city><country country=\"us\">United States</country></aff><aff id=\"aff3\"><label>3</label><institution>Enterprise Solutions Architect, Black &amp; Veatch</institution>, <city>Overland Park, KS</city>, <country country=\"us\">United States</country></aff><author-notes><corresp id=\"c001\"><label>*</label>Correspondence: Soham Ghosh, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:sghosh27@ieee.org\">sghosh27@ieee.org</email></corresp></author-notes><pub-date publication-format=\"electronic\" date-type=\"pub\" iso-8601-date=\"2025-11-19\"><day>19</day><month>11</month><year>2025</year></pub-date><pub-date publication-format=\"electronic\" date-type=\"collection\"><year>2025</year></pub-date><volume>8</volume><issue-id pub-id-type=\"pmc-issue-id\">481073</issue-id><elocation-id>1697169</elocation-id><history><date date-type=\"received\"><day>01</day><month>9</month><year>2025</year></date><date date-type=\"accepted\"><day>28</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>19</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>04</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 09:25:13.193\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Ghosh and Mittal.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Ghosh and Mittal</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\" start_date=\"2025-11-19\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"frai-08-1697169.pdf\"/><abstract><p>Large language models (LLMs) are powerful in language understanding and content generation but frequently fall short of technical accuracy when they are applied to engineering code, standards, and design documents. To mitigate this, we are seeing the emergence of Retrieval-Augmented Generation (RAG) models that ground outputs of LLMs with information from external trustworthy resources, increasing the factual consistency. However, traditional RAG techniques are limited in the treatment of isolated information (limited to the amount of information in a fixed-size chunk) and are deemed ill-equipped to traverse semantically linked technical information. This study introduces a collection of new and highly deployable RAG-LLMs built on the n8n automation system and specifically designed for engineering domains. Framework effectiveness was tested on a set of prompts developed with the help of practicing electrical engineering professionals and should be read through the framework&#8217;s lens for interpretation of national engineering codes, technical standards, and design standards. To mitigate the shortcomings of the conventional retrieval-based chunking methods, a contextual RAG-based approach is employed to align the retrieved content with the query context to improve relevance. Moreover, RAG is adopted to structure knowledge graph retrieval, which can retrieve densely linked concepts from multiple knowledge graphs, thereby promoting more profound semantic understanding in complex technical domains. The study describes the relative benefits of these improvements, points to practical deployment issues, strengths, and weaknesses. All the n8n workflows employed in this study are made available as supplementary materials to facilitate reproducibility and sharing within the engineering research community and practitioners.</p></abstract><kwd-group><kwd>context-aware information retrieval</kwd><kwd>RAG architectures</kwd><kwd>engineering design automation</kwd><kwd>knowledge graphs</kwd><kwd>LLM and intelligence</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that no financial support was received for the research and/or publication of this article.</funding-statement></funding-group><counts><fig-count count=\"8\"/><table-count count=\"6\"/><equation-count count=\"0\"/><ref-count count=\"24\"/><page-count count=\"18\"/><word-count count=\"12095\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Machine Learning and Artificial Intelligence</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1\"><label>1</label><title>Introduction</title><p>Over the past half a decade, large language models (LLMs) have revolutionized many fields, ranging from natural language processing to artificial intelligence. However, despite their extraordinary linguistic skills, LLMs are known to be highly prone to hallucinations, i.e., outputs that are factually wrong or non-sensible for no evident reason. This is due to several challenges of LLM design and operation, which include but are not limited to the use of large training datasets that are often unfiltered, and to the auto-regressive nature of these models, which can cause generated content to be inconsistent (<xref rid=\"ref14\" ref-type=\"bibr\">Li et al., 2023</xref>; <xref rid=\"ref18\" ref-type=\"bibr\">Roustan and Bastardot, 2025</xref>). As such, LLM hallucinations can take forms such as fake evidence, false inference, and totally wrong replies. These hallucinations can be dangerous in critical applications like engineering, reasoning, and diagnosis, and information retrieval (<xref rid=\"ref24\" ref-type=\"bibr\">Zhang et al., 2023</xref>). For example, one study shows that the hallucination rate for the different LLMs ranges between 50 and 82% depending on the operational settings, emphasizing the prevalence of such a phenomenon in machine-produced texts (<xref rid=\"ref15\" ref-type=\"bibr\">Omar et al., 2025</xref>).</p><p>Because hallucinations can have severe consequences, particularly in fields like engineering, healthcare, and law, there is a pressing need to explore new approaches that make LLM outputs more dependable. One approach with potential is Retrieval Augmented Generation (RAG), which integrates LLMs&#8217; skills with retrieval-based approaches to enhance correctness and relevance. RAG enables LLMs to dynamically and modularly include external sources into the generation flow, which greatly mitigates the limitation of being confined to pre-defined training datasets (<xref rid=\"ref10\" ref-type=\"bibr\">Gopi et al., 2024</xref>; <xref rid=\"ref20\" ref-type=\"bibr\">Siddharth and Luo, 2024</xref>). In an applied context, leveraging RAG technology is observed to significantly enhance the capabilities of LLMs in emergent scenarios such as rapid identification of relevant clauses in engineering design code and standard or query-based knowledge exploration for an engineering document cluster. By allowing LLMs to access additional information, a RAG system increases coherence and factuality of outputs, demonstrating their wide applicability across different high-stakes settings (<xref rid=\"ref21\" ref-type=\"bibr\">Superbi et al., 2024</xref>).</p><p>In addition, the recent developments in the comprehension and management of hallucinations relate to the increase in the use of RAG techniques. Using RAG mechanisms (which mix generative capabilities with well-defined retrieval processes), LLMs can self-repair incorrect results by referencing (on-the-fly access) information fetched dynamically from reliable databases. This not only provides the means for a more dependable computational environment but also enhances user confidence in intelligent systems (<xref rid=\"ref6\" ref-type=\"bibr\">Chen et al., 2023</xref>). As LLMs develop, the integration of RAG methods seems to be an important move in order to guarantee that such technologies are capable of generating suitable, context-sensitive, and reliable answers in a growing variety of use cases. Although retrieval-augmented generation (RAG) models can address the aforementioned problems by grounding answers in well-documented sources, the actual implementations of RAG for engineering remain underdeveloped, and there are a number of practical challenges hindering their implementation. These are as follows:</p><list list-type=\"order\"><list-item><p>The barrier to entry is still quite high for the RAG-LLM combination, given that building a well-performing RAG-LLM pipeline usually demands a good deal of programming expertise, which may not be available to many engineering professionals and researchers.</p></list-item><list-item><p>For an RAG-LLM application to truly be successful, it should have the ability to access a user&#8217;s existing document repository in a manner similar to popular applications like Google Drive or SharePoint. Unfortunately, building such integrations is never that simple, and would likely add layers of complexity, such as us having to implement authentication protocols, configure a set of APIs, and understand how to parse a document.</p></list-item><list-item><p>In order to make a RAG pipeline work well, one often also has to adopt more advanced tweaks, such as reranking strategies, context retrieval mechanisms, or knowledge graph-based augmentation. While those approaches increase retrieval precision and relevance of output, they tend to be technically challenging and are based on the use of several NLP tools and frameworks. Altogether, these subtleties limit the accessibility and effective usage of RAG-LLM systems for numerous engineers and applied researchers.</p></list-item></list><p>Real-world engineering research and application have faced these obstacles down, and many practitioners have included them as &#8220;given&#8221; variables. To mitigate the high entry barrier and technical complexities, a fast-deployable recipe has been articulated employing open LLMs and modular RAG pipelines and enjoying simple deployment capabilities at accessible platforms such as LM Studio, AnythingLLM, and n8n. This eliminates the requirement for elaborate programming and allows practitioners to remain focused on the engineering content and not the software infrastructure. A number of workflow implementations have been showcased in the n8n automation framework, including document retrieval, reranking, contextual querying, and knowledge graph integration. Performance testing has been performed to a large extent, based on engineering documents, technical standards, and literature from, e.g., the National Fire Protection Association (NFPA), the Institute of Electrical and Electronics Engineers (IEEE), and the International Council on Large Electric Systems (CIGRE). The findings validate that a rapidly deployable RAG-LLM design can achieve good usability and high performance in technically challenging domains.</p><sec id=\"sec2\"><label>1.1</label><title>Manuscript contribution</title><p>This study pushes forward the methodology when it comes to the development of rapidly deployable retrieval-augmented generation (RAG) systems and evaluation of technical document querying tuned for practicing power system professionals. The specific contributions are as follows:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">RAG-LLM pipeline implementation:</italic> Example of a rapid deployable RAG-LLM pipeline, using LM Studio and AnythingLLM to secure local document retrieval and language model inference is provided. The code to be used for parsing and embedding corpus-specific documents is released and available through the &#8216;Data Availability&#8217; section of the article.</p></list-item><list-item><p><italic toggle=\"yes\">Practitioner-oriented evaluation and design considerations:</italic> The manuscript frames the design and evaluation of RAG workflows with a focus on practitioner guidance, particularly for electrical engineers and applied researchers working in rapidly deployable environments such as n8n. Special attention is given to the challenges encountered by traditional RAG systems when processing dense tabular data and multi-layered exceptions commonly found in technical codes and standards.</p></list-item><list-item><p><italic toggle=\"yes\">Contextual retrieval workflow via n8n:</italic> We introduce a novel RAG contextual workflow implemented in n8n, which supports better processing of tabular structures and &#8216;exception logic&#8217; by orchestration of document processing and dynamic retrieval logic. The full workflow is released to the public and is available through &#8216;Data Availability&#8217; for reproducibility and the enablement of practical use.</p></list-item><list-item><p><italic toggle=\"yes\">Knowledge graph-based retrieval workflow via n8n and Infranodus:</italic> We present a state-of-the-art knowledge graph-augmented RAG workflow leveraging n8n and Infranodus to semantically structure engineering documents and reveal hidden relationships among engineering concepts. This method allows developers to make exploratory queries and cross-reference between linked standards, which is beneficial for complex design and compliance activities. The full process, from graph construction to integration and retrieval logic, is released to the public in the &#8216;Data Availability&#8217; section for reproducibility and to serve as a practical tool.</p></list-item></list></sec><sec id=\"sec3\"><label>1.2</label><title>Manuscript organization</title><p>The remainder of the manuscript is structured as follows: in Section 2, we describe the basics of Retrieval Augmented Generation (RAG), which presents the basic building blocks of the RAG system and a typical structure of the RAG system architecture. Section 3 describes a simple-to-adopt private RAG framework constructed from open-source LLMs, which uses LM Studio and AnythingLLM to be deployed locally. In Section 4, we focused on quickly deployable integration of n8n for document retrieval, performing an in-depth evaluation and ranked prompts based on a question set built from the NFPA 70 National Electrical Code (NEC) 2017. This section also highlights important observations about when traditional RAG-LLM-based pipelines do and do not perform well. Section 5 mitigates the above restrictions via targeted improvements in the area of reranking and contextual RAG workflows. We show scores in comparison to various rerankers (e.g., bge-reranker-base and Cohere Reranker v3. 5) over various embedding models such as OpenAI and Voyage. As an advanced use case, Section 6 discusses a multi-brain RAG implementation of a state-of-the-art knowledge graph-based RAG system and how it can be applied for exploratory querying on a network of engineering standards. Practical guidelines for reducing the consumption and related costs of APIs are presented. Finally, Section 7 summarizes with a summary of our findings, the current limitations, and future work ahead.</p></sec></sec><sec id=\"sec4\"><label>2</label><title>Fundamentals of retrieval-augmented generation (RAG) and present limitations</title><p>Large language models (LLMs), such as GPT-4 and LLaMA, have demonstrated remarkable capabilities at generating coherent, contextually appropriate text. They have an impressive flaw, though they often &#8220;hallucinate,&#8221; generating plausible-sounding but incorrect (or impossible to verify) information. This observation is due to the fact that LLMs produce outputs by relying on statistical associations in their training data and not on grounded truth or domain-specific knowledge. To address this limitation, retrieval-augmented generation (RAG), a novel architectural style, has emerged as a compelling approach. With RAG, the generation process is combined with a retrieval module to dynamically retrieve relevant documents or context passages from an external knowledge source. This is similar in spirit to giving a model access to a domain-specific textbook at test time, to consult ground truth sources before generating answers.</p><p>By building retrieval directly into the generation loop, as illustrated in <xref rid=\"fig1\" ref-type=\"fig\">Figure 1</xref>, a RAG system can largely improve the end quality of correctness and domain relevance. For example, in power systems engineering or regulatory compliance, where specific terms and references are required, RAG provides answers that are tied to source materials collected for that domain. This renders RAG especially well-suited for use cases where there is a high emphasis on knowledge-intensive question answering, code documentation, engineering reasoning, and queries, and where hallucinations can be high-stakes. Nevertheless, RAG is not a replacement for domain adaptation or continual learning. It does not allow a model to generalize new syntactic constructions, programming languages, or stylistic conventions. Instead, it offers an efficient amplification process for enhancing response accuracy within a predefined information domain. In this way, RAG facilitates retrieval, and not representation, which is more suitable for fine-grained applications rather than general domain training.</p><fig position=\"float\" id=\"fig1\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Illustration of RAG architecture, where a user query initiates a retrieval process from a knowledge base (e.g., vector database or document store).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g001.jpg\"><alt-text content-type=\"machine-generated\">Diagram illustrating retrieval-augmented generation. A user query feeds into a retrieval system that finds top relevant documents from a corpus. The retrieved context and query are then used by a generator model to output a response.</alt-text></graphic></fig><p>Although hosted LLM interfaces like ChatGPT offer the ability for a user to upload domain-specific documents (e.g., an electronic engineering textbook or an IEEE master volume) and then engage in retrieval-augmented querying, there are multiple limitations when one has to resort to a cloud-based service for professional or industrial use. These are as follows:</p><list list-type=\"order\"><list-item><p>Data sovereignty and confidentiality, become a pressing concern. Uploaded documents are outside of the control of personal or corporate workstations, effectively by-passing internal policies on data handling, intellectual property agreements, or even regulatory requirements, especially when proprietary designs, sensitive specs, or restricted standards are concerned.</p></list-item><list-item><p>Hosted models often tend to have limits on the length of tokens and contexts that make it hard for technology to work. For instance, in the case of a long publication such as a multi-chapter standard or a big handbook, the material may go above the maximum token limit, even with approaches for breaking up and summarizing documents. This might lead to insufficient context or retrieval and may make response generation less accurate.</p></list-item><list-item><p>In the case of hosted solutions, there is usually no persistent user-manageable storage of the parsed knowledge. After the document is uploaded and used for one session, the embedding and index layers are transient and not seen (allowed access to) by the user. This makes it impossible for engineers to construct long-term, reusable vector databases that can adapt to the progress of ongoing projects or to organizational requirements in terms of the knowledge needed.</p></list-item><list-item><p>The other limitation is the higher cost of operation, and this is more applicable to frequent or large-scale document processing. Subscription fees, usage-based pricing, and data upload limitations while attempting to submit jobs to external services can add up to prevent hosted LLM services at scale economically feasible for long-term engineering workflows or enterprise integration.</p></list-item></list><p>These shortcomings also reveal the importance of self-hosted RAG pipelines, i.e., a system where people could download LLMs themselves and run them locally, parse their own engineering documents, and keep the embeddings in a vector database of their selection (e.g., FAISS, Pinecone, or Qdrant). This kind of configuration gives users full access to the retrieval layer, the ability to customize indexing strategies, and integrate with in-house tools or platforms. In the context of electrical engineering, this means sensitive design documents, grid planning standards, or equipment protocols can be queried securely, reliably, and repeatedly, without using third-party APIs or external cloud services. It also provides the opportunity for sensitive document access policies, auditability, and retention of understanding over time, which is important in high-assurance engineering contexts.</p></sec><sec id=\"sec5\"><label>3</label><title>Opensource LLM and private RAG-LLM pipeline for engineering applications</title><p>Open-source large language models (LLMs) are especially attractive in that their deployments can be conducted locally on premises, such that users can locally perform inference in a secure and private way and without relying on proprietary API endpoints or any external, possibly cloud-based, service. This is especially useful in retrieval-augmented generation (RAG) tasks in engineering domains where data privacy, reproducibility, and system-level control are important. Engineers can consult the Hugging Face Open LLM Leaderboard (<xref rid=\"ref11\" ref-type=\"bibr\">Huggingface, 2025</xref>) to find candidate models for local deployment. Through the filtering into &#8220;mid-range&#8221; models (typically 14B-32B range as shown in <xref rid=\"tab1\" ref-type=\"table\">Table 1</xref>), users can choose those models that compromise between inference performance and hardware feasibility.</p><table-wrap position=\"float\" id=\"tab1\" orientation=\"portrait\"><label>Table 1</label><caption><p>Open LLM leaderboard filtered by &#8216;mid-range&#8217; models and sorted by weighted average of normalized scores from all benchmarks (as of June 2025).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Average score across all benchmarks</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">IFEva<sup>&#8224;</sup></th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">GPQA<sup>&#8225;</sup></th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CO<sub>2</sub> cost</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Qwen2.5-test-32b-it</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>47.37%</bold>\n</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">78.89%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">15.21%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">29.54&#8239;kg</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Horizon-AI-Avengers-V1-32B</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">47.34%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">79.72%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">14.99%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">7.95&#8239;kg</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">FluentlyLM-Prinum</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">47.22%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">80.90%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">18.23%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">21.25&#8239;kg</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Qwen.5-14b-v1.0-e3</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">47.09%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">73.24%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">22.26%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>1.56&#8239;kg</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Qwen2.5-32B-Instruct-abliterated-v2</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">46.89%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">83.34%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">15.66%</td><td align=\"char\" valign=\"middle\" char=\".\" rowspan=\"1\" colspan=\"1\">13.49&#8239;kg</td></tr></tbody></table><table-wrap-foot><p><sup>&#8224;</sup>Instruction-following evaluation (IFEval): This metric tests the model&#8217;s ability to follow explicit formatting instructions.</p><p><sup>&#8225;</sup>Graduate-level Google-Proof Q&amp;A (GPQA): This metric scores the LLM performance based on PhD-level knowledge multiple-choice questions in science.</p><p>Boldface scores represent the highest scores or the lowest carbon footprint.</p></table-wrap-foot></table-wrap><p>The private and locally executed RAG-LLM leverages two open source softwares, namely LM Studio and AnythingLLM; see <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>. Within LM Studio, users can download target LLM models from open source and launch them in their quantized formats (e.g., GGUF) without extra tuning. The settings that may be tuned are those of the system prompt, randomness temperature, and top-K sampling parameter. For an entire RAG pipeline, a second open-source application, AnythingLLM, is leveraged for establishing a local retrieval work environment. PDF files consisting of IEEE standards, electrical codes, or equipment manuals have to be initially changed into the Markdown (. md) and one may upload them to AnythingLLM. Markdown works better for RAG because it provides clean, structured, and token-efficient text that enables accurate chunking and retrieval, unlike the noisy and layout-heavy content in PDFs.</p><fig position=\"float\" id=\"fig2\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Illustration of a private and locally executed RAG-LLM pipeline using LM Studio and AnythingLLM.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g002.jpg\"><alt-text content-type=\"machine-generated\">Diagram illustrating a workflow involving document types (DOC, PDF, PPT, XLS) parsed to Markdown files for a RAG workspace called Anything LLM. User-provided prompts and model parameters interact with a locally hosted LLM at IP 127.0.0.1:1234 powered by LM Studio. User queries go to Anything LLM, with output generated from a RAG chatbot.</alt-text></graphic></fig><p>AnythingLLM also permits users to set the &#8216;Text Chunk Size&#8217; and &#8216;Text Chunk Overlap&#8217;, which are crucial parameters that determine how fine and how much of the context is preserved in document embeddings. AnythingLLM communicates with LM Studio (which is (supposed to be) running locally), in which we embed the LLM, and it makes requests to the cached model to generate text in response to questions about the embedded content. Such a setup guarantees that the model and the knowledge base are local to the user&#8217;s computer, and that the user remains in complete control of the data, the execution, and the experimental reproducibility, without the need to rely on cloud inference or third-party services.</p></sec><sec id=\"sec6\"><label>4</label><title>Rapidly deployable integration with n8n for document retrieval and framework limitations</title><p>Although LM Studio and AnythingLLM provide a fast, secure, and locally deployable solution for developing retrieval-augmented generation (RAG) systems, it comes with limitations in workflow automation, integration, and orchestration capability. For more advanced use cases, when integrating document ingestion, vector store, and even multi-channel query, a flexible, growable automation frame is required. Which is precisely where n8n, a free and open-source, node.js-based workflow automation tool, excels. User of n8n can create automated workflows that incorporate AI with corporate-level business process automation. It runs on-premises, which guarantees data privacy and allows for custom logic, APIs, and plugins, so it is appropriate for engineering, document intelligence, and RAG applications.</p><p><xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref> shows an example RAG workflow using n8n based on the use case of this paper. The workflow functions as follows:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Document upload:</italic> Engineers/technical professionals upload documents such as PPT, PDF, Datasheets, Word-based Specifications, or Manuals built as per the standards in 1 above, into a Google Drive folder assigned for them.</p></list-item><list-item><p><italic toggle=\"yes\">Automated ingestion:</italic> n8n monitors the Google Drive folder for new uploads. When a file is discovered, it initiates a processing pipeline to extract the content, optionally parse it into Markdown or plain text, and embed the text using the user-provided embedding model.</p></list-item><list-item><p><italic toggle=\"yes\">Vector storage:</italic> The resulting representations are saved in a vector database (can be local or cloud-hosted), such as Qdrant, Chroma, Pinecone, to be queried against later.</p></list-item><list-item><p><italic toggle=\"yes\">Query interface:</italic> The end users pose queries using different interfaces such as Gmail, Slack, or a web-based chatbot. There, an n8n agent returns the top-K related chunks from the vector database.</p></list-item><list-item><p><italic toggle=\"yes\">LLM generation:</italic> The resulting snippets are sent to the LLM of the user&#8217;s preference1, which is hosted in a local machine to generate a grounded, context-aware reply.</p></list-item></list><fig position=\"float\" id=\"fig3\" orientation=\"portrait\"><label>Figure 3</label><caption><p>n8n RAG-LLM workflow with a Google Drive-based retrieval system offering a user-friendly framework for engineers and researchers.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g003.jpg\"><alt-text content-type=\"machine-generated\">Flowchart depicting a system for updating a vector database and retrieving top-k matches. The upper section shows a process triggered by Google Drive, where a document is downloaded and embedded into the Pinecone vector store using OpenAI embeddings. A default data loader and recursive character text splitter are involved. The lower part shows integration with an AI agent that receives chat messages, utilizes OpenAI chat models for memory and tools, and interacts with the Pinecone vector store to retrieve embedded data.</alt-text></graphic></fig><p>This modular and event-driven design further decouples ingestion and querying phases, and allows users to automate multi-step, cross-platform knowledge workflows, resulting in a more scalable and maintainable RAG-LLM solution in engineering settings.</p><p>In order to demonstrate the effectiveness of the proposed RAG-LLM pipeline, we performed an experiment on a collection of curated, domain-specific search queries to the National Electrical Code (NEC), a complex regulatory document relied on by practicing electrical engineers. The NEC codebook (in parsed text) was uploaded by the user to the RAG system, allowing for grounded-document retrieval for inference. The evaluation searches were specified to check if the system can:</p><list list-type=\"order\"><list-item><p>Locate and extract numerical values from NEC tables (e.g., minimum burial depths, conductor ampacity ratings, and overcurrent protection limits).</p></list-item><list-item><p>Extract exceptions and conditional clauses from the codebook, as they frequently manifest as annotations or as structural deviations from the norm.</p></list-item><list-item><p>Retrieve the relevant sections and apply necessary contextual logic (e.g., identifying requirements that vary by installation type, voltage class, or application environment).</p></list-item></list><p>Each generated response was evaluated against user-defined ground truth answers, see <xref rid=\"tab2\" ref-type=\"table\">Table 2</xref>, simulating expert validation. For scoring, we used the correctness measure of DeepEval with GEval criteria, which assesses semantic alignment, factual grounding, and contextual completeness between the model&#8217;s response and the reference answer.</p><table-wrap position=\"float\" id=\"tab2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Validation of traditional RAG-LLM responses using an n8n workflow on query tasks based on the 2017 National Electrical Code (NEC).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Question and ground truth</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Actual output and evaluation</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q1. Prompt:</bold> I am trying to find the table in the NEC that deals with a single-phase AC motor and the associated full load current.<break/><bold>A1. Ground truth:</bold> Table 430.248 of the NEC addresses single-phase AC motors. It also addresses the corresponding full load current.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A1. Actual output:</bold> The NEC section is Article 430. This section is titled &#8220;Motors, Motor Circuits, and Controllers.&#8221; Do refer to Table 430.248 of Article 430 to determine the minimum full-load current required for a single-phase AC motor, as indicated by the motor&#8217;s nameplate rating.<break/><bold>Score: 0.9072185128712421</bold><break/><bold>Reason:</bold> The output was found to correctly cite NEC Table 430.248. It aligns with the expected outcome. It additionally provides supplementary context by citing Article 430 and its title, both of which augment factual accuracy and comprehensiveness. The supplementary detail slightly exceeds the expected simplicity; yet, there are no discrepancies or omissions.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q2. Prompt:</bold> I want to know what is the minimum cover required per Section 300.5 of the NEC, for direct buried cables running under a parking lot, based on Table 300.5.<break/><bold>A2. Ground truth:</bold> According to Table 300.5 of the NEC, the minimum cover necessary for a direct buried cable beneath a parking lot is 24 inches.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A2. Actual output:</bold> According to NEC Table 300.5, the minimum burial depth for a direct-buried cable installation under a parking lot is 24 inches (609&#8239;mm). This relates to the upper surface of the finished grade, and further criteria may be considered when determining the appropriate cover for your specific installation.<break/><bold>Score: 0.859266659995407</bold><break/><bold>Reason:</bold> The actual production accurately determines the necessary minimum cover as 24 inches, consistent with the anticipated output. It offers supplementary context by specifying the measurement in millimeters and acknowledging that other factors may affect the cover, which, although not included in the anticipated output, does not contradict it. The information is accurate and complete; nonetheless, the additional details somewhat diverge from the anticipated conciseness.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q3. Prompt:</bold> In the NEC, which section deals with the minimum cover requirement for cables and conduits.<break/><bold>A3. Ground truth:</bold> In the NEC, section 300.5 deals with the minimum cover requirement for cables and conduits.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A3. Actual output:</bold> The minimum cover requirements for cables and conduits are specified in Section 300.5 of the NEC. This section delineates the minimum cover requirements for direct-buried cables, conduits, or other raceways. Furthermore, additional information concerning mechanical protection for direct-buried cables can be found in sections 840.48 and 830.47 of the NEC.<break/><bold>Score: 0.8009300906486064</bold><break/><bold>Reason:</bold> The output accurately identifies Section 300.5 of the NEC as pertaining to minimum cover requirements for cables and conduits. It also offers further information regarding related areas, which is not mandatory but does not contradict the anticipated outcome. There are no factual flaws or omissions about the basic topic, but the other facts, albeit accurate, are superfluous to the specific inquiry.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q4. Prompt:</bold> Is there any restriction on running conductors in parallel in the NEC.<break/><bold>A4. Ground truth</bold>: NEC section 310.10(H) talks about conductors in parallel.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A4. Actual output:</bold> Indeed, Section 310.10(H) of the NEC permits the installation of parallel conductors in raceways, multiconductor cables, or direct-buried single-conductor cables.<break/><bold>Score: 0.5432697039021899</bold><break/><bold>Reason:</bold> The actual output accurately recognizes NEC section 310.10(H) as pertinent to parallel conductors, consistent with the anticipated output. However, the output introduces supplementary details regarding installation methods that are not in the anticipated output. These additional outputs may be perceived as superfluous information. While there are no factual flaws or contradictions, the additional details may detract from the emphasis on the essential information.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q5. Prompt:</bold> Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection?<break/><bold>A5. Ground truth:</bold> The overcurrent protection for two parallel #8 AWG conductors must ensure that the ampacity of each individual conductor is not surpassed if one or more of the parallel conductors become unintentionally disconnected.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A5. Actual output:</bold> Section 310.10(H) of the NEC permits parallel conductors in raceways, multiconductor cables, or direct-buried single conductor cables, provided they satisfy specific criteria:<break/>One of these is that the cumulative current-carrying capacity of the parallel circuits must not surpass the ampacity rating of the raceway or overhead circuit.<break/>In the specified context, when utilizing two parallel #8 AWG conductors, it is crucial to verify that the cumulative current-carrying capability of both circuits does not surpass the ampacity rating of the raceway or overhead circuit.<break/><bold>Score: 0.2610665811531422</bold><break/><bold>Reason:</bold> The actual output fails to specifically mention the restriction on overcurrent protection related to the scenario where one or more parallel conductors become disconnected, which is a key aspect of the expected output.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q6. Prompt:</bold> I am trying to find the table in the NEC that deals with AC motors and associated current.<break/><bold>A6. Ground truth:</bold> Tables 430.248 through 430.250 reflect the typical and most used 4-pole and 2-pole induction motors (AC) in use.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A6. Actual output:</bold> Article 440 of the National Electrical Code (NEC), entitled &#8220;Motor Controllers,&#8221; addresses AC motors and their corresponding currents. Refer to Table 440.5 of Article 440 for the minimum full-load current specifications for single-phase or three-phase AC motors, as determined by the motor&#8217;s nameplate rating.<break/><bold>Score: 0.20462199541570797</bold><break/><bold>Reason:</bold> The actual output erroneously designates Article 440 and Table 440.5 as the pertinent part for AC motors. The correct output should point to Tables 430.248 through 430.250. This signifies a factual inaccuracy and a substantial discrepancy.</td></tr></tbody></table><table-wrap-foot><p>Score is computed using DeepEval&#8217;s correctness with the criteria [GEval] metric.</p><p><sup>*</sup>A parsed version of the 2017 NFPA 70 &#8211; National Electric Code (NEC) was used for the above RAG implementation.</p></table-wrap-foot></table-wrap><p>The GEval metric provides a detailed evaluation based on the following criteria:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Grounding:</italic> Did the model reference or integrate pertinent language from the retrieved NEC section?</p></list-item><list-item><p><italic toggle=\"yes\">Exactness:</italic> In terms of exactness, was the LLM output able to accurately deduce essential quantitative thresholds and regulatory stipulations?</p></list-item><list-item><p><italic toggle=\"yes\">Verifiability:</italic> Can the assertions made by the LLM output be traced back to the source sections of the document?</p></list-item></list><p>This evaluation framework ensures that the RAG-LLM system not only generates plausible text but also provides code-compliant, reference-anchored answers suitable for technical decision-making in real-world engineering workflows.</p><p>Several significant insights were derived from the assessment of the RAG-LLM pipeline via NEC-based queries:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Semantic prompting and section identification:</italic> The RAG-LLM workflow demonstrated efficacy in identifying and correlating the relevant NEC regulations for a specified inquiry. The accuracy was enhanced when prompts were subtly reconfigured to more effectively guide the model toward the desired retrieval pathway. Inquiries that reflected the phrasing or technical terminology of the NEC yielded more accurate outcomes, highlighting the importance of semantic alignment between user input and coding language.</p></list-item><list-item><p><italic toggle=\"yes\">Numerical table retrieval and chunking limitations:</italic> The system&#8217;s effectiveness in handling numerical lookup queries from tabular data was only somewhat successful. Challenges were observed with NEC tables, where the retrieval procedure often failed to consistently reproduce the table. The main cause for this behavior is in the method of content division: when vast tables are segmented into multiple sections, the model tends to frequently acquire only fragments of the original context. This limited viewpoint may generate responses that are either incorrect or just partially accurate. From the way the table&#8217;s continuity is digested, it is recognized that there is a shortcoming in current Retrieval Augmented Generation approaches.</p></list-item><list-item><p><italic toggle=\"yes\">Challenges with multi-condition exceptions:</italic> The RAG-LLM pipeline faced challenges with regulations using multi-conditioned exceptions, commonly seen in NEC provisions that provide different requirements dependent on voltage, environment, or application type. These exceptions are often expressed as enumerated or hierarchical logical criteria, and the system faced difficulties in analyzing and reasoning through the extensive array of situations. As a result, responses sometimes omitted critical qualifying criteria or incorrectly applied the rule entirely. This indicates a current deficiency in handling hierarchical exception logic within extensive regulatory documents.</p></list-item></list><p>These observations suggest specific areas for refinement, particularly in document chunking, semantic prompting, and exception reasoning, which point toward the need for more advanced RAG strategies such as reranking based on a higher number of top-K results and contextual RAG strategies. These enhancements are discussed in detail in the subsequent section.</p></sec><sec id=\"sec7\"><label>5</label><title>Novel enhancements to traditional RAG-LLM workflow to improve performance</title><p>The following section presents targeted upgrades, including re-ranking and contextual retrieval, to overcome the constraints of the typical RAG-LLM pipeline outlined in the preceding section, specifically addressing issues of relevance and specificity. Although sophisticated, the additional features are easy to integrate into the fast-deployable n8n approach. For the purpose of reproducibility and adoption in practice, these advanced n8n workflow JSON files are made available and referred to in the &#8220;Data Availability&#8221; section.</p><sec id=\"sec8\"><label>5.1</label><title>Enhancements based on reranking</title><p>Reranking is a crucial stage in modern information retrieval and retrieval-augmented generation (RAG) pipelines, where a basic retriever first gathers a diverse set of candidate documents, subsequently followed by a more sophisticated model that rearranges them based on their true relevance to the query. Among reranking approaches, cross-attention-based reranking is notable for its ability to represent intricate semantic congruence between the query and candidate passages. Unlike bi-encoder systems that independently encode queries and documents into fixed-length embeddings, cross-attention models (such as BERT or T5 in a cross-encoder configuration) concurrently process the query-document pair within a cohesive transformer architecture. This allows the model to compute token-level interactions, thereby concentrating on contextually relevant segments of each passage in relation to the query. As a result, the representations yield significantly more accurate relevance ratings, often leading to a considerable enhancement in retrieval performance, especially in tasks requiring nuanced understanding or disambiguation. This method requires heightened processing capabilities, making it relatively unfeasible for initial retrieval from large data sets; nonetheless, it is exceptionally effective for reranking a limited selection of top-K candidates, attaining a robust balance between precision and scalability.</p><p>On the implementation side, reranking-based gains are within reach in an n8n pipeline. A vector database in n8n could be enabled to rerank results and link the reranking node to a reranking model, such as Cohere Rerank 3.5, see <xref rid=\"fig4\" ref-type=\"fig\">Figure 4A</xref>. At the back-end, the Rerank API disaggregates the input query into smaller text segments according to the relevant document. Each segment comprises the query followed by a portion of the document, with the segment size determined by the context length of the employed model. For instance, contemplate the subsequent scenario:</p><list list-type=\"bullet\"><list-item><p>The model in use is rerank-v3.5, which supports a maximum context length of 4,096 tokens, and</p></list-item><list-item><p>The input query consists of 100 tokens, and,</p></list-item><list-item><p>The document to be ranked is 10,000 tokens long, and</p></list-item><list-item><p>Document truncation is turned off by assigning &#8216;max_tokens_per_doc&#8217; a value of 10,000.</p></list-item></list><fig position=\"float\" id=\"fig4\" orientation=\"portrait\"><label>Figure 4</label><caption><p><bold>(A)</bold> n8n RAG-LLM workflow with Cohere reranking. <bold>(B)</bold> Relevant scoring based on Cohere chunking on a 10,000-token document.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g004.jpg\"><alt-text content-type=\"machine-generated\">Diagram of an AI chat system. A chat message triggers the AI agent, using a chat model and memory tools. The OpenAI chat model connects to the chat model. Pinecone Vector Store uses embeddings from OpenAI and a Cohere reranker. Section B shows relevance scores with queries and document ranges.</alt-text></graphic></fig><p>Under this setup, the API splits the document into three chunks, as illustrated in <xref rid=\"fig4\" ref-type=\"fig\">Figure 4B</xref>. The final relevance score assigned to the document corresponds to the highest score obtained across these chunks.</p><p>To assess the performance of various embedding models and rerankers, multiple retrieval tests were conducted. <xref rid=\"tab3\" ref-type=\"table\">Table 3</xref> presents the evaluation results using two standard information retrieval metrics: hit rate and Mean Reciprocal Rank (MRR). Three reranking configurations were compared:</p><list list-type=\"order\"><list-item><p>Without any reranking (baseline),</p></list-item><list-item><p>bge-reranker-base, and</p></list-item><list-item><p>Cohere V3.5 reranker.</p></list-item></list><table-wrap position=\"float\" id=\"tab3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Evaluation of rerankers using two standard information retrieval metrics.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Embedding</th><th align=\"center\" valign=\"top\" colspan=\"2\" rowspan=\"1\">Without any reranking (baseline)</th><th align=\"center\" valign=\"top\" colspan=\"2\" rowspan=\"1\">bge-reranker-base</th><th align=\"center\" valign=\"top\" colspan=\"2\" rowspan=\"1\">Cohere V3.5 reranker</th></tr><tr><th rowspan=\"1\" colspan=\"1\"/><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Hit rate</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRR</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Hit rate</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRR</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Hit Rate</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRR</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">OpenAI</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.828</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.692</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.899</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.822</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>0.932</bold>\n</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>0.877</bold>\n</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Voyage</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.766</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.650</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.818</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.777</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.891</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.830</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Google-PaLM</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.815</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.676</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.892</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.752</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.916</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.865</td></tr></tbody></table><table-wrap-foot><p>Boldface score indicates the highest performance.</p></table-wrap-foot></table-wrap><p>With results documented in <xref rid=\"tab3\" ref-type=\"table\">Table 3</xref>, the following conclusions were reached:</p><list list-type=\"order\"><list-item><p>OpenAI + Cohere rerank combinations were found to consistently achieve the highest scores across both hit rate and MRR, thereby positioning them as the top-performing setup.</p></list-item><list-item><p>CohereRerank and bge-reranker-base provide consistent enhancements across various embedding models. This demonstrates their strength and effectiveness in improving search quality, irrespective of the type of embedding backbone employed.</p></list-item></list><p>The influence of rerankers cannot be overstated, as they play a key role in improving the MRR for many embeddings, showing their importance in making search results better. In the next section, the discussion will be directed toward another popular RAG enhancement technique using contextual retrieval.</p></sec><sec id=\"sec9\"><label>5.2</label><title>Enhancements based on contextual retrieval pipeline and its advantages</title><p>For traditional RAG pipelines, source documents are split into smaller text chunks to achieve retrieval precision and efficiency. This segment-wise approach is currently widely used in practice and is effective, especially when the context is self-contained within each segment. However, in domain-specific use cases such as electrical codes and standards, this approach may lead to significant context fragmentation. As an example, consider the following question asked of a database that stores the National Electrical Code (NEC):</p><disp-quote><p>\n<italic toggle=\"yes\">What is the minimum burial depth for direct-buried conductors under a parking lot?</italic>\n</p></disp-quote><p>A retrieved chunk might contain the following response:</p><disp-quote><p>\n<italic toggle=\"yes\">The minimum cover depth shall be 24 inches for direct-buried conductors.</italic>\n</p></disp-quote><p>While this statement appears relevant, it lacks critical contextual qualifiers. The retrieved chunk may omit the information that the requirement applies only for certain types of installations (for example, circuits that are rated 0 to 600 volts) and is located under certain locations (such as driveways or parking lots subject to vehicular traffic). Although the retrieval might be considered successful, the response output by the RAG pipeline might be incomplete or not follow the desired semantics if the installation condition or voltage rating is mentioned in a previous chunk. This points to a fundamental limitation of traditional chunk-based RAG systems: it is possible for each chunk to contain insufficient semantic or structural context to potentially support an in-depth, regulation-compliant answer. In more complex documents such as the NEC, where applicability is often determined by a combination of table values, conditional rules, and cross-referenced sections, such fragmentation can harm both the relevance of retrieved sections and the accuracy of a generative search. These limitations warrant looking for context-aware RAG approaches that try to keep and restore larger contextual windows during both retrieval and generation.</p><p>To address this, contextual RAG augments each chunk with its surrounding textual context (e.g., parent sections, headers, or preceding paragraphs) during embedding. This allows the retriever to maintain semantic continuity and structural fidelity. However, this comes at the cost of an increased token volume. Empirically, contextual RAG embeddings can require 2&#8211;3 times more tokens per chunk than traditional RAG approaches.</p><p>As token volume increases, careful model selection becomes essential, especially for real-time or cost-sensitive applications. For retrieval and summarization tasks that do not require advanced reasoning, it was found to be preferable to select lightweight LLMs with:</p><list list-type=\"bullet\"><list-item><p>High token throughput (e.g., 200&#8211;300 tokens/s),</p></list-item><list-item><p>Low per-token cost (e.g., &lt;$0.10 per input million tokens), and.</p></list-item><list-item><p>Extended context windows (e.g., 1 million tokens).</p></list-item></list><p>These characteristics, summarized in <xref rid=\"tab4\" ref-type=\"table\">Table 4</xref>, ensure that contextual RAG systems remain responsive and scalable, even as embedding and inference loads grow with richer document representations. At the time of writing the paper, Llama 4 Scout 17B 16E fulfilled these criteria and was used for the subsequent demonstrations of contextual RAG; however, given the rapid developmental pace in this domain, researchers should look for similar lightweight LLMs that might better align with the preferred characteristics listed in <xref rid=\"tab4\" ref-type=\"table\">Table 4</xref>.</p><table-wrap position=\"float\" id=\"tab4\" orientation=\"portrait\"><label>Table 4</label><caption><p>A comparison between traditional vs. novel contextual RAG chunking strategies, highlighting trade-offs.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">RAG strategy</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Avg token per chunk (or chunk + context)</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Retrieval fidelity</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Embedding cost</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Traditional RAG</td><td align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">~250&#8211;350 tokens</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Medium</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Low</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Contextual RAG</td><td align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">~600&#8211;1,000 tokens</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">High</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Higher (unless models like Gemini 2.5 Flash are used, with low $/million token cost)</td></tr></tbody></table></table-wrap><p>From a system architecture standpoint, the implementation of contextual RAG within an n8n workflow closely mirrors that of traditional RAG pipelines; recall <xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref>. The overall structure, as illustrated in <xref rid=\"fig5\" ref-type=\"fig\">Figure 5A</xref>, comprises document ingestion, embedding generation, vector storage, retrieval, and language model invocation, and remains fundamentally the same. However, contextual RAG introduces two key enhancements that differentiate it in both design philosophy and execution:</p><list list-type=\"order\"><list-item><p>First, the text splitting and chunking were also done more deliberately, with explicit control on the size, overlap, and structural boundaries of the chunks. This splitting/chunking strategy ensures that semantically cohesive units, for example, complete table entries, complete regulatory exceptions, or paragraph-level logical constructs, are preserved as individual retrieval units. This refinement is essential for preserving the context that is valuable for the downstream retrieval and reasoning.</p></list-item><list-item><p>Second, instead of aggregating top-K retrievals into a single prompt, each chunk is individually passed to a &#8220;Basic LLM Chain&#8221; node within n8n. This node is configured with a well-crafted, structured prompt (user message), <xref rid=\"fig5\" ref-type=\"fig\">Figure 5B</xref>, that guides the LLM in evaluating each chunk&#8217;s relevance and factual contribution to the original query. The design of this prompt is inspired by prompt templates found in contextual RAG applications, such as those published by <xref rid=\"ref1\" ref-type=\"bibr\">Anthropic (2024)</xref>. The chain then filters or ranks responses from multiple chunks before synthesizing a final answer.</p></list-item></list><fig position=\"float\" id=\"fig5\" orientation=\"portrait\"><label>Figure 5</label><caption><p><bold>(A)</bold> n8n RAG-LLM workflow with contextual retrieval system allowing for prepending chunk-specific explanatory context to each chunk before embedding. <bold>(B)</bold> Prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g005.jpg\"><alt-text content-type=\"machine-generated\">A simple knowledge graph diagram showing interconnected nodes related to fossil fuels. Nodes labeled &#8220;explosive,&#8221; &#8220;growth,&#8221; &#8220;automotive,&#8221; and &#8220;sector&#8221; are connected in green. &#8220;Fossil,&#8221; &#8220;fuel,&#8221; &#8220;demand,&#8221; and &#8220;increasing&#8221; are linked in pink, while &#8220;earth,&#8221; &#8220;lack,&#8221; &#8220;rendered&#8221; connect in yellow.</alt-text></graphic></fig><p>The working of the contextual retrieval system is tested by comparing the responses of both the traditional and contextual retrieval systems to questions that would require the system to refer to tabular data or exceptions to code sections in the NEC, and these results are shown in <xref rid=\"tab5\" ref-type=\"table\">Table 5</xref>. The questions in <xref rid=\"tab5\" ref-type=\"table\">Table 5</xref> were framed through a collaborative series of discussions with practitioner engineers familiar with electrical codes and standards. As such, for the design of the experiment, the practitioner engineers had the following responsibilities:</p><list list-type=\"bullet\"><list-item><p>Ensuring the queries being structured are technically correct and align with NEC 2017,</p></list-item><list-item><p>Ensuring that the queries being compiled are a fair representative sample covering all chapters of the NEC,</p></list-item><list-item><p>Developing the ground truth for evaluation purposes.</p></list-item></list><table-wrap position=\"float\" id=\"tab5\" orientation=\"portrait\"><label>Table 5</label><caption><p>Validation of traditional versus contextual RAG-LLM responses using n8n workflows on query tasks based on the 2017 National Electrical Code (NEC).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Q&amp;A responses and scores with traditional RAG-LLM</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Q&amp;A responses and scores with contextual RAG-LLM</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q1. Prompt:</bold> Based on the NEC, for solar photovoltaic (PV) system application, I want to know what is the correction factor that needs to be applied to PV cables for an ambient temperature of 55&#8239;&#176;C. The temperature rating of the conductor being 75&#8239;&#176;C rated.<break/><bold>A1. Ground truth:</bold> According to NEC Table 690.31(A), the applicable correction factor is 0.67.<break/><bold>A1. Actual output from traditional RAG:</bold> According to the National Electrical Code (NEC) 2017, specifically Table 310.15(B)(2)(a) (renumbered in later versions as Table 310.15(B)(1) or Table 310.15(B)(1) Adjustment Factors), the required correction factor is 0.70 for 75&#8239;&#176;C rated PV conductors in 55&#8239;&#176;C ambient conditions.<break/><bold>Score: 0.1981536822161493</bold><break/><bold>Reason:</bold> The actual output yields a correction factor of 0.70 derived from a distinct NEC table, whereas the expected output indicates a correction factor of 0.67 from Table 690.31(A). This signifies a factual inaccuracy and a contradiction with the anticipated product. The real result fails to include the precise reference to Table 690.31(A), which is crucial information from the expected output.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q1. Prompt:</bold> Based on the NEC, for solar photovoltaic (PV) system application, what is the correction factor that needs to be applied to PV cables for an ambient temperature of 55&#8239;&#176;C, and the temperature rating of the conductor being 75&#8239;&#176;C rated.<break/><bold>A1. Ground truth:</bold> According to NEC Table 690.31(A), the applicable correction factor is 0.67.<break/><bold>A1. Actual output from contextual RAG:</bold> According to NEC Table 690.31(A), for a 75&#8239;&#176;C rated conductor in an ambient temperature of 51&#8211;55&#8239;&#176;C, the applicable correction factor is 0.67.<break/><bold>Score: 0.8918686352485045</bold><break/><bold>Reason:</bold> The output is factually true and presents the correct adjustment factor of 0.67, as anticipated. It comprehensively handles the anticipated output by delineating the NEC table and the temperature range, thereby enhancing clarity. The output is consistent and conforms to the context and specifications of the input. The sole small shortcoming is the rather excessive elaboration; however, it does not undermine the accuracy.<break/><bold>Score improved over traditional RAG implementation.</bold></td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q2. Prompt:</bold> Could you please tell me the correction factor for a four current-carrying conductor based on NEC section 310?<break/><bold>A2. Ground Truth:</bold> According to NEC Table 310.15(B)(3)(a), the applicable correction factor is 80%.<break/><bold>A2. Actual output from traditional RAG:</bold> According to Section 310.15(3)(a) of the National Electrical Code (NEC), the adjustment factors for four current-carrying conductors are 1 and 2%. These factors are used to adjust the load diversity of the installation.<break/><bold>Score: 0.08819886431169416</bold><break/><bold>Reason:</bold> The actual output erroneously designates the adjustment factors as 1 and 2%, which is inconsistent with the Expected Output&#8217;s 80% correction factor. It incorrectly cites section 310.15(3)(a) instead of Table 310.15(B)(3)(a). This illustrates factual flaws and inconsistencies with the expected output.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q2. Prompt:</bold> Based on NEC section 310, what is the adjustment factor for a 4 current-carrying conductor?<break/><bold>A2. Ground truth:</bold> According to NEC Table 310.15(B)(3)(a), the applicable correction factor is 80%.<break/><bold>A2. Actual output from contextual RAG:</bold> According to the National Electrical Code (NEC) 2017, specifically Table 310.15(B)(3)(a), the applicable correction factor, as adjusted for ambient temperature if necessary, is 80%.<break/><bold>Score: 0.7562603783294554</bold><break/><bold>Reason:</bold> The output accurately designates the adjustment factor as 80% in accordance with NEC Table 310.15(B)(3)(a), consistent with the anticipated outcome. However, it introduces extraneous information on ambient temperature adjustment, which is neither required by the input nor anticipated in the output, so it slightly diverges from the predicted response.<break/><bold>Score improved over traditional RAG implementation.</bold></td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q3. Prompt:</bold> Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection?<break/><bold>A3. Ground truth:</bold> The system&#8217;s overcurrent protection for two parallel #8 AWG conductors must be built to prevent each conductor&#8217;s ampacity from being exceeded in the event that one or more of the parallel conductors are inadvertently disconnected.<break/><bold>A3. Actual output from traditional RAG:</bold> Section 310.10(H) of the NEC permits parallel conductors in raceways, multiconductor cables, or direct-buried single-conductor cables, provided they satisfy specific standards.<break/>The cumulative current-carrying capacity of the parallel circuits must not surpass the ampacity rating of the raceway or overhead circuit.<break/>If the ungrounded service-entrance conductors are installed in parallel, they must be scaled to correspond with the ampacity and rated voltage of the ungrounded service-entrance wires.<break/>In the provided context, it is crucial to ensure that the combined current-carrying capability of two parallel #8 AWG conductors does not surpass the ampacity rating of the raceway or overhead circuit.<break/><bold>Score: 0.2610665811531422</bold><break/><bold>Reason:</bold> The actual result does not explicitly address the restriction on overcurrent protection concerning the scenario in which one or more parallel conductors become disconnected, a crucial element of the anticipated output.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q3. Prompt:</bold> Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection?<break/><bold>A3. Ground truth:</bold> With two parallel #8 AWG conductors, the overcurrent protection should be such that the ampacity of individual conductors will not be exceeded if one or more of the parallel conductors become inadvertently disconnected.<break/><bold>A3. Actual output from contextual RAG:</bold> NEC 2017, Section 310.10(H) says that for two parallel #8 AWG conductors, each one must be safeguarded so that no one exceeds its ampacity.<break/><bold>Score: 0.8042020327293316</bold><break/><bold>Reason:</bold> The actual output aligns well with the expected output by stating that each conductor must be protected to not exceed its ampacity, which is consistent with the requirement that the ampacity of individual conductors should not be exceeded.<break/><bold>Score improved over traditional RAG implementation.</bold></td></tr></tbody></table><table-wrap-foot><p>Score is computed using DeepEval&#8217;s correctness with the criteria [GEval] metric.</p><p><sup>*</sup>For this implementation, a chunk size of 2,000 was used for both traditional RAG setup and contextual RAG setup, with an overlap of 20.</p></table-wrap-foot></table-wrap><p>The median and interquartile range (IQR) of the performance scoring for contextual and traditional RAG were found to be 0.83 [0.79, 0.89] and 0.62 [0.26, 0.77], respectively. To determine whether there was a significant improvement in the performance of contextual RAG over the traditional RAG, we decided to conduct a one-sided paired Wilcoxon Signed test. The choice of the Wilcoxon Signed test was dictated by the non-normal distribution of the observed scores for the traditional RAG. Considering a significance level of 0.05, the resulting Wilcoxon test reported a <italic toggle=\"yes\">p</italic>-value of &lt;0.0001, indicating the scores from contextual RAG to be statistically greater than those of the traditional RAG.</p><p>While contextual RAG is a significant improvement over traditional RAG-LLM (or LLMs on their own) in terms of retrieval fidelity, both traditional and contextual RAG suffer from hallucination, especially in the following situations:</p><list list-type=\"order\"><list-item><p>Complex or multi-layered prompts, when the question requires reasoning across several conditions or sentences.</p></list-item><list-item><p>Cross-referenced rules are ubiquitous in engineering and regulatory documents. One area of the document refers to definitions, exceptions, or constraints in another section.</p></list-item></list><p>In such instances, the language model is capable of generating plausible, yet semantically distorted responses, even if the correct passage is partially retrieved. This issue arises because the generative component of the pipeline still relies on learned statistical patterns and does not inherently verify or enforce rule-bound reasoning.</p><p>Therefore, rather than being considered as final sources of truth, both traditional and contextual RAG should be viewed as instruments to speed up the search and localization of domain-specific information. The final output must always be checked against the original text from the authoritative source, whether it be a technical handbook, regulatory code, or engineering standard, even if these systems greatly lessen the cognitive and temporal strain of manual technical engineering code-book navigation. In high-assurance fields where precision, safety, and regulatory compliance are non-negotiable, this verification phase is nevertheless crucial.</p></sec></sec><sec id=\"sec10\"><label>6</label><title>State-of-the-art advanced implementation with multi-brain knowledge graph&#8211;based RAG</title><p>Knowledge graphs (<xref rid=\"ref13\" ref-type=\"bibr\">Ji et al., 2022a</xref>; <xref rid=\"ref17\" ref-type=\"bibr\">Peng et al., 2023</xref>; <xref rid=\"ref23\" ref-type=\"bibr\">Tiwari et al., 2021</xref>) provide a superior approach for modeling relationships between concepts by structuring information, including entities and their interconnected associations. While vector databases use numerical proximity in an embedding space to show semantic similarity, knowledge graphs use a graph-based design to keep obvious, understandable relationships, like hierarchical, causal, or functional connections. Between vector RAG systems and knowledge graph-based RAG systems, each has its own advantages and use case scenarios, and one is not a replacement for the other.</p><p>In a knowledge graph, each node represents a distinct concept or entity, and the edges illustrate the nature of their interaction, enabling advanced reasoning, ontology-based inference (<xref rid=\"ref2\" ref-type=\"bibr\">Baclawski et al., 2017</xref>), and contextual elucidation. Neo4j shows that graph databases are the basic building blocks that make it possible for users to run complex queries using graph traversal or SPARQL-like languages. This graph style is highly useful in sectors where there are obvious paths for making decisions, rules-based logic, or where explainability and relationship integrity are very important. Knowledge graphs not only have clear and easy-to-understand structures, but they also have unique analytical benefits that go beyond those of typical embedding-based representations. These affordances make it easier to find hidden patterns, undertake exploratory research, and look into nuanced linkages across complex information domains:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Identification of structural gaps and blind spots:</italic> Knowledge graphs, as a visual and relational depiction of information, can reveal under-connected or isolated nodes, which may signify neglected or poorly integrated concepts. These blind spots can be utilized to produce innovative discoveries by encouraging new, contextually pertinent linkages between dissimilar concepts.</p></list-item><list-item><p><italic toggle=\"yes\">Exploratory pathways for idea navigation:</italic> Knowledge graphs facilitate intuitive exploration among concepts. By using the connectivity of a node inside the graph, users might uncover indirect yet substantial relationships between concepts and hypotheses, perhaps leading to interdisciplinary discoveries or revisions of conceptual frameworks.</p></list-item><list-item><p><italic toggle=\"yes\">Revealing nuance through concept removal:</italic> By algorithmically or manually removing dominant or highly connected nodes, knowledge graphs can surface latent structures and peripheral relationships. This technique brings to the forefront more subtle, contextually rich ideas that are crowded out by the prevailing big picture and thus supports a deeper interpretation and nuanced understanding of the information space.</p></list-item></list><p>In the relation extraction process, semantic relationships are first identified from unstructured text and mapped as edges connecting concept nodes in the knowledge graph (<xref rid=\"ref13\" ref-type=\"bibr\">Ji et al., 2022b</xref>). The graph is then analyzed using the Louvain community detection algorithm (<xref rid=\"ref4\" ref-type=\"bibr\">Blondel et al., 2008</xref>) from network science, which clusters densely connected nodes together. Nodes belonging to the same community are assigned the same color, visually revealing meaningful relational groupings and latent structure within the extracted knowledge. <xref rid=\"fig6\" ref-type=\"fig\">Figure 6</xref> shows such a knowledge graph for the following simplified example: &#8220;<italic toggle=\"yes\">The ever-increasing demand for fossil fuels due to explosive growth in automotive and other industrial sectors has rendered the earth lacking fossil fuels.</italic>&#8221;</p><fig position=\"float\" id=\"fig6\" orientation=\"portrait\"><label>Figure 6</label><caption><p>A simple knowledge graph with the clustering of similar nodes based on the Louvain community detection algorithm. Each cluster is assigned a unique color.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g006.jpg\"><alt-text content-type=\"machine-generated\">A two-part figure; the first part showing a workflow with an AI agent connected to an OpenAI chat model and memory, interacting with three expert brains: (1) Author Research Expert, focusing on grid maintenance and risk assessments. (2) IEEE Resiliency Process Expert, specializing in resiliency frameworks for electricity. (3) CIGRE Resiliency Process Expert, emphasizing sustainable substation resilience. The second part of the figure shows a complex networked knowledge-graph with interconnected keywords like &#8220;distribution,&#8221; &#8220;generation,&#8221; &#8220;energy,&#8221; and &#8220;demand,&#8221; illustrating relationships among various electrical grid-related terms.</alt-text></graphic></fig><p>When extended to real-life text-rich applications, knowledge graphs offer powerful capabilities for representing structured knowledge, and these can be combined with an intelligent AI agent inside a knowledge-graph&#8211;driven Retrieval-Augmented Generation (RAG) framework. In this setup, the agent does not follow a single linear path but instead navigates across one or more linked knowledge graphs depending on the input prompt. Using the graph structure to guide retrieval, the system is able to capture semantically rich context, highlight hidden connections, and generate answers that are both precise and easier to interpret because they remain tied to structured domain knowledge. Such an approach is particularly useful in engineering research, where overlapping and interdependent domains require contextual representation. To illustrate the concept, three separate knowledge graphs were built, each organized around a specific theme. In this case, the thematic focus was an electrical engineering challenge: enhancing grid resiliency by drawing on both the researcher&#8217;s own work and technical reference materials published through IEEE and CIGRE. For demonstration purposes, these three custom-built knowledge graphs, labeled brain 1, brain 2, and brain 3, were created through a manual curation process leveraging the authors&#8217; domain knowledge to support the construction of these knowledge graphs. For more advanced applications, an AI agent can be leveraged to curate the necessary manuscripts before passing them into a knowledge graph for processing. The composition level details for brain 1, brain 2, and brain 3 are as follows:</p><list list-type=\"order\"><list-item><p>Brain #1 contains the authors&#8217; own body of work, including prior publications (<xref rid=\"ref9\" ref-type=\"bibr\">Ghosh and Suryawanshi, 2024</xref>; <xref rid=\"ref8\" ref-type=\"bibr\">Ghosh and Dutta, 2020</xref>) and internal reports addressing topics such as grid maintenance strategies, resilience measurement, substation floor layout, and fire hazard evaluation.</p></list-item><list-item><p>Brain #2 is built from IEEE resources, including technical reports like IEEE TR83 (<xref rid=\"ref5\" ref-type=\"bibr\">Bose et al., 2020</xref>) and selected papers (<xref rid=\"ref19\" ref-type=\"bibr\">Safdarian et al., 2024</xref>; <xref rid=\"ref3\" ref-type=\"bibr\">Bhattarai et al., 2022</xref>; <xref rid=\"ref22\" ref-type=\"bibr\">Tabassum et al., 2024</xref>) from the IEEE Xplore Digital Library. The IEEE resources focus on technical frameworks, analytical tools, and performance indicators that support resilience.</p></list-item><list-item><p>Brain #3 draws on selected CIGRE technical reports (<xref rid=\"ref7\" ref-type=\"bibr\">Ciapessoni et al., 2023</xref>) and related manuscripts (<xref rid=\"ref12\" ref-type=\"bibr\">Itotani et al., 2024</xref>), and assembled to provide expertise on sustainable practices for strengthening substation reliability. This includes the integration of environmentally friendly technologies and the use of remote maintenance approaches.</p></list-item></list><p>The structure of the knowledge graph&#8211;based RAG system is shown in <xref rid=\"fig7\" ref-type=\"fig\">Figure 7A</xref>, with all three knowledge graphs (brain 1, 2, and 3) being created and deployed using the InfraNodus platform (<xref rid=\"ref16\" ref-type=\"bibr\">Paranyushkin, 2024</xref>). <xref rid=\"fig7\" ref-type=\"fig\">Figure 7B</xref> highlights one of these graphs as an example. Much like earlier versions of traditional and contextual RAG, this implementation was built with the n8n platform and has been made accessible for external use, as noted in the data availability section. Depending on the detail and scope of a given research query, the AI agent may consult a single graph, multiple graphs, or all three interconnected &#8220;brains&#8221; to capture the most relevant context and expand the depth of retrieved knowledge. Examples of these queries, along with the corresponding graph-driven outputs, are provided in <xref rid=\"tab6\" ref-type=\"table\">Table 6</xref>. As shown, the agent actively decides which graph or combination of graphs to draw from in order to refine its responses.</p><fig position=\"float\" id=\"fig7\" orientation=\"portrait\"><label>Figure 7</label><caption><p><bold>(A)</bold> n8n RAG-LLM workflow with multiple custom knowledge graphs, each housing unique knowledge. <bold>(B)</bold> A visualization of the &#8216;author research expert&#8217; (brain 1) knowledge graph. Similar nodes are clustered together based on the Louvain community detection, and each of these clusters is assigned a unique color.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g007.jpg\"><alt-text content-type=\"machine-generated\">Flowchart showing an AI agent connected to an OpenAI chat model and memory, interacting with three expert brains: (1) Author Research Expert, focusing on grid maintenance and risk assessments. (2) IEEE Resiliency Process Expert, specializing in resiliency frameworks for electricity. (3) CIGRE Resiliency Process Expert, emphasizing sustainable substation resilience. Below is a complex network graph with interconnected keywords like \"distribution,\" \"generation,\" \"energy,\" and \"demand,\" illustrating relationships among various electrical grid-related terms.</alt-text></graphic></fig><table-wrap position=\"float\" id=\"tab6\" orientation=\"portrait\"><label>Table 6</label><caption><p>Testing the working mechanism of the knowledge graph-based RAG implementation with the AI agent intelligently tapping the necessary dynamic knowledge graphs (brains).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Prompt into knowledge graph-based RAG</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Brains that the AI agent consulted</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Response from the knowledge graph-based RAG agent</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q1. Prompt:</bold> Referring to the IEEE technical reference documents and the author&#8217;s own research, can you summarize very briefly how fast trip relay settings can help in high fire threat areas.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Brain 1 and Brain 2 (see <xref rid=\"fig7\" ref-type=\"fig\">Figure 7A</xref>)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A1. Actual output from knowledge graph-based RAG agent:</bold> Fast-trip relay configurations are particularly advantageous for power systems functioning in areas susceptible to wildfires. Research and IEEE publications indicate that these configurations assist in several essential aspects.<break/>They facilitate the prompt identification and resolution of faults on a line. Prompt isolation prevents a minor disturbance from escalating into a significant failure that could ignite a fire.<break/>In addition, by decreasing the detection threshold, relays act as a preventative measure, diminishing the likelihood of an electrical fault becoming an ignition source.<break/>This method not only mitigates fire danger but also enhances overall reliability by maintaining power continuity and diminishing the probability of outages that could hinder firefighting operations. In summary, implementing fast-trip settings serves as a focused preventive strategy and a pragmatic dependability measure in high-risk zones.</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>Q2. Prompt:</bold> Referring to CIGRE documents and the author&#8217;s own research, can you list briefly what are the benefits of the SF6 alternative circuit breaker?</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Brain 1 and Brain 3 (see <xref rid=\"fig7\" ref-type=\"fig\">Figure 7A</xref>)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\"><bold>A2. Actual output from knowledge graph-based RAG agent:</bold> Vacuum and synthetic-air circuit breakers offer clear advantages over SF&#8326; types. The biggest difference is seen to be in the environmental aspect, given that SF&#8326; is a potent greenhouse gas, while these alternatives eliminate that problem entirely.<break/>Handling is also simpler as operators do not need to deal with gas recovery systems or think about accidental release of potent greenhouse gases.<break/>Maintenance costs are also lower as a result.<break/>Additionally, vacuum breakers, for example, can run for long periods without service, which reduces expenses and downtime. Together, these factors make SF&#8326;-free breakers a practical choice, especially where sustainability targets and safety requirements are becoming stricter.</td></tr></tbody></table></table-wrap><p>An interesting question that may arise here is how the AI agent intelligently selects which knowledge graph(s) to query? In essence, during the construction of the AI agents that tap into the different knowledge graphs, an Infranodus compliant system message is added within the AI agent as follows, which holds to key to the method&#8217;s intelligence: &#8220;<italic toggle=\"yes\">Always use either EightOS or Polysingularity tool before sending a response to the model. You have to use at least one of them, the one that think is more suitable. Or both or all if both or all can provide some help.</italic>&#8221;</p><p>This system message gives the AI agent the necessary context, and as each graph is described, it will know when to choose it, depending on what the user has defined.</p><p>Based on a series of prompting interactions with the knowledge graph-based RAG system, the following conclusions were made:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Higher API usage cost for knowledge graph generation:</italic> In practice, building a knowledge graph from materials such as PDFs or structured documents uses far more API resources than simply loading text into a vector database. A standard embedding job might use only one or two tokens for each word, but graph generation is different. When relation extraction, entity resolution, or ontology alignment are involved, token use can rise to five to ten times higher for the same document. The extra cost comes from the heavier semantic parsing and the need to map relationships rather than just storing word vectors.</p></list-item><list-item><p><italic toggle=\"yes\">Recommendation to reduce API usage:</italic>\n<xref rid=\"fig8\" ref-type=\"fig\">Figure 8</xref> presents a staged approach for rolling out RAG systems with cost in mind. The idea is to begin with conventional setups and only move toward contextual or knowledge-graph&#8211;driven RAG when the task at hand truly demands stronger semantic depth or reasoning across domain-specific relationships.</p></list-item><list-item><p><italic toggle=\"yes\">Limitations with numerical tabular data:</italic> Knowledge graphs are a poor fit for dense numerical tables since their strength lies in capturing concepts and relationships, not large matrix-style datasets. Applying them to standards with heavy tabular content, like the NEC or NFPA documents, ends up being restrictive. In practice, forcing numerical tables into a graph strips out detail and makes retrieval clumsy. Row-to-row dependencies, column statistics, and numeric precision do not map cleanly into entity&#8211;relation triples, so the ability to run meaningful graph-based inference is reduced.</p></list-item><list-item><p><italic toggle=\"yes\">Expanded exploratory potential with multiple knowledge graphs:</italic> One primary advantage of implementing many domain-specific knowledge graphs is that it expands the range of information available to the AI system. The agent can traverse one or more graphs simultaneously, based upon the requirements of the query. This enables the identification of relationships that may not be evident in a singular graph. Consequently, it facilitates more exploratory analysis and thereby aids users in tracking concepts across interconnected domains. This can often reveal obscured connections that may inspire novel avenues of study.</p></list-item><list-item><p><italic toggle=\"yes\">Other observations:</italic> Some additional observations that were made are docketed as follows:</p></list-item></list><list list-type=\"alpha-lower\"><list-item><p>In a traditional vector-based RAG setup, there is a loss of structural information when chunking documents, while such hierarchies and relationships are preserved in a knowledge graph-RAG setup.</p></list-item><list-item><p>Knowledge graph-RAG was better able to support complex reasoning across multiple facts, such as exceptions to certain engineering code sections, or supplementary discussions on a particular subject within a research paper.</p></list-item><list-item><p>While both contextual RAG and knowledge graph&#8211;based RAG aim to improve retrieval-augmented generation, they are architecturally and functionally optimized for different types of information retrieval tasks. Contextual RAG excels at retrieving and preserving semantic continuity from unstructured text, making it particularly effective for answering questions that rely on narrative explanations or rule statements embedded in paragraphs (e.g., code clauses or installation descriptions). In contrast, KG-RAG is optimized for relational reasoning and structured knowledge traversal, making it well-suited for queries that involve interdependence, multi-node relationships, or exception handling.</p></list-item></list><list list-type=\"simple\"><list-item><p>6. <italic toggle=\"yes\">Author&#8217;s recommendation:</italic> As a user, one should make a predetermined assessment as to what RAG structure might work best for their particular use case.</p></list-item></list><fig position=\"float\" id=\"fig8\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Illustration shows a multi-step guide to reduce API usage for RAG implementation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"frai-08-1697169-g008.jpg\"><alt-text content-type=\"machine-generated\">Flowchart illustrating a sequence from top to bottom: \"Begin with traditional RAG\", \"Optimize chunking parameters\", \"Incorporate reranking with higher top-K\", \"Consider contextual RAG\", and \"Apply knowledge graph-based RAG selectively\". Arrows indicate the progression between steps.</alt-text></graphic></fig></sec><sec id=\"sec11\"><label>7</label><title>Summary and future work</title><p>This paper shows that modern automation platforms like n8n make it possible to set up Retrieval-Augmented Generation (RAG) pipelines with very little coding. Engineers and researchers can use these tools to create working RAG systems that pull and process domain-specific material with ease. Depending on the application, repositories can be indexed either through vector databases or knowledge graphs, giving users flexible options for context-aware retrieval. Vector-based RAG remains the more common choice because of its efficiency and simple deployment, but knowledge-graph approaches provide stronger semantic structure and clearer interpretability. That makes them especially useful when navigating the dense interconnections found in engineering standards.</p><p>At the moment, large-scale use of graph-augmented RAG is held back by higher computational demands and API costs linked to graph construction and reasoning. As these costs drop and APIs improve, knowledge-graph retrieval is expected to grow in adoption and could eventually outperform vector search in precision and domain relevance. This pathway is particularly important in electrical engineering, where understanding how technical terms, clauses, and procedures relate to one another is critical. The combination of n8n with tools like Infranodus already demonstrates that this direction is feasible and practical for document-heavy environments.</p><p>Looking ahead, several areas offer opportunities for further development. These include:</p><list list-type=\"order\"><list-item><p><italic toggle=\"yes\">Multimodal retrieval capabilities:</italic> A promising direction for future work is the integration of multimodal retrieval capabilities into contextual and knowledge graph RAG systems. In engineering, valuable information is not limited to text but is often embedded in schematics, drawings, tables, and other structured resources. Future RAG workflows should be able to extract relationships from these non-textual artifacts, associate them with textual knowledge, and represent them within a unified graph structure. For example, a retrieval system could pair a relay&#8217;s specification sheet with the relevant section of a one-line diagram to answer technical queries with greater accuracy. This kind of integrated retrieval would allow reasoning over multiple formats at once and significantly enhance practical utility in engineering applications,</p></list-item><list-item><p><italic toggle=\"yes\">Improving chunking and indexing strategies:</italic> Another important area of development involves improving chunking and indexing strategies, particularly when dealing with dense numerical tables that are common in engineering standards and equipment specifications. Traditional chunking methods often treat these tables as plain text, which strips away their structural meaning and makes retrieval inefficient. Large tables can consume excessive tokens without preserving the underlying relationships between rows, columns, and units. A more structure-aware chunking approach would preserve table headers, normalize units, and index individual cells or logical table sections, allowing the system to retrieve only the relevant values rather than entire tables. This targeted strategy would reduce computational cost and significantly improve precision when responding to engineering queries that rely on numerical data,</p></list-item><list-item><p><italic toggle=\"yes\">Real-time feedback mechanisms:</italic> Future research should also explore real-time feedback mechanisms that allow RAG systems to learn from expert users. Engineering work is inherently iterative, and domain experts can quickly identify gaps or inaccuracies in system outputs. By incorporating features that let users tag, approve, or correct specific pieces of information, RAG systems can refine their retrieval models over time. This type of closed feedback loop would lead to higher precision and reduce the amount of human effort required to verify answers, resulting in more dependable system performance in real engineering workflows., and.</p></list-item><list-item><p><italic toggle=\"yes\">Expanding multilingual capabilities:</italic> Finally, expanding multilingual capabilities will be essential to make RAG systems more robust and globally relevant. Engineering standards and operational manuals are often published in multiple languages, and collaborative projects frequently involve international teams.</p></list-item><list-item><p><italic toggle=\"yes\">Hybrid RAG architecture:</italic> Another promising direction for future research is the development of a hybrid RAG architecture that balances performance with cost efficiency. Knowledge graph&#8211;based RAG (KG) approaches provide strong contextual reasoning but are typically associated with significantly higher API and computational costs compared to vector-based methods. To address this, a combined strategy can be explored in which KG RAG is selectively applied to complex, reasoning-heavy queries, while VectorRAG handles more direct or fact-based retrieval tasks.</p></list-item></list></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank Dmitry Paranyushkin, founder of InfraNodus for his seminal work on the knowledge graphs and for making the InfraNodus platform available to the public and research community.</p></ack><fn-group><fn id=\"fn0001\" fn-type=\"edited-by\"><p>Edited by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3136270/overview\" ext-link-type=\"uri\">Yeon-Chang Lee</ext-link>, Ulsan National Institute of Science and Technology, Republic of Korea</p></fn><fn id=\"fn0002\" fn-type=\"reviewed-by\"><p>Reviewed by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/2942038/overview\" ext-link-type=\"uri\">Joel Oduro-Afriyie</ext-link>, OriGen AI, United States</p><p><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3216163/overview\" ext-link-type=\"uri\">Feng Zhang</ext-link>, Sinopec Research Institute of Petroleum Processing, China</p></fn><fn fn-type=\"abbr\"><p><bold>Abbreviations:</bold> IEEE, Institute of Electrical and Electronics Engineers; LLM, Large Language Models; NEC, National Electric Code (also known as NFPA 70); NFPA, National Fire Protection Association (US); RAG, Retrieval-Augmented Generation.</p></fn></fn-group><sec sec-type=\"data-availability\" id=\"sec13\"><title>Data availability statement</title><p>The n8n templates generated as part of this manuscript can be accessed on the project&#8217;s GitHub page: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/sghosh27/Low-Code-RAG-LLM-Framework-for-Context-Aware-Querying-in-Electrical-Standards-Design-and-Research\" ext-link-type=\"uri\">https://github.com/sghosh27/Low-Code-RAG-LLM-Framework-for-Context-Aware-Querying-in-Electrical-Standards-Design-and-Research</ext-link> along with Python code for document parsing. A Python notebook is also available for scoring LLM generated outputs versus ground truth.</p></sec><sec sec-type=\"author-contributions\" id=\"sec14\"><title>Author contributions</title><p>SG: Visualization, Resources, Conceptualization, Project administration, Software, Methodology, Writing &#8211; review &amp; editing, Supervision, Investigation, Writing &#8211; original draft. GM: Writing &#8211; original draft, Conceptualization, Writing &#8211; review &amp; editing, Software, Methodology, Investigation, Resources.</p></sec><sec sec-type=\"COI-statement\" id=\"sec16\"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type=\"ai-statement\" id=\"sec17\"><title>Generative AI statement</title><p>The authors declare that Gen AI was used in the creation of this manuscript. <xref rid=\"tab2\" ref-type=\"table\">Tables 2</xref>, <xref rid=\"tab5\" ref-type=\"table\">5</xref>, and <xref rid=\"tab6\" ref-type=\"table\">6</xref> within the manuscript contain engineering prompts and answers generated from the Retrieval Augmented Generation systems that were implemented. No Generative AI was used to write the body of the manuscript.</p><p>Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us.</p></sec><sec sec-type=\"disclaimer\" id=\"sec18\"><title>Publisher&#8217;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><sec sec-type=\"disclaimer\" id=\"sec19\"><title>Author disclaimer</title><p>The views and conclusions expressed are solely those of the authors and do not represent the views of any affiliated organization. This study was conducted independently without institutional or agency sponsorship.</p></sec><ref-list><title>References</title><ref id=\"ref1\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><collab id=\"coll1\">Anthropic</collab></person-group>, \"Introducing Contextual Retrieval,\" (<year>2024</year>). Available online at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.anthropic.com/news/contextual-retrieval\" ext-link-type=\"uri\">https://www.anthropic.com/news/contextual-retrieval</ext-link>. (Accessed 06 2025).</mixed-citation></ref><ref id=\"ref2\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Baclawski</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Chan</surname><given-names>E. S.</given-names></name><name name-style=\"western\"><surname>Gawlick</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Ghoneimy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Gross</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z. H.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Framework for ontology-driven decision making</article-title>. <source>Appl. Ontol.</source><volume>12</volume>, <fpage>245</fpage>&#8211;<lpage>273</lpage>. doi: <pub-id pub-id-type=\"doi\">10.3233/AO-170189</pub-id></mixed-citation></ref><ref id=\"ref3\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bhattarai</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sapkota</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Karki</surname><given-names>R.</given-names></name></person-group> (<year>2022</year>). &#8220;<article-title>Analyzing investment strategies for power system resilience</article-title>&#8221; in <source>IEEE Power &amp; Energy Society General Meeting (PESGM)</source> (<publisher-loc>Denver, Colorado, US</publisher-loc>: IEE). doi: <pub-id pub-id-type=\"doi\">10.1109/PESGM48719.2022.9916980</pub-id></mixed-citation></ref><ref id=\"ref4\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Blondel</surname><given-names>V. D.</given-names></name><name name-style=\"western\"><surname>Guillaume</surname><given-names>J.-L.</given-names></name><name name-style=\"western\"><surname>Lambiotte</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Lefebvre</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>Fast unfolding of communities in large networks</article-title>. <source>J. Stat. Mech. Theory Exp.</source><volume>2008</volume>:<fpage>P10008</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1088/1742-5468/2008/10/P10008</pub-id></mixed-citation></ref><ref id=\"ref5\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bose</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Brown</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Chalamala</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Immerman</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Khodaei</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2020</year>). &#8220;<article-title>Resilience framework, methods, and metrics for the electricity sector</article-title>&#8221; in <source>IEEE Power &amp; Energy Society (PES-TR83)</source>. IEEE.</mixed-citation></ref><ref id=\"ref6\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Liuet</surname><given-names>D.</given-names></name></person-group> (<year>2023</year>). \"Hallucination detection: robustly discerning reliable answers in large language models,\" in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. ACM.</mixed-citation></ref><ref id=\"ref7\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ciapessoni</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Cirio</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Pitto</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Van Harte</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Panteli</surname><given-names>M.</given-names></name></person-group> (<year>2023</year>). <article-title>Power system resilience: definition, features and properties</article-title>. <source>CIGRE Sci. Eng.</source><volume>CSE030</volume>:<fpage>10</fpage>.</mixed-citation></ref><ref id=\"ref8\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ghosh</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Dutta</surname><given-names>S.</given-names></name></person-group> (<year>2020</year>). <article-title>A comprehensive forecasting, risk modelling and optimization framework for electric grid hardening and wildfire prevention in the US</article-title>. <source>Int. J. Energy Eng.</source><volume>10</volume>, <fpage>80</fpage>&#8211;<lpage>89</lpage>.</mixed-citation></ref><ref id=\"ref9\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ghosh</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Suryawanshi</surname><given-names>P.</given-names></name></person-group> (<year>2024</year>). <article-title>Enhancing grid resiliency in high fire or flood risk areas: integrating protective relay settings, broken conductor detection, and grid hardening for climate-induced event preparedness</article-title>. <source>J Inst Eng</source><volume>106</volume>, <fpage>393</fpage>&#8211;<lpage>405</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s40031-024-01146-4</pub-id></mixed-citation></ref><ref id=\"ref10\"><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gopi</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sreekanth</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Dehboz</surname><given-names>N.</given-names></name></person-group>, \"<article-title>Enhancing engineering education through LLM-driven adaptive quiz generation: a RAG-based approach</article-title>,\" in <conf-name>IEEE Frontiers in Education Conference (FIE)</conf-name>, <publisher-loc>Washington, DC, US</publisher-loc>: IEE, (<year>2024</year>). doi: <pub-id pub-id-type=\"doi\">10.1109/FIE61694.2024.10893146</pub-id></mixed-citation></ref><ref id=\"ref11\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><collab id=\"coll2\">Huggingface</collab></person-group>, \"Open LLM Leaderboard,\" (<year>2025</year>). Available online at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\" ext-link-type=\"uri\">https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/</ext-link> (Accessed 2025).</mixed-citation></ref><ref id=\"ref12\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Itotani</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Sadahiro</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Tokai</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Hama</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Sugino</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Takeda</surname><given-names>M.</given-names></name></person-group> (<year>2024</year>). &#8220;<article-title>Sustainable improvement on substation resilience and reliability by using eco-friendly equipment and remote maintenance systems</article-title>&#8221; in <source>CIGRE Paris session.</source> CIGRE.</mixed-citation></ref><ref id=\"ref13\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ji</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Pan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Cambria</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Marttinen</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>P. S.</given-names></name></person-group> (<year>2022</year>). <article-title>A survey on knowledge graphs: representation, acquisition, and applications</article-title>. <source>IEEE Trans. Neural Netw. Learn. Syst.</source><volume>33</volume>, <fpage>494</fpage>&#8211;<lpage>514</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/TNNLS.2021.3070843</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">33900922</pub-id></mixed-citation></ref><ref id=\"ref14\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>W. X.</given-names></name><name name-style=\"western\"><surname>Nie</surname><given-names>J.Y.</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>J.R.</given-names></name></person-group> (<year>2023</year>). &#8220;HaluEval: a large-scale hallucination evaluation benchmark&#8221;, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore: Association for Computational Linguistics.</mixed-citation></ref><ref id=\"ref15\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Omar</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Sorin</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Collins</surname><given-names>J. D.</given-names></name><name name-style=\"western\"><surname>Reich</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Freeman</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Gavin</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>Large language models are highly vulnerable to adversarial hallucination attacks in clinical decision support: a multi-model assurance analysis</article-title>. <source>MedRxiv</source>. doi: <pub-id pub-id-type=\"doi\">10.1101/2025.03.18.25324184</pub-id><pub-id pub-id-type=\"pmcid\">PMC12318031</pub-id><pub-id pub-id-type=\"pmid\">40753316</pub-id></mixed-citation></ref><ref id=\"ref16\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Paranyushkin</surname><given-names>D.</given-names></name></person-group> \"<source>InfraNodus: Discover what everyone else is missing</source>,\" (<year>2024</year>). Available online at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://infranodus.com/\" ext-link-type=\"uri\">https://infranodus.com/</ext-link> (Accessed 07 2025).</mixed-citation></ref><ref id=\"ref17\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Naseriparsa</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Osborne</surname><given-names>F.</given-names></name></person-group> (<year>2023</year>). <article-title>Knowledge graphs: opportunities and challenges</article-title>. <source>Artif. Intell. Rev.</source><volume>56</volume>, <fpage>13071</fpage>&#8211;<lpage>13102</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s10462-023-10465-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC10068207</pub-id><pub-id pub-id-type=\"pmid\">37362886</pub-id></mixed-citation></ref><ref id=\"ref18\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roustan</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Bastardot</surname><given-names>F.</given-names></name></person-group> (<year>2025</year>). <article-title>The clinicians&#8217; guide to large language models: a general perspective with a focus on hallucinations</article-title>. <source>Interact. J. Med. Res.</source><volume>14</volume>. doi: <pub-id pub-id-type=\"doi\">10.2196/59823</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39874574</pub-id><pub-id pub-id-type=\"pmcid\">PMC11815294</pub-id></mixed-citation></ref><ref id=\"ref19\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Safdarian</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Wert</surname><given-names>J. L.</given-names></name><name name-style=\"western\"><surname>Cyr</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Overbye</surname><given-names>T. J.</given-names></name></person-group> (<year>2024</year>). &#8220;<article-title>Power system resiliency and reliability issues from renewable resource droughts</article-title>&#8221; in <source>IEEE Kansas power and energy conference (KPEC)</source> (<publisher-loc>Manhattan, Kansas, US</publisher-loc>: IEE). doi: <pub-id pub-id-type=\"doi\">10.1109/KPEC61529.2024.10676081</pub-id></mixed-citation></ref><ref id=\"ref20\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Siddharth</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>J.</given-names></name></person-group> (<year>2024</year>). <article-title>Retrieval Augmented Generation using engineering design knowledge</article-title>. <source>Knowl.-Based Syst.</source><volume>303</volume>:112410. doi: <pub-id pub-id-type=\"doi\">10.1016/j.knosys.2024.112410</pub-id></mixed-citation></ref><ref id=\"ref21\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Superbi</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Pereira</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Santos</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Lattari</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Castro</surname><given-names>B.</given-names></name></person-group> (<year>2024</year>). &#8220;<article-title>Enhancing large language model performance on ENEM math questions using retrieval-augmented generation</article-title>&#8221; in <source>Proceedings of the XVIII Brazilian e-science workshop (Florianopolis, SC, BreSci)</source>.</mixed-citation></ref><ref id=\"ref22\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tabassum</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Bhusal</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Chinthavali</surname><given-names>S.</given-names></name></person-group> (<year>2024</year>). &#8220;<article-title>Power outage forecasting for system resiliency during extreme weather events</article-title>&#8221; in <source>IEEE international conference on big data (BigData)</source> (<publisher-loc>Washington, DC, US</publisher-loc>: IEE). doi: <pub-id pub-id-type=\"doi\">10.1109/BigData62323.2024.10825805</pub-id></mixed-citation></ref><ref id=\"ref23\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tiwari</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Al-Aswadi</surname><given-names>F. N.</given-names></name><name name-style=\"western\"><surname>Gaurav</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>Recent trends in knowledge graphs: theory and practice</article-title>. <source>Soft. Comput.</source><volume>25</volume>, <fpage>8337</fpage>&#8211;<lpage>8355</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s00500-021-05756-8</pub-id></mixed-citation></ref><ref id=\"ref24\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Deng</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><etal/></person-group>, (<year>2023</year>). &#8220;Enhancing uncertainty-based hallucination detection with stronger focus,&#8221; Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (Singapore: Association for Computational Linguistics), 915&#8211;932.</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Front Artif Intell Front Artif Intell 3981 frontai Front. Artif. Intell. Frontiers in Artificial Intelligence 2624-8212 Frontiers Media SA PMC12672433 PMC12672433.1 12672433 12672433 41346860 10.3389/frai.2025.1697169 1 Original Research Advancing engineering research through context-aware and knowledge graph&#8211;based retrieval-augmented generation Ghosh Soham 1 2 * Visualization Resources Conceptualization Project administration Software Methodology Writing &#8211; review &amp; editing Supervision Investigation Writing &#8211; original draft Mittal Gaurav 3 Writing &#8211; original draft Conceptualization Writing &#8211; review &amp; editing Software Methodology Investigation Resources 1 Department of Electrical Engineering, Black &amp; Veatch , Overland Park, KS , United States 2 IEEE IAS/PES Kansas City Section , Kansas City, Kansas, United States 3 Enterprise Solutions Architect, Black &amp; Veatch , Overland Park, KS , United States * Correspondence: Soham Ghosh, sghosh27@ieee.org 19 11 2025 2025 8 481073 1697169 01 9 2025 28 10 2025 19 11 2025 04 12 2025 05 12 2025 Copyright &#169; 2025 Ghosh and Mittal. 2025 Ghosh and Mittal https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY) . The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Large language models (LLMs) are powerful in language understanding and content generation but frequently fall short of technical accuracy when they are applied to engineering code, standards, and design documents. To mitigate this, we are seeing the emergence of Retrieval-Augmented Generation (RAG) models that ground outputs of LLMs with information from external trustworthy resources, increasing the factual consistency. However, traditional RAG techniques are limited in the treatment of isolated information (limited to the amount of information in a fixed-size chunk) and are deemed ill-equipped to traverse semantically linked technical information. This study introduces a collection of new and highly deployable RAG-LLMs built on the n8n automation system and specifically designed for engineering domains. Framework effectiveness was tested on a set of prompts developed with the help of practicing electrical engineering professionals and should be read through the framework&#8217;s lens for interpretation of national engineering codes, technical standards, and design standards. To mitigate the shortcomings of the conventional retrieval-based chunking methods, a contextual RAG-based approach is employed to align the retrieved content with the query context to improve relevance. Moreover, RAG is adopted to structure knowledge graph retrieval, which can retrieve densely linked concepts from multiple knowledge graphs, thereby promoting more profound semantic understanding in complex technical domains. The study describes the relative benefits of these improvements, points to practical deployment issues, strengths, and weaknesses. All the n8n workflows employed in this study are made available as supplementary materials to facilitate reproducibility and sharing within the engineering research community and practitioners. context-aware information retrieval RAG architectures engineering design automation knowledge graphs LLM and intelligence The author(s) declare that no financial support was received for the research and/or publication of this article. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Machine Learning and Artificial Intelligence 1 Introduction Over the past half a decade, large language models (LLMs) have revolutionized many fields, ranging from natural language processing to artificial intelligence. However, despite their extraordinary linguistic skills, LLMs are known to be highly prone to hallucinations, i.e., outputs that are factually wrong or non-sensible for no evident reason. This is due to several challenges of LLM design and operation, which include but are not limited to the use of large training datasets that are often unfiltered, and to the auto-regressive nature of these models, which can cause generated content to be inconsistent ( Li et al., 2023 ; Roustan and Bastardot, 2025 ). As such, LLM hallucinations can take forms such as fake evidence, false inference, and totally wrong replies. These hallucinations can be dangerous in critical applications like engineering, reasoning, and diagnosis, and information retrieval ( Zhang et al., 2023 ). For example, one study shows that the hallucination rate for the different LLMs ranges between 50 and 82% depending on the operational settings, emphasizing the prevalence of such a phenomenon in machine-produced texts ( Omar et al., 2025 ). Because hallucinations can have severe consequences, particularly in fields like engineering, healthcare, and law, there is a pressing need to explore new approaches that make LLM outputs more dependable. One approach with potential is Retrieval Augmented Generation (RAG), which integrates LLMs&#8217; skills with retrieval-based approaches to enhance correctness and relevance. RAG enables LLMs to dynamically and modularly include external sources into the generation flow, which greatly mitigates the limitation of being confined to pre-defined training datasets ( Gopi et al., 2024 ; Siddharth and Luo, 2024 ). In an applied context, leveraging RAG technology is observed to significantly enhance the capabilities of LLMs in emergent scenarios such as rapid identification of relevant clauses in engineering design code and standard or query-based knowledge exploration for an engineering document cluster. By allowing LLMs to access additional information, a RAG system increases coherence and factuality of outputs, demonstrating their wide applicability across different high-stakes settings ( Superbi et al., 2024 ). In addition, the recent developments in the comprehension and management of hallucinations relate to the increase in the use of RAG techniques. Using RAG mechanisms (which mix generative capabilities with well-defined retrieval processes), LLMs can self-repair incorrect results by referencing (on-the-fly access) information fetched dynamically from reliable databases. This not only provides the means for a more dependable computational environment but also enhances user confidence in intelligent systems ( Chen et al., 2023 ). As LLMs develop, the integration of RAG methods seems to be an important move in order to guarantee that such technologies are capable of generating suitable, context-sensitive, and reliable answers in a growing variety of use cases. Although retrieval-augmented generation (RAG) models can address the aforementioned problems by grounding answers in well-documented sources, the actual implementations of RAG for engineering remain underdeveloped, and there are a number of practical challenges hindering their implementation. These are as follows: The barrier to entry is still quite high for the RAG-LLM combination, given that building a well-performing RAG-LLM pipeline usually demands a good deal of programming expertise, which may not be available to many engineering professionals and researchers. For an RAG-LLM application to truly be successful, it should have the ability to access a user&#8217;s existing document repository in a manner similar to popular applications like Google Drive or SharePoint. Unfortunately, building such integrations is never that simple, and would likely add layers of complexity, such as us having to implement authentication protocols, configure a set of APIs, and understand how to parse a document. In order to make a RAG pipeline work well, one often also has to adopt more advanced tweaks, such as reranking strategies, context retrieval mechanisms, or knowledge graph-based augmentation. While those approaches increase retrieval precision and relevance of output, they tend to be technically challenging and are based on the use of several NLP tools and frameworks. Altogether, these subtleties limit the accessibility and effective usage of RAG-LLM systems for numerous engineers and applied researchers. Real-world engineering research and application have faced these obstacles down, and many practitioners have included them as &#8220;given&#8221; variables. To mitigate the high entry barrier and technical complexities, a fast-deployable recipe has been articulated employing open LLMs and modular RAG pipelines and enjoying simple deployment capabilities at accessible platforms such as LM Studio, AnythingLLM, and n8n. This eliminates the requirement for elaborate programming and allows practitioners to remain focused on the engineering content and not the software infrastructure. A number of workflow implementations have been showcased in the n8n automation framework, including document retrieval, reranking, contextual querying, and knowledge graph integration. Performance testing has been performed to a large extent, based on engineering documents, technical standards, and literature from, e.g., the National Fire Protection Association (NFPA), the Institute of Electrical and Electronics Engineers (IEEE), and the International Council on Large Electric Systems (CIGRE). The findings validate that a rapidly deployable RAG-LLM design can achieve good usability and high performance in technically challenging domains. 1.1 Manuscript contribution This study pushes forward the methodology when it comes to the development of rapidly deployable retrieval-augmented generation (RAG) systems and evaluation of technical document querying tuned for practicing power system professionals. The specific contributions are as follows: RAG-LLM pipeline implementation: Example of a rapid deployable RAG-LLM pipeline, using LM Studio and AnythingLLM to secure local document retrieval and language model inference is provided. The code to be used for parsing and embedding corpus-specific documents is released and available through the &#8216;Data Availability&#8217; section of the article. Practitioner-oriented evaluation and design considerations: The manuscript frames the design and evaluation of RAG workflows with a focus on practitioner guidance, particularly for electrical engineers and applied researchers working in rapidly deployable environments such as n8n. Special attention is given to the challenges encountered by traditional RAG systems when processing dense tabular data and multi-layered exceptions commonly found in technical codes and standards. Contextual retrieval workflow via n8n: We introduce a novel RAG contextual workflow implemented in n8n, which supports better processing of tabular structures and &#8216;exception logic&#8217; by orchestration of document processing and dynamic retrieval logic. The full workflow is released to the public and is available through &#8216;Data Availability&#8217; for reproducibility and the enablement of practical use. Knowledge graph-based retrieval workflow via n8n and Infranodus: We present a state-of-the-art knowledge graph-augmented RAG workflow leveraging n8n and Infranodus to semantically structure engineering documents and reveal hidden relationships among engineering concepts. This method allows developers to make exploratory queries and cross-reference between linked standards, which is beneficial for complex design and compliance activities. The full process, from graph construction to integration and retrieval logic, is released to the public in the &#8216;Data Availability&#8217; section for reproducibility and to serve as a practical tool. 1.2 Manuscript organization The remainder of the manuscript is structured as follows: in Section 2, we describe the basics of Retrieval Augmented Generation (RAG), which presents the basic building blocks of the RAG system and a typical structure of the RAG system architecture. Section 3 describes a simple-to-adopt private RAG framework constructed from open-source LLMs, which uses LM Studio and AnythingLLM to be deployed locally. In Section 4, we focused on quickly deployable integration of n8n for document retrieval, performing an in-depth evaluation and ranked prompts based on a question set built from the NFPA 70 National Electrical Code (NEC) 2017. This section also highlights important observations about when traditional RAG-LLM-based pipelines do and do not perform well. Section 5 mitigates the above restrictions via targeted improvements in the area of reranking and contextual RAG workflows. We show scores in comparison to various rerankers (e.g., bge-reranker-base and Cohere Reranker v3. 5) over various embedding models such as OpenAI and Voyage. As an advanced use case, Section 6 discusses a multi-brain RAG implementation of a state-of-the-art knowledge graph-based RAG system and how it can be applied for exploratory querying on a network of engineering standards. Practical guidelines for reducing the consumption and related costs of APIs are presented. Finally, Section 7 summarizes with a summary of our findings, the current limitations, and future work ahead. 2 Fundamentals of retrieval-augmented generation (RAG) and present limitations Large language models (LLMs), such as GPT-4 and LLaMA, have demonstrated remarkable capabilities at generating coherent, contextually appropriate text. They have an impressive flaw, though they often &#8220;hallucinate,&#8221; generating plausible-sounding but incorrect (or impossible to verify) information. This observation is due to the fact that LLMs produce outputs by relying on statistical associations in their training data and not on grounded truth or domain-specific knowledge. To address this limitation, retrieval-augmented generation (RAG), a novel architectural style, has emerged as a compelling approach. With RAG, the generation process is combined with a retrieval module to dynamically retrieve relevant documents or context passages from an external knowledge source. This is similar in spirit to giving a model access to a domain-specific textbook at test time, to consult ground truth sources before generating answers. By building retrieval directly into the generation loop, as illustrated in Figure 1 , a RAG system can largely improve the end quality of correctness and domain relevance. For example, in power systems engineering or regulatory compliance, where specific terms and references are required, RAG provides answers that are tied to source materials collected for that domain. This renders RAG especially well-suited for use cases where there is a high emphasis on knowledge-intensive question answering, code documentation, engineering reasoning, and queries, and where hallucinations can be high-stakes. Nevertheless, RAG is not a replacement for domain adaptation or continual learning. It does not allow a model to generalize new syntactic constructions, programming languages, or stylistic conventions. Instead, it offers an efficient amplification process for enhancing response accuracy within a predefined information domain. In this way, RAG facilitates retrieval, and not representation, which is more suitable for fine-grained applications rather than general domain training. Figure 1 Illustration of RAG architecture, where a user query initiates a retrieval process from a knowledge base (e.g., vector database or document store). Diagram illustrating retrieval-augmented generation. A user query feeds into a retrieval system that finds top relevant documents from a corpus. The retrieved context and query are then used by a generator model to output a response. Although hosted LLM interfaces like ChatGPT offer the ability for a user to upload domain-specific documents (e.g., an electronic engineering textbook or an IEEE master volume) and then engage in retrieval-augmented querying, there are multiple limitations when one has to resort to a cloud-based service for professional or industrial use. These are as follows: Data sovereignty and confidentiality, become a pressing concern. Uploaded documents are outside of the control of personal or corporate workstations, effectively by-passing internal policies on data handling, intellectual property agreements, or even regulatory requirements, especially when proprietary designs, sensitive specs, or restricted standards are concerned. Hosted models often tend to have limits on the length of tokens and contexts that make it hard for technology to work. For instance, in the case of a long publication such as a multi-chapter standard or a big handbook, the material may go above the maximum token limit, even with approaches for breaking up and summarizing documents. This might lead to insufficient context or retrieval and may make response generation less accurate. In the case of hosted solutions, there is usually no persistent user-manageable storage of the parsed knowledge. After the document is uploaded and used for one session, the embedding and index layers are transient and not seen (allowed access to) by the user. This makes it impossible for engineers to construct long-term, reusable vector databases that can adapt to the progress of ongoing projects or to organizational requirements in terms of the knowledge needed. The other limitation is the higher cost of operation, and this is more applicable to frequent or large-scale document processing. Subscription fees, usage-based pricing, and data upload limitations while attempting to submit jobs to external services can add up to prevent hosted LLM services at scale economically feasible for long-term engineering workflows or enterprise integration. These shortcomings also reveal the importance of self-hosted RAG pipelines, i.e., a system where people could download LLMs themselves and run them locally, parse their own engineering documents, and keep the embeddings in a vector database of their selection (e.g., FAISS, Pinecone, or Qdrant). This kind of configuration gives users full access to the retrieval layer, the ability to customize indexing strategies, and integrate with in-house tools or platforms. In the context of electrical engineering, this means sensitive design documents, grid planning standards, or equipment protocols can be queried securely, reliably, and repeatedly, without using third-party APIs or external cloud services. It also provides the opportunity for sensitive document access policies, auditability, and retention of understanding over time, which is important in high-assurance engineering contexts. 3 Opensource LLM and private RAG-LLM pipeline for engineering applications Open-source large language models (LLMs) are especially attractive in that their deployments can be conducted locally on premises, such that users can locally perform inference in a secure and private way and without relying on proprietary API endpoints or any external, possibly cloud-based, service. This is especially useful in retrieval-augmented generation (RAG) tasks in engineering domains where data privacy, reproducibility, and system-level control are important. Engineers can consult the Hugging Face Open LLM Leaderboard ( Huggingface, 2025 ) to find candidate models for local deployment. Through the filtering into &#8220;mid-range&#8221; models (typically 14B-32B range as shown in Table 1 ), users can choose those models that compromise between inference performance and hardware feasibility. Table 1 Open LLM leaderboard filtered by &#8216;mid-range&#8217; models and sorted by weighted average of normalized scores from all benchmarks (as of June 2025). Model Average score across all benchmarks IFEva &#8224; GPQA &#8225; CO 2 cost Qwen2.5-test-32b-it 47.37% 78.89% 15.21% 29.54&#8239;kg Horizon-AI-Avengers-V1-32B 47.34% 79.72% 14.99% 7.95&#8239;kg FluentlyLM-Prinum 47.22% 80.90% 18.23% 21.25&#8239;kg Qwen.5-14b-v1.0-e3 47.09% 73.24% 22.26% 1.56&#8239;kg Qwen2.5-32B-Instruct-abliterated-v2 46.89% 83.34% 15.66% 13.49&#8239;kg &#8224; Instruction-following evaluation (IFEval): This metric tests the model&#8217;s ability to follow explicit formatting instructions. &#8225; Graduate-level Google-Proof Q&amp;A (GPQA): This metric scores the LLM performance based on PhD-level knowledge multiple-choice questions in science. Boldface scores represent the highest scores or the lowest carbon footprint. The private and locally executed RAG-LLM leverages two open source softwares, namely LM Studio and AnythingLLM; see Figure 2 . Within LM Studio, users can download target LLM models from open source and launch them in their quantized formats (e.g., GGUF) without extra tuning. The settings that may be tuned are those of the system prompt, randomness temperature, and top-K sampling parameter. For an entire RAG pipeline, a second open-source application, AnythingLLM, is leveraged for establishing a local retrieval work environment. PDF files consisting of IEEE standards, electrical codes, or equipment manuals have to be initially changed into the Markdown (. md) and one may upload them to AnythingLLM. Markdown works better for RAG because it provides clean, structured, and token-efficient text that enables accurate chunking and retrieval, unlike the noisy and layout-heavy content in PDFs. Figure 2 Illustration of a private and locally executed RAG-LLM pipeline using LM Studio and AnythingLLM. Diagram illustrating a workflow involving document types (DOC, PDF, PPT, XLS) parsed to Markdown files for a RAG workspace called Anything LLM. User-provided prompts and model parameters interact with a locally hosted LLM at IP 127.0.0.1:1234 powered by LM Studio. User queries go to Anything LLM, with output generated from a RAG chatbot. AnythingLLM also permits users to set the &#8216;Text Chunk Size&#8217; and &#8216;Text Chunk Overlap&#8217;, which are crucial parameters that determine how fine and how much of the context is preserved in document embeddings. AnythingLLM communicates with LM Studio (which is (supposed to be) running locally), in which we embed the LLM, and it makes requests to the cached model to generate text in response to questions about the embedded content. Such a setup guarantees that the model and the knowledge base are local to the user&#8217;s computer, and that the user remains in complete control of the data, the execution, and the experimental reproducibility, without the need to rely on cloud inference or third-party services. 4 Rapidly deployable integration with n8n for document retrieval and framework limitations Although LM Studio and AnythingLLM provide a fast, secure, and locally deployable solution for developing retrieval-augmented generation (RAG) systems, it comes with limitations in workflow automation, integration, and orchestration capability. For more advanced use cases, when integrating document ingestion, vector store, and even multi-channel query, a flexible, growable automation frame is required. Which is precisely where n8n, a free and open-source, node.js-based workflow automation tool, excels. User of n8n can create automated workflows that incorporate AI with corporate-level business process automation. It runs on-premises, which guarantees data privacy and allows for custom logic, APIs, and plugins, so it is appropriate for engineering, document intelligence, and RAG applications. Figure 3 shows an example RAG workflow using n8n based on the use case of this paper. The workflow functions as follows: Document upload: Engineers/technical professionals upload documents such as PPT, PDF, Datasheets, Word-based Specifications, or Manuals built as per the standards in 1 above, into a Google Drive folder assigned for them. Automated ingestion: n8n monitors the Google Drive folder for new uploads. When a file is discovered, it initiates a processing pipeline to extract the content, optionally parse it into Markdown or plain text, and embed the text using the user-provided embedding model. Vector storage: The resulting representations are saved in a vector database (can be local or cloud-hosted), such as Qdrant, Chroma, Pinecone, to be queried against later. Query interface: The end users pose queries using different interfaces such as Gmail, Slack, or a web-based chatbot. There, an n8n agent returns the top-K related chunks from the vector database. LLM generation: The resulting snippets are sent to the LLM of the user&#8217;s preference1, which is hosted in a local machine to generate a grounded, context-aware reply. Figure 3 n8n RAG-LLM workflow with a Google Drive-based retrieval system offering a user-friendly framework for engineers and researchers. Flowchart depicting a system for updating a vector database and retrieving top-k matches. The upper section shows a process triggered by Google Drive, where a document is downloaded and embedded into the Pinecone vector store using OpenAI embeddings. A default data loader and recursive character text splitter are involved. The lower part shows integration with an AI agent that receives chat messages, utilizes OpenAI chat models for memory and tools, and interacts with the Pinecone vector store to retrieve embedded data. This modular and event-driven design further decouples ingestion and querying phases, and allows users to automate multi-step, cross-platform knowledge workflows, resulting in a more scalable and maintainable RAG-LLM solution in engineering settings. In order to demonstrate the effectiveness of the proposed RAG-LLM pipeline, we performed an experiment on a collection of curated, domain-specific search queries to the National Electrical Code (NEC), a complex regulatory document relied on by practicing electrical engineers. The NEC codebook (in parsed text) was uploaded by the user to the RAG system, allowing for grounded-document retrieval for inference. The evaluation searches were specified to check if the system can: Locate and extract numerical values from NEC tables (e.g., minimum burial depths, conductor ampacity ratings, and overcurrent protection limits). Extract exceptions and conditional clauses from the codebook, as they frequently manifest as annotations or as structural deviations from the norm. Retrieve the relevant sections and apply necessary contextual logic (e.g., identifying requirements that vary by installation type, voltage class, or application environment). Each generated response was evaluated against user-defined ground truth answers, see Table 2 , simulating expert validation. For scoring, we used the correctness measure of DeepEval with GEval criteria, which assesses semantic alignment, factual grounding, and contextual completeness between the model&#8217;s response and the reference answer. Table 2 Validation of traditional RAG-LLM responses using an n8n workflow on query tasks based on the 2017 National Electrical Code (NEC). Question and ground truth Actual output and evaluation Q1. Prompt: I am trying to find the table in the NEC that deals with a single-phase AC motor and the associated full load current. A1. Ground truth: Table 430.248 of the NEC addresses single-phase AC motors. It also addresses the corresponding full load current. A1. Actual output: The NEC section is Article 430. This section is titled &#8220;Motors, Motor Circuits, and Controllers.&#8221; Do refer to Table 430.248 of Article 430 to determine the minimum full-load current required for a single-phase AC motor, as indicated by the motor&#8217;s nameplate rating. Score: 0.9072185128712421 Reason: The output was found to correctly cite NEC Table 430.248. It aligns with the expected outcome. It additionally provides supplementary context by citing Article 430 and its title, both of which augment factual accuracy and comprehensiveness. The supplementary detail slightly exceeds the expected simplicity; yet, there are no discrepancies or omissions. Q2. Prompt: I want to know what is the minimum cover required per Section 300.5 of the NEC, for direct buried cables running under a parking lot, based on Table 300.5. A2. Ground truth: According to Table 300.5 of the NEC, the minimum cover necessary for a direct buried cable beneath a parking lot is 24 inches. A2. Actual output: According to NEC Table 300.5, the minimum burial depth for a direct-buried cable installation under a parking lot is 24 inches (609&#8239;mm). This relates to the upper surface of the finished grade, and further criteria may be considered when determining the appropriate cover for your specific installation. Score: 0.859266659995407 Reason: The actual production accurately determines the necessary minimum cover as 24 inches, consistent with the anticipated output. It offers supplementary context by specifying the measurement in millimeters and acknowledging that other factors may affect the cover, which, although not included in the anticipated output, does not contradict it. The information is accurate and complete; nonetheless, the additional details somewhat diverge from the anticipated conciseness. Q3. Prompt: In the NEC, which section deals with the minimum cover requirement for cables and conduits. A3. Ground truth: In the NEC, section 300.5 deals with the minimum cover requirement for cables and conduits. A3. Actual output: The minimum cover requirements for cables and conduits are specified in Section 300.5 of the NEC. This section delineates the minimum cover requirements for direct-buried cables, conduits, or other raceways. Furthermore, additional information concerning mechanical protection for direct-buried cables can be found in sections 840.48 and 830.47 of the NEC. Score: 0.8009300906486064 Reason: The output accurately identifies Section 300.5 of the NEC as pertaining to minimum cover requirements for cables and conduits. It also offers further information regarding related areas, which is not mandatory but does not contradict the anticipated outcome. There are no factual flaws or omissions about the basic topic, but the other facts, albeit accurate, are superfluous to the specific inquiry. Q4. Prompt: Is there any restriction on running conductors in parallel in the NEC. A4. Ground truth : NEC section 310.10(H) talks about conductors in parallel. A4. Actual output: Indeed, Section 310.10(H) of the NEC permits the installation of parallel conductors in raceways, multiconductor cables, or direct-buried single-conductor cables. Score: 0.5432697039021899 Reason: The actual output accurately recognizes NEC section 310.10(H) as pertinent to parallel conductors, consistent with the anticipated output. However, the output introduces supplementary details regarding installation methods that are not in the anticipated output. These additional outputs may be perceived as superfluous information. While there are no factual flaws or contradictions, the additional details may detract from the emphasis on the essential information. Q5. Prompt: Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection? A5. Ground truth: The overcurrent protection for two parallel #8 AWG conductors must ensure that the ampacity of each individual conductor is not surpassed if one or more of the parallel conductors become unintentionally disconnected. A5. Actual output: Section 310.10(H) of the NEC permits parallel conductors in raceways, multiconductor cables, or direct-buried single conductor cables, provided they satisfy specific criteria: One of these is that the cumulative current-carrying capacity of the parallel circuits must not surpass the ampacity rating of the raceway or overhead circuit. In the specified context, when utilizing two parallel #8 AWG conductors, it is crucial to verify that the cumulative current-carrying capability of both circuits does not surpass the ampacity rating of the raceway or overhead circuit. Score: 0.2610665811531422 Reason: The actual output fails to specifically mention the restriction on overcurrent protection related to the scenario where one or more parallel conductors become disconnected, which is a key aspect of the expected output. Q6. Prompt: I am trying to find the table in the NEC that deals with AC motors and associated current. A6. Ground truth: Tables 430.248 through 430.250 reflect the typical and most used 4-pole and 2-pole induction motors (AC) in use. A6. Actual output: Article 440 of the National Electrical Code (NEC), entitled &#8220;Motor Controllers,&#8221; addresses AC motors and their corresponding currents. Refer to Table 440.5 of Article 440 for the minimum full-load current specifications for single-phase or three-phase AC motors, as determined by the motor&#8217;s nameplate rating. Score: 0.20462199541570797 Reason: The actual output erroneously designates Article 440 and Table 440.5 as the pertinent part for AC motors. The correct output should point to Tables 430.248 through 430.250. This signifies a factual inaccuracy and a substantial discrepancy. Score is computed using DeepEval&#8217;s correctness with the criteria [GEval] metric. * A parsed version of the 2017 NFPA 70 &#8211; National Electric Code (NEC) was used for the above RAG implementation. The GEval metric provides a detailed evaluation based on the following criteria: Grounding: Did the model reference or integrate pertinent language from the retrieved NEC section? Exactness: In terms of exactness, was the LLM output able to accurately deduce essential quantitative thresholds and regulatory stipulations? Verifiability: Can the assertions made by the LLM output be traced back to the source sections of the document? This evaluation framework ensures that the RAG-LLM system not only generates plausible text but also provides code-compliant, reference-anchored answers suitable for technical decision-making in real-world engineering workflows. Several significant insights were derived from the assessment of the RAG-LLM pipeline via NEC-based queries: Semantic prompting and section identification: The RAG-LLM workflow demonstrated efficacy in identifying and correlating the relevant NEC regulations for a specified inquiry. The accuracy was enhanced when prompts were subtly reconfigured to more effectively guide the model toward the desired retrieval pathway. Inquiries that reflected the phrasing or technical terminology of the NEC yielded more accurate outcomes, highlighting the importance of semantic alignment between user input and coding language. Numerical table retrieval and chunking limitations: The system&#8217;s effectiveness in handling numerical lookup queries from tabular data was only somewhat successful. Challenges were observed with NEC tables, where the retrieval procedure often failed to consistently reproduce the table. The main cause for this behavior is in the method of content division: when vast tables are segmented into multiple sections, the model tends to frequently acquire only fragments of the original context. This limited viewpoint may generate responses that are either incorrect or just partially accurate. From the way the table&#8217;s continuity is digested, it is recognized that there is a shortcoming in current Retrieval Augmented Generation approaches. Challenges with multi-condition exceptions: The RAG-LLM pipeline faced challenges with regulations using multi-conditioned exceptions, commonly seen in NEC provisions that provide different requirements dependent on voltage, environment, or application type. These exceptions are often expressed as enumerated or hierarchical logical criteria, and the system faced difficulties in analyzing and reasoning through the extensive array of situations. As a result, responses sometimes omitted critical qualifying criteria or incorrectly applied the rule entirely. This indicates a current deficiency in handling hierarchical exception logic within extensive regulatory documents. These observations suggest specific areas for refinement, particularly in document chunking, semantic prompting, and exception reasoning, which point toward the need for more advanced RAG strategies such as reranking based on a higher number of top-K results and contextual RAG strategies. These enhancements are discussed in detail in the subsequent section. 5 Novel enhancements to traditional RAG-LLM workflow to improve performance The following section presents targeted upgrades, including re-ranking and contextual retrieval, to overcome the constraints of the typical RAG-LLM pipeline outlined in the preceding section, specifically addressing issues of relevance and specificity. Although sophisticated, the additional features are easy to integrate into the fast-deployable n8n approach. For the purpose of reproducibility and adoption in practice, these advanced n8n workflow JSON files are made available and referred to in the &#8220;Data Availability&#8221; section. 5.1 Enhancements based on reranking Reranking is a crucial stage in modern information retrieval and retrieval-augmented generation (RAG) pipelines, where a basic retriever first gathers a diverse set of candidate documents, subsequently followed by a more sophisticated model that rearranges them based on their true relevance to the query. Among reranking approaches, cross-attention-based reranking is notable for its ability to represent intricate semantic congruence between the query and candidate passages. Unlike bi-encoder systems that independently encode queries and documents into fixed-length embeddings, cross-attention models (such as BERT or T5 in a cross-encoder configuration) concurrently process the query-document pair within a cohesive transformer architecture. This allows the model to compute token-level interactions, thereby concentrating on contextually relevant segments of each passage in relation to the query. As a result, the representations yield significantly more accurate relevance ratings, often leading to a considerable enhancement in retrieval performance, especially in tasks requiring nuanced understanding or disambiguation. This method requires heightened processing capabilities, making it relatively unfeasible for initial retrieval from large data sets; nonetheless, it is exceptionally effective for reranking a limited selection of top-K candidates, attaining a robust balance between precision and scalability. On the implementation side, reranking-based gains are within reach in an n8n pipeline. A vector database in n8n could be enabled to rerank results and link the reranking node to a reranking model, such as Cohere Rerank 3.5, see Figure 4A . At the back-end, the Rerank API disaggregates the input query into smaller text segments according to the relevant document. Each segment comprises the query followed by a portion of the document, with the segment size determined by the context length of the employed model. For instance, contemplate the subsequent scenario: The model in use is rerank-v3.5, which supports a maximum context length of 4,096 tokens, and The input query consists of 100 tokens, and, The document to be ranked is 10,000 tokens long, and Document truncation is turned off by assigning &#8216;max_tokens_per_doc&#8217; a value of 10,000. Figure 4 (A) n8n RAG-LLM workflow with Cohere reranking. (B) Relevant scoring based on Cohere chunking on a 10,000-token document. Diagram of an AI chat system. A chat message triggers the AI agent, using a chat model and memory tools. The OpenAI chat model connects to the chat model. Pinecone Vector Store uses embeddings from OpenAI and a Cohere reranker. Section B shows relevance scores with queries and document ranges. Under this setup, the API splits the document into three chunks, as illustrated in Figure 4B . The final relevance score assigned to the document corresponds to the highest score obtained across these chunks. To assess the performance of various embedding models and rerankers, multiple retrieval tests were conducted. Table 3 presents the evaluation results using two standard information retrieval metrics: hit rate and Mean Reciprocal Rank (MRR). Three reranking configurations were compared: Without any reranking (baseline), bge-reranker-base, and Cohere V3.5 reranker. Table 3 Evaluation of rerankers using two standard information retrieval metrics. Embedding Without any reranking (baseline) bge-reranker-base Cohere V3.5 reranker Hit rate MRR Hit rate MRR Hit Rate MRR OpenAI 0.828 0.692 0.899 0.822 0.932 0.877 Voyage 0.766 0.650 0.818 0.777 0.891 0.830 Google-PaLM 0.815 0.676 0.892 0.752 0.916 0.865 Boldface score indicates the highest performance. With results documented in Table 3 , the following conclusions were reached: OpenAI + Cohere rerank combinations were found to consistently achieve the highest scores across both hit rate and MRR, thereby positioning them as the top-performing setup. CohereRerank and bge-reranker-base provide consistent enhancements across various embedding models. This demonstrates their strength and effectiveness in improving search quality, irrespective of the type of embedding backbone employed. The influence of rerankers cannot be overstated, as they play a key role in improving the MRR for many embeddings, showing their importance in making search results better. In the next section, the discussion will be directed toward another popular RAG enhancement technique using contextual retrieval. 5.2 Enhancements based on contextual retrieval pipeline and its advantages For traditional RAG pipelines, source documents are split into smaller text chunks to achieve retrieval precision and efficiency. This segment-wise approach is currently widely used in practice and is effective, especially when the context is self-contained within each segment. However, in domain-specific use cases such as electrical codes and standards, this approach may lead to significant context fragmentation. As an example, consider the following question asked of a database that stores the National Electrical Code (NEC): What is the minimum burial depth for direct-buried conductors under a parking lot? A retrieved chunk might contain the following response: The minimum cover depth shall be 24 inches for direct-buried conductors. While this statement appears relevant, it lacks critical contextual qualifiers. The retrieved chunk may omit the information that the requirement applies only for certain types of installations (for example, circuits that are rated 0 to 600 volts) and is located under certain locations (such as driveways or parking lots subject to vehicular traffic). Although the retrieval might be considered successful, the response output by the RAG pipeline might be incomplete or not follow the desired semantics if the installation condition or voltage rating is mentioned in a previous chunk. This points to a fundamental limitation of traditional chunk-based RAG systems: it is possible for each chunk to contain insufficient semantic or structural context to potentially support an in-depth, regulation-compliant answer. In more complex documents such as the NEC, where applicability is often determined by a combination of table values, conditional rules, and cross-referenced sections, such fragmentation can harm both the relevance of retrieved sections and the accuracy of a generative search. These limitations warrant looking for context-aware RAG approaches that try to keep and restore larger contextual windows during both retrieval and generation. To address this, contextual RAG augments each chunk with its surrounding textual context (e.g., parent sections, headers, or preceding paragraphs) during embedding. This allows the retriever to maintain semantic continuity and structural fidelity. However, this comes at the cost of an increased token volume. Empirically, contextual RAG embeddings can require 2&#8211;3 times more tokens per chunk than traditional RAG approaches. As token volume increases, careful model selection becomes essential, especially for real-time or cost-sensitive applications. For retrieval and summarization tasks that do not require advanced reasoning, it was found to be preferable to select lightweight LLMs with: High token throughput (e.g., 200&#8211;300 tokens/s), Low per-token cost (e.g., &lt;$0.10 per input million tokens), and. Extended context windows (e.g., 1 million tokens). These characteristics, summarized in Table 4 , ensure that contextual RAG systems remain responsive and scalable, even as embedding and inference loads grow with richer document representations. At the time of writing the paper, Llama 4 Scout 17B 16E fulfilled these criteria and was used for the subsequent demonstrations of contextual RAG; however, given the rapid developmental pace in this domain, researchers should look for similar lightweight LLMs that might better align with the preferred characteristics listed in Table 4 . Table 4 A comparison between traditional vs. novel contextual RAG chunking strategies, highlighting trade-offs. RAG strategy Avg token per chunk (or chunk + context) Retrieval fidelity Embedding cost Traditional RAG ~250&#8211;350 tokens Medium Low Contextual RAG ~600&#8211;1,000 tokens High Higher (unless models like Gemini 2.5 Flash are used, with low $/million token cost) From a system architecture standpoint, the implementation of contextual RAG within an n8n workflow closely mirrors that of traditional RAG pipelines; recall Figure 3 . The overall structure, as illustrated in Figure 5A , comprises document ingestion, embedding generation, vector storage, retrieval, and language model invocation, and remains fundamentally the same. However, contextual RAG introduces two key enhancements that differentiate it in both design philosophy and execution: First, the text splitting and chunking were also done more deliberately, with explicit control on the size, overlap, and structural boundaries of the chunks. This splitting/chunking strategy ensures that semantically cohesive units, for example, complete table entries, complete regulatory exceptions, or paragraph-level logical constructs, are preserved as individual retrieval units. This refinement is essential for preserving the context that is valuable for the downstream retrieval and reasoning. Second, instead of aggregating top-K retrievals into a single prompt, each chunk is individually passed to a &#8220;Basic LLM Chain&#8221; node within n8n. This node is configured with a well-crafted, structured prompt (user message), Figure 5B , that guides the LLM in evaluating each chunk&#8217;s relevance and factual contribution to the original query. The design of this prompt is inspired by prompt templates found in contextual RAG applications, such as those published by Anthropic (2024) . The chain then filters or ranks responses from multiple chunks before synthesizing a final answer. Figure 5 (A) n8n RAG-LLM workflow with contextual retrieval system allowing for prepending chunk-specific explanatory context to each chunk before embedding. (B) Prompt that instructs the model to provide concise, chunk-specific context that explains the chunk using the context of the overall document. A simple knowledge graph diagram showing interconnected nodes related to fossil fuels. Nodes labeled &#8220;explosive,&#8221; &#8220;growth,&#8221; &#8220;automotive,&#8221; and &#8220;sector&#8221; are connected in green. &#8220;Fossil,&#8221; &#8220;fuel,&#8221; &#8220;demand,&#8221; and &#8220;increasing&#8221; are linked in pink, while &#8220;earth,&#8221; &#8220;lack,&#8221; &#8220;rendered&#8221; connect in yellow. The working of the contextual retrieval system is tested by comparing the responses of both the traditional and contextual retrieval systems to questions that would require the system to refer to tabular data or exceptions to code sections in the NEC, and these results are shown in Table 5 . The questions in Table 5 were framed through a collaborative series of discussions with practitioner engineers familiar with electrical codes and standards. As such, for the design of the experiment, the practitioner engineers had the following responsibilities: Ensuring the queries being structured are technically correct and align with NEC 2017, Ensuring that the queries being compiled are a fair representative sample covering all chapters of the NEC, Developing the ground truth for evaluation purposes. Table 5 Validation of traditional versus contextual RAG-LLM responses using n8n workflows on query tasks based on the 2017 National Electrical Code (NEC). Q&amp;A responses and scores with traditional RAG-LLM Q&amp;A responses and scores with contextual RAG-LLM Q1. Prompt: Based on the NEC, for solar photovoltaic (PV) system application, I want to know what is the correction factor that needs to be applied to PV cables for an ambient temperature of 55&#8239;&#176;C. The temperature rating of the conductor being 75&#8239;&#176;C rated. A1. Ground truth: According to NEC Table 690.31(A), the applicable correction factor is 0.67. A1. Actual output from traditional RAG: According to the National Electrical Code (NEC) 2017, specifically Table 310.15(B)(2)(a) (renumbered in later versions as Table 310.15(B)(1) or Table 310.15(B)(1) Adjustment Factors), the required correction factor is 0.70 for 75&#8239;&#176;C rated PV conductors in 55&#8239;&#176;C ambient conditions. Score: 0.1981536822161493 Reason: The actual output yields a correction factor of 0.70 derived from a distinct NEC table, whereas the expected output indicates a correction factor of 0.67 from Table 690.31(A). This signifies a factual inaccuracy and a contradiction with the anticipated product. The real result fails to include the precise reference to Table 690.31(A), which is crucial information from the expected output. Q1. Prompt: Based on the NEC, for solar photovoltaic (PV) system application, what is the correction factor that needs to be applied to PV cables for an ambient temperature of 55&#8239;&#176;C, and the temperature rating of the conductor being 75&#8239;&#176;C rated. A1. Ground truth: According to NEC Table 690.31(A), the applicable correction factor is 0.67. A1. Actual output from contextual RAG: According to NEC Table 690.31(A), for a 75&#8239;&#176;C rated conductor in an ambient temperature of 51&#8211;55&#8239;&#176;C, the applicable correction factor is 0.67. Score: 0.8918686352485045 Reason: The output is factually true and presents the correct adjustment factor of 0.67, as anticipated. It comprehensively handles the anticipated output by delineating the NEC table and the temperature range, thereby enhancing clarity. The output is consistent and conforms to the context and specifications of the input. The sole small shortcoming is the rather excessive elaboration; however, it does not undermine the accuracy. Score improved over traditional RAG implementation. Q2. Prompt: Could you please tell me the correction factor for a four current-carrying conductor based on NEC section 310? A2. Ground Truth: According to NEC Table 310.15(B)(3)(a), the applicable correction factor is 80%. A2. Actual output from traditional RAG: According to Section 310.15(3)(a) of the National Electrical Code (NEC), the adjustment factors for four current-carrying conductors are 1 and 2%. These factors are used to adjust the load diversity of the installation. Score: 0.08819886431169416 Reason: The actual output erroneously designates the adjustment factors as 1 and 2%, which is inconsistent with the Expected Output&#8217;s 80% correction factor. It incorrectly cites section 310.15(3)(a) instead of Table 310.15(B)(3)(a). This illustrates factual flaws and inconsistencies with the expected output. Q2. Prompt: Based on NEC section 310, what is the adjustment factor for a 4 current-carrying conductor? A2. Ground truth: According to NEC Table 310.15(B)(3)(a), the applicable correction factor is 80%. A2. Actual output from contextual RAG: According to the National Electrical Code (NEC) 2017, specifically Table 310.15(B)(3)(a), the applicable correction factor, as adjusted for ambient temperature if necessary, is 80%. Score: 0.7562603783294554 Reason: The output accurately designates the adjustment factor as 80% in accordance with NEC Table 310.15(B)(3)(a), consistent with the anticipated outcome. However, it introduces extraneous information on ambient temperature adjustment, which is neither required by the input nor anticipated in the output, so it slightly diverges from the predicted response. Score improved over traditional RAG implementation. Q3. Prompt: Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection? A3. Ground truth: The system&#8217;s overcurrent protection for two parallel #8 AWG conductors must be built to prevent each conductor&#8217;s ampacity from being exceeded in the event that one or more of the parallel conductors are inadvertently disconnected. A3. Actual output from traditional RAG: Section 310.10(H) of the NEC permits parallel conductors in raceways, multiconductor cables, or direct-buried single-conductor cables, provided they satisfy specific standards. The cumulative current-carrying capacity of the parallel circuits must not surpass the ampacity rating of the raceway or overhead circuit. If the ungrounded service-entrance conductors are installed in parallel, they must be scaled to correspond with the ampacity and rated voltage of the ungrounded service-entrance wires. In the provided context, it is crucial to ensure that the combined current-carrying capability of two parallel #8 AWG conductors does not surpass the ampacity rating of the raceway or overhead circuit. Score: 0.2610665811531422 Reason: The actual result does not explicitly address the restriction on overcurrent protection concerning the scenario in which one or more parallel conductors become disconnected, a crucial element of the anticipated output. Q3. Prompt: Based on section 310.10(H) of the NEC, if I have two parallel #8 AWG conductors, what is my restriction on overcurrent protection? A3. Ground truth: With two parallel #8 AWG conductors, the overcurrent protection should be such that the ampacity of individual conductors will not be exceeded if one or more of the parallel conductors become inadvertently disconnected. A3. Actual output from contextual RAG: NEC 2017, Section 310.10(H) says that for two parallel #8 AWG conductors, each one must be safeguarded so that no one exceeds its ampacity. Score: 0.8042020327293316 Reason: The actual output aligns well with the expected output by stating that each conductor must be protected to not exceed its ampacity, which is consistent with the requirement that the ampacity of individual conductors should not be exceeded. Score improved over traditional RAG implementation. Score is computed using DeepEval&#8217;s correctness with the criteria [GEval] metric. * For this implementation, a chunk size of 2,000 was used for both traditional RAG setup and contextual RAG setup, with an overlap of 20. The median and interquartile range (IQR) of the performance scoring for contextual and traditional RAG were found to be 0.83 [0.79, 0.89] and 0.62 [0.26, 0.77], respectively. To determine whether there was a significant improvement in the performance of contextual RAG over the traditional RAG, we decided to conduct a one-sided paired Wilcoxon Signed test. The choice of the Wilcoxon Signed test was dictated by the non-normal distribution of the observed scores for the traditional RAG. Considering a significance level of 0.05, the resulting Wilcoxon test reported a p -value of &lt;0.0001, indicating the scores from contextual RAG to be statistically greater than those of the traditional RAG. While contextual RAG is a significant improvement over traditional RAG-LLM (or LLMs on their own) in terms of retrieval fidelity, both traditional and contextual RAG suffer from hallucination, especially in the following situations: Complex or multi-layered prompts, when the question requires reasoning across several conditions or sentences. Cross-referenced rules are ubiquitous in engineering and regulatory documents. One area of the document refers to definitions, exceptions, or constraints in another section. In such instances, the language model is capable of generating plausible, yet semantically distorted responses, even if the correct passage is partially retrieved. This issue arises because the generative component of the pipeline still relies on learned statistical patterns and does not inherently verify or enforce rule-bound reasoning. Therefore, rather than being considered as final sources of truth, both traditional and contextual RAG should be viewed as instruments to speed up the search and localization of domain-specific information. The final output must always be checked against the original text from the authoritative source, whether it be a technical handbook, regulatory code, or engineering standard, even if these systems greatly lessen the cognitive and temporal strain of manual technical engineering code-book navigation. In high-assurance fields where precision, safety, and regulatory compliance are non-negotiable, this verification phase is nevertheless crucial. 6 State-of-the-art advanced implementation with multi-brain knowledge graph&#8211;based RAG Knowledge graphs ( Ji et al., 2022a ; Peng et al., 2023 ; Tiwari et al., 2021 ) provide a superior approach for modeling relationships between concepts by structuring information, including entities and their interconnected associations. While vector databases use numerical proximity in an embedding space to show semantic similarity, knowledge graphs use a graph-based design to keep obvious, understandable relationships, like hierarchical, causal, or functional connections. Between vector RAG systems and knowledge graph-based RAG systems, each has its own advantages and use case scenarios, and one is not a replacement for the other. In a knowledge graph, each node represents a distinct concept or entity, and the edges illustrate the nature of their interaction, enabling advanced reasoning, ontology-based inference ( Baclawski et al., 2017 ), and contextual elucidation. Neo4j shows that graph databases are the basic building blocks that make it possible for users to run complex queries using graph traversal or SPARQL-like languages. This graph style is highly useful in sectors where there are obvious paths for making decisions, rules-based logic, or where explainability and relationship integrity are very important. Knowledge graphs not only have clear and easy-to-understand structures, but they also have unique analytical benefits that go beyond those of typical embedding-based representations. These affordances make it easier to find hidden patterns, undertake exploratory research, and look into nuanced linkages across complex information domains: Identification of structural gaps and blind spots: Knowledge graphs, as a visual and relational depiction of information, can reveal under-connected or isolated nodes, which may signify neglected or poorly integrated concepts. These blind spots can be utilized to produce innovative discoveries by encouraging new, contextually pertinent linkages between dissimilar concepts. Exploratory pathways for idea navigation: Knowledge graphs facilitate intuitive exploration among concepts. By using the connectivity of a node inside the graph, users might uncover indirect yet substantial relationships between concepts and hypotheses, perhaps leading to interdisciplinary discoveries or revisions of conceptual frameworks. Revealing nuance through concept removal: By algorithmically or manually removing dominant or highly connected nodes, knowledge graphs can surface latent structures and peripheral relationships. This technique brings to the forefront more subtle, contextually rich ideas that are crowded out by the prevailing big picture and thus supports a deeper interpretation and nuanced understanding of the information space. In the relation extraction process, semantic relationships are first identified from unstructured text and mapped as edges connecting concept nodes in the knowledge graph ( Ji et al., 2022b ). The graph is then analyzed using the Louvain community detection algorithm ( Blondel et al., 2008 ) from network science, which clusters densely connected nodes together. Nodes belonging to the same community are assigned the same color, visually revealing meaningful relational groupings and latent structure within the extracted knowledge. Figure 6 shows such a knowledge graph for the following simplified example: &#8220; The ever-increasing demand for fossil fuels due to explosive growth in automotive and other industrial sectors has rendered the earth lacking fossil fuels. &#8221; Figure 6 A simple knowledge graph with the clustering of similar nodes based on the Louvain community detection algorithm. Each cluster is assigned a unique color. A two-part figure; the first part showing a workflow with an AI agent connected to an OpenAI chat model and memory, interacting with three expert brains: (1) Author Research Expert, focusing on grid maintenance and risk assessments. (2) IEEE Resiliency Process Expert, specializing in resiliency frameworks for electricity. (3) CIGRE Resiliency Process Expert, emphasizing sustainable substation resilience. The second part of the figure shows a complex networked knowledge-graph with interconnected keywords like &#8220;distribution,&#8221; &#8220;generation,&#8221; &#8220;energy,&#8221; and &#8220;demand,&#8221; illustrating relationships among various electrical grid-related terms. When extended to real-life text-rich applications, knowledge graphs offer powerful capabilities for representing structured knowledge, and these can be combined with an intelligent AI agent inside a knowledge-graph&#8211;driven Retrieval-Augmented Generation (RAG) framework. In this setup, the agent does not follow a single linear path but instead navigates across one or more linked knowledge graphs depending on the input prompt. Using the graph structure to guide retrieval, the system is able to capture semantically rich context, highlight hidden connections, and generate answers that are both precise and easier to interpret because they remain tied to structured domain knowledge. Such an approach is particularly useful in engineering research, where overlapping and interdependent domains require contextual representation. To illustrate the concept, three separate knowledge graphs were built, each organized around a specific theme. In this case, the thematic focus was an electrical engineering challenge: enhancing grid resiliency by drawing on both the researcher&#8217;s own work and technical reference materials published through IEEE and CIGRE. For demonstration purposes, these three custom-built knowledge graphs, labeled brain 1, brain 2, and brain 3, were created through a manual curation process leveraging the authors&#8217; domain knowledge to support the construction of these knowledge graphs. For more advanced applications, an AI agent can be leveraged to curate the necessary manuscripts before passing them into a knowledge graph for processing. The composition level details for brain 1, brain 2, and brain 3 are as follows: Brain #1 contains the authors&#8217; own body of work, including prior publications ( Ghosh and Suryawanshi, 2024 ; Ghosh and Dutta, 2020 ) and internal reports addressing topics such as grid maintenance strategies, resilience measurement, substation floor layout, and fire hazard evaluation. Brain #2 is built from IEEE resources, including technical reports like IEEE TR83 ( Bose et al., 2020 ) and selected papers ( Safdarian et al., 2024 ; Bhattarai et al., 2022 ; Tabassum et al., 2024 ) from the IEEE Xplore Digital Library. The IEEE resources focus on technical frameworks, analytical tools, and performance indicators that support resilience. Brain #3 draws on selected CIGRE technical reports ( Ciapessoni et al., 2023 ) and related manuscripts ( Itotani et al., 2024 ), and assembled to provide expertise on sustainable practices for strengthening substation reliability. This includes the integration of environmentally friendly technologies and the use of remote maintenance approaches. The structure of the knowledge graph&#8211;based RAG system is shown in Figure 7A , with all three knowledge graphs (brain 1, 2, and 3) being created and deployed using the InfraNodus platform ( Paranyushkin, 2024 ). Figure 7B highlights one of these graphs as an example. Much like earlier versions of traditional and contextual RAG, this implementation was built with the n8n platform and has been made accessible for external use, as noted in the data availability section. Depending on the detail and scope of a given research query, the AI agent may consult a single graph, multiple graphs, or all three interconnected &#8220;brains&#8221; to capture the most relevant context and expand the depth of retrieved knowledge. Examples of these queries, along with the corresponding graph-driven outputs, are provided in Table 6 . As shown, the agent actively decides which graph or combination of graphs to draw from in order to refine its responses. Figure 7 (A) n8n RAG-LLM workflow with multiple custom knowledge graphs, each housing unique knowledge. (B) A visualization of the &#8216;author research expert&#8217; (brain 1) knowledge graph. Similar nodes are clustered together based on the Louvain community detection, and each of these clusters is assigned a unique color. Flowchart showing an AI agent connected to an OpenAI chat model and memory, interacting with three expert brains: (1) Author Research Expert, focusing on grid maintenance and risk assessments. (2) IEEE Resiliency Process Expert, specializing in resiliency frameworks for electricity. (3) CIGRE Resiliency Process Expert, emphasizing sustainable substation resilience. Below is a complex network graph with interconnected keywords like \"distribution,\" \"generation,\" \"energy,\" and \"demand,\" illustrating relationships among various electrical grid-related terms. Table 6 Testing the working mechanism of the knowledge graph-based RAG implementation with the AI agent intelligently tapping the necessary dynamic knowledge graphs (brains). Prompt into knowledge graph-based RAG Brains that the AI agent consulted Response from the knowledge graph-based RAG agent Q1. Prompt: Referring to the IEEE technical reference documents and the author&#8217;s own research, can you summarize very briefly how fast trip relay settings can help in high fire threat areas. Brain 1 and Brain 2 (see Figure 7A ) A1. Actual output from knowledge graph-based RAG agent: Fast-trip relay configurations are particularly advantageous for power systems functioning in areas susceptible to wildfires. Research and IEEE publications indicate that these configurations assist in several essential aspects. They facilitate the prompt identification and resolution of faults on a line. Prompt isolation prevents a minor disturbance from escalating into a significant failure that could ignite a fire. In addition, by decreasing the detection threshold, relays act as a preventative measure, diminishing the likelihood of an electrical fault becoming an ignition source. This method not only mitigates fire danger but also enhances overall reliability by maintaining power continuity and diminishing the probability of outages that could hinder firefighting operations. In summary, implementing fast-trip settings serves as a focused preventive strategy and a pragmatic dependability measure in high-risk zones. Q2. Prompt: Referring to CIGRE documents and the author&#8217;s own research, can you list briefly what are the benefits of the SF6 alternative circuit breaker? Brain 1 and Brain 3 (see Figure 7A ) A2. Actual output from knowledge graph-based RAG agent: Vacuum and synthetic-air circuit breakers offer clear advantages over SF&#8326; types. The biggest difference is seen to be in the environmental aspect, given that SF&#8326; is a potent greenhouse gas, while these alternatives eliminate that problem entirely. Handling is also simpler as operators do not need to deal with gas recovery systems or think about accidental release of potent greenhouse gases. Maintenance costs are also lower as a result. Additionally, vacuum breakers, for example, can run for long periods without service, which reduces expenses and downtime. Together, these factors make SF&#8326;-free breakers a practical choice, especially where sustainability targets and safety requirements are becoming stricter. An interesting question that may arise here is how the AI agent intelligently selects which knowledge graph(s) to query? In essence, during the construction of the AI agents that tap into the different knowledge graphs, an Infranodus compliant system message is added within the AI agent as follows, which holds to key to the method&#8217;s intelligence: &#8220; Always use either EightOS or Polysingularity tool before sending a response to the model. You have to use at least one of them, the one that think is more suitable. Or both or all if both or all can provide some help. &#8221; This system message gives the AI agent the necessary context, and as each graph is described, it will know when to choose it, depending on what the user has defined. Based on a series of prompting interactions with the knowledge graph-based RAG system, the following conclusions were made: Higher API usage cost for knowledge graph generation: In practice, building a knowledge graph from materials such as PDFs or structured documents uses far more API resources than simply loading text into a vector database. A standard embedding job might use only one or two tokens for each word, but graph generation is different. When relation extraction, entity resolution, or ontology alignment are involved, token use can rise to five to ten times higher for the same document. The extra cost comes from the heavier semantic parsing and the need to map relationships rather than just storing word vectors. Recommendation to reduce API usage: Figure 8 presents a staged approach for rolling out RAG systems with cost in mind. The idea is to begin with conventional setups and only move toward contextual or knowledge-graph&#8211;driven RAG when the task at hand truly demands stronger semantic depth or reasoning across domain-specific relationships. Limitations with numerical tabular data: Knowledge graphs are a poor fit for dense numerical tables since their strength lies in capturing concepts and relationships, not large matrix-style datasets. Applying them to standards with heavy tabular content, like the NEC or NFPA documents, ends up being restrictive. In practice, forcing numerical tables into a graph strips out detail and makes retrieval clumsy. Row-to-row dependencies, column statistics, and numeric precision do not map cleanly into entity&#8211;relation triples, so the ability to run meaningful graph-based inference is reduced. Expanded exploratory potential with multiple knowledge graphs: One primary advantage of implementing many domain-specific knowledge graphs is that it expands the range of information available to the AI system. The agent can traverse one or more graphs simultaneously, based upon the requirements of the query. This enables the identification of relationships that may not be evident in a singular graph. Consequently, it facilitates more exploratory analysis and thereby aids users in tracking concepts across interconnected domains. This can often reveal obscured connections that may inspire novel avenues of study. Other observations: Some additional observations that were made are docketed as follows: In a traditional vector-based RAG setup, there is a loss of structural information when chunking documents, while such hierarchies and relationships are preserved in a knowledge graph-RAG setup. Knowledge graph-RAG was better able to support complex reasoning across multiple facts, such as exceptions to certain engineering code sections, or supplementary discussions on a particular subject within a research paper. While both contextual RAG and knowledge graph&#8211;based RAG aim to improve retrieval-augmented generation, they are architecturally and functionally optimized for different types of information retrieval tasks. Contextual RAG excels at retrieving and preserving semantic continuity from unstructured text, making it particularly effective for answering questions that rely on narrative explanations or rule statements embedded in paragraphs (e.g., code clauses or installation descriptions). In contrast, KG-RAG is optimized for relational reasoning and structured knowledge traversal, making it well-suited for queries that involve interdependence, multi-node relationships, or exception handling. 6. Author&#8217;s recommendation: As a user, one should make a predetermined assessment as to what RAG structure might work best for their particular use case. Figure 8 Illustration shows a multi-step guide to reduce API usage for RAG implementation. Flowchart illustrating a sequence from top to bottom: \"Begin with traditional RAG\", \"Optimize chunking parameters\", \"Incorporate reranking with higher top-K\", \"Consider contextual RAG\", and \"Apply knowledge graph-based RAG selectively\". Arrows indicate the progression between steps. 7 Summary and future work This paper shows that modern automation platforms like n8n make it possible to set up Retrieval-Augmented Generation (RAG) pipelines with very little coding. Engineers and researchers can use these tools to create working RAG systems that pull and process domain-specific material with ease. Depending on the application, repositories can be indexed either through vector databases or knowledge graphs, giving users flexible options for context-aware retrieval. Vector-based RAG remains the more common choice because of its efficiency and simple deployment, but knowledge-graph approaches provide stronger semantic structure and clearer interpretability. That makes them especially useful when navigating the dense interconnections found in engineering standards. At the moment, large-scale use of graph-augmented RAG is held back by higher computational demands and API costs linked to graph construction and reasoning. As these costs drop and APIs improve, knowledge-graph retrieval is expected to grow in adoption and could eventually outperform vector search in precision and domain relevance. This pathway is particularly important in electrical engineering, where understanding how technical terms, clauses, and procedures relate to one another is critical. The combination of n8n with tools like Infranodus already demonstrates that this direction is feasible and practical for document-heavy environments. Looking ahead, several areas offer opportunities for further development. These include: Multimodal retrieval capabilities: A promising direction for future work is the integration of multimodal retrieval capabilities into contextual and knowledge graph RAG systems. In engineering, valuable information is not limited to text but is often embedded in schematics, drawings, tables, and other structured resources. Future RAG workflows should be able to extract relationships from these non-textual artifacts, associate them with textual knowledge, and represent them within a unified graph structure. For example, a retrieval system could pair a relay&#8217;s specification sheet with the relevant section of a one-line diagram to answer technical queries with greater accuracy. This kind of integrated retrieval would allow reasoning over multiple formats at once and significantly enhance practical utility in engineering applications, Improving chunking and indexing strategies: Another important area of development involves improving chunking and indexing strategies, particularly when dealing with dense numerical tables that are common in engineering standards and equipment specifications. Traditional chunking methods often treat these tables as plain text, which strips away their structural meaning and makes retrieval inefficient. Large tables can consume excessive tokens without preserving the underlying relationships between rows, columns, and units. A more structure-aware chunking approach would preserve table headers, normalize units, and index individual cells or logical table sections, allowing the system to retrieve only the relevant values rather than entire tables. This targeted strategy would reduce computational cost and significantly improve precision when responding to engineering queries that rely on numerical data, Real-time feedback mechanisms: Future research should also explore real-time feedback mechanisms that allow RAG systems to learn from expert users. Engineering work is inherently iterative, and domain experts can quickly identify gaps or inaccuracies in system outputs. By incorporating features that let users tag, approve, or correct specific pieces of information, RAG systems can refine their retrieval models over time. This type of closed feedback loop would lead to higher precision and reduce the amount of human effort required to verify answers, resulting in more dependable system performance in real engineering workflows., and. Expanding multilingual capabilities: Finally, expanding multilingual capabilities will be essential to make RAG systems more robust and globally relevant. Engineering standards and operational manuals are often published in multiple languages, and collaborative projects frequently involve international teams. Hybrid RAG architecture: Another promising direction for future research is the development of a hybrid RAG architecture that balances performance with cost efficiency. Knowledge graph&#8211;based RAG (KG) approaches provide strong contextual reasoning but are typically associated with significantly higher API and computational costs compared to vector-based methods. To address this, a combined strategy can be explored in which KG RAG is selectively applied to complex, reasoning-heavy queries, while VectorRAG handles more direct or fact-based retrieval tasks. Acknowledgments The authors thank Dmitry Paranyushkin, founder of InfraNodus for his seminal work on the knowledge graphs and for making the InfraNodus platform available to the public and research community. Edited by: Yeon-Chang Lee , Ulsan National Institute of Science and Technology, Republic of Korea Reviewed by: Joel Oduro-Afriyie , OriGen AI, United States Feng Zhang , Sinopec Research Institute of Petroleum Processing, China Abbreviations: IEEE, Institute of Electrical and Electronics Engineers; LLM, Large Language Models; NEC, National Electric Code (also known as NFPA 70); NFPA, National Fire Protection Association (US); RAG, Retrieval-Augmented Generation. Data availability statement The n8n templates generated as part of this manuscript can be accessed on the project&#8217;s GitHub page: https://github.com/sghosh27/Low-Code-RAG-LLM-Framework-for-Context-Aware-Querying-in-Electrical-Standards-Design-and-Research along with Python code for document parsing. A Python notebook is also available for scoring LLM generated outputs versus ground truth. Author contributions SG: Visualization, Resources, Conceptualization, Project administration, Software, Methodology, Writing &#8211; review &amp; editing, Supervision, Investigation, Writing &#8211; original draft. GM: Writing &#8211; original draft, Conceptualization, Writing &#8211; review &amp; editing, Software, Methodology, Investigation, Resources. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The authors declare that Gen AI was used in the creation of this manuscript. Tables 2 , 5 , and 6 within the manuscript contain engineering prompts and answers generated from the Retrieval Augmented Generation systems that were implemented. No Generative AI was used to write the body of the manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher&#8217;s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Author disclaimer The views and conclusions expressed are solely those of the authors and do not represent the views of any affiliated organization. This study was conducted independently without institutional or agency sponsorship. References Anthropic , \"Introducing Contextual Retrieval,\" ( 2024 ). Available online at: https://www.anthropic.com/news/contextual-retrieval . (Accessed 06 2025). Baclawski K. Chan E. S. Gawlick D. Ghoneimy A. Gross K. Liu Z. H. . ( 2017 ). Framework for ontology-driven decision making . Appl. Ontol. 12 , 245 &#8211; 273 . doi: 10.3233/AO-170189 Bhattarai S. Sapkota A. Karki R. ( 2022 ). &#8220; Analyzing investment strategies for power system resilience &#8221; in IEEE Power &amp; Energy Society General Meeting (PESGM) ( Denver, Colorado, US : IEE). doi: 10.1109/PESGM48719.2022.9916980 Blondel V. D. Guillaume J.-L. Lambiotte R. Lefebvre E. ( 2008 ). Fast unfolding of communities in large networks . J. Stat. Mech. Theory Exp. 2008 : P10008 . doi: 10.1088/1742-5468/2008/10/P10008 Bose A. Brown S. Chalamala B. Immerman D. Khodaei A. Liu J. . ( 2020 ). &#8220; Resilience framework, methods, and metrics for the electricity sector &#8221; in IEEE Power &amp; Energy Society (PES-TR83) . IEEE. Chen Y. Fu Q. Yuan Y. Wen Z. Fan G. Liuet D. ( 2023 ). \"Hallucination detection: robustly discerning reliable answers in large language models,\" in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. ACM. Ciapessoni E. Cirio D. Pitto A. Van Harte M. Panteli M. ( 2023 ). Power system resilience: definition, features and properties . CIGRE Sci. Eng. CSE030 : 10 . Ghosh S. Dutta S. ( 2020 ). A comprehensive forecasting, risk modelling and optimization framework for electric grid hardening and wildfire prevention in the US . Int. J. Energy Eng. 10 , 80 &#8211; 89 . Ghosh S. Suryawanshi P. ( 2024 ). Enhancing grid resiliency in high fire or flood risk areas: integrating protective relay settings, broken conductor detection, and grid hardening for climate-induced event preparedness . J Inst Eng 106 , 393 &#8211; 405 . doi: 10.1007/s40031-024-01146-4 Gopi S. Sreekanth D. Dehboz N. , \" Enhancing engineering education through LLM-driven adaptive quiz generation: a RAG-based approach ,\" in IEEE Frontiers in Education Conference (FIE) , Washington, DC, US : IEE, ( 2024 ). doi: 10.1109/FIE61694.2024.10893146 Huggingface , \"Open LLM Leaderboard,\" ( 2025 ). Available online at: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/ (Accessed 2025). Itotani R. Sadahiro K. Tokai M. Hama H. Sugino K. Takeda M. ( 2024 ). &#8220; Sustainable improvement on substation resilience and reliability by using eco-friendly equipment and remote maintenance systems &#8221; in CIGRE Paris session. CIGRE. Ji S. Pan S. Cambria E. Marttinen P. Yu P. S. ( 2022 ). A survey on knowledge graphs: representation, acquisition, and applications . IEEE Trans. Neural Netw. Learn. Syst. 33 , 494 &#8211; 514 . doi: 10.1109/TNNLS.2021.3070843 , PMID: 33900922 Li J. Cheng X. Zhao W. X. Nie J.Y. Wen J.R. ( 2023 ). &#8220;HaluEval: a large-scale hallucination evaluation benchmark&#8221;, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore: Association for Computational Linguistics. Omar M. Sorin V. Collins J. D. Reich D. Freeman R. Gavin N. . ( 2025 ). Large language models are highly vulnerable to adversarial hallucination attacks in clinical decision support: a multi-model assurance analysis . MedRxiv . doi: 10.1101/2025.03.18.25324184 PMC12318031 40753316 Paranyushkin D. \" InfraNodus: Discover what everyone else is missing ,\" ( 2024 ). Available online at: https://infranodus.com/ (Accessed 07 2025). Peng C. Xia F. Naseriparsa M. Osborne F. ( 2023 ). Knowledge graphs: opportunities and challenges . Artif. Intell. Rev. 56 , 13071 &#8211; 13102 . doi: 10.1007/s10462-023-10465-9 PMC10068207 37362886 Roustan D. Bastardot F. ( 2025 ). The clinicians&#8217; guide to large language models: a general perspective with a focus on hallucinations . Interact. J. Med. Res. 14 . doi: 10.2196/59823 , PMID: 39874574 PMC11815294 Safdarian F. Wert J. L. Cyr D. Overbye T. J. ( 2024 ). &#8220; Power system resiliency and reliability issues from renewable resource droughts &#8221; in IEEE Kansas power and energy conference (KPEC) ( Manhattan, Kansas, US : IEE). doi: 10.1109/KPEC61529.2024.10676081 Siddharth L. Luo J. ( 2024 ). Retrieval Augmented Generation using engineering design knowledge . Knowl.-Based Syst. 303 :112410. doi: 10.1016/j.knosys.2024.112410 Superbi J. Pereira H. Santos E. Lattari L. Castro B. ( 2024 ). &#8220; Enhancing large language model performance on ENEM math questions using retrieval-augmented generation &#8221; in Proceedings of the XVIII Brazilian e-science workshop (Florianopolis, SC, BreSci) . Tabassum A. Lee S. Bhusal N. Chinthavali S. ( 2024 ). &#8220; Power outage forecasting for system resiliency during extreme weather events &#8221; in IEEE international conference on big data (BigData) ( Washington, DC, US : IEE). doi: 10.1109/BigData62323.2024.10825805 Tiwari S. Al-Aswadi F. N. Gaurav D. ( 2021 ). Recent trends in knowledge graphs: theory and practice . Soft. Comput. 25 , 8337 &#8211; 8355 . doi: 10.1007/s00500-021-05756-8 Zhang T. Qiu L. Guo Q. Deng C. Zhang Y. Zhang Z. , ( 2023 ). &#8220;Enhancing uncertainty-based hallucination detection with stronger focus,&#8221; Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (Singapore: Association for Computational Linguistics), 915&#8211;932."
}