{
  "pmcid": "PMC12673485",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:23.468008",
  "metadata": {
    "journal_title": "Frontiers in Neuroinformatics",
    "journal_nlm_ta": "Front Neuroinform",
    "journal_iso_abbrev": "Front Neuroinform",
    "journal": "Frontiers in Neuroinformatics",
    "pmcid": "PMC12673485",
    "pmid": "41346493",
    "doi": "10.3389/fninf.2025.1679196",
    "title": "Cross-modal privacy-preserving synthesis and mixture-of-experts ensemble for robust ASD prediction",
    "authors": [
      "Revathy J.",
      "M. Karthiga"
    ],
    "abstract": "Introduction Autism Spectrum Disorder (ASD) diagnosis remains complex due to limited access to large-scale multimodal datasets and privacy concerns surrounding clinical data. Traditional methods rely heavily on resource-intensive clinical assessments and are constrained by unimodal or non-adaptive learning models. To address these limitations, this study introduces AutismSynthGen, a privacy-preserving framework for synthesizing multimodal ASD data and enhancing prediction accuracy. Materials and methods The proposed system integrates a Multimodal Autism Data Synthesis Network (MADSN), which employs transformer-based encoders and cross-modal attention within a conditional GAN to generate synthetic data across structural MRI, EEG, behavioral vectors, and severity scores. Differential privacy is enforced via DP-SGD ( ε  ≤ 1.0). A complementary Adaptive Multimodal Ensemble Learning (AMEL) module, consisting of five heterogeneous experts and a gating network, is trained on both real and synthetic data. Evaluation is conducted on the ABIDE, NDAR, and SSC datasets using metrics such as AUC, F1 score, MMD, KS statistic, and BLEU. Results Synthetic augmentation improved model performance, yielding validation AUC gains of ≥ 0.04. AMEL achieved an AUC of 0.98 and an F1 score of 0.99 on real data and approached near-perfect internal performance (AUC ≈ 1.00, F1 ≈ 1.00) when synthetic data were included. Distributional metrics (MMD = 0.04; KS = 0.03) and text similarity (BLEU = 0.70) demonstrated high fidelity between the real and synthetic samples. Ablation studies confirmed the importance of cross-modal attention and entropy-regularized expert gating. Discussion AutismSynthGen offers a scalable, privacy-compliant solution for augmenting limited multimodal datasets and enhancing ASD prediction. Future directions include semi-supervised learning, explainable AI for clinical trust, and deployment in federated environments to broaden accessibility while maintaining privacy.",
    "keywords": [
      "Autism spectrum disorder",
      "multimodal data synthesis",
      "differential privacy",
      "generative adversarial network",
      "ensemble learning",
      "transformer",
      "mixture of experts"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Front Neuroinform</journal-id><journal-id journal-id-type=\"iso-abbrev\">Front Neuroinform</journal-id><journal-id journal-id-type=\"pmc-domain-id\">647</journal-id><journal-id journal-id-type=\"pmc-domain\">frontneuroinfo</journal-id><journal-id journal-id-type=\"publisher-id\">Front. Neuroinform.</journal-id><journal-title-group><journal-title>Frontiers in Neuroinformatics</journal-title></journal-title-group><issn pub-type=\"epub\">1662-5196</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12673485</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12673485.1</article-id><article-id pub-id-type=\"pmcaid\">12673485</article-id><article-id pub-id-type=\"pmcaiid\">12673485</article-id><article-id pub-id-type=\"pmid\">41346493</article-id><article-id pub-id-type=\"doi\">10.3389/fninf.2025.1679196</article-id><article-version-alternatives><article-version article-version-type=\"pmc-version\">1</article-version><article-version article-version-type=\"Version of Record\" vocab=\"NISO-RP-8-2008\"/></article-version-alternatives><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Cross-modal privacy-preserving synthesis and mixture-of-experts ensemble for robust ASD prediction</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Revathy</surname><given-names initials=\"J\">J.</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3273141\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>M.</surname><given-names initials=\"K\">Karthiga</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3158561\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib></contrib-group><aff id=\"aff1\"><label>1</label><institution>Department of Artificial Intelligence and Data Science, Christ the King Engineering College</institution>, <city>Coimbatore, Tamil Nadu</city>, <country country=\"in\">India</country></aff><aff id=\"aff2\"><label>2</label><institution>Department of Computer Science and Engineering, Bannari Amman Institute of Technology</institution>, <city>Erode, Tamil Nadu</city>, <country country=\"in\">India</country>,</aff><author-notes><corresp id=\"c001\"><label>*</label>Correspondence: Karthiga M., <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:karthigam@bitsathy.ac.in\">karthigam@bitsathy.ac.in</email></corresp></author-notes><pub-date publication-format=\"electronic\" date-type=\"pub\" iso-8601-date=\"2025-11-19\"><day>19</day><month>11</month><year>2025</year></pub-date><pub-date publication-format=\"electronic\" date-type=\"collection\"><year>2025</year></pub-date><volume>19</volume><issue-id pub-id-type=\"pmc-issue-id\">480826</issue-id><elocation-id>1679196</elocation-id><history><date date-type=\"received\"><day>04</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>28</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>19</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>04</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 09:25:13.193\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Revathy and M.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Revathy and M</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\" start_date=\"2025-11-19\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"fninf-19-1679196.pdf\"/><abstract><sec id=\"sec1\"><title>Introduction</title><p>Autism Spectrum Disorder (ASD) diagnosis remains complex due to limited access to large-scale multimodal datasets and privacy concerns surrounding clinical data. Traditional methods rely heavily on resource-intensive clinical assessments and are constrained by unimodal or non-adaptive learning models. To address these limitations, this study introduces AutismSynthGen, a privacy-preserving framework for synthesizing multimodal ASD data and enhancing prediction accuracy.</p></sec><sec id=\"sec2\"><title>Materials and methods</title><p>The proposed system integrates a Multimodal Autism Data Synthesis Network (MADSN), which employs transformer-based encoders and cross-modal attention within a conditional GAN to generate synthetic data across structural MRI, EEG, behavioral vectors, and severity scores. Differential privacy is enforced via DP-SGD (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0). A complementary Adaptive Multimodal Ensemble Learning (AMEL) module, consisting of five heterogeneous experts and a gating network, is trained on both real and synthetic data. Evaluation is conducted on the ABIDE, NDAR, and SSC datasets using metrics such as AUC, F1 score, MMD, KS statistic, and BLEU.</p></sec><sec id=\"sec3\"><title>Results</title><p>Synthetic augmentation improved model performance, yielding validation AUC gains of &#8805; 0.04. AMEL achieved an AUC of 0.98 and an F1 score of 0.99 on real data and approached near-perfect internal performance (AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00) when synthetic data were included. Distributional metrics (MMD&#8239;=&#8239;0.04; KS&#8239;=&#8239;0.03) and text similarity (BLEU&#8239;=&#8239;0.70) demonstrated high fidelity between the real and synthetic samples. Ablation studies confirmed the importance of cross-modal attention and entropy-regularized expert gating.</p></sec><sec id=\"sec4\"><title>Discussion</title><p>AutismSynthGen offers a scalable, privacy-compliant solution for augmenting limited multimodal datasets and enhancing ASD prediction. Future directions include semi-supervised learning, explainable AI for clinical trust, and deployment in federated environments to broaden accessibility while maintaining privacy.</p></sec></abstract><kwd-group><kwd>Autism spectrum disorder</kwd><kwd>multimodal data synthesis</kwd><kwd>differential privacy</kwd><kwd>generative adversarial network</kwd><kwd>ensemble learning</kwd><kwd>transformer</kwd><kwd>mixture of experts</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that no financial support was received for the research and/or publication of this article.</funding-statement></funding-group><counts><fig-count count=\"30\"/><table-count count=\"4\"/><equation-count count=\"16\"/><ref-count count=\"44\"/><page-count count=\"29\"/><word-count count=\"13815\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec5\"><label>1</label><title>Introduction</title><p>Autism spectrum disorder (ASD) encompasses a group of heterogeneous neurodevelopmental conditions defined by persistent deficits in social communication and interaction, along with restricted, repetitive patterns of behavior and interests. Early and accurate identification of ASD is critical: timely intervention can profoundly improve social, cognitive, and adaptive outcomes, yet standard diagnostic procedures remain labor-intensive and subjective. Clinicians currently rely on structured assessments, such as the Autism Diagnostic Observation Schedule (ADOS) and the Autism Diagnostic Interview&#8211;Revised (ADI-R), which require extensive training, can take several hours per evaluation, and exhibit substantial inter-rater variability (<xref rid=\"ref20\" ref-type=\"bibr\">Levy et al., 2011</xref>). Meanwhile, the prevalence of ASD has risen to an estimated 1&#8211;2% among children worldwide, imposing growing burdens on healthcare systems, educational services, and families (<xref rid=\"ref9\" ref-type=\"bibr\">Ding et al., 2024</xref>; <xref rid=\"ref12\" ref-type=\"bibr\">Friedrich et al., 2023</xref>).</p><p>In response to these limitations, deep learning approaches have emerged as promising solutions for automating the detection of ASD. Convolutional neural networks (CNNs) applied to structural and functional MRI have shown encouraging results. For instance, ASD-DiagNet leveraged an autoencoder with perceptual loss and data augmentation via linear interpolation to achieve up to 80% classification accuracy on fMRI scans (<xref rid=\"ref10\" ref-type=\"bibr\">Eslami et al., 2019</xref>). Similarly, generative adversarial networks (GANs) have been adapted to synthesize realistic biomedical time series. For instance, EEG-GAN demonstrated that GAN-based augmentation of electroencephalographic (EEG) data can enhance downstream classification performance in brain&#8211;computer interface tasks, suggesting applicability to clinical EEG analysis (<xref rid=\"ref15\" ref-type=\"bibr\">Hartmann et al., 2018</xref>). Despite these achievements, such unimodal strategies overlook the full spectrum of ASD biomarkers.</p><p>Integrating multimodal data&#8212;combining neuroimaging, electrophysiology, genetic variants, and behavioral assessments&#8212;can exploit complementary information and boost diagnostic accuracy. Recent reviews confirm that attention-based fusion of fMRI and EEG consistently outperforms single-modality models (<xref rid=\"ref7\" ref-type=\"bibr\">Dcouto and Pradeepkandhasamy, 2024</xref>). Large public resources, including ABIDE (&#8776;2,200 subjects across 17 sites), NDAR (&#8776;1,100 high-density EEG recordings paired with behavioral scales), and SSC (&#8776;2,600 simplex families with whole-exome sequencing and ADOS/ADI-R measures), provide rich multimodal datasets but face challenges of limited cohort sizes, inter-site variability, and stringent privacy constraints (<xref rid=\"ref8\" ref-type=\"bibr\">Di Martino et al., 2017</xref>; <xref rid=\"ref27\" ref-type=\"bibr\">Payakachat et al., 2016</xref>; <xref rid=\"ref20\" ref-type=\"bibr\">Levy et al., 2011</xref>).</p><p>To address data scarcity and privacy concerns, differentially private generative models have been proposed. DP-CGAN introduced per-sample gradient clipping and R&#233;nyi differential privacy accounting to limit privacy leakage while generating synthetic tabular medical records (<xref rid=\"ref35\" ref-type=\"bibr\">Torkzadehmahani et al., 2019</xref>), and DP-CTGAN extended this approach to a federated setting by conditioning on feature subsets (<xref rid=\"ref11\" ref-type=\"bibr\">Fang et al., 2022</xref>). More recently, GARL combined InfoGAN with deep Q-learning to iteratively refine synthetic neuroimaging samples, reporting significant classification gains on ABIDE data (<xref rid=\"ref43\" ref-type=\"bibr\">Zhou et al., 2024a</xref>). However, these approaches typically target a single modality and do not enforce consistency across modalities, limiting their utility for downstream multimodal systems.</p><p>On the predictive front, ensemble learning offers a framework for integrating heterogeneous feature representations. Static ensembles&#8212;such as simple averaging or majority voting&#8212;provide modest gains but fail to adapt weights based on sample-specific modality relevance. Mixture-of-experts architectures, featuring learnable gating networks that dynamically weight model outputs, have shown success in other domains; however, their application to privacy-preserving, multimodal ASD data remains largely unexplored.</p><p>In this study, AutismSynthGen, an end-to-end framework that addresses multimodal data scarcity and privacy while delivering robust ASD prediction, is proposed. The key contributions are as follows:</p><list list-type=\"order\"><list-item><p><bold>Multimodal Data Synthesis (MADSN)</bold>: A conditional GAN with transformer-based encoders (6 layers, eight heads, hidden size 512) and cross-modal attention to jointly model structural MRI, EEG time series, behavioral feature vectors, and calibrated severity scores. Rigorous differential privacy (DP-SGD with clipping norm 1.0 and noise multiplier 1.2) guarantees <italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0 at <italic toggle=\"yes\">&#948;</italic>&#8239;=&#8239;10<sup>&#8722;5</sup>.</p></list-item><list-item><p><bold>Adaptive Ensemble Learning (AMEL)</bold>: A mixture-of-experts classifier integrating five heterogeneous models&#8212;a 3D-CNN, a 1D-CNN, an MLP, a cross-modal transformer, and a graph neural network&#8212;whose logits are adaptively weighted by a two-layer gating MLP (hidden 128, ReLU) with entropy regularization (<italic toggle=\"yes\">&#955;</italic>&#8239;=&#8239;0.01).</p></list-item><list-item><p><bold>Comprehensive Evaluation</bold>: Demonstration on ABIDE, NDAR, and SSC datasets, where MADSN-augmented training raises the validation AUC by &#8805; 0.04 over strong uni- and multimodal baselines.</p></list-item><list-item><p><bold>Statistical and Privacy Analysis</bold>: Conducted extensive ablations on cross-modal consistency and DP parameters, as well as bootstrap confidence intervals and paired Wilcoxon tests, to confirm both the efficacy and stability of AutismSynthGen under <italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0 privacy constraints.</p></list-item></list><p>By unifying transformer-driven multimodal synthesis, formal privacy guarantees, and adaptive ensemble prediction, AutismSynthGen advances the state of the art in reliable, privacy-compliant ASD detection.</p></sec><sec id=\"sec6\"><label>2</label><title>Related research</title><sec id=\"sec7\"><label>2.1</label><title>Unimodal MRI-based ASD detection</title><p>Structural and functional MRI have been extensively studied using deep learning classifiers. Early CNN-based pipelines applied to ABIDE data (<xref rid=\"ref8\" ref-type=\"bibr\">Di Martino et al., 2017</xref>) achieved promising results: Moridian et al. reported up to 78% accuracy but highlighted sensitivity to inter-site variability and limited cohort sizes (<xref rid=\"ref23\" ref-type=\"bibr\">Moridian et al., 2022</xref>), while ASD-DiagNet combined a convolutional autoencoder and perceptual loss to reach &#8776; 80% accuracy on fMRI scans, albeit with coarse anatomical synthesis (<xref rid=\"ref10\" ref-type=\"bibr\">Eslami et al., 2019</xref>). Subsequent research has addressed generalization and richer feature extraction: Liu et al. surveyed advanced neuroimaging models, concluding that hybrid 3D-CNN and attention mechanisms yield stronger embeddings (<xref rid=\"ref22\" ref-type=\"bibr\">Liu et al., 2021</xref>); <xref rid=\"ref16\" ref-type=\"bibr\">Heinsfeld et al. (2018)</xref> demonstrated end-to-end deep models with site-adaptation layers to improve cross-validation performance; <xref rid=\"ref32\" ref-type=\"bibr\">Singh et al. (2023)</xref> introduced transfer learning across ABIDE splits to mitigate dataset bias; and <xref rid=\"ref26\" ref-type=\"bibr\">Okada et al. (2025)</xref> employed RNN-attention networks on volumetric MRI, capturing sequential spatial patterns. Multi-view frameworks, such as MultiView, have further fused different MRI contrasts to enhance detection robustness (<xref rid=\"ref33\" ref-type=\"bibr\">Song et al., 2024</xref>). Additionally, adversarial domain adaptation has been utilized to align feature distributions across sites (<xref rid=\"ref13\" ref-type=\"bibr\">Gupta et al., 2025</xref>). More recently, self-supervised pretraining on resting-state fMRI has been shown to improve downstream ASD classification (<xref rid=\"ref43\" ref-type=\"bibr\">Zhou et al., 2024a</xref>).</p></sec><sec id=\"sec8\"><label>2.2</label><title>Unimodal EEG and behavioral models</title><p>High-density EEG offers complementary temporal biomarkers. EEG-GAN pioneered GAN-driven EEG augmentation, improving downstream classification in BCI contexts, although it has not yet been applied to ASD (<xref rid=\"ref15\" ref-type=\"bibr\">Hartmann et al., 2018</xref>). Aslam et al. reviewed multi-channel EEG feature engineering for ASD, advocating spectral and connectivity features (<xref rid=\"ref2\" ref-type=\"bibr\">Aslam et al., 2022</xref>). Behavioral assessments&#8212;standardized scales for social communication and repetitive behaviors&#8212;have also been modeled directly. Rubio-Mart&#237;n et al. combined SVM, random forests, and an MLP on clinical vectors, achieving an AUC of approximately 0.75 on NDAR behavioral data (<xref rid=\"ref29\" ref-type=\"bibr\">Rubio-Mart&#237;n et al., 2024</xref>). Gamified assessment data, processed via signal-processing pipelines and ML classifiers, further underscored the utility of interactive behavioral measures (<xref rid=\"ref5\" ref-type=\"bibr\">Bernabeu, 2022</xref>; <xref rid=\"ref6\" ref-type=\"bibr\">Borodin et al., 2021</xref>).</p></sec><sec id=\"sec9\"><label>2.3</label><title>Genetic and clinical score-based approaches</title><p>Genomic studies on simplex families have largely focused on risk-locus discovery rather than classification (<xref rid=\"ref21\" ref-type=\"bibr\">Li et al., 2024</xref>). <xref rid=\"ref20\" ref-type=\"bibr\">Levy et al. (2011)</xref> analyzed <italic toggle=\"yes\">de novo</italic> and transmitted CNVs in SSC data to identify ASD-associated variants. Automated pipelines have since applied shallow architectures to SNP embeddings, yet without integrating clinical scales. <xref rid=\"ref3\" ref-type=\"bibr\">Avasthi et al. (2025)</xref> utilized transformer-based NLP to extract clinical text for ASD indicators, and graph convolutional networks have been leveraged to model correlations among behavioral domains (<xref rid=\"ref41\" ref-type=\"bibr\">Washington et al., 2022</xref>). Joint classification and severity prediction via multi-task learning have also been explored (<xref rid=\"ref40\" ref-type=\"bibr\">Wang et al., 2017</xref>).</p></sec><sec id=\"sec10\"><label>2.4</label><title>Privacy-preserving generative models</title><p>Differential privacy (DP) has been integrated into GANs for the synthesis of sensitive medical data. DP-CGAN enforced per-sample clipping and R&#233;nyi DP accounting (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0) on tabular EHRs (<xref rid=\"ref35\" ref-type=\"bibr\">Torkzadehmahani et al., 2019</xref>), while DP-CTGAN extended conditional GANs to federated settings, balancing utility and privacy for mixed datasets (<xref rid=\"ref11\" ref-type=\"bibr\">Fang et al., 2022</xref>). <xref rid=\"ref42\" ref-type=\"bibr\">Zhang et al. (2021)</xref> introduced a DP-federated GAN for continuous medical imaging features, and <xref rid=\"ref39\" ref-type=\"bibr\">Wang et al. (2024)</xref> applied DP-SGM to neuroimaging data (DP-SNM), achieving strong privacy with minimal quality loss. The GARL framework combined InfoGAN with deep Q-learning to iteratively refine MRI synthesis under privacy constraints, although it was limited to imaging alone (<xref rid=\"ref43\" ref-type=\"bibr\">Zhou et al., 2024a</xref>). Broader surveys of privacy-utility trade-offs in medical GANs have mapped parameter impacts on sample fidelity and privacy leakage (<xref rid=\"ref38\" ref-type=\"bibr\">Viswalingam and Kumar, 2025</xref>; <xref rid=\"ref24\" ref-type=\"bibr\">Nanayakkara et al., 2022</xref>).</p></sec><sec id=\"sec11\"><label>2.5</label><title>Multimodal fusion techniques / privacy-preserving frameworks</title><p>Attention-based fusion of heterogeneous modalities has demonstrated superior performance compared to unimodal baselines. <xref rid=\"ref7\" ref-type=\"bibr\">Dcouto and Pradeepkandhasamy (2024)</xref> surveyed recent multimodal deep learning in ASD, highlighting gains from fMRI&#8211;EEG attention fusion but noting a lack of end-to-end models with formal consistency constraints. <xref rid=\"ref4\" ref-type=\"bibr\">Baltru&#353;aitis et al. (2018)</xref> provided a taxonomy of early, late, and hybrid fusion strategies, identifying cross-modal transformers as particularly promising for capturing intermodal correlations. Tools such as MultiView have operationalized early fusion in autism research (<xref rid=\"ref33\" ref-type=\"bibr\">Song et al., 2024</xref>); federated multimodal learning has been proposed to preserve privacy across sites (<xref rid=\"ref19\" ref-type=\"bibr\">Lakhan et al., 2023</xref>), and contrastive self-supervised methods have been introduced for joint embedding of multimodal ASD data (<xref rid=\"ref28\" ref-type=\"bibr\">Qu et al., 2025</xref>; <xref rid=\"ref37\" ref-type=\"bibr\">Vimbi et al., 2025</xref>).</p><p>Recent advances also integrate explainable federated learning for ASD prediction, combining privacy preservation with interpretability (<xref rid=\"ref1\" ref-type=\"bibr\">Alshammari et al., 2024</xref>). Such approaches align with our emphasis on privacy and transparency, although they do not generate synthetic data or enforce cross-modal consistency as in AutismSynthGen.</p></sec><sec id=\"sec12\"><label>2.6</label><title>Ensemble and mixture-of-experts methods</title><p>Adaptive ensemble strategies offer robustness by weighting diverse experts per sample. Sparsely gated mixture-of-experts (MoE) layers have demonstrated scalable adaptive weighting in language models (<xref rid=\"ref31\" ref-type=\"bibr\">Shazeer et al., 2017</xref>); in medical contexts, ensemble deep learning has been applied to multimodal ASD screening, yielding improved sensitivity but without sample-specific gating (<xref rid=\"ref34\" ref-type=\"bibr\">Taiyeb Khosroshahi et al., 2025</xref>). <xref rid=\"ref29\" ref-type=\"bibr\">Rubio-Mart&#237;n et al. (2024)</xref> demonstrated the benefits of simple averaging of heterogeneous classifiers on behavioral data, while <xref rid=\"ref25\" ref-type=\"bibr\">Nguyen et al. (2023)</xref> proposed MoE with gating regularization for noisy medical inputs. Recent studies have applied attention-based MoE to healthcare data, underscoring the importance of entropy penalties in avoiding expert collapse (<xref rid=\"ref14\" ref-type=\"bibr\">Han et al., 2024</xref>).</p></sec><sec id=\"sec13\"><label>2.7</label><title>Privacy-utility trade-off analyses</title><p>Comprehensive investigations into privacy-utility trade-offs have quantified the impact of DP parameters on the performance of generative models (<xref rid=\"ref30\" ref-type=\"bibr\">Schielen et al., 2024</xref>). Nanayakkara et al. evaluated differentially private GANs across imaging benchmarks, mapping <italic toggle=\"yes\">&#949;</italic> values to downstream classification accuracy (<xref rid=\"ref24\" ref-type=\"bibr\">Nanayakkara et al., 2022</xref>). <xref rid=\"tab1\" ref-type=\"table\">Table 1</xref> compares the existing ASD detection frameworks.</p><table-wrap position=\"float\" id=\"tab1\" orientation=\"portrait\"><label>Table 1</label><caption><p>Comparison of existing ASD detection frameworks: key methodologies, datasets employed, principal advantages, and noted limitations.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">S. no</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Ref. no</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Proposed research</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Dataset used</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Pros</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Cons</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">1</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref23\" ref-type=\"bibr\">Moridian et al. (2022)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CNN-based ASD detection</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ABIDE (structural &amp; fMRI)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">End-to-end feature learning</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Sensitive to site variability; limited sample size</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">2</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref10\" ref-type=\"bibr\">Eslami et al. (2019)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ASD DiagNet (autoencoder + GAN augmentation)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ABIDE (fMRI)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Perceptual loss improves feature quality</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Coarse anatomical detail in synthesized images</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">3</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref15\" ref-type=\"bibr\">Hartmann et al. (2018)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">EEG-GAN for EEG synthesis</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Public EEG benchmarks</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Realistic EEG generation</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Not evaluated for ASD</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">4</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref29\" ref-type=\"bibr\">Rubio-Mart&#237;n et al. (2024)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Behavioral + NLP fusion (MLP, SVM, RF)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">NDAR (behavioral scales, text)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Integrates textual and numerical clinical data</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No multimodal interaction</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">5</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref20\" ref-type=\"bibr\">Levy et al. (2011)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CNV risk-locus analysis</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">SSC (de novo CNVs, WES)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Identification of ASD-associated variants</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No predictive classification</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">6</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref35\" ref-type=\"bibr\">Torkzadehmahani et al. (2019)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">DP-CGAN for tabular medical data</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Medical EHR cohorts</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Strong privacy guarantees (&#949;&#8239;&#8804;&#8239;1.0)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Reduced sample realism; tabular only</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">7</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref11\" ref-type=\"bibr\">Fang et al. (2022)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">DP-CTGAN (federated)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MIMIC-III (tabular)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Federated DP; improved utility over DP-CGAN</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Discrete features only</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">8</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref43\" ref-type=\"bibr\">Zhou et al. (2024a)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">GARL (InfoGAN + DQN)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ABIDE (MRI)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Iterative refinement yields high-fidelity MRI samples</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Single modality; no EEG/behavioral consistency</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">9</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref7\" ref-type=\"bibr\">Dcouto and Pradeepkandhasamy (2024)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Attention-based fMRI + EEG fusion review</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multiple studies</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Demonstrates the benefits of hybrid fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Lacks an end-to-end model and privacy guarantees</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">10</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref4\" ref-type=\"bibr\">Baltru&#353;aitis et al. (2018)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multimodal ML survey &amp; taxonomy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">N/A</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Comprehensive fusion taxonomy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No empirical ASD implementation</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">11</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref31\" ref-type=\"bibr\">Shazeer et al. (2017)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Sparsely-gated Mixture-of-Experts (MoE)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Language corpora</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Scalable adaptive weighting via learnable gating</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">High compute; not tailored to medical or multimodal data</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">12</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref42\" ref-type=\"bibr\">Zhang et al. (2021)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">FedDPGAN for medical imaging</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">COVID-19 CT scans</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Federated DP for imaging</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Not applied to ASD</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">13</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref40\" ref-type=\"bibr\">Wang et al. (2017)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">DP-SNM for neuroimaging</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Private neuroimaging cohorts</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">DP for continuous imaging</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Single modality; no fusion</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">14</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref14\" ref-type=\"bibr\">Han et al. (2024)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">FuseMoE: MoE Transformers for Fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multimodal benchmarks</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Flexible cross-modal fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No formal privacy guarantees</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">15</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref24\" ref-type=\"bibr\">Nanayakkara et al. (2022)</xref>\n</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Privacy-utility trade-off visualization</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Synthetic benchmarks</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Maps the DP impact on utility comprehensively</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No ASD-specific evaluation</td></tr></tbody></table></table-wrap></sec><sec id=\"sec14\"><label>2.8</label><title>Research gap</title><p>Despite substantial advances in unimodal deep learning for ASD detection&#8212;such as CNN-based classifiers on fMRI (<xref rid=\"ref23\" ref-type=\"bibr\">Moridian et al., 2022</xref>; <xref rid=\"ref10\" ref-type=\"bibr\">Eslami et al., 2019</xref>), hybrid autoencoder&#8211;GAN models (<xref rid=\"ref10\" ref-type=\"bibr\">Eslami et al., 2019</xref>), and GAN-driven EEG augmentation (<xref rid=\"ref15\" ref-type=\"bibr\">Hartmann et al., 2018</xref>)&#8212;these approaches remain confined to single modalities and often overfit small, heterogeneous cohorts. Differentially private GANs have been applied to tabular medical records (<xref rid=\"ref35\" ref-type=\"bibr\">Torkzadehmahani et al., 2019</xref>) and federated settings (<xref rid=\"ref11\" ref-type=\"bibr\">Fang et al., 2022</xref>; <xref rid=\"ref39\" ref-type=\"bibr\">Wang et al., 2024</xref>), but they neither extend to continuous neuroimaging or time-series data nor enforce consistency across EEG, behavioral, and imaging modalities.</p><p>Although attention-based fusion methods demonstrate improved performance for paired fMRI&#8211;EEG inputs (<xref rid=\"ref7\" ref-type=\"bibr\">Dcouto and Pradeepkandhasamy, 2024</xref>; <xref rid=\"ref44\" ref-type=\"bibr\">Zhou et al., 2024b</xref>) and surveys outline promising multimodal fusion taxonomies (<xref rid=\"ref4\" ref-type=\"bibr\">Baltru&#353;aitis et al., 2018</xref>), end-to-end architectures that jointly synthesize and integrate more than two modalities under formal privacy constraints are still lacking. Finally, ensemble strategies in ASD classification have largely relied on static averaging of expert outputs (<xref rid=\"ref29\" ref-type=\"bibr\">Rubio-Mart&#237;n et al., 2024</xref>), whereas scalable, sample-adaptive mixture-of-experts frameworks that have proven effective in other domains (<xref rid=\"ref31\" ref-type=\"bibr\">Shazeer et al., 2017</xref>) remain unexplored in this context.</p><p>The proposed framework addresses these gaps through two key innovations. First, a transformer-based conditional GAN incorporates cross-modal attention to generate coherent synthetic MRI, EEG, behavioral, and severity data, while differential privacy via DP-SGD (clipping norm 1.0, noise multiplier 1.2) guarantees &#949;&#8239;&#8804;&#8239;1.0 leakage bounds (<xref rid=\"ref11\" ref-type=\"bibr\">Fang et al., 2022</xref>; <xref rid=\"ref35\" ref-type=\"bibr\">Torkzadehmahani et al., 2019</xref>). Second, a mixture-of-experts ensemble employs five heterogeneous models&#8212;3D-CNN, 1D-CNN, MLP, cross-modal transformer, and GNN&#8212;whose logits are dynamically weighted by an entropy-regularized gating network, enabling sample-specific emphasis on the most informative modalities (<xref rid=\"ref31\" ref-type=\"bibr\">Shazeer et al., 2017</xref>; <xref rid=\"ref14\" ref-type=\"bibr\">Han et al., 2024</xref>). Rigorous evaluation on ABIDE (<xref rid=\"ref8\" ref-type=\"bibr\">Di Martino et al., 2017</xref>), NDAR (<xref rid=\"ref27\" ref-type=\"bibr\">Payakachat et al., 2016</xref>), and SSC (<xref rid=\"ref20\" ref-type=\"bibr\">Levy et al., 2011</xref>) demonstrates statistically significant AUC improvements (&#8805; 0.04) over strong unimodal, static ensemble, and non-private baselines, thus bridging the identified research gaps in privacy-compliant multimodal synthesis and adaptive ASD prediction.</p></sec></sec><sec id=\"sec15\"><label>3</label><title>Proposed methodology</title><p>The AutismSynthGen framework jointly learns to synthesize multimodal autism data and to analyze it via an ensemble of predictive models. In our approach, a Multimodal Autism Data Synthesis Network (MADSN) uses transformer-based encoders and a conditional GAN to generate realistic multimodal data (e.g., neuroimaging, demographic vectors, behavioral). A complementary Adaptive Multimodal Ensemble Learning (AMEL) module trains a mixture-of-experts classifier on the synthesized (and real) data, assigning weights to each expert based on its performance and modality. This combined pipeline enables robust autism prediction and data augmentation while incorporating cross-modal consistency and differential privacy constraints for sensitive data. The overall flow is illustrated in <xref rid=\"fig1\" ref-type=\"fig\">Figure 1</xref>.</p><fig position=\"float\" id=\"fig1\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Overall AutismSynthGen architectural workflow.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g001.jpg\"><alt-text content-type=\"machine-generated\">Diagram illustrating a method for autism prediction using a multimodal data synthesis network and adaptive ensemble learning. EEG data, autism severity, and behavioral data are encoded by transformer-based encoders. Real data combined with synthetic data generated by a conditional GAN feeds into a mixture-of-experts system including four experts. Weighted outputs assist in autism prediction. Metrics computed include accuracy, precision, F1, and AUC, with results visualized.</alt-text></graphic></fig><sec id=\"sec16\"><label>3.1</label><title>Dataset description</title><p>The model is trained and validated on three publicly available datasets:</p><list list-type=\"bullet\"><list-item><p><italic toggle=\"yes\">ABIDE (Autism Brain Imaging Data Exchange)</italic>: A multi-site neuroimaging dataset. ABIDE-I/II together include structural MRI (T1-weighted), resting-state functional MRI, and diffusion MRI from hundreds of ASD individuals and controls. Phenotypic assessments (age, IQ, diagnosis) accompany the imaging (<xref rid=\"ref8\" ref-type=\"bibr\">Di Martino et al., 2017</xref>).</p></list-item><list-item><p><italic toggle=\"yes\">NDAR (National Database for Autism Research)</italic>: Aggregates multimodal data, including behavioral assessments and EEG (<xref rid=\"ref27\" ref-type=\"bibr\">Payakachat et al., 2016</xref>).</p></list-item><list-item><p><italic toggle=\"yes\">SSC (SimonsSimplex Collection)</italic>: Includes genetic and behavioral data from families with autistic children (<xref rid=\"ref20\" ref-type=\"bibr\">Levy et al., 2011</xref>).</p></list-item></list><p>First, sourced neuroimaging data from ABIDE I and II, comprising 2,200 subjects (ASD and neurotypical controls) across 17 sites. Second, incorporated 1,100 high-density EEG recordings from the National Database for Autism Research (NDAR), sampled at 250&#8239;Hz alongside standardized behavioral assessments. Third, we included genetic and behavioral data for 2,600 simplex families from the Simons Simplex Collection (SSC), with whole-exome sequencing variants paired with ADOS/ADI-R measures. All data were split into train/validation/test sets in a 70/15/15% ratio, stratified by diagnosis, age, and site to preserve class balance. Experiments were repeated with three distinct random seeds (42, 123, 2025), and results are reported as the mean &#177; SD. It is important to note that evaluation was performed on stratified splits within ABIDE, NDAR, and SSC. No completely external dataset was available for validation. Hence, generalizability beyond these datasets remains to be established. The dataset details are mentioned in <xref rid=\"app1\" ref-type=\"app\">Appendix A</xref>.</p></sec><sec id=\"sec17\"><label>3.2</label><title>Data preprocessing</title><p>Raw magnetic resonance images underwent skull-stripping, affine registration to MNI space, and voxel-wise intensity normalization to zero mean and unit variance. EEG signals were band-pass filtered between 1 and 40&#8239;Hz, notched at 50&#8239;Hz, and epochs exceeding &#177;100&#8239;&#956;V were rejected; remaining segments were z-score normalized on an epoch-wise basis. Continuous features across modalities were imputed to their mean values, while categorical features employed one-hot encoding augmented by an explicit &#8220;unknown&#8221; flag. All continuous features (e.g., voxel intensities, age, and genomic variant counts) are normalized to have a mean of zero and a variance of one to stabilize training. For a feature <inline-formula>\n<mml:math id=\"M1\" overflow=\"scroll\"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>\n</inline-formula>, we compute as in <xref rid=\"EQ1\" ref-type=\"disp-formula\">Equation 1</xref>:</p><disp-formula id=\"EQ1\"><mml:math id=\"M2\" overflow=\"scroll\"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi></mml:mrow><mml:mi>&#963;</mml:mi></mml:mfrac></mml:math><label>(1)</label></disp-formula><p>where <inline-formula>\n<mml:math id=\"M3\" overflow=\"scroll\"><mml:mi>&#956;</mml:mi></mml:math>\n</inline-formula> and <inline-formula>\n<mml:math id=\"M4\" overflow=\"scroll\"><mml:mi>&#963;</mml:mi></mml:math>\n</inline-formula> are the training set&#8217;s mean and standard deviation, respectively. This z-score normalization ensures each feature is on a comparable scale.</p><p>Categorical variables (e.g., gender, site, diagnostic codes) are transformed into one-hot encoded vectors. For a categorical feature with <inline-formula>\n<mml:math id=\"M5\" overflow=\"scroll\"><mml:mi>K</mml:mi></mml:math>\n</inline-formula> classes, a sample <inline-formula>\n<mml:math id=\"M6\" overflow=\"scroll\"><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy=\"true\">{</mml:mo><mml:mn>1..</mml:mn><mml:mi>K</mml:mi><mml:mo stretchy=\"true\">}</mml:mo><mml:mo>,</mml:mo></mml:math>\n</inline-formula>is mapped to a binary vector <inline-formula>\n<mml:math id=\"M7\" overflow=\"scroll\"><mml:mi>h</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy=\"true\">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"true\">}</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:math>\n</inline-formula>such that <inline-formula>\n<mml:math id=\"M8\" overflow=\"scroll\"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math>\n</inline-formula> if and only if <inline-formula>\n<mml:math id=\"M9\" overflow=\"scroll\"><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:math>\n</inline-formula>. Missing values&#8212;common in multi-site clinical datasets&#8212;are imputed using simple statistical approaches. For numerical features, missing entries are replaced with the mean value <inline-formula>\n<mml:math id=\"M10\" overflow=\"scroll\"><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mspace width=\"0.25em\"/></mml:math>\n</inline-formula>computed from the observed data as represented in <xref rid=\"EQ2\" ref-type=\"disp-formula\">Equation 2</xref>:</p><disp-formula id=\"EQ2\"><mml:math id=\"M11\" overflow=\"scroll\"><mml:mover accent=\"true\"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"true\">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy=\"true\">{</mml:mo><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">if</mml:mtext><mml:mspace width=\"0.25em\"/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width=\"0.25em\"/><mml:mtext mathvariant=\"italic\">is observed</mml:mtext><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mtext mathvariant=\"italic\">if</mml:mtext><mml:mspace width=\"0.25em\"/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width=\"0.25em\"/><mml:mtext mathvariant=\"italic\">is missing</mml:mtext><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(2)</label></disp-formula><p>For categorical variables, an additional &#8220;unknown&#8221; category is added to handle missing values. More advanced methods (e.g., k-NN imputation or model-based approaches) are available but are not used here for simplicity and consistency. All preprocessing parameters (<inline-formula>\n<mml:math id=\"M12\" overflow=\"scroll\"><mml:mi>&#956;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>,</mml:mo></mml:math>\n</inline-formula> and encoding schemes) are learned from the training data and consistently applied to the validation, test, and synthetic datasets. Not all subjects had complete multimodal data. Missing features were imputed using mean (continuous) or &#8216;unknown&#8217; category (categorical) values. While pragmatic, this may bias results and motivate the use of advanced missing-modality learning in the future. Behavioral narrative text fields from NDAR/SSC were anonymized, tokenized, and embedded using a pre-trained biomedical language model (BioBERT). The resulting 768-dimensional embeddings were reduced to 128 dimensions using PCA and used as input to MADSN. Synthetic text vectors (&#8220;text_projected&#8221;) generated by MADSN thus represent latent embeddings of behavioral descriptions rather than raw text.</p></sec><sec id=\"sec18\"><label>3.3</label><title>MADSN architecture</title><p>Our Multimodal Autism Data Synthesis Network (MADSN) generates coherent synthetic triplets (<inline-formula>\n<mml:math id=\"M13\" overflow=\"scroll\"><mml:msub><mml:mover accent=\"true\"><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">&#732;</mml:mo></mml:mover><mml:mi mathvariant=\"italic\">MRI</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent=\"true\"><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">&#732;</mml:mo></mml:mover><mml:mi mathvariant=\"italic\">EEG</mml:mi></mml:msub></mml:math>\n</inline-formula>,<inline-formula>\n<mml:math id=\"M14\" overflow=\"scroll\"><mml:msub><mml:mover accent=\"true\"><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">&#732;</mml:mo></mml:mover><mml:mi mathvariant=\"italic\">SNP</mml:mi></mml:msub></mml:math>\n</inline-formula>) by fusing transformer-based embeddings and enforcing cross-modal consistency. Each modality is first encoded via a six-layer transformer (eight heads, hidden size 512), using positional encodings for EEG and learned embeddings for genetic variants and imaging patches. These modality-specific outputs interact with one another through cross-modal attention, producing fused embeddings that are concatenated and projected into a 256-dimensional latent input for the generator. The generator <inline-formula>\n<mml:math id=\"M15\" overflow=\"scroll\"><mml:mi>G</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math>\n</inline-formula> is implemented as a four-layer MLP with LeakyReLU activations, while the discriminator <inline-formula>\n<mml:math id=\"M16\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math>\n</inline-formula> features a shared three-layer MLP trunk branching into modality-specific heads.</p><p>Training follows a conditional GAN paradigm augmented with three loss components: standard adversarial loss <inline-formula>\n<mml:math id=\"M17\" overflow=\"scroll\"><mml:mi>E</mml:mi><mml:mo stretchy=\"true\">[</mml:mo><mml:mo>log</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy=\"true\">[</mml:mo><mml:mo>log</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo></mml:math>\n</inline-formula>, a cross-modal KL-divergence penalty to encourage consistency of joint posteriors, and a privacy penalty implemented via DP-SGD on the discriminator. We set a clipping norm <inline-formula>\n<mml:math id=\"M18\" overflow=\"scroll\"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math>\n</inline-formula> and a noise multiplier <inline-formula>\n<mml:math id=\"M19\" overflow=\"scroll\"><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:mn>1.2</mml:mn></mml:math>\n</inline-formula> to achieve <inline-formula>\n<mml:math id=\"M20\" overflow=\"scroll\"><mml:mi>&#949;</mml:mi><mml:mo>&#8804;</mml:mo><mml:mn>1.0</mml:mn></mml:math>\n</inline-formula> at <inline-formula>\n<mml:math id=\"M21\" overflow=\"scroll\"><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>\n</inline-formula>, ensuring rigorous differential privacy guarantees without sacrificing data utility. <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref> illustrates the architecture of the proposed Multimodal Autism Data Synthesis Network (MADSN). Each input modality <inline-formula>\n<mml:math id=\"M22\" overflow=\"scroll\"><mml:msup><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math>\n</inline-formula>(e.g., EEG, behavioral text, demographic vectors) is first processed through a modality-specific transformer encoder <inline-formula>\n<mml:math id=\"M23\" overflow=\"scroll\"><mml:msub><mml:mi>T</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math>\n</inline-formula> to produce a latent representation <inline-formula>\n<mml:math id=\"M24\" overflow=\"scroll\"><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math>\n</inline-formula> (<xref rid=\"EQ3\" ref-type=\"disp-formula\">Equation 3</xref>):</p><disp-formula id=\"EQ3\"><mml:math id=\"M25\" overflow=\"scroll\"><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy=\"true\">)</mml:mo></mml:math><label>(3)</label></disp-formula><fig position=\"float\" id=\"fig2\" orientation=\"portrait\"><label>Figure 2</label><caption><p>MADSN Architecture.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g002.jpg\"><alt-text content-type=\"machine-generated\">Diagram of the MADSN Architecture illustrating the flow of data through various attention layers. It starts with a Transformer Encoder using self-attention for EEG, behavioral text, and demographic vectors. Cross-modal and shared attention leads to a unified attention layer. Inputs are fed into a generator, producing synthetic samples. These samples are evaluated by a discriminator to determine the probability of being real, using x_gen and x_synth. The architecture combines synthetic and shared attention for concentrated modality embeddings.</alt-text></graphic></fig><p>Each transformer encoder includes self-attention layers, particularly multi-head attention computed as in <xref rid=\"EQ4\" ref-type=\"disp-formula\">Equation 4</xref>:</p><disp-formula id=\"EQ4\"><mml:math id=\"M26\" overflow=\"scroll\"><mml:mtext mathvariant=\"italic\">Attention</mml:mtext><mml:mspace width=\"0.25em\"/><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant=\"italic\">softmax</mml:mtext><mml:mo stretchy=\"true\">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac><mml:mo stretchy=\"true\">)</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:math><label>(4)</label></disp-formula><p>where <inline-formula>\n<mml:math id=\"M27\" overflow=\"scroll\"><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mspace width=\"0.25em\"/></mml:math>\n</inline-formula>are query, key, and value projections of <inline-formula>\n<mml:math id=\"M28\" overflow=\"scroll\"><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math>\n</inline-formula>, and is the dimensionality of the key vectors. Positional encodings are added as necessary to maintain spatial or temporal relationships. Latent features <inline-formula>\n<mml:math id=\"M29\" overflow=\"scroll\"><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mspace width=\"0.25em\"/></mml:math>\n</inline-formula>from all modalities are then fused via cross-modal attention.</p><p>For modalities <inline-formula>\n<mml:math id=\"M30\" overflow=\"scroll\"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:math>\n</inline-formula>, attention weights are computed as in <xref rid=\"EQ5\" ref-type=\"disp-formula\">Equation 5</xref>:</p><disp-formula id=\"EQ5\"><mml:math id=\"M31\" overflow=\"scroll\"><mml:msub><mml:mi>a</mml:mi><mml:mi mathvariant=\"italic\">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant=\"italic\">softmax</mml:mtext><mml:mo stretchy=\"true\">(</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"true\">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy=\"true\">)</mml:mo></mml:math><label>(5)</label></disp-formula><p>All modality embeddings are concatenated and processed through shared attention layers to yield a unified latent vector <inline-formula>\n<mml:math id=\"M32\" overflow=\"scroll\"><mml:mi>z</mml:mi></mml:math>\n</inline-formula>, encoding multimodal context. The generator <inline-formula>\n<mml:math id=\"M33\" overflow=\"scroll\"><mml:mi>G</mml:mi></mml:math>\n</inline-formula> of the conditional GAN receives <inline-formula>\n<mml:math id=\"M34\" overflow=\"scroll\"><mml:mi>z</mml:mi></mml:math>\n</inline-formula>, random noise <inline-formula>\n<mml:math id=\"M35\" overflow=\"scroll\"><mml:mi>&#951;</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math>\n</inline-formula>, and class label <inline-formula>\n<mml:math id=\"M36\" overflow=\"scroll\"><mml:mi>c</mml:mi></mml:math>\n</inline-formula>, and produces synthetic multimodal samples (<xref rid=\"EQ6\" ref-type=\"disp-formula\">Equation 6</xref>):</p><disp-formula id=\"EQ6\"><mml:math id=\"M37\" overflow=\"scroll\"><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant=\"italic\">gen</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>&#951;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math><label>(6)</label></disp-formula><p>which outputs synthetic samples for each modality (stacked or separately). The discriminator <inline-formula>\n<mml:math id=\"M38\" overflow=\"scroll\"><mml:mi>D</mml:mi></mml:math>\n</inline-formula> evaluates real or generated data conditioned on <inline-formula>\n<mml:math id=\"M39\" overflow=\"scroll\"><mml:mi>c</mml:mi></mml:math>\n</inline-formula> and outputs a probability of being real. The GAN training minimizes the following adversarial objective (<xref rid=\"EQ7\" ref-type=\"disp-formula\">Equation 7</xref>):</p><disp-formula id=\"EQ7\"><mml:math id=\"M40\" overflow=\"scroll\"><mml:mtable columnalign=\"left\" displaystyle=\"true\"><mml:mtr><mml:mtd><mml:munder><mml:mo>min</mml:mo><mml:mi mathvariant=\"normal\">G</mml:mi></mml:munder><mml:munder><mml:mo>max</mml:mo><mml:mi>D</mml:mi></mml:munder><mml:mi>V</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext mathvariant=\"italic\">data</mml:mtext></mml:msub></mml:mrow></mml:msub><mml:mo stretchy=\"true\">[</mml:mo><mml:mo>log</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#8239;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>&#951;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"true\">[</mml:mo><mml:mo>log</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>&#951;</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(7)</label></disp-formula><p>where <inline-formula>\n<mml:math id=\"M41\" overflow=\"scroll\"><mml:mi>&#951;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mn>..</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math>\n</inline-formula> is fixed per real sample for training purposes. Training alternates between minimizing the discriminator loss as shown in <xref rid=\"EQ8\" ref-type=\"disp-formula\">Equation 8</xref>:</p><disp-formula id=\"EQ8\"><mml:math id=\"M42\" overflow=\"scroll\"><mml:msub><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo stretchy=\"true\">[</mml:mo><mml:mo>log</mml:mo><mml:mi>D</mml:mi><mml:mspace width=\"0.25em\"/><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant=\"italic\">real</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant=\"italic\">gen</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo></mml:math><label>(8)</label></disp-formula><p>and minimizing the generator loss with a cross-modal consistency penalty (<xref rid=\"EQ9\" ref-type=\"disp-formula\">Equation 9</xref>):</p><disp-formula id=\"EQ9\"><mml:math id=\"M43\" overflow=\"scroll\"><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mo>log</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant=\"italic\">gen</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mtext mathvariant=\"italic\">cons</mml:mtext></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant=\"italic\">cons</mml:mtext></mml:msub></mml:math><label>(9)</label></disp-formula><p>Cross-modal consistency is enforced by ensuring that different modality embeddings agree in latent space as in <xref rid=\"EQ10\" ref-type=\"disp-formula\">Equation 10</xref>:</p><disp-formula id=\"EQ10\"><mml:math id=\"M44\" overflow=\"scroll\"><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant=\"italic\">cons</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo stretchy=\"true\">&#8214;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy=\"true\">&#8214;</mml:mo><mml:mn>2</mml:mn></mml:math><label>(10)</label></disp-formula><p>Finally, for privacy, we incorporate Differential Privacy (DP) into GAN training. Differential Privacy (DP) is incorporated into discriminator training using DP-SGD. A mechanism <inline-formula>\n<mml:math id=\"M45\" overflow=\"scroll\"><mml:mi>M</mml:mi><mml:mspace width=\"0.25em\"/></mml:math>\n</inline-formula>is <inline-formula>\n<mml:math id=\"M46\" overflow=\"scroll\"><mml:mi>&#1013;</mml:mi></mml:math>\n</inline-formula>-differentially private if changing one individual in the dataset changes output probabilities by at most <inline-formula>\n<mml:math id=\"M47\" overflow=\"scroll\"><mml:msup><mml:mi>e</mml:mi><mml:mo>&#8712;</mml:mo></mml:msup></mml:math>\n</inline-formula> (<xref rid=\"EQ11\" ref-type=\"disp-formula\">Equation 11</xref>):</p><disp-formula id=\"EQ11\"><mml:math id=\"M48\" overflow=\"scroll\"><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy=\"true\">[</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>&#8804;</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mo>&#8712;</mml:mo></mml:msup><mml:msub><mml:mi>P</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy=\"true\">[</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>D</mml:mi><mml:mo>&#8242;</mml:mo><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>&#8704;</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8704;</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>&#8242;</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mo stretchy=\"true\">&#8214;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#8242;</mml:mo><mml:mo stretchy=\"true\">&#8214;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><label>(11)</label></disp-formula><p>Concretely, the discriminator gradients are clipped to norm <inline-formula>\n<mml:math id=\"M49\" overflow=\"scroll\"><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:math>\n</inline-formula> and Gaussian noise is added for a mini-batch of size <inline-formula>\n<mml:math id=\"M50\" overflow=\"scroll\"><mml:mi>B</mml:mi></mml:math>\n</inline-formula> as mentioned in <xref rid=\"EQ12\" ref-type=\"disp-formula\">Equation 12</xref>.</p><disp-formula id=\"EQ12\"><mml:math id=\"M51\" overflow=\"scroll\"><mml:mover accent=\"true\"><mml:mi>g</mml:mi><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:munderover><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>max</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy=\"true\">&#8214;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"true\">&#8214;</mml:mo></mml:mrow><mml:mi>C</mml:mi></mml:mfrac><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>&#951;</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&#963;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>I</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo>,</mml:mo></mml:math><label>(12)</label></disp-formula><p>where <inline-formula>\n<mml:math id=\"M52\" overflow=\"scroll\"><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>\n</inline-formula> is the gradient from sample <inline-formula>\n<mml:math id=\"M53\" overflow=\"scroll\"><mml:mi>i</mml:mi></mml:math>\n</inline-formula>. The MADSN generator is trained to minimize (<xref rid=\"EQ13\" ref-type=\"disp-formula\">Equation 13</xref>):</p><disp-formula id=\"EQ13\"><mml:math id=\"M54\" overflow=\"scroll\"><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mtext mathvariant=\"italic\">cons</mml:mtext></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant=\"italic\">cons</mml:mtext></mml:msub></mml:math><label>(13)</label></disp-formula><p>while discriminator training is made private. By combining transformers, cross-modal attention, GAN objectives, and DP constraints, MADSN learns to produce realistic, privacy-preserving synthetic multimodal autism data.</p></sec><sec id=\"sec19\"><label>3.4</label><title>AMEL ensemble learning</title><p>The Adaptive Multimodal Ensemble Learning (AMEL) system takes the augmented dataset (real + synthetic) and trains an ensemble of <inline-formula>\n<mml:math id=\"M55\" overflow=\"scroll\"><mml:mi>K</mml:mi></mml:math>\n</inline-formula> expert classifiers, along with a gating network. The Adaptive Multimodal Ensemble Learning (AMEL) module integrates five experts&#8212;CNN, MLP, regressor, transformer, and GNN&#8212;via a gating network. Each expert processes modality-specific inputs; the gating network assigns adaptive weights to expert outputs, enabling sample-specific fusion. This ensures that if one modality is weak or missing, other experts dominate the prediction. Each expert produces logits, which are concatenated and passed through a two-layer gating MLP (hidden size 128, ReLU) to yield softmax weights <inline-formula>\n<mml:math id=\"M56\" overflow=\"scroll\"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>\n</inline-formula>, regularized by an entropy penalty (<italic toggle=\"yes\">&#955;</italic>&#8239;=&#8239;0.01) to prevent collapse. The ensemble prediction <inline-formula>\n<mml:math id=\"M57\" overflow=\"scroll\"><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mspace width=\"0.25em\"/></mml:math>\n</inline-formula>is trained end-to-end under a cross-entropy loss on held-out labels. <xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref> represents the schematic of the AMEL adaptive ensemble. Each expert <inline-formula>\n<mml:math id=\"M58\" overflow=\"scroll\"><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math>\n</inline-formula> may be specialized to one modality (e.g., <inline-formula>\n<mml:math id=\"M59\" overflow=\"scroll\"><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"italic\">MRI</mml:mi></mml:msub></mml:math>\n</inline-formula>for imaging, <inline-formula>\n<mml:math id=\"M60\" overflow=\"scroll\"><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"italic\">GEN</mml:mi></mml:msub></mml:math>\n</inline-formula>for genetics, and so on), or to different architectures (CNN, MLP, etc.). Given an input <inline-formula>\n<mml:math id=\"M61\" overflow=\"scroll\"><mml:mi>x</mml:mi></mml:math>\n</inline-formula> with all modalities, each expert outputs a prediction <inline-formula>\n<mml:math id=\"M62\" overflow=\"scroll\"><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math>\n</inline-formula>. A gating network <inline-formula>\n<mml:math id=\"M63\" overflow=\"scroll\"><mml:mi>g</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:math>\n</inline-formula> produces scores that are normalized via softmax to obtain weights as mentioned in <xref rid=\"EQ14\" ref-type=\"disp-formula\">Equation 14</xref>:</p><disp-formula id=\"EQ14\"><mml:math id=\"M64\" overflow=\"scroll\"><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>exp</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mo>exp</mml:mo><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><label>(14)</label></disp-formula><fig position=\"float\" id=\"fig3\" orientation=\"portrait\"><label>Figure 3</label><caption><p>AMEL adaptive ensemble architectural workflow.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g003.jpg\"><alt-text content-type=\"machine-generated\">Diagram illustrating an ensemble learning model called AMEL. It shows an augmented dataset input to a gating network and multiple experts labeled from one to k. Each expert outputs a function y equals f of x. The outputs are combined using a summation and softmax operation, resulting in an output y tilde of x.</alt-text></graphic></fig><p>These weights adapt to each sample: e.g., if imaging data is missing or noisy, the model may down-weight the imaging expert. The ensemble prediction is the weighted sum (<xref rid=\"EQ15\" ref-type=\"disp-formula\">Equation 15</xref>):</p><disp-formula id=\"EQ15\"><mml:math id=\"M65\" overflow=\"scroll\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow><mml:mo stretchy=\"true\">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>&#945;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><label>(15)</label></disp-formula><p>The entire system is trained end-to-end by minimizing an ensemble loss: a supervised loss and regularization. Formally,</p><disp-formula id=\"EQ16\"><mml:math id=\"M66\" overflow=\"scroll\"><mml:msub><mml:mi>L</mml:mi><mml:mi mathvariant=\"italic\">cns</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy=\"true\">[</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy=\"true\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow><mml:mo stretchy=\"true\">^</mml:mo></mml:mover><mml:mo stretchy=\"true\">)</mml:mo><mml:mo stretchy=\"true\">]</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits=\"false\">&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy=\"true\">&#8214;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy=\"true\">&#8214;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><label>(16)</label></disp-formula><p>where <inline-formula>\n<mml:math id=\"M67\" overflow=\"scroll\"><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math>\n</inline-formula> are parameters of <inline-formula>\n<mml:math id=\"M68\" overflow=\"scroll\"><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math>\n</inline-formula> and <inline-formula>\n<mml:math id=\"M69\" overflow=\"scroll\"><mml:msub><mml:mi>&#955;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math>\n</inline-formula> can encode modality-specific priors (<xref rid=\"EQ16\" ref-type=\"disp-formula\">Equation 16</xref>). We backpropagate through the gating softmax so that better-performing experts get higher weights. This &#8220;mixture-of-experts&#8221; approach allows the ensemble to <italic toggle=\"yes\">adaptively</italic> integrate modalities, as opposed to static averaging or majority voting. Indeed, adaptive ensemble algorithms (with learned weights) typically outperform fixed-weight ensembles. Overfitting was mitigated through dropout layers (<italic toggle=\"yes\">p</italic>&#8239;=&#8239;0.3 in the MADSN generator, <italic toggle=\"yes\">p</italic>&#8239;=&#8239;0.5 in the AMEL gating), entropy regularization (&#955;&#8239;=&#8239;0.01), and early stopping based on validation AUC. Synthetic samples were generated exclusively from training distributions, ensuring no leakage into validation or test sets. During inference, if a modality is missing or corrupted, its expert output is excluded, and the gating network automatically redistributes weights among the remaining experts. This adaptive weighting allows AMEL to degrade gracefully rather than fail catastrophically in incomplete-modality settings. The outline for the MADSN and AMEL components, as well as their integration, is detailed in <xref rid=\"fig4\" ref-type=\"fig\">Algorithms 1</xref>, <xref rid=\"fig5\" ref-type=\"fig\">2</xref>.</p><fig position=\"float\" id=\"fig4\" orientation=\"portrait\"><label>ALGORITHM 1</label><caption><p>MADSN multimodal synthesis.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g004.jpg\"><alt-text content-type=\"machine-generated\">Steps for training a generator to produce synthetic data using transformer encoders, a generator, and a discriminator. It involves preprocessing data, updating a discriminator with differential privacy, sampling noise, computing loss, and updating parameters through gradient descent. After training, synthetic data can be generated.</alt-text></graphic></fig><fig position=\"float\" id=\"fig5\" orientation=\"portrait\"><label>ALGORITHM 2</label><caption><p>AMEL training and inference.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g005.jpg\"><alt-text content-type=\"machine-generated\">Text detailing a machine learning algorithm. Steps include initializing experts, computing expert outputs, gating scores, ensemble outputs, and loss, with backpropagation to update models. Inference uses new data to compute and weight expert outputs for final prediction.</alt-text></graphic></fig></sec><sec id=\"sec20\"><label>3.5</label><title>Hyperparameter optimization and baselines</title><p>Model hyperparameters were optimized using a Tree-structured Parzen Estimator (TPE) over learning rates for the GAN (10<sup>&#8722;</sup>5&#8211;10<sup>&#8722;3</sup>1), DP-SGD clipping norm (0.1&#8211;2.0), noise multiplier (0.5&#8211;2.0), number of experts <inline-formula>\n<mml:math id=\"M70\" overflow=\"scroll\"><mml:mi>K</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy=\"true\">{</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy=\"true\">}</mml:mo></mml:math>\n</inline-formula>, and gating penalty <inline-formula>\n<mml:math id=\"M71\" overflow=\"scroll\"><mml:mi>&#955;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy=\"true\">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy=\"true\">]</mml:mo></mml:math>\n</inline-formula>. Validation AUC guided early stopping up to 200 epochs, with performance recorded every epoch. We benchmarked our model against several baselines: a single-modality CNN (MRI only), a GAN without the consistency penalty, a GAN trained with standard SGD (without DP), and an ensemble without gating. Our full pipeline achieved a validation AUC of 0.89&#8239;&#177;&#8239;0.01, outperforming all baselines by at least 0.04.</p></sec><sec id=\"sec21\"><label>3.6</label><title>Statistical and computational considerations</title><p>The model&#8217;s performance is evaluated using AUC, F1, maximum-mean discrepancy (MMD) on embeddings, and Kolmogorov&#8211;Smirnov statistics on marginal distributions, with 95% confidence intervals estimated from 1,000 bootstrap resamples. Paired Wilcoxon signed-rank tests were used to assess significance (<italic toggle=\"yes\">p</italic>&#8239;&lt;&#8239;0.05) against each baseline. Experiments were run on four NVIDIA A100 GPUs (256&#8239;GB RAM), with GAN training requiring ~48&#8239;h and ensemble fine-tuning requiring ~12&#8239;h. The GAN and ensemble models contain approximately 12&#8239;M and 8 M parameters, respectively. Training required ~48&#8239;h on four A100 GPUs, which may limit reproducibility in smaller labs. Future studies will explore model compression (e.g., distillation, ONNX export) and federated setups to reduce computational cost.</p></sec></sec><sec sec-type=\"results\" id=\"sec22\"><label>4</label><title>Results and discussion</title><p>The proposed research introduces AutismSynthGen, a novel generative model designed to synthesize multimodal autism-related data, including behavioral texts, electroencephalogram (EEG) signals, and demographic profiles, to address the challenge of limited datasets in autism prediction research. AutismSynthGen leverages the Multimodal Autism Data Synthesis Network (MADSN), a generative adversarial network (GAN) integrated with a transformer-based multimodal fusion module, which encodes modality-specific inputs using transformers, fuses them into a shared latent space via attention-based mechanisms, and employs a conditional GAN to generate clinically relevant synthetic samples conditioned on autism severity levels (mild, moderate, severe). A privacy-preserving loss function, incorporating differential privacy (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0), ensures the protection of sensitive patient information, while a cross-modal consistency regularizer maintains coherence across modalities, aligning EEG patterns with behavioral descriptions and demographic data. The accuracy of the synthetic dataset is validated using multiple machine learning algorithms, including Random Forest, Support Vector Machine (SVM), Convolutional Neural Network (CNN), and Logistic Regression, with the proposed Adaptive Multimodal Ensemble Learning (AMEL) algorithm employed for training. AMEL integrates a weighted ensemble of these base learners, utilizing adaptive weighting and modality-specific regularization to optimize prediction performance, thereby enhancing the effectiveness of the synthetic data for autism classification tasks. The novelty of this approach lies in the combination of MADSN&#8217;s generative capabilities with AMEL&#8217;s adaptive ensemble strategy, addressing data scarcity and privacy concerns while outperforming traditional methods.</p><sec id=\"sec23\"><label>4.1</label><title>Dataset description</title><p>The development and evaluation of AutismSynthGen utilize three well-established, publicly accessible datasets, each providing critical multimodal data for autism research:</p><list list-type=\"order\"><list-item><p><bold>ABIDE (Autism Brain Imaging Data Exchange)</bold>: This dataset includes EEG, functional magnetic resonance imaging (fMRI), and demographic data (e.g., age, gender) from individuals with autism spectrum disorder (ASD) and typically developing controls. It is widely used for studying brain connectivity and autism-related biomarkers. Access to ABIDE is publicly available but requires registration through the official ABIDE portal.</p></list-item><list-item><p><bold>NDAR (National Database for Autism Research)</bold>: NDAR provides a comprehensive repository of autism-related data, including behavioral assessments, EEG recordings, and clinical information. It supports integrative analyses across genetic, neuroimaging, and behavioral domains. Access to NDAR requires a data use agreement, which can be obtained through the NDAR platform.</p></list-item><list-item><p><bold>Simons Simplex Collection (SSC)</bold>: This dataset, provided through SFARI Base, contains behavioral data, clinical assessments, and demographic profiles from families with one child diagnosed with autism spectrum disorder (ASD). SSC is particularly valuable for studying familial and behavioral patterns in autism spectrum disorder (ASD). Access is available through an application on the SFARI Base platform.</p></list-item></list><p>These datasets collectively provide a robust foundation for training and validating AutismSynthGen, ensuring that the generated synthetic data accurately reflects the realistic, multimodal characteristics of autism while adhering to ethical and privacy standards. <xref rid=\"fig6\" ref-type=\"fig\">Figure 4</xref> shows a sample of the raw dataset customized from multimodal data, illustrating key features such as autism severity scores (A1-Score to A8-Score), demographic information (e.g., country, age, relationship), and behavioral/EEG indicators (e.g., EEG_signal, behavioral_text). The dataset includes five anonymized patient records, with columns representing various attributes used for training the AutismSynthGen model.</p><fig position=\"float\" id=\"fig6\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Raw multimodal dataset illustrating key features such as autism severity scores (A1-Score to A8-Score), demographic information (e.g., country, age, relationship), and behavioral/EEG indicators.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g006.jpg\"><alt-text content-type=\"machine-generated\">A table displays data including scores and attributes such as jurisdiction, autism status, country of residence, app usage history, result, age description, relation, ASD classification, EEG signals, and behavioral text. The scores, ranging from A1 to A10, show binary values. Entries include countries like the United States, Brazil, Spain, and Egypt, with varying results and descriptions. Behavioral text excerpts discuss patient states and profiles.</alt-text></graphic></fig><p><xref rid=\"fig7\" ref-type=\"fig\">Figure 5</xref> represents the sample of the pre-processed dataset derived from the raw multimodal data, following the application of data pre-processing techniques. The preprocessing steps include handling missing values by appropriate imputation or removal, encoding categorical variables (e.g., country, relationship) into numerical representations, and normalizing numerical features (e.g., age, severity scores) to ensure consistency and compatibility with the AutismSynthGen model. The dataset retains five anonymized patient records, with refined attributes suitable for model training.</p><fig position=\"float\" id=\"fig7\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Preprocessed multimodal dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g007.jpg\"><alt-text content-type=\"machine-generated\">A table displaying data with columns including A1_Score to A10_Score, along with attributes like country_of_res, used_app_before, result, and others. Each row contains individual scores and additional information such as EEG_signal and Behavioral_Text, which provides contextual or descriptive notes related to each entry.</alt-text></graphic></fig><p><xref rid=\"fig8\" ref-type=\"fig\">Figure 6</xref> represents the graph depicting the discriminator accuracy of the MADSN model during training over 14 iterations. The results presented in <xref rid=\"fig8\" ref-type=\"fig\">Figure 6</xref> demonstrate the training performance of the MADSN discriminator, a critical component of the AutismSynthGen model. The observed increase in discriminator accuracy from 0.40 to 0.65 across 14 iterations signifies robust learning and the model&#8217;s capacity to differentiate between synthetic and real multimodal autism data. The initial rise in accuracy, accompanied by minor fluctuations between iterations 4 and 6, suggests an adaptation phase where the generator and discriminator achieve equilibrium, a common phenomenon in GAN training. The stabilization and subsequent steady improvement post-iteration 6 underscore the efficacy of the transformer-based multimodal fusion and cross-modal consistency regularizer in enhancing data realism. The final accuracy of 0.65 indicates a strong discriminative capability, supporting the reliability of the synthetic data generated for augmenting limited autism datasets.</p><fig position=\"float\" id=\"fig8\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Graph depicting the discriminator accuracy of the MADSN model during training over 14 iterations. The accuracy increases progressively from approximately 0.40 to 0.65, indicating effective learning and convergence of the generative adversarial network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g008.jpg\"><alt-text content-type=\"machine-generated\">Line graph depicting discriminator accuracy during training. The x-axis represents iterations from 0 to 15, and the y-axis shows accuracy ranging from 0.40 to 0.65. The accuracy steadily increases with some fluctuations, indicated by a blue line, starting around 0.41 and reaching approximately 0.65.</alt-text></graphic></fig><p><xref rid=\"fig9\" ref-type=\"fig\">Figure 7</xref> represents the sample of the synthetic data generated by AutismSynthGen, stored in synthetic_data.npy format, showcasing projected text features (text_projected), EEG signals (eeg), and demographic labels (demo_labels) for five synthetic patient records. The results presented in <xref rid=\"fig9\" ref-type=\"fig\">Figure 7</xref> illustrate the efficacy of AutismSynthGen in generating synthetic multimodal data, as evidenced by the sample of synthetic_data.npy. The projected text features, EEG signals, and demographic labels exhibit coherent patterns that align with the pre-processed dataset, confirming the success of the transformer-based multimodal fusion and cross-modal consistency regularizer in maintaining inter-modality relationships. The presence of binary labels (0 and 1) in the demo_labels column indicates the model&#8217;s capability to generate data conditioned on autism severity levels, a key objective of the MADSN framework. The observed variability in synthetic data attributes, such as the range of EEG values and text projections, suggests that the conditional GAN effectively captures the diversity of the original dataset while adhering to the privacy constraints imposed by differential privacy (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0). This synthetic data augmentation is poised to enhance the training of autism prediction models, particularly in scenarios where real-world data is limited. The &#8216;text_projected&#8217; column represents generated behavioral text embeddings. These were evaluated for similarity against real embeddings using BLEU scores, confirming alignment at the representation level. These vectors were not decoded into sentences but integrated directly into AMEL for classification.</p><fig position=\"float\" id=\"fig9\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Sample of synthetic multimodal data generated by AutismSynthGen, including text embeddings (&#8216;text_projected&#8217;), EEG signals, and demographic labels.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g009.jpg\"><alt-text content-type=\"machine-generated\">A table with five rows and columns labeled 'text_projected', 'eeg', 'demo', and 'labels'. Each row contains arrays of numbers in the 'text_projected', 'eeg', and 'demo' columns. The 'labels' column contains binary values: 0 or 1.</alt-text></graphic></fig><p><xref rid=\"fig10\" ref-type=\"fig\">Figure 8</xref> represents the comparison of distribution histograms for EEG values and age between real and synthetic data. The results presented in <xref rid=\"fig10\" ref-type=\"fig\">Figure 8</xref> provide a comparative analysis of the distributions of EEG values and age between real and synthetic data, offering insights into the fidelity of AutismSynthGen&#8217;s output. The EEG distribution demonstrates a strong overlap between real and synthetic data, with both exhibiting a central peak around zero and a comparable spread, suggesting that the MADSN model effectively captures the statistical properties of EEG signals. This alignment validates the efficacy of the transformer-based multimodal fusion and cross-modal consistency regularizer in preserving the structural integrity of EEG patterns. Similarly, the age distribution shows a close match between real and synthetic data, with both histograms displaying similar normalized ranges (0 to 20) and peak densities, indicating the model&#8217;s success in replicating demographic attributes while adhering to the differential privacy constraint (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0). Minor deviations in the tails of the distributions may reflect the impact of the privacy-preserving loss, which prioritizes data utility over exact replication. These findings affirm the synthetic data&#8217;s potential to augment limited real datasets, enhancing the robustness of autism prediction models.</p><fig position=\"float\" id=\"fig10\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Comparison of distribution histograms for EEG values and age between real and synthetic data.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g010.jpg\"><alt-text content-type=\"machine-generated\">Two histograms compare real and synthetic data distributions. The left chart shows EEG distributions with real data in blue and synthetic in orange. The right chart displays age distributions, similarly color-coded, indicating a narrow peak for synthetic age data compared to real age data. Both charts measure density on the Y-axis.</alt-text></graphic></fig><p>To further validate fidelity, we projected real and synthetic embeddings into a 2D space using t-SNE (<xref rid=\"fig11\" ref-type=\"fig\">Figure 9</xref>). Both EEG and behavioral embeddings show a strong overlap between real and generated samples, consistent with the low MMD and KS values. A complementary PCA projection of AMEL&#8217;s latent decision space (<xref rid=\"fig12\" ref-type=\"fig\">Figure 10</xref>) shows that synthetic samples align closely with real data clusters, without forming spurious modes. These visualizations provide intuitive confirmation that AutismSynthGen captures the essential structure of multimodal ASD data.</p><fig position=\"float\" id=\"fig11\" orientation=\"portrait\"><label>Figure 9</label><caption><p>t-SNE visualization of latent embeddings (real&#8239;=&#8239;blue, synthetic&#8239;=&#8239;orange).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g011.jpg\"><alt-text content-type=\"machine-generated\">Scatter plot titled \\\"t-SNE of Latent Embeddings: Real vs Synthetic\\\" comparing real and synthetic data points. Real data is marked in blue, while synthetic data is in orange. The axes are labeled t-SNE 1 and t-SNE 2. Data points appear scattered and intermixed, without distinct clustering.</alt-text></graphic></fig><fig position=\"float\" id=\"fig12\" orientation=\"portrait\"><label>Figure 10</label><caption><p>PCA projection of AMEL latent decision space with an illustrative decision boundary (real&#8239;=&#8239;blue, synthetic&#8239;=&#8239;orange).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g012.jpg\"><alt-text content-type=\"machine-generated\">Scatter plot of PCA projection showing blue dots for real data and orange dots for synthetic data. A diagonal blue line represents the decision boundary. The axes are labeled PC1 and PC2.</alt-text></graphic></fig><p><xref rid=\"fig13\" ref-type=\"fig\">Figure 11</xref> illustrates the Receiver Operating Characteristic (ROC) curves for the proposed AMEL algorithm, comparing its performance on real data (blue) and a combination of real and synthetic data (orange). The results presented in <xref rid=\"fig13\" ref-type=\"fig\">Figure 11</xref> highlight the superior performance of the AMEL algorithm when trained on a combination of real and synthetic data generated by AutismSynthGen.</p><fig position=\"float\" id=\"fig13\" orientation=\"portrait\"><label>Figure 11</label><caption><p>ROC curves comparing AMEL on real data vs. real + synthetic data. Synthetic augmentation enhances near-ceiling performance, with an AUC of 1.0.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g013.jpg\"><alt-text content-type=\"machine-generated\">ROC curve comparing Real Data with Real plus Synthetic data, displaying True Positive Rate against False Positive Rate. Blue line represents Real Data with AUC of 0.9800 and F1 score of 0.9900, while orange line represents Real plus Synthetic data with AUC and F1 score of 1.0000. Dashed diagonal line indicates no-skill classifier.</alt-text></graphic></fig><p>The ROC curve for real data alone exhibits an AUC of 0.98 and an F1-score of 0.99. In contrast, the inclusion of synthetic data elevated the performance to near-perfect levels (AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00), indicating highly consistent internal discrimination. This improvement underscores the efficacy of the synthetic data in augmenting the real dataset, likely due to AMEL&#8217;s adaptive weighting and regularization, which effectively integrate multimodal features (text, EEG, demographics) enhanced by the MADSN&#8217;s generative process. The ideal performance on the augmented dataset may reflect an optimal training scenario, potentially influenced by the synthetic data&#8217;s alignment with real-world distributions (as shown in <xref rid=\"fig7\" ref-type=\"fig\">Figure 5</xref>).</p><p><xref rid=\"fig14\" ref-type=\"fig\">Figure 12</xref> illustrates the confusion matrices for the AMEL algorithm, comparing its performance on real data (left) and a combination of real and synthetic data (right). The results presented in <xref rid=\"fig14\" ref-type=\"fig\">Figure 12</xref> provide a detailed assessment of the AMEL algorithm&#8217;s performance through confusion matrices for real data and a combination of real plus synthetic data. For real data, the matrix reveals 450 true negatives, 50 false positives, 50 false negatives, and 154 true positives, yielding an overall accuracy of approximately 0.904 (calculated as (450&#8239;+&#8239;154) / (450&#8239;+&#8239;50&#8239;+&#8239;50&#8239;+&#8239;154)). In contrast, the inclusion of synthetic data improves the matrix to 480 true negatives, 20 false positives, 30 false negatives, and 174 true positives, resulting in an accuracy of approximately 0.946 (calculated as (480&#8239;+&#8239;174) / (480&#8239;+&#8239;20&#8239;+&#8239;30&#8239;+&#8239;174)). This enhancement, particularly the reduction in false positives and false negatives, underscores the synthetic data&#8217;s contribution to improving classification precision and recall, aligning with the perfect AUC and F1-score observed in <xref rid=\"fig13\" ref-type=\"fig\">Figure 11</xref>.</p><fig position=\"float\" id=\"fig14\" orientation=\"portrait\"><label>Figure 12</label><caption><p>Confusion matrices for the proposed AMEL algorithm, comparing performance on real data (left) and a combination of real plus synthetic data (right).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g014.jpg\"><alt-text content-type=\"machine-generated\">Two confusion matrices compare prediction results. Left matrix: \\\"Real Data\\\" - 450 true negatives, 50 false positives, 50 false negatives, 154 true positives. Right matrix: \\\"Real + Synthetic\\\" - 480 true negatives, 20 false positives, 30 false negatives, 174 true positives.</alt-text></graphic></fig><p>The performance of the AMEL algorithm is evaluated using the following metrics:</p><list list-type=\"bullet\"><list-item><p><bold>MMD (Fused)</bold>: 0.04, indicating a low Maximum Mean Discrepancy between real and synthetic fused multimodal data, suggesting high similarity.</p></list-item><list-item><p><bold>KS Statistic (EEG)</bold>: 0.03, with a <bold>KS <italic toggle=\"yes\">p</italic>-value (EEG)</bold> of 0.06, indicating that the Kolmogorov&#8211;Smirnov test does not reject the null hypothesis of identical EEG distributions at a 5% significance level.</p></list-item><list-item><p><bold>Distributional Similarity (%)</bold>: 95, reflecting a high degree of alignment between real and synthetic data distributions.</p></list-item><list-item><p><bold>F1-Score (Real)</bold>: 0.99, and <bold>AUC (Real)</bold>: 0.98, demonstrating excellent classification performance on real data alone.</p></list-item><list-item><p><bold>F1-Score (Real + Synthetic)</bold>: 1.00, and <bold>AUC (Real + Synthetic)</bold>: 1.00, indicating perfect classification performance with the augmented dataset.</p></list-item><list-item><p><bold>F1 Improvement (%)</bold>: 1.0101, and <bold>AUC Improvement (%)</bold>: 2.0408, quantifying the relative enhancement in performance with synthetic data.</p></list-item><list-item><p><bold>BLEU Score</bold>: 0.7, signifying moderate to high similarity between real and synthetic text features.</p></list-item><list-item><p><bold>Privacy Budget (<italic toggle=\"yes\">&#949;</italic>)</bold>: &#8804; 1.0, indicating no privacy budget expenditure, as the synthetic data generation adheres to differential privacy constraints.</p></list-item></list><p>The evaluation metrics presented in <xref rid=\"fig15\" ref-type=\"fig\">Figure 13</xref> affirm the efficacy of the AMEL algorithm in leveraging synthetic data generated by AutismSynthGen. The low MMD (0.04) and KS statistic (0.03) with a non-significant p-value (0.06) for EEG distributions, alongside a 95% distributional similarity, validate the model&#8217;s ability to replicate real data characteristics, consistent with the observations in <xref rid=\"fig10\" ref-type=\"fig\">Figure 8</xref>. The F1-score improvement of 1.0101% and AUC improvement of 2.0408% when incorporating synthetic data, culminating in perfect scores (F1-score: 1.00, AUC: 1.00), corroborate the enhanced classification performance depicted in <xref rid=\"fig13\" ref-type=\"fig\">Figures 11</xref>, <xref rid=\"fig14\" ref-type=\"fig\">12</xref>. The BLEU score of 0.7 further supports the quality of synthetic text features, while the zero privacy budget (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0) confirms compliance with differential privacy, ensuring patient data protection.</p><fig position=\"float\" id=\"fig15\" orientation=\"portrait\"><label>Figure 13</label><caption><p>Evaluation metrics for the AMEL algorithm, including distributional similarity (MMD&#8239;=&#8239;0.04, KS&#8239;=&#8239;0.03, BLEU&#8239;=&#8239;0.7), classification performance (F1&#8239;=&#8239;0.99 real; 1.00 real + synthetic; AUC&#8239;=&#8239;0.98 real; 1.00 real + synthetic), and privacy compliance (<italic toggle=\"yes\">&#949;</italic>&#8239;&#8804;&#8239;1.0).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g015.jpg\"><alt-text content-type=\"machine-generated\">Bar chart titled \\\"Evaluation Metrics\\\" displaying various metrics along the y-axis and values along the x-axis. Notable metrics include Privacy Budget, BLEU Score, AUC Improvement, F1 Improvement, and others. The Distributional Similarity metric shows a value of 95.0040, standing out significantly compared to others.</alt-text></graphic></fig><p>While quantitative measures (MMD, KS, and BLEU) support fidelity, no clinician-based validation was conducted on synthetic behavioral text or EEG. Future research will involve blinded expert review to confirm clinical realism.</p></sec><sec id=\"sec24\"><label>4.2</label><title>Performance comparison on real data</title><p>The comparison results presented in <xref rid=\"tab2\" ref-type=\"table\">Table 2</xref> illustrate the performance of the proposed AMEL algorithm in comparison to baseline models on real data alone. The AMEL algorithm achieves an accuracy of 0.992908, an F1-score of 0.986301, a precision of 0.972973, a recall of 1.0, an AUC of 1.0, and a log loss of 0.049632, matching CNN&#8217;s performance and surpassing logistic regression (1.0, 1.0, 1.0, 1.0, 1.0, 0.0308908), Random Forest (0.978723, 0.957746, 0.971429, 0.944444, 0.998148, 0.145483), and SVM (0.985816, 0.971429, 1.0, 0.944444, 0.997354, 0.0684). The bar chart visually highlights AMEL&#8217;s competitive edge, particularly in log loss and F1 score, reflecting its effective integration of multimodal features through adaptive weighting and regularization (refer to <xref rid=\"fig16\" ref-type=\"fig\">Figure 14</xref>). While logistic regression exhibits perfect scores, its higher log loss suggests less confidence in predictions compared to AMEL and CNN. These results establish AMEL as a robust baseline for real data, setting the stage for its enhanced performance with synthetic data augmentation, as evidenced by the perfect scores in <xref rid=\"fig13\" ref-type=\"fig\">Figures 11</xref>, <xref rid=\"fig14\" ref-type=\"fig\">12</xref>. The proposed baseline comparison focused on conventional models (CNN, SVM, RF, LR). Recent multimodal attention-based fusion architectures (refs) were excluded due to computational constraints; however, benchmarking against these remains a priority.</p><table-wrap position=\"float\" id=\"tab2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Comparison results of the proposed AMRL algorithm with other existing algorithms on real data.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">F1-Score</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Precision</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Recall</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">AUC</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Log loss</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Logistic Regression</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0308908</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Random Forest</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.978723</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">0.96 &#177; 0.02</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.971429</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.944444</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">0.998 &#177; 0.001</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.145483</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">SVM</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.985816</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">0.97 &#177; 0.01</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.944444</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">0.997 &#177; 0.002</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0684</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CNN</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.992908</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">0.98 &#177; 0.01</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.972973</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.011869</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Proposed AMEL</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>0.992908</bold>\n</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">\n<bold>0.99 &#177; 0.01</bold>\n</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>0.972973</bold>\n</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>1.0</bold>\n</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">\n<bold>1.0 &#177; 0.00</bold>\n</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">\n<bold>0.049632</bold>\n</td></tr></tbody></table><table-wrap-foot><p>Values are reported as mean &#177; SD over three independent runs with random seeds (42, 123, 2025). Bootstrap 95% confidence intervals were computed for AUC and F1 to confirm stability. Bold values represent the results of proposed methodology.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"fig16\" orientation=\"portrait\"><label>Figure 14</label><caption><p>Comparison results of the proposed AMRL algorithm with other algorithms.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g016.jpg\"><alt-text content-type=\"machine-generated\">Bar chart comparing model performance metrics including accuracy, F1-score, precision, recall, AUC, and log loss across five models: Logistic Regression, Random Forest, SVM, CNN, and Proposed AMEL. All models show high metrics, with variations in log loss; Logistic Regression has the lowest at 0.031, and Random Forest the highest at 0.145.</alt-text></graphic></fig><p>The comparison of confusion matrices for all models on real data is presented in <xref rid=\"fig17\" ref-type=\"fig\">Figure 15</xref>. Subfigure (a) for logistic regression shows 480 true negatives, 20 false positives, 30 false negatives, and 470 true positives, indicating perfect accuracy. Subfigure (b) for Random Forest displays 465 true negatives, 35 false positives, 55 false negatives, and 445 true positives, indicating moderate misclassification rates. Subfigure (c) for SVM presents 470 true negatives, 30 false positives, 50 false negatives, and 450 true positives, showing slight improvement. Subfigure (d) for CNN exhibits 475 true negatives, 25 false positives, 40 false negatives, and 460 true positives, demonstrating high accuracy. Subfigure (e) for the proposed AMEL records 478 true negatives, 22 false positives, 38 false negatives, and 462 true positives, highlighting the lowest misclassification rates.</p><fig position=\"float\" id=\"fig17\" orientation=\"portrait\"><label>Figure 15</label><caption><p>Comparison of confusion matrices for all models on real data: <bold>(a)</bold> Logistic Regression, <bold>(b)</bold> Random Forest, <bold>(c)</bold> SVM, <bold>(d)</bold> CNN, and <bold>(e)</bold> the proposed AMEL.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g017.jpg\"><alt-text content-type=\"machine-generated\">Five confusion matrices for different models: (a) Logistic Regression, perfect classification with 105 true negatives and 36 true positives. (b) Random Forest, one false positive and two false negatives. (c) SVM, similar to Random Forest. (d) CNN, one false positive, no false negatives. (e) Real Data, fifty false positives and negatives, 450 true negatives, 154 true positives.</alt-text></graphic></fig><p>The ROC curves in <xref rid=\"fig18\" ref-type=\"fig\">Figure 16</xref> highlight the discriminative performance of the models for autism prediction on real data. Logistic Regression and CNN exhibit perfect AUCs (1.0), consistent with their high accuracy, although Logistic Regression&#8217;s log loss (0.0308908) suggests potential overconfidence. Random Forest (AUC&#8239;=&#8239;0.998148) and SVM (AUC&#8239;=&#8239;0.997354) exhibit strong but slightly lower discrimination, which aligns with their moderate false negative rates. The proposed AMEL matches the perfect AUC of 1.0, reflecting its effective multimodal integration via adaptive weighting, supported by its F1 score (0.986301).</p><fig position=\"float\" id=\"fig18\" orientation=\"portrait\"><label>Figure 16</label><caption><p>ROC curve plot comparing the performance of all models on real data, with logistic regression (AUC&#8239;=&#8239;1.0), random forest (AUC&#8239;=&#8239;0.998148), SVM (AUC&#8239;=&#8239;0.997354), CNN (AUC&#8239;=&#8239;1.0), and the proposed AMEL (AUC&#8239;=&#8239;1.0), distinguished by legend entries, demonstrating their discriminative abilities.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g018.jpg\"><alt-text content-type=\"machine-generated\">ROC curve comparing five models: Logistic Regression, Random Forest, SVM, CNN, and AMEL. The x-axis represents false positive rate, and the y-axis represents true positive rate. The diagonal line represents random performance. AMEL and SVM show close-to-perfect performance, nearly blending at the top-left corner.</alt-text></graphic></fig><p><xref rid=\"fig19\" ref-type=\"fig\">Figure 17</xref> shows the accuracy and loss curves for the CNN and AMEL models, providing insights into their training dynamics. Both models converge to high accuracy (0.99&#8211;1.0), validating their effectiveness. However, CNN&#8217;s loss stabilizes at a lower value (around 0.01), indicating faster convergence and a better fit, while AMEL&#8217;s higher loss (around 0.05) suggests slower stabilization, likely due to its ensemble complexity. This aligns with AMEL&#8217;s log loss (0.049632) and supports its adaptive weighting strategy, which enhances the F1 score but requires optimization.</p><fig position=\"float\" id=\"fig19\" orientation=\"portrait\"><label>Figure 17</label><caption><p>Plot of accuracy and loss curves for CNN and AMEL over training epochs on real data.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g019.jpg\"><alt-text content-type=\"machine-generated\">Four line graphs compare CNN and AMEL models. Top left shows CNN loss with training and validation loss decreasing similarly. Top right displays CNN accuracy with both training and validation accuracy increasing, nearing one point zero. Bottom left depicts AMEL loss with both loss values decreasing. Bottom right illustrates AMEL accuracy with training and validation accuracy increasing, approaching one point zero. Each graph has a legend differentiating between training (blue) and validation (orange) metrics.</alt-text></graphic></fig><sec id=\"sec25\"><label>4.2.1</label><title>Privacy&#8211;utility trade-off</title><p>To evaluate the impact of varying the differential privacy budget, we trained MADSN under &#949; &#8712; {0.1, 0.5, 1.0, 2.0}. <xref rid=\"fig20\" ref-type=\"fig\">Figure 18</xref> shows the resulting fidelity and classification metrics. As expected, stronger privacy (&#949;&#8239;=&#8239;0.1) significantly reduces utility, while relaxed privacy (&#949;&#8239;=&#8239;2.0) preserves utility but weakens guarantees. The intermediate setting &#949;&#8239;=&#8239;1.0 provided the best balance, consistent with our main experiments.</p><fig position=\"float\" id=\"fig20\" orientation=\"portrait\"><label>Figure 18</label><caption><p>Privacy&#8211;utility trade-off for AutismSynthGen. As &#949; decreases (resulting in stronger privacy), the classification AUC drops, while fidelity metrics (MMD, BLEU) worsen. At &#949;&#8239;=&#8239;1.0, the model achieves a balanced trade-off, consistent with the main results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g020.jpg\"><alt-text content-type=\"machine-generated\">Line graph titled \\\"Privacy-Utility Trade-off in AutismSynthGen\\\" showing three metrics against Privacy Budget values ranging from 0 to 2. The Classification AUC (blue line) increases, MMD (orange line) slightly decreases, and BLEU score (green line) rises. Each metric is plotted with distinct markers. The left y-axis measures Classification AUC, while the right y-axis measures MMD/BLEU.</alt-text></graphic></fig></sec><sec id=\"sec26\"><label>4.2.2</label><title>Calibration analysis</title><p>In addition to discrimination metrics such as AUC and F1, the calibration of AutismSynthGen predictions is evaluated. Calibration reflects how well predicted probabilities align with actual observed outcomes, which is particularly important in clinical decision-making, where overconfident or underconfident predictions can lead to misinformed decisions. Brier scores as a quantitative measure of calibration are reported. For AMEL trained on real-only data, the Brier score was 0.041; however, the inclusion of synthetic augmentation improved calibration to 0.018. Lower values indicate better calibration, suggesting that synthetic augmentation not only enhances classification accuracy but also improves the reliability of probability estimates. To further illustrate calibration quality, we plotted reliability diagrams (<xref rid=\"fig21\" ref-type=\"fig\">Figure 19</xref>). For AMEL trained on real-only data, the predicted probabilities tended to be slightly overconfident at higher probability bins. By contrast, AMEL trained with synthetic augmentation produced curves that were much closer to the diagonal line, indicating improved alignment between the predicted and observed outcomes. These findings reinforce that AutismSynthGen improves not only the discriminative ability of models but also the trustworthiness of their confidence estimates, which is critical for clinical adoption, where calibrated risk scores are preferred over raw labels.</p><fig position=\"float\" id=\"fig21\" orientation=\"portrait\"><label>Figure 19</label><caption><p>Reliability diagrams for AMEL with real-only vs. real&#8239;+&#8239;synthetic data.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g021.jpg\"><alt-text content-type=\"machine-generated\">Reliability diagram for AMEL showing observed versus predicted probabilities. A dashed line represents perfect calibration. Real-only data (orange line, Brier=0.302) and real plus synthetic data (blue line, Brier=0.277) are plotted.</alt-text></graphic></fig></sec></sec><sec id=\"sec27\"><label>4.3</label><title>Performance comparison on real + synthetic data</title><p>From <xref rid=\"tab3\" ref-type=\"table\">Table 3</xref> and <xref rid=\"fig22\" ref-type=\"fig\">Figure 20</xref>, it is understood that all models achieved near-perfect internal classification performance (accuracy &#8776; 1.0, F1&#8239;&#8776;&#8239;1.0, precision &#8776; 1.0, recall &#8776; 1.0, and AUC&#8239;&#8776;&#8239;1.0), confirming that synthetic data substantially improved internal consistency and learning stability (<xref rid=\"ref39\" ref-type=\"bibr\">Wang et al., 2024</xref>). All reported AUC and F1 metrics represent mean &#177; standard deviation across three independent random seeds (42, 123, 2025). To quantify metric stability, we also estimated 95% bootstrap confidence intervals using 1,000 resamples from the validation folds. The narrow CIs (&lt; 0.02 width) indicate consistent internal performance across runs. This aligns with recent findings on GAN-augmented medical data (<xref rid=\"ref40\" ref-type=\"bibr\">Wang et al., 2017</xref>). AMEL&#8217;s log loss (1.9&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>5</sup>) surpasses that of CNN (1.3&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>4</sup>), demonstrating that its adaptive ensemble optimally weights multimodal features. The 85% reduction in log loss compared to CNN suggests that AMEL better captures prediction uncertainties (<xref rid=\"ref41\" ref-type=\"bibr\">Washington et al., 2022</xref>). While perfect metrics warrant validation on larger datasets, AMEL&#8217;s performance indicates robust multimodal integration. Although near-perfect internal metrics (AUC&#8239;&#8776;&#8239;1.0, F1&#8239;&#8776;&#8239;1.0) were observed with synthetic augmentation, these results should be interpreted with caution, as they may partly arise from distributional similarity rather than full generalization. While perfect performance was obtained with synthetic augmentation, these results should be viewed as upper-bound estimates. Comparable state-of-the-art multimodal ASD classifiers (e.g., attention-based fusion, explainable federated learning) typically achieve AUC values between 0.85 and 0.95, highlighting the need for caution in interpreting internally perfect scores.</p><table-wrap position=\"float\" id=\"tab3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Performance comparison on real + synthetic data.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">F1-Score</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Precision</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Recall</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">AUC</th><th align=\"center\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Log loss</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Logistic Regression</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0104</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Random Forest</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0042</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">SVM</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0013</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CNN</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.0001</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Proposed AMEL</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">1.0</td><td align=\"char\" valign=\"top\" char=\"&#xB1;\" rowspan=\"1\" colspan=\"1\">1.0 &#177; 0.00</td><td align=\"char\" valign=\"top\" char=\".\" rowspan=\"1\" colspan=\"1\">0.000019</td></tr></tbody></table><table-wrap-foot><p>Metrics are reported as mean &#177; SD (three runs) with corresponding 95% bootstrap confidence intervals. Values near 1.00 reflect internal validation consistency rather than external generalization.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"fig22\" orientation=\"portrait\"><label>Figure 20</label><caption><p>Comparative performance metrics across models on Real + Synthetic Data. While all models achieve perfect classification (accuracy&#8239;=&#8239;F1&#8239;=&#8239;precision&#8239;=&#8239;recall&#8239;=&#8239;AUC&#8239;=&#8239;1.0), AMEL demonstrates superior prediction confidence with log loss (0.000019), an order of magnitude lower than CNN (0.0001), suggesting optimal multimodal fusion.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g022.jpg\"><alt-text content-type=\"machine-generated\">Bar charts comparing classification metrics and log loss across five models: Logistic Regression, Random Forest, SVM, CNN, and Proposed AMEL. The left chart shows uniform scores near 1.0 for accuracy, F1-score, precision, recall, and AUC. The right chart displays log loss, with significantly lower values for CNN and Proposed AMEL.</alt-text></graphic></fig><p>The confusion matrices in <xref rid=\"fig23\" ref-type=\"fig\">Figure 21</xref> compare the performance of (a) logistic regression, (b) random forest, (c) SVM, (d) CNN, and (e) the proposed AMEL on real + synthetic data. Logistic regression and SVM achieve perfect classification (0 false positives/negatives), leveraging linear separability and effective margin maximization, respectively. Random Forest exhibits minimal misclassifications (2 FP, 1 FN) due to ensemble variance, while CNN has one false positive, likely from EEG signal artifacts not fully captured in synthetic data. The proposed AMEL outperforms all others, achieving zero misclassifications through the adaptive multimodal fusion of EEG, text, and demographic features, thereby validating its superior ensemble design.</p><fig position=\"float\" id=\"fig23\" orientation=\"portrait\"><label>Figure 21</label><caption><p>Comparison of confusion matrices for <bold>(a)</bold> logistic regression, <bold>(b)</bold> random forest, <bold>(c)</bold> SVM, <bold>(d)</bold> CNN, and <bold>(e)</bold> proposed AMEL.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g023.jpg\"><alt-text content-type=\"machine-generated\">Five confusion matrices compare prediction results of different models. (a) Logistic Regression: 105 true negatives, 0 false positives, 0 false negatives, 36 true positives. (b) Random Forest: 104 true negatives, 1 false positive, 2 false negatives, 34 true positives. (c) SVM: 105 true negatives, 0 false positives, 2 false negatives, 34 true positives. (d) CNN: 104 true negatives, 1 false positive, 0 false negatives, 36 true positives. (e) Proposed AMEL: 104 true negatives, 1 false positive, 0 false negatives, 36 true positives.</alt-text></graphic></fig><p>All models achieved internally near-perfect AUC values (&#8776; 1.0), reflecting strong internal discrimination on the augmented dataset (<xref rid=\"fig24\" ref-type=\"fig\">Figure 22</xref>). Logistic regression and CNN exhibit the smoothest curves, indicating stable performance across thresholds, while AMEL shows minor initial fluctuations, likely due to its sequential data processing. The results confirm that synthetic data augmentation eliminates the trade-off between sensitivity (true positive rate) and specificity (1&#8212;false positive rate), with all models attaining ideal discrimination (<xref rid=\"ref2\" ref-type=\"bibr\">Aslam et al., 2022</xref>).</p><fig position=\"float\" id=\"fig24\" orientation=\"portrait\"><label>Figure 22</label><caption><p>ROC curves for all models. All curves reach the optimal (0,1) point, reflecting perfect AUC scores (1.0). Line smoothness varies by architecture, with logistic regression (solid blue) showing the most consistent trajectory.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g024.jpg\"><alt-text content-type=\"machine-generated\">ROC curve illustrating the performance of five models: Logistic Regression (blue), Random Forest (orange), SVM (green), CNN (red), and AMEL (purple). The x-axis shows the false positive rate, and the y-axis shows the true positive rate. All curves perform near the top-left, indicating high sensitivity and specificity. A dashed diagonal line represents random chance.</alt-text></graphic></fig><p>Both CNN and AMEL exhibit stable convergence, with training and validation metrics closely aligned, indicating effective learning without overfitting (<xref rid=\"fig25\" ref-type=\"fig\">Figure 23</xref>). The CNN achieves marginally lower final loss (0.0 vs. AMEL&#8217;s 0.1) and higher validation accuracy (95% vs. 90%), suggesting stronger feature extraction from the synthetic data. However, AMEL&#8217;s smoother accuracy progression demonstrates the adaptive ensemble&#8217;s robustness to volatility, particularly between epochs 10 and 20, where the CNN&#8217;s accuracy fluctuates. The sub-0.1 loss values for both models confirm the successful integration of synthetic data, although the CNN&#8217;s faster convergence (by ~5 epochs) highlights its architectural efficiency for this task. Experiments were run on four NVIDIA A100 GPUs (256&#8239;GB RAM), with GAN training requiring ~48&#8239;h and ensemble fine-tuning requiring ~12&#8239;h. The GAN and ensemble models contain approximately 12&#8239;M and 8 M parameters, respectively.</p><fig position=\"float\" id=\"fig25\" orientation=\"portrait\"><label>Figure 23</label><caption><p>Training curves for CNN (top) and AMEL (bottom), showing loss (left) and accuracy (right) over 30 epochs.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g025.jpg\"><alt-text content-type=\"machine-generated\">Four line graphs compare CNN and AMEL models over 30 epochs. Top left: CNN Loss shows training and validation loss decreasing. Top right: CNN Accuracy depicts accuracy increasing, stabilizing around 0.98. Bottom left: AMEL Loss shows a similar decrease in loss. Bottom right: AMEL Accuracy increases, leveling near 0.97.</alt-text></graphic></fig><p><xref rid=\"fig26\" ref-type=\"fig\">Figure 24</xref> shows that proposed AMEL model demonstrates superior performance, achieving 100% accuracy across all runs with zero variance, compared to CNN&#8217;s 99.88% (95% CI: 99.81&#8211;99.95%), with a significant difference (paired t-test: t(9)&#8239;=&#8239;3.67, <italic toggle=\"yes\">p</italic>&#8239;=&#8239;0.0051; Wilcoxon W&#8239;=&#8239;0, <italic toggle=\"yes\">p</italic>&#8239;=&#8239;0.0156) and large effect size (Cohen&#8217;s d&#8239;=&#8239;1.22), confirming AMEL&#8217;s robustness through adaptive multimodal fusion of EEG, text, and demographic features. Additionally, AMEL&#8217;s log loss (1.9&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>5</sup>, 95% CI: 1.3&#8211;2.5&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>5</sup>) is 85% lower than CNN&#8217;s (1.3&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>4</sup>, 95% CI: 0.9&#8211;1.7&#8239;&#215;&#8239;10&#8239;&#8722;&#8239;<sup>4</sup>), with non-overlapping confidence intervals, highlighting its enhanced prediction confidence, which is critical for clinical applications. This perfect accuracy and reduced log loss reflect the synthetic data&#8217;s effectiveness in addressing class imbalance for rare autism subtypes and AMEL&#8217;s optimal feature weighting, mitigating overconfidence observed in single-modality CNN architectures.</p><fig position=\"float\" id=\"fig26\" orientation=\"portrait\"><label>Figure 24</label><caption><p>Model performance comparison. (Left) Accuracy with 95% CIs (AMEL: 100%, CNN: 99.88% [99.81&#8211;99.95]). (Right) Log loss (log scale; AMEL: 1.9&#8239;&#215;&#8239;10<sup>&#8722;5</sup> [1.3&#8211;2.5&#8239;&#215;&#8239;10<sup>&#8722;5</sup>], CNN: 1.3&#8239;&#215;&#8239;10<sup>&#8722;4</sup> [0.9&#8211;1.7&#8239;&#215;&#8239;10<sup>&#8722;4</sup>]). AMEL shows 85% lower log loss (arrow) and statistically superior accuracy (<italic toggle=\"yes\">p</italic>&#8239;&lt;&#8239;0.01).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g026.jpg\"><alt-text content-type=\"machine-generated\">Bar charts comparing model accuracy and calibration with 95% confidence intervals. The left chart shows AMEL with higher accuracy than CNN, with p-value 0.0051 and Cohen's d 1.22. The right chart shows AMEL has significantly lower log loss than CNN, indicating an 85% reduction.</alt-text></graphic></fig></sec><sec id=\"sec28\"><label>4.4</label><title>Ablation study results</title><p>The ablation study in <xref rid=\"fig27\" ref-type=\"fig\">Figure 25</xref> reveals three critical insights: (1) EEG is the most impactful modality, with its removal causing a 12.3% accuracy drop and 420% higher log loss (<italic toggle=\"yes\">p</italic>&#8239;&lt;&#8239;0.001), validating its necessity for robust autism prediction. (2) Text and demographic data also contribute significantly (8.2 and 5.1% accuracy reductions, respectively), proving multimodal integration is essential. (3) The 67% MMD increase when removing transformer fusion demonstrates its vital role in cross-modal alignment, while attention mechanisms maintain EEG-text coherence (KS <italic toggle=\"yes\">p</italic>-value drops to 0.03). These results collectively confirm that both the multimodal inputs and MADSN&#8217;s architectural components are non-redundant for optimal performance. The log loss degradation patterns further suggest that EEG data is particularly crucial for model calibration, likely due to its high-dimensional discriminative features. In modality ablation, EEG removal caused the largest drop in performance (&#8722;12% accuracy), followed by behavioral text (&#8722;8%) and demographics (&#8722;5%). Despite these reductions, the ensemble continued to perform above baseline, demonstrating resilience to missing modalities.</p><fig position=\"float\" id=\"fig27\" orientation=\"portrait\"><label>Figure 25</label><caption><p>Ablation study results. <bold>(a)</bold> Accuracy reduction when removing modalities (EEG shows the largest impact). <bold>(b)</bold> Corresponding log loss increase. <bold>(c)</bold> Component analysis reveals transformer fusion contributes most to data realism (67% MMD increase when removed). All changes are statistically significant (Friedman test <italic toggle=\"yes\">p</italic>&#8239;&lt;&#8239;0.001).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g027.jpg\"><alt-text content-type=\"machine-generated\">Three bar charts labeled A, B, and C. Chart A shows modality ablation impact on classification accuracy, with the full model achieving 100 percent, and reductions from EEG, text, and demographics. Chart B shows modality impact on calibration with increased log loss when ablations occur. Chart C displays MADSN component contribution with MMD increases and KS p-value changes.</alt-text></graphic></fig><sec id=\"sec29\"><label>4.4.1</label><title>Preliminary interpretability analysis</title><p>To explore interpretability, a preliminary analysis is conducted to examine the contributions of modality, feature, and signal levels. <xref rid=\"fig28\" ref-type=\"fig\">Figure 26</xref> shows modality weights from AMEL&#8217;s gating network: EEG contributed most (~42%), followed by behavioral scores (~28%) and severity measures (~15%), with MRI and genetics contributing less. This reflects AMEL&#8217;s adaptive weighting strategy in practice. <xref rid=\"fig29\" ref-type=\"fig\">Figure 27</xref> presents SHAP-style feature importance for behavioral vectors. Social reciprocity (~21%), communication (~18%), and repetitive behaviors (~16%) emerged as the most influential behavioral features, while adaptive skills and sensory sensitivity played secondary roles. <xref rid=\"fig30\" ref-type=\"fig\">Figure 28</xref> shows an EEG saliency map, which visualizes the relative importance across channels and time windows. Frontal-temporal electrodes (e.g., Ch-3, Ch-7) demonstrated higher contributions in early temporal segments, consistent with known neurodevelopmental biomarkers in ASD. Although these analyses are qualitative and exploratory, they highlight that AutismSynthGen is not a &#8220;black box&#8221; but is capable of exposing modality- and feature-level signals that drive its predictions. A systematic, clinician-guided interpretability study will be pursued in future research.</p><fig position=\"float\" id=\"fig28\" orientation=\"portrait\"><label>Figure 26</label><caption><p>Preliminary interpretability analysis from AMEL&#8217;s gating network. EEG consistently receives the highest contribution weight (~42%), followed by behavioral scores (~28%) and severity scores (~15%). MRI and genetics contribute less in this example. These modality-level weights provide qualitative insight into which inputs drive AutismSynthGen&#8217;s predictions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g028.jpg\"><alt-text content-type=\"machine-generated\">Bar chart titled &#8220;AMEL Gating Network &#8211; Modality Contributions&#8221; showing contributions from five modalities: EEG 42.00%, Behavioral Scores 28.00%, Severity Scores 15.00%, MRI 10.00%, and Genetics 5.00%. Normalized contribution weight is on the x-axis.</alt-text></graphic></fig><fig position=\"float\" id=\"fig29\" orientation=\"portrait\"><label>Figure 27</label><caption><p>SHAP-style mean absolute feature importance for behavioral vectors.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g029.jpg\"><alt-text content-type=\"machine-generated\">Bar chart titled \\\"Behavioral Vector Feature Importance (SHAP-style)\\\" with eight colored bars. Social Reciprocity is most important at 21%, followed by Communication at 18%, Repetitive Behaviour at 16%, Restricted Interests at 12%, Adaptive Skills at 11%, Sensory Sensitivity at 10%, Attention at 7%, and Emotion Regulation at 5%. The x-axis shows normalized mean SHAP values.</alt-text></graphic></fig><fig position=\"float\" id=\"fig30\" orientation=\"portrait\"><label>Figure 28</label><caption><p>EEG saliency map across channels and time windows. Darker regions indicate higher importance for classification decisions. Frontal&#8211;temporal channels (e.g., Ch-3, Ch-7) showed strong contributions in early time windows, suggesting temporal&#8211;spatial EEG features that AutismSynthGen leverages for ASD prediction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fninf-19-1679196-g030.jpg\"><alt-text content-type=\"machine-generated\">EEG saliency map showing feature importance across channels and time. It is a heat map with EEG channels on the vertical axis and time in milliseconds on the horizontal axis. Color gradient indicates relative importance, ranging from purple (low) to yellow (high). Notable high importance areas are in channels 14 and 11 at 60 and 90 milliseconds, respectively.</alt-text></graphic></fig><p>In <xref rid=\"tab4\" ref-type=\"table\">Table 4</xref>, AutismSynthGen is compared with several representative recent models. For MCBERT, <xref rid=\"ref18\" ref-type=\"bibr\">Khan and Katarya (2025)</xref> report 93.4% accuracy in a leave-one-site-out evaluation using ABIDE data (<xref rid=\"ref36\" ref-type=\"bibr\">Vidivelli et al., 2025</xref>). The MADDHM model (Vidivelli et al.) achieves approximately 91.03% accuracy on EEG and 91.67% on face modalities in multimodal fusion experiments (<xref rid=\"ref17\" ref-type=\"bibr\">Kasri et al., 2025</xref>). More recently, the Vision Transformer-Mamba hybrid model, applied to the Saliency4ASD dataset, achieves an accuracy of 0.96, an F1 score of 0.95, a sensitivity of 0.97, and a specificity of 0.94, highlighting strong performance in a newer fusion paradigm (<xref rid=\"ref17\" ref-type=\"bibr\">Kasri et al., 2025</xref>). Compared to these existing works, AutismSynthGen distinguishes itself by integrating synthetic data augmentation under differential privacy, cross-modal attention, and a mixture-of-experts fusion pipeline in a unified system. Although our internal validation results approach perfect values, we reiterate that independent external validation remains a vital future direction before claiming generalizability.</p><table-wrap position=\"float\" id=\"tab4\" orientation=\"portrait\"><label>Table 4</label><caption><p>Comparison of AutismSynthGen with selected recent multimodal or hybrid ASD models (2023&#8211;2025).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Modalities / data types</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Dataset(s) / evaluation setting</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Reported performance</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Key differences &amp; comments</th><th align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Reference</th></tr></thead><tbody><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">AutismSynthGen (Proposed)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Imaging + EEG&#8239;+&#8239;Behavioral</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Internal cross-validation (ABIDE, NDAR, SSC)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00 (internal)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Uses synthetic augmentation under differential privacy, mixture-of-experts fusion, and cross-modal attention</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">&#8212;</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MCBERT</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Imaging + meta / behavioral features (via BERT)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ABIDE (leave-one-site-out)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy&#8239;=&#8239;93.4%</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Combines CNN (with spatial + channel attention)&#8239;+&#8239;BERT fusion; no synthetic augmentation or differential privacy applied</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref18\" ref-type=\"bibr\">Khan and Katarya (2025)</xref>\n</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MADDHM (Deep Hybrid Model)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">EEG&#8239;+&#8239;Face/image</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Dataset used in paper (fusion setting)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy &#8776; 91.03% (EEG), 91.67% (face)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Fusion at feature level; does not explicitly include synthetic DP augmentation</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref36\" ref-type=\"bibr\">Vidivelli et al. (2025)</xref>\n</td></tr><tr><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Vision Transformer-Mamba (Hybrid, eye-tracking + image + speech cues)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Eye-tracking + visual/facial cues</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Saliency4ASD dataset</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy&#8239;=&#8239;0.96, F1&#8239;=&#8239;0.95, Sensitivity&#8239;=&#8239;0.97, Specificity&#8239;=&#8239;0.94</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">The recent hybrid model using attention-based fusion and transformer components is a good benchmark for recent works</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">\n<xref rid=\"ref17\" ref-type=\"bibr\">Kasri et al. (2025)</xref>\n</td></tr></tbody></table><table-wrap-foot><p>&#8220;Reported Performance&#8221; refers to the primary metric(s) as presented in each paper under their reported evaluation settings.</p></table-wrap-foot></table-wrap></sec></sec><sec id=\"sec30\"><label>4.5</label><title>Limitations</title><p>Although near-perfect internal metrics (AUC&#8239;&#8776;&#8239;1.0, F1&#8239;&#8776;&#8239;1.0) were observed when combining real and synthetic data, such results should be interpreted cautiously and regarded as upper-bound internal estimates. While they reflect strong alignment between real and generated distributions, they may also partly arise from distributional similarity that reduces the generalization challenge. Notably, real-only performance (AUC&#8239;=&#8239;0.98, F1&#8239;=&#8239;0.99) indicates that the system is not trivially overfitting. Future external validation is needed to establish robustness. Independent validation on unseen cohorts was not feasible due to dataset constraints; thus, generalizability beyond ABIDE, NDAR, and SSC remains to be established. Future studies will incorporate held-out site validation and external benchmarking. While results demonstrate strong performance across ABIDE, NDAR, and SSC, all experiments were confined to publicly available cohorts. Validation on unseen hospital datasets or prospective clinical cohorts is necessary to establish real-world generalizability. While we include preliminary interpretability (gating weights and SHAP-style attributions), a systematic clinician-validated explainability study (e.g., EEG saliency maps, per-item SHAP reviewed by clinicians) remains future work. Currently, AutismSynthGen generates text only at the embedding level; human-readable behavioral narratives are not reconstructed. While this design ensures stability and privacy, future studies will explore transformer-based encoder&#8211;decoder architectures for realistic text generation, combined with blinded clinician review to assess interpretability and clinical realism. Future studies will involve collaborations with clinical sites to test AutismSynthGen on independent, non-public cohorts and assess robustness across diverse populations and acquisition protocols. While our analysis demonstrates privacy&#8211;utility trade-offs across <italic toggle=\"yes\">&#949;</italic> values, these results remain theoretical. Future studies should also test empirical privacy leakage (e.g., membership inference attacks) to complement the theoretical guarantees.</p><p>Complementary to our approach, explainable federated learning frameworks (<xref rid=\"ref1\" ref-type=\"bibr\">Alshammari et al., 2024</xref>) demonstrate how privacy and interpretability can be jointly addressed in distributed ASD prediction. Future studies may explore the integration of federated setups with AutismSynthGen, extending synthetic data generation to decentralized environments.</p></sec><sec id=\"sec31\"><label>4.6</label><title>Ethical considerations</title><p>Although AutismSynthGen enforces differential privacy (&#949;&#8239;&#8804;&#8239;1.0), the residual risk of indirect re-identification cannot be completely excluded. Any release of synthetic ASD data should therefore occur only under controlled access with data use agreements, ensuring prevention of unintended or commercial misuse. Given the clinical and societal sensitivities surrounding ASD, consultation with institutional review boards, clinicians, and patient advocacy groups is essential before broad dissemination. We emphasize that synthetic datasets are intended to support reproducibility and collaborative research, not to bypass established ethical safeguards.</p></sec></sec><sec sec-type=\"conclusions\" id=\"sec32\"><label>5</label><title>Conclusion</title><p>This study introduces AutismSynthGen, a unified framework for privacy-preserving synthesis and adaptive multimodal prediction of AutismSpectrum Disorder (ASD). By combining a transformer-based conditional generative model (MADSN) with differential privacy (&#949;&#8239;&#8804;&#8239;1.0) and an adaptive mixture-of-experts ensemble (AMEL), the framework effectively augmented limited multimodal datasets and improved classification performance across imaging, EEG, and behavioral modalities. Synthetic data enhanced internal validation results, with AUC and F1 values approaching 1.0, and fidelity metrics (MMD&#8239;=&#8239;0.04; KS&#8239;=&#8239;0.03; BLEU&#8239;=&#8239;0.70) demonstrating strong alignment between real and generated samples. While these outcomes underscore the potential of privacy-compliant data synthesis in ASD research, they reflect internal cross-validation within ABIDE, NDAR, and SSC datasets rather than independent external testing. Therefore, the reported near-ceiling performance should be regarded as an upper-bound estimate of internal consistency, not as evidence of clinical generalization. Future studies will focus on validating AutismSynthGen on unseen hospital cohorts and federated clinical sites, assessing its robustness under diverse acquisition settings, and conducting empirical analyses of privacy leakage and interpretability. In addition, extending the framework toward semi-supervised learning, adaptive noise scheduling, and explainable fusion mechanisms will further strengthen its clinical applicability. Ultimately, AutismSynthGen represents a promising step toward scalable, privacy-aware, and interpretable multimodal modeling for neurodevelopmental disorders, but independent external validation remains an essential prerequisite before real-world deployment.</p></sec></body><back><fn-group><fn id=\"fn0001\" fn-type=\"edited-by\"><p>Edited by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/2720614/overview\" ext-link-type=\"uri\">Maryam Naseri</ext-link>, Yale University, United States</p></fn><fn id=\"fn0002\" fn-type=\"reviewed-by\"><p>Reviewed by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/1987253/overview\" ext-link-type=\"uri\">Mahsa Asadi Anar</ext-link>, Shahid Beheshti University of Medical Sciences, Iran</p><p><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3184873/overview\" ext-link-type=\"uri\">Akram Pasha</ext-link>, University of Fujairah, United Arab Emirates</p></fn></fn-group><sec sec-type=\"data-availability\" id=\"sec33\"><title>Data availability statement</title><p>The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/mkarthiga2211/Autism-SynthGen.git\" ext-link-type=\"uri\">https://github.com/mkarthiga2211/Autism-SynthGen.git</ext-link>.</p></sec><sec sec-type=\"ethics-statement\" id=\"sec34\"><title>Ethics statement</title><p>This study was purely computational, and all procedures were performed in compliance with relevant laws and institutional guidelines, as it falls within an area of research that does not require institutional approval by an ethics committee.</p></sec><sec sec-type=\"author-contributions\" id=\"sec35\"><title>Author contributions</title><p>JR: Conceptualization, Investigation, Methodology, Validation, Writing &#8211; original draft. KM: Data curation, Formal analysis, Project administration, Software, Supervision, Visualization, Writing &#8211; original draft, Writing &#8211; review &amp; editing.</p></sec><sec sec-type=\"COI-statement\" id=\"sec37\"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type=\"ai-statement\" id=\"sec38\"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p><p>Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us.</p></sec><sec sec-type=\"disclaimer\" id=\"sec39\"><title>Publisher&#8217;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id=\"ref1\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alshammari</surname><given-names>N. K.</given-names></name><name name-style=\"western\"><surname>Alhusaini</surname><given-names>A. A.</given-names></name><name name-style=\"western\"><surname>Pasha</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Ahamed</surname><given-names>S. S.</given-names></name><name name-style=\"western\"><surname>Gadekallu</surname><given-names>T. R.</given-names></name><name name-style=\"western\"><surname>Abdullah-Al-Wadud</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2024</year>). <article-title>Explainable federated learning for enhanced privacy in autism prediction using deep learning</article-title>. <source>J. Disabil. Res.</source><volume>3</volume>:<fpage>20240081</fpage>. doi: <pub-id pub-id-type=\"doi\">10.57197/JDR-2024-0081</pub-id></mixed-citation></ref><ref id=\"ref2\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Aslam</surname><given-names>A. R.</given-names></name><name name-style=\"western\"><surname>Hafeez</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Heidari</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Altaf</surname><given-names>M. A. B.</given-names></name></person-group> (<year>2022</year>). <article-title>Channels and features identification: a review and a machine-learning based model with large scale feature extraction for emotions and ASD classification</article-title>. <source>Front. Neurosci.</source><volume>16</volume>:<fpage>844851</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fnins.2022.844851</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">35937896</pub-id><pub-id pub-id-type=\"pmcid\">PMC9355483</pub-id></mixed-citation></ref><ref id=\"ref3\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Avasthi</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sanwal</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Tripathi</surname><given-names>S. L.</given-names></name><name name-style=\"western\"><surname>Tyagi</surname><given-names>M.</given-names></name></person-group> (<year>2025</year>). &#8220;<article-title>Transformer models for topic extraction from narratives and biomedical text analysis</article-title>&#8221; in <source>Mining biomedical text, images and visual features for information retrieval</source>. Eds. S. Dash, S. K. Pani, W. P. D. Santos, and J. Y. Chen (<publisher-name>USA: Academic Press</publisher-name>), <fpage>273</fpage>&#8211;<lpage>286</lpage>.</mixed-citation></ref><ref id=\"ref4\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Baltru&#353;aitis</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ahuja</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Morency</surname><given-names>L. P.</given-names></name></person-group> (<year>2018</year>). <article-title>Multimodal machine learning: a survey and taxonomy</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>41</volume>, <fpage>423</fpage>&#8211;<lpage>443</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/TPAMI.2018.2798607</pub-id><pub-id pub-id-type=\"pmid\">29994351</pub-id></mixed-citation></ref><ref id=\"ref5\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bernabeu</surname><given-names>P.</given-names></name></person-group> (<year>2022</year>). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power (doctoral dissertation, Lancaster University)</mixed-citation></ref><ref id=\"ref6\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Borodin</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Duncan</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Khovanova</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Litchev</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Sequences of the stable matching problem</article-title>. <source>arXiv</source>:2201.00645. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2201.00645</pub-id></mixed-citation></ref><ref id=\"ref7\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dcouto</surname><given-names>S. S.</given-names></name><name name-style=\"western\"><surname>Pradeepkandhasamy</surname><given-names>J.</given-names></name></person-group> (<year>2024</year>). <article-title>Multimodal deep learning in early autism detection&#8212;recent advances and challenges</article-title>. <source>Eng. Proc.</source><volume>59</volume>:<fpage>205</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3390/engproc2023059205</pub-id></mixed-citation></ref><ref id=\"ref8\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Di Martino</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>O&#8217;Connor</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Alaerts</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Anderson</surname><given-names>J. S.</given-names></name><name name-style=\"western\"><surname>Assaf</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Enhancing studies of the connectome in autism using the autism brain imaging data exchange II</article-title>. <source>Sci Data</source><volume>4</volume>, <fpage>1</fpage>&#8211;<lpage>15</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1038/sdata.2017.10</pub-id><pub-id pub-id-type=\"pmcid\">PMC5349246</pub-id><pub-id pub-id-type=\"pmid\">28291247</pub-id></mixed-citation></ref><ref id=\"ref9\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>T.</given-names></name></person-group> (<year>2024</year>). <article-title>Deep learning approach to predict autism spectrum disorder: a systematic review and meta-analysis</article-title>. <source>BMC Psychiatry</source><volume>24</volume>:<fpage>739</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1186/s12888-024-06116-0</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39468522</pub-id><pub-id pub-id-type=\"pmcid\">PMC11520796</pub-id><related-article xmlns:xlink=\"http://www.w3.org/1999/xlink\" related-article-type=\"retraction-forward\" ext-link-type=\"pmc\" xlink:href=\"PMC12628624\"/></mixed-citation></ref><ref id=\"ref10\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Eslami</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Mirjalili</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Fong</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Laird</surname><given-names>A. R.</given-names></name><name name-style=\"western\"><surname>Saeed</surname><given-names>F.</given-names></name></person-group> (<year>2019</year>). <article-title>ASD-DiagNet: a hybrid learning approach for detection of autism spectrum disorder using fMRI data</article-title>. <source>Front. Neuroinform.</source><volume>13</volume>:<fpage>70</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fninf.2019.00070</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">31827430</pub-id><pub-id pub-id-type=\"pmcid\">PMC6890833</pub-id></mixed-citation></ref><ref id=\"ref11\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fang</surname><given-names>M. L.</given-names></name><name name-style=\"western\"><surname>Dhami</surname><given-names>D. S.</given-names></name><name name-style=\"western\"><surname>Kersting</surname><given-names>K.</given-names></name></person-group> (<year>2022</year>). &#8220;<article-title>Dp-ctgan: differentially private medical data generation using ctgans</article-title>&#8221; in <source>International conference on artificial intelligence in medicine</source>. Eds. E. Bertino, W. Gao, B. Steffen, and M. Yung (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>178</fpage>&#8211;<lpage>188</lpage>.</mixed-citation></ref><ref id=\"ref12\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Friedrich</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Stammer</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Schramowski</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Kersting</surname><given-names>K.</given-names></name></person-group> (<year>2023</year>). <article-title>A typology for exploring the mitigation of shortcut behaviour</article-title>. <source>Nat. Mach. Intell.</source><volume>5</volume>, <fpage>319</fpage>&#8211;<lpage>330</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1038/s42256-023-00612-w</pub-id></mixed-citation></ref><ref id=\"ref13\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gupta</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Aly</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Ifeachor</surname><given-names>E.</given-names></name></person-group> (<year>2025</year>). &#8220;Cross-domain transfer learning for domain adaptation in autism Spectrum disorder diagnosis.&#8221; In: <italic toggle=\"yes\">18th international conference on health informatics</italic>.</mixed-citation></ref><ref id=\"ref14\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Harris</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Ho</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Saria</surname><given-names>S.</given-names></name></person-group> (<year>2024</year>). <article-title>Fusemoe: mixture-of-experts transformers for fleximodal fusion</article-title>. <source>Adv. Neural Inf. Proces. Syst.</source><volume>37</volume>, <fpage>67850</fpage>&#8211;<lpage>67900</lpage>. doi: <pub-id pub-id-type=\"doi\">10.52202/079017-2167</pub-id></mixed-citation></ref><ref id=\"ref15\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hartmann</surname><given-names>K. G.</given-names></name><name name-style=\"western\"><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><name name-style=\"western\"><surname>Ball</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>EEG-GAN: generative adversarial networks for electroencephalographic (EEG) brain signals</article-title>. <source>arXiv</source>:1806.01875. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.1806.01875</pub-id></mixed-citation></ref><ref id=\"ref16\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Heinsfeld</surname><given-names>A. S.</given-names></name><name name-style=\"western\"><surname>Franco</surname><given-names>A. R.</given-names></name><name name-style=\"western\"><surname>Craddock</surname><given-names>R. C.</given-names></name><name name-style=\"western\"><surname>Buchweitz</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Meneguzzi</surname><given-names>F.</given-names></name></person-group> (<year>2018</year>). <article-title>Identification of autism spectrum disorder using deep learning and the ABIDE dataset</article-title>. <source>NeuroImage: Clin.</source><volume>17</volume>, <fpage>16</fpage>&#8211;<lpage>23</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.nicl.2017.08.017</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">29034163</pub-id><pub-id pub-id-type=\"pmcid\">PMC5635344</pub-id></mixed-citation></ref><ref id=\"ref17\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kasri</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Himeur</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Copiaco</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mansoor</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Albanna</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Eapen</surname><given-names>V.</given-names></name></person-group> (<year>2025</year>). <article-title>Hybrid vision transformer-mamba framework for autism diagnosis via eye-tracking analysis</article-title>. <source>arXiv</source>:2506.06886. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2506.06886</pub-id></mixed-citation></ref><ref id=\"ref18\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Khan</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Katarya</surname><given-names>R.</given-names></name></person-group> (<year>2025</year>). <article-title>MCBERT: a multi-modal framework for the diagnosis of autism spectrum disorder</article-title>. <source>Biol. Psychol.</source><volume>194</volume>:<fpage>108976</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.biopsycho.2024.108976</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39722324</pub-id></mixed-citation></ref><ref id=\"ref19\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lakhan</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mohammed</surname><given-names>M. A.</given-names></name><name name-style=\"western\"><surname>Abdulkareem</surname><given-names>K. H.</given-names></name><name name-style=\"western\"><surname>Hamouda</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Alyahya</surname><given-names>S.</given-names></name></person-group> (<year>2023</year>). <article-title>Autism spectrum disorder detection framework for children based on federated learning integrated CNN-LSTM</article-title>. <source>Comput. Biol. Med.</source><volume>166</volume>:<fpage>107539</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2023.107539</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">37804778</pub-id></mixed-citation></ref><ref id=\"ref20\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Levy</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Ronemus</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yamrom</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>Y. H.</given-names></name><name name-style=\"western\"><surname>Leotta</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Kendall</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Rare de novo and transmitted copy-number variation in autistic spectrum disorders</article-title>. <source>Neuron</source><volume>70</volume>, <fpage>886</fpage>&#8211;<lpage>897</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.neuron.2011.05.015</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">21658582</pub-id></mixed-citation></ref><ref id=\"ref21\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shah</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2024</year>). <article-title>Therapeutic application of human type 2 innate lymphoid cells via induction of granzyme B-mediated tumor cell death</article-title>. <source>Cell</source><volume>187</volume>, <fpage>624</fpage>&#8211;<lpage>641</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.cell.2023.12.015</pub-id><pub-id pub-id-type=\"pmid\">38211590</pub-id><pub-id pub-id-type=\"pmcid\">PMC11442011</pub-id></mixed-citation></ref><ref id=\"ref22\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>Autism spectrum disorder studies using fMRI data and machine learning: a review</article-title>. <source>Front. Neurosci.</source><volume>15</volume>:<fpage>697870</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fnins.2021.697870</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">34602966</pub-id><pub-id pub-id-type=\"pmcid\">PMC8480393</pub-id></mixed-citation></ref><ref id=\"ref23\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Moridian</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Ghassemi</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Jafari</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Salloum-Asfar</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sadeghi</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Khodatars</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Automatic autism spectrum disorder detection using artificial intelligence methods with MRI neuroimaging: a review</article-title>. <source>Front. Mol. Neurosci.</source><volume>15</volume>:<fpage>999605</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fnmol.2022.999605</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">36267703</pub-id><pub-id pub-id-type=\"pmcid\">PMC9577321</pub-id></mixed-citation></ref><ref id=\"ref24\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nanayakkara</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Bater</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Hullman</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Rogers</surname><given-names>J.</given-names></name></person-group> (<year>2022</year>). <article-title>Visualizing privacy-utility trade-offs in differentially private data releases</article-title>. <source>Proc. Priv. Enhanc. Technol.</source><volume>2022</volume>, <fpage>601</fpage>&#8211;<lpage>618</lpage>. doi: <pub-id pub-id-type=\"doi\">10.2478/popets-2022-0058</pub-id></mixed-citation></ref><ref id=\"ref25\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nguyen</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ho</surname><given-names>N.</given-names></name></person-group> (<year>2023</year>). <article-title>Demystifying softmax gating function in Gaussian mixture of experts</article-title>. <source>Adv. Neural Inf. Proces. Syst.</source><volume>36</volume>, <fpage>4624</fpage>&#8211;<lpage>4652</lpage>.</mixed-citation></ref><ref id=\"ref26\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Okada</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Morita</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Tonsho</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Kiyota</surname><given-names>M.</given-names></name></person-group>, (<year>2025</year>). <source>The role of the globus pallidus subregions in the schizophrenia spectrum continuum</source>. [Preprint]. doi: <pub-id pub-id-type=\"doi\">10.21203/rs.3.rs-6439243/v1</pub-id></mixed-citation></ref><ref id=\"ref27\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Payakachat</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Tilford</surname><given-names>J. M.</given-names></name><name name-style=\"western\"><surname>Ungar</surname><given-names>W. J.</given-names></name></person-group> (<year>2016</year>). <article-title>National Database for autism research (NDAR): big data opportunities for health services research and health technology assessment</article-title>. <source>PharmacoEconomics</source><volume>34</volume>, <fpage>127</fpage>&#8211;<lpage>138</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s40273-015-0331-6</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">26446859</pub-id><pub-id pub-id-type=\"pmcid\">PMC4761298</pub-id></mixed-citation></ref><ref id=\"ref28\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chui</surname><given-names>M. L.</given-names></name><name name-style=\"western\"><surname>Pu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Gunda</surname><given-names>S. T.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>The application of deep learning for lymph node segmentation: a systematic review</article-title>. <source>IEEE Access</source><volume>13</volume>, <fpage>97208</fpage>&#8211;<lpage>97227</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/ACCESS.2025.3575454</pub-id></mixed-citation></ref><ref id=\"ref29\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rubio-Mart&#237;n</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Garc&#237;a-Ord&#225;s</surname><given-names>M. T.</given-names></name><name name-style=\"western\"><surname>Bay&#243;n-Guti&#233;rrez</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Prieto-Fern&#225;ndez</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Ben&#237;tez-Andrades</surname><given-names>J. A.</given-names></name></person-group> (<year>2024</year>). <article-title>Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing</article-title>. <source>Health Info. Sci. Syst.</source><volume>12</volume>:<fpage>20</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s13755-024-00281-y</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">38455725</pub-id><pub-id pub-id-type=\"pmcid\">PMC10917721</pub-id></mixed-citation></ref><ref id=\"ref30\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Schielen</surname><given-names>S. J.</given-names></name><name name-style=\"western\"><surname>Pilmeyer</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Aldenkamp</surname><given-names>A. P.</given-names></name><name name-style=\"western\"><surname>Zinger</surname><given-names>S.</given-names></name></person-group> (<year>2024</year>). <article-title>The diagnosis of ASD with MRI: a systematic review and meta-analysis</article-title>. <source>Transl. Psychiatry</source><volume>14</volume>:<fpage>318</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1038/s41398-024-03024-5</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39095368</pub-id><pub-id pub-id-type=\"pmcid\">PMC11297045</pub-id></mixed-citation></ref><ref id=\"ref31\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Mirhoseini</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Maziarz</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Davis</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Le</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Hinton</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Outrageously large neural networks: the sparsely-gated mixture-of-experts layer</article-title>. <source>arXiv</source>:1701.06538. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.1701.06538</pub-id></mixed-citation></ref><ref id=\"ref32\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Singh</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Malhotra</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Mengi</surname><given-names>M.</given-names></name></person-group> (<year>2023</year>). &#8220;<article-title>TransLearning ASD: detection of autism Spectrum disorder using domain adaptation and transfer learning-based approach on RS-FMRI data</article-title>&#8221; in <source>Artificial intelligence communication technology</source>. Eds. Harish Sharma, Mukesh Saraswat and Sandeep Kumar (India: SCRS), <fpage>863</fpage>&#8211;<lpage>871</lpage>.</mixed-citation></ref><ref id=\"ref33\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M.</given-names></name></person-group> (<year>2024</year>). <article-title>Multi-view and multimodal graph convolutional neural network for autism spectrum disorder diagnosis</article-title>. <source>Mathematics</source><volume>12</volume>:<fpage>1648</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3390/math12111648</pub-id></mixed-citation></ref><ref id=\"ref34\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Taiyeb Khosroshahi</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Morsali</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gharakhanlou</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Motamedi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Hassanbaghlou</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Vahedi</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>Explainable artificial intelligence in neuroimaging of Alzheimer&#8217;s disease</article-title>. <source>Diagnostics</source><volume>15</volume>:<fpage>612</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3390/diagnostics15050612</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">40075859</pub-id><pub-id pub-id-type=\"pmcid\">PMC11899653</pub-id></mixed-citation></ref><ref id=\"ref35\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Torkzadehmahani</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Kairouz</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Paten</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). &#8220;Dp-cgan: differentially private synthetic data and label generation.&#8221; In: <italic toggle=\"yes\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</italic>.</mixed-citation></ref><ref id=\"ref36\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vidivelli</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Padmakumari</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Shanthi</surname><given-names>P.</given-names></name></person-group> (<year>2025</year>). <article-title>Multimodal autism detection: deep hybrid model with improved feature level fusion</article-title>. <source>Comput. Methods Prog. Biomed.</source><volume>260</volume>:<fpage>108492</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.cmpb.2024.108492</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39700689</pub-id></mixed-citation></ref><ref id=\"ref37\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vimbi</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Shaffi</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Sadiq</surname><given-names>M. A.</given-names></name><name name-style=\"western\"><surname>Sirasanagandla</surname><given-names>S. R.</given-names></name><name name-style=\"western\"><surname>Aradhya</surname><given-names>V. M.</given-names></name><name name-style=\"western\"><surname>Kaiser</surname><given-names>M. S.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>Application of explainable artificial intelligence in autism spectrum disorder detection</article-title>. <source>Cogn. Comput.</source><volume>17</volume>:<fpage>104</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s12559-025-10462-w</pub-id></mixed-citation></ref><ref id=\"ref38\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Viswalingam</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>D.</given-names></name></person-group> (<year>2025</year>). &#8220;<article-title>Digital health solutions: enhancing medication adherence in COPD treatment</article-title>&#8221; in <source>Advanced drug delivery Systems in Management of chronic obstructive pulmonary disease</source>. Eds. P. Prasher, M. Sharma, G. Liu, A. Chakraborty, and K. Dua (<publisher-name>Florida, USA: CRC Press</publisher-name>), <fpage>213</fpage>&#8211;<lpage>238</lpage>.</mixed-citation></ref><ref id=\"ref39\"><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Pang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Rao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group> (<year>2024</year>). &#8220;Dp-promise: differentially private diffusion probabilistic models for image synthesis.&#8221; In: <italic toggle=\"yes\">33rd USENIX security symposium</italic>, pp.1063&#8211;1080.</mixed-citation></ref><ref id=\"ref40\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Nie</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Multi-task diagnosis for autism spectrum disorders using multi-modality features: a multi-center study</article-title>. <source>Hum. Brain Mapp.</source><volume>38</volume>, <fpage>3081</fpage>&#8211;<lpage>3097</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1002/hbm.23575</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">28345269</pub-id><pub-id pub-id-type=\"pmcid\">PMC5427005</pub-id></mixed-citation></ref><ref id=\"ref41\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Washington</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Mutlu</surname><given-names>C. O.</given-names></name><name name-style=\"western\"><surname>Kline</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Paskov</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Stockham</surname><given-names>N. T.</given-names></name><name name-style=\"western\"><surname>Chrisman</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Challenges and opportunities for machine learning classification of behavior and mental state from images</article-title>. <source>arXiv</source>:2201.11197. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2201.11197</pub-id></mixed-citation></ref><ref id=\"ref42\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Barnawi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Xi</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Y.</given-names></name></person-group> (<year>2021</year>). <article-title>FedDPGAN: federated differentially private generative adversarial networks framework for the detection of COVID-19 pneumonia</article-title>. <source>Inf. Syst. Front.</source><volume>23</volume>, <fpage>1403</fpage>&#8211;<lpage>1415</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/s10796-021-10144-6</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">34149305</pub-id><pub-id pub-id-type=\"pmcid\">PMC8204125</pub-id></mixed-citation></ref><ref id=\"ref43\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Duan</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Dvornek</surname><given-names>N. C.</given-names></name></person-group> (<year>2024a</year>). &#8220;<article-title>Self-supervised pre-training tasks for an fMRI time-series transformer in autism detection</article-title>&#8221; in <source>International workshop on machine learning in clinical neuroimaging</source>. Ed. P. L. Monaco (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer Nature Switzerland</publisher-name>), <fpage>145</fpage>&#8211;<lpage>154</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-031-78761-4_14</pub-id><pub-id pub-id-type=\"pmcid\">PMC11951341</pub-id><pub-id pub-id-type=\"pmid\">40160559</pub-id></mixed-citation></ref><ref id=\"ref44\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name></person-group> (<year>2024b</year>). <article-title>Advancing ASD identification with neuroimaging: a novel GARL methodology integrating deep Q-learning and generative adversarial networks</article-title>. <source>BMC Med. Imaging</source><volume>24</volume>:<fpage>186</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1186/s12880-024-01360-y</pub-id>, PMID: <pub-id pub-id-type=\"pmid\">39054419</pub-id><pub-id pub-id-type=\"pmcid\">PMC11270770</pub-id></mixed-citation></ref></ref-list><app-group><app id=\"app1\"><title>Appendix: a dataset and implementation details</title><p>Due to confidentiality, the full custom dataset cannot be publicly released. A subset of anonymized sample images is available at [<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/mkarthiga2211/Autism-SynthGen.git\" ext-link-type=\"uri\">https://github.com/mkarthiga2211/Autism-SynthGen.git</ext-link>]. The implementation code for Autism-SynthGen is publicly available at [<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/mkarthiga2211/Autism-SynthGen.git\" ext-link-type=\"uri\">https://github.com/mkarthiga2211/Autism-SynthGen.git</ext-link>], allowing for replication with alternative datasets. To enhance reproducibility, we provide full environment details (Python 3.9, PyTorch 2.0, Hugging Face Transformers 4.32, Scikit-learn 1.3), along with CUDA 11.7 compatibility. Training was conducted on 4&#8239;&#215;&#8239;NVIDIA A100 GPUs (40&#8239;GB each). Pretrained weights for MADSN and AMEL are available in the repository. A structured model card is included to document the model&#8217;s purpose, architecture, training setup, datasets used, limitations, and ethical considerations.</p></app></app-group></back></article></pmc-articleset>",
  "text": "pmc Front Neuroinform Front Neuroinform 647 frontneuroinfo Front. Neuroinform. Frontiers in Neuroinformatics 1662-5196 Frontiers Media SA PMC12673485 PMC12673485.1 12673485 12673485 41346493 10.3389/fninf.2025.1679196 1 Original Research Cross-modal privacy-preserving synthesis and mixture-of-experts ensemble for robust ASD prediction Revathy J. 1 Conceptualization Investigation Methodology Validation Writing &#8211; original draft M. Karthiga 2 * Data curation Formal analysis Project administration Software Supervision Visualization Writing &#8211; original draft Writing &#8211; review &amp; editing 1 Department of Artificial Intelligence and Data Science, Christ the King Engineering College , Coimbatore, Tamil Nadu , India 2 Department of Computer Science and Engineering, Bannari Amman Institute of Technology , Erode, Tamil Nadu , India , * Correspondence: Karthiga M., karthigam@bitsathy.ac.in 19 11 2025 2025 19 480826 1679196 04 8 2025 28 10 2025 19 11 2025 04 12 2025 05 12 2025 Copyright &#169; 2025 Revathy and M. 2025 Revathy and M https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY) . The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Introduction Autism Spectrum Disorder (ASD) diagnosis remains complex due to limited access to large-scale multimodal datasets and privacy concerns surrounding clinical data. Traditional methods rely heavily on resource-intensive clinical assessments and are constrained by unimodal or non-adaptive learning models. To address these limitations, this study introduces AutismSynthGen, a privacy-preserving framework for synthesizing multimodal ASD data and enhancing prediction accuracy. Materials and methods The proposed system integrates a Multimodal Autism Data Synthesis Network (MADSN), which employs transformer-based encoders and cross-modal attention within a conditional GAN to generate synthetic data across structural MRI, EEG, behavioral vectors, and severity scores. Differential privacy is enforced via DP-SGD ( &#949; &#8239;&#8804;&#8239;1.0). A complementary Adaptive Multimodal Ensemble Learning (AMEL) module, consisting of five heterogeneous experts and a gating network, is trained on both real and synthetic data. Evaluation is conducted on the ABIDE, NDAR, and SSC datasets using metrics such as AUC, F1 score, MMD, KS statistic, and BLEU. Results Synthetic augmentation improved model performance, yielding validation AUC gains of &#8805; 0.04. AMEL achieved an AUC of 0.98 and an F1 score of 0.99 on real data and approached near-perfect internal performance (AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00) when synthetic data were included. Distributional metrics (MMD&#8239;=&#8239;0.04; KS&#8239;=&#8239;0.03) and text similarity (BLEU&#8239;=&#8239;0.70) demonstrated high fidelity between the real and synthetic samples. Ablation studies confirmed the importance of cross-modal attention and entropy-regularized expert gating. Discussion AutismSynthGen offers a scalable, privacy-compliant solution for augmenting limited multimodal datasets and enhancing ASD prediction. Future directions include semi-supervised learning, explainable AI for clinical trust, and deployment in federated environments to broaden accessibility while maintaining privacy. Autism spectrum disorder multimodal data synthesis differential privacy generative adversarial network ensemble learning transformer mixture of experts The author(s) declare that no financial support was received for the research and/or publication of this article. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1 Introduction Autism spectrum disorder (ASD) encompasses a group of heterogeneous neurodevelopmental conditions defined by persistent deficits in social communication and interaction, along with restricted, repetitive patterns of behavior and interests. Early and accurate identification of ASD is critical: timely intervention can profoundly improve social, cognitive, and adaptive outcomes, yet standard diagnostic procedures remain labor-intensive and subjective. Clinicians currently rely on structured assessments, such as the Autism Diagnostic Observation Schedule (ADOS) and the Autism Diagnostic Interview&#8211;Revised (ADI-R), which require extensive training, can take several hours per evaluation, and exhibit substantial inter-rater variability ( Levy et al., 2011 ). Meanwhile, the prevalence of ASD has risen to an estimated 1&#8211;2% among children worldwide, imposing growing burdens on healthcare systems, educational services, and families ( Ding et al., 2024 ; Friedrich et al., 2023 ). In response to these limitations, deep learning approaches have emerged as promising solutions for automating the detection of ASD. Convolutional neural networks (CNNs) applied to structural and functional MRI have shown encouraging results. For instance, ASD-DiagNet leveraged an autoencoder with perceptual loss and data augmentation via linear interpolation to achieve up to 80% classification accuracy on fMRI scans ( Eslami et al., 2019 ). Similarly, generative adversarial networks (GANs) have been adapted to synthesize realistic biomedical time series. For instance, EEG-GAN demonstrated that GAN-based augmentation of electroencephalographic (EEG) data can enhance downstream classification performance in brain&#8211;computer interface tasks, suggesting applicability to clinical EEG analysis ( Hartmann et al., 2018 ). Despite these achievements, such unimodal strategies overlook the full spectrum of ASD biomarkers. Integrating multimodal data&#8212;combining neuroimaging, electrophysiology, genetic variants, and behavioral assessments&#8212;can exploit complementary information and boost diagnostic accuracy. Recent reviews confirm that attention-based fusion of fMRI and EEG consistently outperforms single-modality models ( Dcouto and Pradeepkandhasamy, 2024 ). Large public resources, including ABIDE (&#8776;2,200 subjects across 17 sites), NDAR (&#8776;1,100 high-density EEG recordings paired with behavioral scales), and SSC (&#8776;2,600 simplex families with whole-exome sequencing and ADOS/ADI-R measures), provide rich multimodal datasets but face challenges of limited cohort sizes, inter-site variability, and stringent privacy constraints ( Di Martino et al., 2017 ; Payakachat et al., 2016 ; Levy et al., 2011 ). To address data scarcity and privacy concerns, differentially private generative models have been proposed. DP-CGAN introduced per-sample gradient clipping and R&#233;nyi differential privacy accounting to limit privacy leakage while generating synthetic tabular medical records ( Torkzadehmahani et al., 2019 ), and DP-CTGAN extended this approach to a federated setting by conditioning on feature subsets ( Fang et al., 2022 ). More recently, GARL combined InfoGAN with deep Q-learning to iteratively refine synthetic neuroimaging samples, reporting significant classification gains on ABIDE data ( Zhou et al., 2024a ). However, these approaches typically target a single modality and do not enforce consistency across modalities, limiting their utility for downstream multimodal systems. On the predictive front, ensemble learning offers a framework for integrating heterogeneous feature representations. Static ensembles&#8212;such as simple averaging or majority voting&#8212;provide modest gains but fail to adapt weights based on sample-specific modality relevance. Mixture-of-experts architectures, featuring learnable gating networks that dynamically weight model outputs, have shown success in other domains; however, their application to privacy-preserving, multimodal ASD data remains largely unexplored. In this study, AutismSynthGen, an end-to-end framework that addresses multimodal data scarcity and privacy while delivering robust ASD prediction, is proposed. The key contributions are as follows: Multimodal Data Synthesis (MADSN) : A conditional GAN with transformer-based encoders (6 layers, eight heads, hidden size 512) and cross-modal attention to jointly model structural MRI, EEG time series, behavioral feature vectors, and calibrated severity scores. Rigorous differential privacy (DP-SGD with clipping norm 1.0 and noise multiplier 1.2) guarantees &#949; &#8239;&#8804;&#8239;1.0 at &#948; &#8239;=&#8239;10 &#8722;5 . Adaptive Ensemble Learning (AMEL) : A mixture-of-experts classifier integrating five heterogeneous models&#8212;a 3D-CNN, a 1D-CNN, an MLP, a cross-modal transformer, and a graph neural network&#8212;whose logits are adaptively weighted by a two-layer gating MLP (hidden 128, ReLU) with entropy regularization ( &#955; &#8239;=&#8239;0.01). Comprehensive Evaluation : Demonstration on ABIDE, NDAR, and SSC datasets, where MADSN-augmented training raises the validation AUC by &#8805; 0.04 over strong uni- and multimodal baselines. Statistical and Privacy Analysis : Conducted extensive ablations on cross-modal consistency and DP parameters, as well as bootstrap confidence intervals and paired Wilcoxon tests, to confirm both the efficacy and stability of AutismSynthGen under &#949; &#8239;&#8804;&#8239;1.0 privacy constraints. By unifying transformer-driven multimodal synthesis, formal privacy guarantees, and adaptive ensemble prediction, AutismSynthGen advances the state of the art in reliable, privacy-compliant ASD detection. 2 Related research 2.1 Unimodal MRI-based ASD detection Structural and functional MRI have been extensively studied using deep learning classifiers. Early CNN-based pipelines applied to ABIDE data ( Di Martino et al., 2017 ) achieved promising results: Moridian et al. reported up to 78% accuracy but highlighted sensitivity to inter-site variability and limited cohort sizes ( Moridian et al., 2022 ), while ASD-DiagNet combined a convolutional autoencoder and perceptual loss to reach &#8776; 80% accuracy on fMRI scans, albeit with coarse anatomical synthesis ( Eslami et al., 2019 ). Subsequent research has addressed generalization and richer feature extraction: Liu et al. surveyed advanced neuroimaging models, concluding that hybrid 3D-CNN and attention mechanisms yield stronger embeddings ( Liu et al., 2021 ); Heinsfeld et al. (2018) demonstrated end-to-end deep models with site-adaptation layers to improve cross-validation performance; Singh et al. (2023) introduced transfer learning across ABIDE splits to mitigate dataset bias; and Okada et al. (2025) employed RNN-attention networks on volumetric MRI, capturing sequential spatial patterns. Multi-view frameworks, such as MultiView, have further fused different MRI contrasts to enhance detection robustness ( Song et al., 2024 ). Additionally, adversarial domain adaptation has been utilized to align feature distributions across sites ( Gupta et al., 2025 ). More recently, self-supervised pretraining on resting-state fMRI has been shown to improve downstream ASD classification ( Zhou et al., 2024a ). 2.2 Unimodal EEG and behavioral models High-density EEG offers complementary temporal biomarkers. EEG-GAN pioneered GAN-driven EEG augmentation, improving downstream classification in BCI contexts, although it has not yet been applied to ASD ( Hartmann et al., 2018 ). Aslam et al. reviewed multi-channel EEG feature engineering for ASD, advocating spectral and connectivity features ( Aslam et al., 2022 ). Behavioral assessments&#8212;standardized scales for social communication and repetitive behaviors&#8212;have also been modeled directly. Rubio-Mart&#237;n et al. combined SVM, random forests, and an MLP on clinical vectors, achieving an AUC of approximately 0.75 on NDAR behavioral data ( Rubio-Mart&#237;n et al., 2024 ). Gamified assessment data, processed via signal-processing pipelines and ML classifiers, further underscored the utility of interactive behavioral measures ( Bernabeu, 2022 ; Borodin et al., 2021 ). 2.3 Genetic and clinical score-based approaches Genomic studies on simplex families have largely focused on risk-locus discovery rather than classification ( Li et al., 2024 ). Levy et al. (2011) analyzed de novo and transmitted CNVs in SSC data to identify ASD-associated variants. Automated pipelines have since applied shallow architectures to SNP embeddings, yet without integrating clinical scales. Avasthi et al. (2025) utilized transformer-based NLP to extract clinical text for ASD indicators, and graph convolutional networks have been leveraged to model correlations among behavioral domains ( Washington et al., 2022 ). Joint classification and severity prediction via multi-task learning have also been explored ( Wang et al., 2017 ). 2.4 Privacy-preserving generative models Differential privacy (DP) has been integrated into GANs for the synthesis of sensitive medical data. DP-CGAN enforced per-sample clipping and R&#233;nyi DP accounting ( &#949; &#8239;&#8804;&#8239;1.0) on tabular EHRs ( Torkzadehmahani et al., 2019 ), while DP-CTGAN extended conditional GANs to federated settings, balancing utility and privacy for mixed datasets ( Fang et al., 2022 ). Zhang et al. (2021) introduced a DP-federated GAN for continuous medical imaging features, and Wang et al. (2024) applied DP-SGM to neuroimaging data (DP-SNM), achieving strong privacy with minimal quality loss. The GARL framework combined InfoGAN with deep Q-learning to iteratively refine MRI synthesis under privacy constraints, although it was limited to imaging alone ( Zhou et al., 2024a ). Broader surveys of privacy-utility trade-offs in medical GANs have mapped parameter impacts on sample fidelity and privacy leakage ( Viswalingam and Kumar, 2025 ; Nanayakkara et al., 2022 ). 2.5 Multimodal fusion techniques / privacy-preserving frameworks Attention-based fusion of heterogeneous modalities has demonstrated superior performance compared to unimodal baselines. Dcouto and Pradeepkandhasamy (2024) surveyed recent multimodal deep learning in ASD, highlighting gains from fMRI&#8211;EEG attention fusion but noting a lack of end-to-end models with formal consistency constraints. Baltru&#353;aitis et al. (2018) provided a taxonomy of early, late, and hybrid fusion strategies, identifying cross-modal transformers as particularly promising for capturing intermodal correlations. Tools such as MultiView have operationalized early fusion in autism research ( Song et al., 2024 ); federated multimodal learning has been proposed to preserve privacy across sites ( Lakhan et al., 2023 ), and contrastive self-supervised methods have been introduced for joint embedding of multimodal ASD data ( Qu et al., 2025 ; Vimbi et al., 2025 ). Recent advances also integrate explainable federated learning for ASD prediction, combining privacy preservation with interpretability ( Alshammari et al., 2024 ). Such approaches align with our emphasis on privacy and transparency, although they do not generate synthetic data or enforce cross-modal consistency as in AutismSynthGen. 2.6 Ensemble and mixture-of-experts methods Adaptive ensemble strategies offer robustness by weighting diverse experts per sample. Sparsely gated mixture-of-experts (MoE) layers have demonstrated scalable adaptive weighting in language models ( Shazeer et al., 2017 ); in medical contexts, ensemble deep learning has been applied to multimodal ASD screening, yielding improved sensitivity but without sample-specific gating ( Taiyeb Khosroshahi et al., 2025 ). Rubio-Mart&#237;n et al. (2024) demonstrated the benefits of simple averaging of heterogeneous classifiers on behavioral data, while Nguyen et al. (2023) proposed MoE with gating regularization for noisy medical inputs. Recent studies have applied attention-based MoE to healthcare data, underscoring the importance of entropy penalties in avoiding expert collapse ( Han et al., 2024 ). 2.7 Privacy-utility trade-off analyses Comprehensive investigations into privacy-utility trade-offs have quantified the impact of DP parameters on the performance of generative models ( Schielen et al., 2024 ). Nanayakkara et al. evaluated differentially private GANs across imaging benchmarks, mapping &#949; values to downstream classification accuracy ( Nanayakkara et al., 2022 ). Table 1 compares the existing ASD detection frameworks. Table 1 Comparison of existing ASD detection frameworks: key methodologies, datasets employed, principal advantages, and noted limitations. S. no Ref. no Proposed research Dataset used Pros Cons 1 Moridian et al. (2022) CNN-based ASD detection ABIDE (structural &amp; fMRI) End-to-end feature learning Sensitive to site variability; limited sample size 2 Eslami et al. (2019) ASD DiagNet (autoencoder + GAN augmentation) ABIDE (fMRI) Perceptual loss improves feature quality Coarse anatomical detail in synthesized images 3 Hartmann et al. (2018) EEG-GAN for EEG synthesis Public EEG benchmarks Realistic EEG generation Not evaluated for ASD 4 Rubio-Mart&#237;n et al. (2024) Behavioral + NLP fusion (MLP, SVM, RF) NDAR (behavioral scales, text) Integrates textual and numerical clinical data No multimodal interaction 5 Levy et al. (2011) CNV risk-locus analysis SSC (de novo CNVs, WES) Identification of ASD-associated variants No predictive classification 6 Torkzadehmahani et al. (2019) DP-CGAN for tabular medical data Medical EHR cohorts Strong privacy guarantees (&#949;&#8239;&#8804;&#8239;1.0) Reduced sample realism; tabular only 7 Fang et al. (2022) DP-CTGAN (federated) MIMIC-III (tabular) Federated DP; improved utility over DP-CGAN Discrete features only 8 Zhou et al. (2024a) GARL (InfoGAN + DQN) ABIDE (MRI) Iterative refinement yields high-fidelity MRI samples Single modality; no EEG/behavioral consistency 9 Dcouto and Pradeepkandhasamy (2024) Attention-based fMRI + EEG fusion review Multiple studies Demonstrates the benefits of hybrid fusion Lacks an end-to-end model and privacy guarantees 10 Baltru&#353;aitis et al. (2018) Multimodal ML survey &amp; taxonomy N/A Comprehensive fusion taxonomy No empirical ASD implementation 11 Shazeer et al. (2017) Sparsely-gated Mixture-of-Experts (MoE) Language corpora Scalable adaptive weighting via learnable gating High compute; not tailored to medical or multimodal data 12 Zhang et al. (2021) FedDPGAN for medical imaging COVID-19 CT scans Federated DP for imaging Not applied to ASD 13 Wang et al. (2017) DP-SNM for neuroimaging Private neuroimaging cohorts DP for continuous imaging Single modality; no fusion 14 Han et al. (2024) FuseMoE: MoE Transformers for Fusion Multimodal benchmarks Flexible cross-modal fusion No formal privacy guarantees 15 Nanayakkara et al. (2022) Privacy-utility trade-off visualization Synthetic benchmarks Maps the DP impact on utility comprehensively No ASD-specific evaluation 2.8 Research gap Despite substantial advances in unimodal deep learning for ASD detection&#8212;such as CNN-based classifiers on fMRI ( Moridian et al., 2022 ; Eslami et al., 2019 ), hybrid autoencoder&#8211;GAN models ( Eslami et al., 2019 ), and GAN-driven EEG augmentation ( Hartmann et al., 2018 )&#8212;these approaches remain confined to single modalities and often overfit small, heterogeneous cohorts. Differentially private GANs have been applied to tabular medical records ( Torkzadehmahani et al., 2019 ) and federated settings ( Fang et al., 2022 ; Wang et al., 2024 ), but they neither extend to continuous neuroimaging or time-series data nor enforce consistency across EEG, behavioral, and imaging modalities. Although attention-based fusion methods demonstrate improved performance for paired fMRI&#8211;EEG inputs ( Dcouto and Pradeepkandhasamy, 2024 ; Zhou et al., 2024b ) and surveys outline promising multimodal fusion taxonomies ( Baltru&#353;aitis et al., 2018 ), end-to-end architectures that jointly synthesize and integrate more than two modalities under formal privacy constraints are still lacking. Finally, ensemble strategies in ASD classification have largely relied on static averaging of expert outputs ( Rubio-Mart&#237;n et al., 2024 ), whereas scalable, sample-adaptive mixture-of-experts frameworks that have proven effective in other domains ( Shazeer et al., 2017 ) remain unexplored in this context. The proposed framework addresses these gaps through two key innovations. First, a transformer-based conditional GAN incorporates cross-modal attention to generate coherent synthetic MRI, EEG, behavioral, and severity data, while differential privacy via DP-SGD (clipping norm 1.0, noise multiplier 1.2) guarantees &#949;&#8239;&#8804;&#8239;1.0 leakage bounds ( Fang et al., 2022 ; Torkzadehmahani et al., 2019 ). Second, a mixture-of-experts ensemble employs five heterogeneous models&#8212;3D-CNN, 1D-CNN, MLP, cross-modal transformer, and GNN&#8212;whose logits are dynamically weighted by an entropy-regularized gating network, enabling sample-specific emphasis on the most informative modalities ( Shazeer et al., 2017 ; Han et al., 2024 ). Rigorous evaluation on ABIDE ( Di Martino et al., 2017 ), NDAR ( Payakachat et al., 2016 ), and SSC ( Levy et al., 2011 ) demonstrates statistically significant AUC improvements (&#8805; 0.04) over strong unimodal, static ensemble, and non-private baselines, thus bridging the identified research gaps in privacy-compliant multimodal synthesis and adaptive ASD prediction. 3 Proposed methodology The AutismSynthGen framework jointly learns to synthesize multimodal autism data and to analyze it via an ensemble of predictive models. In our approach, a Multimodal Autism Data Synthesis Network (MADSN) uses transformer-based encoders and a conditional GAN to generate realistic multimodal data (e.g., neuroimaging, demographic vectors, behavioral). A complementary Adaptive Multimodal Ensemble Learning (AMEL) module trains a mixture-of-experts classifier on the synthesized (and real) data, assigning weights to each expert based on its performance and modality. This combined pipeline enables robust autism prediction and data augmentation while incorporating cross-modal consistency and differential privacy constraints for sensitive data. The overall flow is illustrated in Figure 1 . Figure 1 Overall AutismSynthGen architectural workflow. Diagram illustrating a method for autism prediction using a multimodal data synthesis network and adaptive ensemble learning. EEG data, autism severity, and behavioral data are encoded by transformer-based encoders. Real data combined with synthetic data generated by a conditional GAN feeds into a mixture-of-experts system including four experts. Weighted outputs assist in autism prediction. Metrics computed include accuracy, precision, F1, and AUC, with results visualized. 3.1 Dataset description The model is trained and validated on three publicly available datasets: ABIDE (Autism Brain Imaging Data Exchange) : A multi-site neuroimaging dataset. ABIDE-I/II together include structural MRI (T1-weighted), resting-state functional MRI, and diffusion MRI from hundreds of ASD individuals and controls. Phenotypic assessments (age, IQ, diagnosis) accompany the imaging ( Di Martino et al., 2017 ). NDAR (National Database for Autism Research) : Aggregates multimodal data, including behavioral assessments and EEG ( Payakachat et al., 2016 ). SSC (SimonsSimplex Collection) : Includes genetic and behavioral data from families with autistic children ( Levy et al., 2011 ). First, sourced neuroimaging data from ABIDE I and II, comprising 2,200 subjects (ASD and neurotypical controls) across 17 sites. Second, incorporated 1,100 high-density EEG recordings from the National Database for Autism Research (NDAR), sampled at 250&#8239;Hz alongside standardized behavioral assessments. Third, we included genetic and behavioral data for 2,600 simplex families from the Simons Simplex Collection (SSC), with whole-exome sequencing variants paired with ADOS/ADI-R measures. All data were split into train/validation/test sets in a 70/15/15% ratio, stratified by diagnosis, age, and site to preserve class balance. Experiments were repeated with three distinct random seeds (42, 123, 2025), and results are reported as the mean &#177; SD. It is important to note that evaluation was performed on stratified splits within ABIDE, NDAR, and SSC. No completely external dataset was available for validation. Hence, generalizability beyond these datasets remains to be established. The dataset details are mentioned in Appendix A . 3.2 Data preprocessing Raw magnetic resonance images underwent skull-stripping, affine registration to MNI space, and voxel-wise intensity normalization to zero mean and unit variance. EEG signals were band-pass filtered between 1 and 40&#8239;Hz, notched at 50&#8239;Hz, and epochs exceeding &#177;100&#8239;&#956;V were rejected; remaining segments were z-score normalized on an epoch-wise basis. Continuous features across modalities were imputed to their mean values, while categorical features employed one-hot encoding augmented by an explicit &#8220;unknown&#8221; flag. All continuous features (e.g., voxel intensities, age, and genomic variant counts) are normalized to have a mean of zero and a variance of one to stabilize training. For a feature x i , we compute as in Equation 1 : x i &#8242; = x i &#8722; &#956; &#963; (1) where &#956; and &#963; are the training set&#8217;s mean and standard deviation, respectively. This z-score normalization ensures each feature is on a comparable scale. Categorical variables (e.g., gender, site, diagnostic codes) are transformed into one-hot encoded vectors. For a categorical feature with K classes, a sample c &#8712; { 1.. K } , is mapped to a binary vector h &#8712; { 0 , 1 } K such that h j = 1 if and only if c = j . Missing values&#8212;common in multi-site clinical datasets&#8212;are imputed using simple statistical approaches. For numerical features, missing entries are replaced with the mean value &#956; x computed from the observed data as represented in Equation 2 : x i ^ = { x i , if x i is observed , &#956; x , if x i is missing (2) For categorical variables, an additional &#8220;unknown&#8221; category is added to handle missing values. More advanced methods (e.g., k-NN imputation or model-based approaches) are available but are not used here for simplicity and consistency. All preprocessing parameters ( &#956; , &#963; , and encoding schemes) are learned from the training data and consistently applied to the validation, test, and synthetic datasets. Not all subjects had complete multimodal data. Missing features were imputed using mean (continuous) or &#8216;unknown&#8217; category (categorical) values. While pragmatic, this may bias results and motivate the use of advanced missing-modality learning in the future. Behavioral narrative text fields from NDAR/SSC were anonymized, tokenized, and embedded using a pre-trained biomedical language model (BioBERT). The resulting 768-dimensional embeddings were reduced to 128 dimensions using PCA and used as input to MADSN. Synthetic text vectors (&#8220;text_projected&#8221;) generated by MADSN thus represent latent embeddings of behavioral descriptions rather than raw text. 3.3 MADSN architecture Our Multimodal Autism Data Synthesis Network (MADSN) generates coherent synthetic triplets ( x &#732; MRI , x &#732; EEG , x &#732; SNP ) by fusing transformer-based embeddings and enforcing cross-modal consistency. Each modality is first encoded via a six-layer transformer (eight heads, hidden size 512), using positional encodings for EEG and learned embeddings for genetic variants and imaging patches. These modality-specific outputs interact with one another through cross-modal attention, producing fused embeddings that are concatenated and projected into a 256-dimensional latent input for the generator. The generator G ( z , y ) is implemented as a four-layer MLP with LeakyReLU activations, while the discriminator D ( x , y ) features a shared three-layer MLP trunk branching into modality-specific heads. Training follows a conditional GAN paradigm augmented with three loss components: standard adversarial loss E [ log ( D ( x ) ] + E [ log ( 1 &#8722; D ( G ( z ) ) ) ] , a cross-modal KL-divergence penalty to encourage consistency of joint posteriors, and a privacy penalty implemented via DP-SGD on the discriminator. We set a clipping norm C = 1.0 and a noise multiplier &#963; = 1.2 to achieve &#949; &#8804; 1.0 at &#948; = 10 &#8722; 5 , ensuring rigorous differential privacy guarantees without sacrificing data utility. Figure 2 illustrates the architecture of the proposed Multimodal Autism Data Synthesis Network (MADSN). Each input modality x m (e.g., EEG, behavioral text, demographic vectors) is first processed through a modality-specific transformer encoder T m to produce a latent representation h m ( Equation 3 ): h m = T m ( x m ) (3) Figure 2 MADSN Architecture. Diagram of the MADSN Architecture illustrating the flow of data through various attention layers. It starts with a Transformer Encoder using self-attention for EEG, behavioral text, and demographic vectors. Cross-modal and shared attention leads to a unified attention layer. Inputs are fed into a generator, producing synthetic samples. These samples are evaluated by a discriminator to determine the probability of being real, using x_gen and x_synth. The architecture combines synthetic and shared attention for concentrated modality embeddings. Each transformer encoder includes self-attention layers, particularly multi-head attention computed as in Equation 4 : Attention ( Q , K , V ) = softmax ( Q K T d k ) V , (4) where Q , K , V are query, key, and value projections of h m , d k , and is the dimensionality of the key vectors. Positional encodings are added as necessary to maintain spatial or temporal relationships. Latent features h m from all modalities are then fused via cross-modal attention. For modalities i , j , attention weights are computed as in Equation 5 : a ij = softmax ( ( W q h i ) ( W k h j ) T d ) ( W v h j ) (5) All modality embeddings are concatenated and processed through shared attention layers to yield a unified latent vector z , encoding multimodal context. The generator G of the conditional GAN receives z , random noise &#951; ~ N ( 0 , I ) , and class label c , and produces synthetic multimodal samples ( Equation 6 ): x gen = G ( z , &#951; , c ) (6) which outputs synthetic samples for each modality (stacked or separately). The discriminator D evaluates real or generated data conditioned on c and outputs a probability of being real. The GAN training minimizes the following adversarial objective ( Equation 7 ): min G max D V ( D , G ) = E x ~ p data [ log D ( x , c ) ] + &#8239; E &#951; , c [ log ( ( b ) &#8722; D ( G ( z ( &#951; ) , c ) , c ) ) ] , (7) where &#951; = T 1 x 1 , .. , T m x m is fixed per real sample for training purposes. Training alternates between minimizing the discriminator loss as shown in Equation 8 : L D = &#8722; [ log D ( x real , c ) + log ( 1 &#8722; D ( x gen , c ) ) ] (8) and minimizing the generator loss with a cross-modal consistency penalty ( Equation 9 ): L G = &#8722; log ( D ( x gen , c ) + &#955; cons L cons (9) Cross-modal consistency is enforced by ensuring that different modality embeddings agree in latent space as in Equation 10 : L cons = &#8721; i &#8800; j &#8214; h i &#8722; h j &#8214; 2 (10) Finally, for privacy, we incorporate Differential Privacy (DP) into GAN training. Differential Privacy (DP) is incorporated into discriminator training using DP-SGD. A mechanism M is &#1013; -differentially private if changing one individual in the dataset changes output probabilities by at most e &#8712; ( Equation 11 ): P r [ M ( D ) &#8712; S ] &#8804; e &#8712; P r [ M ( D &#8242; ) &#8712; S ] &#8704; S , &#8704; D , D &#8242; : &#8214; D &#8722; D &#8242; &#8214; 1 = 1 (11) Concretely, the discriminator gradients are clipped to norm c , and Gaussian noise is added for a mini-batch of size B as mentioned in Equation 12 . g &#175; = 1 B &#8721; i = 1 B g i max ( 1 , &#8214; g i &#8214; C ) + &#951; ( 0 , &#963; 2 C 2 I ) , (12) where g i is the gradient from sample i . The MADSN generator is trained to minimize ( Equation 13 ): L G + &#955; cons L cons (13) while discriminator training is made private. By combining transformers, cross-modal attention, GAN objectives, and DP constraints, MADSN learns to produce realistic, privacy-preserving synthetic multimodal autism data. 3.4 AMEL ensemble learning The Adaptive Multimodal Ensemble Learning (AMEL) system takes the augmented dataset (real + synthetic) and trains an ensemble of K expert classifiers, along with a gating network. The Adaptive Multimodal Ensemble Learning (AMEL) module integrates five experts&#8212;CNN, MLP, regressor, transformer, and GNN&#8212;via a gating network. Each expert processes modality-specific inputs; the gating network assigns adaptive weights to expert outputs, enabling sample-specific fusion. This ensures that if one modality is weak or missing, other experts dominate the prediction. Each expert produces logits, which are concatenated and passed through a two-layer gating MLP (hidden size 128, ReLU) to yield softmax weights w i , regularized by an entropy penalty ( &#955; &#8239;=&#8239;0.01) to prevent collapse. The ensemble prediction y ^ = &#8721; i = 1 5 w i f i ( x ) is trained end-to-end under a cross-entropy loss on held-out labels. Figure 3 represents the schematic of the AMEL adaptive ensemble. Each expert f k may be specialized to one modality (e.g., f MRI for imaging, f GEN for genetics, and so on), or to different architectures (CNN, MLP, etc.). Given an input x with all modalities, each expert outputs a prediction y k = f k ( x ) . A gating network g ( x ) produces scores that are normalized via softmax to obtain weights as mentioned in Equation 14 : a k = exp ( g k ( x ) ) &#8721; j = 1 K exp ( g j ( x ) ) , &#8721; k = 1 K a k = 1 (14) Figure 3 AMEL adaptive ensemble architectural workflow. Diagram illustrating an ensemble learning model called AMEL. It shows an augmented dataset input to a gating network and multiple experts labeled from one to k. Each expert outputs a function y equals f of x. The outputs are combined using a summation and softmax operation, resulting in an output y tilde of x. These weights adapt to each sample: e.g., if imaging data is missing or noisy, the model may down-weight the imaging expert. The ensemble prediction is the weighted sum ( Equation 15 ): y ( x ) ^ = &#8721; k = 1 K &#945; k y k (15) The entire system is trained end-to-end by minimizing an ensemble loss: a supervised loss and regularization. Formally, L cns = E ( x , y ) [ l ( y , y ( x ) ^ ) ] + &#8721; k = 1 K &#955; k &#8214; &#952; k &#8214; 2 (16) where &#952; k are parameters of f k , and &#955; k can encode modality-specific priors ( Equation 16 ). We backpropagate through the gating softmax so that better-performing experts get higher weights. This &#8220;mixture-of-experts&#8221; approach allows the ensemble to adaptively integrate modalities, as opposed to static averaging or majority voting. Indeed, adaptive ensemble algorithms (with learned weights) typically outperform fixed-weight ensembles. Overfitting was mitigated through dropout layers ( p &#8239;=&#8239;0.3 in the MADSN generator, p &#8239;=&#8239;0.5 in the AMEL gating), entropy regularization (&#955;&#8239;=&#8239;0.01), and early stopping based on validation AUC. Synthetic samples were generated exclusively from training distributions, ensuring no leakage into validation or test sets. During inference, if a modality is missing or corrupted, its expert output is excluded, and the gating network automatically redistributes weights among the remaining experts. This adaptive weighting allows AMEL to degrade gracefully rather than fail catastrophically in incomplete-modality settings. The outline for the MADSN and AMEL components, as well as their integration, is detailed in Algorithms 1 , 2 . ALGORITHM 1 MADSN multimodal synthesis. Steps for training a generator to produce synthetic data using transformer encoders, a generator, and a discriminator. It involves preprocessing data, updating a discriminator with differential privacy, sampling noise, computing loss, and updating parameters through gradient descent. After training, synthetic data can be generated. ALGORITHM 2 AMEL training and inference. Text detailing a machine learning algorithm. Steps include initializing experts, computing expert outputs, gating scores, ensemble outputs, and loss, with backpropagation to update models. Inference uses new data to compute and weight expert outputs for final prediction. 3.5 Hyperparameter optimization and baselines Model hyperparameters were optimized using a Tree-structured Parzen Estimator (TPE) over learning rates for the GAN (10 &#8722; 5&#8211;10 &#8722;3 1), DP-SGD clipping norm (0.1&#8211;2.0), noise multiplier (0.5&#8211;2.0), number of experts K &#8712; { 3 , 5 , 7 } , and gating penalty &#955; &#8712; [ 0 , 0.1 ] . Validation AUC guided early stopping up to 200 epochs, with performance recorded every epoch. We benchmarked our model against several baselines: a single-modality CNN (MRI only), a GAN without the consistency penalty, a GAN trained with standard SGD (without DP), and an ensemble without gating. Our full pipeline achieved a validation AUC of 0.89&#8239;&#177;&#8239;0.01, outperforming all baselines by at least 0.04. 3.6 Statistical and computational considerations The model&#8217;s performance is evaluated using AUC, F1, maximum-mean discrepancy (MMD) on embeddings, and Kolmogorov&#8211;Smirnov statistics on marginal distributions, with 95% confidence intervals estimated from 1,000 bootstrap resamples. Paired Wilcoxon signed-rank tests were used to assess significance ( p &#8239;&lt;&#8239;0.05) against each baseline. Experiments were run on four NVIDIA A100 GPUs (256&#8239;GB RAM), with GAN training requiring ~48&#8239;h and ensemble fine-tuning requiring ~12&#8239;h. The GAN and ensemble models contain approximately 12&#8239;M and 8 M parameters, respectively. Training required ~48&#8239;h on four A100 GPUs, which may limit reproducibility in smaller labs. Future studies will explore model compression (e.g., distillation, ONNX export) and federated setups to reduce computational cost. 4 Results and discussion The proposed research introduces AutismSynthGen, a novel generative model designed to synthesize multimodal autism-related data, including behavioral texts, electroencephalogram (EEG) signals, and demographic profiles, to address the challenge of limited datasets in autism prediction research. AutismSynthGen leverages the Multimodal Autism Data Synthesis Network (MADSN), a generative adversarial network (GAN) integrated with a transformer-based multimodal fusion module, which encodes modality-specific inputs using transformers, fuses them into a shared latent space via attention-based mechanisms, and employs a conditional GAN to generate clinically relevant synthetic samples conditioned on autism severity levels (mild, moderate, severe). A privacy-preserving loss function, incorporating differential privacy ( &#949; &#8239;&#8804;&#8239;1.0), ensures the protection of sensitive patient information, while a cross-modal consistency regularizer maintains coherence across modalities, aligning EEG patterns with behavioral descriptions and demographic data. The accuracy of the synthetic dataset is validated using multiple machine learning algorithms, including Random Forest, Support Vector Machine (SVM), Convolutional Neural Network (CNN), and Logistic Regression, with the proposed Adaptive Multimodal Ensemble Learning (AMEL) algorithm employed for training. AMEL integrates a weighted ensemble of these base learners, utilizing adaptive weighting and modality-specific regularization to optimize prediction performance, thereby enhancing the effectiveness of the synthetic data for autism classification tasks. The novelty of this approach lies in the combination of MADSN&#8217;s generative capabilities with AMEL&#8217;s adaptive ensemble strategy, addressing data scarcity and privacy concerns while outperforming traditional methods. 4.1 Dataset description The development and evaluation of AutismSynthGen utilize three well-established, publicly accessible datasets, each providing critical multimodal data for autism research: ABIDE (Autism Brain Imaging Data Exchange) : This dataset includes EEG, functional magnetic resonance imaging (fMRI), and demographic data (e.g., age, gender) from individuals with autism spectrum disorder (ASD) and typically developing controls. It is widely used for studying brain connectivity and autism-related biomarkers. Access to ABIDE is publicly available but requires registration through the official ABIDE portal. NDAR (National Database for Autism Research) : NDAR provides a comprehensive repository of autism-related data, including behavioral assessments, EEG recordings, and clinical information. It supports integrative analyses across genetic, neuroimaging, and behavioral domains. Access to NDAR requires a data use agreement, which can be obtained through the NDAR platform. Simons Simplex Collection (SSC) : This dataset, provided through SFARI Base, contains behavioral data, clinical assessments, and demographic profiles from families with one child diagnosed with autism spectrum disorder (ASD). SSC is particularly valuable for studying familial and behavioral patterns in autism spectrum disorder (ASD). Access is available through an application on the SFARI Base platform. These datasets collectively provide a robust foundation for training and validating AutismSynthGen, ensuring that the generated synthetic data accurately reflects the realistic, multimodal characteristics of autism while adhering to ethical and privacy standards. Figure 4 shows a sample of the raw dataset customized from multimodal data, illustrating key features such as autism severity scores (A1-Score to A8-Score), demographic information (e.g., country, age, relationship), and behavioral/EEG indicators (e.g., EEG_signal, behavioral_text). The dataset includes five anonymized patient records, with columns representing various attributes used for training the AutismSynthGen model. Figure 4 Raw multimodal dataset illustrating key features such as autism severity scores (A1-Score to A8-Score), demographic information (e.g., country, age, relationship), and behavioral/EEG indicators. A table displays data including scores and attributes such as jurisdiction, autism status, country of residence, app usage history, result, age description, relation, ASD classification, EEG signals, and behavioral text. The scores, ranging from A1 to A10, show binary values. Entries include countries like the United States, Brazil, Spain, and Egypt, with varying results and descriptions. Behavioral text excerpts discuss patient states and profiles. Figure 5 represents the sample of the pre-processed dataset derived from the raw multimodal data, following the application of data pre-processing techniques. The preprocessing steps include handling missing values by appropriate imputation or removal, encoding categorical variables (e.g., country, relationship) into numerical representations, and normalizing numerical features (e.g., age, severity scores) to ensure consistency and compatibility with the AutismSynthGen model. The dataset retains five anonymized patient records, with refined attributes suitable for model training. Figure 5 Preprocessed multimodal dataset. A table displaying data with columns including A1_Score to A10_Score, along with attributes like country_of_res, used_app_before, result, and others. Each row contains individual scores and additional information such as EEG_signal and Behavioral_Text, which provides contextual or descriptive notes related to each entry. Figure 6 represents the graph depicting the discriminator accuracy of the MADSN model during training over 14 iterations. The results presented in Figure 6 demonstrate the training performance of the MADSN discriminator, a critical component of the AutismSynthGen model. The observed increase in discriminator accuracy from 0.40 to 0.65 across 14 iterations signifies robust learning and the model&#8217;s capacity to differentiate between synthetic and real multimodal autism data. The initial rise in accuracy, accompanied by minor fluctuations between iterations 4 and 6, suggests an adaptation phase where the generator and discriminator achieve equilibrium, a common phenomenon in GAN training. The stabilization and subsequent steady improvement post-iteration 6 underscore the efficacy of the transformer-based multimodal fusion and cross-modal consistency regularizer in enhancing data realism. The final accuracy of 0.65 indicates a strong discriminative capability, supporting the reliability of the synthetic data generated for augmenting limited autism datasets. Figure 6 Graph depicting the discriminator accuracy of the MADSN model during training over 14 iterations. The accuracy increases progressively from approximately 0.40 to 0.65, indicating effective learning and convergence of the generative adversarial network. Line graph depicting discriminator accuracy during training. The x-axis represents iterations from 0 to 15, and the y-axis shows accuracy ranging from 0.40 to 0.65. The accuracy steadily increases with some fluctuations, indicated by a blue line, starting around 0.41 and reaching approximately 0.65. Figure 7 represents the sample of the synthetic data generated by AutismSynthGen, stored in synthetic_data.npy format, showcasing projected text features (text_projected), EEG signals (eeg), and demographic labels (demo_labels) for five synthetic patient records. The results presented in Figure 7 illustrate the efficacy of AutismSynthGen in generating synthetic multimodal data, as evidenced by the sample of synthetic_data.npy. The projected text features, EEG signals, and demographic labels exhibit coherent patterns that align with the pre-processed dataset, confirming the success of the transformer-based multimodal fusion and cross-modal consistency regularizer in maintaining inter-modality relationships. The presence of binary labels (0 and 1) in the demo_labels column indicates the model&#8217;s capability to generate data conditioned on autism severity levels, a key objective of the MADSN framework. The observed variability in synthetic data attributes, such as the range of EEG values and text projections, suggests that the conditional GAN effectively captures the diversity of the original dataset while adhering to the privacy constraints imposed by differential privacy ( &#949; &#8239;&#8804;&#8239;1.0). This synthetic data augmentation is poised to enhance the training of autism prediction models, particularly in scenarios where real-world data is limited. The &#8216;text_projected&#8217; column represents generated behavioral text embeddings. These were evaluated for similarity against real embeddings using BLEU scores, confirming alignment at the representation level. These vectors were not decoded into sentences but integrated directly into AMEL for classification. Figure 7 Sample of synthetic multimodal data generated by AutismSynthGen, including text embeddings (&#8216;text_projected&#8217;), EEG signals, and demographic labels. A table with five rows and columns labeled 'text_projected', 'eeg', 'demo', and 'labels'. Each row contains arrays of numbers in the 'text_projected', 'eeg', and 'demo' columns. The 'labels' column contains binary values: 0 or 1. Figure 8 represents the comparison of distribution histograms for EEG values and age between real and synthetic data. The results presented in Figure 8 provide a comparative analysis of the distributions of EEG values and age between real and synthetic data, offering insights into the fidelity of AutismSynthGen&#8217;s output. The EEG distribution demonstrates a strong overlap between real and synthetic data, with both exhibiting a central peak around zero and a comparable spread, suggesting that the MADSN model effectively captures the statistical properties of EEG signals. This alignment validates the efficacy of the transformer-based multimodal fusion and cross-modal consistency regularizer in preserving the structural integrity of EEG patterns. Similarly, the age distribution shows a close match between real and synthetic data, with both histograms displaying similar normalized ranges (0 to 20) and peak densities, indicating the model&#8217;s success in replicating demographic attributes while adhering to the differential privacy constraint ( &#949; &#8239;&#8804;&#8239;1.0). Minor deviations in the tails of the distributions may reflect the impact of the privacy-preserving loss, which prioritizes data utility over exact replication. These findings affirm the synthetic data&#8217;s potential to augment limited real datasets, enhancing the robustness of autism prediction models. Figure 8 Comparison of distribution histograms for EEG values and age between real and synthetic data. Two histograms compare real and synthetic data distributions. The left chart shows EEG distributions with real data in blue and synthetic in orange. The right chart displays age distributions, similarly color-coded, indicating a narrow peak for synthetic age data compared to real age data. Both charts measure density on the Y-axis. To further validate fidelity, we projected real and synthetic embeddings into a 2D space using t-SNE ( Figure 9 ). Both EEG and behavioral embeddings show a strong overlap between real and generated samples, consistent with the low MMD and KS values. A complementary PCA projection of AMEL&#8217;s latent decision space ( Figure 10 ) shows that synthetic samples align closely with real data clusters, without forming spurious modes. These visualizations provide intuitive confirmation that AutismSynthGen captures the essential structure of multimodal ASD data. Figure 9 t-SNE visualization of latent embeddings (real&#8239;=&#8239;blue, synthetic&#8239;=&#8239;orange). Scatter plot titled \\\"t-SNE of Latent Embeddings: Real vs Synthetic\\\" comparing real and synthetic data points. Real data is marked in blue, while synthetic data is in orange. The axes are labeled t-SNE 1 and t-SNE 2. Data points appear scattered and intermixed, without distinct clustering. Figure 10 PCA projection of AMEL latent decision space with an illustrative decision boundary (real&#8239;=&#8239;blue, synthetic&#8239;=&#8239;orange). Scatter plot of PCA projection showing blue dots for real data and orange dots for synthetic data. A diagonal blue line represents the decision boundary. The axes are labeled PC1 and PC2. Figure 11 illustrates the Receiver Operating Characteristic (ROC) curves for the proposed AMEL algorithm, comparing its performance on real data (blue) and a combination of real and synthetic data (orange). The results presented in Figure 11 highlight the superior performance of the AMEL algorithm when trained on a combination of real and synthetic data generated by AutismSynthGen. Figure 11 ROC curves comparing AMEL on real data vs. real + synthetic data. Synthetic augmentation enhances near-ceiling performance, with an AUC of 1.0. ROC curve comparing Real Data with Real plus Synthetic data, displaying True Positive Rate against False Positive Rate. Blue line represents Real Data with AUC of 0.9800 and F1 score of 0.9900, while orange line represents Real plus Synthetic data with AUC and F1 score of 1.0000. Dashed diagonal line indicates no-skill classifier. The ROC curve for real data alone exhibits an AUC of 0.98 and an F1-score of 0.99. In contrast, the inclusion of synthetic data elevated the performance to near-perfect levels (AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00), indicating highly consistent internal discrimination. This improvement underscores the efficacy of the synthetic data in augmenting the real dataset, likely due to AMEL&#8217;s adaptive weighting and regularization, which effectively integrate multimodal features (text, EEG, demographics) enhanced by the MADSN&#8217;s generative process. The ideal performance on the augmented dataset may reflect an optimal training scenario, potentially influenced by the synthetic data&#8217;s alignment with real-world distributions (as shown in Figure 5 ). Figure 12 illustrates the confusion matrices for the AMEL algorithm, comparing its performance on real data (left) and a combination of real and synthetic data (right). The results presented in Figure 12 provide a detailed assessment of the AMEL algorithm&#8217;s performance through confusion matrices for real data and a combination of real plus synthetic data. For real data, the matrix reveals 450 true negatives, 50 false positives, 50 false negatives, and 154 true positives, yielding an overall accuracy of approximately 0.904 (calculated as (450&#8239;+&#8239;154) / (450&#8239;+&#8239;50&#8239;+&#8239;50&#8239;+&#8239;154)). In contrast, the inclusion of synthetic data improves the matrix to 480 true negatives, 20 false positives, 30 false negatives, and 174 true positives, resulting in an accuracy of approximately 0.946 (calculated as (480&#8239;+&#8239;174) / (480&#8239;+&#8239;20&#8239;+&#8239;30&#8239;+&#8239;174)). This enhancement, particularly the reduction in false positives and false negatives, underscores the synthetic data&#8217;s contribution to improving classification precision and recall, aligning with the perfect AUC and F1-score observed in Figure 11 . Figure 12 Confusion matrices for the proposed AMEL algorithm, comparing performance on real data (left) and a combination of real plus synthetic data (right). Two confusion matrices compare prediction results. Left matrix: \\\"Real Data\\\" - 450 true negatives, 50 false positives, 50 false negatives, 154 true positives. Right matrix: \\\"Real + Synthetic\\\" - 480 true negatives, 20 false positives, 30 false negatives, 174 true positives. The performance of the AMEL algorithm is evaluated using the following metrics: MMD (Fused) : 0.04, indicating a low Maximum Mean Discrepancy between real and synthetic fused multimodal data, suggesting high similarity. KS Statistic (EEG) : 0.03, with a KS p -value (EEG) of 0.06, indicating that the Kolmogorov&#8211;Smirnov test does not reject the null hypothesis of identical EEG distributions at a 5% significance level. Distributional Similarity (%) : 95, reflecting a high degree of alignment between real and synthetic data distributions. F1-Score (Real) : 0.99, and AUC (Real) : 0.98, demonstrating excellent classification performance on real data alone. F1-Score (Real + Synthetic) : 1.00, and AUC (Real + Synthetic) : 1.00, indicating perfect classification performance with the augmented dataset. F1 Improvement (%) : 1.0101, and AUC Improvement (%) : 2.0408, quantifying the relative enhancement in performance with synthetic data. BLEU Score : 0.7, signifying moderate to high similarity between real and synthetic text features. Privacy Budget ( &#949; ) : &#8804; 1.0, indicating no privacy budget expenditure, as the synthetic data generation adheres to differential privacy constraints. The evaluation metrics presented in Figure 13 affirm the efficacy of the AMEL algorithm in leveraging synthetic data generated by AutismSynthGen. The low MMD (0.04) and KS statistic (0.03) with a non-significant p-value (0.06) for EEG distributions, alongside a 95% distributional similarity, validate the model&#8217;s ability to replicate real data characteristics, consistent with the observations in Figure 8 . The F1-score improvement of 1.0101% and AUC improvement of 2.0408% when incorporating synthetic data, culminating in perfect scores (F1-score: 1.00, AUC: 1.00), corroborate the enhanced classification performance depicted in Figures 11 , 12 . The BLEU score of 0.7 further supports the quality of synthetic text features, while the zero privacy budget ( &#949; &#8239;&#8804;&#8239;1.0) confirms compliance with differential privacy, ensuring patient data protection. Figure 13 Evaluation metrics for the AMEL algorithm, including distributional similarity (MMD&#8239;=&#8239;0.04, KS&#8239;=&#8239;0.03, BLEU&#8239;=&#8239;0.7), classification performance (F1&#8239;=&#8239;0.99 real; 1.00 real + synthetic; AUC&#8239;=&#8239;0.98 real; 1.00 real + synthetic), and privacy compliance ( &#949; &#8239;&#8804;&#8239;1.0). Bar chart titled \\\"Evaluation Metrics\\\" displaying various metrics along the y-axis and values along the x-axis. Notable metrics include Privacy Budget, BLEU Score, AUC Improvement, F1 Improvement, and others. The Distributional Similarity metric shows a value of 95.0040, standing out significantly compared to others. While quantitative measures (MMD, KS, and BLEU) support fidelity, no clinician-based validation was conducted on synthetic behavioral text or EEG. Future research will involve blinded expert review to confirm clinical realism. 4.2 Performance comparison on real data The comparison results presented in Table 2 illustrate the performance of the proposed AMEL algorithm in comparison to baseline models on real data alone. The AMEL algorithm achieves an accuracy of 0.992908, an F1-score of 0.986301, a precision of 0.972973, a recall of 1.0, an AUC of 1.0, and a log loss of 0.049632, matching CNN&#8217;s performance and surpassing logistic regression (1.0, 1.0, 1.0, 1.0, 1.0, 0.0308908), Random Forest (0.978723, 0.957746, 0.971429, 0.944444, 0.998148, 0.145483), and SVM (0.985816, 0.971429, 1.0, 0.944444, 0.997354, 0.0684). The bar chart visually highlights AMEL&#8217;s competitive edge, particularly in log loss and F1 score, reflecting its effective integration of multimodal features through adaptive weighting and regularization (refer to Figure 14 ). While logistic regression exhibits perfect scores, its higher log loss suggests less confidence in predictions compared to AMEL and CNN. These results establish AMEL as a robust baseline for real data, setting the stage for its enhanced performance with synthetic data augmentation, as evidenced by the perfect scores in Figures 11 , 12 . The proposed baseline comparison focused on conventional models (CNN, SVM, RF, LR). Recent multimodal attention-based fusion architectures (refs) were excluded due to computational constraints; however, benchmarking against these remains a priority. Table 2 Comparison results of the proposed AMRL algorithm with other existing algorithms on real data. Model Accuracy F1-Score Precision Recall AUC Log loss Logistic Regression 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.0308908 Random Forest 0.978723 0.96 &#177; 0.02 0.971429 0.944444 0.998 &#177; 0.001 0.145483 SVM 0.985816 0.97 &#177; 0.01 1.0 0.944444 0.997 &#177; 0.002 0.0684 CNN 0.992908 0.98 &#177; 0.01 0.972973 1.0 1.0 &#177; 0.00 0.011869 Proposed AMEL 0.992908 0.99 &#177; 0.01 0.972973 1.0 1.0 &#177; 0.00 0.049632 Values are reported as mean &#177; SD over three independent runs with random seeds (42, 123, 2025). Bootstrap 95% confidence intervals were computed for AUC and F1 to confirm stability. Bold values represent the results of proposed methodology. Figure 14 Comparison results of the proposed AMRL algorithm with other algorithms. Bar chart comparing model performance metrics including accuracy, F1-score, precision, recall, AUC, and log loss across five models: Logistic Regression, Random Forest, SVM, CNN, and Proposed AMEL. All models show high metrics, with variations in log loss; Logistic Regression has the lowest at 0.031, and Random Forest the highest at 0.145. The comparison of confusion matrices for all models on real data is presented in Figure 15 . Subfigure (a) for logistic regression shows 480 true negatives, 20 false positives, 30 false negatives, and 470 true positives, indicating perfect accuracy. Subfigure (b) for Random Forest displays 465 true negatives, 35 false positives, 55 false negatives, and 445 true positives, indicating moderate misclassification rates. Subfigure (c) for SVM presents 470 true negatives, 30 false positives, 50 false negatives, and 450 true positives, showing slight improvement. Subfigure (d) for CNN exhibits 475 true negatives, 25 false positives, 40 false negatives, and 460 true positives, demonstrating high accuracy. Subfigure (e) for the proposed AMEL records 478 true negatives, 22 false positives, 38 false negatives, and 462 true positives, highlighting the lowest misclassification rates. Figure 15 Comparison of confusion matrices for all models on real data: (a) Logistic Regression, (b) Random Forest, (c) SVM, (d) CNN, and (e) the proposed AMEL. Five confusion matrices for different models: (a) Logistic Regression, perfect classification with 105 true negatives and 36 true positives. (b) Random Forest, one false positive and two false negatives. (c) SVM, similar to Random Forest. (d) CNN, one false positive, no false negatives. (e) Real Data, fifty false positives and negatives, 450 true negatives, 154 true positives. The ROC curves in Figure 16 highlight the discriminative performance of the models for autism prediction on real data. Logistic Regression and CNN exhibit perfect AUCs (1.0), consistent with their high accuracy, although Logistic Regression&#8217;s log loss (0.0308908) suggests potential overconfidence. Random Forest (AUC&#8239;=&#8239;0.998148) and SVM (AUC&#8239;=&#8239;0.997354) exhibit strong but slightly lower discrimination, which aligns with their moderate false negative rates. The proposed AMEL matches the perfect AUC of 1.0, reflecting its effective multimodal integration via adaptive weighting, supported by its F1 score (0.986301). Figure 16 ROC curve plot comparing the performance of all models on real data, with logistic regression (AUC&#8239;=&#8239;1.0), random forest (AUC&#8239;=&#8239;0.998148), SVM (AUC&#8239;=&#8239;0.997354), CNN (AUC&#8239;=&#8239;1.0), and the proposed AMEL (AUC&#8239;=&#8239;1.0), distinguished by legend entries, demonstrating their discriminative abilities. ROC curve comparing five models: Logistic Regression, Random Forest, SVM, CNN, and AMEL. The x-axis represents false positive rate, and the y-axis represents true positive rate. The diagonal line represents random performance. AMEL and SVM show close-to-perfect performance, nearly blending at the top-left corner. Figure 17 shows the accuracy and loss curves for the CNN and AMEL models, providing insights into their training dynamics. Both models converge to high accuracy (0.99&#8211;1.0), validating their effectiveness. However, CNN&#8217;s loss stabilizes at a lower value (around 0.01), indicating faster convergence and a better fit, while AMEL&#8217;s higher loss (around 0.05) suggests slower stabilization, likely due to its ensemble complexity. This aligns with AMEL&#8217;s log loss (0.049632) and supports its adaptive weighting strategy, which enhances the F1 score but requires optimization. Figure 17 Plot of accuracy and loss curves for CNN and AMEL over training epochs on real data. Four line graphs compare CNN and AMEL models. Top left shows CNN loss with training and validation loss decreasing similarly. Top right displays CNN accuracy with both training and validation accuracy increasing, nearing one point zero. Bottom left depicts AMEL loss with both loss values decreasing. Bottom right illustrates AMEL accuracy with training and validation accuracy increasing, approaching one point zero. Each graph has a legend differentiating between training (blue) and validation (orange) metrics. 4.2.1 Privacy&#8211;utility trade-off To evaluate the impact of varying the differential privacy budget, we trained MADSN under &#949; &#8712; {0.1, 0.5, 1.0, 2.0}. Figure 18 shows the resulting fidelity and classification metrics. As expected, stronger privacy (&#949;&#8239;=&#8239;0.1) significantly reduces utility, while relaxed privacy (&#949;&#8239;=&#8239;2.0) preserves utility but weakens guarantees. The intermediate setting &#949;&#8239;=&#8239;1.0 provided the best balance, consistent with our main experiments. Figure 18 Privacy&#8211;utility trade-off for AutismSynthGen. As &#949; decreases (resulting in stronger privacy), the classification AUC drops, while fidelity metrics (MMD, BLEU) worsen. At &#949;&#8239;=&#8239;1.0, the model achieves a balanced trade-off, consistent with the main results. Line graph titled \\\"Privacy-Utility Trade-off in AutismSynthGen\\\" showing three metrics against Privacy Budget values ranging from 0 to 2. The Classification AUC (blue line) increases, MMD (orange line) slightly decreases, and BLEU score (green line) rises. Each metric is plotted with distinct markers. The left y-axis measures Classification AUC, while the right y-axis measures MMD/BLEU. 4.2.2 Calibration analysis In addition to discrimination metrics such as AUC and F1, the calibration of AutismSynthGen predictions is evaluated. Calibration reflects how well predicted probabilities align with actual observed outcomes, which is particularly important in clinical decision-making, where overconfident or underconfident predictions can lead to misinformed decisions. Brier scores as a quantitative measure of calibration are reported. For AMEL trained on real-only data, the Brier score was 0.041; however, the inclusion of synthetic augmentation improved calibration to 0.018. Lower values indicate better calibration, suggesting that synthetic augmentation not only enhances classification accuracy but also improves the reliability of probability estimates. To further illustrate calibration quality, we plotted reliability diagrams ( Figure 19 ). For AMEL trained on real-only data, the predicted probabilities tended to be slightly overconfident at higher probability bins. By contrast, AMEL trained with synthetic augmentation produced curves that were much closer to the diagonal line, indicating improved alignment between the predicted and observed outcomes. These findings reinforce that AutismSynthGen improves not only the discriminative ability of models but also the trustworthiness of their confidence estimates, which is critical for clinical adoption, where calibrated risk scores are preferred over raw labels. Figure 19 Reliability diagrams for AMEL with real-only vs. real&#8239;+&#8239;synthetic data. Reliability diagram for AMEL showing observed versus predicted probabilities. A dashed line represents perfect calibration. Real-only data (orange line, Brier=0.302) and real plus synthetic data (blue line, Brier=0.277) are plotted. 4.3 Performance comparison on real + synthetic data From Table 3 and Figure 20 , it is understood that all models achieved near-perfect internal classification performance (accuracy &#8776; 1.0, F1&#8239;&#8776;&#8239;1.0, precision &#8776; 1.0, recall &#8776; 1.0, and AUC&#8239;&#8776;&#8239;1.0), confirming that synthetic data substantially improved internal consistency and learning stability ( Wang et al., 2024 ). All reported AUC and F1 metrics represent mean &#177; standard deviation across three independent random seeds (42, 123, 2025). To quantify metric stability, we also estimated 95% bootstrap confidence intervals using 1,000 resamples from the validation folds. The narrow CIs (&lt; 0.02 width) indicate consistent internal performance across runs. This aligns with recent findings on GAN-augmented medical data ( Wang et al., 2017 ). AMEL&#8217;s log loss (1.9&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 5 ) surpasses that of CNN (1.3&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 4 ), demonstrating that its adaptive ensemble optimally weights multimodal features. The 85% reduction in log loss compared to CNN suggests that AMEL better captures prediction uncertainties ( Washington et al., 2022 ). While perfect metrics warrant validation on larger datasets, AMEL&#8217;s performance indicates robust multimodal integration. Although near-perfect internal metrics (AUC&#8239;&#8776;&#8239;1.0, F1&#8239;&#8776;&#8239;1.0) were observed with synthetic augmentation, these results should be interpreted with caution, as they may partly arise from distributional similarity rather than full generalization. While perfect performance was obtained with synthetic augmentation, these results should be viewed as upper-bound estimates. Comparable state-of-the-art multimodal ASD classifiers (e.g., attention-based fusion, explainable federated learning) typically achieve AUC values between 0.85 and 0.95, highlighting the need for caution in interpreting internally perfect scores. Table 3 Performance comparison on real + synthetic data. Model Accuracy F1-Score Precision Recall AUC Log loss Logistic Regression 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.0104 Random Forest 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.0042 SVM 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.0013 CNN 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.0001 Proposed AMEL 1.0 1.0 &#177; 0.00 1.0 1.0 1.0 &#177; 0.00 0.000019 Metrics are reported as mean &#177; SD (three runs) with corresponding 95% bootstrap confidence intervals. Values near 1.00 reflect internal validation consistency rather than external generalization. Figure 20 Comparative performance metrics across models on Real + Synthetic Data. While all models achieve perfect classification (accuracy&#8239;=&#8239;F1&#8239;=&#8239;precision&#8239;=&#8239;recall&#8239;=&#8239;AUC&#8239;=&#8239;1.0), AMEL demonstrates superior prediction confidence with log loss (0.000019), an order of magnitude lower than CNN (0.0001), suggesting optimal multimodal fusion. Bar charts comparing classification metrics and log loss across five models: Logistic Regression, Random Forest, SVM, CNN, and Proposed AMEL. The left chart shows uniform scores near 1.0 for accuracy, F1-score, precision, recall, and AUC. The right chart displays log loss, with significantly lower values for CNN and Proposed AMEL. The confusion matrices in Figure 21 compare the performance of (a) logistic regression, (b) random forest, (c) SVM, (d) CNN, and (e) the proposed AMEL on real + synthetic data. Logistic regression and SVM achieve perfect classification (0 false positives/negatives), leveraging linear separability and effective margin maximization, respectively. Random Forest exhibits minimal misclassifications (2 FP, 1 FN) due to ensemble variance, while CNN has one false positive, likely from EEG signal artifacts not fully captured in synthetic data. The proposed AMEL outperforms all others, achieving zero misclassifications through the adaptive multimodal fusion of EEG, text, and demographic features, thereby validating its superior ensemble design. Figure 21 Comparison of confusion matrices for (a) logistic regression, (b) random forest, (c) SVM, (d) CNN, and (e) proposed AMEL. Five confusion matrices compare prediction results of different models. (a) Logistic Regression: 105 true negatives, 0 false positives, 0 false negatives, 36 true positives. (b) Random Forest: 104 true negatives, 1 false positive, 2 false negatives, 34 true positives. (c) SVM: 105 true negatives, 0 false positives, 2 false negatives, 34 true positives. (d) CNN: 104 true negatives, 1 false positive, 0 false negatives, 36 true positives. (e) Proposed AMEL: 104 true negatives, 1 false positive, 0 false negatives, 36 true positives. All models achieved internally near-perfect AUC values (&#8776; 1.0), reflecting strong internal discrimination on the augmented dataset ( Figure 22 ). Logistic regression and CNN exhibit the smoothest curves, indicating stable performance across thresholds, while AMEL shows minor initial fluctuations, likely due to its sequential data processing. The results confirm that synthetic data augmentation eliminates the trade-off between sensitivity (true positive rate) and specificity (1&#8212;false positive rate), with all models attaining ideal discrimination ( Aslam et al., 2022 ). Figure 22 ROC curves for all models. All curves reach the optimal (0,1) point, reflecting perfect AUC scores (1.0). Line smoothness varies by architecture, with logistic regression (solid blue) showing the most consistent trajectory. ROC curve illustrating the performance of five models: Logistic Regression (blue), Random Forest (orange), SVM (green), CNN (red), and AMEL (purple). The x-axis shows the false positive rate, and the y-axis shows the true positive rate. All curves perform near the top-left, indicating high sensitivity and specificity. A dashed diagonal line represents random chance. Both CNN and AMEL exhibit stable convergence, with training and validation metrics closely aligned, indicating effective learning without overfitting ( Figure 23 ). The CNN achieves marginally lower final loss (0.0 vs. AMEL&#8217;s 0.1) and higher validation accuracy (95% vs. 90%), suggesting stronger feature extraction from the synthetic data. However, AMEL&#8217;s smoother accuracy progression demonstrates the adaptive ensemble&#8217;s robustness to volatility, particularly between epochs 10 and 20, where the CNN&#8217;s accuracy fluctuates. The sub-0.1 loss values for both models confirm the successful integration of synthetic data, although the CNN&#8217;s faster convergence (by ~5 epochs) highlights its architectural efficiency for this task. Experiments were run on four NVIDIA A100 GPUs (256&#8239;GB RAM), with GAN training requiring ~48&#8239;h and ensemble fine-tuning requiring ~12&#8239;h. The GAN and ensemble models contain approximately 12&#8239;M and 8 M parameters, respectively. Figure 23 Training curves for CNN (top) and AMEL (bottom), showing loss (left) and accuracy (right) over 30 epochs. Four line graphs compare CNN and AMEL models over 30 epochs. Top left: CNN Loss shows training and validation loss decreasing. Top right: CNN Accuracy depicts accuracy increasing, stabilizing around 0.98. Bottom left: AMEL Loss shows a similar decrease in loss. Bottom right: AMEL Accuracy increases, leveling near 0.97. Figure 24 shows that proposed AMEL model demonstrates superior performance, achieving 100% accuracy across all runs with zero variance, compared to CNN&#8217;s 99.88% (95% CI: 99.81&#8211;99.95%), with a significant difference (paired t-test: t(9)&#8239;=&#8239;3.67, p &#8239;=&#8239;0.0051; Wilcoxon W&#8239;=&#8239;0, p &#8239;=&#8239;0.0156) and large effect size (Cohen&#8217;s d&#8239;=&#8239;1.22), confirming AMEL&#8217;s robustness through adaptive multimodal fusion of EEG, text, and demographic features. Additionally, AMEL&#8217;s log loss (1.9&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 5 , 95% CI: 1.3&#8211;2.5&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 5 ) is 85% lower than CNN&#8217;s (1.3&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 4 , 95% CI: 0.9&#8211;1.7&#8239;&#215;&#8239;10&#8239;&#8722;&#8239; 4 ), with non-overlapping confidence intervals, highlighting its enhanced prediction confidence, which is critical for clinical applications. This perfect accuracy and reduced log loss reflect the synthetic data&#8217;s effectiveness in addressing class imbalance for rare autism subtypes and AMEL&#8217;s optimal feature weighting, mitigating overconfidence observed in single-modality CNN architectures. Figure 24 Model performance comparison. (Left) Accuracy with 95% CIs (AMEL: 100%, CNN: 99.88% [99.81&#8211;99.95]). (Right) Log loss (log scale; AMEL: 1.9&#8239;&#215;&#8239;10 &#8722;5 [1.3&#8211;2.5&#8239;&#215;&#8239;10 &#8722;5 ], CNN: 1.3&#8239;&#215;&#8239;10 &#8722;4 [0.9&#8211;1.7&#8239;&#215;&#8239;10 &#8722;4 ]). AMEL shows 85% lower log loss (arrow) and statistically superior accuracy ( p &#8239;&lt;&#8239;0.01). Bar charts comparing model accuracy and calibration with 95% confidence intervals. The left chart shows AMEL with higher accuracy than CNN, with p-value 0.0051 and Cohen's d 1.22. The right chart shows AMEL has significantly lower log loss than CNN, indicating an 85% reduction. 4.4 Ablation study results The ablation study in Figure 25 reveals three critical insights: (1) EEG is the most impactful modality, with its removal causing a 12.3% accuracy drop and 420% higher log loss ( p &#8239;&lt;&#8239;0.001), validating its necessity for robust autism prediction. (2) Text and demographic data also contribute significantly (8.2 and 5.1% accuracy reductions, respectively), proving multimodal integration is essential. (3) The 67% MMD increase when removing transformer fusion demonstrates its vital role in cross-modal alignment, while attention mechanisms maintain EEG-text coherence (KS p -value drops to 0.03). These results collectively confirm that both the multimodal inputs and MADSN&#8217;s architectural components are non-redundant for optimal performance. The log loss degradation patterns further suggest that EEG data is particularly crucial for model calibration, likely due to its high-dimensional discriminative features. In modality ablation, EEG removal caused the largest drop in performance (&#8722;12% accuracy), followed by behavioral text (&#8722;8%) and demographics (&#8722;5%). Despite these reductions, the ensemble continued to perform above baseline, demonstrating resilience to missing modalities. Figure 25 Ablation study results. (a) Accuracy reduction when removing modalities (EEG shows the largest impact). (b) Corresponding log loss increase. (c) Component analysis reveals transformer fusion contributes most to data realism (67% MMD increase when removed). All changes are statistically significant (Friedman test p &#8239;&lt;&#8239;0.001). Three bar charts labeled A, B, and C. Chart A shows modality ablation impact on classification accuracy, with the full model achieving 100 percent, and reductions from EEG, text, and demographics. Chart B shows modality impact on calibration with increased log loss when ablations occur. Chart C displays MADSN component contribution with MMD increases and KS p-value changes. 4.4.1 Preliminary interpretability analysis To explore interpretability, a preliminary analysis is conducted to examine the contributions of modality, feature, and signal levels. Figure 26 shows modality weights from AMEL&#8217;s gating network: EEG contributed most (~42%), followed by behavioral scores (~28%) and severity measures (~15%), with MRI and genetics contributing less. This reflects AMEL&#8217;s adaptive weighting strategy in practice. Figure 27 presents SHAP-style feature importance for behavioral vectors. Social reciprocity (~21%), communication (~18%), and repetitive behaviors (~16%) emerged as the most influential behavioral features, while adaptive skills and sensory sensitivity played secondary roles. Figure 28 shows an EEG saliency map, which visualizes the relative importance across channels and time windows. Frontal-temporal electrodes (e.g., Ch-3, Ch-7) demonstrated higher contributions in early temporal segments, consistent with known neurodevelopmental biomarkers in ASD. Although these analyses are qualitative and exploratory, they highlight that AutismSynthGen is not a &#8220;black box&#8221; but is capable of exposing modality- and feature-level signals that drive its predictions. A systematic, clinician-guided interpretability study will be pursued in future research. Figure 26 Preliminary interpretability analysis from AMEL&#8217;s gating network. EEG consistently receives the highest contribution weight (~42%), followed by behavioral scores (~28%) and severity scores (~15%). MRI and genetics contribute less in this example. These modality-level weights provide qualitative insight into which inputs drive AutismSynthGen&#8217;s predictions. Bar chart titled &#8220;AMEL Gating Network &#8211; Modality Contributions&#8221; showing contributions from five modalities: EEG 42.00%, Behavioral Scores 28.00%, Severity Scores 15.00%, MRI 10.00%, and Genetics 5.00%. Normalized contribution weight is on the x-axis. Figure 27 SHAP-style mean absolute feature importance for behavioral vectors. Bar chart titled \\\"Behavioral Vector Feature Importance (SHAP-style)\\\" with eight colored bars. Social Reciprocity is most important at 21%, followed by Communication at 18%, Repetitive Behaviour at 16%, Restricted Interests at 12%, Adaptive Skills at 11%, Sensory Sensitivity at 10%, Attention at 7%, and Emotion Regulation at 5%. The x-axis shows normalized mean SHAP values. Figure 28 EEG saliency map across channels and time windows. Darker regions indicate higher importance for classification decisions. Frontal&#8211;temporal channels (e.g., Ch-3, Ch-7) showed strong contributions in early time windows, suggesting temporal&#8211;spatial EEG features that AutismSynthGen leverages for ASD prediction. EEG saliency map showing feature importance across channels and time. It is a heat map with EEG channels on the vertical axis and time in milliseconds on the horizontal axis. Color gradient indicates relative importance, ranging from purple (low) to yellow (high). Notable high importance areas are in channels 14 and 11 at 60 and 90 milliseconds, respectively. In Table 4 , AutismSynthGen is compared with several representative recent models. For MCBERT, Khan and Katarya (2025) report 93.4% accuracy in a leave-one-site-out evaluation using ABIDE data ( Vidivelli et al., 2025 ). The MADDHM model (Vidivelli et al.) achieves approximately 91.03% accuracy on EEG and 91.67% on face modalities in multimodal fusion experiments ( Kasri et al., 2025 ). More recently, the Vision Transformer-Mamba hybrid model, applied to the Saliency4ASD dataset, achieves an accuracy of 0.96, an F1 score of 0.95, a sensitivity of 0.97, and a specificity of 0.94, highlighting strong performance in a newer fusion paradigm ( Kasri et al., 2025 ). Compared to these existing works, AutismSynthGen distinguishes itself by integrating synthetic data augmentation under differential privacy, cross-modal attention, and a mixture-of-experts fusion pipeline in a unified system. Although our internal validation results approach perfect values, we reiterate that independent external validation remains a vital future direction before claiming generalizability. Table 4 Comparison of AutismSynthGen with selected recent multimodal or hybrid ASD models (2023&#8211;2025). Model Modalities / data types Dataset(s) / evaluation setting Reported performance Key differences &amp; comments Reference AutismSynthGen (Proposed) Imaging + EEG&#8239;+&#8239;Behavioral Internal cross-validation (ABIDE, NDAR, SSC) AUC&#8239;&#8776;&#8239;1.00, F1&#8239;&#8776;&#8239;1.00 (internal) Uses synthetic augmentation under differential privacy, mixture-of-experts fusion, and cross-modal attention &#8212; MCBERT Imaging + meta / behavioral features (via BERT) ABIDE (leave-one-site-out) Accuracy&#8239;=&#8239;93.4% Combines CNN (with spatial + channel attention)&#8239;+&#8239;BERT fusion; no synthetic augmentation or differential privacy applied Khan and Katarya (2025) MADDHM (Deep Hybrid Model) EEG&#8239;+&#8239;Face/image Dataset used in paper (fusion setting) Accuracy &#8776; 91.03% (EEG), 91.67% (face) Fusion at feature level; does not explicitly include synthetic DP augmentation Vidivelli et al. (2025) Vision Transformer-Mamba (Hybrid, eye-tracking + image + speech cues) Eye-tracking + visual/facial cues Saliency4ASD dataset Accuracy&#8239;=&#8239;0.96, F1&#8239;=&#8239;0.95, Sensitivity&#8239;=&#8239;0.97, Specificity&#8239;=&#8239;0.94 The recent hybrid model using attention-based fusion and transformer components is a good benchmark for recent works Kasri et al. (2025) &#8220;Reported Performance&#8221; refers to the primary metric(s) as presented in each paper under their reported evaluation settings. 4.5 Limitations Although near-perfect internal metrics (AUC&#8239;&#8776;&#8239;1.0, F1&#8239;&#8776;&#8239;1.0) were observed when combining real and synthetic data, such results should be interpreted cautiously and regarded as upper-bound internal estimates. While they reflect strong alignment between real and generated distributions, they may also partly arise from distributional similarity that reduces the generalization challenge. Notably, real-only performance (AUC&#8239;=&#8239;0.98, F1&#8239;=&#8239;0.99) indicates that the system is not trivially overfitting. Future external validation is needed to establish robustness. Independent validation on unseen cohorts was not feasible due to dataset constraints; thus, generalizability beyond ABIDE, NDAR, and SSC remains to be established. Future studies will incorporate held-out site validation and external benchmarking. While results demonstrate strong performance across ABIDE, NDAR, and SSC, all experiments were confined to publicly available cohorts. Validation on unseen hospital datasets or prospective clinical cohorts is necessary to establish real-world generalizability. While we include preliminary interpretability (gating weights and SHAP-style attributions), a systematic clinician-validated explainability study (e.g., EEG saliency maps, per-item SHAP reviewed by clinicians) remains future work. Currently, AutismSynthGen generates text only at the embedding level; human-readable behavioral narratives are not reconstructed. While this design ensures stability and privacy, future studies will explore transformer-based encoder&#8211;decoder architectures for realistic text generation, combined with blinded clinician review to assess interpretability and clinical realism. Future studies will involve collaborations with clinical sites to test AutismSynthGen on independent, non-public cohorts and assess robustness across diverse populations and acquisition protocols. While our analysis demonstrates privacy&#8211;utility trade-offs across &#949; values, these results remain theoretical. Future studies should also test empirical privacy leakage (e.g., membership inference attacks) to complement the theoretical guarantees. Complementary to our approach, explainable federated learning frameworks ( Alshammari et al., 2024 ) demonstrate how privacy and interpretability can be jointly addressed in distributed ASD prediction. Future studies may explore the integration of federated setups with AutismSynthGen, extending synthetic data generation to decentralized environments. 4.6 Ethical considerations Although AutismSynthGen enforces differential privacy (&#949;&#8239;&#8804;&#8239;1.0), the residual risk of indirect re-identification cannot be completely excluded. Any release of synthetic ASD data should therefore occur only under controlled access with data use agreements, ensuring prevention of unintended or commercial misuse. Given the clinical and societal sensitivities surrounding ASD, consultation with institutional review boards, clinicians, and patient advocacy groups is essential before broad dissemination. We emphasize that synthetic datasets are intended to support reproducibility and collaborative research, not to bypass established ethical safeguards. 5 Conclusion This study introduces AutismSynthGen, a unified framework for privacy-preserving synthesis and adaptive multimodal prediction of AutismSpectrum Disorder (ASD). By combining a transformer-based conditional generative model (MADSN) with differential privacy (&#949;&#8239;&#8804;&#8239;1.0) and an adaptive mixture-of-experts ensemble (AMEL), the framework effectively augmented limited multimodal datasets and improved classification performance across imaging, EEG, and behavioral modalities. Synthetic data enhanced internal validation results, with AUC and F1 values approaching 1.0, and fidelity metrics (MMD&#8239;=&#8239;0.04; KS&#8239;=&#8239;0.03; BLEU&#8239;=&#8239;0.70) demonstrating strong alignment between real and generated samples. While these outcomes underscore the potential of privacy-compliant data synthesis in ASD research, they reflect internal cross-validation within ABIDE, NDAR, and SSC datasets rather than independent external testing. Therefore, the reported near-ceiling performance should be regarded as an upper-bound estimate of internal consistency, not as evidence of clinical generalization. Future studies will focus on validating AutismSynthGen on unseen hospital cohorts and federated clinical sites, assessing its robustness under diverse acquisition settings, and conducting empirical analyses of privacy leakage and interpretability. In addition, extending the framework toward semi-supervised learning, adaptive noise scheduling, and explainable fusion mechanisms will further strengthen its clinical applicability. Ultimately, AutismSynthGen represents a promising step toward scalable, privacy-aware, and interpretable multimodal modeling for neurodevelopmental disorders, but independent external validation remains an essential prerequisite before real-world deployment. Edited by: Maryam Naseri , Yale University, United States Reviewed by: Mahsa Asadi Anar , Shahid Beheshti University of Medical Sciences, Iran Akram Pasha , University of Fujairah, United Arab Emirates Data availability statement The datasets presented in this study can be found in online repositories. The names of the repository/repositories and accession number(s) can be found at: https://github.com/mkarthiga2211/Autism-SynthGen.git . Ethics statement This study was purely computational, and all procedures were performed in compliance with relevant laws and institutional guidelines, as it falls within an area of research that does not require institutional approval by an ethics committee. Author contributions JR: Conceptualization, Investigation, Methodology, Validation, Writing &#8211; original draft. KM: Data curation, Formal analysis, Project administration, Software, Supervision, Visualization, Writing &#8211; original draft, Writing &#8211; review &amp; editing. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The author(s) declare that no Gen AI was used in the creation of this manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher&#8217;s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References Alshammari N. K. Alhusaini A. A. Pasha A. Ahamed S. S. Gadekallu T. R. Abdullah-Al-Wadud M. . ( 2024 ). Explainable federated learning for enhanced privacy in autism prediction using deep learning . J. Disabil. Res. 3 : 20240081 . doi: 10.57197/JDR-2024-0081 Aslam A. R. Hafeez N. Heidari H. Altaf M. A. B. ( 2022 ). Channels and features identification: a review and a machine-learning based model with large scale feature extraction for emotions and ASD classification . Front. Neurosci. 16 : 844851 . doi: 10.3389/fnins.2022.844851 , PMID: 35937896 PMC9355483 Avasthi S. Sanwal T. Tripathi S. L. Tyagi M. ( 2025 ). &#8220; Transformer models for topic extraction from narratives and biomedical text analysis &#8221; in Mining biomedical text, images and visual features for information retrieval . Eds. S. Dash, S. K. Pani, W. P. D. Santos, and J. Y. Chen ( USA: Academic Press ), 273 &#8211; 286 . Baltru&#353;aitis T. Ahuja C. Morency L. P. ( 2018 ). Multimodal machine learning: a survey and taxonomy . IEEE Trans. Pattern Anal. Mach. Intell. 41 , 423 &#8211; 443 . doi: 10.1109/TPAMI.2018.2798607 29994351 Bernabeu P. ( 2022 ). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power (doctoral dissertation, Lancaster University) Borodin M. Chen E. Duncan A. Khovanova T. Litchev B. Liu J. . ( 2021 ). Sequences of the stable matching problem . arXiv :2201.00645. doi: 10.48550/arXiv.2201.00645 Dcouto S. S. Pradeepkandhasamy J. ( 2024 ). Multimodal deep learning in early autism detection&#8212;recent advances and challenges . Eng. Proc. 59 : 205 . doi: 10.3390/engproc2023059205 Di Martino A. O&#8217;Connor D. Chen B. Alaerts K. Anderson J. S. Assaf M. . ( 2017 ). Enhancing studies of the connectome in autism using the autism brain imaging data exchange II . Sci Data 4 , 1 &#8211; 15 . doi: 10.1038/sdata.2017.10 PMC5349246 28291247 Ding Y. Zhang H. Qiu T. ( 2024 ). Deep learning approach to predict autism spectrum disorder: a systematic review and meta-analysis . BMC Psychiatry 24 : 739 . doi: 10.1186/s12888-024-06116-0 , PMID: 39468522 PMC11520796 Eslami T. Mirjalili V. Fong A. Laird A. R. Saeed F. ( 2019 ). ASD-DiagNet: a hybrid learning approach for detection of autism spectrum disorder using fMRI data . Front. Neuroinform. 13 : 70 . doi: 10.3389/fninf.2019.00070 , PMID: 31827430 PMC6890833 Fang M. L. Dhami D. S. Kersting K. ( 2022 ). &#8220; Dp-ctgan: differentially private medical data generation using ctgans &#8221; in International conference on artificial intelligence in medicine . Eds. E. Bertino, W. Gao, B. Steffen, and M. Yung ( Cham : Springer ), 178 &#8211; 188 . Friedrich F. Stammer W. Schramowski P. Kersting K. ( 2023 ). A typology for exploring the mitigation of shortcut behaviour . Nat. Mach. Intell. 5 , 319 &#8211; 330 . doi: 10.1038/s42256-023-00612-w Gupta K. Aly A. Ifeachor E. ( 2025 ). &#8220;Cross-domain transfer learning for domain adaptation in autism Spectrum disorder diagnosis.&#8221; In: 18th international conference on health informatics . Han X. Nguyen H. Harris C. Ho N. Saria S. ( 2024 ). Fusemoe: mixture-of-experts transformers for fleximodal fusion . Adv. Neural Inf. Proces. Syst. 37 , 67850 &#8211; 67900 . doi: 10.52202/079017-2167 Hartmann K. G. Schirrmeister R. T. Ball T. ( 2018 ). EEG-GAN: generative adversarial networks for electroencephalographic (EEG) brain signals . arXiv :1806.01875. doi: 10.48550/arXiv.1806.01875 Heinsfeld A. S. Franco A. R. Craddock R. C. Buchweitz A. Meneguzzi F. ( 2018 ). Identification of autism spectrum disorder using deep learning and the ABIDE dataset . NeuroImage: Clin. 17 , 16 &#8211; 23 . doi: 10.1016/j.nicl.2017.08.017 , PMID: 29034163 PMC5635344 Kasri W. Himeur Y. Copiaco A. Mansoor W. Albanna A. Eapen V. ( 2025 ). Hybrid vision transformer-mamba framework for autism diagnosis via eye-tracking analysis . arXiv :2506.06886. doi: 10.48550/arXiv.2506.06886 Khan K. Katarya R. ( 2025 ). MCBERT: a multi-modal framework for the diagnosis of autism spectrum disorder . Biol. Psychol. 194 : 108976 . doi: 10.1016/j.biopsycho.2024.108976 , PMID: 39722324 Lakhan A. Mohammed M. A. Abdulkareem K. H. Hamouda H. Alyahya S. ( 2023 ). Autism spectrum disorder detection framework for children based on federated learning integrated CNN-LSTM . Comput. Biol. Med. 166 : 107539 . doi: 10.1016/j.compbiomed.2023.107539 , PMID: 37804778 Levy D. Ronemus M. Yamrom B. Lee Y. H. Leotta A. Kendall J. . ( 2011 ). Rare de novo and transmitted copy-number variation in autistic spectrum disorders . Neuron 70 , 886 &#8211; 897 . doi: 10.1016/j.neuron.2011.05.015 , PMID: 21658582 Li Z. Ma R. Tang H. Guo J. Shah Z. Zhang J. . ( 2024 ). Therapeutic application of human type 2 innate lymphoid cells via induction of granzyme B-mediated tumor cell death . Cell 187 , 624 &#8211; 641 . doi: 10.1016/j.cell.2023.12.015 38211590 PMC11442011 Liu M. Li B. Hu D. ( 2021 ). Autism spectrum disorder studies using fMRI data and machine learning: a review . Front. Neurosci. 15 : 697870 . doi: 10.3389/fnins.2021.697870 , PMID: 34602966 PMC8480393 Moridian P. Ghassemi N. Jafari M. Salloum-Asfar S. Sadeghi D. Khodatars M. . ( 2022 ). Automatic autism spectrum disorder detection using artificial intelligence methods with MRI neuroimaging: a review . Front. Mol. Neurosci. 15 : 999605 . doi: 10.3389/fnmol.2022.999605 , PMID: 36267703 PMC9577321 Nanayakkara P. Bater J. He X. Hullman J. Rogers J. ( 2022 ). Visualizing privacy-utility trade-offs in differentially private data releases . Proc. Priv. Enhanc. Technol. 2022 , 601 &#8211; 618 . doi: 10.2478/popets-2022-0058 Nguyen H. Nguyen T. Ho N. ( 2023 ). Demystifying softmax gating function in Gaussian mixture of experts . Adv. Neural Inf. Proces. Syst. 36 , 4624 &#8211; 4652 . Okada N. Morita K. Tonsho S. Kiyota M. , ( 2025 ). The role of the globus pallidus subregions in the schizophrenia spectrum continuum . [Preprint]. doi: 10.21203/rs.3.rs-6439243/v1 Payakachat N. Tilford J. M. Ungar W. J. ( 2016 ). National Database for autism research (NDAR): big data opportunities for health services research and health technology assessment . PharmacoEconomics 34 , 127 &#8211; 138 . doi: 10.1007/s40273-015-0331-6 , PMID: 26446859 PMC4761298 Qu J. Han X. Chui M. L. Pu Y. Gunda S. T. Chen Z. . ( 2025 ). The application of deep learning for lymph node segmentation: a systematic review . IEEE Access 13 , 97208 &#8211; 97227 . doi: 10.1109/ACCESS.2025.3575454 Rubio-Mart&#237;n S. Garc&#237;a-Ord&#225;s M. T. Bay&#243;n-Guti&#233;rrez M. Prieto-Fern&#225;ndez N. Ben&#237;tez-Andrades J. A. ( 2024 ). Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing . Health Info. Sci. Syst. 12 : 20 . doi: 10.1007/s13755-024-00281-y , PMID: 38455725 PMC10917721 Schielen S. J. Pilmeyer J. Aldenkamp A. P. Zinger S. ( 2024 ). The diagnosis of ASD with MRI: a systematic review and meta-analysis . Transl. Psychiatry 14 : 318 . doi: 10.1038/s41398-024-03024-5 , PMID: 39095368 PMC11297045 Shazeer N. Mirhoseini A. Maziarz K. Davis A. Le Q. Hinton G. . ( 2017 ). Outrageously large neural networks: the sparsely-gated mixture-of-experts layer . arXiv :1701.06538. doi: 10.48550/arXiv.1701.06538 Singh S. Malhotra D. Mengi M. ( 2023 ). &#8220; TransLearning ASD: detection of autism Spectrum disorder using domain adaptation and transfer learning-based approach on RS-FMRI data &#8221; in Artificial intelligence communication technology . Eds. Harish Sharma, Mukesh Saraswat and Sandeep Kumar (India: SCRS), 863 &#8211; 871 . Song T. Ren Z. Zhang J. Wang M. ( 2024 ). Multi-view and multimodal graph convolutional neural network for autism spectrum disorder diagnosis . Mathematics 12 : 1648 . doi: 10.3390/math12111648 Taiyeb Khosroshahi M. Morsali S. Gharakhanlou S. Motamedi A. Hassanbaghlou S. Vahedi H. . ( 2025 ). Explainable artificial intelligence in neuroimaging of Alzheimer&#8217;s disease . Diagnostics 15 : 612 . doi: 10.3390/diagnostics15050612 , PMID: 40075859 PMC11899653 Torkzadehmahani R. Kairouz P. Paten B. ( 2019 ). &#8220;Dp-cgan: differentially private synthetic data and label generation.&#8221; In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops . Vidivelli S. Padmakumari P. Shanthi P. ( 2025 ). Multimodal autism detection: deep hybrid model with improved feature level fusion . Comput. Methods Prog. Biomed. 260 : 108492 . doi: 10.1016/j.cmpb.2024.108492 , PMID: 39700689 Vimbi V. Shaffi N. Sadiq M. A. Sirasanagandla S. R. Aradhya V. M. Kaiser M. S. . ( 2025 ). Application of explainable artificial intelligence in autism spectrum disorder detection . Cogn. Comput. 17 : 104 . doi: 10.1007/s12559-025-10462-w Viswalingam V. Kumar D. ( 2025 ). &#8220; Digital health solutions: enhancing medication adherence in COPD treatment &#8221; in Advanced drug delivery Systems in Management of chronic obstructive pulmonary disease . Eds. P. Prasher, M. Sharma, G. Liu, A. Chakraborty, and K. Dua ( Florida, USA: CRC Press ), 213 &#8211; 238 . Wang H. Pang S. Lu Z. Rao Y. Zhou Y. ( 2024 ). &#8220;Dp-promise: differentially private diffusion probabilistic models for image synthesis.&#8221; In: 33rd USENIX security symposium , pp.1063&#8211;1080. Wang J. Wang Q. Peng J. Nie D. Zhao F. Kim M. . ( 2017 ). Multi-task diagnosis for autism spectrum disorders using multi-modality features: a multi-center study . Hum. Brain Mapp. 38 , 3081 &#8211; 3097 . doi: 10.1002/hbm.23575 , PMID: 28345269 PMC5427005 Washington P. Mutlu C. O. Kline A. Paskov K. Stockham N. T. Chrisman B. . ( 2022 ). Challenges and opportunities for machine learning classification of behavior and mental state from images . arXiv :2201.11197. doi: 10.48550/arXiv.2201.11197 Zhang L. Shen B. Barnawi A. Xi S. Kumar N. Wu Y. ( 2021 ). FedDPGAN: federated differentially private generative adversarial networks framework for the detection of COVID-19 pneumonia . Inf. Syst. Front. 23 , 1403 &#8211; 1415 . doi: 10.1007/s10796-021-10144-6 , PMID: 34149305 PMC8204125 Zhou Y. Duan P. Du Y. Dvornek N. C. ( 2024a ). &#8220; Self-supervised pre-training tasks for an fMRI time-series transformer in autism detection &#8221; in International workshop on machine learning in clinical neuroimaging . Ed. P. L. Monaco ( Cham : Springer Nature Switzerland ), 145 &#8211; 154 . 10.1007/978-3-031-78761-4_14 PMC11951341 40160559 Zhou Y. Jia G. Ren Y. Ren Y. Xiao Z. Wang Y. ( 2024b ). Advancing ASD identification with neuroimaging: a novel GARL methodology integrating deep Q-learning and generative adversarial networks . BMC Med. Imaging 24 : 186 . doi: 10.1186/s12880-024-01360-y , PMID: 39054419 PMC11270770 Appendix: a dataset and implementation details Due to confidentiality, the full custom dataset cannot be publicly released. A subset of anonymized sample images is available at [ https://github.com/mkarthiga2211/Autism-SynthGen.git ]. The implementation code for Autism-SynthGen is publicly available at [ https://github.com/mkarthiga2211/Autism-SynthGen.git ], allowing for replication with alternative datasets. To enhance reproducibility, we provide full environment details (Python 3.9, PyTorch 2.0, Hugging Face Transformers 4.32, Scikit-learn 1.3), along with CUDA 11.7 compatibility. Training was conducted on 4&#8239;&#215;&#8239;NVIDIA A100 GPUs (40&#8239;GB each). Pretrained weights for MADSN and AMEL are available in the repository. A structured model card is included to document the model&#8217;s purpose, architecture, training setup, datasets used, limitations, and ethical considerations."
}