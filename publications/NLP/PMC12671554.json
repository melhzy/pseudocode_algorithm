{
  "pmcid": "PMC12671554",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:25.696103",
  "metadata": {
    "journal_title": "Findings of ACL. EMNLP. Conference on Empirical Methods in Natural Language Processing",
    "journal_nlm_ta": "Find ACL EMNLP",
    "journal": "Findings of ACL. EMNLP. Conference on Empirical Methods in Natural Language Processing",
    "pmcid": "PMC12671554",
    "pmid": "41341630",
    "doi": "10.18653/v1/2024.findings-emnlp.311",
    "title": "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?",
    "authors": [
      "Gao Yanjun",
      "Myers Skatje",
      "Chen Shan",
      "Dligach Dmitriy",
      "Miller Timothy A",
      "Bitterman Danielle",
      "Churpek Matthew",
      "Afshar Majid"
    ],
    "abstract": "The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data, especially numerical data pivotal in clinical contexts, into LLM paradigms has not been thoroughly explored. In this study, we examine the effectiveness of vector representations from last hidden states of LLMs for medical diagnostics and prognostics using electronic health record (EHR) data. We compare the performance of these embeddings with that of raw numerical EHR data when used as feature inputs to traditional machine learning (ML) algorithms that excel at tabular data learning, such as eXtreme Gradient Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to represent abnormal physiological data and evaluating their utilities as feature extractors to enhance ML classifiers for predicting diagnoses, length of stay, and mortality. Furthermore, we examine prompt engineering techniques on zero-shot and few-shot LLM embeddings to measure their impact comprehensively. Although findings suggest the raw data features still prevail in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications.  1"
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><!--The publisher of this article does not allow downloading of the full text in XML form.--><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Find ACL EMNLP</journal-id><journal-id journal-id-type=\"pmc-domain-id\">319</journal-id><journal-id journal-id-type=\"pmc-domain\">nihpa</journal-id><journal-title-group><journal-title>Findings of ACL. EMNLP. Conference on Empirical Methods in Natural Language Processing</journal-title></journal-title-group><custom-meta-group><custom-meta><meta-name>pmc-is-collection-domain</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-collection-title</meta-name><meta-value>NIHPA Author Manuscripts</meta-value></custom-meta></custom-meta-group></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12671554</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12671554.1</article-id><article-id pub-id-type=\"pmcaid\">12671554</article-id><article-id pub-id-type=\"pmcaiid\">12671554</article-id><article-id pub-id-type=\"manuscript-id\">NIHMS2035521</article-id><article-id pub-id-type=\"pmid\">41341630</article-id><article-id pub-id-type=\"doi\">10.18653/v1/2024.findings-emnlp.311</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHMS2035521</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHPA2035521</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names initials=\"Y\">Yanjun</given-names></name><xref rid=\"A1\" ref-type=\"aff\">1</xref><xref rid=\"A2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Myers</surname><given-names initials=\"S\">Skatje</given-names></name><xref rid=\"A2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"S\">Shan</given-names></name><xref rid=\"A3\" ref-type=\"aff\">3</xref><xref rid=\"A5\" ref-type=\"aff\">5</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Dligach</surname><given-names initials=\"D\">Dmitriy</given-names></name><xref rid=\"A4\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Miller</surname><given-names initials=\"TA\">Timothy A</given-names></name><xref rid=\"A5\" ref-type=\"aff\">5</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Bitterman</surname><given-names initials=\"D\">Danielle</given-names></name><xref rid=\"A3\" ref-type=\"aff\">3</xref><xref rid=\"A6\" ref-type=\"aff\">6</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Churpek</surname><given-names initials=\"M\">Matthew</given-names></name><xref rid=\"A2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Afshar</surname><given-names initials=\"M\">Majid</given-names></name><xref rid=\"A2\" ref-type=\"aff\">2</xref></contrib></contrib-group><aff id=\"A1\"><label>1</label>University of Colorado</aff><aff id=\"A2\"><label>2</label>University of Wisconsin Madison</aff><aff id=\"A3\"><label>3</label>Mass General Brigham, Harvard Medical School</aff><aff id=\"A4\"><label>4</label>Loyola University Chicago</aff><aff id=\"A5\"><label>5</label>Boston Children Hospital, Harvard Medical School</aff><aff id=\"A6\"><label>6</label>Dana-Farber Cancer Institute</aff><author-notes><corresp id=\"CR1\">Contact: <email>yanjun.gao@cuanschutz.edu</email>. This work is completed when Yanjun was in University of Wisconsin-Madison.</corresp></author-notes><pub-date pub-type=\"ppub\"><month>11</month><year>2024</year></pub-date><volume>2024</volume><issue-id pub-id-type=\"pmc-issue-id\">501654</issue-id><fpage>5414</fpage><lpage>5428</lpage><pub-history><event event-type=\"nihms-submitted\"><date><day>24</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-release\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 05:25:15.070\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"nihms-2035521.pdf\"/><abstract id=\"ABS1\"><p id=\"P1\">The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data, especially numerical data pivotal in clinical contexts, into LLM paradigms has not been thoroughly explored. In this study, we examine the effectiveness of vector representations from last hidden states of LLMs for medical diagnostics and prognostics using electronic health record (EHR) data. We compare the performance of these embeddings with that of raw numerical EHR data when used as feature inputs to traditional machine learning (ML) algorithms that excel at tabular data learning, such as eXtreme Gradient Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to represent abnormal physiological data and evaluating their utilities as feature extractors to enhance ML classifiers for predicting diagnoses, length of stay, and mortality. Furthermore, we examine prompt engineering techniques on zero-shot and few-shot LLM embeddings to measure their impact comprehensively. Although findings suggest the raw data features still prevail in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications. <sup><xref rid=\"FN1\" ref-type=\"fn\">1</xref></sup></p></abstract><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front></article></pmc-articleset>",
  "text": "Find ACL EMNLP 319 nihpa Findings of ACL. EMNLP. Conference on Empirical Methods in Natural Language Processing pmc-is-collection-domain yes pmc-collection-title NIHPA Author Manuscripts PMC12671554 PMC12671554.1 12671554 12671554 NIHMS2035521 41341630 10.18653/v1/2024.findings-emnlp.311 NIHMS2035521 NIHPA2035521 1 Article When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications? Gao Yanjun 1 2 Myers Skatje 2 Chen Shan 3 5 Dligach Dmitriy 4 Miller Timothy A 5 Bitterman Danielle 3 6 Churpek Matthew 2 Afshar Majid 2 1 University of Colorado 2 University of Wisconsin Madison 3 Mass General Brigham, Harvard Medical School 4 Loyola University Chicago 5 Boston Children Hospital, Harvard Medical School 6 Dana-Farber Cancer Institute Contact: yanjun.gao@cuanschutz.edu . This work is completed when Yanjun was in University of Wisconsin-Madison. 11 2024 2024 501654 5414 5428 24 11 2025 03 12 2025 03 12 2025 05 12 2025 The introduction of Large Language Models (LLMs) has advanced data representation and analysis, bringing significant progress in their use for medical questions and answering. Despite these advancements, integrating tabular data, especially numerical data pivotal in clinical contexts, into LLM paradigms has not been thoroughly explored. In this study, we examine the effectiveness of vector representations from last hidden states of LLMs for medical diagnostics and prognostics using electronic health record (EHR) data. We compare the performance of these embeddings with that of raw numerical EHR data when used as feature inputs to traditional machine learning (ML) algorithms that excel at tabular data learning, such as eXtreme Gradient Boosting. We focus on instruction-tuned LLMs in a zero-shot setting to represent abnormal physiological data and evaluating their utilities as feature extractors to enhance ML classifiers for predicting diagnoses, length of stay, and mortality. Furthermore, we examine prompt engineering techniques on zero-shot and few-shot LLM embeddings to measure their impact comprehensively. Although findings suggest the raw data features still prevail in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications. 1 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access no pmc-prop-olf no pmc-prop-manuscript yes pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes"
}