{
  "pmcid": "PMC12675903",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:21.346995",
  "metadata": {
    "journal_title": "Bioresources and Bioprocessing",
    "journal_nlm_ta": "Bioresour Bioprocess",
    "journal_iso_abbrev": "Bioresour Bioprocess",
    "journal": "Bioresources and Bioprocessing",
    "pmcid": "PMC12675903",
    "pmid": "41335148",
    "doi": "10.1186/s40643-025-00979-1",
    "title": "Development of robust machine learning models to estimate hydrochar higher heating value and yield based upon biomass proximate analysis",
    "year": "2025",
    "month": "12",
    "day": "3",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "3"
    },
    "authors": [
      "Hou Guoliang",
      "Alkhayyat Ahmad",
      "Almalkawi Ahmad",
      "Yadav Anupam",
      "Shreenidhi H. S.",
      "Saini Vishnu",
      "Shomurotova Shirin",
      "Singh Devendra",
      "Jain Vatsal",
      "Smerat Aseel",
      "Khalid Ahmad"
    ],
    "abstract": "This study introduces a robust machine learning framework for predicting hydrochar yield and higher heating value (HHV) using biomass proximate analysis. A curated dataset of 481 samples was assembled, featuring input variables such as fixed carbon, volatile matter, ash content, reaction time, temperature, and water content. Hydrochar yield and HHV served as the target outputs. To enhance data quality, Monte Carlo Outlier Detection (MCOD) was employed to eliminate anomalous entries. Thirteen machine learning algorithms, including convolutional neural networks (CNN), linear regression, decision trees, and advanced ensemble methods (CatBoost, LightGBM, XGBoost) were systematically compared. CatBoost demonstrated superior performance, achieving an R 2  of 0.98 and mean squared error (MSE) of 0.05 for HHV prediction, and an R 2  of 0.94 with MSE of 0.03 for yield estimation. SHAP analysis identified ash content as the most influential feature for HHV prediction, while temperature, water content, and fixed carbon were key drivers of yield. These results validate the effectiveness of gradient boosting models, particularly CatBoost, in accurately modeling hydrothermal carbonization outcomes and supporting data-driven biomass valorization strategies. Graphical abstract \n \n Supplementary Information The online version contains supplementary material available at 10.1186/s40643-025-00979-1.",
    "keywords": [
      "Biomass proximate analysis",
      "Hydrochar yield prediction",
      "Machine learning",
      "Higher heating value (HHV)",
      "CatBoost algorithm"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Bioresour Bioprocess</journal-id><journal-id journal-id-type=\"iso-abbrev\">Bioresour Bioprocess</journal-id><journal-id journal-id-type=\"pmc-domain-id\">4570</journal-id><journal-id journal-id-type=\"pmc-domain\">biobio</journal-id><journal-title-group><journal-title>Bioresources and Bioprocessing</journal-title></journal-title-group><issn pub-type=\"epub\">2197-4365</issn><publisher><publisher-name>Springer</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12675903</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12675903.1</article-id><article-id pub-id-type=\"pmcaid\">12675903</article-id><article-id pub-id-type=\"pmcaiid\">12675903</article-id><article-id pub-id-type=\"pmid\">41335148</article-id><article-id pub-id-type=\"doi\">10.1186/s40643-025-00979-1</article-id><article-id pub-id-type=\"publisher-id\">979</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Development of robust machine learning models to estimate hydrochar higher heating value and yield based upon biomass proximate analysis</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Hou</surname><given-names initials=\"G\">Guoliang</given-names></name><address><email>houguoliang@ccsfu.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Alkhayyat</surname><given-names initials=\"A\">Ahmad</given-names></name><address><email>ahmedalkhayyat85@gmail.com</email></address><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Almalkawi</surname><given-names initials=\"A\">Ahmad</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Yadav</surname><given-names initials=\"A\">Anupam</given-names></name><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Shreenidhi</surname><given-names initials=\"HS\">H. S.</given-names></name><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Saini</surname><given-names initials=\"V\">Vishnu</given-names></name><xref ref-type=\"aff\" rid=\"Aff6\">6</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Shomurotova</surname><given-names initials=\"S\">Shirin</given-names></name><xref ref-type=\"aff\" rid=\"Aff7\">7</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Singh</surname><given-names initials=\"D\">Devendra</given-names></name><xref ref-type=\"aff\" rid=\"Aff8\">8</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Jain</surname><given-names initials=\"V\">Vatsal</given-names></name><xref ref-type=\"aff\" rid=\"Aff9\">9</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Smerat</surname><given-names initials=\"A\">Aseel</given-names></name><xref ref-type=\"aff\" rid=\"Aff10\">10</xref><xref ref-type=\"aff\" rid=\"Aff11\">11</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Khalid</surname><given-names initials=\"A\">Ahmad</given-names></name><address><email>ahmad.khalidd1401@gmail.com</email></address><xref ref-type=\"aff\" rid=\"Aff12\">12</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00cbhey71</institution-id><institution-id institution-id-type=\"GRID\">grid.443294.c</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1791 567X</institution-id><institution>School of Mathematics, </institution><institution>Changchun Normal University, </institution></institution-wrap>Changchun, 130032 Jilin China </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/024dzaa63</institution-id><institution>Department of Computers Techniques Engineering, College of Technical Engineering, </institution><institution>The Islamic University, </institution></institution-wrap>Najaf, Iraq </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02hvzvg02</institution-id><institution-id institution-id-type=\"GRID\">grid.501970.a</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0418 6164</institution-id><institution>Center for ESL &amp; Academic Preparation, </institution><institution>Modern College of Business and Science, </institution></institution-wrap>Muscat, Oman </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/05fnxgv12</institution-id><institution-id institution-id-type=\"GRID\">grid.448881.9</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1774 2318</institution-id><institution>Department of Computer Engineering and Application, </institution><institution>GLA University, </institution></institution-wrap>Mathura, 281406 India </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01cnqpt53</institution-id><institution-id institution-id-type=\"GRID\">grid.449351.e</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1769 1282</institution-id><institution>Department of Computer Science and Engineering, School of Engineering and Technology, </institution><institution>JAIN (Deemed to Be University), </institution></institution-wrap>Bangalore, Karnataka India </aff><aff id=\"Aff6\"><label>6</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03b6ffh07</institution-id><institution-id institution-id-type=\"GRID\">grid.412552.5</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1764 278X</institution-id><institution>Sharda School of Engineering and Sciences, </institution><institution>Sharda University, </institution></institution-wrap>Knowledge Park III, Greater Noida, 201310 Uttar Pradesh India </aff><aff id=\"Aff7\"><label>7</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/051g1n833</institution-id><institution-id institution-id-type=\"GRID\">grid.502767.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0403 3387</institution-id><institution>Department of Chemistry Teaching Methods, </institution><institution>National Pedagogical University of Uzbekistan, </institution></institution-wrap>Bunyodkor Street 27, Tashkent, Uzbekistan </aff><aff id=\"Aff8\"><label>8</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00ba6pg24</institution-id><institution-id institution-id-type=\"GRID\">grid.449906.6</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 4659 5193</institution-id><institution>Department of Computer Science &amp; Engineering, Uttaranchal Institute of Technology, </institution><institution>Uttaranchal University, </institution></institution-wrap>Dehradun, Uttarakhand 248007 India </aff><aff id=\"Aff9\"><label>9</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/057d6z539</institution-id><institution-id institution-id-type=\"GRID\">grid.428245.d</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1765 3753</institution-id><institution>Centre for Research Impact &amp; Outcome, Chitkara University Institute of Engineering and Technology, </institution><institution>Chitkara University, </institution></institution-wrap>Rajpura, Punjab 140401 India </aff><aff id=\"Aff10\"><label>10</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00xddhq60</institution-id><institution-id institution-id-type=\"GRID\">grid.116345.4</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0644 1915</institution-id><institution>Faculty of Educational Sciences, </institution><institution>Al-Ahliyya Amman University, </institution></institution-wrap>Amman, 19328 Jordan </aff><aff id=\"Aff11\"><label>11</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/0034me914</institution-id><institution-id institution-id-type=\"GRID\">grid.412431.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0444 045X</institution-id><institution>Department of Biosciences, Saveetha School of Engineering, </institution><institution>Saveetha Institute of Medical and Technical Sciences, </institution></institution-wrap>Chennai, 602105 India </aff><aff id=\"Aff12\"><label>12</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04hcvaf32</institution-id><institution-id institution-id-type=\"GRID\">grid.412413.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 2299 4112</institution-id><institution>Faculty of Engineering, </institution><institution>Sana&#8217;a University, </institution></institution-wrap>Sanaa, Yemen </aff></contrib-group><pub-date pub-type=\"epub\"><day>3</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>12</month><year>2025</year></pub-date><volume>12</volume><issue>1</issue><issue-id pub-id-type=\"pmc-issue-id\">478465</issue-id><elocation-id>138</elocation-id><history><date date-type=\"received\"><day>22</day><month>7</month><year>2025</year></date><date date-type=\"rev-recd\"><day>22</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>12</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>05</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 00:25:12.533\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"40643_2025_Article_979.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">This study introduces a robust machine learning framework for predicting hydrochar yield and higher heating value (HHV) using biomass proximate analysis. A curated dataset of 481 samples was assembled, featuring input variables such as fixed carbon, volatile matter, ash content, reaction time, temperature, and water content. Hydrochar yield and HHV served as the target outputs. To enhance data quality, Monte Carlo Outlier Detection (MCOD) was employed to eliminate anomalous entries. Thirteen machine learning algorithms, including convolutional neural networks (CNN), linear regression, decision trees, and advanced ensemble methods (CatBoost, LightGBM, XGBoost) were systematically compared. CatBoost demonstrated superior performance, achieving an R<sup>2</sup> of 0.98 and mean squared error (MSE) of 0.05 for HHV prediction, and an R<sup>2</sup> of 0.94 with MSE of 0.03 for yield estimation. SHAP analysis identified ash content as the most influential feature for HHV prediction, while temperature, water content, and fixed carbon were key drivers of yield. These results validate the effectiveness of gradient boosting models, particularly CatBoost, in accurately modeling hydrothermal carbonization outcomes and supporting data-driven biomass valorization strategies.</p><sec><title>Graphical abstract</title><p id=\"Par2\">\n<graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"MO19\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Figa_HTML.jpg\"/>\n</p></sec><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1186/s40643-025-00979-1.</p></sec></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Biomass proximate analysis</kwd><kwd>Hydrochar yield prediction</kwd><kwd>Machine learning</kwd><kwd>Higher heating value (HHV)</kwd><kwd>CatBoost algorithm</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; State Key Laboratory of Bioreactor Engineering, East China University of Science and Technology 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par3\">The majority of chemical requirements and global energy are presently satisfied by fossil fuels. In light of the declining availability of fossil reserves and their adversative environmental consequences, such as air pollution and climate change, scientists are exploring justifiable alternatives for energy production (Yang et al. <xref ref-type=\"bibr\" rid=\"CR79\">2022</xref>; Aghdam et al. <xref ref-type=\"bibr\" rid=\"CR3\">2023</xref>; Khezerlooe-ye Aghdam et al. <xref ref-type=\"bibr\" rid=\"CR40\">2019</xref>; Li 2025; Wu et al. 2025; Yu et al. 2023). Carbon&#8211;neutral biomass energy has appeared as a prominent focus among renewable energy resources over recent decades (Chen et al. <xref ref-type=\"bibr\" rid=\"CR15\">2022a</xref>; Wang et al. 2025; Xu et al. 2024) primarily because it can be transformed into fossil-like liquid, gaseous, and solid fuels with versatile applications. As the most significant renewable source of energy on Earth, bioenergy constitutes roughly 10 percent of the worldwide key energy supply (Tauro et al. <xref ref-type=\"bibr\" rid=\"CR66\">2021</xref>; Li et al. <xref ref-type=\"bibr\" rid=\"CR44\">2025</xref>; Niu et al. <xref ref-type=\"bibr\" rid=\"CR51\">2022</xref>; Dou et al. <xref ref-type=\"bibr\" rid=\"CR17\">2025</xref>). Approximately semi of worldwide biomass usage is dedicated to cooking and heating in emergent nations (Bhutto et al. <xref ref-type=\"bibr\" rid=\"CR11\">2019</xref>). To address this, shifting from traditional biomass to advanced modern energy solutions like biodiesel, biogas, biopower, and bioethanol is essential to minimize adverse environmental effects.</p><p id=\"Par4\">Several thermochemical technologies have been proposed to change biomass into biochemicals and biofuels (Chen et al. <xref ref-type=\"bibr\" rid=\"CR16\">2022b</xref>). Thermochemical manners, such as pyrolysis, gasification, combustion, and hydrothermal carbonization (HTC), have drawn substantial scientific interest (Hai et al. <xref ref-type=\"bibr\" rid=\"CR28\">2023</xref>; Paula et al. <xref ref-type=\"bibr\" rid=\"CR52\">2022</xref>). These approaches allow the straight conversion of biomass into value-added products without the need for chemically harsh and energy-intensive pretreatment procedures. However, the high content of moisture for many biomass feedstocks, both dedicated and waste-derived, renders them inappropriate for certain thermochemical pathways like pyrolysis, combustion, and gasification (Li 2025; Shafizadeh et al. <xref ref-type=\"bibr\" rid=\"CR59\">2022</xref>; Tian et al. 2025). Current methods are inefficient at handling wet biomass, resulting in substantial expenses for humidity decrease. Hydrothermal processing technologies, with HTC at the forefront, present a practical and cost-effective solution by eliminating the energy-intensive drying phase. Beyond this, HTC is also more energy-efficient than other thermal conversion processes like gasification and pyrolysis (Zhang et al. <xref ref-type=\"bibr\" rid=\"CR83\">2019</xref>) and, due to its slighter reaction circumstances, generates minimum amounts of toxic gases, like sulfur oxides and nitrogen.</p><p id=\"Par5\">The hydrothermal carbonization (HTC) procedure is conducted in pressed water at 180 and 260 &#176;C, mimicking the ordinary coalification development that happened over millions of years. This progression produces a carbon-rich known as hydrochar (Xu et al. <xref ref-type=\"bibr\" rid=\"CR76\">2020</xref>), Hydrochar&#8217;s properties make it comparable to lignite. Biochar which formed in pyrolysis, is specifically suited for submissions for instance, water treatment, carbon sequestration, and agriculture due to its large carbon content, reduced reactivity, and high surface area (Xiong et al. <xref ref-type=\"bibr\" rid=\"CR74\">2019</xref>). Hydrochar has higher ash content, lower carbon content, and greater reactivity (Xu et al. <xref ref-type=\"bibr\" rid=\"CR77\">2021</xref>; Shi et al. <xref ref-type=\"bibr\" rid=\"CR61\">2021</xref>), positioning it as an interesting compound for soil amendment, energy generation, and pollutant adsorption (Li et al. <xref ref-type=\"bibr\" rid=\"CR44\">2025</xref>). Generated under comparatively mild circumstances in comparison to biochar, hydrochar demonstrates reduced porosity, lower stability, lower pH, and smaller specific surface area (Liu et al. <xref ref-type=\"bibr\" rid=\"CR46\">2022</xref>). However, its abundance of oxygenated groups, containing carbonyl, carboxyl and phenolic hydroxyl, enhances its ability to adsorb metallic element (Tsarpali et al. <xref ref-type=\"bibr\" rid=\"CR69\">2022</xref>).</p><p id=\"Par6\">The structural and physico-chemical properties of hydrochar, highly depend on the nature of the biomass and the parameters of the HTC. Detection of the ideal treating conditions for a given biomass feedstock typically involves many laboratory tests, which are resource-intensive, laborious, and expensive. These restrictions hinder the detailed characterization of hydrochar&#8217;s quantity and quality. The inherent complexity of interactions and mechanisms during the hydrothermal carbonization (HTC) process renders current computational techniques inadequate for accurately predicting hydrochar properties. Consequently, it is imperative to develop more effective and precise methodologies for assessing the structural and physicochemical characteristics of hydrochar, thereby ensuring its appropriate utilization across various applications (Shafizadeh et al. <xref ref-type=\"bibr\" rid=\"CR60\">2023</xref>; Fang et al. 2025; Xu and Liang 2025; Yang et al. 2025; Zhang et al. 2025).</p><p id=\"Par7\">This work presents a novel and rigorous methodology for predicting hydrochar&#8217;s HHV and yield. Our primary contribution is the systematic evaluation of a broad spectrum of machine learning algorithms, including traditional models (linear regression, SVM) and advanced ensemble and deep learning techniques (ANN, CNN, RF, XGBoost, CatBoost, LightGBM). This comprehensive head-to-head performance comparison sets a new benchmark in this research area. We also introduce a robust Monte Carlo-based outlier detection to enhance dataset reliability, a step often overlooked in similar studies. To ensure the trustworthiness of our findings, we provide a detailed analysis of model performance using multiple metrics and graphical representations. The final novelty of our work is the application of SHAP analysis on the best-performing model, which not only validates our results but also provides crucial insights into the underlying relationships between biomass composition and hydrochar properties, thereby advancing the fundamental understanding of this process. The complete methodology context is illustrated in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;1</label><caption><p>Overall workflow taken in this study to construct the data-driven models and choose the top-performing one subsequently</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig1_HTML.jpg\"/></fig></p></sec><sec id=\"Sec2\"><title>Machine learning backgrounds</title><p id=\"Par8\">This segment details the machine learning algorithms.</p><sec id=\"Sec3\"><title>Convolutional neural network</title><p id=\"Par9\">It is a specialized class of algorithms planned to process structured data, such as images, by leveraging their spatial hierarchies. Inspired by the visual cortex of the human brain, CNNs utilize a blend of pooling, convolutional, and fully connected layers to detect and learn features at different levels of complexity. Convolutional layers use filters to recognize patterns like textures and edges, while pooling layers decrease dimensionality, increasing computational efficiency and resilience to input variations. Fully connected layers integrate these features to produce final predictions. The training process, driven by backpropagation, adjusts the network&#8217;s weights to minimize prediction error, while activation functions such as ReLU introduce non-linearity to capture complex relationships in the data.</p><p id=\"Par10\">CNNs excel in a variety of computer vision tasks, including image classification, object detection, and face recognition, where their ability to extract features automatically makes them highly effective. Beyond traditional vision applications, CNNs have been extended to other fields as well. In medical imaging, they assist in disease diagnosis by analyzing features in scans or X-rays. In e-commerce and video analytics, CNNs are employed for tasks such as optical character recognition (OCR) and automated tagging. Even in autonomous driving, CNNs play a vital role in lane detection and obstacle avoidance, while adaptations of CNNs in natural language processing (NLP) are used for sequence modeling and text analysis. Their adaptability across domains makes them indispensable tools in modern artificial intelligence.</p><p id=\"Par11\">Despite their advantages, CNNs come with challenges. They require large labeled datasets and substantial computational resources for training, which can be a limiting factor in many applications. CNNs are also susceptible to adversarial attacks, where subtle changes to input data can mislead predictions. Moreover, overfitting is a common issue, particularly when training with limited data, necessitating regularization techniques and hyperparameter optimization to achieve strong generalization (Li et al. <xref ref-type=\"bibr\" rid=\"CR43\">2022</xref>; Kim <xref ref-type=\"bibr\" rid=\"CR41\">2017</xref>; Lopez Pinaya, et al. <xref ref-type=\"bibr\" rid=\"CR47\">2020</xref>; Aloysius and Geetha <xref ref-type=\"bibr\" rid=\"CR6\">2017</xref>; Chagas, et al. <xref ref-type=\"bibr\" rid=\"CR13\">2018</xref>).</p></sec><sec id=\"Sec4\"><title>Artificial neural network</title><p id=\"Par12\">ANNs are computational manners encouraged by biological neural systems consisting of interconnected layers of neurons. Data is processed through these layers, with each connection having a weight that is adjusted during learning to emphasize or de-emphasize certain features. This learning process is driven by backpropagation, where the loss function&#8217;s gradient is computed and used to update the weights iteratively. This enables ANNs to progress their ability to model complex patterns progressively (Fan et al. 2025; Yue 2025; Zhang et al. 2017).</p><p id=\"Par13\">ANNs are highly versatile and can be applied across various fields. In healthcare, they are used for medical diagnostics, imaging, or genomics data to detect disease patterns. In finance, ANNs assist in credit scoring and risk assessment, leveraging historical data to predict outcomes. They are also used in marketing for customer segmentation and behavior analysis, in natural language processing for tasks like sentiment analysis, and robotics for autonomous decision-making.</p><p id=\"Par14\">Despite their advantages, ANNs have challenges. They need large data and substantial computational power to train, and their training can be time-consuming. Additionally, the &#8220;black-box&#8221; nature of ANNs makes interpreting their decision-making processes difficult, which is problematic in areas requiring transparency, such as healthcare or legal systems. Nonetheless, ANNs remain powerful tools for tasks involving complex data (Khan, et al. <xref ref-type=\"bibr\" rid=\"CR39\">2019</xref>; Ahmadi et al. <xref ref-type=\"bibr\" rid=\"CR4\">2013</xref>; Gardner and Dorling <xref ref-type=\"bibr\" rid=\"CR24\">1998</xref>; Zhu <xref ref-type=\"bibr\" rid=\"CR87\">2022</xref>; Heidari et al. <xref ref-type=\"bibr\" rid=\"CR30\">2016</xref>).</p></sec><sec id=\"Sec5\"><title>Decision tree</title><p id=\"Par15\">A Decision Tree is a flowchart-like used in machine learning, where each internal node characterizes a test on a feature, and branches represent possible outcomes. Leaf nodes indicate the final decision or classification. The tree splits the data based on criteria such as information gain, Gini impurity, or variance reduction, targeting to generate subsets that are as homogeneous as possible.</p><p id=\"Par16\">The tree starts with the root node demonstrating the entire data. Then, it splits the data into smaller subsets based on feature values, optimizing the separation of classes or minimizing variance in regression. The feature that most effectively splits the data, i.e., providing the highest information gain or variance reduction, tends to be placed near the root. This process continues until stopping conditions are met.</p><p id=\"Par17\">Decision Trees are valued for their interpretability and simplicity. The structure lets users to trace the path from the root to the leaf to understand the decision-making process. This makes them suitable for applications like medical diagnostics, where transparency is critical, or in finance for tasks like credit scoring.</p><p id=\"Par18\">However, Decision Trees are prone to overfitting, especially with complex trees. This can be mitigated with pruning or ensemble techniques such as Random Forests or Gradient Boosting. Despite their limitations, Decision Trees remain a influential tool for various practical machine-learning tasks due to their clarity and ease of use (Ghiasi et al. <xref ref-type=\"bibr\" rid=\"CR25\">2020</xref>; Hautaniemi et al. <xref ref-type=\"bibr\" rid=\"CR29\">2005</xref>).</p></sec><sec id=\"Sec6\"><title>Random forest</title><p id=\"Par19\">Random Forest is a technique that generates multiple decision trees during training and aggregates their predictions to enhance model precision and robustness. This method, based on bagging (bootstrap aggregating), involves training several models on various random subsets of the original dataset drawn with replacement. By averaging predictions in regression or using majority voting in classification, Random Forest reduces the overfitting common in individual decision trees, leading to more generalized predictions. The randomization process introduces decorrelation among the trees, which effectively reduces variance and enhances prediction stability. Additionally, each tree is trained on a bootstrapped dataset, and at each split, a random subset of features is considered. This ensures that each tree learns different patterns, contributing to the ensemble&#8217;s overall strength.</p><p id=\"Par20\">Random Forest is widely applied in various fields, such as finance, healthcare, and ecology, owing to its robustness and reliability in making predictions from large datasets. It excels in risk assessment, fraud detection, medical diagnoses, and ecological modeling. Despite its advantages, the model can be computationally expensive and resource-intensive due to the large number of trees. Its complexity also reduces interpretability, as it is difficult to visualize the decision-making process across all trees. Nonetheless, Random Forest offers a powerful balance between bias and variance, with an internal feature importance measure that helps identify key variables in predictive modeling tasks. (Sarica et al. <xref ref-type=\"bibr\" rid=\"CR58\">2017</xref>; Rigatti <xref ref-type=\"bibr\" rid=\"CR54\">2017</xref>; Feng, et al. <xref ref-type=\"bibr\" rid=\"CR23\">2020</xref>; Ao et al. <xref ref-type=\"bibr\" rid=\"CR7\">2019</xref>; Cha et al. <xref ref-type=\"bibr\" rid=\"CR12\">2021</xref>).</p></sec><sec id=\"Sec7\"><title>Linear regression</title><p id=\"Par21\">It&#8217;s a foundational statistical approach that launches a linear relationship between a target variable and its influencing factors. In simple linear regression, a single independent variable forecasts the dependent variable, whereas multiple linear regression incorporates several predictors. The model assumes a linear relationship and is expressed as&#160;<italic toggle=\"yes\">Y</italic>&#8201;=&#8201;<italic toggle=\"yes\">&#946;</italic>0&#8201;+&#8201;<italic toggle=\"yes\">&#946;</italic>1<italic toggle=\"yes\">X</italic>1&#8201;+&#8201;<italic toggle=\"yes\">&#946;</italic>2<italic toggle=\"yes\">X</italic>2&#8201;+&#8201;&#8230;&#8201;+&#8201;<italic toggle=\"yes\">&#946;nXn</italic>&#8201;+&#8201;<italic toggle=\"yes\">&#949;</italic>, where&#160;<italic toggle=\"yes\">&#946;</italic>&#160;represents coefficients and&#160;<italic toggle=\"yes\">&#949;</italic>&#160;denotes the error term. The goal is to determine the coefficients that minimize the sum of squared residuals, a process achieved through&#160;least squares estimation. Model performance is assessed using metrics such as&#160;R-squared&#160;and&#160;residual analysis&#160;to evaluate goodness-of-fit and predictive accuracy.</p><p id=\"Par22\">Linear Regression is widely applied across disciplines due to its simplicity and interpretability. In economics, it models relationships between financial variables, while in marketing, it helps analyze consumer behavior. In biology, it is used for dose&#8211;response modeling, and in engineering, it predicts system outputs based on input data. Despite its advantages, Linear Regression assumes a strict linear relationship, making it unsuitable for complex, non-linear data. It is also sensitive to&#160;outliers&#160;and suffers from&#160;multicollinearity, which can distort coefficient estimates. To address these challenges, techniques such as&#160;variable transformation,&#160;regularization, and&#160;polynomial regression&#160;are often employed. In spite of its limitation, Linear Regression remains a fundamental manner for understanding data relationships and making initial predictive assessments (Guo and Wang <xref ref-type=\"bibr\" rid=\"CR26\">2019</xref>; Hope and Chapter <xref ref-type=\"bibr\" rid=\"CR35\">2020</xref>; Chen et al. <xref ref-type=\"bibr\" rid=\"CR14\">2019</xref>).</p></sec><sec id=\"Sec8\"><title>Ridge regression</title><p id=\"Par23\">This approach enhances linear regression by incorporating an L2 regularization penalty to address multicollinearity and mitigate overfitting. By adding a penalty term proportional to the square of the coefficients&#8217; amounts, it shrinks coefficients toward zero without eliminating them, promoting model stability. The penalty is controlled by a hyperparameter, lambda, which determines the degree of regularization and balances bias and variance. This adjustment makes Ridge Regression well-suited for datasets with correlated predictors by reducing the sensitivity of the fitted model to multicollinearity. Unlike ordinary least squares (OLS), Ridge favors smaller coefficients, improving stability and predictive performance when handling a large number of correlated features or medium-sized effects. Importantly, all predictors are retained, maintaining a more holistic model.</p><p id=\"Par24\">Ridge Regression is particularly valuable in high-dimensional datasets, such as genetic studies, where it identifies significant predictors like key genes while managing correlations effectively. In finance, it aids in credit risk modeling by stabilizing models with many variables, and in sales forecasting, it reduces overfitting while accounting for multicollinearity. Ridge offers a robust approach for prediction in complex datasets, though it does not perform feature selection, limiting its ability to produce sparse models. Its effectiveness depends on careful tuning of the regularization parameter, requiring computational resources and expertise for optimal implementation (Hoerl and Kennard <xref ref-type=\"bibr\" rid=\"CR32\">1970a</xref>, <xref ref-type=\"bibr\" rid=\"CR33\">1970b</xref>; Smith and Campbell <xref ref-type=\"bibr\" rid=\"CR63\">1980</xref>; Hoerl <xref ref-type=\"bibr\" rid=\"CR31\">2020</xref>).</p></sec><sec id=\"Sec9\"><title>Lasso regression</title><p id=\"Par25\">This is a linear regression method that incorporates L1 regularization to develop both prediction exactness and model interpretability. The key feature of Lasso is its ability to perform variable selection by penalizing the absolute size of the regression coefficients, thereby driving some of them to exactly zero. This results in a sparse model that selects only the most significant predictors, making it valuable when working with data holding a large number of predictors. By shrinking less important variables&#8217; coefficients to zero, Lasso mitigates overfitting, ensuring that irrelevant or redundant features do not influence the model. The strength of the regularization is controlled by a hyperparameter, lambda, which adjusts the trade-off between bias and variance, allowing for the retention of only relevant predictors.</p><p id=\"Par26\">Lasso Regression is widely applied across various domains, such as bioinformatics, where it helps identify significant genetic markers from a large set of genes, and econometrics, where it selects key economic predictors for forecasting trends. In engineering, particularly in signal processing, Lasso aids in feature selection while simplifying models. Its main advantage is its ability to produce a simpler, more understandable model by reducing the number of predictors. However, Lasso can be less effective when dealing with highly correlated variables, as it may arbitrarily choose one over others. The model&#8217;s performance heavily relies on the careful selection of the regularization parameter, which requires fine-tuning to achieve optimal results. In scenarios with high multicollinearity, Ridge Regression may sometimes outperform Lasso due to its ability to handle correlated features more effectively (Kang, et al. <xref ref-type=\"bibr\" rid=\"CR37\">2020</xref>; Roth <xref ref-type=\"bibr\" rid=\"CR56\">2004</xref>; Emmert-Streib and Dehmer <xref ref-type=\"bibr\" rid=\"CR18\">2019</xref>).</p></sec><sec id=\"Sec10\"><title>Support vector regression</title><p id=\"Par27\">SVR extends SVM to regression tasks, predicting continuous values while maintaining errors within a specified margin (<inline-formula id=\"IEq90\"><tex-math id=\"d33e645\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ \\upepsilon $$\\end{document}</tex-math></inline-formula>). It utilizes an insensitive loss function, reducing the impact of minor deviations and focusing on overall trends. By applying kernel functions like polynomial, linear, and radial basis functions (RBF), SVR effectively models both linear and nonlinear relations.</p><p id=\"Par28\">SVR is widely used in fields requiring precise forecasting, such as stock price prediction, real estate valuation, and energy consumption analysis. However, its computational cost is high, especially for large datasets, and performance depends on careful parameter tuning to balance accuracy and efficiency (Smola and Sch&#246;lkopf <xref ref-type=\"bibr\" rid=\"CR64\">2004</xref>; Zhang and O&#8217;Donnell <xref ref-type=\"bibr\" rid=\"CR85\">2020</xref>; Kavitha et al. <xref ref-type=\"bibr\" rid=\"CR38\">2016</xref>).</p></sec><sec id=\"Sec11\"><title>Gradient boosting machine</title><p id=\"Par29\">GBMs are cooperative learning models which combine multiple weak learners, archetypally decision trees, to build a strong predictive model. The core principle of GBMs is residual learning, where each new model corrects the errors of the previous one by focusing on the residual errors from earlier iterations. This process minimizes a differentiable loss function (e.g., mean squared error for regression or binary log loss for classification), progressively enhancing model accuracy. At each stage, GBMs add new models that adjust based on the previous iteration&#8217;s errors, creating a refined prediction with each step. The sequential learning process ensures that the model effectively captures complex data patterns and interactions.</p><p id=\"Par30\">GBMs have a broad range of applications, from web search rankings and credit scoring to fraud detection and healthcare analytics, offering high accuracy and predictive power. Their success in data science competitions highlights their ability to capture intricate data structures. However, their computational intensity, especially with large datasets, and the complexity of model interpretation pose challenges. Regularization and hyperparameter tuning are essential to prevent overfitting and ensure optimal performance, requiring significant expertise to balance model complexity and interpretability (Zulfiqar et al. <xref ref-type=\"bibr\" rid=\"CR88\">2021</xref>; Touzani et al. <xref ref-type=\"bibr\" rid=\"CR68\">2018</xref>; Fan et al. <xref ref-type=\"bibr\" rid=\"CR19\">2019</xref>).</p><sec id=\"Sec12\"><title>K-nearest neighbors</title><p id=\"Par31\">KNN is a simple, non-parametric algorithm applied for regression uses. It operates by classifying a new data point based on the majority class of its &#8216;k&#8217; closest training points, using distance metrics like Euclidean distance. KNN does not require explicit model training; instead, it stores all data points and performs predictions by calculating distances between the test point and each stored point. The choice of &#8216;k&#8217; is crucial: a smaller &#8216;k&#8217; makes the model sensitive to noise, while a larger &#8216;k&#8217; may smooth out valuable details. KNN is widely used in applications such as recommendation systems, image recognition, anomaly detection, and medical diagnostics. Despite its simplicity and flexibility, KNN can be computationally expensive, especially with high-dimensional data, requiring efficient distance computations. Additionally, performance heavily depends on the correct selection of &#8216;k&#8217; and distance metric (Holt et al. <xref ref-type=\"bibr\" rid=\"CR34\">1987</xref>; Bansal et al. <xref ref-type=\"bibr\" rid=\"CR8\">2022</xref>; Kramer and K-Nearest Neighbors, in Dimensionality Reduction with Unsupervised Nearest Neighbors. <xref ref-type=\"bibr\" rid=\"CR42\">2013</xref>).</p></sec><sec id=\"Sec13\"><title>Extreme gradient boosting</title><p id=\"Par32\">It is an advanced, scalable, and distributed gradient-boosting decision tree context designed to enhance the performance and speed of traditional gradient-boosting methods. By incorporating algorithmic optimizations like parallel tree boosting, it efficiently processes large datasets. Regularization terms, including L1 (Lasso) and L2 (Ridge), are integrated into the loss function to improve model performance and prevent overfitting. XGBoost&#8217;s architecture leverages decision trees as base learners, enhanced by efficient computational strategies such as weighted quantile sketch and column block compression. Its versatility makes it widely used in areas such as fraud detection, bioinformatics, and customer churn analysis, where non-linear relations and large-scale data require precise predictive capabilities. Although powerful, XGBoost&#8217;s complexity demands expertise for fine-tuning hyperparameters, as it can be challenging to interpret and optimize without careful parameter adjustments (Sado et al. <xref ref-type=\"bibr\" rid=\"CR57\">2023</xref>; Tyralis and Papacharalampous <xref ref-type=\"bibr\" rid=\"CR70\">2021</xref>).</p></sec><sec id=\"Sec14\"><title>Light gradient boosting machine</title><p id=\"Par33\">It&#8217;s a gradient-boosting system that utilizes decision trees as its core learning algorithm that are optimized for efficiency and speed across large datasets. Emphasizing lighter algorithmic infrastructure, it processes input data faster using advanced innovations like the histogram-based algorithm and leaf-wise growth strategy. LightGBM, designed to enhance computational speed without trading off accuracy, introduces exclusive feature bundling (EFB) and gradient-based one-side sampling, which reduce data features and observations during splitting&#8212;improving model precision while diminishing both time and space complexity during learning processes, suited for faster-distributed computing.</p><p id=\"Par34\">LightGBM&#8217;s key architectural innovations let it deploy faster, with efficient memory usage through customized data structures such as histograms, enabling faster data partitioning and decision tree expansion. By selecting the best leaf to grow instead of using the standard level-wise strategies, LightGBM achieves high precision in predicting with minimized complexity. The exclusive feature bundling reduces feature dimensionality in datasets, assisting in streamlined processing even with categorical data critical in real-world applications. Moreover, gradient-based one-side sampling confines observed samples to a compact representative, mitigating computational lag during model refinement and reducing overfitting potential.</p><p id=\"Par35\">LightGBM&#8217;s efficiency and capacity have made it a favorite choice in high-speed predictive modeling areas such as click-through rate predictions in advertising technology, customer churn prediction in telecommunications, and risk assessment in financial markets, where speed and precision are critical. It is extensively used in environments demanding resource scalability with fast-paced data analytics like retail demand forecasting and supply chain optimization. Its strength in running large-scale data predictions allows LightGBM to address real-world complexity challenges ranging from image processing tasks to genetic studies with massive sample size demands.</p><p id=\"Par36\">LightGBM&#8217;s principal advantage is its excellent performance speeds combined with scalable accuracy, particularly in high-dimensional data scenarios. Its capability to handle categorical variables differentiates it distinctly from similar boosting algorithms. However, precise tuning is required to avoid overfitting and optimize performance, necessitating knowledge of various hyperparameters. LightGBM models can be less interpretable than simpler algorithms due to the intricate nature of multi-layered processes, posing challenges when deployed in environments where model transparency is vital. Nonetheless, for applications prioritizing speed and extensive variable handling, LightGBM continues to be a preferred choice. (Fan et al. <xref ref-type=\"bibr\" rid=\"CR19\">2019</xref>; Guo et al. <xref ref-type=\"bibr\" rid=\"CR27\">2023</xref>; Taha and Malebary <xref ref-type=\"bibr\" rid=\"CR65\">2020</xref>).</p></sec><sec id=\"Sec15\"><title>Elastic net</title><p id=\"Par37\">It is a versatile regression approach integrating the regularization strengths of both Lasso (L1) and Ridge (L2) to improve model performance, especially where predictors outnumber observations or exhibit significant collinearity. Elastic Net balances between these penalties, facilitating the advantages of variable selection and shrinkage concurrently, without wholly assigning any input values zero coefficients, hence improving upon the limitations encountered with Lasso&#8212;specifically in retaining correlated feature pairs. The inclusion of both penalty types ensures dimensionality reduction power, governed by a dual-tuning parameter controlling the balance effect between Lasso-like sparse representations and Ridge-inspired continuous scaling.</p><p id=\"Par38\">Elastic Net is based on a linear model with a loss function penalized by both L1 and L2 norms. Through hyperparameters, lambda for regularization, and alpha determining the mix ratio between L1 and L2 penalties, Elastic Net fits models jointly using weighted combinations of penalties. This combined penalty optimizes performance by robustly shrinking coefficients, leading to better generalization of unseen data by controlling prediction variance. Elastic Net addresses issues like suppression of important predictors and multicollinearity through regularization effects, thus offering a flexible toolkit for hypothesis testing and exploratory modeling where typical regression methods might fail.</p><p id=\"Par39\">Due to its sophisticated handling of multicollinearity and feature selection, Elastic Net is extensively applied in machine learning tasks where the number of predictors is substantial. It is used in genomics to interpret vast gene expression data, ensuring model stability while highlighting predictive genes. It holds value in finance, too, where economic forecasting and asset price modeling demand predictive robustness from correlated market features. Elastic Net is instrumental in ensuring impressive, generalizable results, making it a pragmatic choice for complex systems requiring balanced precision, featured influence, and performance from data-rich in dimensional structure and interweaved variabilities.</p><p id=\"Par40\">Elastic Net&#8217;s combined penalty approach simultaneously empowers it with dimensionality reduction and predictive refinement capabilities, navigating multicollinear complications and ensuring stable prediction outputs. It enhances flexibility in dealing with diverse data structures, balancing sparsity and continuity in modeling approach useful when both feature selection and interpretability matter. However, setting the parameters for L1 and L2 combinations can be data-intensive, requiring expertise to operate efficiently without bias. The increased computational cost due to its dual regularization nature and potential complexity make Elastic Net less suited for very large datasets unless ample processing resources and expert analysis are involved (Qi and Yang <xref ref-type=\"bibr\" rid=\"CR53\">2022</xref>; Mokhtari et al. <xref ref-type=\"bibr\" rid=\"CR50\">2020</xref>).</p></sec><sec id=\"Sec16\"><title>Categorical boosting</title><p id=\"Par41\">It is a specialized gradient-boosting manner that is excellently engineered to manage categorical data, which is typically a challenge to standard gradient-based methods. CatBoost intrinsically incorporates sophisticated algorithmic insights to address categorical feature processing, applying a combination of ordered boosting&#8212;ensuring model stability by preventing overfitting&#8212;and innovative techniques for handling categorical data transformation during learning. Employing permutations and gradient-learning frameworks adeptly, CatBoost constructs a robust learning path, expertly optimizing prediction precision while maintaining a high resilience to typical overfit pitfalls induced within the data transformation processes known to plague standard boosting regimes.</p><p id=\"Par42\">The design of CatBoost features ordered boosting to reflect the temporal sequence of data rather than isolated instances, preserving underlying temporal patterns across data transformations. Its exclusive feature bundling method aggregates sparse categorical features, minimizing dimensionality while maintaining essential data fidelity. This mechanism enhances computational speed and prediction accuracy. CatBoost leverages sophisticated loss function computation enhancements across robust-level ensemble refinement, differentiating itself broadly from typical gradient boosting frameworks&#8212;catapulting precision in categorical transformation outcomes and boosting robustness toward permutation-induced variability for complex practical applications.</p><p id=\"Par43\">CatBoost excels in handling datasets rich with categorical variables and bias prevention needs, facilitating application in a variety of contexts, such as online advertising, where categorical information predominates in click-through predictions. The algorithm also supports customer segmentation tasks and recommendation system improvements across dynamic industries like e-commerce and digital marketing. In domains requiring rigorous analysis and robust prediction across diverse data types, like insurance and healthcare analytics, CatBoost ensures precision in identifying and learning from categorical variable interactions essential to robust decision-making architectures.</p><p id=\"Par44\">CatBoost&#8217;s core strengths lie in its tailored approach to managing categorical variables, effectively preventing overfitting through ordered boosted solutions and precise feature handling. Its adeptness in maintaining high accuracy across complex categorical data domains distinguishes it in market competition layers, offering seamless applications with substantially reduced human intervention necessary for categorical encoding. The challenges with CatBoost center primarily on computational resources cost, it demands ample resources when operated on very extensive datasets. Effective initial setup and parameter tuning are crucial, requiring domain expertise to optimize the model engaging in outcomes better suited for high-value, real-world categorical data deployment tasks (Cha et al. <xref ref-type=\"bibr\" rid=\"CR12\">2021</xref>).</p></sec></sec></sec><sec id=\"Sec17\"><title>Scalability and energy efficiency of hydrochar production</title><p id=\"Par45\">The transition of electrocatalytic reactors from laboratory research to industrial application presents a number of significant engineering challenges. These barriers are not related to the catalyst material itself, but rather to the design of the reactor, including limitations in mass transport, energy losses, and the long-term stability of components.</p><p id=\"Par46\">A primary challenge is managing mass transport, especially for reactions involving gaseous reactants like CO<sub>2</sub>. The low solubility and slow diffusion of these gases in liquid electrolytes can severely limit the reaction rate at high current densities. To overcome this, engineers have developed Gas Diffusion Electrodes (GDEs), which provide a critical interface where gas, liquid, and solid phases meet, allowing for efficient delivery of reactants to the catalyst.</p><p id=\"Par47\">Another major set of barriers involves energy efficiency. There are two main types of energy losses to address. Ohmic losses are caused by electrical resistance in the reactor&#8217;s components, which can be minimized by optimizing electrode spacing and electrolyte conductivity. Kinetic overpotential, on the other hand, is the inherent energy barrier of the reaction itself. This is primarily an issue of catalyst design, and can be improved by developing more active and efficient catalytic materials.</p><p id=\"Par48\">Finally, ensuring the long-term stability and durability of the reactor is a key engineering hurdle. GDEs are particularly vulnerable to degradation. Common failure modes include &#8220;flooding,&#8221; where the pores of the electrode become saturated with electrolyte, blocking gas flow. This is often caused by the degradation of the hydrophobic materials within the GDE. Additionally, carbon-based GDEs can suffer from corrosion in the harsh electrochemical environment, leading to a loss of structural integrity. To address these issues, research is focused on developing more durable, and even carbon-free, GDEs.</p></sec><sec id=\"Sec18\"><title>Scalability and energy efficiency of hydrochar production</title><p id=\"Par49\">The practical applicability and commercial viability of hydrochar production via hydrothermal carbonization (HTC) are contingent on the process&#8217;s scalability and energy efficiency, aspects often overlooked in laboratory-scale studies. The HTC process offers a distinct advantage over other thermochemical methods, such as pyrolysis, by its ability to process wet biomass feedstocks without an energy-intensive pre-drying step. This inherent efficiency is a key driver for the technology&#8217;s potential for industrial-scale deployment.</p><p id=\"Par50\">The HTC reaction itself is exothermic, releasing energy that can be captured and recycled to heat incoming feedstock, thereby reducing overall energy consumption. In a large-scale implementation with sewage sludge, it was shown that only about 20% of the fuel energy content of the final hydrochar product was required to power the process. Specific energy consumption figures from a study on grape marc showed thermal energy and power consumption of 1170 kWh and 160 kWh, respectively, per ton of hydrochar produced. When compared to conventional methods for processing wet waste, HTC can reduce energy consumption for drying by up to 70% by leveraging heat recirculation.</p><p id=\"Par51\">The transition of HTC from laboratory to commercial scale, while challenging, is well underway. Key challenges include optimizing reactor design, managing the process water, and ensuring consistent product quality. Despite these obstacles, numerous companies have successfully developed and implemented industrial-scale HTC plants, validating the technology&#8217;s scalability. The process is considered an effective solution for converting a wide range of biomass into valuable materials due to its relatively mild process conditions and industrial scalability.</p><p id=\"Par52\">From an economic perspective, cost&#8211;benefit analyses demonstrate the potential for hydrochar to be a profitable product. One techno-economic assessment found a breakeven selling price of $117 per ton for co-hydrochar, while another study calculated a production cost of 157 &#8364;/ton with a break-even point of 200 &#8364;/ton for pelletized hydrochar. The specific mechanical energy required for the process has been reported to be in the range of 83.5 to 152.3 kWh/t, directly addressing the importance of energy cost metrics. Furthermore, one analysis found the total energy consumption per ton of hydrochar-derived sand to be 695.14 kWh, with an energy cost of approximately $41.70 per ton, demonstrating the potential for significant energy costs in downstream processing.</p><p id=\"Par53\">This integration of process efficiency and economic feasibility is critical for the long-term adoption of hydrochar as a sustainable product. By converting low-value, high-moisture biomass into a stable, energy-dense solid fuel, HTC offers a solution that not only mitigates waste disposal costs but also contributes to a circular economy by creating a valuable product with a positive energy balance.</p></sec><sec id=\"Sec19\"><title>Data investigation and clarification</title><p id=\"Par54\">The data required to build the models were earlier compiled in Shafizadeh et al. (<xref ref-type=\"bibr\" rid=\"CR60\">2023</xref>). In this research, the independent inputs include reaction processing circumstances and biomass characteristics. Biomass features are categorized into proximate and ultimate analyses, with the proximate investigation covering fixed carbon, volatile matter, and ash content (all in wt%). For this work, the proximate features are selected as input. As such, the input features are volatile matter, fixed carbon, water content, reaction-time, temperature, and ash-content. The outputs are hydrochar- higher-heating-value (HHV, in MJ/kg) and hydrochar yield (in wt%). The data-bank consists of 481 rows.</p><p id=\"Par55\">Figure&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref>A, <xref rid=\"Fig2\" ref-type=\"fig\">B</xref> present the boxplots of all parameters reflected in the modelling of HHV and yield. In current study, a total of 337 data points were utilized for model training, while 72 points were assigned to both testing and validation processes.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;2</label><caption><p>Matrix-plot for <bold>A</bold> yield <bold>B</bold> HHV</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e805\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig2a_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e806\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig2b_HTML.jpg\"/></fig></p><p id=\"Par56\">Figure&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref> illustrates the Pearson correlation coefficients analyzed in this study. The Pearson coefficient is a statistical measure used to quantify the power and direction of the linear relationship between two parameters. This coefficient ranges from&#8201;&#8722;&#8201;1 to&#8201;+&#8201;1, where&#8201;+&#8201;1 indicates a perfect positive linear correlation,&#8201;&#8722;&#8201;1 represents a perfect negative linear correlation, and 0 signifies the absence of any relation. The following eaution describes the calculating of the Pearson correlation (Abbasi et al. <xref ref-type=\"bibr\" rid=\"CR1\">2023</xref>; Bemani et al. <xref ref-type=\"bibr\" rid=\"CR10\">2023</xref>; Madani and Alipour <xref ref-type=\"bibr\" rid=\"CR48\">2022</xref>; Madani et al. <xref ref-type=\"bibr\" rid=\"CR49\">2021</xref>):<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;3</label><caption><p>Matrix explaining the correlation quantities related to model <bold>A</bold> yield and <bold>B</bold> HHV</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e838\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig3a_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e839\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig3b_HTML.jpg\"/></fig><disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e840\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${r}_{j}=\\frac{\\sum_{i=1}^{n}({I}_{i.j}-\\overline{{I }_{j}})({Z}_{i}-\\overline{Z })}{\\sqrt{\\sum_{i=1}^{n}{({I}_{i.j}-\\overline{{I }_{j}})}^{2}\\sum_{i=1}^{n}{({Z}_{i}-\\overline{Z })}^{2}}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par57\">In this formula, Z represents the second variable and is characterized by its averaged amount, Z&#772;, in which &#298; denotes the average amount of the approximate Ij. The relationships among the inputs and the target parameters namely, HHV and yield&#8212;are particularly noteworthy. As shown in Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>A, it is evident that each input variable exhibits some degree of correlation with hydrochar yield, with the exception of Ash content, that shows a positive correlation. Other inputs demonstrate a negative correlation with hydrochar yield. Similarly, Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>B highlights the correlations among hydrochar HHV and input variables. In this case, parameters such as Temperature, Fixed Carbon, and Volatile Matter exhibit a positive correlation with HHV, while the remaining factors present a negative correlation.</p><p id=\"Par58\">Before developing models based on data, it is crucial to guarantee the data quality by identifying and tackling potential outliers. In this work, the Monte Carlo Outlier Detection (MCOD) was utilized due to its success and efficiency when dealing with large data. MCOD syndicates random sampling with density based manners to detect anomalies, defining outliers as data points that deviate significantly from their neighbors in terms of local density. These anomalies are identified by examining the distribution of data points within their local neighborhood.</p><p id=\"Par59\">The MCOD algorithm leverages Monte Carlo sampling to select a representative subset of the data, thereby reducing the computational burden significantly. Its primary advantage lies in its efficiency, mostly for high-dimension datasets. By means of random sampling, MCOD evades the need to examine the full dataset directly, resulting in lower computational costs and making it especially appropriate for real-time applications. However, there are trade-offs; the accuracy of the results may depend on factors such as the number of samples and the arrangement of the adjacent neighbors (k). Nevertheless, MCOD remains an invaluable tool for data investigation and outlier detection, principally in scenarios where estimated results are acceptable or computational resources are limited. Its balance between computational effectiveness and detection accuracy is a key asset in identifying outliers in complex data-sets.</p><p id=\"Par60\">Figure&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref>A, <xref rid=\"Fig4\" ref-type=\"fig\">B</xref> present boxplots of the data used for yield and HHV modeling, showcasing the spreading of points and highlighting the ranges suitable for model development. The boxplots indicate that the majority of points fall within the expected range, affirming the consistency and superiority of the data. For training, the entire dataset was utilized to ensure robust model developing. By incorporating the full range of data, the models are equipped to capture essential patterns and variability, enhancing their ability to oversimplify effectively to unseen data and improving calculation accuracy (Jia et al. <xref ref-type=\"bibr\" rid=\"CR36\">2018</xref>; Rocco and Moreno <xref ref-type=\"bibr\" rid=\"CR55\">2002</xref>).<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;4</label><caption><p>Outlier detection via the Monte Carlo and boxplots for the <bold>A</bold> yield and <bold>B</bold> HHV data validated the data&#8217;s spreading and confirmed its appropriateness for building dependable models</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig4_HTML.jpg\"/></fig></p></sec><sec id=\"Sec20\"><title>Results and discussions</title><p id=\"Par61\">This research uses diverse methods, from linear regression to convolutional neural networks and collective techniques (LightGBM, XGBoost, CatBoost), to forecast hydrochar higher heating value (HHV) and yield from biomass. This approach allows for a inclusive evaluation of model performance. Model accuracy and explanatory power are assessed using the mean squared error (MSE), coefficient of determination (R<sup>2</sup>), and mean relative deviation percent (MRD%). R<sup>2</sup> measures the model&#8217;s capacity to clarify data variance, while MSE quantifies prediction accuracy, providing a benchmark for comparing models across datasets. The research includes detailed calculations and interpretations of the metrics to clarify implications for HHV and yield prediction (Feller et al. <xref ref-type=\"bibr\" rid=\"CR22\">2004</xref>; Aghdam et al. <xref ref-type=\"bibr\" rid=\"CR2\">2022</xref>; Bassir and Madani <xref ref-type=\"bibr\" rid=\"CR9\">2019</xref>; Shoushtari et al. <xref ref-type=\"bibr\" rid=\"CR62\">2020</xref>).<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e905\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ R{ - }squared\\;\\left( {R^{2} } \\right) = 1 - \\frac{{\\mathop \\sum \\nolimits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - {\\text{y}}_{{\\text{i}}}^{{{\\text{predicted}}}} } \\right)^{2} }}{{\\mathop \\sum \\nolimits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - \\overline{{{\\text{y}}^{{{\\text{real}}}} }} } \\right)^{2} }} $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e909\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Mean\\;squared\\;error\\;\\left( {MSE} \\right) = \\frac{1}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {y_{i}^{real} - y_{i}^{predicted} } \\right)^{2} $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e913\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Standard\\;Deviation\\;\\left( \\sigma \\right) = \\sqrt {\\frac{1}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - \\overline{{{\\text{y}}^{{{\\text{real}}}} }} } \\right)^{2} } $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e917\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Mean\\;relative\\;deviation\\;\\left( {MRE} \\right) = \\frac{100}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {\\frac{{y_{i}^{real} - y_{i}^{predicted} }}{{y_{i}^{real} }}} \\right) $$\\end{document}</tex-math></disp-formula></p><p id=\"Par62\">where, <inline-formula id=\"IEq1\"><tex-math id=\"d33e923\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{y}}}_{{\\varvec{i}}}^{{\\varvec{r}}{\\varvec{e}}{\\varvec{a}}{\\varvec{l}}}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq2\"><tex-math id=\"d33e927\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\varvec{y}}}_{{\\varvec{i}}}^{{\\varvec{p}}{\\varvec{r}}{\\varvec{e}}{\\varvec{d}}{\\varvec{i}}{\\varvec{c}}{\\varvec{t}}{\\varvec{e}}{\\varvec{d}}}$$\\end{document}</tex-math></inline-formula> indicating the real and model output, respectively. Also, N signifies the dataset&#8217;s size.</p><p id=\"Par63\">A comparative investigation of several regression models was executed to evaluate their predictive performance on a benchmark dataset. The detailed results, summarized in Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> (for biomass yield prediction) and Table&#160;<xref rid=\"Tab2\" ref-type=\"table\">2</xref> (for biomass HHV prediction), highlight CatBoost as the best model for both yield and HHV prediction tasks. CatBoost demonstrated a strong correlation with the actual values, along with minimal standard deviation, indicating its high stability and accuracy.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table&#160;1</label><caption><p>Created specific calculation metrics to appraise the efficiency of each technique on yield prediction</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train MRD%</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation MRD%</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test MRD%</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ANN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">24.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">33.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">34.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.93</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">18.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">20.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">31.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Linear Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.24</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.36</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.13</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">183.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">184.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">258.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">23.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">24.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ridge Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.24</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.34</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.14</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">184.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">190.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">254.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">23.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">25.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lasso Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.24</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.36</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.13</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">184.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">185.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">257.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">23.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">24.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Elastic Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.23</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.34</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.14</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">185.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">192.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">254.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">23.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">25.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SVR</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.39</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.42</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.28</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">148.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">166.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">213.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">21.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">23.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">27.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Random Forest</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.85</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.84</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.83</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">35.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">45.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">51.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Gradient Boosting</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.77</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.74</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.77</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">56.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">KNN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.71</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.69</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.72</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">70.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">91.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">13.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">15.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Decision Tree</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.66</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.71</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.73</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">XGBoost</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.81</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.85</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.82</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">46.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">42.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">53.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LightGBM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.84</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">25.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">33.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">46.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CatBoost</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">22.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">32.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.1</td></tr></tbody></table></table-wrap><table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table&#160;2</label><caption><p>Created specific calculation metrics to review the effectiveness of each technique</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test R<sup>2</sup></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test MSE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train MRD%</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation MRD%</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Test MRD%</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ANN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.85</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.82</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.93</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Linear Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.57</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.67</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.58</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ridge Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.57</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.67</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.58</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lasso Regression</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.57</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.67</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.58</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Elastic Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.57</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.67</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.58</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SVR</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.65</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.80</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.63</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Random Forest</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.93</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.97</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.79</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Gradient Boosting</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.84</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">KNN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.82</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.82</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Decision Tree</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.70</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">XGBoost</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.86</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.79</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LightGBM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.95</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CatBoost</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.97</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.93</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ANN</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.85</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.82</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.1</td></tr></tbody></table></table-wrap></p><p id=\"Par64\">Specifically, Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> shows that CatBoost achieved an R<sup>2</sup> value of 0.90 for biomass yield prediction, while Table&#160;<xref rid=\"Tab2\" ref-type=\"table\">2</xref> indicates an R<sup>2</sup> value of 0.93 for HHV prediction. These values suggest that CatBoost effectively captures the basic relations in the data. It also exhibited the lowest MSE values, 30.38 for yield prediction and 1.63 for HHV prediction. Additionally, the Mean Relative Deviation (MRD) values were minimal, with 8.09% for yield prediction and 3.4% for HHV prediction, further confirming its superior performance in both regression tasks.</p><p id=\"Par65\">Conversely, simpler models including Lasso Regression, Linear Regression, Ridge Regression, Decision Trees, Elastic Net, and K-Nearest Neighbors (KNN) showed significantly poorer performance. These models showed lower R<sup>2</sup> values and higher MSE values for both yield and HHV predictions. This performance disparity is consistent with prior research, which has demonstrated that gradient-boosting methods like CatBoost tend to outperform traditional models in complex regression tasks. The findings in this study emphasize the advantages of using advanced models like CatBoost for regression tasks in biomass yield and HHV prediction, especially when compared to simpler, more traditional approaches (Ajin et al. <xref ref-type=\"bibr\" rid=\"CR5\">2024</xref>; Vishwakarma et al. <xref ref-type=\"bibr\" rid=\"CR71\">2024</xref>).</p><p id=\"Par66\">Current work hires graphical techniques, such as cross plots and relative deviation values, to evaluate the reliability of various algorithms in predicting hydrochar HHV and yield biomass proximate analysis. These visual tools are instrumental in providing a comprehensive understanding of the models&#8217; performances, effectively highlighting patterns and discrepancies between the predicted and real data.</p><p id=\"Par67\">Figures&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref> and <xref rid=\"Fig6\" ref-type=\"fig\">6</xref> present a comparison of the calculated and real data across different datasets, for all established models in predicting yield and HHV. The investigation reveals that the CatBoost shows near-perfect alignment between the real and modeled reults, demonstrating its great abilities. This close agreement underscores CatBoost&#8217;s strength in capturing the complicated relations within the data.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;5</label><caption><p>The accuracy of models for HHV prediction determined by comparing their predicted outcomes against observed values</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1680\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig5a_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1681\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig5b_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1682\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig5c_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1683\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig5d_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1684\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig5e_HTML.jpg\"/></fig><fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;6</label><caption><p>The accuracy of models for HHV prediction determined by comparing their predicted outcomes against observed values</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1691\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig6a_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1692\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig6b_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1693\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig6c_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1694\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig6d_HTML.jpg\"/><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1695\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig6e_HTML.jpg\"/></fig></p><p id=\"Par68\">Additionally, Figs.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref> and <xref rid=\"Fig8\" ref-type=\"fig\">8</xref> introduce cross-validation plots that display the correlations between real and calculated amounts for all models in predicting hydrochar yield and HHV. The CatBoost shows a clear trend where data points for both parameters cluster tightly around the y&#8201;=&#8201;x bisector, with the fitted lines strictly mirroring this ideal relationship.This alignment serves as a testament to the model&#8217;s strong prediction, confirming its proficiency in accurately forecasting outcomes that align with real-world observations.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;7</label><caption><p>A visual assessment of the model&#8217;s predictive capability for yield, performed using cross-plots</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig7_HTML.jpg\"/></fig><fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;8</label><caption><p>A visual assessment of the model&#8217;s predictive capability for HHV performed using cross-plots</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig8_HTML.jpg\"/></fig></p><p id=\"Par69\">Figures&#160;<xref rid=\"Fig9\" ref-type=\"fig\">9</xref> and <xref rid=\"Fig10\" ref-type=\"fig\">10</xref> explore error distributions over scatter plot of relative errors in predicting HHV and yield. In the case of the CatBoost model, the errors are evenly spread nearby the x-axis, signifying minimal variance and reliable prediction accuracy. This further reinforces the reliability of the model, enhancing confidence in its forecasting ability.<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;9</label><caption><p>A comprehensive analysis of relative deviation percentages provided for all models employed in biomass yield prediction</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO9\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig9_HTML.jpg\"/></fig><fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;10</label><caption><p>Relative deviation percentages meticulously detailed for the cohorts across the entire models employed for biomass yield prediction</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO10\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig10_HTML.jpg\"/></fig></p><p id=\"Par70\">Lastly, Figs.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref> and <xref rid=\"Fig12\" ref-type=\"fig\">12</xref> illustrate the prediction distribution of all models across the different modeling phases for forcasing the yield and HHV. The performance of models in predicting HHV is evaluated by comparing the models&#8217; outputs with empirical observations.This consistency highlights CatBoost&#8217;s robustness and stability, positioning it as the most reliable model for real-world usages.<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;11</label><caption><p>Information on the frequency of the data sets employed for biomass yield</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO11\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig11_HTML.jpg\"/></fig><fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;12</label><caption><p>Information on the frequency of the data sets employed for biomass HHV prediction</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO12\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig12_HTML.jpg\"/></fig></p><p id=\"Par71\">To truly understand how machine learning models make predictions, it&#8217;s essential to know the significance of each input feature. In this research, we utilize Shapley Additive exPlanations (SHAP). This powerful method, rooted in game-theoretic principles, provides a clear and rigorous framework for interpreting model outputs, revealing precisely how individual features contribute to predictions.</p><p id=\"Par72\">Figure <xref rid=\"Fig13\" ref-type=\"fig\">13</xref>A, B display the SHAP results for the inputs, along with the feature status derived from the Random Forest for calculating hydrochar HHV and yield regarding biomass proximate. The parameters are ranked in downward order according to their SHAP amounts, with the highest-ranked features exerting the most significant influence on model results. The investigation indicates that ash content is the primary determinant for HHV prediction, while temperature, water content, and fixed carbon are identified as the most crucial parameters for predicting yield.</p><p id=\"Par73\">These findings are instrumental in identifying the key factors that affect hydrochar HHV and yield in the context of biomass proximate analysis. By providing intuitions into the relationships between inputs and desired variables, better performance is facilitated, enhancing its utility in real-world industries. Additionally, clarity regarding feature significance is essential for guiding future research and optimization strategies, enabling decisions by quantifying the unique influence of each parameter on predictions.<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig.&#160;13</label><caption><p>Random Forest and Mean SHAP Insights into feature contributions for prediction <bold>A</bold> yield and <bold>B</bold> HHV parameters</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO13\" position=\"float\" orientation=\"portrait\" xlink:href=\"40643_2025_979_Fig13_HTML.jpg\"/></fig></p><p id=\"Par74\">The predictive power of our machine learning models stems from the inherent relationships between the proximate analysis of the raw biomass and the fundamental chemical and physical changes occurring during hydrothermal carbonization (HTC). A deeper mechanistic understanding of these relationships is essential for a more complete interpretation of our model&#8217;s predictions.</p><p id=\"Par75\">The fixed carbon (FC) content of the biomass is a key parameter directly influencing the final HHV of the hydrochar. Fixed carbon represents the non-volatile combustible material left after the release of volatile matter. During the HTC process, while some devolatilization occurs, the fixed carbon fraction remains largely intact. As a result, biomass with a higher initial FC content will naturally produce a hydrochar with a greater proportion of fixed carbon, thereby increasing its energy density and resulting in a higher HHV. Our models effectively capture this direct correlation, demonstrating that a higher FC content in the feedstock is a strong positive predictor for the final hydrochar HHV.</p><p id=\"Par76\">Conversely, the volatile matter (VM) content is a primary factor governing the final hydrochar yield. Volatile matter consists of organic compounds that are released as gases and liquids during the HTC process. This release of VM from the solid biomass matrix represents a significant mass loss. Consequently, biomass feedstocks with higher volatile matter content will undergo a greater degree of devolatilization and decomposition, leading to a lower overall hydrochar yield. Our models&#8217; ability to predict yield based on VM content highlights the critical role of this parameter in determining the efficiency of solid fuel recovery during HTC.</p><p id=\"Par77\">Finally, the ash content and moisture content of the biomass also play important, albeit different, roles. Ash is an inert component that does not contribute to the combustion process. Therefore, a higher ash content in the feedstock dilutes the combustible fixed carbon and volatile matter in the resulting hydrochar, which invariably lowers its HHV. This effect is consistently captured by our models, reinforcing the importance of using low-ash biomass for high-quality hydrochar production. The initial moisture content acts as the reaction medium for the HTC process. While not a direct component of the final hydrochar&#8217;s chemical makeup, it influences the reaction kinetics, including the temperature and pressure profiles. These conditions affect the extent of carbonization and dehydration reactions, ultimately having a secondary but significant impact on both the hydrochar yield and HHV. The success of our models in predicting these properties from moisture content suggests that the models have learned these complex, indirect relationships.</p><p id=\"Par78\">This mechanistic analysis validates our models&#8217; predictive capabilities and provides practical insights for optimizing the hydrochar production process. By controlling the input biomass&#8217;s proximate analysis, particularly by selecting feedstocks with high fixed carbon, low volatile matter, and low ash content, it is possible to produce a high-yield, high-HHV hydrochar, thereby maximizing the efficiency of this sustainable energy conversion pathway.</p></sec><sec id=\"Sec21\"><title>Conclusion</title><p id=\"Par79\">This research delivers a inclusive investigation of biomass proximate analysis for forecasting hydrochar-yield and higher-heating-value (HHV) by machine learning. Key parameters such as fixed carbon, temperature,volatile matter, ash content, reaction time, and water content were analyzed, with Pearson correlation revealing their influence on hydrochar yield and HHV. The Monte Carlo Outlier Detecting (MCOD) ensured data reliability, enhancing the success of the modeling. Among the models evaluated, CatBoost outperformed others with R<sup>2</sup> values of 0.90 (yield prediction) and 0.93 (HHV prediction), as well as low mean relative deviation (MRD) and mean squared error (MSE). The study highpoints the superiority of gradient-boosting methods over simpler models in capturing complex relationships. Visual validation tools further confirmed CatBoost&#8217;s accuracy and consistency, while SHAP analysis identified ash content as the key factor in HHV prediction and temperature, water content, and fixed carbon as critical for yield prediction. In conclusion, this research emphasizes the potential of advanced machine learning models, particularly gradient-boosting algorithms like CatBoost, in optimizing biomass-to-hydrochar conversion processes. These findings hold significant implications for improving bioenergy production and guiding future research in biomass modeling and conversion.</p></sec><sec id=\"Sec22\" sec-type=\"supplementary-material\"><title>Supplementary Information</title><p>\n<supplementary-material content-type=\"local-data\" id=\"MOESM1\" position=\"float\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"40643_2025_979_MOESM1_ESM.xlsx\" position=\"float\" orientation=\"portrait\"><caption><p>Additional file1.</p></caption></media></supplementary-material>\n</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was supported by the Natural Science Foundation of Changchun Normal University (2021003).</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>GH and AA performed software and coding. SH, SS, AS did the formal study. AA, VS, VJ provided the methodology and exact steps needed to be done. SH, VS, VJ, AK curated the data and validated the results. AY, SS, AS, GH visualized the plots and wrote the manuscript. AK finalized the manuscript and supervised the whole project. All authors read and approved the final manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>Data is available in the supplementary material file.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\"><title>Ethics approval and consent to participate</title><p id=\"Par80\">Not applicable.</p></notes><notes id=\"FPar2\"><title>Consent for publication</title><p id=\"Par81\">All authors agree to publish this work.</p></notes><notes id=\"FPar3\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par82\">None.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abbasi</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Aghdam</surname><given-names>SK-y</given-names></name><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name></person-group><article-title>Modeling subcritical multi-phase flow through surface chokes with new production parameters</article-title><source>Flow Meas Instrum</source><year>2023</year><volume>89</volume><fpage>102293</fpage><pub-id pub-id-type=\"doi\">10.1016/j.flowmeasinst.2022.102293</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Abbasi P, Aghdam SK-y, Madani M (2023) Modeling subcritical multi-phase flow through surface chokes with new production parameters. Flow Meas Instrum 89:102293</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Aghdam</surname><given-names>SK-y</given-names></name><etal/></person-group><article-title>Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study</article-title><source>Arab J Sci Eng</source><year>2022</year><pub-id pub-id-type=\"doi\">10.1007/s13369-022-07552-4</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Aghdam SK-y et al (2022) Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study. Arab J Sci Eng. 10.1007/s13369-022-07552-4</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Aghdam</surname><given-names>SK-y</given-names></name><etal/></person-group><article-title>Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study</article-title><source>Arab J Sci Eng</source><year>2023</year><volume>48</volume><issue>7</issue><fpage>9461</fpage><lpage>9476</lpage><pub-id pub-id-type=\"doi\">10.1007/s13369-022-07552-4</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Aghdam SK-y et al (2023) Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study. Arab J Sci Eng 48(7):9461&#8211;9476</mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ahmadi</surname><given-names>MA</given-names></name><etal/></person-group><article-title>Evolving artificial neural network and imperialist competitive algorithm for prediction oil flow rate of the reservoir</article-title><source>Appl Soft Comput</source><year>2013</year><volume>13</volume><issue>2</issue><fpage>1085</fpage><lpage>1098</lpage><pub-id pub-id-type=\"doi\">10.1016/j.asoc.2012.10.009</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Ahmadi MA et al (2013) Evolving artificial neural network and imperialist competitive algorithm for prediction oil flow rate of the reservoir. Appl Soft Comput 13(2):1085&#8211;1098</mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ajin</surname><given-names>RS</given-names></name><name name-style=\"western\"><surname>Segoni</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Fanti</surname><given-names>R</given-names></name></person-group><article-title>Optimization of SVR and CatBoost models using metaheuristic algorithms to assess landslide susceptibility</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>24851</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-72663-x</pub-id><pub-id pub-id-type=\"pmid\">39438526</pub-id><pub-id pub-id-type=\"pmcid\">PMC11496660</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Ajin RS, Segoni S, Fanti R (2024) Optimization of SVR and CatBoost models using metaheuristic algorithms to assess landslide susceptibility. Sci Rep 14(1):24851<pub-id pub-id-type=\"pmid\">39438526</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-72663-x</pub-id><pub-id pub-id-type=\"pmcid\">PMC11496660</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><mixed-citation publication-type=\"other\">Aloysius N, Geetha M (2017) A review on deep convolutional neural networks. In: 2017 International Conference on Communication and Signal Processing (ICCSP).</mixed-citation></ref><ref id=\"CR7\"><citation-alternatives><element-citation id=\"ec-CR7\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>The linear random forest algorithm and its advantages in machine learning assisted logging regression modeling</article-title><source>J Pet Sci Eng</source><year>2019</year><volume>174</volume><fpage>776</fpage><lpage>789</lpage><pub-id pub-id-type=\"doi\">10.1016/j.petrol.2018.11.067</pub-id></element-citation><mixed-citation id=\"mc-CR7\" publication-type=\"journal\">Ao Y et al (2019) The linear random forest algorithm and its advantages in machine learning assisted logging regression modeling. J Pet Sci Eng 174:776&#8211;789</mixed-citation></citation-alternatives></ref><ref id=\"CR8\"><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bansal</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Goyal</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title>A comparative analysis of K-nearest neighbor, genetic, support vector machine, decision tree, and long short term memory algorithms in machine learning</article-title><source>Decis Anal J</source><year>2022</year><volume>3</volume><fpage>100071</fpage><pub-id pub-id-type=\"doi\">10.1016/j.dajour.2022.100071</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Bansal M, Goyal A, Choudhary A (2022) A comparative analysis of K-nearest neighbor, genetic, support vector machine, decision tree, and long short term memory algorithms in machine learning. Decis Anal J 3:100071</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bassir</surname><given-names>SM</given-names></name><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name></person-group><article-title>Predicting asphaltene precipitation during titration of diluted crude oil with paraffin using artificial neural network (ANN)</article-title><source>Pet Sci Technol</source><year>2019</year><volume>37</volume><issue>24</issue><fpage>2397</fpage><lpage>2403</lpage><pub-id pub-id-type=\"doi\">10.1080/10916466.2019.1570261</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Bassir SM, Madani M (2019) Predicting asphaltene precipitation during titration of diluted crude oil with paraffin using artificial neural network (ANN). Pet Sci Technol 37(24):2397&#8211;2403</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bemani</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Kazemi</surname><given-names>A</given-names></name></person-group><article-title>Machine learning-based estimation of nano-lubricants viscosity in different operating conditions</article-title><source>Fuel</source><year>2023</year><volume>352</volume><fpage>129102</fpage><pub-id pub-id-type=\"doi\">10.1016/j.fuel.2023.129102</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Bemani A, Madani M, Kazemi A (2023) Machine learning-based estimation of nano-lubricants viscosity in different operating conditions. Fuel 352:129102</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bhutto</surname><given-names>AW</given-names></name><etal/></person-group><article-title>Promoting sustainability of use of biomass as energy resource: Pakistan&#8217;s perspective</article-title><source>Environ Sci Pollut Res Int</source><year>2019</year><volume>26</volume><fpage>29606</fpage><lpage>29619</lpage><pub-id pub-id-type=\"doi\">10.1007/s11356-019-06179-7</pub-id><pub-id pub-id-type=\"pmid\">31452125</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Bhutto AW et al (2019) Promoting sustainability of use of biomass as energy resource: Pakistan&#8217;s perspective. Environ Sci Pollut Res Int 26:29606&#8211;29619<pub-id pub-id-type=\"pmid\">31452125</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11356-019-06179-7</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cha</surname><given-names>G-W</given-names></name><name name-style=\"western\"><surname>Moon</surname><given-names>H-J</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>Y-C</given-names></name></person-group><article-title>Comparison of random forest and gradient boosting machine models for predicting demolition waste based on small datasets and categorical variables</article-title><source>Int J Environ Res Public Health</source><year>2021</year><pub-id pub-id-type=\"doi\">10.3390/ijerph18168530</pub-id><pub-id pub-id-type=\"pmid\">34444277</pub-id><pub-id pub-id-type=\"pmcid\">PMC8392226</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Cha G-W, Moon H-J, Kim Y-C (2021) Comparison of random forest and gradient boosting machine models for predicting demolition waste based on small datasets and categorical variables. Int J Environ Res Public Health. 10.3390/ijerph18168530<pub-id pub-id-type=\"pmid\">34444277</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/ijerph18168530</pub-id><pub-id pub-id-type=\"pmcid\">PMC8392226</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><mixed-citation publication-type=\"other\">Chagas P et al. (2018) Evaluation of convolutional neural network architectures for chart image classification. In: 2018 International Joint Conference on Neural Networks (IJCNN).</mixed-citation></ref><ref id=\"CR14\"><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><etal/></person-group><article-title>A comparison of linear regression, regularization, and machine learning algorithms to develop Europe-wide spatial models of fine particles and nitrogen dioxide</article-title><source>Environ Int</source><year>2019</year><volume>130</volume><fpage>104934</fpage><pub-id pub-id-type=\"doi\">10.1016/j.envint.2019.104934</pub-id><pub-id pub-id-type=\"pmid\">31229871</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Chen J et al (2019) A comparison of linear regression, regularization, and machine learning algorithms to develop Europe-wide spatial models of fine particles and nitrogen dioxide. Environ Int 130:104934<pub-id pub-id-type=\"pmid\">31229871</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.envint.2019.104934</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>W-H</given-names></name><etal/></person-group><article-title>A comparative analysis of biomass torrefaction severity index prediction from machine learning</article-title><source>Appl Energy</source><year>2022</year><volume>324</volume><fpage>119689</fpage><pub-id pub-id-type=\"doi\">10.1016/j.apenergy.2022.119689</pub-id></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Chen W-H et al (2022a) A comparative analysis of biomass torrefaction severity index prediction from machine learning. Appl Energy 324:119689</mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>W-H</given-names></name><etal/></person-group><article-title>Forecast of glucose production from biomass wet torrefaction using statistical approach along with multivariate adaptive regression splines, neural network and decision tree</article-title><source>Appl Energy</source><year>2022</year><volume>324</volume><fpage>119775</fpage><pub-id pub-id-type=\"doi\">10.1016/j.apenergy.2022.119775</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Chen W-H et al (2022b) Forecast of glucose production from biomass wet torrefaction using statistical approach along with multivariate adaptive regression splines, neural network and decision tree. Appl Energy 324:119775</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dou</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Development and process simulation of a biomass driven SOFC-based electricity and ammonia production plant using green hydrogen; AI-based machine learning-assisted tri-objective optimization</article-title><source>Int J Hydrogen Energy</source><year>2025</year><volume>133</volume><fpage>440</fpage><lpage>457</lpage><pub-id pub-id-type=\"doi\">10.1016/j.ijhydene.2025.04.497</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Dou Z et al (2025) Development and process simulation of a biomass driven SOFC-based electricity and ammonia production plant using green hydrogen; AI-based machine learning-assisted tri-objective optimization. Int J Hydrogen Energy 133:440&#8211;457</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Emmert-Streib</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Dehmer</surname><given-names>M</given-names></name></person-group><article-title>High-dimensional LASSO-based computational regression models: regularization, shrinkage, and selection</article-title><source>Machine Learn Knowl Extraction</source><year>2019</year><volume>1</volume><issue>1</issue><fpage>359</fpage><lpage>383</lpage><pub-id pub-id-type=\"doi\">10.3390/make1010021</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Emmert-Streib F, Dehmer M (2019) High-dimensional LASSO-based computational regression models: regularization, shrinkage, and selection. Machine Learn Knowl Extraction 1(1):359&#8211;383</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fan</surname><given-names>J</given-names></name><etal/></person-group><article-title>Light gradient boosting machine: an efficient soft computing model for estimating daily reference evapotranspiration with local and external meteorological data</article-title><source>Agric Water Manage</source><year>2019</year><volume>225</volume><fpage>105758</fpage><pub-id pub-id-type=\"doi\">10.1016/j.agwat.2019.105758</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Fan J et al (2019) Light gradient boosting machine: an efficient soft computing model for estimating daily reference evapotranspiration with local and external meteorological data. Agric Water Manage 225:105758</mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Feller</surname><given-names>N</given-names></name><etal/></person-group><article-title>MRD parameters using immunophenotypic detection methods are highly reliable in predicting survival in acute myeloid leukaemia</article-title><source>Leukemia</source><year>2004</year><volume>18</volume><issue>8</issue><fpage>1380</fpage><lpage>1390</lpage><pub-id pub-id-type=\"doi\">10.1038/sj.leu.2403405</pub-id><pub-id pub-id-type=\"pmid\">15201848</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Feller N et al (2004) MRD parameters using immunophenotypic detection methods are highly reliable in predicting survival in acute myeloid leukaemia. Leukemia 18(8):1380&#8211;1390<pub-id pub-id-type=\"pmid\">15201848</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/sj.leu.2403405</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><mixed-citation publication-type=\"other\">Feng W et al (2020) FSRF: An improved random forest for classification. In: 2020 IEEE International Conference on Advances in Electrical Engineering and Computer Applications( AEECA).</mixed-citation></ref><ref id=\"CR24\"><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gardner</surname><given-names>MW</given-names></name><name name-style=\"western\"><surname>Dorling</surname><given-names>SR</given-names></name></person-group><article-title>Artificial neural networks (the multilayer perceptron)&#8212;a review of applications in the atmospheric sciences</article-title><source>Atmos Environ</source><year>1998</year><volume>32</volume><issue>14</issue><fpage>2627</fpage><lpage>2636</lpage><pub-id pub-id-type=\"doi\">10.1016/S1352-2310(97)00447-0</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Gardner MW, Dorling SR (1998) Artificial neural networks (the multilayer perceptron)&#8212;A review of applications in the atmospheric sciences. Atmos Environ 32(14):2627&#8211;2636. 10.1016/S1352-2310(97)00447-0</mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ghiasi</surname><given-names>MM</given-names></name><name name-style=\"western\"><surname>Zendehboudi</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Mohsenipour</surname><given-names>AA</given-names></name></person-group><article-title>Decision tree-based diagnosis of coronary artery disease: CART model</article-title><source>Comput Methods Programs Biomed</source><year>2020</year><volume>192</volume><fpage>105400</fpage><pub-id pub-id-type=\"doi\">10.1016/j.cmpb.2020.105400</pub-id><pub-id pub-id-type=\"pmid\">32179311</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Ghiasi MM, Zendehboudi S, Mohsenipour AA (2020) Decision tree-based diagnosis of coronary artery disease: CART model. Comput Methods Programs Biomed 192:105400<pub-id pub-id-type=\"pmid\">32179311</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.cmpb.2020.105400</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><citation-alternatives><element-citation id=\"ec-CR26\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name></person-group><article-title>Comparison of linearization methods for modeling the Langmuir adsorption isotherm</article-title><source>J Mol Liq</source><year>2019</year><volume>296</volume><fpage>111850</fpage><pub-id pub-id-type=\"doi\">10.1016/j.molliq.2019.111850</pub-id></element-citation><mixed-citation id=\"mc-CR26\" publication-type=\"journal\">Guo X, Wang J (2019) Comparison of linearization methods for modeling the Langmuir adsorption isotherm. J Mol Liq 296:111850</mixed-citation></citation-alternatives></ref><ref id=\"CR27\"><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>J</given-names></name><etal/></person-group><article-title>Prediction of heating and cooling loads based on light gradient boosting machine algorithms</article-title><source>Build Environ</source><year>2023</year><volume>236</volume><fpage>110252</fpage><pub-id pub-id-type=\"doi\">10.1016/j.buildenv.2023.110252</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Guo J et al (2023) Prediction of heating and cooling loads based on light gradient boosting machine algorithms. Build Environ 236:110252</mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hai</surname><given-names>A</given-names></name><etal/></person-group><article-title>Machine learning models for the prediction of total yield and specific surface area of biochar derived from agricultural biomass by pyrolysis</article-title><source>Environ Technol Innov</source><year>2023</year><volume>30</volume><fpage>103071</fpage><pub-id pub-id-type=\"doi\">10.1016/j.eti.2023.103071</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Hai A et al (2023) Machine learning models for the prediction of total yield and specific surface area of biochar derived from agricultural biomass by pyrolysis. Environ Technol Innov 30:103071</mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hautaniemi</surname><given-names>S</given-names></name><etal/></person-group><article-title>Modeling of signal&#8211;response cascades using decision tree analysis</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><issue>9</issue><fpage>2027</fpage><lpage>2035</lpage><pub-id pub-id-type=\"doi\">10.1093/bioinformatics/bti278</pub-id><pub-id pub-id-type=\"pmid\">15657095</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Hautaniemi S et al (2005) Modeling of signal&#8211;response cascades using decision tree analysis. Bioinformatics 21(9):2027&#8211;2035<pub-id pub-id-type=\"pmid\">15657095</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/bti278</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><citation-alternatives><element-citation id=\"ec-CR30\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Heidari</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Sobati</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Movahedirad</surname><given-names>S</given-names></name></person-group><article-title>Accurate prediction of nanofluid viscosity using a multilayer perceptron artificial neural network (MLP-ANN)</article-title><source>Chemometr Intell Lab Syst</source><year>2016</year><volume>155</volume><fpage>73</fpage><lpage>85</lpage><pub-id pub-id-type=\"doi\">10.1016/j.chemolab.2016.03.031</pub-id></element-citation><mixed-citation id=\"mc-CR30\" publication-type=\"journal\">Heidari E, Sobati MA, Movahedirad S (2016) Accurate prediction of nanofluid viscosity using a multilayer perceptron artificial neural network (MLP-ANN). Chemometr Intell Lab Syst 155:73&#8211;85</mixed-citation></citation-alternatives></ref><ref id=\"CR31\"><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hoerl</surname><given-names>RW</given-names></name></person-group><article-title>Ridge regression: a historical context</article-title><source>Technometrics</source><year>2020</year><volume>62</volume><issue>4</issue><fpage>420</fpage><lpage>425</lpage><pub-id pub-id-type=\"doi\">10.1080/00401706.2020.1742207</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Hoerl RW (2020) Ridge regression: a historical context. Technometrics 62(4):420&#8211;425</mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hoerl</surname><given-names>AE</given-names></name><name name-style=\"western\"><surname>Kennard</surname><given-names>RW</given-names></name></person-group><article-title>Ridge regression: applications to nonorthogonal problems</article-title><source>Technometrics</source><year>1970</year><volume>12</volume><issue>1</issue><fpage>69</fpage><lpage>82</lpage><pub-id pub-id-type=\"doi\">10.1080/00401706.1970.10488635</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Hoerl AE, Kennard RW (1970a) Ridge regression: applications to nonorthogonal problems. Technometrics 12(1):69&#8211;82</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hoerl</surname><given-names>AE</given-names></name><name name-style=\"western\"><surname>Kennard</surname><given-names>RW</given-names></name></person-group><article-title>Ridge regression: biased estimation for nonorthogonal problems</article-title><source>Technometrics</source><year>1970</year><volume>12</volume><issue>1</issue><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type=\"doi\">10.1080/00401706.1970.10488634</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Hoerl AE, Kennard RW (1970b) Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12(1):55&#8211;67</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Holt</surname><given-names>CA</given-names></name><etal/></person-group><article-title>Use of potassium/lime drilling-fluid system in Navarin Basin drilling</article-title><source>SPE Drill Eng</source><year>1987</year><volume>2</volume><issue>04</issue><fpage>323</fpage><lpage>330</lpage><pub-id pub-id-type=\"doi\">10.2118/14755-PA</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Holt CA et al (1987) Use of potassium/lime drilling-fluid system in Navarin Basin drilling. SPE Drill Eng 2(04):323&#8211;330</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><mixed-citation publication-type=\"other\">Hope TMH (2020) Chapter 4 - Linear regression. In: Machine Learning, Mechelli A, Vieira S, (Eds). Academic Press. p 67&#8211;81.</mixed-citation></ref><ref id=\"CR36\"><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jia</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>J</given-names></name></person-group><article-title>Intelligent interpolation by Monte Carlo machine learning</article-title><source>Geophysics</source><year>2018</year><volume>83</volume><issue>2</issue><fpage>V83</fpage><lpage>V97</lpage><pub-id pub-id-type=\"doi\">10.1190/geo2017-0294.1</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Jia Y, Yu S, Ma J (2018) Intelligent interpolation by Monte Carlo machine learning. Geophysics 83(2):V83&#8211;V97</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><mixed-citation publication-type=\"other\">Kang J et al LASSO-Based machine learning algorithm for prediction of lymph node metastasis in T1 colorectal cancer. crt, 2020. <bold>53</bold>(3): p. 773&#8211;783.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.4143/crt.2020.974</pub-id><pub-id pub-id-type=\"pmcid\">PMC8291173</pub-id><pub-id pub-id-type=\"pmid\">33421980</pub-id></mixed-citation></ref><ref id=\"CR38\"><mixed-citation publication-type=\"other\">Kavitha S, Varuna S, Ramya R (2016) A comparative analysis on linear regression and support vector regression. In: 2016 Online International Conference on Green Engineering and Technologies (IC-GET)</mixed-citation></ref><ref id=\"CR39\"><mixed-citation publication-type=\"other\">Khan MR et al. Machine learning application for oil rate prediction in artificial gas lift wells. In: SPE Middle East Oil and Gas Show and Conference. 2019.</mixed-citation></ref><ref id=\"CR40\"><citation-alternatives><element-citation id=\"ec-CR40\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Khezerlooe-ye Aghdam</surname><given-names>S</given-names></name><etal/></person-group><article-title>Mechanistic assessment of Seidlitzia rosmarinus-derived surfactant for restraining shale hydration: a comprehensive experimental investigation</article-title><source>Chem Eng Res des</source><year>2019</year><volume>147</volume><fpage>570</fpage><lpage>578</lpage><pub-id pub-id-type=\"doi\">10.1016/j.cherd.2019.05.042</pub-id></element-citation><mixed-citation id=\"mc-CR40\" publication-type=\"journal\">Khezerlooe-ye Aghdam S et al (2019) Mechanistic assessment of Seidlitzia rosmarinus-derived surfactant for restraining shale hydration: a comprehensive experimental investigation. Chem Eng Res des 147:570&#8211;578</mixed-citation></citation-alternatives></ref><ref id=\"CR41\"><citation-alternatives><element-citation id=\"ec-CR41\" publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>P</given-names></name></person-group><article-title>Convolutional Neural Network</article-title><source>MATLAB deep learning: with machine learning, neural networks and artificial intelligence</source><year>2017</year><publisher-loc>Berkeley, CA</publisher-loc><publisher-name>Apress</publisher-name><fpage>121</fpage><lpage>147</lpage></element-citation><mixed-citation id=\"mc-CR41\" publication-type=\"book\">Kim P (2017) Convolutional Neural Network. MATLAB deep learning: with machine learning, neural networks and artificial intelligence. Apress, Berkeley, CA, pp 121&#8211;147</mixed-citation></citation-alternatives></ref><ref id=\"CR42\"><citation-alternatives><element-citation id=\"ec-CR42\" publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kramer</surname><given-names>O</given-names></name></person-group><source>K-nearest neighbors, in dimensionality reduction with unsupervised nearest neighbors</source><year>2013</year><publisher-loc>Berlin, Heidelberg</publisher-loc><publisher-name>Springer</publisher-name><fpage>13</fpage><lpage>23</lpage></element-citation><mixed-citation id=\"mc-CR42\" publication-type=\"book\">Kramer O (2013) K-nearest neighbors, in dimensionality reduction with unsupervised nearest neighbors. Springer, Berlin, Heidelberg, pp 13&#8211;23</mixed-citation></citation-alternatives></ref><ref id=\"CR43\"><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><etal/></person-group><article-title>A survey of convolutional neural networks: analysis, applications, and prospects</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2022</year><volume>33</volume><issue>12</issue><fpage>6999</fpage><lpage>7019</lpage><pub-id pub-id-type=\"doi\">10.1109/TNNLS.2021.3084827</pub-id><pub-id pub-id-type=\"pmid\">34111009</pub-id></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Li Z et al (2022) A survey of convolutional neural networks: analysis, applications, and prospects. IEEE Trans Neural Netw Learn Syst 33(12):6999&#8211;7019<pub-id pub-id-type=\"pmid\">34111009</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNNLS.2021.3084827</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><citation-alternatives><element-citation id=\"ec-CR44\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>Catalytic cracking of biomass tar for hydrogen-rich gas production: parameter optimization using response surface methodology combined with deterministic finite automaton</article-title><source>Renew Energy</source><year>2025</year><volume>241</volume><fpage>122368</fpage><pub-id pub-id-type=\"doi\">10.1016/j.renene.2025.122368</pub-id></element-citation><mixed-citation id=\"mc-CR44\" publication-type=\"journal\">Li X et al (2025) Catalytic cracking of biomass tar for hydrogen-rich gas production: parameter optimization using response surface methodology combined with deterministic finite automaton. Renew Energy 241:122368</mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><citation-alternatives><element-citation id=\"ec-CR46\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>F</given-names></name><etal/></person-group><article-title>Organics composition and microbial analysis reveal the different roles of biochar and hydrochar in affecting methane oxidation from paddy soil</article-title><source>Sci Total Environ</source><year>2022</year><volume>843</volume><fpage>157036</fpage><pub-id pub-id-type=\"doi\">10.1016/j.scitotenv.2022.157036</pub-id><pub-id pub-id-type=\"pmid\">35772551</pub-id></element-citation><mixed-citation id=\"mc-CR46\" publication-type=\"journal\">Liu F et al (2022) Organics composition and microbial analysis reveal the different roles of biochar and hydrochar in affecting methane oxidation from paddy soil. Sci Total Environ 843:157036<pub-id pub-id-type=\"pmid\">35772551</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.scitotenv.2022.157036</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR47\"><mixed-citation publication-type=\"other\">Lopez Pinaya WH et al (2020) Chapter 10 - Convolutional neural networks. In: Machine learning Mechelli A, Vieira S (Eds). Academic Press. p 173&#8211;191.</mixed-citation></ref><ref id=\"CR48\"><citation-alternatives><element-citation id=\"ec-CR48\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Alipour</surname><given-names>M</given-names></name></person-group><article-title>Gas-oil gravity drainage mechanism in fractured oil reservoirs: surrogate model development and sensitivity analysis</article-title><source>Comput Geosci</source><year>2022</year><volume>26</volume><issue>5</issue><fpage>1323</fpage><lpage>1343</lpage><pub-id pub-id-type=\"doi\">10.1007/s10596-022-10161-7</pub-id></element-citation><mixed-citation id=\"mc-CR48\" publication-type=\"journal\">Madani M, Alipour M (2022) Gas-oil gravity drainage mechanism in fractured oil reservoirs: surrogate model development and sensitivity analysis. Comput Geosci 26(5):1323&#8211;1343</mixed-citation></citation-alternatives></ref><ref id=\"CR49\"><citation-alternatives><element-citation id=\"ec-CR49\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Moraveji</surname><given-names>MK</given-names></name><name name-style=\"western\"><surname>Sharifi</surname><given-names>M</given-names></name></person-group><article-title>Modeling apparent viscosity of waxy crude oils doped with polymeric wax inhibitors</article-title><source>J Pet Sci Eng</source><year>2021</year><volume>196</volume><fpage>108076</fpage><pub-id pub-id-type=\"doi\">10.1016/j.petrol.2020.108076</pub-id></element-citation><mixed-citation id=\"mc-CR49\" publication-type=\"journal\">Madani M, Moraveji MK, Sharifi M (2021) Modeling apparent viscosity of waxy crude oils doped with polymeric wax inhibitors. J Pet Sci Eng 196:108076</mixed-citation></citation-alternatives></ref><ref id=\"CR50\"><citation-alternatives><element-citation id=\"ec-CR50\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mokhtari</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Navidi</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Mooney</surname><given-names>M</given-names></name></person-group><article-title>White-box regression (elastic net) modeling of earth pressure balance shield machine advance rate</article-title><source>Autom Constr</source><year>2020</year><volume>115</volume><fpage>103208</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2020.103208</pub-id></element-citation><mixed-citation id=\"mc-CR50\" publication-type=\"journal\">Mokhtari S, Navidi W, Mooney M (2020) White-box regression (elastic net) modeling of earth pressure balance shield machine advance rate. Autom Constr 115:103208</mixed-citation></citation-alternatives></ref><ref id=\"CR51\"><citation-alternatives><element-citation id=\"ec-CR51\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Niu</surname><given-names>X</given-names></name><etal/></person-group><article-title>Thermodynamic analysis of supercritical Brayton cycles using CO2-based binary mixtures for solar power tower system application</article-title><source>Energy</source><year>2022</year><volume>254</volume><fpage>124286</fpage><pub-id pub-id-type=\"doi\">10.1016/j.energy.2022.124286</pub-id></element-citation><mixed-citation id=\"mc-CR51\" publication-type=\"journal\">Niu X et al (2022) Thermodynamic analysis of supercritical Brayton cycles using CO2-based binary mixtures for solar power tower system application. Energy 254:124286</mixed-citation></citation-alternatives></ref><ref id=\"CR52\"><citation-alternatives><element-citation id=\"ec-CR52\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Paula</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>Machine learning and natural language processing enable a data-oriented experimental design approach for producing biochar and hydrochar from biomass</article-title><source>Chem Mater</source><year>2022</year><volume>34</volume><issue>3</issue><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type=\"doi\">10.1021/acs.chemmater.1c02961</pub-id></element-citation><mixed-citation id=\"mc-CR52\" publication-type=\"journal\">Paula AJ et al (2022) Machine learning and natural language processing enable a data-oriented experimental design approach for producing biochar and hydrochar from biomass. Chem Mater 34(3):979&#8211;990</mixed-citation></citation-alternatives></ref><ref id=\"CR53\"><citation-alternatives><element-citation id=\"ec-CR53\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qi</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H</given-names></name></person-group><article-title>Elastic net nonparallel hyperplane support vector machine and its geometrical rationality</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2022</year><volume>33</volume><issue>12</issue><fpage>7199</fpage><lpage>7209</lpage><pub-id pub-id-type=\"doi\">10.1109/TNNLS.2021.3084404</pub-id><pub-id pub-id-type=\"pmid\">34097622</pub-id></element-citation><mixed-citation id=\"mc-CR53\" publication-type=\"journal\">Qi K, Yang H (2022) Elastic net nonparallel hyperplane support vector machine and its geometrical rationality. IEEE Trans Neural Netw Learn Syst 33(12):7199&#8211;7209<pub-id pub-id-type=\"pmid\">34097622</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNNLS.2021.3084404</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR54\"><citation-alternatives><element-citation id=\"ec-CR54\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rigatti</surname><given-names>SJ</given-names></name></person-group><article-title>Random forest</article-title><source>J Insur Med</source><year>2017</year><volume>47</volume><issue>1</issue><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type=\"doi\">10.17849/insm-47-01-31-39.1</pub-id><pub-id pub-id-type=\"pmid\">28836909</pub-id></element-citation><mixed-citation id=\"mc-CR54\" publication-type=\"journal\">Rigatti SJ (2017) Random forest. J Insur Med 47(1):31&#8211;39<pub-id pub-id-type=\"pmid\">28836909</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.17849/insm-47-01-31-39.1</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR55\"><citation-alternatives><element-citation id=\"ec-CR55\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rocco</surname><given-names>CM</given-names></name><name name-style=\"western\"><surname>Moreno</surname><given-names>JA</given-names></name></person-group><article-title>Fast Monte Carlo reliability evaluation using support vector machine</article-title><source>Reliab Eng Syst Saf</source><year>2002</year><volume>76</volume><issue>3</issue><fpage>237</fpage><lpage>243</lpage><pub-id pub-id-type=\"doi\">10.1016/S0951-8320(02)00015-7</pub-id></element-citation><mixed-citation id=\"mc-CR55\" publication-type=\"journal\">Rocco CM, Moreno JA (2002) Fast Monte Carlo reliability evaluation using support vector machine. Reliab Eng Syst Saf 76(3):237&#8211;243</mixed-citation></citation-alternatives></ref><ref id=\"CR56\"><citation-alternatives><element-citation id=\"ec-CR56\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roth</surname><given-names>V</given-names></name></person-group><article-title>The generalized LASSO</article-title><source>IEEE Trans Neural Netw</source><year>2004</year><volume>15</volume><issue>1</issue><fpage>16</fpage><lpage>28</lpage><pub-id pub-id-type=\"doi\">10.1109/TNN.2003.809398</pub-id><pub-id pub-id-type=\"pmid\">15387244</pub-id></element-citation><mixed-citation id=\"mc-CR56\" publication-type=\"journal\">Roth V (2004) The generalized LASSO. IEEE Trans Neural Netw 15(1):16&#8211;28<pub-id pub-id-type=\"pmid\">15387244</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNN.2003.809398</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR57\"><citation-alternatives><element-citation id=\"ec-CR57\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sado</surname><given-names>S</given-names></name><etal/></person-group><article-title>Current state of application of machine learning for investigation of MgO-C refractories: a review</article-title><source>Materials</source><year>2023</year><volume>16</volume><issue>23</issue><fpage>7396</fpage><pub-id pub-id-type=\"doi\">10.3390/ma16237396</pub-id><pub-id pub-id-type=\"pmid\">38068140</pub-id><pub-id pub-id-type=\"pmcid\">PMC10707161</pub-id></element-citation><mixed-citation id=\"mc-CR57\" publication-type=\"journal\">Sado S et al (2023) Current state of application of machine learning for investigation of MgO-C refractories: a review. Materials 16(23):7396<pub-id pub-id-type=\"pmid\">38068140</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/ma16237396</pub-id><pub-id pub-id-type=\"pmcid\">PMC10707161</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR58\"><citation-alternatives><element-citation id=\"ec-CR58\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sarica</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Cerasa</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Quattrone</surname><given-names>A</given-names></name></person-group><article-title>Random forest algorithm for the classification of neuroimaging data in Alzheimer&#8217;s disease: a systematic review</article-title><source>Front Aging Neurosci</source><year>2017</year><pub-id pub-id-type=\"doi\">10.3389/fnagi.2017.00329</pub-id><pub-id pub-id-type=\"pmid\">29056906</pub-id><pub-id pub-id-type=\"pmcid\">PMC5635046</pub-id></element-citation><mixed-citation id=\"mc-CR58\" publication-type=\"journal\">Sarica A, Cerasa A, Quattrone A (2017) Random forest algorithm for the classification of neuroimaging data in Alzheimer&#8217;s disease: a systematic review. Front Aging Neurosci. 10.3389/fnagi.2017.00329<pub-id pub-id-type=\"pmid\">29056906</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fnagi.2017.00329</pub-id><pub-id pub-id-type=\"pmcid\">PMC5635046</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR59\"><citation-alternatives><element-citation id=\"ec-CR59\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shafizadeh</surname><given-names>A</given-names></name><etal/></person-group><article-title>Machine learning predicts and optimizes hydrothermal liquefaction of biomass</article-title><source>Chem Eng J (Lausanne)</source><year>2022</year><volume>445</volume><fpage>136579</fpage><pub-id pub-id-type=\"doi\">10.1016/j.cej.2022.136579</pub-id></element-citation><mixed-citation id=\"mc-CR59\" publication-type=\"journal\">Shafizadeh A et al (2022) Machine learning predicts and optimizes hydrothermal liquefaction of biomass. Chem Eng J (Lausanne) 445:136579</mixed-citation></citation-alternatives></ref><ref id=\"CR60\"><citation-alternatives><element-citation id=\"ec-CR60\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shafizadeh</surname><given-names>A</given-names></name><etal/></person-group><article-title>Machine learning-based characterization of hydrochar from biomass: implications for sustainable energy and material production</article-title><source>Fuel</source><year>2023</year><volume>347</volume><fpage>128467</fpage><pub-id pub-id-type=\"doi\">10.1016/j.fuel.2023.128467</pub-id></element-citation><mixed-citation id=\"mc-CR60\" publication-type=\"journal\">Shafizadeh A et al (2023) Machine learning-based characterization of hydrochar from biomass: implications for sustainable energy and material production. Fuel 347:128467</mixed-citation></citation-alternatives></ref><ref id=\"CR61\"><citation-alternatives><element-citation id=\"ec-CR61\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Combined microbial transcript and metabolic analysis reveals the different roles of hydrochar and biochar in promoting anaerobic digestion of waste activated sludge</article-title><source>Water Res</source><year>2021</year><volume>205</volume><fpage>117679</fpage><pub-id pub-id-type=\"doi\">10.1016/j.watres.2021.117679</pub-id><pub-id pub-id-type=\"pmid\">34600232</pub-id></element-citation><mixed-citation id=\"mc-CR61\" publication-type=\"journal\">Shi Z et al (2021) Combined microbial transcript and metabolic analysis reveals the different roles of hydrochar and biochar in promoting anaerobic digestion of waste activated sludge. Water Res 205:117679<pub-id pub-id-type=\"pmid\">34600232</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.watres.2021.117679</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR62\"><citation-alternatives><element-citation id=\"ec-CR62\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shoushtari</surname><given-names>AB</given-names></name><name name-style=\"western\"><surname>Asadolahpour</surname><given-names>SR</given-names></name><name name-style=\"western\"><surname>Madani</surname><given-names>M</given-names></name></person-group><article-title>Thermodynamic investigation of asphaltene precipitation and deposition profile in wellbore: a case study</article-title><source>J Mol Liq</source><year>2020</year><volume>320</volume><fpage>114468</fpage><pub-id pub-id-type=\"doi\">10.1016/j.molliq.2020.114468</pub-id></element-citation><mixed-citation id=\"mc-CR62\" publication-type=\"journal\">Shoushtari AB, Asadolahpour SR, Madani M (2020) Thermodynamic investigation of asphaltene precipitation and deposition profile in wellbore: a case study. J Mol Liq 320:114468</mixed-citation></citation-alternatives></ref><ref id=\"CR63\"><citation-alternatives><element-citation id=\"ec-CR63\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Smith</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Campbell</surname><given-names>F</given-names></name></person-group><article-title>A critique of some ridge regression methods</article-title><source>J Am Stat Assoc</source><year>1980</year><volume>75</volume><issue>369</issue><fpage>74</fpage><lpage>81</lpage><pub-id pub-id-type=\"doi\">10.1080/01621459.1980.10477428</pub-id></element-citation><mixed-citation id=\"mc-CR63\" publication-type=\"journal\">Smith G, Campbell F (1980) A critique of some ridge regression methods. J Am Stat Assoc 75(369):74&#8211;81</mixed-citation></citation-alternatives></ref><ref id=\"CR64\"><citation-alternatives><element-citation id=\"ec-CR64\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Smola</surname><given-names>AJ</given-names></name><name name-style=\"western\"><surname>Sch&#246;lkopf</surname><given-names>B</given-names></name></person-group><article-title>A tutorial on support vector regression</article-title><source>Stat Comput</source><year>2004</year><volume>14</volume><issue>3</issue><fpage>199</fpage><lpage>222</lpage><pub-id pub-id-type=\"doi\">10.1023/B:STCO.0000035301.49549.88</pub-id></element-citation><mixed-citation id=\"mc-CR64\" publication-type=\"journal\">Smola AJ, Sch&#246;lkopf B (2004) A tutorial on support vector regression. Stat Comput 14(3):199&#8211;222</mixed-citation></citation-alternatives></ref><ref id=\"CR65\"><citation-alternatives><element-citation id=\"ec-CR65\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Taha</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Malebary</surname><given-names>SJ</given-names></name></person-group><article-title>An intelligent approach to credit card fraud detection using an optimized light gradient boosting machine</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>25579</fpage><lpage>25587</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2020.2971354</pub-id></element-citation><mixed-citation id=\"mc-CR65\" publication-type=\"journal\">Taha AA, Malebary SJ (2020) An intelligent approach to credit card fraud detection using an optimized light gradient boosting machine. IEEE Access 8:25579&#8211;25587</mixed-citation></citation-alternatives></ref><ref id=\"CR66\"><citation-alternatives><element-citation id=\"ec-CR66\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tauro</surname><given-names>R</given-names></name><etal/></person-group><article-title>An integrated user-friendly web-based spatial platform for bioenergy planning</article-title><source>Biomass Bioenergy</source><year>2021</year><volume>145</volume><fpage>105939</fpage><pub-id pub-id-type=\"doi\">10.1016/j.biombioe.2020.105939</pub-id></element-citation><mixed-citation id=\"mc-CR66\" publication-type=\"journal\">Tauro R et al (2021) An integrated user-friendly web-based spatial platform for bioenergy planning. Biomass Bioenergy 145:105939</mixed-citation></citation-alternatives></ref><ref id=\"CR68\"><citation-alternatives><element-citation id=\"ec-CR68\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Touzani</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Granderson</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Fernandes</surname><given-names>S</given-names></name></person-group><article-title>Gradient boosting machine for modeling the energy consumption of commercial buildings</article-title><source>Energy Build</source><year>2018</year><volume>158</volume><fpage>1533</fpage><lpage>1543</lpage><pub-id pub-id-type=\"doi\">10.1016/j.enbuild.2017.11.039</pub-id></element-citation><mixed-citation id=\"mc-CR68\" publication-type=\"journal\">Touzani S, Granderson J, Fernandes S (2018) Gradient boosting machine for modeling the energy consumption of commercial buildings. Energy Build 158:1533&#8211;1543</mixed-citation></citation-alternatives></ref><ref id=\"CR69\"><citation-alternatives><element-citation id=\"ec-CR69\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tsarpali</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Kuhn</surname><given-names>JN</given-names></name><name name-style=\"western\"><surname>Philippidis</surname><given-names>GP</given-names></name></person-group><article-title>Hydrothermal carbonization of residual algal biomass for production of hydrochar as a biobased metal adsorbent</article-title><source>Sustainability</source><year>2022</year><volume>14</volume><issue>1</issue><fpage>455</fpage><pub-id pub-id-type=\"doi\">10.3390/su14010455</pub-id></element-citation><mixed-citation id=\"mc-CR69\" publication-type=\"journal\">Tsarpali M, Kuhn JN, Philippidis GP (2022) Hydrothermal carbonization of residual algal biomass for production of hydrochar as a biobased metal adsorbent. Sustainability 14(1):455</mixed-citation></citation-alternatives></ref><ref id=\"CR70\"><citation-alternatives><element-citation id=\"ec-CR70\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tyralis</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Papacharalampous</surname><given-names>G</given-names></name></person-group><article-title>Boosting algorithms in energy research: a systematic review</article-title><source>Neural Comput Appl</source><year>2021</year><volume>33</volume><issue>21</issue><fpage>14101</fpage><lpage>14117</lpage><pub-id pub-id-type=\"doi\">10.1007/s00521-021-05995-8</pub-id></element-citation><mixed-citation id=\"mc-CR70\" publication-type=\"journal\">Tyralis H, Papacharalampous G (2021) Boosting algorithms in energy research: a systematic review. Neural Comput Appl 33(21):14101&#8211;14117</mixed-citation></citation-alternatives></ref><ref id=\"CR71\"><citation-alternatives><element-citation id=\"ec-CR71\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vishwakarma</surname><given-names>DK</given-names></name><etal/></person-group><article-title>Evaluation of CatBoost method for predicting weekly pan evaporation in subtropical and sub-humid regions</article-title><source>Pure Appl Geophys</source><year>2024</year><volume>181</volume><issue>2</issue><fpage>719</fpage><lpage>747</lpage><pub-id pub-id-type=\"doi\">10.1007/s00024-023-03426-4</pub-id></element-citation><mixed-citation id=\"mc-CR71\" publication-type=\"journal\">Vishwakarma DK et al (2024) Evaluation of CatBoost method for predicting weekly pan evaporation in subtropical and sub-humid regions. Pure Appl Geophys 181(2):719&#8211;747</mixed-citation></citation-alternatives></ref><ref id=\"CR74\"><citation-alternatives><element-citation id=\"ec-CR74\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xiong</surname><given-names>J-b</given-names></name><etal/></person-group><article-title>Study on the hydrothermal carbonization of swine manure: the effect of process parameters on the yield/properties of hydrochar and process water</article-title><source>J Anal Appl Pyrolysis</source><year>2019</year><volume>144</volume><fpage>104692</fpage><pub-id pub-id-type=\"doi\">10.1016/j.jaap.2019.104692</pub-id></element-citation><mixed-citation id=\"mc-CR74\" publication-type=\"journal\">Xiong J-b et al (2019) Study on the hydrothermal carbonization of swine manure: the effect of process parameters on the yield/properties of hydrochar and process water. J Anal Appl Pyrolysis 144:104692</mixed-citation></citation-alternatives></ref><ref id=\"CR76\"><citation-alternatives><element-citation id=\"ec-CR76\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>Z-X</given-names></name><etal/></person-group><article-title>Benign-by-design N-doped carbonaceous materials obtained from the hydrothermal carbonization of sewage sludge for supercapacitor applications</article-title><source>Green Chem</source><year>2020</year><volume>22</volume><issue>12</issue><fpage>3885</fpage><lpage>3895</lpage><pub-id pub-id-type=\"doi\">10.1039/D0GC01272F</pub-id></element-citation><mixed-citation id=\"mc-CR76\" publication-type=\"journal\">Xu Z-X et al (2020) Benign-by-design N-doped carbonaceous materials obtained from the hydrothermal carbonization of sewage sludge for supercapacitor applications. Green Chem 22(12):3885&#8211;3895</mixed-citation></citation-alternatives></ref><ref id=\"CR77\"><citation-alternatives><element-citation id=\"ec-CR77\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>S</given-names></name><etal/></person-group><article-title>Effect of biomass type and pyrolysis temperature on nitrogen in biochar, and the comparison with hydrochar</article-title><source>Fuel</source><year>2021</year><volume>291</volume><fpage>120128</fpage><pub-id pub-id-type=\"doi\">10.1016/j.fuel.2021.120128</pub-id></element-citation><mixed-citation id=\"mc-CR77\" publication-type=\"journal\">Xu S et al (2021) Effect of biomass type and pyrolysis temperature on nitrogen in biochar, and the comparison with hydrochar. Fuel 291:120128</mixed-citation></citation-alternatives></ref><ref id=\"CR79\"><citation-alternatives><element-citation id=\"ec-CR79\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Biomass microwave pyrolysis characterization by machine learning for sustainable rural biorefineries</article-title><source>Renew Energy</source><year>2022</year><volume>201</volume><fpage>70</fpage><lpage>86</lpage><pub-id pub-id-type=\"doi\">10.1016/j.renene.2022.11.028</pub-id></element-citation><mixed-citation id=\"mc-CR79\" publication-type=\"journal\">Yang Y et al (2022) Biomass microwave pyrolysis characterization by machine learning for sustainable rural biorefineries. Renew Energy 201:70&#8211;86</mixed-citation></citation-alternatives></ref><ref id=\"CR83\"><citation-alternatives><element-citation id=\"ec-CR83\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Effects of temperature, time and acidity of hydrothermal carbonization on the hydrochar properties and nitrogen recovery from corn stover</article-title><source>Biomass Bioenergy</source><year>2019</year><volume>122</volume><fpage>175</fpage><lpage>182</lpage><pub-id pub-id-type=\"doi\">10.1016/j.biombioe.2019.01.035</pub-id></element-citation><mixed-citation id=\"mc-CR83\" publication-type=\"journal\">Zhang Y et al (2019) Effects of temperature, time and acidity of hydrothermal carbonization on the hydrochar properties and nitrogen recovery from corn stover. Biomass Bioenergy 122:175&#8211;182</mixed-citation></citation-alternatives></ref><ref id=\"CR85\"><mixed-citation publication-type=\"other\">Zhang F, O&#8217;Donnell LJ (2020) Chapter 7 - Support vector regression. In: Machine learning, Mechelli A, Vieira S (Eds.). Academic Press. p 123&#8211;140.</mixed-citation></ref><ref id=\"CR87\"><mixed-citation publication-type=\"other\">Zhu Q (2022) Treatment and prevention of stuck pipe based on artificial neural networks analysis. In: Offshore Technology Conference Asia</mixed-citation></ref><ref id=\"CR88\"><citation-alternatives><element-citation id=\"ec-CR88\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zulfiqar</surname><given-names>H</given-names></name><etal/></person-group><article-title>Identification of cyclin protein using gradient boost decision tree algorithm</article-title><source>Comput Struct Biotechnol J</source><year>2021</year><volume>19</volume><fpage>4123</fpage><lpage>4131</lpage><pub-id pub-id-type=\"doi\">10.1016/j.csbj.2021.07.013</pub-id><pub-id pub-id-type=\"pmid\">34527186</pub-id><pub-id pub-id-type=\"pmcid\">PMC8346528</pub-id></element-citation><mixed-citation id=\"mc-CR88\" publication-type=\"journal\">Zulfiqar H et al (2021) Identification of cyclin protein using gradient boost decision tree algorithm. Comput Struct Biotechnol J 19:4123&#8211;4131<pub-id pub-id-type=\"pmid\">34527186</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.csbj.2021.07.013</pub-id><pub-id pub-id-type=\"pmcid\">PMC8346528</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Bioresour Bioprocess Bioresour Bioprocess 4570 biobio Bioresources and Bioprocessing 2197-4365 Springer PMC12675903 PMC12675903.1 12675903 12675903 41335148 10.1186/s40643-025-00979-1 979 1 Research Development of robust machine learning models to estimate hydrochar higher heating value and yield based upon biomass proximate analysis Hou Guoliang houguoliang@ccsfu.edu.cn 1 Alkhayyat Ahmad ahmedalkhayyat85@gmail.com 2 Almalkawi Ahmad 3 Yadav Anupam 4 Shreenidhi H. S. 5 Saini Vishnu 6 Shomurotova Shirin 7 Singh Devendra 8 Jain Vatsal 9 Smerat Aseel 10 11 Khalid Ahmad ahmad.khalidd1401@gmail.com 12 1 https://ror.org/00cbhey71 grid.443294.c 0000 0004 1791 567X School of Mathematics, Changchun Normal University, Changchun, 130032 Jilin China 2 https://ror.org/024dzaa63 Department of Computers Techniques Engineering, College of Technical Engineering, The Islamic University, Najaf, Iraq 3 https://ror.org/02hvzvg02 grid.501970.a 0000 0004 0418 6164 Center for ESL &amp; Academic Preparation, Modern College of Business and Science, Muscat, Oman 4 https://ror.org/05fnxgv12 grid.448881.9 0000 0004 1774 2318 Department of Computer Engineering and Application, GLA University, Mathura, 281406 India 5 https://ror.org/01cnqpt53 grid.449351.e 0000 0004 1769 1282 Department of Computer Science and Engineering, School of Engineering and Technology, JAIN (Deemed to Be University), Bangalore, Karnataka India 6 https://ror.org/03b6ffh07 grid.412552.5 0000 0004 1764 278X Sharda School of Engineering and Sciences, Sharda University, Knowledge Park III, Greater Noida, 201310 Uttar Pradesh India 7 https://ror.org/051g1n833 grid.502767.1 0000 0004 0403 3387 Department of Chemistry Teaching Methods, National Pedagogical University of Uzbekistan, Bunyodkor Street 27, Tashkent, Uzbekistan 8 https://ror.org/00ba6pg24 grid.449906.6 0000 0004 4659 5193 Department of Computer Science &amp; Engineering, Uttaranchal Institute of Technology, Uttaranchal University, Dehradun, Uttarakhand 248007 India 9 https://ror.org/057d6z539 grid.428245.d 0000 0004 1765 3753 Centre for Research Impact &amp; Outcome, Chitkara University Institute of Engineering and Technology, Chitkara University, Rajpura, Punjab 140401 India 10 https://ror.org/00xddhq60 grid.116345.4 0000 0004 0644 1915 Faculty of Educational Sciences, Al-Ahliyya Amman University, Amman, 19328 Jordan 11 https://ror.org/0034me914 grid.412431.1 0000 0004 0444 045X Department of Biosciences, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, 602105 India 12 https://ror.org/04hcvaf32 grid.412413.1 0000 0001 2299 4112 Faculty of Engineering, Sana&#8217;a University, Sanaa, Yemen 3 12 2025 12 2025 12 1 478465 138 22 7 2025 22 10 2025 12 11 2025 03 12 2025 05 12 2025 05 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . This study introduces a robust machine learning framework for predicting hydrochar yield and higher heating value (HHV) using biomass proximate analysis. A curated dataset of 481 samples was assembled, featuring input variables such as fixed carbon, volatile matter, ash content, reaction time, temperature, and water content. Hydrochar yield and HHV served as the target outputs. To enhance data quality, Monte Carlo Outlier Detection (MCOD) was employed to eliminate anomalous entries. Thirteen machine learning algorithms, including convolutional neural networks (CNN), linear regression, decision trees, and advanced ensemble methods (CatBoost, LightGBM, XGBoost) were systematically compared. CatBoost demonstrated superior performance, achieving an R 2 of 0.98 and mean squared error (MSE) of 0.05 for HHV prediction, and an R 2 of 0.94 with MSE of 0.03 for yield estimation. SHAP analysis identified ash content as the most influential feature for HHV prediction, while temperature, water content, and fixed carbon were key drivers of yield. These results validate the effectiveness of gradient boosting models, particularly CatBoost, in accurately modeling hydrothermal carbonization outcomes and supporting data-driven biomass valorization strategies. Graphical abstract Supplementary Information The online version contains supplementary material available at 10.1186/s40643-025-00979-1. Keywords Biomass proximate analysis Hydrochar yield prediction Machine learning Higher heating value (HHV) CatBoost algorithm pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; State Key Laboratory of Bioreactor Engineering, East China University of Science and Technology 2025 Introduction The majority of chemical requirements and global energy are presently satisfied by fossil fuels. In light of the declining availability of fossil reserves and their adversative environmental consequences, such as air pollution and climate change, scientists are exploring justifiable alternatives for energy production (Yang et al. 2022 ; Aghdam et al. 2023 ; Khezerlooe-ye Aghdam et al. 2019 ; Li 2025; Wu et al. 2025; Yu et al. 2023). Carbon&#8211;neutral biomass energy has appeared as a prominent focus among renewable energy resources over recent decades (Chen et al. 2022a ; Wang et al. 2025; Xu et al. 2024) primarily because it can be transformed into fossil-like liquid, gaseous, and solid fuels with versatile applications. As the most significant renewable source of energy on Earth, bioenergy constitutes roughly 10 percent of the worldwide key energy supply (Tauro et al. 2021 ; Li et al. 2025 ; Niu et al. 2022 ; Dou et al. 2025 ). Approximately semi of worldwide biomass usage is dedicated to cooking and heating in emergent nations (Bhutto et al. 2019 ). To address this, shifting from traditional biomass to advanced modern energy solutions like biodiesel, biogas, biopower, and bioethanol is essential to minimize adverse environmental effects. Several thermochemical technologies have been proposed to change biomass into biochemicals and biofuels (Chen et al. 2022b ). Thermochemical manners, such as pyrolysis, gasification, combustion, and hydrothermal carbonization (HTC), have drawn substantial scientific interest (Hai et al. 2023 ; Paula et al. 2022 ). These approaches allow the straight conversion of biomass into value-added products without the need for chemically harsh and energy-intensive pretreatment procedures. However, the high content of moisture for many biomass feedstocks, both dedicated and waste-derived, renders them inappropriate for certain thermochemical pathways like pyrolysis, combustion, and gasification (Li 2025; Shafizadeh et al. 2022 ; Tian et al. 2025). Current methods are inefficient at handling wet biomass, resulting in substantial expenses for humidity decrease. Hydrothermal processing technologies, with HTC at the forefront, present a practical and cost-effective solution by eliminating the energy-intensive drying phase. Beyond this, HTC is also more energy-efficient than other thermal conversion processes like gasification and pyrolysis (Zhang et al. 2019 ) and, due to its slighter reaction circumstances, generates minimum amounts of toxic gases, like sulfur oxides and nitrogen. The hydrothermal carbonization (HTC) procedure is conducted in pressed water at 180 and 260 &#176;C, mimicking the ordinary coalification development that happened over millions of years. This progression produces a carbon-rich known as hydrochar (Xu et al. 2020 ), Hydrochar&#8217;s properties make it comparable to lignite. Biochar which formed in pyrolysis, is specifically suited for submissions for instance, water treatment, carbon sequestration, and agriculture due to its large carbon content, reduced reactivity, and high surface area (Xiong et al. 2019 ). Hydrochar has higher ash content, lower carbon content, and greater reactivity (Xu et al. 2021 ; Shi et al. 2021 ), positioning it as an interesting compound for soil amendment, energy generation, and pollutant adsorption (Li et al. 2025 ). Generated under comparatively mild circumstances in comparison to biochar, hydrochar demonstrates reduced porosity, lower stability, lower pH, and smaller specific surface area (Liu et al. 2022 ). However, its abundance of oxygenated groups, containing carbonyl, carboxyl and phenolic hydroxyl, enhances its ability to adsorb metallic element (Tsarpali et al. 2022 ). The structural and physico-chemical properties of hydrochar, highly depend on the nature of the biomass and the parameters of the HTC. Detection of the ideal treating conditions for a given biomass feedstock typically involves many laboratory tests, which are resource-intensive, laborious, and expensive. These restrictions hinder the detailed characterization of hydrochar&#8217;s quantity and quality. The inherent complexity of interactions and mechanisms during the hydrothermal carbonization (HTC) process renders current computational techniques inadequate for accurately predicting hydrochar properties. Consequently, it is imperative to develop more effective and precise methodologies for assessing the structural and physicochemical characteristics of hydrochar, thereby ensuring its appropriate utilization across various applications (Shafizadeh et al. 2023 ; Fang et al. 2025; Xu and Liang 2025; Yang et al. 2025; Zhang et al. 2025). This work presents a novel and rigorous methodology for predicting hydrochar&#8217;s HHV and yield. Our primary contribution is the systematic evaluation of a broad spectrum of machine learning algorithms, including traditional models (linear regression, SVM) and advanced ensemble and deep learning techniques (ANN, CNN, RF, XGBoost, CatBoost, LightGBM). This comprehensive head-to-head performance comparison sets a new benchmark in this research area. We also introduce a robust Monte Carlo-based outlier detection to enhance dataset reliability, a step often overlooked in similar studies. To ensure the trustworthiness of our findings, we provide a detailed analysis of model performance using multiple metrics and graphical representations. The final novelty of our work is the application of SHAP analysis on the best-performing model, which not only validates our results but also provides crucial insights into the underlying relationships between biomass composition and hydrochar properties, thereby advancing the fundamental understanding of this process. The complete methodology context is illustrated in Fig.&#160; 1 . Fig.&#160;1 Overall workflow taken in this study to construct the data-driven models and choose the top-performing one subsequently Machine learning backgrounds This segment details the machine learning algorithms. Convolutional neural network It is a specialized class of algorithms planned to process structured data, such as images, by leveraging their spatial hierarchies. Inspired by the visual cortex of the human brain, CNNs utilize a blend of pooling, convolutional, and fully connected layers to detect and learn features at different levels of complexity. Convolutional layers use filters to recognize patterns like textures and edges, while pooling layers decrease dimensionality, increasing computational efficiency and resilience to input variations. Fully connected layers integrate these features to produce final predictions. The training process, driven by backpropagation, adjusts the network&#8217;s weights to minimize prediction error, while activation functions such as ReLU introduce non-linearity to capture complex relationships in the data. CNNs excel in a variety of computer vision tasks, including image classification, object detection, and face recognition, where their ability to extract features automatically makes them highly effective. Beyond traditional vision applications, CNNs have been extended to other fields as well. In medical imaging, they assist in disease diagnosis by analyzing features in scans or X-rays. In e-commerce and video analytics, CNNs are employed for tasks such as optical character recognition (OCR) and automated tagging. Even in autonomous driving, CNNs play a vital role in lane detection and obstacle avoidance, while adaptations of CNNs in natural language processing (NLP) are used for sequence modeling and text analysis. Their adaptability across domains makes them indispensable tools in modern artificial intelligence. Despite their advantages, CNNs come with challenges. They require large labeled datasets and substantial computational resources for training, which can be a limiting factor in many applications. CNNs are also susceptible to adversarial attacks, where subtle changes to input data can mislead predictions. Moreover, overfitting is a common issue, particularly when training with limited data, necessitating regularization techniques and hyperparameter optimization to achieve strong generalization (Li et al. 2022 ; Kim 2017 ; Lopez Pinaya, et al. 2020 ; Aloysius and Geetha 2017 ; Chagas, et al. 2018 ). Artificial neural network ANNs are computational manners encouraged by biological neural systems consisting of interconnected layers of neurons. Data is processed through these layers, with each connection having a weight that is adjusted during learning to emphasize or de-emphasize certain features. This learning process is driven by backpropagation, where the loss function&#8217;s gradient is computed and used to update the weights iteratively. This enables ANNs to progress their ability to model complex patterns progressively (Fan et al. 2025; Yue 2025; Zhang et al. 2017). ANNs are highly versatile and can be applied across various fields. In healthcare, they are used for medical diagnostics, imaging, or genomics data to detect disease patterns. In finance, ANNs assist in credit scoring and risk assessment, leveraging historical data to predict outcomes. They are also used in marketing for customer segmentation and behavior analysis, in natural language processing for tasks like sentiment analysis, and robotics for autonomous decision-making. Despite their advantages, ANNs have challenges. They need large data and substantial computational power to train, and their training can be time-consuming. Additionally, the &#8220;black-box&#8221; nature of ANNs makes interpreting their decision-making processes difficult, which is problematic in areas requiring transparency, such as healthcare or legal systems. Nonetheless, ANNs remain powerful tools for tasks involving complex data (Khan, et al. 2019 ; Ahmadi et al. 2013 ; Gardner and Dorling 1998 ; Zhu 2022 ; Heidari et al. 2016 ). Decision tree A Decision Tree is a flowchart-like used in machine learning, where each internal node characterizes a test on a feature, and branches represent possible outcomes. Leaf nodes indicate the final decision or classification. The tree splits the data based on criteria such as information gain, Gini impurity, or variance reduction, targeting to generate subsets that are as homogeneous as possible. The tree starts with the root node demonstrating the entire data. Then, it splits the data into smaller subsets based on feature values, optimizing the separation of classes or minimizing variance in regression. The feature that most effectively splits the data, i.e., providing the highest information gain or variance reduction, tends to be placed near the root. This process continues until stopping conditions are met. Decision Trees are valued for their interpretability and simplicity. The structure lets users to trace the path from the root to the leaf to understand the decision-making process. This makes them suitable for applications like medical diagnostics, where transparency is critical, or in finance for tasks like credit scoring. However, Decision Trees are prone to overfitting, especially with complex trees. This can be mitigated with pruning or ensemble techniques such as Random Forests or Gradient Boosting. Despite their limitations, Decision Trees remain a influential tool for various practical machine-learning tasks due to their clarity and ease of use (Ghiasi et al. 2020 ; Hautaniemi et al. 2005 ). Random forest Random Forest is a technique that generates multiple decision trees during training and aggregates their predictions to enhance model precision and robustness. This method, based on bagging (bootstrap aggregating), involves training several models on various random subsets of the original dataset drawn with replacement. By averaging predictions in regression or using majority voting in classification, Random Forest reduces the overfitting common in individual decision trees, leading to more generalized predictions. The randomization process introduces decorrelation among the trees, which effectively reduces variance and enhances prediction stability. Additionally, each tree is trained on a bootstrapped dataset, and at each split, a random subset of features is considered. This ensures that each tree learns different patterns, contributing to the ensemble&#8217;s overall strength. Random Forest is widely applied in various fields, such as finance, healthcare, and ecology, owing to its robustness and reliability in making predictions from large datasets. It excels in risk assessment, fraud detection, medical diagnoses, and ecological modeling. Despite its advantages, the model can be computationally expensive and resource-intensive due to the large number of trees. Its complexity also reduces interpretability, as it is difficult to visualize the decision-making process across all trees. Nonetheless, Random Forest offers a powerful balance between bias and variance, with an internal feature importance measure that helps identify key variables in predictive modeling tasks. (Sarica et al. 2017 ; Rigatti 2017 ; Feng, et al. 2020 ; Ao et al. 2019 ; Cha et al. 2021 ). Linear regression It&#8217;s a foundational statistical approach that launches a linear relationship between a target variable and its influencing factors. In simple linear regression, a single independent variable forecasts the dependent variable, whereas multiple linear regression incorporates several predictors. The model assumes a linear relationship and is expressed as&#160; Y &#8201;=&#8201; &#946; 0&#8201;+&#8201; &#946; 1 X 1&#8201;+&#8201; &#946; 2 X 2&#8201;+&#8201;&#8230;&#8201;+&#8201; &#946;nXn &#8201;+&#8201; &#949; , where&#160; &#946; &#160;represents coefficients and&#160; &#949; &#160;denotes the error term. The goal is to determine the coefficients that minimize the sum of squared residuals, a process achieved through&#160;least squares estimation. Model performance is assessed using metrics such as&#160;R-squared&#160;and&#160;residual analysis&#160;to evaluate goodness-of-fit and predictive accuracy. Linear Regression is widely applied across disciplines due to its simplicity and interpretability. In economics, it models relationships between financial variables, while in marketing, it helps analyze consumer behavior. In biology, it is used for dose&#8211;response modeling, and in engineering, it predicts system outputs based on input data. Despite its advantages, Linear Regression assumes a strict linear relationship, making it unsuitable for complex, non-linear data. It is also sensitive to&#160;outliers&#160;and suffers from&#160;multicollinearity, which can distort coefficient estimates. To address these challenges, techniques such as&#160;variable transformation,&#160;regularization, and&#160;polynomial regression&#160;are often employed. In spite of its limitation, Linear Regression remains a fundamental manner for understanding data relationships and making initial predictive assessments (Guo and Wang 2019 ; Hope and Chapter 2020 ; Chen et al. 2019 ). Ridge regression This approach enhances linear regression by incorporating an L2 regularization penalty to address multicollinearity and mitigate overfitting. By adding a penalty term proportional to the square of the coefficients&#8217; amounts, it shrinks coefficients toward zero without eliminating them, promoting model stability. The penalty is controlled by a hyperparameter, lambda, which determines the degree of regularization and balances bias and variance. This adjustment makes Ridge Regression well-suited for datasets with correlated predictors by reducing the sensitivity of the fitted model to multicollinearity. Unlike ordinary least squares (OLS), Ridge favors smaller coefficients, improving stability and predictive performance when handling a large number of correlated features or medium-sized effects. Importantly, all predictors are retained, maintaining a more holistic model. Ridge Regression is particularly valuable in high-dimensional datasets, such as genetic studies, where it identifies significant predictors like key genes while managing correlations effectively. In finance, it aids in credit risk modeling by stabilizing models with many variables, and in sales forecasting, it reduces overfitting while accounting for multicollinearity. Ridge offers a robust approach for prediction in complex datasets, though it does not perform feature selection, limiting its ability to produce sparse models. Its effectiveness depends on careful tuning of the regularization parameter, requiring computational resources and expertise for optimal implementation (Hoerl and Kennard 1970a , 1970b ; Smith and Campbell 1980 ; Hoerl 2020 ). Lasso regression This is a linear regression method that incorporates L1 regularization to develop both prediction exactness and model interpretability. The key feature of Lasso is its ability to perform variable selection by penalizing the absolute size of the regression coefficients, thereby driving some of them to exactly zero. This results in a sparse model that selects only the most significant predictors, making it valuable when working with data holding a large number of predictors. By shrinking less important variables&#8217; coefficients to zero, Lasso mitigates overfitting, ensuring that irrelevant or redundant features do not influence the model. The strength of the regularization is controlled by a hyperparameter, lambda, which adjusts the trade-off between bias and variance, allowing for the retention of only relevant predictors. Lasso Regression is widely applied across various domains, such as bioinformatics, where it helps identify significant genetic markers from a large set of genes, and econometrics, where it selects key economic predictors for forecasting trends. In engineering, particularly in signal processing, Lasso aids in feature selection while simplifying models. Its main advantage is its ability to produce a simpler, more understandable model by reducing the number of predictors. However, Lasso can be less effective when dealing with highly correlated variables, as it may arbitrarily choose one over others. The model&#8217;s performance heavily relies on the careful selection of the regularization parameter, which requires fine-tuning to achieve optimal results. In scenarios with high multicollinearity, Ridge Regression may sometimes outperform Lasso due to its ability to handle correlated features more effectively (Kang, et al. 2020 ; Roth 2004 ; Emmert-Streib and Dehmer 2019 ). Support vector regression SVR extends SVM to regression tasks, predicting continuous values while maintaining errors within a specified margin ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ \\upepsilon $$\\end{document} ). It utilizes an insensitive loss function, reducing the impact of minor deviations and focusing on overall trends. By applying kernel functions like polynomial, linear, and radial basis functions (RBF), SVR effectively models both linear and nonlinear relations. SVR is widely used in fields requiring precise forecasting, such as stock price prediction, real estate valuation, and energy consumption analysis. However, its computational cost is high, especially for large datasets, and performance depends on careful parameter tuning to balance accuracy and efficiency (Smola and Sch&#246;lkopf 2004 ; Zhang and O&#8217;Donnell 2020 ; Kavitha et al. 2016 ). Gradient boosting machine GBMs are cooperative learning models which combine multiple weak learners, archetypally decision trees, to build a strong predictive model. The core principle of GBMs is residual learning, where each new model corrects the errors of the previous one by focusing on the residual errors from earlier iterations. This process minimizes a differentiable loss function (e.g., mean squared error for regression or binary log loss for classification), progressively enhancing model accuracy. At each stage, GBMs add new models that adjust based on the previous iteration&#8217;s errors, creating a refined prediction with each step. The sequential learning process ensures that the model effectively captures complex data patterns and interactions. GBMs have a broad range of applications, from web search rankings and credit scoring to fraud detection and healthcare analytics, offering high accuracy and predictive power. Their success in data science competitions highlights their ability to capture intricate data structures. However, their computational intensity, especially with large datasets, and the complexity of model interpretation pose challenges. Regularization and hyperparameter tuning are essential to prevent overfitting and ensure optimal performance, requiring significant expertise to balance model complexity and interpretability (Zulfiqar et al. 2021 ; Touzani et al. 2018 ; Fan et al. 2019 ). K-nearest neighbors KNN is a simple, non-parametric algorithm applied for regression uses. It operates by classifying a new data point based on the majority class of its &#8216;k&#8217; closest training points, using distance metrics like Euclidean distance. KNN does not require explicit model training; instead, it stores all data points and performs predictions by calculating distances between the test point and each stored point. The choice of &#8216;k&#8217; is crucial: a smaller &#8216;k&#8217; makes the model sensitive to noise, while a larger &#8216;k&#8217; may smooth out valuable details. KNN is widely used in applications such as recommendation systems, image recognition, anomaly detection, and medical diagnostics. Despite its simplicity and flexibility, KNN can be computationally expensive, especially with high-dimensional data, requiring efficient distance computations. Additionally, performance heavily depends on the correct selection of &#8216;k&#8217; and distance metric (Holt et al. 1987 ; Bansal et al. 2022 ; Kramer and K-Nearest Neighbors, in Dimensionality Reduction with Unsupervised Nearest Neighbors. 2013 ). Extreme gradient boosting It is an advanced, scalable, and distributed gradient-boosting decision tree context designed to enhance the performance and speed of traditional gradient-boosting methods. By incorporating algorithmic optimizations like parallel tree boosting, it efficiently processes large datasets. Regularization terms, including L1 (Lasso) and L2 (Ridge), are integrated into the loss function to improve model performance and prevent overfitting. XGBoost&#8217;s architecture leverages decision trees as base learners, enhanced by efficient computational strategies such as weighted quantile sketch and column block compression. Its versatility makes it widely used in areas such as fraud detection, bioinformatics, and customer churn analysis, where non-linear relations and large-scale data require precise predictive capabilities. Although powerful, XGBoost&#8217;s complexity demands expertise for fine-tuning hyperparameters, as it can be challenging to interpret and optimize without careful parameter adjustments (Sado et al. 2023 ; Tyralis and Papacharalampous 2021 ). Light gradient boosting machine It&#8217;s a gradient-boosting system that utilizes decision trees as its core learning algorithm that are optimized for efficiency and speed across large datasets. Emphasizing lighter algorithmic infrastructure, it processes input data faster using advanced innovations like the histogram-based algorithm and leaf-wise growth strategy. LightGBM, designed to enhance computational speed without trading off accuracy, introduces exclusive feature bundling (EFB) and gradient-based one-side sampling, which reduce data features and observations during splitting&#8212;improving model precision while diminishing both time and space complexity during learning processes, suited for faster-distributed computing. LightGBM&#8217;s key architectural innovations let it deploy faster, with efficient memory usage through customized data structures such as histograms, enabling faster data partitioning and decision tree expansion. By selecting the best leaf to grow instead of using the standard level-wise strategies, LightGBM achieves high precision in predicting with minimized complexity. The exclusive feature bundling reduces feature dimensionality in datasets, assisting in streamlined processing even with categorical data critical in real-world applications. Moreover, gradient-based one-side sampling confines observed samples to a compact representative, mitigating computational lag during model refinement and reducing overfitting potential. LightGBM&#8217;s efficiency and capacity have made it a favorite choice in high-speed predictive modeling areas such as click-through rate predictions in advertising technology, customer churn prediction in telecommunications, and risk assessment in financial markets, where speed and precision are critical. It is extensively used in environments demanding resource scalability with fast-paced data analytics like retail demand forecasting and supply chain optimization. Its strength in running large-scale data predictions allows LightGBM to address real-world complexity challenges ranging from image processing tasks to genetic studies with massive sample size demands. LightGBM&#8217;s principal advantage is its excellent performance speeds combined with scalable accuracy, particularly in high-dimensional data scenarios. Its capability to handle categorical variables differentiates it distinctly from similar boosting algorithms. However, precise tuning is required to avoid overfitting and optimize performance, necessitating knowledge of various hyperparameters. LightGBM models can be less interpretable than simpler algorithms due to the intricate nature of multi-layered processes, posing challenges when deployed in environments where model transparency is vital. Nonetheless, for applications prioritizing speed and extensive variable handling, LightGBM continues to be a preferred choice. (Fan et al. 2019 ; Guo et al. 2023 ; Taha and Malebary 2020 ). Elastic net It is a versatile regression approach integrating the regularization strengths of both Lasso (L1) and Ridge (L2) to improve model performance, especially where predictors outnumber observations or exhibit significant collinearity. Elastic Net balances between these penalties, facilitating the advantages of variable selection and shrinkage concurrently, without wholly assigning any input values zero coefficients, hence improving upon the limitations encountered with Lasso&#8212;specifically in retaining correlated feature pairs. The inclusion of both penalty types ensures dimensionality reduction power, governed by a dual-tuning parameter controlling the balance effect between Lasso-like sparse representations and Ridge-inspired continuous scaling. Elastic Net is based on a linear model with a loss function penalized by both L1 and L2 norms. Through hyperparameters, lambda for regularization, and alpha determining the mix ratio between L1 and L2 penalties, Elastic Net fits models jointly using weighted combinations of penalties. This combined penalty optimizes performance by robustly shrinking coefficients, leading to better generalization of unseen data by controlling prediction variance. Elastic Net addresses issues like suppression of important predictors and multicollinearity through regularization effects, thus offering a flexible toolkit for hypothesis testing and exploratory modeling where typical regression methods might fail. Due to its sophisticated handling of multicollinearity and feature selection, Elastic Net is extensively applied in machine learning tasks where the number of predictors is substantial. It is used in genomics to interpret vast gene expression data, ensuring model stability while highlighting predictive genes. It holds value in finance, too, where economic forecasting and asset price modeling demand predictive robustness from correlated market features. Elastic Net is instrumental in ensuring impressive, generalizable results, making it a pragmatic choice for complex systems requiring balanced precision, featured influence, and performance from data-rich in dimensional structure and interweaved variabilities. Elastic Net&#8217;s combined penalty approach simultaneously empowers it with dimensionality reduction and predictive refinement capabilities, navigating multicollinear complications and ensuring stable prediction outputs. It enhances flexibility in dealing with diverse data structures, balancing sparsity and continuity in modeling approach useful when both feature selection and interpretability matter. However, setting the parameters for L1 and L2 combinations can be data-intensive, requiring expertise to operate efficiently without bias. The increased computational cost due to its dual regularization nature and potential complexity make Elastic Net less suited for very large datasets unless ample processing resources and expert analysis are involved (Qi and Yang 2022 ; Mokhtari et al. 2020 ). Categorical boosting It is a specialized gradient-boosting manner that is excellently engineered to manage categorical data, which is typically a challenge to standard gradient-based methods. CatBoost intrinsically incorporates sophisticated algorithmic insights to address categorical feature processing, applying a combination of ordered boosting&#8212;ensuring model stability by preventing overfitting&#8212;and innovative techniques for handling categorical data transformation during learning. Employing permutations and gradient-learning frameworks adeptly, CatBoost constructs a robust learning path, expertly optimizing prediction precision while maintaining a high resilience to typical overfit pitfalls induced within the data transformation processes known to plague standard boosting regimes. The design of CatBoost features ordered boosting to reflect the temporal sequence of data rather than isolated instances, preserving underlying temporal patterns across data transformations. Its exclusive feature bundling method aggregates sparse categorical features, minimizing dimensionality while maintaining essential data fidelity. This mechanism enhances computational speed and prediction accuracy. CatBoost leverages sophisticated loss function computation enhancements across robust-level ensemble refinement, differentiating itself broadly from typical gradient boosting frameworks&#8212;catapulting precision in categorical transformation outcomes and boosting robustness toward permutation-induced variability for complex practical applications. CatBoost excels in handling datasets rich with categorical variables and bias prevention needs, facilitating application in a variety of contexts, such as online advertising, where categorical information predominates in click-through predictions. The algorithm also supports customer segmentation tasks and recommendation system improvements across dynamic industries like e-commerce and digital marketing. In domains requiring rigorous analysis and robust prediction across diverse data types, like insurance and healthcare analytics, CatBoost ensures precision in identifying and learning from categorical variable interactions essential to robust decision-making architectures. CatBoost&#8217;s core strengths lie in its tailored approach to managing categorical variables, effectively preventing overfitting through ordered boosted solutions and precise feature handling. Its adeptness in maintaining high accuracy across complex categorical data domains distinguishes it in market competition layers, offering seamless applications with substantially reduced human intervention necessary for categorical encoding. The challenges with CatBoost center primarily on computational resources cost, it demands ample resources when operated on very extensive datasets. Effective initial setup and parameter tuning are crucial, requiring domain expertise to optimize the model engaging in outcomes better suited for high-value, real-world categorical data deployment tasks (Cha et al. 2021 ). Scalability and energy efficiency of hydrochar production The transition of electrocatalytic reactors from laboratory research to industrial application presents a number of significant engineering challenges. These barriers are not related to the catalyst material itself, but rather to the design of the reactor, including limitations in mass transport, energy losses, and the long-term stability of components. A primary challenge is managing mass transport, especially for reactions involving gaseous reactants like CO 2 . The low solubility and slow diffusion of these gases in liquid electrolytes can severely limit the reaction rate at high current densities. To overcome this, engineers have developed Gas Diffusion Electrodes (GDEs), which provide a critical interface where gas, liquid, and solid phases meet, allowing for efficient delivery of reactants to the catalyst. Another major set of barriers involves energy efficiency. There are two main types of energy losses to address. Ohmic losses are caused by electrical resistance in the reactor&#8217;s components, which can be minimized by optimizing electrode spacing and electrolyte conductivity. Kinetic overpotential, on the other hand, is the inherent energy barrier of the reaction itself. This is primarily an issue of catalyst design, and can be improved by developing more active and efficient catalytic materials. Finally, ensuring the long-term stability and durability of the reactor is a key engineering hurdle. GDEs are particularly vulnerable to degradation. Common failure modes include &#8220;flooding,&#8221; where the pores of the electrode become saturated with electrolyte, blocking gas flow. This is often caused by the degradation of the hydrophobic materials within the GDE. Additionally, carbon-based GDEs can suffer from corrosion in the harsh electrochemical environment, leading to a loss of structural integrity. To address these issues, research is focused on developing more durable, and even carbon-free, GDEs. Scalability and energy efficiency of hydrochar production The practical applicability and commercial viability of hydrochar production via hydrothermal carbonization (HTC) are contingent on the process&#8217;s scalability and energy efficiency, aspects often overlooked in laboratory-scale studies. The HTC process offers a distinct advantage over other thermochemical methods, such as pyrolysis, by its ability to process wet biomass feedstocks without an energy-intensive pre-drying step. This inherent efficiency is a key driver for the technology&#8217;s potential for industrial-scale deployment. The HTC reaction itself is exothermic, releasing energy that can be captured and recycled to heat incoming feedstock, thereby reducing overall energy consumption. In a large-scale implementation with sewage sludge, it was shown that only about 20% of the fuel energy content of the final hydrochar product was required to power the process. Specific energy consumption figures from a study on grape marc showed thermal energy and power consumption of 1170 kWh and 160 kWh, respectively, per ton of hydrochar produced. When compared to conventional methods for processing wet waste, HTC can reduce energy consumption for drying by up to 70% by leveraging heat recirculation. The transition of HTC from laboratory to commercial scale, while challenging, is well underway. Key challenges include optimizing reactor design, managing the process water, and ensuring consistent product quality. Despite these obstacles, numerous companies have successfully developed and implemented industrial-scale HTC plants, validating the technology&#8217;s scalability. The process is considered an effective solution for converting a wide range of biomass into valuable materials due to its relatively mild process conditions and industrial scalability. From an economic perspective, cost&#8211;benefit analyses demonstrate the potential for hydrochar to be a profitable product. One techno-economic assessment found a breakeven selling price of $117 per ton for co-hydrochar, while another study calculated a production cost of 157 &#8364;/ton with a break-even point of 200 &#8364;/ton for pelletized hydrochar. The specific mechanical energy required for the process has been reported to be in the range of 83.5 to 152.3 kWh/t, directly addressing the importance of energy cost metrics. Furthermore, one analysis found the total energy consumption per ton of hydrochar-derived sand to be 695.14 kWh, with an energy cost of approximately $41.70 per ton, demonstrating the potential for significant energy costs in downstream processing. This integration of process efficiency and economic feasibility is critical for the long-term adoption of hydrochar as a sustainable product. By converting low-value, high-moisture biomass into a stable, energy-dense solid fuel, HTC offers a solution that not only mitigates waste disposal costs but also contributes to a circular economy by creating a valuable product with a positive energy balance. Data investigation and clarification The data required to build the models were earlier compiled in Shafizadeh et al. ( 2023 ). In this research, the independent inputs include reaction processing circumstances and biomass characteristics. Biomass features are categorized into proximate and ultimate analyses, with the proximate investigation covering fixed carbon, volatile matter, and ash content (all in wt%). For this work, the proximate features are selected as input. As such, the input features are volatile matter, fixed carbon, water content, reaction-time, temperature, and ash-content. The outputs are hydrochar- higher-heating-value (HHV, in MJ/kg) and hydrochar yield (in wt%). The data-bank consists of 481 rows. Figure&#160; 2 A, B present the boxplots of all parameters reflected in the modelling of HHV and yield. In current study, a total of 337 data points were utilized for model training, while 72 points were assigned to both testing and validation processes. Fig.&#160;2 Matrix-plot for A yield B HHV Figure&#160; 3 illustrates the Pearson correlation coefficients analyzed in this study. The Pearson coefficient is a statistical measure used to quantify the power and direction of the linear relationship between two parameters. This coefficient ranges from&#8201;&#8722;&#8201;1 to&#8201;+&#8201;1, where&#8201;+&#8201;1 indicates a perfect positive linear correlation,&#8201;&#8722;&#8201;1 represents a perfect negative linear correlation, and 0 signifies the absence of any relation. The following eaution describes the calculating of the Pearson correlation (Abbasi et al. 2023 ; Bemani et al. 2023 ; Madani and Alipour 2022 ; Madani et al. 2021 ): Fig.&#160;3 Matrix explaining the correlation quantities related to model A yield and B HHV 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${r}_{j}=\\frac{\\sum_{i=1}^{n}({I}_{i.j}-\\overline{{I }_{j}})({Z}_{i}-\\overline{Z })}{\\sqrt{\\sum_{i=1}^{n}{({I}_{i.j}-\\overline{{I }_{j}})}^{2}\\sum_{i=1}^{n}{({Z}_{i}-\\overline{Z })}^{2}}}$$\\end{document} In this formula, Z represents the second variable and is characterized by its averaged amount, Z&#772;, in which &#298; denotes the average amount of the approximate Ij. The relationships among the inputs and the target parameters namely, HHV and yield&#8212;are particularly noteworthy. As shown in Fig.&#160; 3 A, it is evident that each input variable exhibits some degree of correlation with hydrochar yield, with the exception of Ash content, that shows a positive correlation. Other inputs demonstrate a negative correlation with hydrochar yield. Similarly, Fig.&#160; 3 B highlights the correlations among hydrochar HHV and input variables. In this case, parameters such as Temperature, Fixed Carbon, and Volatile Matter exhibit a positive correlation with HHV, while the remaining factors present a negative correlation. Before developing models based on data, it is crucial to guarantee the data quality by identifying and tackling potential outliers. In this work, the Monte Carlo Outlier Detection (MCOD) was utilized due to its success and efficiency when dealing with large data. MCOD syndicates random sampling with density based manners to detect anomalies, defining outliers as data points that deviate significantly from their neighbors in terms of local density. These anomalies are identified by examining the distribution of data points within their local neighborhood. The MCOD algorithm leverages Monte Carlo sampling to select a representative subset of the data, thereby reducing the computational burden significantly. Its primary advantage lies in its efficiency, mostly for high-dimension datasets. By means of random sampling, MCOD evades the need to examine the full dataset directly, resulting in lower computational costs and making it especially appropriate for real-time applications. However, there are trade-offs; the accuracy of the results may depend on factors such as the number of samples and the arrangement of the adjacent neighbors (k). Nevertheless, MCOD remains an invaluable tool for data investigation and outlier detection, principally in scenarios where estimated results are acceptable or computational resources are limited. Its balance between computational effectiveness and detection accuracy is a key asset in identifying outliers in complex data-sets. Figure&#160; 4 A, B present boxplots of the data used for yield and HHV modeling, showcasing the spreading of points and highlighting the ranges suitable for model development. The boxplots indicate that the majority of points fall within the expected range, affirming the consistency and superiority of the data. For training, the entire dataset was utilized to ensure robust model developing. By incorporating the full range of data, the models are equipped to capture essential patterns and variability, enhancing their ability to oversimplify effectively to unseen data and improving calculation accuracy (Jia et al. 2018 ; Rocco and Moreno 2002 ). Fig.&#160;4 Outlier detection via the Monte Carlo and boxplots for the A yield and B HHV data validated the data&#8217;s spreading and confirmed its appropriateness for building dependable models Results and discussions This research uses diverse methods, from linear regression to convolutional neural networks and collective techniques (LightGBM, XGBoost, CatBoost), to forecast hydrochar higher heating value (HHV) and yield from biomass. This approach allows for a inclusive evaluation of model performance. Model accuracy and explanatory power are assessed using the mean squared error (MSE), coefficient of determination (R 2 ), and mean relative deviation percent (MRD%). R 2 measures the model&#8217;s capacity to clarify data variance, while MSE quantifies prediction accuracy, providing a benchmark for comparing models across datasets. The research includes detailed calculations and interpretations of the metrics to clarify implications for HHV and yield prediction (Feller et al. 2004 ; Aghdam et al. 2022 ; Bassir and Madani 2019 ; Shoushtari et al. 2020 ). 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ R{ - }squared\\;\\left( {R^{2} } \\right) = 1 - \\frac{{\\mathop \\sum \\nolimits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - {\\text{y}}_{{\\text{i}}}^{{{\\text{predicted}}}} } \\right)^{2} }}{{\\mathop \\sum \\nolimits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - \\overline{{{\\text{y}}^{{{\\text{real}}}} }} } \\right)^{2} }} $$\\end{document} 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Mean\\;squared\\;error\\;\\left( {MSE} \\right) = \\frac{1}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {y_{i}^{real} - y_{i}^{predicted} } \\right)^{2} $$\\end{document} 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Standard\\;Deviation\\;\\left( \\sigma \\right) = \\sqrt {\\frac{1}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {{\\text{y}}_{{\\text{i}}}^{{{\\text{real}}}} - \\overline{{{\\text{y}}^{{{\\text{real}}}} }} } \\right)^{2} } $$\\end{document} 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Mean\\;relative\\;deviation\\;\\left( {MRE} \\right) = \\frac{100}{N}\\mathop \\sum \\limits_{i = 1}^{N} \\left( {\\frac{{y_{i}^{real} - y_{i}^{predicted} }}{{y_{i}^{real} }}} \\right) $$\\end{document} where, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\varvec{y}}}_{{\\varvec{i}}}^{{\\varvec{r}}{\\varvec{e}}{\\varvec{a}}{\\varvec{l}}}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\varvec{y}}}_{{\\varvec{i}}}^{{\\varvec{p}}{\\varvec{r}}{\\varvec{e}}{\\varvec{d}}{\\varvec{i}}{\\varvec{c}}{\\varvec{t}}{\\varvec{e}}{\\varvec{d}}}$$\\end{document} indicating the real and model output, respectively. Also, N signifies the dataset&#8217;s size. A comparative investigation of several regression models was executed to evaluate their predictive performance on a benchmark dataset. The detailed results, summarized in Table&#160; 1 (for biomass yield prediction) and Table&#160; 2 (for biomass HHV prediction), highlight CatBoost as the best model for both yield and HHV prediction tasks. CatBoost demonstrated a strong correlation with the actual values, along with minimal standard deviation, indicating its high stability and accuracy. Table&#160;1 Created specific calculation metrics to appraise the efficiency of each technique on yield prediction Model Train R 2 Validation R 2 Test R 2 Train MSE Validation MSE Test MSE Train MRD% Validation MRD% Test MRD% ANN 0.90 0.88 0.89 24.9 33.4 34.1 7.5 8.3 9.4 CNN 0.92 0.93 0.89 18.2 20.6 31.5 6.6 7.0 9.0 Linear Regression 0.24 0.36 0.13 183.9 184.8 258.6 23.8 24.5 30.6 Ridge Regression 0.24 0.34 0.14 184.9 190.1 254.7 23.9 25.0 30.7 Lasso Regression 0.24 0.36 0.13 184.1 185.4 257.3 23.8 24.6 30.5 Elastic Net 0.23 0.34 0.14 185.4 192.2 254.4 23.9 25.2 30.7 SVR 0.39 0.42 0.28 148.8 166.9 213.2 21.3 23.5 27.8 Random Forest 0.85 0.84 0.83 35.9 45.4 51.7 7.6 8.5 9.0 Gradient Boosting 0.77 0.74 0.77 56.5 76.5 68.8 11.2 12.5 12.9 KNN 0.71 0.69 0.72 70.9 91.1 81.7 13.5 14.0 15.6 Decision Tree 0.66 0.71 0.73 82.1 83.4 79.9 7.3 8.6 7.5 XGBoost 0.81 0.85 0.82 46.3 42.3 53.6 8.5 10.4 10.1 LightGBM 0.89 0.89 0.84 25.9 33.0 46.5 8.1 8.5 10.6 CatBoost 0.91 0.89 0.90 22.2 32.6 30.4 6.9 7.4 8.1 Table&#160;2 Created specific calculation metrics to review the effectiveness of each technique Model Train R 2 Validation R 2 Test R 2 Train MSE Validation MSE Test MSE Train MRD% Validation MRD% Test MRD% ANN 0.85 0.88 0.82 3.6 2.2 4.5 6.0 5.2 7.1 CNN 0.92 0.93 0.89 1.9 1.4 2.8 4.6 4.3 5.4 Linear Regression 0.57 0.67 0.58 10.4 6.3 10.5 11.0 9.5 11.9 Ridge Regression 0.57 0.67 0.58 10.4 6.3 10.5 11.1 9.5 11.9 Lasso Regression 0.57 0.67 0.58 10.4 6.3 10.5 11.0 9.5 11.8 Elastic Net 0.57 0.67 0.58 10.4 6.3 10.5 11.1 9.5 11.8 SVR 0.65 0.80 0.63 8.5 3.8 9.2 9.1 6.2 9.7 Random Forest 0.93 0.97 0.79 1.7 0.6 5.2 2.9 2.5 4.2 Gradient Boosting 0.90 0.90 0.84 2.5 2.0 3.9 5.0 5.1 5.8 KNN 0.82 0.89 0.82 4.5 2.1 4.6 6.4 5.1 7.1 Decision Tree 0.91 0.99 0.70 2.2 0.2 7.5 1.2 0.3 3.0 XGBoost 0.86 0.91 0.79 3.3 1.8 5.2 4.9 4.4 5.3 LightGBM 0.94 0.95 0.90 1.5 1.1 2.5 3.8 3.8 4.2 CatBoost 0.96 0.97 0.93 0.9 0.5 1.6 2.9 2.5 3.4 ANN 0.85 0.88 0.82 3.6 2.2 4.5 6.0 5.2 7.1 Specifically, Table&#160; 1 shows that CatBoost achieved an R 2 value of 0.90 for biomass yield prediction, while Table&#160; 2 indicates an R 2 value of 0.93 for HHV prediction. These values suggest that CatBoost effectively captures the basic relations in the data. It also exhibited the lowest MSE values, 30.38 for yield prediction and 1.63 for HHV prediction. Additionally, the Mean Relative Deviation (MRD) values were minimal, with 8.09% for yield prediction and 3.4% for HHV prediction, further confirming its superior performance in both regression tasks. Conversely, simpler models including Lasso Regression, Linear Regression, Ridge Regression, Decision Trees, Elastic Net, and K-Nearest Neighbors (KNN) showed significantly poorer performance. These models showed lower R 2 values and higher MSE values for both yield and HHV predictions. This performance disparity is consistent with prior research, which has demonstrated that gradient-boosting methods like CatBoost tend to outperform traditional models in complex regression tasks. The findings in this study emphasize the advantages of using advanced models like CatBoost for regression tasks in biomass yield and HHV prediction, especially when compared to simpler, more traditional approaches (Ajin et al. 2024 ; Vishwakarma et al. 2024 ). Current work hires graphical techniques, such as cross plots and relative deviation values, to evaluate the reliability of various algorithms in predicting hydrochar HHV and yield biomass proximate analysis. These visual tools are instrumental in providing a comprehensive understanding of the models&#8217; performances, effectively highlighting patterns and discrepancies between the predicted and real data. Figures&#160; 5 and 6 present a comparison of the calculated and real data across different datasets, for all established models in predicting yield and HHV. The investigation reveals that the CatBoost shows near-perfect alignment between the real and modeled reults, demonstrating its great abilities. This close agreement underscores CatBoost&#8217;s strength in capturing the complicated relations within the data. Fig.&#160;5 The accuracy of models for HHV prediction determined by comparing their predicted outcomes against observed values Fig.&#160;6 The accuracy of models for HHV prediction determined by comparing their predicted outcomes against observed values Additionally, Figs.&#160; 7 and 8 introduce cross-validation plots that display the correlations between real and calculated amounts for all models in predicting hydrochar yield and HHV. The CatBoost shows a clear trend where data points for both parameters cluster tightly around the y&#8201;=&#8201;x bisector, with the fitted lines strictly mirroring this ideal relationship.This alignment serves as a testament to the model&#8217;s strong prediction, confirming its proficiency in accurately forecasting outcomes that align with real-world observations. Fig.&#160;7 A visual assessment of the model&#8217;s predictive capability for yield, performed using cross-plots Fig.&#160;8 A visual assessment of the model&#8217;s predictive capability for HHV performed using cross-plots Figures&#160; 9 and 10 explore error distributions over scatter plot of relative errors in predicting HHV and yield. In the case of the CatBoost model, the errors are evenly spread nearby the x-axis, signifying minimal variance and reliable prediction accuracy. This further reinforces the reliability of the model, enhancing confidence in its forecasting ability. Fig.&#160;9 A comprehensive analysis of relative deviation percentages provided for all models employed in biomass yield prediction Fig.&#160;10 Relative deviation percentages meticulously detailed for the cohorts across the entire models employed for biomass yield prediction Lastly, Figs.&#160; 11 and 12 illustrate the prediction distribution of all models across the different modeling phases for forcasing the yield and HHV. The performance of models in predicting HHV is evaluated by comparing the models&#8217; outputs with empirical observations.This consistency highlights CatBoost&#8217;s robustness and stability, positioning it as the most reliable model for real-world usages. Fig.&#160;11 Information on the frequency of the data sets employed for biomass yield Fig.&#160;12 Information on the frequency of the data sets employed for biomass HHV prediction To truly understand how machine learning models make predictions, it&#8217;s essential to know the significance of each input feature. In this research, we utilize Shapley Additive exPlanations (SHAP). This powerful method, rooted in game-theoretic principles, provides a clear and rigorous framework for interpreting model outputs, revealing precisely how individual features contribute to predictions. Figure 13 A, B display the SHAP results for the inputs, along with the feature status derived from the Random Forest for calculating hydrochar HHV and yield regarding biomass proximate. The parameters are ranked in downward order according to their SHAP amounts, with the highest-ranked features exerting the most significant influence on model results. The investigation indicates that ash content is the primary determinant for HHV prediction, while temperature, water content, and fixed carbon are identified as the most crucial parameters for predicting yield. These findings are instrumental in identifying the key factors that affect hydrochar HHV and yield in the context of biomass proximate analysis. By providing intuitions into the relationships between inputs and desired variables, better performance is facilitated, enhancing its utility in real-world industries. Additionally, clarity regarding feature significance is essential for guiding future research and optimization strategies, enabling decisions by quantifying the unique influence of each parameter on predictions. Fig.&#160;13 Random Forest and Mean SHAP Insights into feature contributions for prediction A yield and B HHV parameters The predictive power of our machine learning models stems from the inherent relationships between the proximate analysis of the raw biomass and the fundamental chemical and physical changes occurring during hydrothermal carbonization (HTC). A deeper mechanistic understanding of these relationships is essential for a more complete interpretation of our model&#8217;s predictions. The fixed carbon (FC) content of the biomass is a key parameter directly influencing the final HHV of the hydrochar. Fixed carbon represents the non-volatile combustible material left after the release of volatile matter. During the HTC process, while some devolatilization occurs, the fixed carbon fraction remains largely intact. As a result, biomass with a higher initial FC content will naturally produce a hydrochar with a greater proportion of fixed carbon, thereby increasing its energy density and resulting in a higher HHV. Our models effectively capture this direct correlation, demonstrating that a higher FC content in the feedstock is a strong positive predictor for the final hydrochar HHV. Conversely, the volatile matter (VM) content is a primary factor governing the final hydrochar yield. Volatile matter consists of organic compounds that are released as gases and liquids during the HTC process. This release of VM from the solid biomass matrix represents a significant mass loss. Consequently, biomass feedstocks with higher volatile matter content will undergo a greater degree of devolatilization and decomposition, leading to a lower overall hydrochar yield. Our models&#8217; ability to predict yield based on VM content highlights the critical role of this parameter in determining the efficiency of solid fuel recovery during HTC. Finally, the ash content and moisture content of the biomass also play important, albeit different, roles. Ash is an inert component that does not contribute to the combustion process. Therefore, a higher ash content in the feedstock dilutes the combustible fixed carbon and volatile matter in the resulting hydrochar, which invariably lowers its HHV. This effect is consistently captured by our models, reinforcing the importance of using low-ash biomass for high-quality hydrochar production. The initial moisture content acts as the reaction medium for the HTC process. While not a direct component of the final hydrochar&#8217;s chemical makeup, it influences the reaction kinetics, including the temperature and pressure profiles. These conditions affect the extent of carbonization and dehydration reactions, ultimately having a secondary but significant impact on both the hydrochar yield and HHV. The success of our models in predicting these properties from moisture content suggests that the models have learned these complex, indirect relationships. This mechanistic analysis validates our models&#8217; predictive capabilities and provides practical insights for optimizing the hydrochar production process. By controlling the input biomass&#8217;s proximate analysis, particularly by selecting feedstocks with high fixed carbon, low volatile matter, and low ash content, it is possible to produce a high-yield, high-HHV hydrochar, thereby maximizing the efficiency of this sustainable energy conversion pathway. Conclusion This research delivers a inclusive investigation of biomass proximate analysis for forecasting hydrochar-yield and higher-heating-value (HHV) by machine learning. Key parameters such as fixed carbon, temperature,volatile matter, ash content, reaction time, and water content were analyzed, with Pearson correlation revealing their influence on hydrochar yield and HHV. The Monte Carlo Outlier Detecting (MCOD) ensured data reliability, enhancing the success of the modeling. Among the models evaluated, CatBoost outperformed others with R 2 values of 0.90 (yield prediction) and 0.93 (HHV prediction), as well as low mean relative deviation (MRD) and mean squared error (MSE). The study highpoints the superiority of gradient-boosting methods over simpler models in capturing complex relationships. Visual validation tools further confirmed CatBoost&#8217;s accuracy and consistency, while SHAP analysis identified ash content as the key factor in HHV prediction and temperature, water content, and fixed carbon as critical for yield prediction. In conclusion, this research emphasizes the potential of advanced machine learning models, particularly gradient-boosting algorithms like CatBoost, in optimizing biomass-to-hydrochar conversion processes. These findings hold significant implications for improving bioenergy production and guiding future research in biomass modeling and conversion. Supplementary Information Additional file1. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements This work was supported by the Natural Science Foundation of Changchun Normal University (2021003). Author contributions GH and AA performed software and coding. SH, SS, AS did the formal study. AA, VS, VJ provided the methodology and exact steps needed to be done. SH, VS, VJ, AK curated the data and validated the results. AY, SS, AS, GH visualized the plots and wrote the manuscript. AK finalized the manuscript and supervised the whole project. All authors read and approved the final manuscript. Data availability Data is available in the supplementary material file. Declarations Ethics approval and consent to participate Not applicable. Consent for publication All authors agree to publish this work. Competing interests None. References Abbasi P Aghdam SK-y Madani M Modeling subcritical multi-phase flow through surface chokes with new production parameters Flow Meas Instrum 2023 89 102293 10.1016/j.flowmeasinst.2022.102293 Abbasi P, Aghdam SK-y, Madani M (2023) Modeling subcritical multi-phase flow through surface chokes with new production parameters. Flow Meas Instrum 89:102293 Aghdam SK-y Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study Arab J Sci Eng 2022 10.1007/s13369-022-07552-4 Aghdam SK-y et al (2022) Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study. Arab J Sci Eng. 10.1007/s13369-022-07552-4 Aghdam SK-y Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study Arab J Sci Eng 2023 48 7 9461 9476 10.1007/s13369-022-07552-4 Aghdam SK-y et al (2023) Thermodynamic modeling of saponin adsorption behavior on sandstone rocks: an experimental study. Arab J Sci Eng 48(7):9461&#8211;9476 Ahmadi MA Evolving artificial neural network and imperialist competitive algorithm for prediction oil flow rate of the reservoir Appl Soft Comput 2013 13 2 1085 1098 10.1016/j.asoc.2012.10.009 Ahmadi MA et al (2013) Evolving artificial neural network and imperialist competitive algorithm for prediction oil flow rate of the reservoir. Appl Soft Comput 13(2):1085&#8211;1098 Ajin RS Segoni S Fanti R Optimization of SVR and CatBoost models using metaheuristic algorithms to assess landslide susceptibility Sci Rep 2024 14 1 24851 10.1038/s41598-024-72663-x 39438526 PMC11496660 Ajin RS, Segoni S, Fanti R (2024) Optimization of SVR and CatBoost models using metaheuristic algorithms to assess landslide susceptibility. Sci Rep 14(1):24851 39438526 10.1038/s41598-024-72663-x PMC11496660 Aloysius N, Geetha M (2017) A review on deep convolutional neural networks. In: 2017 International Conference on Communication and Signal Processing (ICCSP). Ao Y The linear random forest algorithm and its advantages in machine learning assisted logging regression modeling J Pet Sci Eng 2019 174 776 789 10.1016/j.petrol.2018.11.067 Ao Y et al (2019) The linear random forest algorithm and its advantages in machine learning assisted logging regression modeling. J Pet Sci Eng 174:776&#8211;789 Bansal M Goyal A Choudhary A A comparative analysis of K-nearest neighbor, genetic, support vector machine, decision tree, and long short term memory algorithms in machine learning Decis Anal J 2022 3 100071 10.1016/j.dajour.2022.100071 Bansal M, Goyal A, Choudhary A (2022) A comparative analysis of K-nearest neighbor, genetic, support vector machine, decision tree, and long short term memory algorithms in machine learning. Decis Anal J 3:100071 Bassir SM Madani M Predicting asphaltene precipitation during titration of diluted crude oil with paraffin using artificial neural network (ANN) Pet Sci Technol 2019 37 24 2397 2403 10.1080/10916466.2019.1570261 Bassir SM, Madani M (2019) Predicting asphaltene precipitation during titration of diluted crude oil with paraffin using artificial neural network (ANN). Pet Sci Technol 37(24):2397&#8211;2403 Bemani A Madani M Kazemi A Machine learning-based estimation of nano-lubricants viscosity in different operating conditions Fuel 2023 352 129102 10.1016/j.fuel.2023.129102 Bemani A, Madani M, Kazemi A (2023) Machine learning-based estimation of nano-lubricants viscosity in different operating conditions. Fuel 352:129102 Bhutto AW Promoting sustainability of use of biomass as energy resource: Pakistan&#8217;s perspective Environ Sci Pollut Res Int 2019 26 29606 29619 10.1007/s11356-019-06179-7 31452125 Bhutto AW et al (2019) Promoting sustainability of use of biomass as energy resource: Pakistan&#8217;s perspective. Environ Sci Pollut Res Int 26:29606&#8211;29619 31452125 10.1007/s11356-019-06179-7 Cha G-W Moon H-J Kim Y-C Comparison of random forest and gradient boosting machine models for predicting demolition waste based on small datasets and categorical variables Int J Environ Res Public Health 2021 10.3390/ijerph18168530 34444277 PMC8392226 Cha G-W, Moon H-J, Kim Y-C (2021) Comparison of random forest and gradient boosting machine models for predicting demolition waste based on small datasets and categorical variables. Int J Environ Res Public Health. 10.3390/ijerph18168530 34444277 10.3390/ijerph18168530 PMC8392226 Chagas P et al. (2018) Evaluation of convolutional neural network architectures for chart image classification. In: 2018 International Joint Conference on Neural Networks (IJCNN). Chen J A comparison of linear regression, regularization, and machine learning algorithms to develop Europe-wide spatial models of fine particles and nitrogen dioxide Environ Int 2019 130 104934 10.1016/j.envint.2019.104934 31229871 Chen J et al (2019) A comparison of linear regression, regularization, and machine learning algorithms to develop Europe-wide spatial models of fine particles and nitrogen dioxide. Environ Int 130:104934 31229871 10.1016/j.envint.2019.104934 Chen W-H A comparative analysis of biomass torrefaction severity index prediction from machine learning Appl Energy 2022 324 119689 10.1016/j.apenergy.2022.119689 Chen W-H et al (2022a) A comparative analysis of biomass torrefaction severity index prediction from machine learning. Appl Energy 324:119689 Chen W-H Forecast of glucose production from biomass wet torrefaction using statistical approach along with multivariate adaptive regression splines, neural network and decision tree Appl Energy 2022 324 119775 10.1016/j.apenergy.2022.119775 Chen W-H et al (2022b) Forecast of glucose production from biomass wet torrefaction using statistical approach along with multivariate adaptive regression splines, neural network and decision tree. Appl Energy 324:119775 Dou Z Development and process simulation of a biomass driven SOFC-based electricity and ammonia production plant using green hydrogen; AI-based machine learning-assisted tri-objective optimization Int J Hydrogen Energy 2025 133 440 457 10.1016/j.ijhydene.2025.04.497 Dou Z et al (2025) Development and process simulation of a biomass driven SOFC-based electricity and ammonia production plant using green hydrogen; AI-based machine learning-assisted tri-objective optimization. Int J Hydrogen Energy 133:440&#8211;457 Emmert-Streib F Dehmer M High-dimensional LASSO-based computational regression models: regularization, shrinkage, and selection Machine Learn Knowl Extraction 2019 1 1 359 383 10.3390/make1010021 Emmert-Streib F, Dehmer M (2019) High-dimensional LASSO-based computational regression models: regularization, shrinkage, and selection. Machine Learn Knowl Extraction 1(1):359&#8211;383 Fan J Light gradient boosting machine: an efficient soft computing model for estimating daily reference evapotranspiration with local and external meteorological data Agric Water Manage 2019 225 105758 10.1016/j.agwat.2019.105758 Fan J et al (2019) Light gradient boosting machine: an efficient soft computing model for estimating daily reference evapotranspiration with local and external meteorological data. Agric Water Manage 225:105758 Feller N MRD parameters using immunophenotypic detection methods are highly reliable in predicting survival in acute myeloid leukaemia Leukemia 2004 18 8 1380 1390 10.1038/sj.leu.2403405 15201848 Feller N et al (2004) MRD parameters using immunophenotypic detection methods are highly reliable in predicting survival in acute myeloid leukaemia. Leukemia 18(8):1380&#8211;1390 15201848 10.1038/sj.leu.2403405 Feng W et al (2020) FSRF: An improved random forest for classification. In: 2020 IEEE International Conference on Advances in Electrical Engineering and Computer Applications( AEECA). Gardner MW Dorling SR Artificial neural networks (the multilayer perceptron)&#8212;a review of applications in the atmospheric sciences Atmos Environ 1998 32 14 2627 2636 10.1016/S1352-2310(97)00447-0 Gardner MW, Dorling SR (1998) Artificial neural networks (the multilayer perceptron)&#8212;A review of applications in the atmospheric sciences. Atmos Environ 32(14):2627&#8211;2636. 10.1016/S1352-2310(97)00447-0 Ghiasi MM Zendehboudi S Mohsenipour AA Decision tree-based diagnosis of coronary artery disease: CART model Comput Methods Programs Biomed 2020 192 105400 10.1016/j.cmpb.2020.105400 32179311 Ghiasi MM, Zendehboudi S, Mohsenipour AA (2020) Decision tree-based diagnosis of coronary artery disease: CART model. Comput Methods Programs Biomed 192:105400 32179311 10.1016/j.cmpb.2020.105400 Guo X Wang J Comparison of linearization methods for modeling the Langmuir adsorption isotherm J Mol Liq 2019 296 111850 10.1016/j.molliq.2019.111850 Guo X, Wang J (2019) Comparison of linearization methods for modeling the Langmuir adsorption isotherm. J Mol Liq 296:111850 Guo J Prediction of heating and cooling loads based on light gradient boosting machine algorithms Build Environ 2023 236 110252 10.1016/j.buildenv.2023.110252 Guo J et al (2023) Prediction of heating and cooling loads based on light gradient boosting machine algorithms. Build Environ 236:110252 Hai A Machine learning models for the prediction of total yield and specific surface area of biochar derived from agricultural biomass by pyrolysis Environ Technol Innov 2023 30 103071 10.1016/j.eti.2023.103071 Hai A et al (2023) Machine learning models for the prediction of total yield and specific surface area of biochar derived from agricultural biomass by pyrolysis. Environ Technol Innov 30:103071 Hautaniemi S Modeling of signal&#8211;response cascades using decision tree analysis Bioinformatics 2005 21 9 2027 2035 10.1093/bioinformatics/bti278 15657095 Hautaniemi S et al (2005) Modeling of signal&#8211;response cascades using decision tree analysis. Bioinformatics 21(9):2027&#8211;2035 15657095 10.1093/bioinformatics/bti278 Heidari E Sobati MA Movahedirad S Accurate prediction of nanofluid viscosity using a multilayer perceptron artificial neural network (MLP-ANN) Chemometr Intell Lab Syst 2016 155 73 85 10.1016/j.chemolab.2016.03.031 Heidari E, Sobati MA, Movahedirad S (2016) Accurate prediction of nanofluid viscosity using a multilayer perceptron artificial neural network (MLP-ANN). Chemometr Intell Lab Syst 155:73&#8211;85 Hoerl RW Ridge regression: a historical context Technometrics 2020 62 4 420 425 10.1080/00401706.2020.1742207 Hoerl RW (2020) Ridge regression: a historical context. Technometrics 62(4):420&#8211;425 Hoerl AE Kennard RW Ridge regression: applications to nonorthogonal problems Technometrics 1970 12 1 69 82 10.1080/00401706.1970.10488635 Hoerl AE, Kennard RW (1970a) Ridge regression: applications to nonorthogonal problems. Technometrics 12(1):69&#8211;82 Hoerl AE Kennard RW Ridge regression: biased estimation for nonorthogonal problems Technometrics 1970 12 1 55 67 10.1080/00401706.1970.10488634 Hoerl AE, Kennard RW (1970b) Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12(1):55&#8211;67 Holt CA Use of potassium/lime drilling-fluid system in Navarin Basin drilling SPE Drill Eng 1987 2 04 323 330 10.2118/14755-PA Holt CA et al (1987) Use of potassium/lime drilling-fluid system in Navarin Basin drilling. SPE Drill Eng 2(04):323&#8211;330 Hope TMH (2020) Chapter 4 - Linear regression. In: Machine Learning, Mechelli A, Vieira S, (Eds). Academic Press. p 67&#8211;81. Jia Y Yu S Ma J Intelligent interpolation by Monte Carlo machine learning Geophysics 2018 83 2 V83 V97 10.1190/geo2017-0294.1 Jia Y, Yu S, Ma J (2018) Intelligent interpolation by Monte Carlo machine learning. Geophysics 83(2):V83&#8211;V97 Kang J et al LASSO-Based machine learning algorithm for prediction of lymph node metastasis in T1 colorectal cancer. crt, 2020. 53 (3): p. 773&#8211;783. 10.4143/crt.2020.974 PMC8291173 33421980 Kavitha S, Varuna S, Ramya R (2016) A comparative analysis on linear regression and support vector regression. In: 2016 Online International Conference on Green Engineering and Technologies (IC-GET) Khan MR et al. Machine learning application for oil rate prediction in artificial gas lift wells. In: SPE Middle East Oil and Gas Show and Conference. 2019. Khezerlooe-ye Aghdam S Mechanistic assessment of Seidlitzia rosmarinus-derived surfactant for restraining shale hydration: a comprehensive experimental investigation Chem Eng Res des 2019 147 570 578 10.1016/j.cherd.2019.05.042 Khezerlooe-ye Aghdam S et al (2019) Mechanistic assessment of Seidlitzia rosmarinus-derived surfactant for restraining shale hydration: a comprehensive experimental investigation. Chem Eng Res des 147:570&#8211;578 Kim P Convolutional Neural Network MATLAB deep learning: with machine learning, neural networks and artificial intelligence 2017 Berkeley, CA Apress 121 147 Kim P (2017) Convolutional Neural Network. MATLAB deep learning: with machine learning, neural networks and artificial intelligence. Apress, Berkeley, CA, pp 121&#8211;147 Kramer O K-nearest neighbors, in dimensionality reduction with unsupervised nearest neighbors 2013 Berlin, Heidelberg Springer 13 23 Kramer O (2013) K-nearest neighbors, in dimensionality reduction with unsupervised nearest neighbors. Springer, Berlin, Heidelberg, pp 13&#8211;23 Li Z A survey of convolutional neural networks: analysis, applications, and prospects IEEE Trans Neural Netw Learn Syst 2022 33 12 6999 7019 10.1109/TNNLS.2021.3084827 34111009 Li Z et al (2022) A survey of convolutional neural networks: analysis, applications, and prospects. IEEE Trans Neural Netw Learn Syst 33(12):6999&#8211;7019 34111009 10.1109/TNNLS.2021.3084827 Li X Catalytic cracking of biomass tar for hydrogen-rich gas production: parameter optimization using response surface methodology combined with deterministic finite automaton Renew Energy 2025 241 122368 10.1016/j.renene.2025.122368 Li X et al (2025) Catalytic cracking of biomass tar for hydrogen-rich gas production: parameter optimization using response surface methodology combined with deterministic finite automaton. Renew Energy 241:122368 Liu F Organics composition and microbial analysis reveal the different roles of biochar and hydrochar in affecting methane oxidation from paddy soil Sci Total Environ 2022 843 157036 10.1016/j.scitotenv.2022.157036 35772551 Liu F et al (2022) Organics composition and microbial analysis reveal the different roles of biochar and hydrochar in affecting methane oxidation from paddy soil. Sci Total Environ 843:157036 35772551 10.1016/j.scitotenv.2022.157036 Lopez Pinaya WH et al (2020) Chapter 10 - Convolutional neural networks. In: Machine learning Mechelli A, Vieira S (Eds). Academic Press. p 173&#8211;191. Madani M Alipour M Gas-oil gravity drainage mechanism in fractured oil reservoirs: surrogate model development and sensitivity analysis Comput Geosci 2022 26 5 1323 1343 10.1007/s10596-022-10161-7 Madani M, Alipour M (2022) Gas-oil gravity drainage mechanism in fractured oil reservoirs: surrogate model development and sensitivity analysis. Comput Geosci 26(5):1323&#8211;1343 Madani M Moraveji MK Sharifi M Modeling apparent viscosity of waxy crude oils doped with polymeric wax inhibitors J Pet Sci Eng 2021 196 108076 10.1016/j.petrol.2020.108076 Madani M, Moraveji MK, Sharifi M (2021) Modeling apparent viscosity of waxy crude oils doped with polymeric wax inhibitors. J Pet Sci Eng 196:108076 Mokhtari S Navidi W Mooney M White-box regression (elastic net) modeling of earth pressure balance shield machine advance rate Autom Constr 2020 115 103208 10.1016/j.autcon.2020.103208 Mokhtari S, Navidi W, Mooney M (2020) White-box regression (elastic net) modeling of earth pressure balance shield machine advance rate. Autom Constr 115:103208 Niu X Thermodynamic analysis of supercritical Brayton cycles using CO2-based binary mixtures for solar power tower system application Energy 2022 254 124286 10.1016/j.energy.2022.124286 Niu X et al (2022) Thermodynamic analysis of supercritical Brayton cycles using CO2-based binary mixtures for solar power tower system application. Energy 254:124286 Paula AJ Machine learning and natural language processing enable a data-oriented experimental design approach for producing biochar and hydrochar from biomass Chem Mater 2022 34 3 979 990 10.1021/acs.chemmater.1c02961 Paula AJ et al (2022) Machine learning and natural language processing enable a data-oriented experimental design approach for producing biochar and hydrochar from biomass. Chem Mater 34(3):979&#8211;990 Qi K Yang H Elastic net nonparallel hyperplane support vector machine and its geometrical rationality IEEE Trans Neural Netw Learn Syst 2022 33 12 7199 7209 10.1109/TNNLS.2021.3084404 34097622 Qi K, Yang H (2022) Elastic net nonparallel hyperplane support vector machine and its geometrical rationality. IEEE Trans Neural Netw Learn Syst 33(12):7199&#8211;7209 34097622 10.1109/TNNLS.2021.3084404 Rigatti SJ Random forest J Insur Med 2017 47 1 31 39 10.17849/insm-47-01-31-39.1 28836909 Rigatti SJ (2017) Random forest. J Insur Med 47(1):31&#8211;39 28836909 10.17849/insm-47-01-31-39.1 Rocco CM Moreno JA Fast Monte Carlo reliability evaluation using support vector machine Reliab Eng Syst Saf 2002 76 3 237 243 10.1016/S0951-8320(02)00015-7 Rocco CM, Moreno JA (2002) Fast Monte Carlo reliability evaluation using support vector machine. Reliab Eng Syst Saf 76(3):237&#8211;243 Roth V The generalized LASSO IEEE Trans Neural Netw 2004 15 1 16 28 10.1109/TNN.2003.809398 15387244 Roth V (2004) The generalized LASSO. IEEE Trans Neural Netw 15(1):16&#8211;28 15387244 10.1109/TNN.2003.809398 Sado S Current state of application of machine learning for investigation of MgO-C refractories: a review Materials 2023 16 23 7396 10.3390/ma16237396 38068140 PMC10707161 Sado S et al (2023) Current state of application of machine learning for investigation of MgO-C refractories: a review. Materials 16(23):7396 38068140 10.3390/ma16237396 PMC10707161 Sarica A Cerasa A Quattrone A Random forest algorithm for the classification of neuroimaging data in Alzheimer&#8217;s disease: a systematic review Front Aging Neurosci 2017 10.3389/fnagi.2017.00329 29056906 PMC5635046 Sarica A, Cerasa A, Quattrone A (2017) Random forest algorithm for the classification of neuroimaging data in Alzheimer&#8217;s disease: a systematic review. Front Aging Neurosci. 10.3389/fnagi.2017.00329 29056906 10.3389/fnagi.2017.00329 PMC5635046 Shafizadeh A Machine learning predicts and optimizes hydrothermal liquefaction of biomass Chem Eng J (Lausanne) 2022 445 136579 10.1016/j.cej.2022.136579 Shafizadeh A et al (2022) Machine learning predicts and optimizes hydrothermal liquefaction of biomass. Chem Eng J (Lausanne) 445:136579 Shafizadeh A Machine learning-based characterization of hydrochar from biomass: implications for sustainable energy and material production Fuel 2023 347 128467 10.1016/j.fuel.2023.128467 Shafizadeh A et al (2023) Machine learning-based characterization of hydrochar from biomass: implications for sustainable energy and material production. Fuel 347:128467 Shi Z Combined microbial transcript and metabolic analysis reveals the different roles of hydrochar and biochar in promoting anaerobic digestion of waste activated sludge Water Res 2021 205 117679 10.1016/j.watres.2021.117679 34600232 Shi Z et al (2021) Combined microbial transcript and metabolic analysis reveals the different roles of hydrochar and biochar in promoting anaerobic digestion of waste activated sludge. Water Res 205:117679 34600232 10.1016/j.watres.2021.117679 Shoushtari AB Asadolahpour SR Madani M Thermodynamic investigation of asphaltene precipitation and deposition profile in wellbore: a case study J Mol Liq 2020 320 114468 10.1016/j.molliq.2020.114468 Shoushtari AB, Asadolahpour SR, Madani M (2020) Thermodynamic investigation of asphaltene precipitation and deposition profile in wellbore: a case study. J Mol Liq 320:114468 Smith G Campbell F A critique of some ridge regression methods J Am Stat Assoc 1980 75 369 74 81 10.1080/01621459.1980.10477428 Smith G, Campbell F (1980) A critique of some ridge regression methods. J Am Stat Assoc 75(369):74&#8211;81 Smola AJ Sch&#246;lkopf B A tutorial on support vector regression Stat Comput 2004 14 3 199 222 10.1023/B:STCO.0000035301.49549.88 Smola AJ, Sch&#246;lkopf B (2004) A tutorial on support vector regression. Stat Comput 14(3):199&#8211;222 Taha AA Malebary SJ An intelligent approach to credit card fraud detection using an optimized light gradient boosting machine IEEE Access 2020 8 25579 25587 10.1109/ACCESS.2020.2971354 Taha AA, Malebary SJ (2020) An intelligent approach to credit card fraud detection using an optimized light gradient boosting machine. IEEE Access 8:25579&#8211;25587 Tauro R An integrated user-friendly web-based spatial platform for bioenergy planning Biomass Bioenergy 2021 145 105939 10.1016/j.biombioe.2020.105939 Tauro R et al (2021) An integrated user-friendly web-based spatial platform for bioenergy planning. Biomass Bioenergy 145:105939 Touzani S Granderson J Fernandes S Gradient boosting machine for modeling the energy consumption of commercial buildings Energy Build 2018 158 1533 1543 10.1016/j.enbuild.2017.11.039 Touzani S, Granderson J, Fernandes S (2018) Gradient boosting machine for modeling the energy consumption of commercial buildings. Energy Build 158:1533&#8211;1543 Tsarpali M Kuhn JN Philippidis GP Hydrothermal carbonization of residual algal biomass for production of hydrochar as a biobased metal adsorbent Sustainability 2022 14 1 455 10.3390/su14010455 Tsarpali M, Kuhn JN, Philippidis GP (2022) Hydrothermal carbonization of residual algal biomass for production of hydrochar as a biobased metal adsorbent. Sustainability 14(1):455 Tyralis H Papacharalampous G Boosting algorithms in energy research: a systematic review Neural Comput Appl 2021 33 21 14101 14117 10.1007/s00521-021-05995-8 Tyralis H, Papacharalampous G (2021) Boosting algorithms in energy research: a systematic review. Neural Comput Appl 33(21):14101&#8211;14117 Vishwakarma DK Evaluation of CatBoost method for predicting weekly pan evaporation in subtropical and sub-humid regions Pure Appl Geophys 2024 181 2 719 747 10.1007/s00024-023-03426-4 Vishwakarma DK et al (2024) Evaluation of CatBoost method for predicting weekly pan evaporation in subtropical and sub-humid regions. Pure Appl Geophys 181(2):719&#8211;747 Xiong J-b Study on the hydrothermal carbonization of swine manure: the effect of process parameters on the yield/properties of hydrochar and process water J Anal Appl Pyrolysis 2019 144 104692 10.1016/j.jaap.2019.104692 Xiong J-b et al (2019) Study on the hydrothermal carbonization of swine manure: the effect of process parameters on the yield/properties of hydrochar and process water. J Anal Appl Pyrolysis 144:104692 Xu Z-X Benign-by-design N-doped carbonaceous materials obtained from the hydrothermal carbonization of sewage sludge for supercapacitor applications Green Chem 2020 22 12 3885 3895 10.1039/D0GC01272F Xu Z-X et al (2020) Benign-by-design N-doped carbonaceous materials obtained from the hydrothermal carbonization of sewage sludge for supercapacitor applications. Green Chem 22(12):3885&#8211;3895 Xu S Effect of biomass type and pyrolysis temperature on nitrogen in biochar, and the comparison with hydrochar Fuel 2021 291 120128 10.1016/j.fuel.2021.120128 Xu S et al (2021) Effect of biomass type and pyrolysis temperature on nitrogen in biochar, and the comparison with hydrochar. Fuel 291:120128 Yang Y Biomass microwave pyrolysis characterization by machine learning for sustainable rural biorefineries Renew Energy 2022 201 70 86 10.1016/j.renene.2022.11.028 Yang Y et al (2022) Biomass microwave pyrolysis characterization by machine learning for sustainable rural biorefineries. Renew Energy 201:70&#8211;86 Zhang Y Effects of temperature, time and acidity of hydrothermal carbonization on the hydrochar properties and nitrogen recovery from corn stover Biomass Bioenergy 2019 122 175 182 10.1016/j.biombioe.2019.01.035 Zhang Y et al (2019) Effects of temperature, time and acidity of hydrothermal carbonization on the hydrochar properties and nitrogen recovery from corn stover. Biomass Bioenergy 122:175&#8211;182 Zhang F, O&#8217;Donnell LJ (2020) Chapter 7 - Support vector regression. In: Machine learning, Mechelli A, Vieira S (Eds.). Academic Press. p 123&#8211;140. Zhu Q (2022) Treatment and prevention of stuck pipe based on artificial neural networks analysis. In: Offshore Technology Conference Asia Zulfiqar H Identification of cyclin protein using gradient boost decision tree algorithm Comput Struct Biotechnol J 2021 19 4123 4131 10.1016/j.csbj.2021.07.013 34527186 PMC8346528 Zulfiqar H et al (2021) Identification of cyclin protein using gradient boost decision tree algorithm. Comput Struct Biotechnol J 19:4123&#8211;4131 34527186 10.1016/j.csbj.2021.07.013 PMC8346528"
}