{
  "pmcid": "PMC12675596",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:22.167718",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12675596",
    "pmid": "41330959",
    "doi": "10.1038/s41598-025-26653-2",
    "title": "Lightweight malicious URL detection using deep learning and large language models",
    "year": "2025",
    "month": "12",
    "day": "2",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "2"
    },
    "authors": [
      "Kibriya Hareem",
      "Amin Rashid",
      "Alshamrani Sultan S.",
      "Rehman Safia",
      "Hassan Mehdi",
      "Alsubaei Faisal S."
    ],
    "abstract": "With thousands of new websites emerging daily, distinguishing between legitimate and malicious web pages has become increasingly challenging, as many of these sites compromise users’ private data without consent, posing severe cybersecurity threats. The absence of robust detection mechanisms exposes users to cyberattacks, financial fraud, and identity theft. While several Machine Learning (ML)-based techniques exist, they suffer from limitations such as reliance on handcrafted features and difficulty in adapting to evolving attack patterns. To mitigate these challenges, this paper introduces a fully automated deep learning (DL) based framework designed for the detection of malicious Uniform Resource Locators (URLs). The framework utilizes Large Language Models (LLMs) to generate high-quality URL embeddings that capture complex patterns and token relationships in URLs without manual feature engineering. These embeddings are then classified into four categories, i.e., defacement, malware, benign, and phishing, using a customized DL-based model that is finalized using extensive ablation experiments. The proposed DL model uses Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers to capture long-range dependencies between the embeddings. The proposed system achieved the highest accuracy of 97.5% using a Bidirectional Encoder Representations from Transformers (BERT) and a DL-based model. With only 0.5 M parameters, the BERT + DL model can classify samples in 0.119 ms. Additionally, to enhance interpretability and trustworthiness, the eXplainable AI (XAI) technique called Local Interpretable Model-Agnostic Explanations (LIME) is used to visualize model decisions to ensure the model’s transparency and reliability in a real-time setting.",
    "keywords": [
      "Energy science and technology",
      "Engineering"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12675596</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12675596.1</article-id><article-id pub-id-type=\"pmcaid\">12675596</article-id><article-id pub-id-type=\"pmcaiid\">12675596</article-id><article-id pub-id-type=\"pmid\">41330959</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-26653-2</article-id><article-id pub-id-type=\"publisher-id\">26653</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Lightweight malicious URL detection using deep learning and large language models</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Kibriya</surname><given-names initials=\"H\">Hareem</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Amin</surname><given-names initials=\"R\">Rashid</given-names></name><address><email>rashid.sdn1@gmail.com</email></address><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Alshamrani</surname><given-names initials=\"SS\">Sultan S.</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Rehman</surname><given-names initials=\"S\">Safia</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Hassan</surname><given-names initials=\"M\">Mehdi</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Alsubaei</surname><given-names initials=\"FS\">Faisal S.</given-names></name><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03yfe9v83</institution-id><institution-id institution-id-type=\"GRID\">grid.444783.8</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0607 2515</institution-id><institution>Department of Computer Science, </institution><institution>Air University, </institution></institution-wrap>Islamabad, Pakistan </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03v00ka07</institution-id><institution-id institution-id-type=\"GRID\">grid.442854.b</institution-id><institution>Department of Computer Science, </institution><institution>University of Engineering and Technology, </institution></institution-wrap>Taxila, Pakistan </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/014g1a453</institution-id><institution-id institution-id-type=\"GRID\">grid.412895.3</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0419 5255</institution-id><institution>Department of Information Technology, College of Computer and Information Technology, </institution><institution>Taif University, </institution></institution-wrap>P.O. Box 11099, Taif, 21944 Saudi Arabia </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/015ya8798</institution-id><institution-id institution-id-type=\"GRID\">grid.460099.2</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 4912 2893</institution-id><institution>Department of Cybersecurity, College of Computer Science and Engineering, </institution><institution>University of Jeddah, </institution></institution-wrap>Jeddah, Saudi Arabia </aff></contrib-group><pub-date pub-type=\"epub\"><day>2</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>43044</elocation-id><history><date date-type=\"received\"><day>10</day><month>5</month><year>2025</year></date><date date-type=\"accepted\"><day>29</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>02</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>05</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 00:25:12.533\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_26653.pdf\"/><abstract id=\"Abs1\"><p id=\"Par8\">With thousands of new websites emerging daily, distinguishing between legitimate and malicious web pages has become increasingly challenging, as many of these sites compromise users&#8217; private data without consent, posing severe cybersecurity threats. The absence of robust detection mechanisms exposes users to cyberattacks, financial fraud, and identity theft. While several Machine Learning (ML)-based techniques exist, they suffer from limitations such as reliance on handcrafted features and difficulty in adapting to evolving attack patterns. To mitigate these challenges, this paper introduces a fully automated deep learning (DL) based framework designed for the detection of malicious Uniform Resource Locators (URLs). The framework utilizes Large Language Models (LLMs) to generate high-quality URL embeddings that capture complex patterns and token relationships in URLs without manual feature engineering. These embeddings are then classified into four categories, i.e., defacement, malware, benign, and phishing, using a customized DL-based model that is finalized using extensive ablation experiments. The proposed DL model uses Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers to capture long-range dependencies between the embeddings. The proposed system achieved the highest accuracy of 97.5% using a Bidirectional Encoder Representations from Transformers (BERT) and a DL-based model. With only 0.5&#160;M parameters, the BERT&#8201;+&#8201;DL model can classify samples in 0.119 ms. Additionally, to enhance interpretability and trustworthiness, the eXplainable AI (XAI) technique called Local Interpretable Model-Agnostic Explanations (LIME) is used to visualize model decisions to ensure the model&#8217;s transparency and reliability in a real-time setting.</p></abstract><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Energy science and technology</kwd><kwd>Engineering</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type=\"FundRef\">https://doi.org/10.13039/501100006261</institution-id><institution>Taif University</institution></institution-wrap></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par9\">The widespread proliferation of the internet has revolutionized access to information, services, communication, and transactions, connecting millions of people worldwide. However, this connectivity has also introduced significant cybersecurity challenges, particularly the widespread proliferation of malicious URLs, making phishing website detection a key focus in cybersecurity. These harmful links are exploited by cybercriminals to engage in different cyberattacks, including phishing, unauthorized data breaches, and malware distribution, thus posing severe threats to individuals and organizations alike<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. According to a 2013 RSA report, approximately 450,000 websites were affected by phishing attacks, leading to estimated financial losses of around USD.</p><p id=\"Par10\">5.9 billion<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref>,<xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>. To mitigate threats, malicious URLs are blacklisted; however, their effectiveness is limited, as new malicious URLs continuously emerge. The attackers use sophisticated deception techniques to make the illegitimate URL look legitimate by using URL spoofing and obfuscation, etc., to evade detection and attack unsuspecting users. The malicious websites are carefully designed in such a way that they closely resemble legitimate web pages, thus making it increasingly difficult for users, especially those with limited cybersecurity awareness, to differentiate between authentic and harmful sites<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>.</p><p id=\"Par11\">To address the growing threat of malicious URLs, numerous ML-based techniques have been proposed by researchers. However, these approaches often face significant limitations due to inherent challenges. By the time a malicious URL is identified and incorporated into a blacklist, it is often too late, as many users may have already been compromised<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>. Moreover, the increasing sophistication and variability of malicious URLs require continuous rule maintenance, which is not only error-prone but also a time-consuming and labor-intensive process. This reliance on static rule-based mechanisms and human expertise makes these systems rigid and less adaptable to evolving URL patterns. Another problem is the general lack of user awareness about the deceptive tactics employed by cyber attackers, which further heightens the risk of successful cyber attacks via malicious URLs<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>.</p><p id=\"Par12\">Despite their advantages, the DL-based methods face several challenges as well. One significant drawback is their high memory and computational resource requirements, which can delay response times. This computational overhead may provide adversaries with a window of opportunity to bypass detection, thus making it impractical for real-time detection<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. The DL-based models often lack interpretability and transparency, as their decision-making processes are not known, thus being called &#8220;black boxes&#8221;. This absence of model interpretability significantly hinders user understanding and trust in the outputs produced by DL models<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref>,<xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. Another issue is the requirement of DL-based systems for the availability of large, balanced, and up-to-date datasets. In practice, such datasets are often scarce. Collection and development of a customised dataset is not only costly but requires expert knowledge as well<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref>,<xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>. The combination of computational complexity, limited interpretability, and data dependency poses significant barriers to the practical deployment and long-term reliability of DL-based solutions in cybersecurity.</p><p id=\"Par13\">Recently, LLMs have emerged as a promising solution to many of the limitations associated with DL-based models in the cybersecurity domain. Due to the training of these models on massive datasets and their ability to understand and generate human-like language, LLMs have demonstrated effectiveness in various security-related tasks<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>. Motivated by these advancements and the challenges in malicious URL detection, this paper proposes a fully automated URL classification framework that integrates the strengths of both LLMs and DL models. The main contributions of this study are as follows:</p><p id=\"Par14\">\n<list list-type=\"bullet\"><list-item><p id=\"Par15\">Utilize LLMs to generate high-quality URL embeddings that capture both lexical and contextual characteristics, while ensuring low resource consumption suitable for real-time detection.</p></list-item><list-item><p id=\"Par16\">Design a lightweight, customized DL framework using LSTM and GRU layers to model long-range dependencies between the embeddings effectively. Optimize the architecture through comprehensive ablation studies and hyperparameter tuning to maximize classification efficiency.</p></list-item><list-item><p id=\"Par17\">Evaluate the model on previously unseen and obfuscated URL samples to rigorously assess its robustness against advanced evasion tactics and stealthy adversarial techniques.</p></list-item><list-item><p id=\"Par18\">Employ XAI techniques to develop user trust and facilitate informed decision-making in security applications.</p></list-item></list>\n</p><p id=\"Par19\">The remaining paper is organized as follows: Sect.&#160;2 presents a critical analysis of recent studies. The proposed methodology is detailed in Sect.&#160;3, followed by the presentation of results in Sect.&#160;4. Finally, Sect.&#160;5 concludes the article.</p><sec id=\"Sec2\"><title>Literature review</title><p id=\"Par20\">In recent years, numerous researchers have proposed various automated techniques for the classification of malicious URLs. For example, Roy et al.<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup> developed PhishLang using MobileBERT for contextual analysis of websites. PhishLang detected 25,796 phishing URLs using Generative Pre-trained Transformer (GPT) 3.5 Turbo. Moreover, the results were visually analyzed using XAI to enhance users&#8217; trust. Additionally, a browser extension was developed to facilitate user interaction. Kaisser et al.<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup> proposed a framework for classifying malicious web links using GPT&#8722;3.5 Turbo and GPT&#8722;4, alongside various ML models. The framework extracted both manual features and GPT-generated features, which were then classified using different models, and attained 95% accuracy. However, the model is trained on only 1,000 random URL samples; hence, the system requires rigorous validation before deployment. Yu et al.<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup> developed an M-BERT-based model to detect malicious websites using a customised dataset and attained 0.94 precision. Tang et al.<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup> proposed a classification approach using BERT, GPT, Ernie, ML/LSTM, and ConvBERT. The system achieved an accuracy of 93.1%, but despite employing lightweight models, its overall performance was suboptimal. Su et al.<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup> introduced a BERT-based model that attained 98% accuracy. The system was trained and validated on the ISCX 2016 dataset, containing nearly 100,000 URLs. However, due to the dataset&#8217;s limited samples, a thorough validation is required before deployment in real-world scenarios. Rashid et al.<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> employed one-shot learning with LLMs for malicious URL classification by using Chain-of-Thought reasoning. Human interpretable interpretations were generated to explain the classification outcomes. The approach was evaluated on three benchmark datasets using five LLMs, namely GPT&#8722;4 Turbo, Claude 3 Opus, Gemini, LLaMA 2 &amp; 3. Among these, GPT&#8722;4 Turbo achieved the highest F1 score of 0.92 in both zero-shot and five-shot classification scenarios. Zhang et al.<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> proposed AdaptPUD, a URL-based phishing detection method using a Token-Property Embedding technique to capture both semantic and structural URL features. The model used a hybrid model combining multi-channel CNNs, Bi-GRU, Self-Attention, and Concept Drift Detection for adaptive, incremental learning. The model obtained over 91% accuracy, and detected phishing URLs in 0.19 ms. However, the model only performs binary classification. Moreover, it has low performance.</p><p id=\"Par21\">Dorta et al.<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup> proposed a fraudulent URL detection system combining traditional ML and Quantum Machine Learning (QML) techniques. The system achieved approximately 90% accuracy on 180,000 URL samples. However, classical ML models outperformed QML due to the immaturity of quantum hardware and the lack of optimized algorithms. Jalil et al.<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> developed an ML-based phishing URL detection framework that relies solely on lexical features extracted from URLs. Using.</p><p id=\"Par22\">TF-IDF and entropy-based features, they achieved a maximum accuracy of 96.8%. However, the reliance on ML techniques alone introduces inherent limitations, therefore necessitating further validation before deployment. Ariwan et al.<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup> proposed a Kernel Principal Component Analysis-Support Vectors Machine-Genetic Algorithm (PCA-SVM-GA)-based model for detecting malicious URLs. The system used Kernel PCA for dimensionality reduction, SVM for classification, and GA for optimization, which attained an accuracy of 93.52%. Despite its effectiveness, the approach is constrained by its dependence on handcrafted feature extraction, which has inherent limitations. Li et al.<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup> used LLMs for malicious website detection. The system utilized zero-shot and few-shot prompting with GPT&#8722;3.5 (175B parameters) and ChatGPT, eliminating the need for large-scale annotated datasets. The system achieved 96% accuracy using GPT&#8722;3.5. However, it is computationally expensive and was trained on a relatively small dataset of approximately 1,000 samples, thus requiring extensive validation before deployment. Singh et al.<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup> used a Bidirectional LSTM (BiLSTM) network with a Convolutional Block Attention Module (CBAM) and Spatial Pyramid Pooling (SPP). The model was evaluated on two benchmark datasets, each comprising two classes: phishing and benign URLs. Similarly, Zaimi et al.<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup> proposed a hybrid DL framework that combines DistilBERT for contextual URL feature extraction with a CNN&#8211;LSTM classifier for malicious URL detection, achieving 98% accuracy. While the models in<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup> and<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup> demonstrate strong performance, the applicability is limited due to the binary classification setting, which restricts the deployment in more complex, real-world environments where more granular classification is often required.</p><p id=\"Par23\">Aljofey et al.<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup> proposed BERT-PhishFinder for phishing URL detection using fine-tuned DistilBERT embeddings. The model was enhanced by incorporating SpatialDropout1D, global pooling, and parallel dense layers to extract features. The model achieved over 99.30% accuracy. However, the model performs binary classification. Hence, needs to be extended to multi-class classification for real-world deployment. Buu et al.<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup> proposed a fuzzy-calibrated transformer network for phishing URL detection, to combine the learning capability of a transformer-based deep learning model with the interpretability of fuzzy logic. The model used Gaussian membership functions and fuzzy rule weighting, and includes a recalibration mechanism that updates fuzzy parameters and retrains the model when performance drops. The model attained 98.9% accuracy. Despite a significant performance, the model is limited to binary classification. Moreover, the use of fuzzy logic introduces challenges in scalability, expert dependency, and computational overhead during rule recalibration.</p><p id=\"Par24\">Existing approaches for URL detection primarily rely on DL techniques. However, these systems face several limitations, including the use of low-quality or limited datasets, suboptimal performance, and high computational complexity. Additionally, many of these models are restricted to binary classification, failing to differentiate between specific types of malicious URLs. A further limitation is the lack of explainability, which hinders user trust and transparency. Therefore, such systems are not well-suited for deployment in URL detection applications.</p></sec><sec id=\"Sec3\"><title>Proposed methodology</title><p id=\"Par25\">An end-to-end framework is proposed for malicious URL detection, which integrates state-of-the-art LLMs with a customized lightweight DL model. The model initially generates the embedding using the encoder/decoder of pre-trained LLMs (GPT-2, Tiny Llama, T5-Large, and BERT). These embeddings are then classified into four URL categories, i.e., Phishing, Malware, Defacement, and Benign, using a customized DL model that is finalized after extensive ablation experiments. The overall architecture of the proposed framework is illustrated in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>. Finally, to ensure transparency and trustworthiness, the results of the proposed model are interpreted using an XAI technique called LIME.</p></sec><sec id=\"Sec4\"><title>Dataset acquisition</title><p id=\"Par26\">The study utilizes a publicly available dataset obtained from Kaggle, comprising URLs categorized into four distinct classes: Phishing, Benign, Malware, and Defacement. Malware URLs are designed to distribute malicious software, such as viruses, ransomware, or spyware, that can compromise user devices and steal sensitive data after download. Defacement URLs typically target websites by altering their appearance and then injecting unauthorized content, usually as an act of cyber vandalism or hacktivism. Phishing URLs mimic legitimate websites to deceive users into providing confidential information, including login credentials, banking details, etc. All these URLs appear legitimate at first glance, but are deceptive and malicious. Finally, the Benign URLs in the dataset are safe and legitimate web addresses that do not pose any security threats, serving regular online content without malicious intent<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref>,<xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup>.</p><p id=\"Par27\">The dataset<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup> consists of 651,191 URLs, out of which 428,103 URLs belong to the benign category, 96,457 to defacement, 94,111 to phishing, and 32,520 to malware. This repository is sourced and combined from multiple datasets, including ISCX-URL&#8722;2016<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>, Phish Tank<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup>, Malware Domains<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>, and Phish Storm<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup>. The makers of this dataset combined the datasets from multiple sources and augmented the benign class samples only, while merging the rest. It is worth mentioning that the dataset has not been preprocessed to retain the elements, i.e., symbols and characters that are essential for effective malicious URL detection. Furthermore, the labels from the original dataset have been carefully verified and corrected to ensure accuracy in the experimental results. Duplicate entries were removed before processing. Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref> presents the class-wise distribution of the.</p><p id=\"Par28\">\n<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Block diagram illustrating the workflow of the proposed URL classification framework.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e424\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig1_HTML.jpg\"/></fig>\n</p><p id=\"Par29\">dataset employed in this study. The dataset is partitioned using a 60:40 train&#8211;test split, wherein 60% of the randomly selected samples are utilized for training, and the remaining 40% are reserved for evaluating the performance of the proposed model.</p><p id=\"Par30\">\n<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Pie chart illustrating dataset distribution in a class-wise manner.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e436\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig2_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec5\"><title>Problem formulation</title><p id=\"Par31\">The rapid proliferation of internet usage has contributed to the widespread emergence of malicious URLs as a significant cybersecurity threat. These URLs are being used as a means of cyberattack, thus endangering the privacy and security of users. The study is conducted using a publicly available dataset. Let the set of URLs be represented as:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e443\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U=\\left\\{u_1,\\:u_2,\\dots,u_n\\right\\},$$\\end{document}</tex-math></disp-formula></p><p id=\"Par32\">Where <italic toggle=\"yes\">u</italic><sub><italic toggle=\"yes\">i</italic></sub> denotes URLs contained in the dataset.</p><p id=\"Par33\">The aim to develop a robust and lightweight framework to to classify each URL <italic toggle=\"yes\">u</italic><sub><italic toggle=\"yes\">i</italic></sub> from a labelled dataset <italic toggle=\"yes\">D</italic> = {(<italic toggle=\"yes\">u</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>1</sub>), (<italic toggle=\"yes\">u</italic><sub>2</sub>, <italic toggle=\"yes\">y</italic><sub>2</sub>), <italic toggle=\"yes\">..</italic>, (<italic toggle=\"yes\">u</italic><sub><italic toggle=\"yes\">n</italic></sub>, <italic toggle=\"yes\">y</italic><sub><italic toggle=\"yes\">n</italic></sub>)} into a specified label <italic toggle=\"yes\">y</italic><sub><italic toggle=\"yes\">i</italic></sub>, using LLMs and a lightweight customised DL model, such that:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e505\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_i=\\begin{array}{l} {\\rm}0\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Benign\\: class}}\\\\ 1\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Defacement\\: Class}}\\\\ 2\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Malware\\: Class}}\\\\ 3\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Phishing\\: Class}} \\end{array}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec6\"><title>URL embedding generation using LLMs</title><p id=\"Par34\">With the recent surge in AI advancements, LLMs have gained significant attention for their remarkable performance in NLP-related tasks. Their ability to learn context and patterns from massive datasets makes them highly valuable across domains. LLMs are advanced DL models built on transformer architectures<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup> that use self-attention mechanisms to process and generate human-like text. These models convert input text into high-dimensional embeddings, which are then processed through multi-layered neural networks using self-attention to capture contextual relationships within text sequences<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>. Initially, the input sequence is tokenized and passed through an embedding layer to transform the discrete tokens into dense vector representations. Each LM utilizes a model-specific subword tokenization strategy to preprocess input URLs. Specifically, BERT employs WordPiece tokenization, GPT&#8722;2 and TinyLLaMA use Byte-Pair Encoding (BPE), while T5 adopts a SentencePiece tokenizer with a unigram language model. Since transformers lack inherent order awareness, positional encoding is added to retain sequential information.</p><p id=\"Par35\">In encoder-based models like BERT and T5 (encoder side), the input embeddings are passed through several self-attention and feed-forward networks to produce rich contextual embeddings that capture bidirectional dependencies. In contrast, decoder-only models such as GPT-2 and TinyLLaMA generate embeddings using masked self-attention, enabling autoregressive processing where each token attends only to previous ones. Although decoder models are primarily designed for generation tasks, the hidden states from their intermediate or final layers can be used as contextual embeddings for classification tasks. In both architectures, the multi-head self-attention mechanism captures long-range dependencies across the input sequence. In contrast, the position-wise feed-forward networks perform non-linear transformations to enhance the features. In the standard transformer model, a decoder is employed in conjunction with the encoder to facilitate autoregressive sequence generation. An overview of the transformer architecture is presented in Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>.</p><p id=\"Par36\">Transformers have since evolved into various architectures, with each adapting the original framework through minimal changes to the encoder/decoder structures. In this study, embeddings are extracted using either the encoder (BERT, T5) or the decoder (GPT, Tiny LLaMA), depending on the model. The use of LLMs for embedding vector generation is motivated by their pretraining on massive, domain-diverse corpora and their ability to learn complex semantic and syntactic patterns through millions of parameters. Their ability to capture sequential dependencies without the need for manual feature engineering makes them well-suited for detecting malicious URLs. BERT, introduced by Google in 2018, adopts an encoder-only transformer architecture composed of multiple stacked layers, each containing a multi-head self-attention mechanism followed by a position-wise feed-forward neural network<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>. The BERT-Base model contains 12 layers (transformer blocks), each with 12 attention heads, and approximately 110 million parameters. Each attention head in the multi-head mechanism allows the model to attend to different parts of the input sequence, enabling a richer understanding of contextual relationships. BERT uses absolute positional encodings, which are added to the input token embeddings to incorporate positional information before the attention layers process the data. The model employs the Gaussian Error Linear Unit (GeLU) as its activation function, which improves gradient flow and contributes to stable training. Layer normalization is applied after each sub-layer (self-attention and feed-forward) to enhance training stability and convergence. Unlike unidirectional models that process text left-to-right or right-to-left, BERT uses a bidirectional training objective through masked language modeling (MLM), where random tokens in the input are masked, and the model is trained to predict them based on both left and right context. This bidirectional context modeling allows BERT to effectively capture deeper semantic and structural patterns, making it well-suited for encoding meaningful representations of URL components.</p><p id=\"Par37\">GPT is a generative LLM developed by OpenAI<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup> that is pre-trained on a vast amount of internet data and contains a total of 117 million parameters. GPT&#8722;2 is a decoder-only Transformer architecture with masked self-attention, meaning each token can only attend to previous tokens. GPT&#8722;2 base has 12 layers, with 12 self-attention heads per layer, and scales up to larger models with more parameters. Unlike BERT, GPT&#8722;2 does not use predefined absolute positional encodings; instead, it employs learned positional embeddings, allowing it to dynamically determine positional relationships rather than relying on fixed encodings. GPT&#8722;2 also uses GELU activation, similar to BERT, and applies layer normalization before each self-attention block. However, unlike BERT, GPT&#8722;2 employs an autoregressive approach, which restricts each token to attending only to preceding tokens, thus preventing bidirectional context modeling.</p><p id=\"Par38\">This study also utilizes TinyLLaMA, a lightweight variant of the LLaMA family, sharing architectural similarities and tokenizer design with LLaMA-2, but significantly smaller in scale, containing approximately 1.1 billion parameters<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>. Like.</p><p id=\"Par39\">\n<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Transformer architecture showing the encoder-decoder structure<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e557\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig3_HTML.jpg\"/></fig></p><p id=\"Par40\">GPT models, TinyLLaMA adopts a decoder-only transformer architecture optimized for autoregressive tasks. It incorporates several architectural improvements, including grouped-query attention (GQA), rotary positional embeddings (RoPE), and the SwiGLU (Swish-Gated Linear Unit) activation function. The model consists of 22 transformer blocks, each featuring 32 attention heads, organized into 4 query groups with eight heads per group to support efficient grouped-query attention. This structure improves memory usage and computational efficiency compared to standard multi-head attention. Unlike models with absolute positional encodings, TinyLLaMA uses rotary positional embeddings, which dynamically encode relative position information and enhance the model&#8217;s ability to generalize across variable-length sequences. Additionally, it employs SwiGLU activation and RMSNorm (Root Mean Square Layer Normalization) as part of a pre-normalization setup, offering improved training stability and faster convergence compared to GELU-based architectures.</p><p id=\"Par41\">Lastly, T5-Large (Text-to-Text Transfer Transformer), introduced by Google in 2019, adopts a full encoder&#8211;decoder transformer architecture designed to frame all NLP tasks in a unified text-to-text format<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>. The T5-Large model comprises 24 encoder layers and 24 decoder layers, each equipped with 16 attention heads, and contains approximately 770 million parameters with an embedding size of 1024. T5 incorporates a learned relative positional bias in place of absolute or rotary positional encodings. This mechanism modulates attention scores based on the relative distances between tokens, which enables the model to generalize more effectively across input sequences of varying lengths. Unlike models such as BERT and GPT&#8722;2, which use pre-layer normalization, T5 applies layer normalization after the self-attention and feed-forward layers (i.e., post-layer normalization). The LLM also uses SwiGLU (Swish-Gated Linear Unit) activation function, which improves gradient flow and computational efficiency compared to ReLU or GELU. While not as lightweight as some alternatives, T5-Large is robust and highly expressive, making it effective in capturing the structural and contextual information within URLs. All these models fall under the umbrella of LLMs, i.e., GPT&#8722;2, T5, and TinyLLaMA are generative language models, whereas BERT is a masked language model.</p><p id=\"Par42\">The URLs are initially tokenized, where the models break URLs into sub-word tokens, enabling a more granular and meaningful representation of the input data. Let a URL sequence be represented as:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e568\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{URL}=\\left\\{t_1,\\:t_2\\dots,t_n\\right\\},$$\\end{document}</tex-math></disp-formula></p><p id=\"Par43\">Where <italic toggle=\"yes\">t</italic> represents the token in the URL. Truncation is applied to limit the tokens to a maximum length (n) of 786 for BERT and GPT-2, 2048 for Tiny Llama, and 1024 for T5 (Large). These tokens are then transformed into dense input embeddings using the LLM encoder&#8217;s embedding layer:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e577\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E=\\left\\{e_1,\\:e_2,\\dots,e_n\\right\\}\\:\\:\\:\\:e_i=W_e[t_i]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par44\">where <italic toggle=\"yes\">W</italic><sub><italic toggle=\"yes\">e</italic></sub> represents the learnable embedding matrix, and <italic toggle=\"yes\">e</italic><sub><italic toggle=\"yes\">i</italic></sub> is the dense vector representation of the <italic toggle=\"yes\">i</italic>-th token (<italic toggle=\"yes\">t</italic><sub><italic toggle=\"yes\">i</italic></sub>). Since Transformers are inherently order-agnostic, the positional encodings (PE) are used to enhance comprehension of token order further:<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e606\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E^\\prime_i=e_i+PE(i),$$\\end{document}</tex-math></disp-formula></p><p id=\"Par45\">where PE(<italic toggle=\"yes\">i</italic>)for the <italic toggle=\"yes\">i</italic>-th position, defined as:<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e618\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{PE}_{(i,2k)}=\\text{sin}\\frac{i}{10000^{2k/d}}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e622\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{PE}_{(i,2k+1)}=\\text{cos}\\frac{i}{10000^{2k/d}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par46\">here <italic toggle=\"yes\">d</italic> is the dimension of the embedding vector, and <italic toggle=\"yes\">k</italic> is the index of the embedding dimension.</p><p id=\"Par47\">The tokenized and encoded data is then fed into the LLM&#8217;s transformer layers. In these layers, the multi-head self-attention mechanism enables these models to focus on relevant portions of the input sequence:<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e636\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{Attention}(Q,K,V)=\\text{softmax}\\frac{QK^T}{\\sqrt[]{\\overline{dk}}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par48\">Where <italic toggle=\"yes\">Q</italic>, <italic toggle=\"yes\">K</italic>, and <italic toggle=\"yes\">V</italic> denote the query, key, and value matrices, respectively, and <italic toggle=\"yes\">d</italic><sub><italic toggle=\"yes\">k</italic></sub> represents the dimensionality of the key vectors<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>. The final context-rich embeddings from these LLMs are then supplied to the customised DL framework for the classification of URLs into different categories.</p><p id=\"Par49\">\n<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Proposed DL based framework for malicious URL classification.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e669\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig4_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec7\"><title>LLM generated embedding classification via customised DL model</title><p id=\"Par50\">The LLM-generated embeddings are then supplied to the proposed DL model illustrated in Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref> for classification.</p><p id=\"Par51\">The first learnable layer of the architecture is a one-dimensional convolutional (Conv-1D) layer consisting of 64 filters, each with a kernel size of 3. This layer performs a convolution operation over the input sequence, which can be mathematically described as:<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e680\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$y_i=\\sum_{k=0}^{K-1}wk\\cdot{x_{i+k}}+b\\:\\:\\:\\:\\text{for}\\:i\\:\\epsilon[0,N-K],$$\\end{document}</tex-math></disp-formula></p><p id=\"Par52\">where <italic toggle=\"yes\">y</italic><sub><italic toggle=\"yes\">i</italic></sub> denotes the output at position <italic toggle=\"yes\">i</italic>, <italic toggle=\"yes\">x</italic> is the input sequence of length <italic toggle=\"yes\">N</italic>, <italic toggle=\"yes\">w</italic><sub><italic toggle=\"yes\">k</italic></sub> represents the <italic toggle=\"yes\">k</italic>-th weight of the convolutional kernel of size <italic toggle=\"yes\">K</italic>, and <italic toggle=\"yes\">b</italic> is the bias term. This operation enables the model to extract local patterns from the sequence data.</p><p id=\"Par53\">A Rectified Linear Unit (ReLU) activation function is applied element-wise to the output to introduce non-linearity into the model. The ReLU function is defined as:<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e719\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{ReLU(z)}=\\text{max(0,z)},$$\\end{document}</tex-math></disp-formula></p><p id=\"Par54\">Where <italic toggle=\"yes\">z</italic> is the input to the activation function. If <italic toggle=\"yes\">z</italic> is negative, the output is set to zero; otherwise, it remains unchanged. This non-linear transformation plays a critical role in accelerating training convergence and alleviating the vanishing gradient problem, thereby improving the model&#8217;s learning capacity.</p><p id=\"Par55\">The output of the convolutional layer is then passed to an LSTM layer containing 32 neurons. The LSTM layer processes the input by maintaining a memory cell that is selectively updated using its different gating mechanisms; these gates include the input gate, forget gate, and output gate. The input gate (<italic toggle=\"yes\">i</italic><sub><italic toggle=\"yes\">t</italic></sub>) determines the new infomration (<italic toggle=\"yes\">C</italic>&#732;<italic toggle=\"yes\">t</italic>) should be incorporated into the memory cell (<italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">t</italic></sub>). The forget gate (<italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">t</italic></sub>) determines what fraction of the previous memory content (<italic toggle=\"yes\">C</italic><sub><italic toggle=\"yes\">t</italic>&#8722;1</sub>) has to be kept or discarded. The output gate (<italic toggle=\"yes\">o</italic><sub><italic toggle=\"yes\">t</italic></sub>) controls the portion of the updated memory cell to be exposed as the hidden state output (<italic toggle=\"yes\">h</italic><sub><italic toggle=\"yes\">t</italic></sub>). The candidate memory cell (<italic toggle=\"yes\">C</italic>&#732;<italic toggle=\"yes\">t</italic>) is computed via a non-linear transformation, typically a tanh activation applied to a weighted sum of the current input and the previous hidden state. The memory cell is then updated as:<disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e784\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_t=f_t\\star{C_{t-1}}+i_t\\star{\\widetilde{C}}_t,$$\\end{document}</tex-math></disp-formula></p><p id=\"Par56\">This gating mechanism enables the LSTM to retain essential long-term dependencies while discarding irrelevant information, thus maintaining contextual information over extended time steps<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup>.</p><p id=\"Par57\">Next, a GRU layer with 64 neurons is employed. Compared to the LSTM architecture, GRU is much simpler as it combines the forget and input gates into a single update gate. The simpler design reduces computational complexity while maintaining the ability to model long-range dependencies<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup>. Following the initial GRU layer, the proposed framework integrates an additional.</p><p id=\"Par58\">LSTM layer with 128 neurons and a subsequent GRU layer with 256 neurons to further capture temporal dependencies within the data. The output from these layers is then passed through a fully connected (Dense) layer comprising 128 neurons. Layer normalization is applied to this output to stabilize and normalize the data, which is then passed through an additional FC layer comprising 64 neurons. A dropout layer is also used with a drop rate of 0.3 to eliminate 30% of the neurons during training randomly. The final classification layer (dense) contains four neurons, each representing a class, i.e., Malware, Defacement, Phishing, or Benign. Detailed configuration of the proposed IDS is depicted in Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref>.</p><p id=\"Par59\">\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Layer-wise specifications of customised DL model with embeddings from TinyLLaMA, GPT-2, T5, and BERT.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Layer</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Configuration</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Input</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Input Shape: D (D&#8201;=&#8201;768/1024/2048)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv-1D</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Filters: 64, Kernel Size: 3, ReLU, Padding: Same</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LSTM 1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 32</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU 1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 64</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LSTM 2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 128, Dropout: 0.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU 2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 256, Recurrent Dropout: 0.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense 1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 128, Activation: ReLU</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LayerNormalization</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Applied after Dense</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense 2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 64, Activation: ReLU</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout Rate: 0.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense (Softmax)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Units: 4, Activation: Softmax</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec8\"><title>Result visualization using XAI</title><p id=\"Par60\">Finally, the predictions are analyzed using XAI techniques. Despite the robust performance of DL models across various domains, the challenge of a lack of transparency and non-explainability of the results and the model&#8217;s inner workings remains. The rationale behind specific decisions, the key features or regions influencing outcomes, and the level of confidence in predictions are still missing. Hence, to make these models more explainable, transparent, and trustworthy, a field of XAI has emerged, aiming to transform these &#8220;black box&#8221; models into interpretable and understandable systems. This study uses LIME, which is an XAI technique designed to provide local interpretability by explaining the model&#8217;s behavior for a specific instance. It achieves this by approximating the complex model with a simpler, interpretable surrogate that closely replicates the original model&#8217;s predictions. This technique provides a single plot of explanations with words that contributed either positively or negatively towards the classification<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup>.</p></sec><sec id=\"Sec9\"><title>The proposed framework results</title><p id=\"Par61\">This section provides an elaborate explanation of the results obtained from the proposed LLM&#8201;+&#8201;DL classification framework.</p></sec><sec id=\"Sec10\"><title>Performance metrics</title><p id=\"Par62\">The proposed model is assessed using various state-of-the-art metrics such as Accuracy, Precision, Recall, and F1-Score. Accuracy is a well-known metric that shows the overall correctness of the model by computing the frequency of correct predictions made by the model. Accuracy is calculated in Eq.&#160;12. Here, TP, TN, FP, and FN denote True Positive, True Negative, False Positive, and False Negative, respectively.<disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e894\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{Accuracy}=\\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par63\">In scenarios involving imbalanced data, accuracy alone may not serve as a reliable metric to evaluate the model, as a model can attain high accuracy by predominantly predicting the majority class. Therefore, the proposed framework is also assessed using additional metrics, including precision, recall, and F1-score. Precision calculates the ratio of correctly predicted positive instances to the total predicted positives. Higher precision indicates greater reliability in the model&#8217;s positive predictions. Mathematically, the equation can be calculated as in Eq.&#160;13.<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e900\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{Precision}=\\frac{\\text{TP}}{\\text{FP+TP}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par64\">Recall is a popular metric that measures the proportion of actual positive cases correctly identified by the model. It can be calculated as:<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e906\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{Recall}=\\frac{\\text{TP}}{\\text{FN+TP}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par65\">Finally, the model is also evaluated using the F1-score, which computes a harmonic mean of precision and recall that balances the trade-off between these two metrics. This metric is beneficial in an imbalanced class scenario. F1-score is calculated in Eq.&#160;15.<disp-formula id=\"Equ15\"><label>15</label><tex-math id=\"d33e912\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_i-\\text{Score}=2\\times\\frac{\\text{Recall}\\times\\text{Precision}}{\\text{Recall}+\\text{Precision}}$$\\end{document}</tex-math></disp-formula></p></sec></sec><sec id=\"Sec11\"><title>Results</title><p id=\"Par66\">This section presents the results achieved by the proposed model, with classification labels defined as follows: Class 0 represents Benign, Class 1 denotes Defacement, Class 2 corresponds to Malware, and Class 3 indicates Phishing. The proposed DL model is trained for a maximum of 30 epochs with a batch size of 64. To prevent overfitting, early stopping is applied with a patience of 5 epochs, monitoring the validation loss throughout training. The proposed framework achieved 97.5% accuracy with both BERT- and T5-based embeddings. The confusion matrix corresponding to the BERT-based deep learning model is shown in Fig.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>. The diagonal entries in the confusion matrix represent correct predictions for each class: 169,274 for class 0, 37,772 for class 1, 12,286 for class 2, and 34,539 for class 3. The off-diagonal elements represent misclassifications, such as 2,206 instances of class 0 incorrectly classified as class 3, and 2,344 cases of class 3 misclassified as class 0, thereby reflecting the overall robustness of the model.</p><p id=\"Par67\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Confusion matrix of the BERT-based DL model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e931\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig5_HTML.jpg\"/></fig>\n</p><p id=\"Par68\">The classification report obtained from the BERT&#8201;+&#8201;DL framework is depicted in Table&#160;<xref rid=\"Tab2\" ref-type=\"table\">2</xref>. The proposed models attained an average precision, recall, and F1-score of 0.97, 0.97, and 0.96, respectively.</p><p id=\"Par69\">\n<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Detailed classification report showing precision, recall, and F1-score for the BERT-based.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td></tr></tbody></table></table-wrap>\n</p><sec id=\"Sec12\"><title>Deep learning model</title><p id=\"Par70\">BERT&#8217;s Precision Recall (PR) curve is illustrated in Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>a. The graph shows that the proposed framework attained very high AP scores, ranging from 0.97 to 1.0. Figure&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>b shows the Receiver Operating Curve (ROC) obtained from the BERT&#8201;+&#8201;DL model. The graph shows a perfect Area Under Curve (AUC) of 1.0 from classes 0,1, and 2. Class 2 obtained an AUC of 0.99. The curves show a robust performance of the proposed model over the test set. The oscillations in the class are due to a class imbalance issue. Despite such class imbalance, the model performed exceptionally well in terms of all the metrics.</p><p id=\"Par71\">\n<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Classification performance of the BERT-based deep learning model for malicious URL detection. (<bold>a</bold>) PR Curve illustrating the trade-off between precision and recall (<bold>b</bold>) ROC depicting the model&#8217;s capability to distinguish between classes based on true positive and false positive rates. (<bold>a</bold>) PR Curve, (<bold>b</bold>) ROC Curve.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1040\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig6_HTML.jpg\"/></fig>\n</p><p id=\"Par72\">The learning curves indicating (a) Loss and (b) Accuracy are depicted in Fig.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref>. The graph shows a sudden rise in the curve, indicating that the model started learning from the data. Afterward, the learning stabilized and eventually stopped after the loss score stopped decreasing (due to early stopping).</p><p id=\"Par73\">\n<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Learning curve showing model accuracy over epochs for the BERT&#8201;+&#8201;DL framework.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1055\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig7_HTML.jpg\"/></fig>\n</p><p id=\"Par74\">This study also used the GPT-2 model for extracting the embeddings, which are then classified using a customised DL model. The proposed framework achieved an accuracy of 97% using the GPT-2&#8201;+&#8201;DL model. A detailed analysis of each class&#8217;s performance is presented in the classification report shown in Table&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref>. The proposed GPT 2-DL Model achieved an average Precision of 0.94, a Recall of 0.96, and an F1-score of 0.94, demonstrating its strong classification capability and robustness in accurately categorizing URLs into their respective categories.</p><p id=\"Par75\">\n<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Performance summary of the GPT-2&#8201;+&#8201;Deep learning model based on classification Metrics.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par76\">To showcase the performance of the proposed GPT 2&#8201;+&#8201;DL framework, the ROC is computed and illustrated (as shown in Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>a). An AUC of 1.0 for Classes 0, 1, and 2 indicates perfect classification, while Class 3 has an AUC of 0.99. The curves are close to the upper left corner, high above the random guess line, which highlights that the model achieves high TPR with low FPR for all classes, showing the model&#8217;s robustness despite class imbalance issues.</p><p id=\"Par77\">Figure <xref rid=\"Fig8\" ref-type=\"fig\">8</xref>b illustrates the PR curve generated by the proposed method. The PR curve calculates the trade-off between precision and recall for the multi-class classification task. Each curve corresponds to an individual class, with the AP score representing the area under the respective curve. In this graph, Class 0 and Class 1 achieve perfect precision and recall with AP scores of 1.0. Class 2 and Class 3 show a slightly lower but still high performance, with AP scores of 0.98 and 0.96, respectively. The curves are close to the top right corner, which indicates that the classifier maintains high precision even as recall increases. This suggests the model performs well across all classes, with minimal false positives or negatives. The gradual decline in precision for Classes 2 and 3 at high recall reflects a minor trade-off, which is typical when more true positives are obtained.</p><p id=\"Par78\">\n<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Performance assessment of the GPT-2-based deep learning model. (<bold>a</bold>) shows the ROC curve, highlighting the model&#8217;s capability to distinguish between malicious and benign URLs. (<bold>b</bold>) illustrates the class-wise Precision&#8211;Recall curves, indicating the balance between precision and recall across different classes. (<bold>a</bold>) ROC Curve, (<bold>b</bold>) PR Curves.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1153\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig8_HTML.jpg\"/></fig>\n</p><p id=\"Par79\">The Tiny Llama&#8201;+&#8201;DL model achieved a final accuracy of 96.5%. A detailed class-wise breakdown of precision, recall, and F1-scores is provided in Table&#160;<xref rid=\"Tab4\" ref-type=\"table\">4</xref>. On average, the model attained a precision of 0.95, a recall of 0.93, and an F1-score of 0.94.</p><p id=\"Par80\">\n<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Performance summary of the tiny LLaMA&#8201;+&#8201;DL model based on classification.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.93</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.89</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec13\"><title>Metrics</title><p id=\"Par81\">The ROC curve is depicted in Fig.&#160;<xref rid=\"Fig9\" ref-type=\"fig\">9</xref>a. The near-perfect AUC values (1.00 for Classes 0, 1, and 2, and 0.99 for Class 3) indicate that the model accurately distinguishes between classes with minimal misclassification. The curves are tightly clustered near the top-left corner, showing a high actual positive rate while keeping false positives low, demonstrating an outstanding predictive performance across all classes. The PR curve is depicted in Fig.&#160;<xref rid=\"Fig9\" ref-type=\"fig\">9</xref>b, showing accuracy and robustness in the identification and classification of different types of URLs.</p><p id=\"Par82\">The classification report for the T5-Large&#8201;+&#8201;DL model is presented in Table&#160;<xref rid=\"Tab5\" ref-type=\"table\">5</xref>. The model achieved average scores of 0.97 for accuracy, 0.97 for precision, 0.95 for recall, and 0.96 for F1-score.</p><p id=\"Par83\">The PR curve showing a trade-off between Precision and Recall is shown in Fig.&#160;<xref rid=\"Fig10\" ref-type=\"fig\">10</xref>a. The graph shows perfect AP scores of 1.0 for Class 0 and Class (1) Whereas Class 2 and Class 3 obtained AP scores of 0.98 and 0.97, respectively. ROC curve obtained from the T5&#8201;+&#8201;DL model depicted in Fig.&#160;<xref rid=\"Fig10\" ref-type=\"fig\">10</xref>b shows perfect AUC of the model for Classes 0,1, and (2) Whereas, Class 3 obtained an AUC score of 0.99.</p><p id=\"Par84\">\n<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>valuation of the Tiny LLaMA-based DL model for malicious URL classification. (<bold>a</bold>) ROC curve illustrating the trade-off between true positive and false positive rates. (<bold>b</bold>) PR curve showing the model&#8217;s performance across varying decision thresholds. (<bold>a</bold>) ROC Curve, (<bold>b</bold>) PR Curve.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1276\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig9_HTML.jpg\"/></fig>\n</p><p id=\"Par85\">\n<fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>Performance evaluation of the T5-based deep learning model. (<bold>a</bold>) shows the Precision&#8211;Recall curve indicating classification performance across different thresholds. (<bold>b</bold>) presents class-wise ROC curves illustrating the trade-off between true positive and false positive rates. (<bold>a</bold>) Precision&#8211;Recall Curve, (<bold>b</bold>) Class-wise ROC Curves.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1299\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig10_HTML.jpg\"/></fig>\n</p><p id=\"Par86\">\n<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Classification report of the T5-large&#8201;+&#8201;DL model for malicious URL classification.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.97</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.94</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td></tr></tbody></table></table-wrap>\n</p></sec></sec><sec id=\"Sec14\"><title>Discussion</title><p id=\"Par87\">This section analyzes the model&#8217;s performance in terms of computational complexity, time, and accuracy. Finally, the predictions obtained from the proposed model are visually analyzed using the XAI technique.</p><sec id=\"Sec15\"><title>Key observations</title><p id=\"Par88\">Table&#160;<xref rid=\"Tab6\" ref-type=\"table\">6</xref> provides a comparative analysis of LLMs used in this study for generating URL embeddings. The table compares the LLMs in terms of their performance, compactness, and speed. The embeddings generated from these LLMs were finally classified using a lightweight DL model, which was finalized using extensive ablation experiments. The comparison depicts that the BERT&#8201;+&#8201;DL model achieved 97.5% accuracy with only 0.5&#160;M parameters. Whereas GPT-2, T5-Large, and Tiny Llama obtained accuracy of 97%, 97.5%, and 96.5%, respectively. The inference times (in seconds) per sample reported in the table show the average testing time of the models on 260,438 samples. Even though T5 and BERT attained almost the same performance, the BERT-based model is lightweight and quicker compared to T5. BERT&#8201;+&#8201;DL model classified the samples in a minimum time of 0.11 ms/sample, computed on a set of 260,438 test samples.</p><p id=\"Par89\">BERT&#8217;s exceptional performance can be attributed to its bidirectional attention mechanism, which enhances its ability to capture contextual dependencies effectively. It is worth mentioning that these LLMs were only used for embedding generation rather than direct training; hence, their parameters were not included in the overall model parameter count. The reduced parameter count of the proposed BERT&#8201;+&#8201;DL model makes it well-suited for deployment in environments with limited computational resources. Furthermore, in addition to being lightweight, the model also demonstrates robustness in quick detection and URL classification, making it well-suited for real-time applications in URL identification and categorization.</p><p id=\"Par90\">\n<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Evaluation of model efficiency and performance via accuracy, parameter count, and inference time.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">No. of parameters</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Testing time (s)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tiny Llama&#8201;+&#8201;DL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.5%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.4&#160;M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.13 ms/sample</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BERT (Base)&#8201;+&#8201;DL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.5%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5&#160;M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.11 ms/sample</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">T5 (Large)&#8201;+&#8201;DL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.5%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.6&#160;M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.15 ms/sample</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GPT-2&#8201;+&#8201;DL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.0%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5&#160;M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.11 ms/sample</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par91\">To finalize the DL model, a series of ablation experiments were conducted by systematically adding or removing layers to evaluate their impact, as illustrated in Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>. The number of neurons in each layer was empirically assessed, revealing that the proposed method achieved optimal performance with a configuration of 32, 64, and 128 neurons. In the first experiment, the complete architecture was employed, incorporating Conv-1D, Dense (FC), Layer Normalization, Dropout, Soft-max layers, and two sets of LSTM and GRU. The architecture achieved an accuracy of 97.5%. In the second experiment, a simplified architecture with only one set of LSTM and GRU layers attained an accuracy of 97.3%. Further, in the third experiment, all GRU layers were removed, retaining only LSTM layers, which resulted in an accuracy of 97.6%Similarly, in the fourth experiment, only LSTM layers were retained while removing GRU layers, leading to an accuracy of 97.5%. Although the models in Experiments 3 and 4 slightly outperformed the original architecture, they lacked the hybrid combination of LSTM and GRU layers, which is crucial for comprehensive feature extraction and understanding. The proposed DL model maintains a balanced architecture, neither excessively deep nor too shallow, and ensures effective feature learning while optimizing computational efficiency for real-time deployment.</p><p id=\"Par92\">To improve the interpretability of model predictions, the proposed framework integrates an XAI technique known as LIME. Given that DL models are frequently perceived as &#8220;black boxes&#8221; due to their opaque internal mechanisms, XAI methods such as LIME are employed to provide visual and local explanations of the model&#8217;s decision-making process, thereby enhancing transparency and facilitating the assessment of model reliability and trustworthiness. In this study, four randomly selected URL samples from the test dataset were evaluated using LIME to demonstrate the robustness and effectiveness of the proposed model. Furthermore, the fidelity scores associated with each LIME explanation are reported, reflecting the degree to which the explanation aligns with the model&#8217;s original prediction.</p><p id=\"Par93\">\n<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Ablation study evaluating the contribution of key components in the proposed model.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Layer configuration</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy %</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv1D LSTM (32)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.5%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LSTM (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU (256)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Layer Normalization Dense (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout Dense (4) Softmax</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv1D LSTM (32)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.3%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Layer Normalization Dense (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout Dense (4) Softmax</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv1D LSTM (32)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.6%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LSTM (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Layer Normalization Dense (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout Dense (4) Softmax</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv1D GRU (64)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.5%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GRU (256)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense (128)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Layer Normalization Dense (64)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dropout Dense (4) Softmax</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr></tbody></table></table-wrap>\n</p><p id=\"Par94\">Figure <xref rid=\"Fig11\" ref-type=\"fig\">11</xref>a showcases a benign URL classification with 100% confidence. Words highlighted in blue contributed positively towards benign class classification. The model successfully identifies non-malicious URLs by recognizing common lexical patterns found in legitimate domains, demonstrating its robustness in distinguishing benign URLs from malicious ones. The fidelity score of 0.98 indicates that the LIME explanation closely approximates the original model&#8217;s prediction, reflecting high local faithfulness of the surrogate explanation model. Figure&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>b visualizes a URL classified as &#8220;phishing&#8221; with a prediction probability of 0.98. Figure&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>c represents a URL identified as &#8220;malware&#8221; with a prediction probability of 1.0 and a fidelity score of 0.73. The highlighted words (in green) contributed positively toward the malware classification. Despite lower training samples, the model accurately detects malware samples. Figure&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>d represents a URL classified as &#8220;defacement&#8221; with a prediction probability of 1.0 and a fidelity score of 0.82. The explanation highlights features (in orange) that contributed positively to the classification. At the same time, the fidelity score of 0.82 suggests a reasonably strong alignment between the LIME explanation and the model&#8217;s true decision-making behavior.</p><p id=\"Par95\">It is worth mentioning that specific standard tokens, such as &#8220;com&#8221; and &#8220;http&#8221;, &#8220;www&#8221;, appear in all four URL categories, yet the model effectively differentiates between these URLs. This suggests that the classification is not solely based on individual tokens but rather on the contextual relationships among these tokens. The LLM-based embeddings encode the contextual and structural relationships between tokens that allow the model to distinguish URLs based on their overall meaning rather than individual words. Moreover, the use of LSTMs and GRUs in the DL model enables it to capture long-term sequential dependencies and patterns within these tokens. Unlike traditional rule-based approaches, which might flag URLs containing specific keywords, the model understands the overall composition and meaning of a URL by processing embeddings that encode the structural relationship of the URL. Hence, this combination of LLM embeddings and sequential processing enables the model to generalize well and accurately classify these URLs while minimizing reliance on individual token presence. The results highlight the model&#8217;s capability to learn meaningful representations from the training data, making it robust and adaptable to real-world URL classification tasks.</p><p id=\"Par96\">A comparative performance analysis with existing systems is presented in Table&#160;<xref rid=\"Tab8\" ref-type=\"table\">8</xref>. Several LLM-based techniques have been developed to detect malicious URLs. For example, Kaisser et al.<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup> used GPT&#8722;3.5 Turbo and GPT&#8722;4 for phishing URL detection, achieving 95.0% accuracy; however, the model was trained on a limited dataset comprising only 1,000 URL samples, which restricts its generalizability. Yu et al.<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup> used an M-BERT-based model that attained an accuracy of 94.5% on a custom dataset containing 0.6&#160;M URL samples. Despite a larger training dataset, the performance of the model remains relatively lower. Roy et al.<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup> used different LLMs, including MobileBERT, GPT&#8722;2, DistilBERT, Tiny LLaMA, and Bloom, with MobileBERT achieving the highest accuracy of 96.0% for phishing URL detection.</p><p id=\"Par97\">\n<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>LIME-based interpretable visualizations of BERT model predictions across different URL classes: (<bold>a</bold>) Benign, (<bold>b</bold>) Phishing, (<bold>c</bold>) Malware, and (<bold>d</bold>) Defacement. The highlighted tokens represent the most influential features guiding classification decisions. (<bold>a</bold>) Benign Class, (<bold>b</bold>) Phishing Class, (<bold>c</bold>) Malware Class, (<bold>d</bold>) Defacement Class.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1630\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig11_HTML.jpg\"/></fig>\n</p><p id=\"Par98\">On a similar dataset compared to ours, Zaimi et al.<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup> proposed a malicious URL detection framework that combines DistilBERT for feature extraction with a CNN-based classifier. The authors utilized a merged dataset<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup>, reformulated the task as binary classification, and obtained 98% accuracy. Similarly, Al Saedi et al.<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref></sup> employed URL-based, Whois-based, and cyber threat intelligence (CTI) features. Using n-gram and TF&#8211;IDF for feature representation and mutual information for feature selection, the model employed a two-stage ensemble: RF followed by a Multilayer Perceptron (MLP) meta-classifier. The model attained 96.8% overall accuracy. Shetty et al.<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup> proposed a lexical analysis-based approach to detect URLs across four categories, i.e., phishing, malware, benign, and malignant, that obtained 97% accuracy using RF. However, the study is limited to lexical features, which may not capture deeper semantic patterns present in complex URL structures.</p><p id=\"Par99\">In contrast, the present study employs LLMs to generate rich semantic embeddings, which are subsequently classified using a customized DL framework in a multi-label classification setting to identify URLs across four categories: phishing, malware, defacement, and benign. The proposed framework achieved an accuracy of 98% on binary classification and 97.5% on the multi-class classification task, thus demonstrating a superior performance compared to existing approaches.</p><p id=\"Par100\">\n<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Comparison of proposed URL classification framework with the existing state-of-the-art systems.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">References</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Technique</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Classes</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Roy et al.<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">MobileBERT</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Phishing, Benign</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.0%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Kaisser et al.<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">GPT-3.5-turbo and GPT-4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Malicious, Benign</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.0%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Yu et al.<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">M-BERT</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Benign, Malicious</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.5%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Zaimi et al.<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DistilBERT&#8201;+&#8201;CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Benign, Malicious</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Al Saedi et al.<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">TF&#8211;IDF, RF, MLP</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Benign, Malicious</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.80%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Shetty et al.<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lexical Analysis, RF</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>Benign, Malware, Defacement,</p><p>Phishing</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97%</td></tr><tr><td align=\"left\" rowspan=\"2\" colspan=\"1\">\n<bold>Proposed</bold>\n</td><td align=\"left\" rowspan=\"2\" colspan=\"1\">\n<bold>BERT&#8201;+&#8201;DL</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p><bold>Benign</bold>,<bold> Malware</bold>,<bold> Defacement</bold>,</p><p>\n<bold>Phishing</bold>\n</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>97.5%</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>Benign</bold>,<bold> Malicious</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>98%</bold>\n</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par101\">The proposed framework is also evaluated on a set of previously unseen URL samples acquired from online sources to assess its generalization capability. Table&#160;<xref rid=\"Tab9\" ref-type=\"table\">9</xref> summarizes the evaluation results, including the URL (obfuscated for the safety of readers), a brief description of its context, the actual label, and the model&#8217;s predicted label. The proposed BERT&#8201;+&#8201;DL model accurately identified three out of four samples, with misclassification observed between malware and phishing due to a relatively small number of malware samples in the training distribution compared to other classes. While the evaluation is limited to a test set of one URL sample from each of the four classes, the results demonstrate the framework&#8217;s strong potential to generalize effectively to novel, real-world data with a very high confidence score.</p><p id=\"Par102\">\n<table-wrap id=\"Tab9\" position=\"float\" orientation=\"portrait\"><label>Table 9</label><caption><p>Evaluation of the proposed framework&#8217;s generalization capability on unseen URL samples.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">URL</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Description</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Actual<break/>Label</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Predicted<break/>Label</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Confidence<break/>Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">https://www.nationalgeographic.com/travel/topic/best-of-the-world-hub</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Benign URL<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"> http://thebestofminneapolis.org</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>This site was defaced in 2020 by Iranian hackers</p><p>who posted images and messages in protest of the assassination of General Qasem Soleimani<sup><xref ref-type=\"bibr\" rid=\"CR46\">46</xref></sup>.</p></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">http://init[dot]icloud-diagnostics.com</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>This domain was used as a command and control</p><p>server for the XcodeGhost malware<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup>.</p></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">https://firebasestorage.googleapis.com/v0/b/owambe-4ce77.appspot.com/o/arsenaldozens/indexcopy2.html?alt=media&amp;token=bbb56e5d-96d2-4da7-a82f-e0bfed8d24c3&amp;email=creader@palaceresorts.com</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>This URL was part of a phishing campaign targeting</p><p>employees in the travel industry<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup>.</p></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.99</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par103\">The proposed framework is also tested against a set of adversarially obfuscated URLs randomly crafted using common evasion techniques from the URLs above. In example (1), leetspeak (replacing &#8217;o&#8217; with &#8217;0&#8217;) and bracketed symbols were used to bypass traditional filters, yet the model correctly classified the URL. Example (2) combined redirection in the query string, domain spoofing, and leetspeak to imitate a legitimate source, and was also accurately identified. Example (3) presented a more complex case with full leetspeak, excessive URL padding, and a phishing structure embedded within a long, trusted-looking subpath; the model successfully predicted this as well. However, in example (4), which used homoglyph characters (such as Cyrillic &#8217;i&#8217; and &#8217;o&#8217;), deceptive subdomain chaining, and a fake top-level domain (TLD), the model misclassified the input. These results, shown in Fig.&#160;<xref rid=\"Fig12\" ref-type=\"fig\">12</xref>, highlight the performance of the proposed model against diverse obfuscation methods.</p><p id=\"Par104\">\n<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p>Evaluation of the proposed model on obfuscated URLs using adversarial techniques.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1919\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26653_Fig12_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec16\"><title>Strengths of the proposed approach</title><p id=\"Par105\">The proposed framework uses LLM-based embeddings, which are classified using a customised DL model. Unlike conventional approaches, this study utilizes embeddings generated by pre-trained LLMs, which offer significant advantages as they are extensively trained on vast datasets. Hence, these models can effectively capture structural patterns, token relationships, and contextual cues within URLs, resulting in rich feature representations that enhance classification performance. Moreover, one of the main advantages of this approach is that LLMs do not require retraining, which makes them highly efficient for task-specific applications. This significantly reduces training costs and computational overhead while improving model robustness. As demonstrated in the results, the BERT&#8201;+&#8201;DL model emerged as the most effective model for embedding generation and classification, achieving the highest accuracy of 97.5%. The model produced strong performance, but is also relatively lightweight, with a total of 0.5&#160;M parameters. Hence, it is suitable for deployment in resource-constrained environments. To further enhance transparency and interpretability, the proposed framework integrates an XAI module using LIME that visually analyzes the model predictions to explain the model&#8217;s decision-making process. Since DL models often function as black boxes, users may struggle to understand the rationale behind their predictions. By incorporating XAI, the framework strengthens transparency, making it more reliable and applicable for real-world URL classification tasks. Furthermore, the proposed framework performed exceptionally well on unseen real-time URLs, thus proving its robustness and efficacy. The proposed approach can be extended and integrated into web browsers for real-time URL filtering to block malicious links before they reach the end-users.</p><sec id=\"Sec17\"><title>Limitations of the proposed approach</title><p id=\"Par106\">Although the proposed framework utilizes pre-trained LLMs, the embedding generation process still requires a GPU. Therefore, very large LMs with billions of parameters were not explored. Furthermore, due to the lack of a comprehensive dataset, the study relies on a merged dataset composed of multiple existing datasets, with each dataset containing a limited number of URL samples. Hence, the database acquired from different sources was merged to ensure a sufficient number of samples for training and evaluation.</p></sec></sec></sec><sec id=\"Sec18\"><title>Conclusion</title><p id=\"Par107\">As cyber threats evolve, traditional URL detection mechanisms struggle due to their reliance on handcrafted features and inability to adapt to emerging attack patterns. To address these issues, this paper uses well-known LLMs to generate high-quality URL embeddings to capture the context. These URLs are then classified using a customised DL model, which was optimized.</p><p id=\"Par108\">through extensive ablation experiments. The proposed framework is trained and evaluated on well-known datasets and achieves 97.5% accuracy using the BERT&#8201;+&#8201;DL model. Moreover, the model is lightweight, contains only 0.5&#160;M parameters, and can perform classification within 0.11 ms/sample. Finally, the predictions made by the model are visually interpreted using LIME, a well-known XAI technique. This helps in evaluating the model&#8217;s transparency, trustworthiness, and interpretability in its decision-making process. A comparison of the proposed method with existing systems depicts that the proposed model is not only lightweight but is also accurate in the classification of URLs, thus it is deployable in a real-time scenario. In the future, LLMs with billions of parameters can be explored, particularly by fine-tuning them for this specific task. Moreover, this research can be expanded to manually collected datasets to facilitate more extensive and robust experimentation.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>All authors contributed equally.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>This research was funded by Taif University, Saudi Arabia, Project No. TU-DSPP-2024-52. This work was funded by the University of Jeddah, Jeddah, Saudi Arabia, under grant No. (UJ-21-ICI-2). Therefore, the authors thank the University of Jeddah for its technical and financial support.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The code is available here: dx.doi.org/10.6084/m9.figshare.29937974.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par109\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>V</given-names></name></person-group><article-title>Service function chain orchestration across multiple domains: A full mesh aggregation approach</article-title><source>IEEE Trans. Netw. Serv. Manag</source><year>2018</year><volume>15</volume><fpage>1175</fpage><lpage>1191</lpage><pub-id pub-id-type=\"doi\">10.1109/TNSM.2018.2861717</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Sun, G., Li, Y., Liao, D. &amp; Chang, V. Service function chain orchestration across multiple domains: A full mesh aggregation approach. <italic toggle=\"yes\">IEEE Trans. Netw. Serv. Manag</italic>. <bold>15</bold>, 1175&#8211;1191 (2018).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zou</surname><given-names>X</given-names></name><etal/></person-group><article-title>From hyper-dimensional structures to linear structures: maintaining deduplicated data&#8217;s locality</article-title><source>ACM Trans. Storage (TOS)</source><year>2022</year><volume>18</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type=\"doi\">10.1145/3507921</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Zou, X. et al. From hyper-dimensional structures to linear structures: maintaining deduplicated data&#8217;s locality. <italic toggle=\"yes\">ACM Trans. Storage (TOS)</italic>. <bold>18</bold>, 1&#8211;28 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>Z</given-names></name></person-group><article-title>Resilient formation tracking for networked swarm systems under malicious data deception attacks</article-title><source>Int. J. Robust. Nonlinear Control</source><year>2025</year><volume>35</volume><fpage>2043</fpage><lpage>2052</lpage><pub-id pub-id-type=\"doi\">10.1002/rnc.7777</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Liu, Y., Li, W., Dong, X. &amp; Ren, Z. Resilient formation tracking for networked swarm systems under malicious data deception attacks. <italic toggle=\"yes\">Int. J. Robust. Nonlinear Control</italic>. <bold>35</bold>, 2043&#8211;2052 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Magazine, C. Cybercrime To Cost The World $10.5 Trillion Annually By 2025. Available at: https://cybersecurityventures.com/hackerpocalypse-cybercrime-report-2016/ . (2025).</mixed-citation></ref><ref id=\"CR5\"><label>5.</label><mixed-citation publication-type=\"other\">Zenggang, X. et al. Ndlsc: A new deep learning-based approach to smart contract vulnerability detection. <italic toggle=\"yes\">J Signal. Process. Syst.</italic><bold>97</bold>, 1&#8211;20 (2025).</mixed-citation></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xia</surname><given-names>W</given-names></name><etal/></person-group><article-title>The design of fast and lightweight resemblance detection for efficient post-deduplication delta compression</article-title><source>ACM Trans. Storage</source><year>2023</year><volume>19</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type=\"doi\">10.1145/3584663</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Xia, W. et al. The design of fast and lightweight resemblance detection for efficient post-deduplication delta compression. <italic toggle=\"yes\">ACM Trans. Storage</italic>. <bold>19</bold>, 1&#8211;30 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><citation-alternatives><element-citation id=\"ec-CR7\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jain</surname><given-names>AK</given-names></name><name name-style=\"western\"><surname>Kaur</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>NK</given-names></name><name name-style=\"western\"><surname>Khare</surname><given-names>A</given-names></name></person-group><article-title>Detecting smishing messages using Bert and advanced Nlp techniques</article-title><source>SN Comput. Sci.</source><year>2025</year><volume>6</volume><fpage>109</fpage><pub-id pub-id-type=\"doi\">10.1007/s42979-024-03532-7</pub-id></element-citation><mixed-citation id=\"mc-CR7\" publication-type=\"journal\">Jain, A. K., Kaur, K., Gupta, N. K. &amp; Khare, A. Detecting smishing messages using Bert and advanced Nlp techniques. <italic toggle=\"yes\">SN Comput. Sci.</italic><bold>6</bold>, 109 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR8\"><label>8.</label><mixed-citation publication-type=\"other\">Tang, D. et al. A low-rate Dos attack mitigation scheme based on Port and traffic state in Sdn. <italic toggle=\"yes\">IEEE Trans. Comput</italic><bold>74</bold> (2025).</mixed-citation></ref><ref id=\"CR9\"><label>9.</label><mixed-citation publication-type=\"other\">Liu, Y., Dong, X., Zio, E. &amp; Cui, Y. Active resilient secure control for heterogeneous swarm systems under malicious cyber-attacks. <italic toggle=\"yes\">IEEE Trans. Syst. Man. Cybern Syst.</italic><bold>55</bold> (2025).</mixed-citation></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Grabphisher: phishing scams detection in Ethereum via temporally evolving Gnns</article-title><source>IEEE Trans. Serv. Comput.</source><year>2024</year><volume>17</volume><fpage>3727</fpage><lpage>3741</lpage><pub-id pub-id-type=\"doi\">10.1109/TSC.2024.3411449</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Zhang, J. et al. Grabphisher: phishing scams detection in Ethereum via temporally evolving Gnns. <italic toggle=\"yes\">IEEE Trans. Serv. Comput.</italic><bold>17</bold>, 3727&#8211;3741 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gowdhaman</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Dhanapal</surname><given-names>R</given-names></name></person-group><article-title>Hybrid deep learning-based intrusion detection system for wireless sensor network</article-title><source>Int. J. Veh. Inf. Commun. Syst.</source><year>2024</year><volume>9</volume><fpage>239</fpage><lpage>255</lpage></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Gowdhaman, V. &amp; Dhanapal, R. Hybrid deep learning-based intrusion detection system for wireless sensor network. <italic toggle=\"yes\">Int. J. Veh. Inf. Commun. Syst.</italic><bold>9</bold>, 239&#8211;255 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><mixed-citation publication-type=\"other\">Liu, S. et al. The scales of justitia: A comprehensive survey on safety evaluation of llms. <italic toggle=\"yes\">arXiv preprint arXiv:2506.11094</italic> (2025).</mixed-citation></ref><ref id=\"CR13\"><label>13.</label><mixed-citation publication-type=\"other\">Roy, S. S., Nilizadeh, S. &amp; Phishlang A lightweight, client-side phishing detection framework using mobilebert for real-time, explainable threat mitigation. <italic toggle=\"yes\">arXiv preprint arXiv:2408.05667</italic> (2024).</mixed-citation></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Kaisser, T. &amp; Coste, C. I. SciTePress,. Using chat gpt for malicious web links detection. In <italic toggle=\"yes\">Proceedings of the 20th International Con- ference on Web Information Systems and Technologies - Volume 1: WEBIST</italic>, 425&#8211;432, DOI: 10.5220/0013069200003825. INSTICC (2024).</mixed-citation></ref><ref id=\"CR15\"><label>15.</label><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>B</given-names></name><etal/></person-group><article-title>Efficient classification of malicious urls: M-bert&#8212;a modified Bert variant for enhanced semantic Understanding</article-title><source>Ieee Access.</source><year>2024</year><volume>12</volume><fpage>13453</fpage><lpage>13468</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2024.3357095</pub-id></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Yu, B. et al. Efficient classification of malicious urls: M-bert&#8212;a modified Bert variant for enhanced semantic Understanding. <italic toggle=\"yes\">Ieee Access.</italic><bold>12</bold>, 13453&#8211;13468 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><label>16.</label><mixed-citation publication-type=\"other\">Tang, F., Yu, B., Zhao, S. &amp; Xu, M. Towards fraudulent url classification with large language model based on deep learning. In <italic toggle=\"yes\">4th International Conference on Computer Vision, Image and Deep Learning (CVIDL)</italic>, 503&#8211;507 (IEEE, 2023)., 503&#8211;507 (IEEE, 2023). (2023).</mixed-citation></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Su</surname><given-names>MY</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>KL</given-names></name></person-group><article-title>Bert-based approaches to identifying malicious urls</article-title><source>Sensors</source><year>2023</year><volume>23</volume><fpage>8499</fpage><pub-id pub-id-type=\"doi\">10.3390/s23208499</pub-id><pub-id pub-id-type=\"pmid\">37896591</pub-id><pub-id pub-id-type=\"pmcid\">PMC10610561</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Su, M. Y. &amp; Su, K. L. Bert-based approaches to identifying malicious urls. <italic toggle=\"yes\">Sensors</italic><bold>23</bold>, 8499 (2023).<pub-id pub-id-type=\"pmid\">37896591</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s23208499</pub-id><pub-id pub-id-type=\"pmcid\">PMC10610561</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rashid</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Ranaweera</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Doyle</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Seneviratne</surname><given-names>S</given-names></name></person-group><article-title>Llms are one-shot url classifiers and explainers</article-title><source>Comput. Networks</source><year>2025</year><volume>258</volume><fpage>111004</fpage><pub-id pub-id-type=\"doi\">10.1016/j.comnet.2024.111004</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Rashid, F., Ranaweera, N., Doyle, B. &amp; Seneviratne, S. Llms are one-shot url classifiers and explainers. <italic toggle=\"yes\">Comput. Networks</italic>. <bold>258</bold>, 111004 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><mixed-citation publication-type=\"other\">Zhang, Z., Wu, J., Lu, N., Shi, W. &amp; Liu, Z. Adaptpud: an accurate url-based detection approach against tailored deceptive phishing websites. <italic toggle=\"yes\">Comput Networks.</italic> 111303 (2025).</mixed-citation></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">Reyes-Dorta, N. &amp; Caballero-Gil, P. &amp; Rosa-Remedios, C. Detection of malicious urls using machine learning. <italic toggle=\"yes\">Wirel Networks.</italic><bold>30</bold>, 1&#8211;18 (2024).</mixed-citation></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jalil</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Usman</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Fong</surname><given-names>A</given-names></name></person-group><article-title>Highly accurate phishing url detection based on machine learning</article-title><source>J. Ambient Intell. Humaniz. Comput.</source><year>2023</year><volume>14</volume><fpage>9233</fpage><lpage>9251</lpage><pub-id pub-id-type=\"doi\">10.1007/s12652-022-04426-3</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Jalil, S., Usman, M. &amp; Fong, A. Highly accurate phishing url detection based on machine learning. <italic toggle=\"yes\">J. Ambient Intell. Humaniz. Comput.</italic><bold>14</bold>, 9233&#8211;9251 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">Ariawan, S. et al. IEEE,. Intelligent malicious url detection using kernel pca-svm-ga model with feature analysis. In <italic toggle=\"yes\">2024 International Conference on Data Science and Network Security (ICDSNS)</italic>, 1&#8211;6 (2024).</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Li, L. &amp; Gong, B. Prompting large language models for malicious webpage detection. In <italic toggle=\"yes\">2023 IEEE 4th international conference on pattern recognition and machine learning (PRML)</italic>, 393&#8211;400IEEE, (2023).</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name><etal/></person-group><article-title>An integrated Csppc and Bilstm framework for malicious url detection</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>6659</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-91148-z</pub-id><pub-id pub-id-type=\"pmid\">39994324</pub-id><pub-id pub-id-type=\"pmcid\">PMC11850714</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Zhou, J. et al. An integrated Csppc and Bilstm framework for malicious url detection. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold>, 6659 (2025).<pub-id pub-id-type=\"pmid\">39994324</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-91148-z</pub-id><pub-id pub-id-type=\"pmcid\">PMC11850714</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zaimi</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Safi Eljil</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Hafidi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Lamia</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Nait-Abdesselam</surname><given-names>F</given-names></name></person-group><article-title>An enhanced mechanism for malicious url detection using deep learning and distilbert-based feature extraction</article-title><source>J. Supercomput</source><year>2025</year><volume>81</volume><fpage>438</fpage><pub-id pub-id-type=\"doi\">10.1007/s11227-024-06908-x</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Zaimi, R., Safi Eljil, K., Hafidi, M., Lamia, M. &amp; Nait-Abdesselam, F. An enhanced mechanism for malicious url detection using deep learning and distilbert-based feature extraction. <italic toggle=\"yes\">J. Supercomput</italic>. <bold>81</bold>, 438 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Aljofey, A., Bello, S. A., Lu, J. &amp; Xu, C. Bert-phishfinder: A robust model for accurate phishing url detection with optimized distilbert. <italic toggle=\"yes\">IEEE Trans. Dependable Secur. Comput.</italic><bold>22</bold> (2025).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Buu, S. J. &amp; Cho, S. B. A transformer network calibrated with fuzzy logic for phishing url detection. <italic toggle=\"yes\">Fuzzy Sets Syst.</italic><bold>517</bold> 109474 (2025).</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Tian, Y., Yu, Y., Sun, J. &amp; Wang, Y. From past to present: A survey of malicious url detection techniques, datasets and code repositories. <italic toggle=\"yes\">arXiv preprint arXiv:2504.16449</italic> (2025).</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Malicious URLs dataset &#8212; kaggle.com. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset\">https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset</ext-link>. [Accessed 06-01-2025].</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">URL. | Datasets | Research | Canadian Institute for Cybersecurity | UNB &#8212; unb.ca. (2016). Available at: https://www.unb.ca/cic/datasets/ url-2016.html . (2025).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yasin</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Fatima</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>JA</given-names></name><name name-style=\"western\"><surname>Afzal</surname><given-names>W</given-names></name></person-group><article-title>Behind the bait: delving into phishtank&#8217;s hidden data</article-title><source>Data Brief.</source><year>2024</year><volume>52</volume><fpage>109959</fpage><pub-id pub-id-type=\"doi\">10.1016/j.dib.2023.109959</pub-id><pub-id pub-id-type=\"pmid\">38152492</pub-id><pub-id pub-id-type=\"pmcid\">PMC10751815</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Yasin, A., Fatima, R., Khan, J. A. &amp; Afzal, W. Behind the bait: delving into phishtank&#8217;s hidden data. <italic toggle=\"yes\">Data Brief.</italic><bold>52</bold>, 109959 (2024).<pub-id pub-id-type=\"pmid\">38152492</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.dib.2023.109959</pub-id><pub-id pub-id-type=\"pmcid\">PMC10751815</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><mixed-citation publication-type=\"other\">Community Projects -. RiskAnalytics &#8212; riskanalytics.com. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://riskanalytics.com/community/\">https://riskanalytics.com/community/</ext-link> (2025). [Accessed 23-07-2025].</mixed-citation></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Marchal</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Fran&#231;ois</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>State</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Engel</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Phishstorm</surname></name></person-group><article-title>Detecting phishing with streaming analytics</article-title><source>IEEE Trans. Netw. Serv. Manag</source><year>2014</year><volume>11</volume><fpage>458</fpage><lpage>471</lpage><pub-id pub-id-type=\"doi\">10.1109/TNSM.2014.2377295</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Marchal, S., Fran&#231;ois, J., State, R., Engel, T. &amp; Phishstorm Detecting phishing with streaming analytics. <italic toggle=\"yes\">IEEE Trans. Netw. Serv. Manag</italic>. <bold>11</bold>, 458&#8211;471 (2014).</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><mixed-citation publication-type=\"other\">Vaswani, A. et al. Attention is all you need. <italic toggle=\"yes\">Adv Neural Inform. Process. Systems</italic> 30 (2017).</mixed-citation></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Naveed, H. et al. A comprehensive overview of large Language models. <italic toggle=\"yes\">ACM Trans. Intell. Syst. Technol</italic><bold>16</bold> (2023).</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><mixed-citation publication-type=\"other\">Devlin, J. &amp; Bert Pre-training of deep bidirectional transformers for language understanding. <italic toggle=\"yes\">arXiv preprint arXiv:1810.04805</italic> (2018).</mixed-citation></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Openai-community/gpt2. &#183; Hugging Face &#8212; huggingface.co. Available at:&#160;<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://huggingface.co/openai-community/gpt2\">https://huggingface.co/openai-community/gpt2</ext-link>. (2024).</mixed-citation></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Zhang, P., Zeng, G., Wang, T. &amp; Lu, W. Tinyllama: An open-source small language model. <italic toggle=\"yes\">arXiv preprint arXiv:2401.02385</italic> (2024).</mixed-citation></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raffel</surname><given-names>C</given-names></name><etal/></person-group><article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title><source>J. Mach. Learn. Res.</source><year>2020</year><volume>21</volume><fpage>1</fpage><lpage>67</lpage></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. <italic toggle=\"yes\">J. Mach. Learn. Res.</italic><bold>21</bold>, 1&#8211;67 (2020).34305477\n</mixed-citation></citation-alternatives></ref><ref id=\"CR40\"><label>40.</label><citation-alternatives><element-citation id=\"ec-CR40\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kibriya</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Siddiqa</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>WZ</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>MK</given-names></name></person-group><article-title>Towards safer online communities: deep learning and explainable Ai for hate speech detection and classification</article-title><source>Comput. Electr. Eng.</source><year>2024</year><volume>116</volume><fpage>109153</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compeleceng.2024.109153</pub-id></element-citation><mixed-citation id=\"mc-CR40\" publication-type=\"journal\">Kibriya, H., Siddiqa, A., Khan, W. Z. &amp; Khan, M. K. Towards safer online communities: deep learning and explainable Ai for hate speech detection and classification. <italic toggle=\"yes\">Comput. Electr. Eng.</italic><bold>116</bold>, 109153 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Fu, R., Zhang, Z. &amp; Li, L. Using lstm and gru neural network methods for traffic flow prediction. In <italic toggle=\"yes\">2016 31st Youth academic annual conference of Chinese association of automation (YAC)</italic>, 324&#8211;328IEEE, (2016).</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><citation-alternatives><element-citation id=\"ec-CR42\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Salih</surname><given-names>AM</given-names></name><etal/></person-group><article-title>A perspective on explainable artificial intelligence methods: Shap and lime</article-title><source>Adv. Intell. Syst.</source><year>2025</year><volume>7</volume><fpage>2400304</fpage><pub-id pub-id-type=\"doi\">10.1002/aisy.202400304</pub-id></element-citation><mixed-citation id=\"mc-CR42\" publication-type=\"journal\">Salih, A. M. et al. A perspective on explainable artificial intelligence methods: Shap and lime. <italic toggle=\"yes\">Adv. Intell. Syst.</italic><bold>7</bold>, 2400304 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alsaedi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Ghaleb</surname><given-names>FA</given-names></name><name name-style=\"western\"><surname>Saeed</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Ahmad</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Alasli</surname><given-names>M</given-names></name></person-group><article-title>Cyber threat intelligence-based malicious url detection model using ensemble learning</article-title><source>Sensors</source><year>2022</year><volume>22</volume><fpage>3373</fpage><pub-id pub-id-type=\"doi\">10.3390/s22093373</pub-id><pub-id pub-id-type=\"pmid\">35591061</pub-id><pub-id pub-id-type=\"pmcid\">PMC9101641</pub-id></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Alsaedi, M., Ghaleb, F. A., Saeed, F., Ahmad, J. &amp; Alasli, M. Cyber threat intelligence-based malicious url detection model using ensemble learning. <italic toggle=\"yes\">Sensors</italic><bold>22</bold>, 3373 (2022).<pub-id pub-id-type=\"pmid\">35591061</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s22093373</pub-id><pub-id pub-id-type=\"pmcid\">PMC9101641</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">R, U. S. D. &amp; Patil, A. &amp; Mohana. Malicious url detection and classification analysis using machine learning models. In <italic toggle=\"yes\">2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)</italic>, 470&#8211;476, (2023). 10.1109/IDCIoT56793.2023.10053422</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><mixed-citation publication-type=\"other\">Best of the World.&#160;-nationalgeographic.com. Available at: https://www.nationalgeographic.com/travel/topic/ best-of-the-world-hub. (2023).</mixed-citation></ref><ref id=\"CR46\"><label>46.</label><mixed-citation publication-type=\"other\">Team, C. Web Defacement Attacks: 5 Website Defacement Examples &#8212; websitesecuritystore.com. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://websitesecuritystore.com/blog/website-defacement-attacks-examples\">https://websitesecuritystore.com/blog/website-defacement-attacks-examples</ext-link>. [Accessed 10-05-2025].</mixed-citation></ref><ref id=\"CR47\"><label>47.</label><mixed-citation publication-type=\"other\">XcodeGhost - Wikipedia. &#8212; en.wikipedia.org. Available at:&#160;<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://en.wikipedia.org/wiki/XcodeGhost\">https://en.wikipedia.org/wiki/XcodeGhost</ext-link>. (2025).</mixed-citation></ref><ref id=\"CR48\"><label>48.</label><mixed-citation publication-type=\"other\">Anna Chung, S. B. Phishing Eager Travelers &#8212; unit42.paloaltonetworks.com. Available at:&#160;https://unit42.paloaltonetworks.com/ travel-themed-phishing/ . (2025).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12675596 PMC12675596.1 12675596 12675596 41330959 10.1038/s41598-025-26653-2 26653 1 Article Lightweight malicious URL detection using deep learning and large language models Kibriya Hareem 1 Amin Rashid rashid.sdn1@gmail.com 2 Alshamrani Sultan S. 3 Rehman Safia 2 Hassan Mehdi 1 Alsubaei Faisal S. 4 1 https://ror.org/03yfe9v83 grid.444783.8 0000 0004 0607 2515 Department of Computer Science, Air University, Islamabad, Pakistan 2 https://ror.org/03v00ka07 grid.442854.b Department of Computer Science, University of Engineering and Technology, Taxila, Pakistan 3 https://ror.org/014g1a453 grid.412895.3 0000 0004 0419 5255 Department of Information Technology, College of Computer and Information Technology, Taif University, P.O. Box 11099, Taif, 21944 Saudi Arabia 4 https://ror.org/015ya8798 grid.460099.2 0000 0004 4912 2893 Department of Cybersecurity, College of Computer Science and Engineering, University of Jeddah, Jeddah, Saudi Arabia 2 12 2025 2025 15 478255 43044 10 5 2025 29 10 2025 02 12 2025 05 12 2025 05 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . With thousands of new websites emerging daily, distinguishing between legitimate and malicious web pages has become increasingly challenging, as many of these sites compromise users&#8217; private data without consent, posing severe cybersecurity threats. The absence of robust detection mechanisms exposes users to cyberattacks, financial fraud, and identity theft. While several Machine Learning (ML)-based techniques exist, they suffer from limitations such as reliance on handcrafted features and difficulty in adapting to evolving attack patterns. To mitigate these challenges, this paper introduces a fully automated deep learning (DL) based framework designed for the detection of malicious Uniform Resource Locators (URLs). The framework utilizes Large Language Models (LLMs) to generate high-quality URL embeddings that capture complex patterns and token relationships in URLs without manual feature engineering. These embeddings are then classified into four categories, i.e., defacement, malware, benign, and phishing, using a customized DL-based model that is finalized using extensive ablation experiments. The proposed DL model uses Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers to capture long-range dependencies between the embeddings. The proposed system achieved the highest accuracy of 97.5% using a Bidirectional Encoder Representations from Transformers (BERT) and a DL-based model. With only 0.5&#160;M parameters, the BERT&#8201;+&#8201;DL model can classify samples in 0.119 ms. Additionally, to enhance interpretability and trustworthiness, the eXplainable AI (XAI) technique called Local Interpretable Model-Agnostic Explanations (LIME) is used to visualize model decisions to ensure the model&#8217;s transparency and reliability in a real-time setting. Subject terms Energy science and technology Engineering https://doi.org/10.13039/501100006261 Taif University pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction The widespread proliferation of the internet has revolutionized access to information, services, communication, and transactions, connecting millions of people worldwide. However, this connectivity has also introduced significant cybersecurity challenges, particularly the widespread proliferation of malicious URLs, making phishing website detection a key focus in cybersecurity. These harmful links are exploited by cybercriminals to engage in different cyberattacks, including phishing, unauthorized data breaches, and malware distribution, thus posing severe threats to individuals and organizations alike 1 . According to a 2013 RSA report, approximately 450,000 websites were affected by phishing attacks, leading to estimated financial losses of around USD. 5.9 billion 2 , 3 . To mitigate threats, malicious URLs are blacklisted; however, their effectiveness is limited, as new malicious URLs continuously emerge. The attackers use sophisticated deception techniques to make the illegitimate URL look legitimate by using URL spoofing and obfuscation, etc., to evade detection and attack unsuspecting users. The malicious websites are carefully designed in such a way that they closely resemble legitimate web pages, thus making it increasingly difficult for users, especially those with limited cybersecurity awareness, to differentiate between authentic and harmful sites 4 . To address the growing threat of malicious URLs, numerous ML-based techniques have been proposed by researchers. However, these approaches often face significant limitations due to inherent challenges. By the time a malicious URL is identified and incorporated into a blacklist, it is often too late, as many users may have already been compromised 5 . Moreover, the increasing sophistication and variability of malicious URLs require continuous rule maintenance, which is not only error-prone but also a time-consuming and labor-intensive process. This reliance on static rule-based mechanisms and human expertise makes these systems rigid and less adaptable to evolving URL patterns. Another problem is the general lack of user awareness about the deceptive tactics employed by cyber attackers, which further heightens the risk of successful cyber attacks via malicious URLs 6 . Despite their advantages, the DL-based methods face several challenges as well. One significant drawback is their high memory and computational resource requirements, which can delay response times. This computational overhead may provide adversaries with a window of opportunity to bypass detection, thus making it impractical for real-time detection 7 . The DL-based models often lack interpretability and transparency, as their decision-making processes are not known, thus being called &#8220;black boxes&#8221;. This absence of model interpretability significantly hinders user understanding and trust in the outputs produced by DL models 8 , 9 . Another issue is the requirement of DL-based systems for the availability of large, balanced, and up-to-date datasets. In practice, such datasets are often scarce. Collection and development of a customised dataset is not only costly but requires expert knowledge as well 10 , 11 . The combination of computational complexity, limited interpretability, and data dependency poses significant barriers to the practical deployment and long-term reliability of DL-based solutions in cybersecurity. Recently, LLMs have emerged as a promising solution to many of the limitations associated with DL-based models in the cybersecurity domain. Due to the training of these models on massive datasets and their ability to understand and generate human-like language, LLMs have demonstrated effectiveness in various security-related tasks 12 . Motivated by these advancements and the challenges in malicious URL detection, this paper proposes a fully automated URL classification framework that integrates the strengths of both LLMs and DL models. The main contributions of this study are as follows: Utilize LLMs to generate high-quality URL embeddings that capture both lexical and contextual characteristics, while ensuring low resource consumption suitable for real-time detection. Design a lightweight, customized DL framework using LSTM and GRU layers to model long-range dependencies between the embeddings effectively. Optimize the architecture through comprehensive ablation studies and hyperparameter tuning to maximize classification efficiency. Evaluate the model on previously unseen and obfuscated URL samples to rigorously assess its robustness against advanced evasion tactics and stealthy adversarial techniques. Employ XAI techniques to develop user trust and facilitate informed decision-making in security applications. The remaining paper is organized as follows: Sect.&#160;2 presents a critical analysis of recent studies. The proposed methodology is detailed in Sect.&#160;3, followed by the presentation of results in Sect.&#160;4. Finally, Sect.&#160;5 concludes the article. Literature review In recent years, numerous researchers have proposed various automated techniques for the classification of malicious URLs. For example, Roy et al. 13 developed PhishLang using MobileBERT for contextual analysis of websites. PhishLang detected 25,796 phishing URLs using Generative Pre-trained Transformer (GPT) 3.5 Turbo. Moreover, the results were visually analyzed using XAI to enhance users&#8217; trust. Additionally, a browser extension was developed to facilitate user interaction. Kaisser et al. 14 proposed a framework for classifying malicious web links using GPT&#8722;3.5 Turbo and GPT&#8722;4, alongside various ML models. The framework extracted both manual features and GPT-generated features, which were then classified using different models, and attained 95% accuracy. However, the model is trained on only 1,000 random URL samples; hence, the system requires rigorous validation before deployment. Yu et al. 15 developed an M-BERT-based model to detect malicious websites using a customised dataset and attained 0.94 precision. Tang et al. 16 proposed a classification approach using BERT, GPT, Ernie, ML/LSTM, and ConvBERT. The system achieved an accuracy of 93.1%, but despite employing lightweight models, its overall performance was suboptimal. Su et al. 17 introduced a BERT-based model that attained 98% accuracy. The system was trained and validated on the ISCX 2016 dataset, containing nearly 100,000 URLs. However, due to the dataset&#8217;s limited samples, a thorough validation is required before deployment in real-world scenarios. Rashid et al. 18 employed one-shot learning with LLMs for malicious URL classification by using Chain-of-Thought reasoning. Human interpretable interpretations were generated to explain the classification outcomes. The approach was evaluated on three benchmark datasets using five LLMs, namely GPT&#8722;4 Turbo, Claude 3 Opus, Gemini, LLaMA 2 &amp; 3. Among these, GPT&#8722;4 Turbo achieved the highest F1 score of 0.92 in both zero-shot and five-shot classification scenarios. Zhang et al. 19 proposed AdaptPUD, a URL-based phishing detection method using a Token-Property Embedding technique to capture both semantic and structural URL features. The model used a hybrid model combining multi-channel CNNs, Bi-GRU, Self-Attention, and Concept Drift Detection for adaptive, incremental learning. The model obtained over 91% accuracy, and detected phishing URLs in 0.19 ms. However, the model only performs binary classification. Moreover, it has low performance. Dorta et al. 20 proposed a fraudulent URL detection system combining traditional ML and Quantum Machine Learning (QML) techniques. The system achieved approximately 90% accuracy on 180,000 URL samples. However, classical ML models outperformed QML due to the immaturity of quantum hardware and the lack of optimized algorithms. Jalil et al. 21 developed an ML-based phishing URL detection framework that relies solely on lexical features extracted from URLs. Using. TF-IDF and entropy-based features, they achieved a maximum accuracy of 96.8%. However, the reliance on ML techniques alone introduces inherent limitations, therefore necessitating further validation before deployment. Ariwan et al. 22 proposed a Kernel Principal Component Analysis-Support Vectors Machine-Genetic Algorithm (PCA-SVM-GA)-based model for detecting malicious URLs. The system used Kernel PCA for dimensionality reduction, SVM for classification, and GA for optimization, which attained an accuracy of 93.52%. Despite its effectiveness, the approach is constrained by its dependence on handcrafted feature extraction, which has inherent limitations. Li et al. 23 used LLMs for malicious website detection. The system utilized zero-shot and few-shot prompting with GPT&#8722;3.5 (175B parameters) and ChatGPT, eliminating the need for large-scale annotated datasets. The system achieved 96% accuracy using GPT&#8722;3.5. However, it is computationally expensive and was trained on a relatively small dataset of approximately 1,000 samples, thus requiring extensive validation before deployment. Singh et al. 24 used a Bidirectional LSTM (BiLSTM) network with a Convolutional Block Attention Module (CBAM) and Spatial Pyramid Pooling (SPP). The model was evaluated on two benchmark datasets, each comprising two classes: phishing and benign URLs. Similarly, Zaimi et al. 25 proposed a hybrid DL framework that combines DistilBERT for contextual URL feature extraction with a CNN&#8211;LSTM classifier for malicious URL detection, achieving 98% accuracy. While the models in 24 and 25 demonstrate strong performance, the applicability is limited due to the binary classification setting, which restricts the deployment in more complex, real-world environments where more granular classification is often required. Aljofey et al. 26 proposed BERT-PhishFinder for phishing URL detection using fine-tuned DistilBERT embeddings. The model was enhanced by incorporating SpatialDropout1D, global pooling, and parallel dense layers to extract features. The model achieved over 99.30% accuracy. However, the model performs binary classification. Hence, needs to be extended to multi-class classification for real-world deployment. Buu et al. 27 proposed a fuzzy-calibrated transformer network for phishing URL detection, to combine the learning capability of a transformer-based deep learning model with the interpretability of fuzzy logic. The model used Gaussian membership functions and fuzzy rule weighting, and includes a recalibration mechanism that updates fuzzy parameters and retrains the model when performance drops. The model attained 98.9% accuracy. Despite a significant performance, the model is limited to binary classification. Moreover, the use of fuzzy logic introduces challenges in scalability, expert dependency, and computational overhead during rule recalibration. Existing approaches for URL detection primarily rely on DL techniques. However, these systems face several limitations, including the use of low-quality or limited datasets, suboptimal performance, and high computational complexity. Additionally, many of these models are restricted to binary classification, failing to differentiate between specific types of malicious URLs. A further limitation is the lack of explainability, which hinders user trust and transparency. Therefore, such systems are not well-suited for deployment in URL detection applications. Proposed methodology An end-to-end framework is proposed for malicious URL detection, which integrates state-of-the-art LLMs with a customized lightweight DL model. The model initially generates the embedding using the encoder/decoder of pre-trained LLMs (GPT-2, Tiny Llama, T5-Large, and BERT). These embeddings are then classified into four URL categories, i.e., Phishing, Malware, Defacement, and Benign, using a customized DL model that is finalized after extensive ablation experiments. The overall architecture of the proposed framework is illustrated in Fig.&#160; 1 . Finally, to ensure transparency and trustworthiness, the results of the proposed model are interpreted using an XAI technique called LIME. Dataset acquisition The study utilizes a publicly available dataset obtained from Kaggle, comprising URLs categorized into four distinct classes: Phishing, Benign, Malware, and Defacement. Malware URLs are designed to distribute malicious software, such as viruses, ransomware, or spyware, that can compromise user devices and steal sensitive data after download. Defacement URLs typically target websites by altering their appearance and then injecting unauthorized content, usually as an act of cyber vandalism or hacktivism. Phishing URLs mimic legitimate websites to deceive users into providing confidential information, including login credentials, banking details, etc. All these URLs appear legitimate at first glance, but are deceptive and malicious. Finally, the Benign URLs in the dataset are safe and legitimate web addresses that do not pose any security threats, serving regular online content without malicious intent 25 , 28 . The dataset 29 consists of 651,191 URLs, out of which 428,103 URLs belong to the benign category, 96,457 to defacement, 94,111 to phishing, and 32,520 to malware. This repository is sourced and combined from multiple datasets, including ISCX-URL&#8722;2016 30 , Phish Tank 31 , Malware Domains 32 , and Phish Storm 33 . The makers of this dataset combined the datasets from multiple sources and augmented the benign class samples only, while merging the rest. It is worth mentioning that the dataset has not been preprocessed to retain the elements, i.e., symbols and characters that are essential for effective malicious URL detection. Furthermore, the labels from the original dataset have been carefully verified and corrected to ensure accuracy in the experimental results. Duplicate entries were removed before processing. Figure 2 presents the class-wise distribution of the. Fig. 1 Block diagram illustrating the workflow of the proposed URL classification framework. dataset employed in this study. The dataset is partitioned using a 60:40 train&#8211;test split, wherein 60% of the randomly selected samples are utilized for training, and the remaining 40% are reserved for evaluating the performance of the proposed model. Fig. 2 Pie chart illustrating dataset distribution in a class-wise manner. Problem formulation The rapid proliferation of internet usage has contributed to the widespread emergence of malicious URLs as a significant cybersecurity threat. These URLs are being used as a means of cyberattack, thus endangering the privacy and security of users. The study is conducted using a publicly available dataset. Let the set of URLs be represented as: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$U=\\left\\{u_1,\\:u_2,\\dots,u_n\\right\\},$$\\end{document} Where u i denotes URLs contained in the dataset. The aim to develop a robust and lightweight framework to to classify each URL u i from a labelled dataset D = {( u 1 , y 1 ), ( u 2 , y 2 ), .. , ( u n , y n )} into a specified label y i , using LLMs and a lightweight customised DL model, such that: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$y_i=\\begin{array}{l} {\\rm}0\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Benign\\: class}}\\\\ 1\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Defacement\\: Class}}\\\\ 2\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Malware\\: Class}}\\\\ 3\\:\\:{\\rm{if}}\\:{u_i}\\:{\\rm{belongs\\: to\\: the\\: Phishing\\: Class}} \\end{array}$$\\end{document} URL embedding generation using LLMs With the recent surge in AI advancements, LLMs have gained significant attention for their remarkable performance in NLP-related tasks. Their ability to learn context and patterns from massive datasets makes them highly valuable across domains. LLMs are advanced DL models built on transformer architectures 34 that use self-attention mechanisms to process and generate human-like text. These models convert input text into high-dimensional embeddings, which are then processed through multi-layered neural networks using self-attention to capture contextual relationships within text sequences 35 . Initially, the input sequence is tokenized and passed through an embedding layer to transform the discrete tokens into dense vector representations. Each LM utilizes a model-specific subword tokenization strategy to preprocess input URLs. Specifically, BERT employs WordPiece tokenization, GPT&#8722;2 and TinyLLaMA use Byte-Pair Encoding (BPE), while T5 adopts a SentencePiece tokenizer with a unigram language model. Since transformers lack inherent order awareness, positional encoding is added to retain sequential information. In encoder-based models like BERT and T5 (encoder side), the input embeddings are passed through several self-attention and feed-forward networks to produce rich contextual embeddings that capture bidirectional dependencies. In contrast, decoder-only models such as GPT-2 and TinyLLaMA generate embeddings using masked self-attention, enabling autoregressive processing where each token attends only to previous ones. Although decoder models are primarily designed for generation tasks, the hidden states from their intermediate or final layers can be used as contextual embeddings for classification tasks. In both architectures, the multi-head self-attention mechanism captures long-range dependencies across the input sequence. In contrast, the position-wise feed-forward networks perform non-linear transformations to enhance the features. In the standard transformer model, a decoder is employed in conjunction with the encoder to facilitate autoregressive sequence generation. An overview of the transformer architecture is presented in Fig.&#160; 3 . Transformers have since evolved into various architectures, with each adapting the original framework through minimal changes to the encoder/decoder structures. In this study, embeddings are extracted using either the encoder (BERT, T5) or the decoder (GPT, Tiny LLaMA), depending on the model. The use of LLMs for embedding vector generation is motivated by their pretraining on massive, domain-diverse corpora and their ability to learn complex semantic and syntactic patterns through millions of parameters. Their ability to capture sequential dependencies without the need for manual feature engineering makes them well-suited for detecting malicious URLs. BERT, introduced by Google in 2018, adopts an encoder-only transformer architecture composed of multiple stacked layers, each containing a multi-head self-attention mechanism followed by a position-wise feed-forward neural network 36 . The BERT-Base model contains 12 layers (transformer blocks), each with 12 attention heads, and approximately 110 million parameters. Each attention head in the multi-head mechanism allows the model to attend to different parts of the input sequence, enabling a richer understanding of contextual relationships. BERT uses absolute positional encodings, which are added to the input token embeddings to incorporate positional information before the attention layers process the data. The model employs the Gaussian Error Linear Unit (GeLU) as its activation function, which improves gradient flow and contributes to stable training. Layer normalization is applied after each sub-layer (self-attention and feed-forward) to enhance training stability and convergence. Unlike unidirectional models that process text left-to-right or right-to-left, BERT uses a bidirectional training objective through masked language modeling (MLM), where random tokens in the input are masked, and the model is trained to predict them based on both left and right context. This bidirectional context modeling allows BERT to effectively capture deeper semantic and structural patterns, making it well-suited for encoding meaningful representations of URL components. GPT is a generative LLM developed by OpenAI 37 that is pre-trained on a vast amount of internet data and contains a total of 117 million parameters. GPT&#8722;2 is a decoder-only Transformer architecture with masked self-attention, meaning each token can only attend to previous tokens. GPT&#8722;2 base has 12 layers, with 12 self-attention heads per layer, and scales up to larger models with more parameters. Unlike BERT, GPT&#8722;2 does not use predefined absolute positional encodings; instead, it employs learned positional embeddings, allowing it to dynamically determine positional relationships rather than relying on fixed encodings. GPT&#8722;2 also uses GELU activation, similar to BERT, and applies layer normalization before each self-attention block. However, unlike BERT, GPT&#8722;2 employs an autoregressive approach, which restricts each token to attending only to preceding tokens, thus preventing bidirectional context modeling. This study also utilizes TinyLLaMA, a lightweight variant of the LLaMA family, sharing architectural similarities and tokenizer design with LLaMA-2, but significantly smaller in scale, containing approximately 1.1 billion parameters 38 . Like. Fig. 3 Transformer architecture showing the encoder-decoder structure 34 . GPT models, TinyLLaMA adopts a decoder-only transformer architecture optimized for autoregressive tasks. It incorporates several architectural improvements, including grouped-query attention (GQA), rotary positional embeddings (RoPE), and the SwiGLU (Swish-Gated Linear Unit) activation function. The model consists of 22 transformer blocks, each featuring 32 attention heads, organized into 4 query groups with eight heads per group to support efficient grouped-query attention. This structure improves memory usage and computational efficiency compared to standard multi-head attention. Unlike models with absolute positional encodings, TinyLLaMA uses rotary positional embeddings, which dynamically encode relative position information and enhance the model&#8217;s ability to generalize across variable-length sequences. Additionally, it employs SwiGLU activation and RMSNorm (Root Mean Square Layer Normalization) as part of a pre-normalization setup, offering improved training stability and faster convergence compared to GELU-based architectures. Lastly, T5-Large (Text-to-Text Transfer Transformer), introduced by Google in 2019, adopts a full encoder&#8211;decoder transformer architecture designed to frame all NLP tasks in a unified text-to-text format 39 . The T5-Large model comprises 24 encoder layers and 24 decoder layers, each equipped with 16 attention heads, and contains approximately 770 million parameters with an embedding size of 1024. T5 incorporates a learned relative positional bias in place of absolute or rotary positional encodings. This mechanism modulates attention scores based on the relative distances between tokens, which enables the model to generalize more effectively across input sequences of varying lengths. Unlike models such as BERT and GPT&#8722;2, which use pre-layer normalization, T5 applies layer normalization after the self-attention and feed-forward layers (i.e., post-layer normalization). The LLM also uses SwiGLU (Swish-Gated Linear Unit) activation function, which improves gradient flow and computational efficiency compared to ReLU or GELU. While not as lightweight as some alternatives, T5-Large is robust and highly expressive, making it effective in capturing the structural and contextual information within URLs. All these models fall under the umbrella of LLMs, i.e., GPT&#8722;2, T5, and TinyLLaMA are generative language models, whereas BERT is a masked language model. The URLs are initially tokenized, where the models break URLs into sub-word tokens, enabling a more granular and meaningful representation of the input data. Let a URL sequence be represented as: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{URL}=\\left\\{t_1,\\:t_2\\dots,t_n\\right\\},$$\\end{document} Where t represents the token in the URL. Truncation is applied to limit the tokens to a maximum length (n) of 786 for BERT and GPT-2, 2048 for Tiny Llama, and 1024 for T5 (Large). These tokens are then transformed into dense input embeddings using the LLM encoder&#8217;s embedding layer: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E=\\left\\{e_1,\\:e_2,\\dots,e_n\\right\\}\\:\\:\\:\\:e_i=W_e[t_i]$$\\end{document} where W e represents the learnable embedding matrix, and e i is the dense vector representation of the i -th token ( t i ). Since Transformers are inherently order-agnostic, the positional encodings (PE) are used to enhance comprehension of token order further: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E^\\prime_i=e_i+PE(i),$$\\end{document} where PE( i )for the i -th position, defined as: 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{PE}_{(i,2k)}=\\text{sin}\\frac{i}{10000^{2k/d}}$$\\end{document} 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{PE}_{(i,2k+1)}=\\text{cos}\\frac{i}{10000^{2k/d}}$$\\end{document} here d is the dimension of the embedding vector, and k is the index of the embedding dimension. The tokenized and encoded data is then fed into the LLM&#8217;s transformer layers. In these layers, the multi-head self-attention mechanism enables these models to focus on relevant portions of the input sequence: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{Attention}(Q,K,V)=\\text{softmax}\\frac{QK^T}{\\sqrt[]{\\overline{dk}}}$$\\end{document} Where Q , K , and V denote the query, key, and value matrices, respectively, and d k represents the dimensionality of the key vectors 34 . The final context-rich embeddings from these LLMs are then supplied to the customised DL framework for the classification of URLs into different categories. Fig. 4 Proposed DL based framework for malicious URL classification. LLM generated embedding classification via customised DL model The LLM-generated embeddings are then supplied to the proposed DL model illustrated in Fig.&#160; 4 for classification. The first learnable layer of the architecture is a one-dimensional convolutional (Conv-1D) layer consisting of 64 filters, each with a kernel size of 3. This layer performs a convolution operation over the input sequence, which can be mathematically described as: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$y_i=\\sum_{k=0}^{K-1}wk\\cdot{x_{i+k}}+b\\:\\:\\:\\:\\text{for}\\:i\\:\\epsilon[0,N-K],$$\\end{document} where y i denotes the output at position i , x is the input sequence of length N , w k represents the k -th weight of the convolutional kernel of size K , and b is the bias term. This operation enables the model to extract local patterns from the sequence data. A Rectified Linear Unit (ReLU) activation function is applied element-wise to the output to introduce non-linearity into the model. The ReLU function is defined as: 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{ReLU(z)}=\\text{max(0,z)},$$\\end{document} Where z is the input to the activation function. If z is negative, the output is set to zero; otherwise, it remains unchanged. This non-linear transformation plays a critical role in accelerating training convergence and alleviating the vanishing gradient problem, thereby improving the model&#8217;s learning capacity. The output of the convolutional layer is then passed to an LSTM layer containing 32 neurons. The LSTM layer processes the input by maintaining a memory cell that is selectively updated using its different gating mechanisms; these gates include the input gate, forget gate, and output gate. The input gate ( i t ) determines the new infomration ( C &#732; t ) should be incorporated into the memory cell ( C t ). The forget gate ( f t ) determines what fraction of the previous memory content ( C t &#8722;1 ) has to be kept or discarded. The output gate ( o t ) controls the portion of the updated memory cell to be exposed as the hidden state output ( h t ). The candidate memory cell ( C &#732; t ) is computed via a non-linear transformation, typically a tanh activation applied to a weighted sum of the current input and the previous hidden state. The memory cell is then updated as: 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_t=f_t\\star{C_{t-1}}+i_t\\star{\\widetilde{C}}_t,$$\\end{document} This gating mechanism enables the LSTM to retain essential long-term dependencies while discarding irrelevant information, thus maintaining contextual information over extended time steps 40 . Next, a GRU layer with 64 neurons is employed. Compared to the LSTM architecture, GRU is much simpler as it combines the forget and input gates into a single update gate. The simpler design reduces computational complexity while maintaining the ability to model long-range dependencies 41 . Following the initial GRU layer, the proposed framework integrates an additional. LSTM layer with 128 neurons and a subsequent GRU layer with 256 neurons to further capture temporal dependencies within the data. The output from these layers is then passed through a fully connected (Dense) layer comprising 128 neurons. Layer normalization is applied to this output to stabilize and normalize the data, which is then passed through an additional FC layer comprising 64 neurons. A dropout layer is also used with a drop rate of 0.3 to eliminate 30% of the neurons during training randomly. The final classification layer (dense) contains four neurons, each representing a class, i.e., Malware, Defacement, Phishing, or Benign. Detailed configuration of the proposed IDS is depicted in Table&#160; 1 . Table 1 Layer-wise specifications of customised DL model with embeddings from TinyLLaMA, GPT-2, T5, and BERT. Layer Configuration Input Input Shape: D (D&#8201;=&#8201;768/1024/2048) Conv-1D Filters: 64, Kernel Size: 3, ReLU, Padding: Same LSTM 1 Units: 32 GRU 1 Units: 64 LSTM 2 Units: 128, Dropout: 0.3 GRU 2 Units: 256, Recurrent Dropout: 0.2 Dense 1 Units: 128, Activation: ReLU LayerNormalization Applied after Dense Dense 2 Units: 64, Activation: ReLU Dropout Dropout Rate: 0.3 Dense (Softmax) Units: 4, Activation: Softmax Result visualization using XAI Finally, the predictions are analyzed using XAI techniques. Despite the robust performance of DL models across various domains, the challenge of a lack of transparency and non-explainability of the results and the model&#8217;s inner workings remains. The rationale behind specific decisions, the key features or regions influencing outcomes, and the level of confidence in predictions are still missing. Hence, to make these models more explainable, transparent, and trustworthy, a field of XAI has emerged, aiming to transform these &#8220;black box&#8221; models into interpretable and understandable systems. This study uses LIME, which is an XAI technique designed to provide local interpretability by explaining the model&#8217;s behavior for a specific instance. It achieves this by approximating the complex model with a simpler, interpretable surrogate that closely replicates the original model&#8217;s predictions. This technique provides a single plot of explanations with words that contributed either positively or negatively towards the classification 42 . The proposed framework results This section provides an elaborate explanation of the results obtained from the proposed LLM&#8201;+&#8201;DL classification framework. Performance metrics The proposed model is assessed using various state-of-the-art metrics such as Accuracy, Precision, Recall, and F1-Score. Accuracy is a well-known metric that shows the overall correctness of the model by computing the frequency of correct predictions made by the model. Accuracy is calculated in Eq.&#160;12. Here, TP, TN, FP, and FN denote True Positive, True Negative, False Positive, and False Negative, respectively. 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{Accuracy}=\\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}}$$\\end{document} In scenarios involving imbalanced data, accuracy alone may not serve as a reliable metric to evaluate the model, as a model can attain high accuracy by predominantly predicting the majority class. Therefore, the proposed framework is also assessed using additional metrics, including precision, recall, and F1-score. Precision calculates the ratio of correctly predicted positive instances to the total predicted positives. Higher precision indicates greater reliability in the model&#8217;s positive predictions. Mathematically, the equation can be calculated as in Eq.&#160;13. 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{Precision}=\\frac{\\text{TP}}{\\text{FP+TP}}$$\\end{document} Recall is a popular metric that measures the proportion of actual positive cases correctly identified by the model. It can be calculated as: 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{Recall}=\\frac{\\text{TP}}{\\text{FN+TP}}$$\\end{document} Finally, the model is also evaluated using the F1-score, which computes a harmonic mean of precision and recall that balances the trade-off between these two metrics. This metric is beneficial in an imbalanced class scenario. F1-score is calculated in Eq.&#160;15. 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_i-\\text{Score}=2\\times\\frac{\\text{Recall}\\times\\text{Precision}}{\\text{Recall}+\\text{Precision}}$$\\end{document} Results This section presents the results achieved by the proposed model, with classification labels defined as follows: Class 0 represents Benign, Class 1 denotes Defacement, Class 2 corresponds to Malware, and Class 3 indicates Phishing. The proposed DL model is trained for a maximum of 30 epochs with a batch size of 64. To prevent overfitting, early stopping is applied with a patience of 5 epochs, monitoring the validation loss throughout training. The proposed framework achieved 97.5% accuracy with both BERT- and T5-based embeddings. The confusion matrix corresponding to the BERT-based deep learning model is shown in Fig.&#160; 5 . The diagonal entries in the confusion matrix represent correct predictions for each class: 169,274 for class 0, 37,772 for class 1, 12,286 for class 2, and 34,539 for class 3. The off-diagonal elements represent misclassifications, such as 2,206 instances of class 0 incorrectly classified as class 3, and 2,344 cases of class 3 misclassified as class 0, thereby reflecting the overall robustness of the model. Fig. 5 Confusion matrix of the BERT-based DL model. The classification report obtained from the BERT&#8201;+&#8201;DL framework is depicted in Table&#160; 2 . The proposed models attained an average precision, recall, and F1-score of 0.97, 0.97, and 0.96, respectively. Table 2 Detailed classification report showing precision, recall, and F1-score for the BERT-based. Class Accuracy Precision Recall F1-Score 0 0.99 0.98 0.99 0.99 1 0.99 0.98 0.99 0.98 2 0.94 0.98 0.94 0.96 3 0.92 0.92 0.92 0.92 Deep learning model BERT&#8217;s Precision Recall (PR) curve is illustrated in Fig.&#160; 6 a. The graph shows that the proposed framework attained very high AP scores, ranging from 0.97 to 1.0. Figure&#160; 6 b shows the Receiver Operating Curve (ROC) obtained from the BERT&#8201;+&#8201;DL model. The graph shows a perfect Area Under Curve (AUC) of 1.0 from classes 0,1, and 2. Class 2 obtained an AUC of 0.99. The curves show a robust performance of the proposed model over the test set. The oscillations in the class are due to a class imbalance issue. Despite such class imbalance, the model performed exceptionally well in terms of all the metrics. Fig. 6 Classification performance of the BERT-based deep learning model for malicious URL detection. ( a ) PR Curve illustrating the trade-off between precision and recall ( b ) ROC depicting the model&#8217;s capability to distinguish between classes based on true positive and false positive rates. ( a ) PR Curve, ( b ) ROC Curve. The learning curves indicating (a) Loss and (b) Accuracy are depicted in Fig.&#160; 7 . The graph shows a sudden rise in the curve, indicating that the model started learning from the data. Afterward, the learning stabilized and eventually stopped after the loss score stopped decreasing (due to early stopping). Fig. 7 Learning curve showing model accuracy over epochs for the BERT&#8201;+&#8201;DL framework. This study also used the GPT-2 model for extracting the embeddings, which are then classified using a customised DL model. The proposed framework achieved an accuracy of 97% using the GPT-2&#8201;+&#8201;DL model. A detailed analysis of each class&#8217;s performance is presented in the classification report shown in Table&#160; 3 . The proposed GPT 2-DL Model achieved an average Precision of 0.94, a Recall of 0.96, and an F1-score of 0.94, demonstrating its strong classification capability and robustness in accurately categorizing URLs into their respective categories. Table 3 Performance summary of the GPT-2&#8201;+&#8201;Deep learning model based on classification Metrics. Class Precision Recall F1-Score 0 0.98 0.99 0.99 1 0.96 0.99 0.98 2 0.96 0.91 0.94 3 0.94 0.88 0.91 To showcase the performance of the proposed GPT 2&#8201;+&#8201;DL framework, the ROC is computed and illustrated (as shown in Fig.&#160; 8 a). An AUC of 1.0 for Classes 0, 1, and 2 indicates perfect classification, while Class 3 has an AUC of 0.99. The curves are close to the upper left corner, high above the random guess line, which highlights that the model achieves high TPR with low FPR for all classes, showing the model&#8217;s robustness despite class imbalance issues. Figure 8 b illustrates the PR curve generated by the proposed method. The PR curve calculates the trade-off between precision and recall for the multi-class classification task. Each curve corresponds to an individual class, with the AP score representing the area under the respective curve. In this graph, Class 0 and Class 1 achieve perfect precision and recall with AP scores of 1.0. Class 2 and Class 3 show a slightly lower but still high performance, with AP scores of 0.98 and 0.96, respectively. The curves are close to the top right corner, which indicates that the classifier maintains high precision even as recall increases. This suggests the model performs well across all classes, with minimal false positives or negatives. The gradual decline in precision for Classes 2 and 3 at high recall reflects a minor trade-off, which is typical when more true positives are obtained. Fig. 8 Performance assessment of the GPT-2-based deep learning model. ( a ) shows the ROC curve, highlighting the model&#8217;s capability to distinguish between malicious and benign URLs. ( b ) illustrates the class-wise Precision&#8211;Recall curves, indicating the balance between precision and recall across different classes. ( a ) ROC Curve, ( b ) PR Curves. The Tiny Llama&#8201;+&#8201;DL model achieved a final accuracy of 96.5%. A detailed class-wise breakdown of precision, recall, and F1-scores is provided in Table&#160; 4 . On average, the model attained a precision of 0.95, a recall of 0.93, and an F1-score of 0.94. Table 4 Performance summary of the tiny LLaMA&#8201;+&#8201;DL model based on classification. Class Accuracy Precision Recall F1-Score 0 98 0.98 0.98 0.98 1 98 0.98 0.98 0.98 2 88 0.98 0.88 0.93 3 90 0.88 0.90 0.89 Metrics The ROC curve is depicted in Fig.&#160; 9 a. The near-perfect AUC values (1.00 for Classes 0, 1, and 2, and 0.99 for Class 3) indicate that the model accurately distinguishes between classes with minimal misclassification. The curves are tightly clustered near the top-left corner, showing a high actual positive rate while keeping false positives low, demonstrating an outstanding predictive performance across all classes. The PR curve is depicted in Fig.&#160; 9 b, showing accuracy and robustness in the identification and classification of different types of URLs. The classification report for the T5-Large&#8201;+&#8201;DL model is presented in Table&#160; 5 . The model achieved average scores of 0.97 for accuracy, 0.97 for precision, 0.95 for recall, and 0.96 for F1-score. The PR curve showing a trade-off between Precision and Recall is shown in Fig.&#160; 10 a. The graph shows perfect AP scores of 1.0 for Class 0 and Class (1) Whereas Class 2 and Class 3 obtained AP scores of 0.98 and 0.97, respectively. ROC curve obtained from the T5&#8201;+&#8201;DL model depicted in Fig.&#160; 10 b shows perfect AUC of the model for Classes 0,1, and (2) Whereas, Class 3 obtained an AUC score of 0.99. Fig. 9 valuation of the Tiny LLaMA-based DL model for malicious URL classification. ( a ) ROC curve illustrating the trade-off between true positive and false positive rates. ( b ) PR curve showing the model&#8217;s performance across varying decision thresholds. ( a ) ROC Curve, ( b ) PR Curve. Fig. 10 Performance evaluation of the T5-based deep learning model. ( a ) shows the Precision&#8211;Recall curve indicating classification performance across different thresholds. ( b ) presents class-wise ROC curves illustrating the trade-off between true positive and false positive rates. ( a ) Precision&#8211;Recall Curve, ( b ) Class-wise ROC Curves. Table 5 Classification report of the T5-large&#8201;+&#8201;DL model for malicious URL classification. Class Accuracy Precision Recall F1-Score 0 0.99 0.98 0.99 0.98 1 0.99 0.97 0.99 0.98 2 0.94 0.98 0.94 0.96 3 0.91 0.94 0.90 0.92 Discussion This section analyzes the model&#8217;s performance in terms of computational complexity, time, and accuracy. Finally, the predictions obtained from the proposed model are visually analyzed using the XAI technique. Key observations Table&#160; 6 provides a comparative analysis of LLMs used in this study for generating URL embeddings. The table compares the LLMs in terms of their performance, compactness, and speed. The embeddings generated from these LLMs were finally classified using a lightweight DL model, which was finalized using extensive ablation experiments. The comparison depicts that the BERT&#8201;+&#8201;DL model achieved 97.5% accuracy with only 0.5&#160;M parameters. Whereas GPT-2, T5-Large, and Tiny Llama obtained accuracy of 97%, 97.5%, and 96.5%, respectively. The inference times (in seconds) per sample reported in the table show the average testing time of the models on 260,438 samples. Even though T5 and BERT attained almost the same performance, the BERT-based model is lightweight and quicker compared to T5. BERT&#8201;+&#8201;DL model classified the samples in a minimum time of 0.11 ms/sample, computed on a set of 260,438 test samples. BERT&#8217;s exceptional performance can be attributed to its bidirectional attention mechanism, which enhances its ability to capture contextual dependencies effectively. It is worth mentioning that these LLMs were only used for embedding generation rather than direct training; hence, their parameters were not included in the overall model parameter count. The reduced parameter count of the proposed BERT&#8201;+&#8201;DL model makes it well-suited for deployment in environments with limited computational resources. Furthermore, in addition to being lightweight, the model also demonstrates robustness in quick detection and URL classification, making it well-suited for real-time applications in URL identification and categorization. Table 6 Evaluation of model efficiency and performance via accuracy, parameter count, and inference time. Model Accuracy No. of parameters Testing time (s) Tiny Llama&#8201;+&#8201;DL 96.5% 1.4&#160;M 0.13 ms/sample BERT (Base)&#8201;+&#8201;DL 97.5% 0.5&#160;M 0.11 ms/sample T5 (Large)&#8201;+&#8201;DL 97.5% 0.6&#160;M 0.15 ms/sample GPT-2&#8201;+&#8201;DL 97.0% 0.5&#160;M 0.11 ms/sample To finalize the DL model, a series of ablation experiments were conducted by systematically adding or removing layers to evaluate their impact, as illustrated in Table&#160; 7 . The number of neurons in each layer was empirically assessed, revealing that the proposed method achieved optimal performance with a configuration of 32, 64, and 128 neurons. In the first experiment, the complete architecture was employed, incorporating Conv-1D, Dense (FC), Layer Normalization, Dropout, Soft-max layers, and two sets of LSTM and GRU. The architecture achieved an accuracy of 97.5%. In the second experiment, a simplified architecture with only one set of LSTM and GRU layers attained an accuracy of 97.3%. Further, in the third experiment, all GRU layers were removed, retaining only LSTM layers, which resulted in an accuracy of 97.6%Similarly, in the fourth experiment, only LSTM layers were retained while removing GRU layers, leading to an accuracy of 97.5%. Although the models in Experiments 3 and 4 slightly outperformed the original architecture, they lacked the hybrid combination of LSTM and GRU layers, which is crucial for comprehensive feature extraction and understanding. The proposed DL model maintains a balanced architecture, neither excessively deep nor too shallow, and ensures effective feature learning while optimizing computational efficiency for real-time deployment. To improve the interpretability of model predictions, the proposed framework integrates an XAI technique known as LIME. Given that DL models are frequently perceived as &#8220;black boxes&#8221; due to their opaque internal mechanisms, XAI methods such as LIME are employed to provide visual and local explanations of the model&#8217;s decision-making process, thereby enhancing transparency and facilitating the assessment of model reliability and trustworthiness. In this study, four randomly selected URL samples from the test dataset were evaluated using LIME to demonstrate the robustness and effectiveness of the proposed model. Furthermore, the fidelity scores associated with each LIME explanation are reported, reflecting the degree to which the explanation aligns with the model&#8217;s original prediction. Table 7 Ablation study evaluating the contribution of key components in the proposed model. Layer configuration Accuracy % Conv1D LSTM (32) 97.5% GRU (64) LSTM (128) GRU (256) Dense (128) Layer Normalization Dense (64) Dropout Dense (4) Softmax Conv1D LSTM (32) 97.3% GRU (64) Dense (128) Layer Normalization Dense (64) Dropout Dense (4) Softmax Conv1D LSTM (32) 97.6% LSTM (128) Dense (128) Layer Normalization Dense (64) Dropout Dense (4) Softmax Conv1D GRU (64) 97.5% GRU (256) Dense (128) Layer Normalization Dense (64) Dropout Dense (4) Softmax Figure 11 a showcases a benign URL classification with 100% confidence. Words highlighted in blue contributed positively towards benign class classification. The model successfully identifies non-malicious URLs by recognizing common lexical patterns found in legitimate domains, demonstrating its robustness in distinguishing benign URLs from malicious ones. The fidelity score of 0.98 indicates that the LIME explanation closely approximates the original model&#8217;s prediction, reflecting high local faithfulness of the surrogate explanation model. Figure&#160; 11 b visualizes a URL classified as &#8220;phishing&#8221; with a prediction probability of 0.98. Figure&#160; 11 c represents a URL identified as &#8220;malware&#8221; with a prediction probability of 1.0 and a fidelity score of 0.73. The highlighted words (in green) contributed positively toward the malware classification. Despite lower training samples, the model accurately detects malware samples. Figure&#160; 11 d represents a URL classified as &#8220;defacement&#8221; with a prediction probability of 1.0 and a fidelity score of 0.82. The explanation highlights features (in orange) that contributed positively to the classification. At the same time, the fidelity score of 0.82 suggests a reasonably strong alignment between the LIME explanation and the model&#8217;s true decision-making behavior. It is worth mentioning that specific standard tokens, such as &#8220;com&#8221; and &#8220;http&#8221;, &#8220;www&#8221;, appear in all four URL categories, yet the model effectively differentiates between these URLs. This suggests that the classification is not solely based on individual tokens but rather on the contextual relationships among these tokens. The LLM-based embeddings encode the contextual and structural relationships between tokens that allow the model to distinguish URLs based on their overall meaning rather than individual words. Moreover, the use of LSTMs and GRUs in the DL model enables it to capture long-term sequential dependencies and patterns within these tokens. Unlike traditional rule-based approaches, which might flag URLs containing specific keywords, the model understands the overall composition and meaning of a URL by processing embeddings that encode the structural relationship of the URL. Hence, this combination of LLM embeddings and sequential processing enables the model to generalize well and accurately classify these URLs while minimizing reliance on individual token presence. The results highlight the model&#8217;s capability to learn meaningful representations from the training data, making it robust and adaptable to real-world URL classification tasks. A comparative performance analysis with existing systems is presented in Table&#160; 8 . Several LLM-based techniques have been developed to detect malicious URLs. For example, Kaisser et al. 14 used GPT&#8722;3.5 Turbo and GPT&#8722;4 for phishing URL detection, achieving 95.0% accuracy; however, the model was trained on a limited dataset comprising only 1,000 URL samples, which restricts its generalizability. Yu et al. 15 used an M-BERT-based model that attained an accuracy of 94.5% on a custom dataset containing 0.6&#160;M URL samples. Despite a larger training dataset, the performance of the model remains relatively lower. Roy et al. 13 used different LLMs, including MobileBERT, GPT&#8722;2, DistilBERT, Tiny LLaMA, and Bloom, with MobileBERT achieving the highest accuracy of 96.0% for phishing URL detection. Fig. 11 LIME-based interpretable visualizations of BERT model predictions across different URL classes: ( a ) Benign, ( b ) Phishing, ( c ) Malware, and ( d ) Defacement. The highlighted tokens represent the most influential features guiding classification decisions. ( a ) Benign Class, ( b ) Phishing Class, ( c ) Malware Class, ( d ) Defacement Class. On a similar dataset compared to ours, Zaimi et al. 25 proposed a malicious URL detection framework that combines DistilBERT for feature extraction with a CNN-based classifier. The authors utilized a merged dataset 29 , reformulated the task as binary classification, and obtained 98% accuracy. Similarly, Al Saedi et al. 43 employed URL-based, Whois-based, and cyber threat intelligence (CTI) features. Using n-gram and TF&#8211;IDF for feature representation and mutual information for feature selection, the model employed a two-stage ensemble: RF followed by a Multilayer Perceptron (MLP) meta-classifier. The model attained 96.8% overall accuracy. Shetty et al. 44 proposed a lexical analysis-based approach to detect URLs across four categories, i.e., phishing, malware, benign, and malignant, that obtained 97% accuracy using RF. However, the study is limited to lexical features, which may not capture deeper semantic patterns present in complex URL structures. In contrast, the present study employs LLMs to generate rich semantic embeddings, which are subsequently classified using a customized DL framework in a multi-label classification setting to identify URLs across four categories: phishing, malware, defacement, and benign. The proposed framework achieved an accuracy of 98% on binary classification and 97.5% on the multi-class classification task, thus demonstrating a superior performance compared to existing approaches. Table 8 Comparison of proposed URL classification framework with the existing state-of-the-art systems. References Technique Classes Accuracy Roy et al. 13 MobileBERT Phishing, Benign 96.0% Kaisser et al. 14 GPT-3.5-turbo and GPT-4 Malicious, Benign 95.0% Yu et al. 15 M-BERT Benign, Malicious 94.5% Zaimi et al. 25 DistilBERT&#8201;+&#8201;CNN Benign, Malicious 98% Al Saedi et al. 43 TF&#8211;IDF, RF, MLP Benign, Malicious 96.80% Shetty et al. 44 Lexical Analysis, RF Benign, Malware, Defacement, Phishing 97% Proposed BERT&#8201;+&#8201;DL Benign , Malware , Defacement , Phishing 97.5% Benign , Malicious 98% The proposed framework is also evaluated on a set of previously unseen URL samples acquired from online sources to assess its generalization capability. Table&#160; 9 summarizes the evaluation results, including the URL (obfuscated for the safety of readers), a brief description of its context, the actual label, and the model&#8217;s predicted label. The proposed BERT&#8201;+&#8201;DL model accurately identified three out of four samples, with misclassification observed between malware and phishing due to a relatively small number of malware samples in the training distribution compared to other classes. While the evaluation is limited to a test set of one URL sample from each of the four classes, the results demonstrate the framework&#8217;s strong potential to generalize effectively to novel, real-world data with a very high confidence score. Table 9 Evaluation of the proposed framework&#8217;s generalization capability on unseen URL samples. URL Description Actual Label Predicted Label Confidence Score https://www.nationalgeographic.com/travel/topic/best-of-the-world-hub Benign URL 45 0 0 0.98 http://thebestofminneapolis.org This site was defaced in 2020 by Iranian hackers who posted images and messages in protest of the assassination of General Qasem Soleimani 46 . 1 1 0.92 http://init[dot]icloud-diagnostics.com This domain was used as a command and control server for the XcodeGhost malware 47 . 2 3 0.99 https://firebasestorage.googleapis.com/v0/b/owambe-4ce77.appspot.com/o/arsenaldozens/indexcopy2.html?alt=media&amp;token=bbb56e5d-96d2-4da7-a82f-e0bfed8d24c3&amp;email=creader@palaceresorts.com This URL was part of a phishing campaign targeting employees in the travel industry 48 . 3 3 0.99 The proposed framework is also tested against a set of adversarially obfuscated URLs randomly crafted using common evasion techniques from the URLs above. In example (1), leetspeak (replacing &#8217;o&#8217; with &#8217;0&#8217;) and bracketed symbols were used to bypass traditional filters, yet the model correctly classified the URL. Example (2) combined redirection in the query string, domain spoofing, and leetspeak to imitate a legitimate source, and was also accurately identified. Example (3) presented a more complex case with full leetspeak, excessive URL padding, and a phishing structure embedded within a long, trusted-looking subpath; the model successfully predicted this as well. However, in example (4), which used homoglyph characters (such as Cyrillic &#8217;i&#8217; and &#8217;o&#8217;), deceptive subdomain chaining, and a fake top-level domain (TLD), the model misclassified the input. These results, shown in Fig.&#160; 12 , highlight the performance of the proposed model against diverse obfuscation methods. Fig. 12 Evaluation of the proposed model on obfuscated URLs using adversarial techniques. Strengths of the proposed approach The proposed framework uses LLM-based embeddings, which are classified using a customised DL model. Unlike conventional approaches, this study utilizes embeddings generated by pre-trained LLMs, which offer significant advantages as they are extensively trained on vast datasets. Hence, these models can effectively capture structural patterns, token relationships, and contextual cues within URLs, resulting in rich feature representations that enhance classification performance. Moreover, one of the main advantages of this approach is that LLMs do not require retraining, which makes them highly efficient for task-specific applications. This significantly reduces training costs and computational overhead while improving model robustness. As demonstrated in the results, the BERT&#8201;+&#8201;DL model emerged as the most effective model for embedding generation and classification, achieving the highest accuracy of 97.5%. The model produced strong performance, but is also relatively lightweight, with a total of 0.5&#160;M parameters. Hence, it is suitable for deployment in resource-constrained environments. To further enhance transparency and interpretability, the proposed framework integrates an XAI module using LIME that visually analyzes the model predictions to explain the model&#8217;s decision-making process. Since DL models often function as black boxes, users may struggle to understand the rationale behind their predictions. By incorporating XAI, the framework strengthens transparency, making it more reliable and applicable for real-world URL classification tasks. Furthermore, the proposed framework performed exceptionally well on unseen real-time URLs, thus proving its robustness and efficacy. The proposed approach can be extended and integrated into web browsers for real-time URL filtering to block malicious links before they reach the end-users. Limitations of the proposed approach Although the proposed framework utilizes pre-trained LLMs, the embedding generation process still requires a GPU. Therefore, very large LMs with billions of parameters were not explored. Furthermore, due to the lack of a comprehensive dataset, the study relies on a merged dataset composed of multiple existing datasets, with each dataset containing a limited number of URL samples. Hence, the database acquired from different sources was merged to ensure a sufficient number of samples for training and evaluation. Conclusion As cyber threats evolve, traditional URL detection mechanisms struggle due to their reliance on handcrafted features and inability to adapt to emerging attack patterns. To address these issues, this paper uses well-known LLMs to generate high-quality URL embeddings to capture the context. These URLs are then classified using a customised DL model, which was optimized. through extensive ablation experiments. The proposed framework is trained and evaluated on well-known datasets and achieves 97.5% accuracy using the BERT&#8201;+&#8201;DL model. Moreover, the model is lightweight, contains only 0.5&#160;M parameters, and can perform classification within 0.11 ms/sample. Finally, the predictions made by the model are visually interpreted using LIME, a well-known XAI technique. This helps in evaluating the model&#8217;s transparency, trustworthiness, and interpretability in its decision-making process. A comparison of the proposed method with existing systems depicts that the proposed model is not only lightweight but is also accurate in the classification of URLs, thus it is deployable in a real-time scenario. In the future, LLMs with billions of parameters can be explored, particularly by fine-tuning them for this specific task. Moreover, this research can be expanded to manually collected datasets to facilitate more extensive and robust experimentation. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions All authors contributed equally. Funding This research was funded by Taif University, Saudi Arabia, Project No. TU-DSPP-2024-52. This work was funded by the University of Jeddah, Jeddah, Saudi Arabia, under grant No. (UJ-21-ICI-2). Therefore, the authors thank the University of Jeddah for its technical and financial support. Data availability The code is available here: dx.doi.org/10.6084/m9.figshare.29937974. Declarations Competing interests The authors declare no competing interests. References 1. Sun G Li Y Liao D Chang V Service function chain orchestration across multiple domains: A full mesh aggregation approach IEEE Trans. Netw. Serv. Manag 2018 15 1175 1191 10.1109/TNSM.2018.2861717 Sun, G., Li, Y., Liao, D. &amp; Chang, V. Service function chain orchestration across multiple domains: A full mesh aggregation approach. IEEE Trans. Netw. Serv. Manag . 15 , 1175&#8211;1191 (2018). 2. Zou X From hyper-dimensional structures to linear structures: maintaining deduplicated data&#8217;s locality ACM Trans. Storage (TOS) 2022 18 1 28 10.1145/3507921 Zou, X. et al. From hyper-dimensional structures to linear structures: maintaining deduplicated data&#8217;s locality. ACM Trans. Storage (TOS) . 18 , 1&#8211;28 (2022). 3. Liu Y Li W Dong X Ren Z Resilient formation tracking for networked swarm systems under malicious data deception attacks Int. J. Robust. Nonlinear Control 2025 35 2043 2052 10.1002/rnc.7777 Liu, Y., Li, W., Dong, X. &amp; Ren, Z. Resilient formation tracking for networked swarm systems under malicious data deception attacks. Int. J. Robust. Nonlinear Control . 35 , 2043&#8211;2052 (2025). 4. Magazine, C. Cybercrime To Cost The World $10.5 Trillion Annually By 2025. Available at: https://cybersecurityventures.com/hackerpocalypse-cybercrime-report-2016/ . (2025). 5. Zenggang, X. et al. Ndlsc: A new deep learning-based approach to smart contract vulnerability detection. J Signal. Process. Syst. 97 , 1&#8211;20 (2025). 6. Xia W The design of fast and lightweight resemblance detection for efficient post-deduplication delta compression ACM Trans. Storage 2023 19 1 30 10.1145/3584663 Xia, W. et al. The design of fast and lightweight resemblance detection for efficient post-deduplication delta compression. ACM Trans. Storage . 19 , 1&#8211;30 (2023). 7. Jain AK Kaur K Gupta NK Khare A Detecting smishing messages using Bert and advanced Nlp techniques SN Comput. Sci. 2025 6 109 10.1007/s42979-024-03532-7 Jain, A. K., Kaur, K., Gupta, N. K. &amp; Khare, A. Detecting smishing messages using Bert and advanced Nlp techniques. SN Comput. Sci. 6 , 109 (2025). 8. Tang, D. et al. A low-rate Dos attack mitigation scheme based on Port and traffic state in Sdn. IEEE Trans. Comput 74 (2025). 9. Liu, Y., Dong, X., Zio, E. &amp; Cui, Y. Active resilient secure control for heterogeneous swarm systems under malicious cyber-attacks. IEEE Trans. Syst. Man. Cybern Syst. 55 (2025). 10. Zhang J Grabphisher: phishing scams detection in Ethereum via temporally evolving Gnns IEEE Trans. Serv. Comput. 2024 17 3727 3741 10.1109/TSC.2024.3411449 Zhang, J. et al. Grabphisher: phishing scams detection in Ethereum via temporally evolving Gnns. IEEE Trans. Serv. Comput. 17 , 3727&#8211;3741 (2024). 11. Gowdhaman V Dhanapal R Hybrid deep learning-based intrusion detection system for wireless sensor network Int. J. Veh. Inf. Commun. Syst. 2024 9 239 255 Gowdhaman, V. &amp; Dhanapal, R. Hybrid deep learning-based intrusion detection system for wireless sensor network. Int. J. Veh. Inf. Commun. Syst. 9 , 239&#8211;255 (2024). 12. Liu, S. et al. The scales of justitia: A comprehensive survey on safety evaluation of llms. arXiv preprint arXiv:2506.11094 (2025). 13. Roy, S. S., Nilizadeh, S. &amp; Phishlang A lightweight, client-side phishing detection framework using mobilebert for real-time, explainable threat mitigation. arXiv preprint arXiv:2408.05667 (2024). 14. Kaisser, T. &amp; Coste, C. I. SciTePress,. Using chat gpt for malicious web links detection. In Proceedings of the 20th International Con- ference on Web Information Systems and Technologies - Volume 1: WEBIST , 425&#8211;432, DOI: 10.5220/0013069200003825. INSTICC (2024). 15. Yu B Efficient classification of malicious urls: M-bert&#8212;a modified Bert variant for enhanced semantic Understanding Ieee Access. 2024 12 13453 13468 10.1109/ACCESS.2024.3357095 Yu, B. et al. Efficient classification of malicious urls: M-bert&#8212;a modified Bert variant for enhanced semantic Understanding. Ieee Access. 12 , 13453&#8211;13468 (2024). 16. Tang, F., Yu, B., Zhao, S. &amp; Xu, M. Towards fraudulent url classification with large language model based on deep learning. In 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL) , 503&#8211;507 (IEEE, 2023)., 503&#8211;507 (IEEE, 2023). (2023). 17. Su MY Su KL Bert-based approaches to identifying malicious urls Sensors 2023 23 8499 10.3390/s23208499 37896591 PMC10610561 Su, M. Y. &amp; Su, K. L. Bert-based approaches to identifying malicious urls. Sensors 23 , 8499 (2023). 37896591 10.3390/s23208499 PMC10610561 18. Rashid F Ranaweera N Doyle B Seneviratne S Llms are one-shot url classifiers and explainers Comput. Networks 2025 258 111004 10.1016/j.comnet.2024.111004 Rashid, F., Ranaweera, N., Doyle, B. &amp; Seneviratne, S. Llms are one-shot url classifiers and explainers. Comput. Networks . 258 , 111004 (2025). 19. Zhang, Z., Wu, J., Lu, N., Shi, W. &amp; Liu, Z. Adaptpud: an accurate url-based detection approach against tailored deceptive phishing websites. Comput Networks. 111303 (2025). 20. Reyes-Dorta, N. &amp; Caballero-Gil, P. &amp; Rosa-Remedios, C. Detection of malicious urls using machine learning. Wirel Networks. 30 , 1&#8211;18 (2024). 21. Jalil S Usman M Fong A Highly accurate phishing url detection based on machine learning J. Ambient Intell. Humaniz. Comput. 2023 14 9233 9251 10.1007/s12652-022-04426-3 Jalil, S., Usman, M. &amp; Fong, A. Highly accurate phishing url detection based on machine learning. J. Ambient Intell. Humaniz. Comput. 14 , 9233&#8211;9251 (2023). 22. Ariawan, S. et al. IEEE,. Intelligent malicious url detection using kernel pca-svm-ga model with feature analysis. In 2024 International Conference on Data Science and Network Security (ICDSNS) , 1&#8211;6 (2024). 23. Li, L. &amp; Gong, B. Prompting large language models for malicious webpage detection. In 2023 IEEE 4th international conference on pattern recognition and machine learning (PRML) , 393&#8211;400IEEE, (2023). 24. Zhou J An integrated Csppc and Bilstm framework for malicious url detection Sci. Rep. 2025 15 6659 10.1038/s41598-025-91148-z 39994324 PMC11850714 Zhou, J. et al. An integrated Csppc and Bilstm framework for malicious url detection. Sci. Rep. 15 , 6659 (2025). 39994324 10.1038/s41598-025-91148-z PMC11850714 25. Zaimi R Safi Eljil K Hafidi M Lamia M Nait-Abdesselam F An enhanced mechanism for malicious url detection using deep learning and distilbert-based feature extraction J. Supercomput 2025 81 438 10.1007/s11227-024-06908-x Zaimi, R., Safi Eljil, K., Hafidi, M., Lamia, M. &amp; Nait-Abdesselam, F. An enhanced mechanism for malicious url detection using deep learning and distilbert-based feature extraction. J. Supercomput . 81 , 438 (2025). 26. Aljofey, A., Bello, S. A., Lu, J. &amp; Xu, C. Bert-phishfinder: A robust model for accurate phishing url detection with optimized distilbert. IEEE Trans. Dependable Secur. Comput. 22 (2025). 27. Buu, S. J. &amp; Cho, S. B. A transformer network calibrated with fuzzy logic for phishing url detection. Fuzzy Sets Syst. 517 109474 (2025). 28. Tian, Y., Yu, Y., Sun, J. &amp; Wang, Y. From past to present: A survey of malicious url detection techniques, datasets and code repositories. arXiv preprint arXiv:2504.16449 (2025). 29. Malicious URLs dataset &#8212; kaggle.com. https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset . [Accessed 06-01-2025]. 30. URL. | Datasets | Research | Canadian Institute for Cybersecurity | UNB &#8212; unb.ca. (2016). Available at: https://www.unb.ca/cic/datasets/ url-2016.html . (2025). 31. Yasin A Fatima R Khan JA Afzal W Behind the bait: delving into phishtank&#8217;s hidden data Data Brief. 2024 52 109959 10.1016/j.dib.2023.109959 38152492 PMC10751815 Yasin, A., Fatima, R., Khan, J. A. &amp; Afzal, W. Behind the bait: delving into phishtank&#8217;s hidden data. Data Brief. 52 , 109959 (2024). 38152492 10.1016/j.dib.2023.109959 PMC10751815 32. Community Projects -. RiskAnalytics &#8212; riskanalytics.com. https://riskanalytics.com/community/ (2025). [Accessed 23-07-2025]. 33. Marchal S Fran&#231;ois J State R Engel T Phishstorm Detecting phishing with streaming analytics IEEE Trans. Netw. Serv. Manag 2014 11 458 471 10.1109/TNSM.2014.2377295 Marchal, S., Fran&#231;ois, J., State, R., Engel, T. &amp; Phishstorm Detecting phishing with streaming analytics. IEEE Trans. Netw. Serv. Manag . 11 , 458&#8211;471 (2014). 34. Vaswani, A. et al. Attention is all you need. Adv Neural Inform. Process. Systems 30 (2017). 35. Naveed, H. et al. A comprehensive overview of large Language models. ACM Trans. Intell. Syst. Technol 16 (2023). 36. Devlin, J. &amp; Bert Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). 37. Openai-community/gpt2. &#183; Hugging Face &#8212; huggingface.co. Available at:&#160; https://huggingface.co/openai-community/gpt2 . (2024). 38. Zhang, P., Zeng, G., Wang, T. &amp; Lu, W. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 (2024). 39. Raffel C Exploring the limits of transfer learning with a unified text-to-text transformer J. Mach. Learn. Res. 2020 21 1 67 Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21 , 1&#8211;67 (2020).34305477 40. Kibriya H Siddiqa A Khan WZ Khan MK Towards safer online communities: deep learning and explainable Ai for hate speech detection and classification Comput. Electr. Eng. 2024 116 109153 10.1016/j.compeleceng.2024.109153 Kibriya, H., Siddiqa, A., Khan, W. Z. &amp; Khan, M. K. Towards safer online communities: deep learning and explainable Ai for hate speech detection and classification. Comput. Electr. Eng. 116 , 109153 (2024). 41. Fu, R., Zhang, Z. &amp; Li, L. Using lstm and gru neural network methods for traffic flow prediction. In 2016 31st Youth academic annual conference of Chinese association of automation (YAC) , 324&#8211;328IEEE, (2016). 42. Salih AM A perspective on explainable artificial intelligence methods: Shap and lime Adv. Intell. Syst. 2025 7 2400304 10.1002/aisy.202400304 Salih, A. M. et al. A perspective on explainable artificial intelligence methods: Shap and lime. Adv. Intell. Syst. 7 , 2400304 (2025). 43. Alsaedi M Ghaleb FA Saeed F Ahmad J Alasli M Cyber threat intelligence-based malicious url detection model using ensemble learning Sensors 2022 22 3373 10.3390/s22093373 35591061 PMC9101641 Alsaedi, M., Ghaleb, F. A., Saeed, F., Ahmad, J. &amp; Alasli, M. Cyber threat intelligence-based malicious url detection model using ensemble learning. Sensors 22 , 3373 (2022). 35591061 10.3390/s22093373 PMC9101641 44. R, U. S. D. &amp; Patil, A. &amp; Mohana. Malicious url detection and classification analysis using machine learning models. In 2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT) , 470&#8211;476, (2023). 10.1109/IDCIoT56793.2023.10053422 45. Best of the World.&#160;-nationalgeographic.com. Available at: https://www.nationalgeographic.com/travel/topic/ best-of-the-world-hub. (2023). 46. Team, C. Web Defacement Attacks: 5 Website Defacement Examples &#8212; websitesecuritystore.com. https://websitesecuritystore.com/blog/website-defacement-attacks-examples . [Accessed 10-05-2025]. 47. XcodeGhost - Wikipedia. &#8212; en.wikipedia.org. Available at:&#160; https://en.wikipedia.org/wiki/XcodeGhost . (2025). 48. Anna Chung, S. B. Phishing Eager Travelers &#8212; unit42.paloaltonetworks.com. Available at:&#160;https://unit42.paloaltonetworks.com/ travel-themed-phishing/ . (2025)."
}