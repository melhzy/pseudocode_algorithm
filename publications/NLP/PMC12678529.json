{
  "pmcid": "PMC12678529",
  "source": "PMC",
  "download_date": "2025-12-09T16:37:19.654138",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12678529",
    "pmid": "41331299",
    "doi": "10.1038/s41598-025-17581-2",
    "title": "A novel liver image classification network for accurate diagnosis of liver diseases",
    "year": "2025",
    "month": "12",
    "day": "2",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "2"
    },
    "authors": [
      "He Xiaolei",
      "Wang Xilong",
      "Wang Yan",
      "Li Hui",
      "Liu Shuo",
      "Wang Jun",
      "Feng Yan",
      "Wang Qi",
      "Chen Jie"
    ],
    "abstract": "In medical imaging diagnosis, the identification of normal liver, fatty liver, and cirrhosis is often challenging due to subtle morphological and density differences. Previous studies have used CNNs, MLPs or transformers to extract lesion features. However, CNN’s global representation is limited, while MLPs and transformers have insufficient local modeling, resulting in insufficient lesion information mining. Therefore, this article proposes a hybrid network CMT-Net, which unifies the local perception of CNNs, high-dimensional mapping of MLPs, and global dependencies of Transformers into a single architecture, significantly improving the accuracy of CT liver three classification. The core components of CMT-Net include an efficient transformer (ET) module, which focuses on extracting local feature details while progressively integrating global information, significantly enhancing the model feature representation and generalization capabilities. Additionally, this paper introduces a Hybrid MLP (HM) module that combines Token-Mixing MLP and Channel-Mixing MLP to achieve deep fusion of spatial and channel information, further improving feature extraction. To validate the proposed algorithm, extensive experiments were conducted on a dataset of three liver diseases collected from the Imaging Department of Urumqi People’s Hospital. The results demonstrate that CMT-Net achieves outstanding performance in classifying normal liver, fatty liver, and cirrhosis. These findings not only provide an effective tool for precise liver disease diagnosis but also offer new directions for deep learning model design in medical image classification tasks.",
    "keywords": [
      "Computer science",
      "Information technology"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12678529</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12678529.1</article-id><article-id pub-id-type=\"pmcaid\">12678529</article-id><article-id pub-id-type=\"pmcaiid\">12678529</article-id><article-id pub-id-type=\"pmid\">41331299</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-17581-2</article-id><article-id pub-id-type=\"publisher-id\">17581</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A novel liver image classification network for accurate diagnosis of liver diseases</article-title></title-group><contrib-group><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>He</surname><given-names initials=\"X\">Xiaolei</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"X\">Xilong</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"Y\">Yan</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"H\">Hui</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"S\">Shuo</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"J\">Jun</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Feng</surname><given-names initials=\"Y\">Yan</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"Q\">Qi</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"J\">Jie</given-names></name><address><email>lamal2000@163.com</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02r247g67</institution-id><institution-id institution-id-type=\"GRID\">grid.410644.3</institution-id><institution>Radiographic Image Center, </institution><institution>People&#8217;s Hospital of Xinjiang Uygur Autonomous Region, </institution></institution-wrap>&#220;r&#252;mqi, Xinjiang 830001 China </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01p455v08</institution-id><institution-id institution-id-type=\"GRID\">grid.13394.3c</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1799 3993</institution-id><institution>Graduate School, </institution><institution>Xinjiang Medical University, </institution></institution-wrap>&#220;r&#252;mqi, 830017 Xinjiang China </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01p455v08</institution-id><institution-id institution-id-type=\"GRID\">grid.13394.3c</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1799 3993</institution-id><institution>PCR Biology Laboratory, </institution><institution>Xinjiang Medical University, </institution></institution-wrap>&#220;r&#252;mqi, 830000 Xinjiang China </aff></contrib-group><pub-date pub-type=\"epub\"><day>2</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>43121</elocation-id><history><date date-type=\"received\"><day>18</day><month>6</month><year>2025</year></date><date date-type=\"accepted\"><day>25</day><month>8</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>02</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>06</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-06 00:25:12.373\"><day>06</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_17581.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">In medical imaging diagnosis, the identification of normal liver, fatty liver, and cirrhosis is often challenging due to subtle morphological and density differences. Previous studies have used CNNs, MLPs or transformers to extract lesion features. However, CNN&#8217;s global representation is limited, while MLPs and transformers have insufficient local modeling, resulting in insufficient lesion information mining. Therefore, this article proposes a hybrid network CMT-Net, which unifies the local perception of CNNs, high-dimensional mapping of MLPs, and global dependencies of Transformers into a single architecture, significantly improving the accuracy of CT liver three classification. The core components of CMT-Net include an efficient transformer (ET) module, which focuses on extracting local feature details while progressively integrating global information, significantly enhancing the model feature representation and generalization capabilities. Additionally, this paper introduces a Hybrid MLP (HM) module that combines Token-Mixing MLP and Channel-Mixing MLP to achieve deep fusion of spatial and channel information, further improving feature extraction. To validate the proposed algorithm, extensive experiments were conducted on a dataset of three liver diseases collected from the Imaging Department of Urumqi People&#8217;s Hospital. The results demonstrate that CMT-Net achieves outstanding performance in classifying normal liver, fatty liver, and cirrhosis. These findings not only provide an effective tool for precise liver disease diagnosis but also offer new directions for deep learning model design in medical image classification tasks.</p></abstract><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Information technology</kwd></kwd-group><funding-group><award-group><funding-source><institution>The Hospital Level Project 20230118 of Xinjiang Uygur Autonomous Region People&#8217;s Hospital</institution></funding-source><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id><award-id>20230118</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">Liver diseases are among the leading global health concerns, with fatty liver disease and cirrhosis being two common chronic liver conditions &#160;<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. Fatty liver disease results from excessive fat accumulation in the liver, while cirrhosis is the outcome of long-term liver damage leading to fibrosis and nodular regeneration &#160;<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. Normal liver, fatty liver, and cirrhosis represent different stages of liver disease progression, with fatty liver serving as a critical intermediate stage between normal liver and cirrhosis&#160;<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>. Understanding their interrelationships is essential for prevention, diagnosis, and treatment.</p><p id=\"Par3\">In early-stage fatty liver, imaging modalities such as ultrasound and CT may reveal only minor changes in liver morphology and density, making differentiation from normal liver difficult&#160;<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. Due to limited experience or insufficient imaging resolution, clinicians may misdiagnose early fatty liver as normal, delaying timely intervention. Similarly, early cirrhosis may exhibit subtle density variations in CT images, resembling mild fatty liver, leading to potential misclassification&#160;<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>.</p><p id=\"Par4\">Recent advances in deep learning have significantly enhanced the role of medical image classification algorithms in liver disease diagnosis. Notably, CNNs have demonstrated remarkable advantages in liver disease classification&#160;<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. First, CNNs can automatically extract lesion features from images without relying on handcrafted feature extraction methods. Through localized convolutional operations, CNNs effectively capture fine-grained details in CT images, such as uniform texture in normal liver, density variations in fatty liver, and nodular patterns in cirrhosis&#160;<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. Second, CNN-based models can learn complex nonlinear mappings through training, enabling high-accuracy classification. Furthermore, transfer learning and data augmentation techniques mitigate the challenge of limited medical image data&#160;<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>, allowing deep learning models to perform well even with small datasets. Addressing the issues of effective deep features in liver cancer CT images within CNNs but limited global generalization ability, and fast training speed of Kernel Extreme Learning Machines (KELM) but sensitive kernel function selection, Jesi et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup> proposed an end-to-end &#8220;Differential CNN-KELM&#8221; framework for liver cancer recognition. This algorithm combines the advantages of CNNs and KELM to enhance recognition accuracy and training efficiency. Addressing the issue of liver fibrosis, Abinaya et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup> developed a BiLSTM-CNN hybrid network. This network adopts a serial strategy of &#8220;CNN capturing local features and BiLSTM capturing long-range dependencies&#8221;, achieving an accuracy rate of over 96% in fibrosis staging within a lightweight parameter budget. It provides an efficient and practical deep learning backbone for rapid and non-invasive clinical assessment. Although CNNs have achieved remarkable results in liver disease recognition, their inherent limitations cannot be ignored. Limited by their local receptive fields, CNNs struggle to effectively capture lesion features across different levels or global scope, leading to missed diagnoses of diffuse or small metastatic lesions.</p><p id=\"Par5\">In recent years, transformer have achieved great success in natural language processing (NLP), have been increasingly applied to computer vision and medical image analysis &#160;<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>. The self-attention mechanism in transformers enables efficient modeling of global image information, making them particularly suitable for medical image classification &#160;<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref>,<xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>. In liver disease imaging, transformer can simultaneously analyze overall liver morphology, spatial relationships between lesions and surrounding tissues, and long-range dependencies, improving classification accuracy for complex cases. Additionally, transformer offer architectural flexibility, facilitating extensions and improvements (e.g., TransUNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, Swin Transformer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>). Addressing the two major issues of insufficient global context in traditional U-Net and excessively large number of parameters in pure Transformer, Ou et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup> proposed a lightweight CNN-Transformer hybrid network to achieve precise and end-to-end automatic segmentation of liver and tumors in CT images. Addressing the issues of pure CNNs or pure Vision Transformers (ViTs), such as difficulty in balancing local and global features, sensitivity to hyperparameters, and susceptibility to overfitting, Kumar et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup> utilized the Grey Wolf Optimization (GWO) algorithm to perform end-to-end tuning on the Swin Transformer-UNet hybrid network, providing a fully automated liver CT segmentation solution that can be deployed immediately for clinical use. Although Transformers have demonstrated excellent global modeling capabilities in liver disease classification, certain issues still persist. Firstly, the self-attention mechanism lacks sufficient focus on fine-grained textures in small liver lesions and low-contrast areas, which can easily lead to missed detection of early or small-volume liver lesions. Secondly, pure Transformers have a large number of parameters, making it difficult to run them in real-time on conventional Picture Archiving and Communication System(PACS) or edge devices&#160;<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Visualization of network parameters and accuracy.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig1_HTML.jpg\"/></fig></p><p id=\"Par6\">Moreover, the emergence of MLP-Mixer, a pure multilayer perceptron (MLP)-based architecture, has demonstrated competitive performance in image classification benchmarks, reigniting interest in MLPs &#160;<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup>. Like transformer, MLPs possess global modeling capabilities absent in CNNs, enabling holistic feature extraction from medical images&#160;<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>. MLPs also benefit from positional priors, allowing them to leverage spatial information for tasks such as identifying irregular nodule distributions in cirrhosis. CNNs and ViTs primarily focus on improving segmentation accuracy, often neglecting issues such as segmentation speed. Yang et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> achieved end-to-end precise segmentation of CT liver tumors using a &#8220;lightweight Tokenized-MLP + dual attention&#8221; approach. In summary, although MLP has advantages in global modeling and positional prior knowledge in liver disease classification, it is still constrained by key bottlenecks such as poor geometric robustness, large parameter count, sensitivity to noise, overfitting in small sample sizes, and insufficient interpretability.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Visualization of network FLOPs and accuracy.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig2_HTML.jpg\"/></fig></p><p id=\"Par7\">To address these limitations and leverage the imaging characteristics of normal liver, fatty liver, and cirrhosis in CT, we propose CMT-Net, a hybrid medical image classification algorithm integrating CNN, MLP, and transformer modules. CMT-Net employs a hierarchical single-branch serial design, incorporating a Hybrid MLP (HM) module and an Efficient Transformer (ET) module. The ET module uses Grouping Cascade Attention (GCA) to partition feature maps into groups, focusing on local details while progressively integrating global information. The HM module combines Token-Mixing MLP and Channel-Mixing MLP to fuse spatial and channel information, enhancing feature representation. In the shallow stages (Stages 1&#8211;3), CNN and ET modules extract features, while the intermediate stage (Stage 4) employs the HM module. Deeper stages (Stages 5&#8211;6) reuse CNN and ET modules to balance local and global feature extraction. Extensive experiments on three liver datasets demonstrate that CMT-Net outperforms existing CNN, ViT, and hybrid-based methods in classification accuracy while maintaining low computational costs (Figs. <xref rid=\"Fig1\" ref-type=\"fig\">1</xref>, <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>). In summary, our contributions are as follows: <list list-type=\"order\"><list-item><p id=\"Par8\">We propose an Efficient Transformer (ET) module with a Grouping Cascade Attention (GCA) mechanism that partitions feature maps into groups and processes them in a cascaded manner, enhancing feature representation and generalization.</p></list-item><list-item><p id=\"Par9\">We design a Hybrid MLP (HM) module that integrates Token-Mixing MLP and Channel-Mixing MLP to fuse spatial and channel information effectively.</p></list-item><list-item><p id=\"Par10\">We introduce CMT-Net, a hybrid medical image classification network combining CNN, MLP, and transformer in a single-branch multi-stage architecture. Experiments on three liver datasets show that CMT-Net achieves state-of-the-art classification performance with fewer parameters and FLOPs.</p></list-item></list></p></sec><sec id=\"Sec2\"><title>Related work</title><p id=\"Par11\">Through an in-depth review of existing literature and technological advancements, we observe that while CNNs&#160;<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, transformers, and multilayer perceptrons (MLPs) have been widely applied in medical image processing, research on algorithms for CT-based liver disease classification remains relatively limited. To advance this field, we systematically analyze existing deep learning-based medical image classification methods, aiming to provide more effective solutions for precise liver disease diagnosis.</p><sec id=\"Sec3\"><title>CNN-based methods</title><p id=\"Par12\">Since the widespread adoption of AlexNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>, VGG&#160;<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>, GoogleNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>, ResNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>, and DenseNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup> in natural image processing, these milestone achievements have also provided robust technical support for medical image analysis. LitefusionNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup> is a lightweight feature fusion network designed to enhance medical image classification performance through intelligent feature fusion. By combining the strengths of MobileNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup> and MobileNetV2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup> architectures, it improves feature discriminability and classification accuracy across diverse medical image datasets. Zhu et al. &#160;<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup> proposed an improved ResNet algorithm with a dual-attention mechanism (channel and spatial attention), significantly boosting classification performance while reducing overfitting in abdominal CT image quality assessment. Maglogiannis et al. &#160;<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>, introduced a healthcare framework for breast cancer diagnosis that fuses deep learning architectures with optimized algorithms, achieving higher diagnostic accuracy by combining medical imaging and clinical data. CI-Net &#160;<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup> mimics dermatologists&#8217; diagnostic workflows through three modules: a Lesion Area Attention Module to focus on central regions, a Feature Extraction Module, and a Lesion Feature Attention Module to correlate critical local features. This design significantly improves skin lesion recognition accuracy, outperforming existing methods on benchmark datasets. To alleviate the problems of small sample overfitting and scale mismatch, Gedeon et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup> proposed a lightweight LivlesioNet. This network uses parallel 3 <inline-formula id=\"IEq1\"><tex-math id=\"d33e421\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 3, 5 <inline-formula id=\"IEq2\"><tex-math id=\"d33e425\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 5, and 7 <inline-formula id=\"IEq3\"><tex-math id=\"d33e429\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 7 dynamic convolutions to capture multi-scale lesion features, and uses lightweight attention gating to refine fusion, achieving efficient four classification of liver cancer, hemangioma, metastatic lesions, and normal liver. Manjula et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup> proposed an end-to-end CT liver tumor detection framework. On the one hand, ResUNet is used to segment tumor regions, and on the other hand, the segmentation results are input into a random forest classifier along with radiomics features to achieve precise localization and benign/malignant discrimination of lesions.</p><p id=\"Par13\">Although CNNs have made significant breakthroughs in the field of liver disease recognition, their application still faces several key challenges. Firstly, CNNs are prone to insufficient generalization performance in small sample data scenarios. Secondly, although convolutional kernels can effectively extract local features, they are difficult to establish global semantic associations; On the other hand, fixed scale convolution operations limit the adaptability of the model to multi-scale lesions, resulting in significant differences in its sensitivity to identifying lesions of different sizes.</p></sec><sec id=\"Sec4\"><title>Transformer-based methods</title><p id=\"Par14\">The ViTs demonstrated exceptional performance in natural image classification by capturing global context via self-attention mechanisms&#8211;a capability lacking in CNNs local receptive fields. This advantage has spurred interest in transformer-based medical image classification. For instance, DEF-SwinE2NET &#160;<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup> integrates swin transformer (global features) and efficientNet-B0 (local features) for brain tumor classification, employing feature fusion to enhance discriminative power.</p><p id=\"Par15\">In retinal disease classification, proposed a lightweight model using OCT images, where adaptive pooling and attention mechanisms optimize both accuracy and clinical applicability&#160;<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>. TransNetV&#160;<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup> hybridizes CNNs and transformers for colorectal cancer classification, leveraging CNN-extracted local features and transformer-based global context understanding. Tahir et al. &#160;<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup> introduced adaptive convolution (dynamic kernel adjustment) and dynamic attention mechanisms in a convolution-transformer network, achieving state-of-the-art performance in histopathology image classification. To solve the problem of blurred edges and over segmentation of small tumors in CT liver tumors, Cheng et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup> proposed EG-UNETR. Firstly, cross layer skip connections inject multi head attention to capture multi-scale context and denoise it. Secondly, parallel dilated convolution and mixed attention enhance semantics. Finally, edge guided attention is used to refine contours and achieve sub-pixel level segmentation. Zhao et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup> addressed the challenges of significant differences in the scale and class imbalance of liver lesions, limited receptive field of pure CNN, and excessive number of pure ViT parameters. They used lightweight CNN to capture local textures and micro ViT to obtain global context, and combined the two to achieve SOTA accuracy and interpretability in small sample CT five classification tasks.</p><p id=\"Par16\">Although Transformer has outstanding global modeling capabilities, it still has limitations in liver disease classification. Firstly, Transformer lacks a variable receptive field, making it difficult to adaptively capture multi-scale features of liver lesions. Secondly, the fixed patch mechanism of Transformer is prone to losing sub-pixel level small tumor details.</p></sec><sec id=\"Sec5\"><title>MLP-based methods</title><p id=\"Par17\">MLP-Mixer pioneered an all-MLP architecture for vision tasks&#160;<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>, replacing convolutions and attention with two MLP types: Channel-Mixing MLPs and Token-Mixing MLPs. This design reduces computational costs while maintaining performance on large-scale datasets. ResMLP &#160;<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup> further simplified image classification by independently processing patch interactions across channels and proposed &#8220;Class-MLP&#8221; to replace average pooling.</p><p id=\"Par18\">The gMLP architecture eliminated self-attention &#160;<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref></sup>, using gated MLPs with parameterized spatial-channel projections instead. It scales comparably to transformers with increasing data and compute resources. Li et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup> achieved hierarchical multi-scale representations via lightweight convolutional MLPs, while S2-MLP &#160;<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup> replaced token-mixing MLPs with spatial-shift operations, improving efficiency without sacrificing accuracy. Wang et al.&#160;<sup><xref ref-type=\"bibr\" rid=\"CR46\">46</xref></sup> proposed a lightweight full MLP architecture MA Net for real-time segmentation of 2D ultrasound liver cancer ablation areas. Firstly, MA Net divides the image into multi-level tokens. Then, the cascaded MLP Mixer captures local global features in the image. Finally, channel space lightweight attention suppresses artifacts and sharpens ablation boundaries.</p><p id=\"Par19\">Although MLP has good scalability in visual tasks due to its fully connected structure, liver disease classification still exposes its deep shortcomings. Firstly, the lack of local receptive fields in MLP results in limited ability to capture the subtle structures of sub-pixel lesions. Secondly, the static weights mixed with tokens are difficult to adapt to the multi-scale changes of lesions. Finally, the efficiency of hierarchical feature fusion is low, and the overall classification accuracy is limited by structural bottlenecks.</p></sec></sec><sec id=\"Sec6\"><title>Methodology</title><p id=\"Par20\">This section details the structure and functionality of each module in CMT-Net. Section 3.1 outlines the overall architecture, followed by explanations of the Efficient Transformer (ET) and Hybrid MLP (HM) modules in Sections 3.2 and 3.3, respectively. Section 3.4 introduces the loss function.</p><sec id=\"Sec7\"><title>Overall architecture</title><p id=\"Par22\">Figure <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> illustrates the CMT-Net architecture, comprising four key modules: Feature Mining Block (FMB), Efficient Transformer (ET), Hybrid MLP (HM), and Multi-scale Bottleneck Convolution (MBC).</p><p id=\"Par23\">The input image resolution is set to <inline-formula id=\"IEq4\"><tex-math id=\"d33e516\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_{in} \\in R^{3\\times 224\\times 224}$$\\end{document}</tex-math></inline-formula>. Given the high proportion of background information in raw medical images, Stage 1(Stem) employs a <inline-formula id=\"IEq5\"><tex-math id=\"d33e520\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3\\times 3$$\\end{document}</tex-math></inline-formula> convolutional kernel for preliminary feature extraction. This design efficiently suppresses background interference while preserving lesion characteristics.</p><p>\n<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e526\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X_1 = Conv2d\\left( X_{i n}\\right) \\quad X_1 \\in R^{24 \\times 112 \\times 112} ; Conv2d \\in \\mathbb {R}^{(in=3 ,out =24, kernel =3, stride =2, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>Here, <inline-formula id=\"IEq6\"><tex-math id=\"d33e533\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X_1 \\in R^{24 \\times 112 \\times 112}$$\\end{document}</tex-math></inline-formula> represents the feature map output by Stage1 (Stem). <inline-formula id=\"IEq7\"><tex-math id=\"d33e537\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Conv2d \\in \\mathbb {R}^{(in=3,out =24, kernel =3, stride =2, padding =1)}$$\\end{document}</tex-math></inline-formula> represents the specific parameters of the convolution kernel.</p><p id=\"Par24\">In Stage 2, the FMB module further analyzes features using sequential <inline-formula id=\"IEq8\"><tex-math id=\"d33e543\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3\\times 3$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq9\"><tex-math id=\"d33e547\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1\\times 1$$\\end{document}</tex-math></inline-formula> convolutions, coupled with the SiLU activation function. SiLU optimizes gradient propagation during training and retains partial negative inputs, enhancing feature discriminability.</p><p>\n<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e553\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{2 \\_1}=B N\\left( Conv2d\\left( X_1\\right) \\right) \\quad X_{2 \\_1} \\in R^{64 \\times 112 \\times 112}; Conv2d \\in \\mathbb {R}^{(in =24, out =64, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>\n<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e560\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{2 \\_2} = SiLU\\left( X_{2 \\_1}\\right) \\quad X_{2 \\_2} \\in R^{64 \\times 112 \\times 112} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>\n<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e567\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{2}=B N\\left( Conv2d\\left( X_{2 \\_2}\\right) \\right) \\quad X_{2} \\in R^{64 \\times 112 \\times 112}; Conv2d \\in \\mathbb {R}^{(in =64, out =64, kernel =1, stride =1, padding =0)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>Stage 3 integrates the proposed ET module, which combines local feature extraction with progressive global information fusion (Section 3.2). This significantly improves feature representation and model generalization.</p><p>\n<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e575\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X_3=E T\\left( X_2\\right) \\quad X_3 \\in R^{128 \\times 56 \\times 56} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>Stage 4 introduces the HM module (Section 3.3), where Token-Mixing MLP and Channel-Mixing MLP synergistically fuse spatial and channel information, refining feature extraction.</p><p>\n<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e584\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X_4=HM\\left( X_4\\right) \\quad X_4 \\in R^{256 \\times 28 \\times 28} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>In Stage 5, the MBC module leverages multi-scale convolutions to enhance lesion representation. As network depth increases, lesion features dominate the feature maps. MBC hierarchical convolutions capture fine-grained details while enriching feature diversity.</p><p>\n<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e592\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{5 \\_1}=SiLU\\left( Conv2d\\left( X_4\\right) \\right) \\quad X_{5 \\_1} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =256, out =512, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>\n<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e599\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{5 \\_2}=Sigmoid\\left( Conv2d\\left( X_{5 \\_1}\\right) \\right) \\quad X_{5 \\_2} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =512, out =512, kernel =1, stride =1, padding =0)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>\n<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e606\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; X_{5}=BN\\left( Conv2d\\left( X_{5 \\_2}\\right) \\right) \\quad X_{5 \\_2} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =512, out =512, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>Finally, Stage 6 reapplies the ET module to consolidate local-global feature interactions, boosting classification accuracy.</p><p>\n<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e614\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X_6=E T\\left( X_5\\right) \\quad X_6 \\in R^{1024 \\times 14 \\times 14} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par21\">\n<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Example of the overall network architecture of CMT-Net. CMT-Net includes FMB, ET, HM, and MBC modules.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig3_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec8\"><title>Efficient transformer (ET) module</title><p id=\"Par26\">As shown in Fig. <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>, the ET module mainly uses patch embedding (composed of convolution kernels), feed forward network (composed of convolution kernels) and GCA module to extract local and global information. First, we introduce the overall workflow of ET module. Next, we will introduce the specific workflow of GCA module in detail. Assume that the size of the feature map input to the ET module is <inline-formula id=\"IEq10\"><tex-math id=\"d33e636\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{in} \\in R^{C\\times H\\times W}.$$\\end{document}</tex-math></inline-formula></p><p id=\"Par25\">\n<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Flowchart of the ET module. Here, the Patch Embedding and Feed Forward Network utilize convolutional kernels for local feature extraction, while the GCA module is primarily responsible for global information extraction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig4_HTML.jpg\"/></fig>\n</p><p id=\"Par27\">First, the Patch Embedding module extracts the local details in the input feature map through convolution operation.</p><p><disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e652\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_{1}=B N\\left( Conv2d\\left( F_{in}\\right) \\right) \\quad F_{1} \\in R^{C \\times H \\times W} ; Conv2d \\in \\mathbb {R}^{(in =C, out = C, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Then, the Feed Forward Network maps the number of channels of the feature graph to the target dimension through convolution operation.</p><p><disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e658\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_{2}=B N\\left( Conv2d\\left( F_{1}\\right) \\right) \\quad F_{2} \\in R^{2C \\times (H/2) \\times (W/2)} ; Conv2d \\in \\mathbb {R}^{(in =C, out = 2C, kernel =1, stride =2, padding =0)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Next, the feature map is input into the GCA module to extract the global information.<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e663\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_3=GCA\\left( F_2\\right) \\quad F_3 \\in R^{2C \\times (H/2) \\times (W/2)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Finally, the feature map is extracted by Patch Embedding and Feed Forward Network modules respectively.<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e668\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_{out} = B N\\left( Conv2d_{-} 2\\left( B N\\left( Conv2 d_{-} 1\\left( F_3\\right) \\right) \\right) \\right) \\quad F_{out} \\in R^{2C \\times (H/2) \\times (W/2)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Among them, <inline-formula id=\"IEq11\"><tex-math id=\"d33e673\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Conv2d_{-} 1 \\in \\mathbb {R}^{(in =2C, out = 2C, kernel =3, stride =1, padding =1)}$$\\end{document}</tex-math></inline-formula> is the convolution kernel in Patch Embedding. <inline-formula id=\"IEq12\"><tex-math id=\"d33e677\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Conv2d_{-} 2 \\in \\mathbb {R}^{(in =2C, out = 2C, kernel =1, stride =1, padding =0)}$$\\end{document}</tex-math></inline-formula> is the convolution kernel in the Feed Forward Network.</p><p id=\"Par28\">Next, we will explain the specific workflow of GCA in ET module in detail. To facilitate readers&#8217; understanding, we assume that the size of the feature map input into GCA is <inline-formula id=\"IEq13\"><tex-math id=\"d33e683\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{in} \\in R^{C\\times H\\times W}$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par29\">First, the input feature map is evenly divided into 4 parts along the channel dimension.<disp-formula id=\"Equ15\"><label>15</label><tex-math id=\"d33e689\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Head^i = Split(T_{in}) \\quad 1\\le i \\le 4; \\ Head^i \\in R^{(C/4)\\times H\\times W} \\end{aligned}$$\\end{document}</tex-math></disp-formula>To reduce the computational complexity in the subsequent weight coefficient solving process, we ingeniously perform regional division on the feature map in the spatial dimension.<disp-formula id=\"Equ16\"><label>16</label><tex-math id=\"d33e694\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Head^i_j = Split(Head^i) \\quad 1\\le i \\le 4; 1\\le j \\le (H/2); Head^i_j \\in R^{(C/4)\\times j \\times j} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Here, the spatial dimension <inline-formula id=\"IEq14\"><tex-math id=\"d33e699\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j\\times j$$\\end{document}</tex-math></inline-formula> of each divided feature map block is denoted as <inline-formula id=\"IEq15\"><tex-math id=\"d33e703\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Head^i_j$$\\end{document}</tex-math></inline-formula>. Next, each feature map block is individually fed into the self-attention mechanism to compute the pixel-wise correlations. The specific formula is as follows:<disp-formula id=\"Equ17\"><label>17</label><tex-math id=\"d33e707\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; Q^i_j = Reshape(Conv2d(Head^i_j)) \\quad Q^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1}; Conv2d \\in \\mathbb {R}^{(in =(C/4), out = (C/4), kernel =5, stride =1, padding =2)} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ18\"><label>18</label><tex-math id=\"d33e711\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; K^i_j = Reshape(Head^i_j) \\quad K^i_j \\in R^{(C/4)\\times 1\\times (j\\times j)} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ19\"><label>19</label><tex-math id=\"d33e716\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; V^i_j = Reshape(Head^i_j) \\quad V^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ20\"><label>20</label><tex-math id=\"d33e720\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; Head^i_j = Self - Att\\{Q^i_j, K^i_j, V^i_j\\} \\quad Head^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ21\"><label>21</label><tex-math id=\"d33e724\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; Head^i_j = Reshape(Head^i_j) \\quad Head^i_j \\in R^{(C/4)\\times j\\times j} \\end{aligned}$$\\end{document}</tex-math></disp-formula>The output feature map blocks <inline-formula id=\"IEq16\"><tex-math id=\"d33e729\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Head^i_j$$\\end{document}</tex-math></inline-formula> are then spatially concatenated to reconstruct a complete feature map <inline-formula id=\"IEq17\"><tex-math id=\"d33e733\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Head^i$$\\end{document}</tex-math></inline-formula> that incorporates global information.<disp-formula id=\"Equ22\"><label>22</label><tex-math id=\"d33e737\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Head^i = Concat(Head^i_j) \\quad Head^i \\in R^{(C/4)\\times H\\times W} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Finally, <inline-formula id=\"IEq18\"><tex-math id=\"d33e742\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Head^i$$\\end{document}</tex-math></inline-formula> are concatenated along the channel dimension to produce the final feature map.<disp-formula id=\"Equ23\"><label>23</label><tex-math id=\"d33e747\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} T_{Output} = Concat(Head^i) \\quad T_{Output} \\in \\mathbb {R}^{C\\times H\\times W} \\end{aligned}$$\\end{document}</tex-math></disp-formula>In summary, we propose an efficient feature processing module. First, the input feature map is evenly split into four parts along the channel dimension. To reduce computational complexity, we perform spatial partitioning on the feature maps. Each partitioned block is then fed into a self-attention mechanism to compute pixel-wise correlations accurately. The output blocks are spatially reassembled to form a complete feature map integrating global information. Finally, these maps are concatenated along the channel dimension to generate the final output. This pipeline not only optimizes computational efficiency but also enhances feature representation by focusing on local details and progressively integrating global context.</p></sec><sec id=\"Sec9\"><title>Hybrid MLP (HM) module</title><p id=\"Par31\">As shown in Fig. <xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, we illustrate the detailed working of the HM module. Suppose the size of the output feature is <inline-formula id=\"IEq19\"><tex-math id=\"d33e759\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_{in} \\in R^{C\\times H\\times W}$$\\end{document}</tex-math></inline-formula>. First, the input feature map passes through a convolutional layer to extract spatial features. Here, the convolution operation does not change the shape of the feature map, and the output feature map still maintains the shape of <inline-formula id=\"IEq20\"><tex-math id=\"d33e763\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_1 \\in R^{C\\times H\\times W}.$$\\end{document}</tex-math></inline-formula></p><p>\n<disp-formula id=\"Equ24\"><label>24</label><tex-math id=\"d33e768\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Q_1 = Conv2d\\left( Q_{i n}\\right) \\quad Q_1 \\in R^{C\\times H \\times W} ; Conv2d \\in \\mathbb {R}^{(in=C ,out =C, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p>The convolved feature map then undergoes a transpose operation, transforming its shape from <inline-formula id=\"IEq21\"><tex-math id=\"d33e775\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_1$$\\end{document}</tex-math></inline-formula> to <inline-formula id=\"IEq22\"><tex-math id=\"d33e779\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_2 \\in R^{(H \\times W) \\times C}.$$\\end{document}</tex-math></inline-formula></p><p>\n<disp-formula id=\"Equ25\"><label>25</label><tex-math id=\"d33e784\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Q_2 = Transpose(Q_1) \\quad Q_2 \\in R^{(H\\times W)\\times C } \\end{aligned}$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par30\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Flowchart of the HM module, which mainly consists of Channel Mixing MLP and Token Mixing MLP.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig5_HTML.jpg\"/></fig>\n</p><p>After the transpose, the features of each channel are integrated into a vector, providing a basis for the Channel Mixing MLP to perform feature interaction and mixing between channels.</p><p id=\"Par32\">Next, the transposed feature map is fed into the Channel Mixing MLP. Assuming the weights of the fully connected layer are <inline-formula id=\"IEq23\"><tex-math id=\"d33e802\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{fc}$$\\end{document}</tex-math></inline-formula> and the bias is <inline-formula id=\"IEq24\"><tex-math id=\"d33e806\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$b_{fc}$$\\end{document}</tex-math></inline-formula>, the output of the Channel Mixing MLP can be expressed as:<disp-formula id=\"Equ26\"><label>26</label><tex-math id=\"d33e810\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Q_{fc} = \\sigma (W_{fc} \\cdot Q_2 + b_{fc}) \\quad Q_{fc} \\in R^{(H\\times W)\\times C }; W_{fc} \\in R^{C\\times C}; b_{fc} \\in R^{C\\times 1} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Where <inline-formula id=\"IEq25\"><tex-math id=\"d33e815\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma$$\\end{document}</tex-math></inline-formula> represents the ReLU activation function. We apply the ReLU activation to introduce non-linearity, enabling the model to learn more complex feature representations.</p><p id=\"Par33\">In summary, the computational formula for the Channel Mixing MLP can be expressed as:<disp-formula id=\"Equ27\"><label>27</label><tex-math id=\"d33e821\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Q_{out} = \\sigma (W_{fc} \\cdot (Transpose(Conv2d(Q_{in}))) + b_{fc}) \\quad Q_{out} \\in R^{(H\\times W) \\times C } \\end{aligned}$$\\end{document}</tex-math></disp-formula>Where <inline-formula id=\"IEq26\"><tex-math id=\"d33e826\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_{in}$$\\end{document}</tex-math></inline-formula> denotes the input feature map, <inline-formula id=\"IEq27\"><tex-math id=\"d33e830\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q_{out}$$\\end{document}</tex-math></inline-formula> denotes the output feature map, <italic toggle=\"yes\">Transpose</italic>(.) represents reshaping the feature map into a one-dimensional vector, <inline-formula id=\"IEq28\"><tex-math id=\"d33e837\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma$$\\end{document}</tex-math></inline-formula> denotes the activation function. Equation (<xref rid=\"Equ26\" ref-type=\"disp-formula\">26</xref>) equation summarizes the core computation of the Channel Mixing MLP, which achieves cross-channel information interaction and feature mixing through the combination of fully connected layers and activation functions.</p><p id=\"Par34\">The feature graph output by the channel mixing MLP is first dimensionally transformed to meet the needs of the token mixing MLP.<disp-formula id=\"Equ28\"><label>28</label><tex-math id=\"d33e847\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Y_{in} = Transpose(Q_{out}) \\quad Y_{in} \\in R^{ C \\times (H\\times W)} \\end{aligned}$$\\end{document}</tex-math></disp-formula>The transposed feature map is then fed into the Token Mixing MLP, which performs spatial feature mixing via fully connected layers. The input feature map has a shape of [<italic toggle=\"yes\">C</italic>,&#160;<italic toggle=\"yes\">N</italic>], where <inline-formula id=\"IEq29\"><tex-math id=\"d33e858\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N = H\\times W$$\\end{document}</tex-math></inline-formula> represents the total number of spatial elements and <italic toggle=\"yes\">C</italic> denotes the number of channels. The computation of the Token Mixing MLP can be expressed as:</p><p><disp-formula id=\"Equ29\"><label>29</label><tex-math id=\"d33e866\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Y_{ft} = \\sigma (W_{ft} \\cdot Y_{in} + b_{ft}) \\quad Y_{ft} \\in R^{ C\\times (H\\times W) }; W_{ft} \\in R^{(H\\times W)\\times (H\\times W)}; b_{ft} \\in R^{(H\\times W)\\times 1} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Where <inline-formula id=\"IEq30\"><tex-math id=\"d33e871\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma$$\\end{document}</tex-math></inline-formula> is the GELU activation function. Similar to other activation functions, Gelu introduces nonlinearity to enable neural networks to learn and model more complex function mappings. Additionally, the GELU function is continuously differentiable and smooth, which facilitates stable gradient propagation during backpropagation, mitigating issues such as gradient vanishing or exploding.</p><p id=\"Par35\">In summary, the computational formula for the Token Mixing MLP can be expressed as:</p><p><disp-formula id=\"Equ30\"><label>30</label><tex-math id=\"d33e879\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Y_{out} = \\sigma (W_{ft} \\cdot (Transpose(Q_{out}) + b_{ft})) \\quad Y_{out} \\in R^{C \\times (H\\times W) } \\end{aligned}$$\\end{document}</tex-math></disp-formula>The Token-Mixing MLP enhances the model ability to capture spatial information by mixing features across the spatial dimension. This design allows features from each channel to interact across all spatial locations, thereby enriching the expressiveness of the features.</p><p id=\"Par36\">Subsequently, dimensional transformation is performed on the feature graph to reconstruct and generate a complete feature graph.<disp-formula id=\"Equ31\"><label>31</label><tex-math id=\"d33e886\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P = Transpose(Y_{out}) \\quad P \\in R^{C \\times H \\times W} \\end{aligned}$$\\end{document}</tex-math></disp-formula>Finally, the final feature map is output through convolution operation.<disp-formula id=\"Equ32\"><label>32</label><tex-math id=\"d33e891\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} P_{out}=Conv2d\\left( P\\right) \\quad P_{out} \\in R^{2C \\times (H/2) \\times (W/2)} ; Conv2d \\in \\mathbb {R}^{(in =C, out = 2C, kernel =3, stride =2, padding =1)} \\end{aligned}$$\\end{document}</tex-math></disp-formula></p></sec></sec><sec id=\"Sec10\"><title>Experiments and analysis</title><p id=\"Par37\">In this section, we validate the effectiveness and authenticity of the proposed method through extensive experiments and analyses. Section 4.1 introduces the three medical image classification datasets used for validation, including information such as the number of images and classification categories. Section 4.2 describes the relevant experimental settings, including the environment and hyperparameter configurations used to run the experiments. Section 4.3 presents the evaluation metrics employed in the experiments, along with a detailed explanation of the rationale for using these metrics. Section 4.4 demonstrates the results of the proposed method on three datasets and conducts comparative experiments with state-of-the-art models. Section 4.5 validates the effectiveness of the proposed method through ablation experiments.</p><sec id=\"Sec11\"><title>Datasets</title><p id=\"Par38\">All medical images in this article were collected using GE Revolution 256 CT and Siemens Dual Source Spiral CT (Somatom Definition Flash) equipment from the Radiology Imaging Center of Xinjiang Uygur Autonomous Region People&#8217;s Hospital. The patients are all outpatient or inpatient patients who seek treatment at the People&#8217;s Hospital of Xinjiang Uygur Autonomous Region. Note that the selection of the 12th thoracic vertebra level and its upper/lower planes during data collection is due to the relatively large area of the liver in these planes, which facilitates later analysis and diagnosis.</p><p id=\"Par39\"><bold>The liver disease dataset from twelve cervical layers (Twelve Cervical Layers)</bold>: This dataset consists of 3000 liver CT images from 3000 patients. Among them, the number of normal liver, fatty liver, and cirrhosis images is 1000 each. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ\">https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ</ext-link>.</p><p id=\"Par40\"><bold>The liver diseases in the upper layer of the twelve cervical vertebrae (The upper layers of the twelve cervical vertebrae)</bold>: This dataset consists of 3004 liver CT images from 3004 patients. Among them, there are 1002 cirrhosis, and 1001 images of fatty liver and normal liver images, respectively. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ\">https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ</ext-link>.</p><p id=\"Par41\"><bold>The liver diseases in the inferior layer of the twelve cervical vertebrae (The inferior layer of the twelve cervical vertebrae)</bold>: This dataset consists of 3004 liver CT images from 3002 patients. Among them, there are 1001 normal liver images, and 1001 images of fatty liver and cirrhosis, respectively. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ\">https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ</ext-link>.</p></sec><sec id=\"Sec12\"><title>Experimental details</title><p id=\"Par42\">To ensure the authenticity and validity of the experimental results, this paper uses standardized and unified experimental settings. In terms of input parameters, the default image resolution of the model is uniformly set to <inline-formula id=\"IEq31\"><tex-math id=\"d33e931\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$224 \\times 224$$\\end{document}</tex-math></inline-formula>, a standard specification widely recognized in the medical image field. The batch size is fixed at 32, a parameter setting that effectively balances computational efficiency and model training effects. During the image preprocessing stage, no complex data augmentation techniques were used; instead, basic operations such as random cropping, random horizontal flipping, and normalization were employed to ensure data diversity while maintaining data distribution consistency.</p><p id=\"Par43\">During model training, the Adam optimization algorithm was selected. As a gradient-based optimization method, Adam is highly regarded in the deep learning community for its fast convergence and excellent generalization capabilities. The initial learning rate was set to 0.0001, and a cosine annealing decay strategy was used to dynamically adjust the learning rate. The hyperparameter <inline-formula id=\"IEq32\"><tex-math id=\"d33e937\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$T_{max}$$\\end{document}</tex-math></inline-formula> was set to 10 to regulate the learning rate decay rate and ensure stable and efficient model training. The default number of training epochs was set to 300. All experiments were completed on a single NVIDIA TITAN RTX 24G GPU to ensure consistency and reproducibility of the experimental environment.</p></sec><sec id=\"Sec13\"><title>Evaluation metrics</title><p id=\"Par44\">In medical image classification tasks, a single evaluation metric is often insufficient to fully reflect model performance. To achieve a precise assessment of model performance, this paper constructs a multi-dimensional evaluation metric system. Core evaluation metrics including Accuracy (Acc), Precision, Recall, and F1-score were selected. Accuracy, a key evaluation parameter in classification tasks, intuitively reflects the overall correctness of model classification. Precision focuses on the accuracy of the model positive predictions, while Recall measures the model ability to identify true positive examples. Given the trade-off between Precision and Recall in some scenarios, the F1-score is introduced to comprehensively consider both, providing a more comprehensive evaluation of the model classification performance.</p><p id=\"Par45\">Additionally, the Receiver Operating Characteristic (ROC) curve and its Area Under the Curve (AUC) were used as supplementary evaluation metrics. The ROC curve visually demonstrates the model classification performance by plotting the True Positive Rate against the False Positive Rate at different classification thresholds. The AUC value quantifies the quality of the ROC curve, providing an objective basis for comparing the classification performance of different models. Through the comprehensive application of these multi-dimensional evaluation metrics, a comprehensive and accurate assessment of the model&#8217;s performance in medical image classification tasks can be achieved. The specific calculation formulas for each metric are as follows:<disp-formula id=\"Equ33\"><label>33</label><tex-math id=\"d33e947\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; \\text {Accuracy (ACC)} = \\frac{\\text {TP} + \\text {TN}}{\\text {TP} + \\text {FP} + \\text {TN} + \\text {FN}} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ34\"><label>34</label><tex-math id=\"d33e951\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; \\text {Precision} = \\frac{\\text {TP}}{\\text {TP} + \\text {FP}} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ35\"><label>35</label><tex-math id=\"d33e955\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; \\text {Recall} = \\frac{\\text {TP}}{\\text {TP} + \\text {FN}} \\end{aligned}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ36\"><label>36</label><tex-math id=\"d33e959\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} &amp; \\text {F1 Score} = \\frac{2 * \\text {Precision} * \\text {Recall}}{\\text {Precision} + \\text {Recall}} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where TP is True Positive,FP is False Positive,TN is True Negative, and FN is False Negative. The AUC is calculated as:<disp-formula id=\"Equ37\"><label>37</label><tex-math id=\"d33e964\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\text {AUC} = \\frac{\\sum _{i \\in \\text {positiveClass}} \\text {rank}_i - M(1 + M)/2}{M * N} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <italic toggle=\"yes\">M</italic> is the number of positive samples,<italic toggle=\"yes\">N</italic> is the number of negative samples, and <inline-formula id=\"IEq33\"><tex-math id=\"d33e976\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$rank_{i}$$\\end{document}</tex-math></inline-formula> is the model predicted probability for sample <italic toggle=\"yes\">i</italic>.</p></sec><sec id=\"Sec14\"><title>Experimental results</title><p id=\"Par46\">To comprehensively evaluate the performance of CMT-Net, comparative experiments were conducted on three datasets: Twelve cervical layers, The upper layers of the twelve cervical vertebrae, and The inferior layer of the twelve cervical vertebrae. The method was systematically compared with various approaches based on CNNs, MLPs, vision transformers (ViTs), and their hybrid architectures. In the comparison of CNN-based methods, both classical architectures such as ResNet and EfficientNetV2, as well as cutting-edge improved models like RepVGG, ConvNext, InceptionNext, and FasterNet, were included. Meanwhile, to deeply explore the performance of pure MLP architectures and CNN-MLP hybrid networks in medical image classification tasks, this study selected state-of-the-art (SOTA) models for comparative experiments, including Conv-SdMLPMixer and SCNet, where SCNet represents the latest research achievements of CNN+MLP hybrid architectures in the field of medical image classification.</p><p id=\"Par47\">Additionally, this paper conducted a comparative analysis of ViT-based models and their hybrid architectures, involving classical models like BiFormer and current top-performing SOTA methods such as GroupMixFormer.</p><sec id=\"Sec15\"><title>Comparative experimental results on the twelve cervical layers dataset</title><p id=\"Par48\">Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref> shows the five key metrics&#8212;-Accuracy (Acc), F1-score, Precision, Recall, and AUC&#8211;of CNN, transformer, MLP-based networks, and their hybrid models on the Twelve cervical layers dataset. Meanwhile, the parameter count (Params) and floating-point operations (FLOPs) of each model are provided to comprehensively evaluate performance and computational complexity. The optimal metrics among the compared models are bolded in the table.</p><p>\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Comparative experiments on the The Twelve cervical layers dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Pre</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Re</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Auc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Parma</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FLOPs</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin-S&#160;<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.44</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">48.8M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">8.6G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet50&#160;<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.08</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.22</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.47</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">23.5M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">4.1G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SCNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.62</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.62</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">55.1M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">12.1G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BiFormer-S&#160;<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.31</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.40</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">25.0M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">4.2G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv-SdMLPMixer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.40</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.78</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">49.8M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10.8G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GroupMixFormer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR49\">49</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.36</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22.1M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.1G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.47</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">25.2M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">6.4G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTM&#160;<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.04</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.21</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">28.7M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10.3G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">InceptionNext&#160;<sup><xref ref-type=\"bibr\" rid=\"CR52\">52</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.26</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.26</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">47.1M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">8.4G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FasterNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.42</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.73</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>13.7M</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>1.9G</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">RepVGG&#160;<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">45.8M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">9.9G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNetV2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR55\">55</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.77</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.94</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20.2M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2.9G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CrossViT&#160;<sup><xref ref-type=\"bibr\" rid=\"CR56\">56</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.21</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.14</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.24</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">26.7M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.3G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DaViT&#160;<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.27</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.36</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.59</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">28.3M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">4.5G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.67</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.71</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.87</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.67</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.50</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.5M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.0G</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par49\">The data in the table show that our CMT-Net demonstrated excellent performance on the Twelve cervical layers dataset, with an Acc of 88.67%, an F1-score of 88.71%, a Precision of 88.87%, a Recall of 88.67%, and an AUC of 91.50%. Among these, Acc, F1-score, Precision, Recall, and AUC all outperformed other comparative methods.</p><p id=\"Par50\">Among CNN-based methods, ResNet50 and RepVGG showed relatively superior performance. However, compared with RepVGG, our method achieved significant optimization in model complexity, using only approximately one-third of the parameters and half of the FLOPs, while significantly improving key performance metrics. Specifically, the Accuracy (Acc) increased by 2.17% percentage points, the F1-score increased by 2.33%, the Precision increased by 1.75%, the Recall increased by 1.82%, and the Area Under the Curve (AUC) increased by 1.62%. This indicates that our method significantly enhances the model&#8217;s classification ability while maintaining efficient computation.</p><p id=\"Par51\">When compared with transformer-based methods, using the best-performing BiFormer-S as a reference, our method also showed strong advantages. In terms of Accuracy, our method improved by 2.34% percentage points compared with BiFormer-S, the F1-score increased by 2.4%, the Precision increased by 2.47%, the Recall increased by 2.34%, and the AUC increased by 1.75%. These improvements strongly demonstrate that our method has more accurate representation and classification capabilities when processing complex data features.</p><p id=\"Par52\">For MLP-based methods, Conv-SdMLPMixer represents the current top performance level. Compared with it, our method improved by 2.34% in Accuracy, significantly increased the F1-score by 4.22%, improved the Precision by 0.47%, and increased the AUC by 1.72%. Notably, our method achieved these performance improvements using only approximately 40% of the parameters of Conv-SdMLPMixer. This result fully demonstrates the effectiveness and innovation of our method in model lightweighting and performance optimization, providing highly valuable new ideas and solutions for research and applications in related fields.</p><p id=\"Par53\">In summary, this result not only validates the robustness and scalability of CMT Net in complex medical imaging, but also provides hope for deploying CMT Net to PACS or end devices for high-precision liver disease screening.</p></sec><sec id=\"Sec16\"><title>Comparative experimental results on the upper layers of the twelve cervical vertebrae dataset</title><p id=\"Par54\">Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> presents the five key performance metrics&#8211;Acc, F1-score, Precision, Recall, and AUC&#8211;of different methods on The upper layers of the twelve cervical vertebrae dataset. On this dataset, our method demonstrated excellent performance. As can be clearly seen from the table data, our method significantly outperformed all comparative methods in all five key evaluation metrics&#8211;Accuracy (Acc), F1-score, Precision, Recall, and Area Under the Curve (AUC)&#8211;ranking first. Specifically, our method achieved an Accuracy of 88.83%, an F1-score of 88.84%, a Precision of 88.99%, a Recall of 88.73%, and an AUC of 91.62%.</p><p>\n<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Comparative experiments onThe upper layers of the twelve cervical vertebrae.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Pre</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Re</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Auc</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin-S&#160;<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.51</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.55</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.13</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Resnet50&#160;<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.82</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.53</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.13</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BiFormer-S&#160;<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.82</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.84</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.13</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SCNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.30</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.46</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.62</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv-SdMLPMixer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.09</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.46</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.56</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.38</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GroupMixFormer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR49\">49</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.35</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.47</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.03</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.50</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTM&#160;<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.04</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.86</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.25</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">InceptionNext&#160;<sup><xref ref-type=\"bibr\" rid=\"CR52\">52</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.48</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.60</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.57</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.12</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Fasternet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.03</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.80</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.75</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Repvgg-Big2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.43</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.38</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNetV2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR55\">55</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.25</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.83</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.84</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.99</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.73</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.62</bold>\n</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par55\">As shown in Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref>, Compared with other advanced methods, our method showed more obvious advantages in various metrics. Taking Accuracy as an example, compared with the outstanding Eff-CTNet, our method achieved a 1.5% improvement; compared with the classical ResNet50, the Accuracy increased by 2%. In terms of F1-score, the improvement compared with Eff-CTNet was 1.51%, while the improvement compared with InceptionNeXt reached 3.36%, indicating that our method achieved a better balance between Precision and Recall.</p><p id=\"Par56\">In the Precision metric, our method improved by 1.53% percentage points compared with Conv-SdMLPMixer, demonstrating stronger positive sample recognition ability; in terms of Recall, it improved by 2.27% compared with SCNet, indicating that our method can more effectively identify actual positive samples; in the AUC metric, it improved by 1.24% percentage points compared with Conv-SdMLPMixer, further proving the superiority of our method in classification performance.</p><p id=\"Par57\">In terms of parameter count (Params) and FLOPs, while achieving optimal performance metrics, our method also has certain competitiveness in computational complexity, able to reasonably control computational resource consumption while ensuring high performance, providing a more advantageous choice for practical applications.</p><p id=\"Par58\">Overall, CMT-Net leads existing CNN, Transformer, and MLP methods with an accuracy of 88.83% and an AUC of 91.62%, while maintaining a moderate scale in terms of parameter count and FLOPs, balancing accuracy and efficiency. The above results are expected to directly embed the CMT-Net model into existing PACS and continuously optimize it through periodic incremental fine-tuning, significantly reducing misdiagnosis and missed diagnosis, providing an immediately implementable AI solution for early screening, follow-up, and large-scale physical examinations of liver diseases.</p></sec><sec id=\"Sec17\"><title>Comparative experimental results on the inferior layer of the twelve cervical vertebrae Dataset</title><p id=\"Par59\">As shown in Table <xref rid=\"Tab3\" ref-type=\"table\">3</xref>, our method performed excellently on The inferior layer of the twelve cervical vertebrae dataset, with an Acc of 86.67%, an F1-score of 86.37, a Precision of 86.72%, a Recall of 86.97%, and a high AUC of 90.00%. Among these five key metrics, Acc, F1-score, Precision, Recall, and AUC all outperformed all other comparative methods.</p><p>\n<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Comparative experiments on The inferior layer of the twelve cervical vertebrae.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Pre</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Re</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Auc</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin-S &#160;<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.47</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.48</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.65</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Resnet50&#160;<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.92</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.84</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.86</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.88</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SCNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.86</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BiFormer-S&#160;<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.22</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.06</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.01</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv-SdMLPMixer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.51</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.50</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GroupMixFormer&#160;<sup><xref ref-type=\"bibr\" rid=\"CR49\">49</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.74</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.09</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">74.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.50</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.29</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.52</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.38</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-CTM &#160;<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.79</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.81</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.88</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">InceptionNext&#160;<sup><xref ref-type=\"bibr\" rid=\"CR52\">52</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.82</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.38</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FasterNet&#160;<sup><xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.72</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.02</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.69</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.75</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Repvgg-B1g2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.31</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.25</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNetV2&#160;<sup><xref ref-type=\"bibr\" rid=\"CR55\">55</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.03</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.37</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.72</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.00</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par60\">When compared with traditional CNN methods, our method showed significant advantages. Taking ResNet50 as an example, its Accuracy (Acc) was 83.83%, while our method improved the Accuracy by 2.84% compared with ResNet50; in terms of F1-score, ResNet50 was 83.92%, and our method achieved an improvement of 2.45%. Additionally, our method achieved varying degrees of improvement in key metrics such as Precision, Recall, and AUC, strongly demonstrating its advancement compared with traditional CNN methods.</p><p id=\"Par61\">When compared with transformer-based methods, our method also had outstanding advantages in various metrics. Among many transformer-based methods, GroupMixFormer performed relatively well; however, compared with it, our method improved the Accuracy by 2%, the F1-score by 1.63%, the Precision and Recall by 1.63% each, and the AUC by 1.5%, further highlighting the superiority of our method.</p><p id=\"Par62\">In the MLP-based method system, Conv-SdMLPMixer is a representative of relatively excellent performance. Even so, our method still showed strong competitiveness when compared with it. Specifically, our method improved the Accuracy by 2%, the F1-score by 2.01%, the Precision by 1.21%, the Recall by 2%, and the AUC by 1.5%. In summary, whether compared with CNN, transformer, or MLP-related methods, our method has significantly surpassed them in multiple key performance metrics, providing a more promising solution for related research fields.</p><p id=\"Par63\">In summary, this model maintains high performance while still having controllable computational overhead, and can be directly embedded into existing PACS or edge devices to achieve real-time, high-precision liver disease screening and follow-up, providing a plug and play solution for clinical implementation.</p></sec></sec><sec id=\"Sec18\"><title>Visual analysis</title><p id=\"Par64\">The Grad-CAM visualization results of different methods on three datasets are shown in Figure <xref rid=\"Fig6\" ref-type=\"fig\">6</xref>. From the visualization results, there are differences in the activation levels of each method in key regions. Traditional CNN methods such as ResNet, FasterNet, and EfficientNetV2 reflect different emphases on capturing malignant sample features, with obvious activation in specific structural regions of the image, indicating their sensitivity to the features of these regions. BiFormer, a transformer-based method, benefits from the transformer architecture advantage in capturing long-range dependency relationships, and its activation regions may pay more attention to globally correlated features in the samples. Compared with other methods, our method has more precise and concentrated activation in key regions, able to accurately locate the lesion areas in the liver, demonstrating effective capture of the key features of malignant samples. This means that our method can more accurately focus on the core information related to the determination of malignant samples in feature extraction and classification decision-making. The visualization results further verify the accuracy of the metrics in the tables, indicating that the model not only performs excellently in numerical metrics but also has considerable advantages in actual lesion localization capabilities.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>The Grad-CAM visualization results of different methods on three datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig6_HTML.jpg\"/></fig></p><p id=\"Par65\">Figure <xref rid=\"Fig7\" ref-type=\"fig\">7</xref> shows the ROC curves of different methods on the three datasets. On The Twelve cervical layers dataset, the ROC curves of traditional CNN models such as ResNet and InceptionNeXt are relatively close, and their AUC values are also similar, indicating that their overall classification performances on this dataset are comparable. For The upper layer of the twelve cervical vertebrae and The inferior layer of the twelve cervical vertebrae datasets, the performance differences between models are more significant. Transformer-based models such as BiFormer showed certain performance advantages, but our method&#8217;s (Ours) ROC curve is located above the others, standing out among many models, indicating better performance in balancing the False Positive Rate (FPR) and True Positive Rate (TPR), and being able to more effectively identify positive and negative samples. Through the analysis of the ROC curves of different models on the three datasets, our method demonstrated excellent performance in medical image classification tasks, showing significant advantages over other classical models.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Visualization of ROC curves for different methods on three datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig7_HTML.jpg\"/></fig></p><p id=\"Par66\">Figure <xref rid=\"Fig8\" ref-type=\"fig\">8</xref> shows the visualization results of the confusion matrices of the proposed model method on the three medical image datasets. In the confusion matrix of The Twelve cervical layers dataset, the elements on the main diagonal represent the number of samples correctly classified by the model, with relatively high values, indicating that the model has a certain overall classification ability. For The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, the confusion matrices also show the classification effects of the model on various categories. However, the &#8220;Carcinoma&#8221; category has a high probability of being misclassified as other categories, which may be due to the model&#8217;s difficulty in distinguishing certain categories with similar features, leading to a decrease in classification accuracy.<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Visualization examples of confusion matrices in three different datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_17581_Fig8_HTML.jpg\"/></fig></p></sec><sec id=\"Sec19\"><title>Ablation studies</title><p id=\"Par67\">To deeply explore the roles of the components of the proposed method and the impact of key module parameter settings, this section introduces ablation experiments. Figure <xref rid=\"Tab4\" ref-type=\"table\">4</xref> shows the results of the ablation experiments.</p><p>\n<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Performance comparison across different cervical layers and configurations.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Cervical Layers</th><th align=\"left\" rowspan=\"2\" colspan=\"1\">Configuration</th><th align=\"left\" rowspan=\"2\" colspan=\"1\">Method</th><th align=\"left\" colspan=\"8\" rowspan=\"1\">Metrics</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc(M&#177;SD)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Acc(CI)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Pre</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Re</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Auc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Parma</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FLOPs</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"7\" colspan=\"1\">Twelve cervical layers</td><td align=\"left\" rowspan=\"4\" colspan=\"1\">w/o Each Module</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83&#177;0.52</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[85.33, 87.92]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.77</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.94</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>20.2M</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>2.9G</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+ET</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.67&#177;0.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[86.42, 88.86]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.65</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.67</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.7M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3.8G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+HM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.17&#177;0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[86.85, 89.30]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30.3M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">6.7G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>88.67</bold>&#177;<bold>0.42</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[87.63, 89.71]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.71</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.87</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.67</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.50</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.5M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.0G</td></tr><tr><td align=\"left\" rowspan=\"3\" colspan=\"1\">Window Size</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.00&#177;0.46</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[86.86, 89.14]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.5M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.0G</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">7(ours)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>88.67</bold>&#177;<bold>0.32</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[87.88, 89.46]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.71</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.87</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.67</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.50</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>29.5M</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>5.0G</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.83&#177;0.30</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[87.08, 88.58]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.92</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.5M</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5.0G</td></tr><tr><td align=\"left\" rowspan=\"7\" colspan=\"1\"><p>The upper layers of</p><p>the twelve cervical vertebrae</p></td><td align=\"left\" rowspan=\"4\" colspan=\"1\">w/o Each Module</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00&#177;0.41</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[85.98, 88.02]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.01</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+ET</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.00&#177;0.26</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[87.35, 88.65]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.02</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+HM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.50&#177;0.32</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[86.71, 88.29]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.46</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">87.66</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>88.83</bold>&#177;<bold>0.29</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[88.11, 89.55]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.84</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.99</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.73</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.62</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" rowspan=\"3\" colspan=\"1\">Window Size</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.33&#177;0.31</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[87.56, 89.10]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.53</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.33</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">7(ours)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>88.83</bold>&#177;<bold>0.30</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[88.08, 89.58]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.84</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.99</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>88.73</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>91.62</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.50&#177;0.36</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[87.61, 89.39]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.53</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.77</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.49</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">91.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" rowspan=\"7\" colspan=\"1\"><p>The inferior layer of</p><p>the twelve cervical vertebrae</p></td><td align=\"left\" rowspan=\"4\" colspan=\"1\">w/o Each Module</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.83&#177;0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[83.71, 85.95]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.03</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">84.83</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">88.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+ET</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.50&#177;0.43</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[84.43, 86.57]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.56</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.81</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline+HM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.00&#177;0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[84.76, 87.24]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.05</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.21</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>86.67</bold>&#177;<bold>0.44</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[85.58, 87.76]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.67</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.72</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.97</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>90.00</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" rowspan=\"3\" colspan=\"1\">Window Size</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.33&#177;0.40</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[85.34, 87.32]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.26</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.55</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.63</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.75</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">7(ours)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>86.67</bold>&#177;<bold>0.36</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>[85.78, 87.56]</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.87</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.72</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>86.97</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>90.00</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.50&#177;0.38</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">[85.56, 87.44]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.40</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.62</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr></tbody></table><table-wrap-foot><p>Among them, ACC (M&#177;SD) represents the mean and standard deviation of ACC. ACC (CI) represents the 95% confidence interval of ACC.</p></table-wrap-foot></table-wrap>\n</p><sec id=\"Sec20\"><title>Impact of each module</title><p id=\"Par68\">On the three datasets, using the Baseline model as the foundation, we conducted experiments by separately adding the ET module and the HM module, and compared them with the complete model.</p><p id=\"Par69\">As shown in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, on The Twelve cervical layers dataset, the Baseline model achieved certain results in metrics such as Accuracy and F1-score, with an Accuracy of 86.83%. After adding the ET module, the model&#8217;s metrics improved, with the Accuracy reaching 87.67%, indicating that the ET module can effectively enhance the model&#8217;s ability to extract and process medical image features, thereby improving classification performance. After adding the HM module, the Accuracy further increased to 88.17%, indicating that the HM module played a positive role in feature fusion and classification decision-making. The proposed model, after integrating these modules, achieved an Accuracy of 88.67%, outperforming the cases of adding modules individually in multiple metrics, fully demonstrating the advantages of the collaborative work of each module, which can more comprehensively and accurately extract image features, thereby improving classification accuracy. Similar trends were observed on The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, achieving better classification results. This again proves the positive impact of the ET and HM modules on model performance and the effectiveness of the overall model structure in integrating the advantages of each module.</p></sec><sec id=\"Sec21\"><title>Impact of window size in the GCA module</title><p id=\"Par70\">As shown in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, for the key parameter of the GCA module, which divides the feature map into regions in the spatial dimension, we set different values (5, 7, 9) for experiments. In a paper to be submitted to a top Chinese Academy of Sciences (CAS) Zone 1 journal on the theme of medical image classification, the experimental results on the three datasets show that when the feature map patch size was 5<inline-formula id=\"IEq34\"><tex-math id=\"d33e2629\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>5, the model showed certain performance in various metrics, such as an Accuracy of 88.00% on The Twelve cervical layers dataset. When the feature map patch size was adjusted to 7<inline-formula id=\"IEq35\"><tex-math id=\"d33e2633\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>7, the Accuracy increased to 88.67%, with corresponding improvements in other metrics. This indicates that when the feature map patch size is 7<inline-formula id=\"IEq36\"><tex-math id=\"d33e2637\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>7, the GCA module can more accurately capture key feature information in the image and optimize feature interaction, thereby improving model performance. When the feature map patch size became 9<inline-formula id=\"IEq37\"><tex-math id=\"d33e2641\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>9, the model Accuracy decreased to 87.83%, indicating that overly large feature map patches may introduce excessive redundant information, interfering with the model extraction of key features and leading to performance degradation. On The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, the results also show that the GCA module performed best with a feature map patch size of 7<inline-formula id=\"IEq38\"><tex-math id=\"d33e2646\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula>7, providing strong support for the overall performance improvement of the model.</p><p id=\"Par71\">In summary, the ablation experiment results clearly demonstrate the significant impact of the proposed ET module, HM module, and the feature map patch size in the GCA module on the performance of medical image classification models, strongly supporting the effectiveness and rationality of our proposed method and providing important insights for the further optimization of subsequent models.</p></sec></sec></sec><sec id=\"Sec22\"><title>Conclusion</title><p id=\"Par72\">To address the challenge of identifying normal liver, fatty liver, and liver cirrhosis CT images in medical imaging diagnosis&#8211;caused by subtle differences in morphology and density&#8211;this study proposes CMT-Net, a hybrid medical image classification network that integrates the advantages of CNNs, MLPs, and Transformer. The network employs an Efficient Transformer module (ET) to focus on local feature details and gradually integrate global information, while a Hybrid MLP module (HM) enables deep fusion of spatial and channel-wise information, forming a hierarchical single-branch serial feature extraction architecture. Experimental results demonstrate that CMT-Net significantly outperforms existing CNN, ViT, and hybrid-based methods on multiple datasets. Ablation studies validate the effectiveness of the core modules, and visualization analyses show its ability to accurately localize lesion areas. This research not only provides a new tool for the precise diagnosis of liver diseases but also offers novel insights for designing deep learning models in medical image classification. However, this study only focused on the static CT images of three types of liver diseases (normal, fatty liver and liver cirrhosis), and did not involve multiphase images, multiple organ lesions or more complex clinical scenes (such as early screening of liver cancer and fibrosis classification). In addition, this study did not integrate clinical indicators (laboratory examination, medical history) or other modes (MRI, ultrasonic elastography), which may miss complementary diagnostic information.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>Xiaolei He and Xilong Wang contributed equally to this work.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>X.H.: Software, Funding acquisition. X.W.: Conceptualization. Y.W.: Validation. H.L.: Writing - review &amp; editing. S.L.: Formal analysis. J.W.: Resources. Y.F.: Supervision. Q.W.: Visualization. J.C.: Writing - original draft.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>This study was funded by the Hospital Level Project 20230118 of Xinjiang Uygur Autonomous Region People&#8217;s Hospital.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The dataset used in this study was from the Medical Imaging Center of Xinjiang Uygur Autonomous Region People&#8217;s Hospital. The relevant data has been stored in the following link: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pan.baidu.com/s/1JkmdP2Ukc4iHM3108AYrUw?pwd=hug3\">https://pan.baidu.com/s/1JkmdP2Ukc4iHM3108AYrUw?pwd=hug3</ext-link>. Other researchers can download and use it normally. If you have any questions while using the data, please contact the corresponding author through the following methods: 13565898386@163.com.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par77\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Younossi</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Global burden of nafld and nash: trends, predictions, risk factors and prevention</article-title><source>Nat. Rev. Gastroenterol. Hepatol.</source><year>2018</year><volume>15</volume><fpage>11</fpage><lpage>20</lpage><pub-id pub-id-type=\"doi\">10.1038/nrgastro.2017.109</pub-id><pub-id pub-id-type=\"pmid\">28930295</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Younossi, Z. et al. Global burden of nafld and nash: trends, predictions, risk factors and prevention. <italic toggle=\"yes\">Nat. Rev. Gastroenterol. Hepatol.</italic><bold>15</bold>, 11&#8211;20 (2018).<pub-id pub-id-type=\"pmid\">28930295</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/nrgastro.2017.109</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gin&#232;s</surname><given-names>P</given-names></name><etal/></person-group><article-title>Liver cirrhosis</article-title><source>The Lancet</source><year>2021</year><volume>398</volume><fpage>1359</fpage><lpage>1376</lpage><pub-id pub-id-type=\"doi\">10.1016/S0140-6736(21)01374-X</pub-id><pub-id pub-id-type=\"pmid\">34543610</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Gin&#232;s, P. et al. Liver cirrhosis. <italic toggle=\"yes\">The Lancet</italic><bold>398</bold>, 1359&#8211;1376 (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/S0140-6736(21)01374-X</pub-id><pub-id pub-id-type=\"pmid\">34543610</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Friedman</surname><given-names>SL</given-names></name></person-group><article-title>Mechanisms of hepatic fibrogenesis</article-title><source>Gastroenterology</source><year>2008</year><volume>134</volume><fpage>1655</fpage><lpage>1669</lpage><pub-id pub-id-type=\"doi\">10.1053/j.gastro.2008.03.003</pub-id><pub-id pub-id-type=\"pmid\">18471545</pub-id><pub-id pub-id-type=\"pmcid\">PMC2888539</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Friedman, S. L. Mechanisms of hepatic fibrogenesis. <italic toggle=\"yes\">Gastroenterology</italic><bold>134</bold>, 1655&#8211;1669 (2008).<pub-id pub-id-type=\"pmid\">18471545</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1053/j.gastro.2008.03.003</pub-id><pub-id pub-id-type=\"pmcid\">PMC2888539</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lee</surname><given-names>DH</given-names></name></person-group><article-title>Imaging evaluation of non-alcoholic fatty liver disease: focused on quantification</article-title><source>Clin. Mol. Hepatol.</source><year>2017</year><volume>23</volume><fpage>290</fpage><pub-id pub-id-type=\"doi\">10.3350/cmh.2017.0042</pub-id><pub-id pub-id-type=\"pmid\">28994271</pub-id><pub-id pub-id-type=\"pmcid\">PMC5760010</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Lee, D. H. Imaging evaluation of non-alcoholic fatty liver disease: focused on quantification. <italic toggle=\"yes\">Clin. Mol. Hepatol.</italic><bold>23</bold>, 290 (2017).<pub-id pub-id-type=\"pmid\">28994271</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3350/cmh.2017.0042</pub-id><pub-id pub-id-type=\"pmcid\">PMC5760010</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tang</surname><given-names>A</given-names></name><etal/></person-group><article-title>Accuracy of mr imaging-estimated proton density fat fraction for classification of dichotomized histologic steatosis grades in nonalcoholic fatty liver disease</article-title><source>Radiology</source><year>2015</year><volume>274</volume><fpage>416</fpage><lpage>425</lpage><pub-id pub-id-type=\"doi\">10.1148/radiol.14140754</pub-id><pub-id pub-id-type=\"pmid\">25247408</pub-id><pub-id pub-id-type=\"pmcid\">PMC4314291</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Tang, A. et al. Accuracy of mr imaging-estimated proton density fat fraction for classification of dichotomized histologic steatosis grades in nonalcoholic fatty liver disease. <italic toggle=\"yes\">Radiology</italic><bold>274</bold>, 416&#8211;425 (2015).<pub-id pub-id-type=\"pmid\">25247408</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1148/radiol.14140754</pub-id><pub-id pub-id-type=\"pmcid\">PMC4314291</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Litjens</surname><given-names>G</given-names></name><etal/></person-group><article-title>A survey on deep learning in medical image analysis</article-title><source>Med. Image Anal.</source><year>2017</year><volume>42</volume><fpage>60</fpage><lpage>88</lpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2017.07.005</pub-id><pub-id pub-id-type=\"pmid\">28778026</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Litjens, G. et al. A survey on deep learning in medical image analysis. <italic toggle=\"yes\">Med. Image Anal.</italic><bold>42</bold>, 60&#8211;88 (2017).<pub-id pub-id-type=\"pmid\">28778026</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2017.07.005</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic toggle=\"yes\">Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</italic> 234&#8211;241 (Springer, 2015).</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N</given-names></name><etal/></person-group><article-title>Convolutional neural networks for medical image analysis: full training or fine tuning?</article-title><source>IEEE Trans. Med. Imaging</source><year>2016</year><volume>35</volume><fpage>1299</fpage><lpage>1312</lpage><pub-id pub-id-type=\"doi\">10.1109/TMI.2016.2535302</pub-id><pub-id pub-id-type=\"pmid\">26978662</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Tajbakhsh, N. et al. Convolutional neural networks for medical image analysis: full training or fine tuning?. <italic toggle=\"yes\">IEEE Trans. Med. Imaging</italic><bold>35</bold>, 1299&#8211;1312 (2016).<pub-id pub-id-type=\"pmid\">26978662</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2016.2535302</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jesi</surname><given-names>PM</given-names></name><name name-style=\"western\"><surname>Daniel</surname><given-names>VAA</given-names></name></person-group><article-title>Differential cnn and kelm integration for accurate liver cancer detection</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>95</volume><fpage>106419</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.106419</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Jesi, P. M. &amp; Daniel, V. A. A. Differential cnn and kelm integration for accurate liver cancer detection. <italic toggle=\"yes\">Biomed. Signal Process. Control</italic><bold>95</bold>, 106419 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abinaya</surname><given-names>RJ</given-names></name><name name-style=\"western\"><surname>Rajakumar</surname><given-names>G</given-names></name></person-group><article-title>Accurate liver fibrosis detection through hybrid mrmr-bilstm-cnn architecture with histogram equalization and optimization</article-title><source>J. Imaging Inf. Med.</source><year>2024</year><volume>37</volume><fpage>1008</fpage><lpage>1022</lpage><pub-id pub-id-type=\"doi\">10.1007/s10278-024-00995-1</pub-id><pub-id pub-id-type=\"pmcid\">PMC11169190</pub-id><pub-id pub-id-type=\"pmid\">38351226</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Abinaya, R. J. &amp; Rajakumar, G. Accurate liver fibrosis detection through hybrid mrmr-bilstm-cnn architecture with histogram equalization and optimization. <italic toggle=\"yes\">J. Imaging Inf. Med.</italic><bold>37</bold>, 1008&#8211;1022 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s10278-024-00995-1</pub-id><pub-id pub-id-type=\"pmcid\">PMC11169190</pub-id><pub-id pub-id-type=\"pmid\">38351226</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><mixed-citation publication-type=\"other\">Dosovitskiy, A. <italic toggle=\"yes\">et al.</italic> An image is worth 16x16 words: Transformers for image recognition at scale. preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2010.11929\">arXiv:2010.11929</ext-link> (2020).</mixed-citation></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mir</surname><given-names>AN</given-names></name><name name-style=\"western\"><surname>Rizvi</surname><given-names>DR</given-names></name><name name-style=\"western\"><surname>Ahmad</surname><given-names>MR</given-names></name></person-group><article-title>Enhancing histopathological image analysis: an explainable vision transformer approach with comprehensive interpretation methods and evaluation of explanation quality</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>149</volume><fpage>110519</fpage><pub-id pub-id-type=\"doi\">10.1016/j.engappai.2025.110519</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Mir, A. N., Rizvi, D. R. &amp; Ahmad, M. R. Enhancing histopathological image analysis: an explainable vision transformer approach with comprehensive interpretation methods and evaluation of explanation quality. <italic toggle=\"yes\">Eng. Appl. Artif. Intell.</italic><bold>149</bold>, 110519 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mir</surname><given-names>AN</given-names></name><name name-style=\"western\"><surname>Rizvi</surname><given-names>DR</given-names></name></person-group><article-title>Advancements in deep learning and explainable artificial intelligence for enhanced medical image analysis: a comprehensive survey and future directions</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>158</volume><fpage>111413</fpage><pub-id pub-id-type=\"doi\">10.1016/j.engappai.2025.111413</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Mir, A. N. &amp; Rizvi, D. R. Advancements in deep learning and explainable artificial intelligence for enhanced medical image analysis: a comprehensive survey and future directions. <italic toggle=\"yes\">Eng. Appl. Artif. Intell.</italic><bold>158</bold>, 111413 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Chen, J. <italic toggle=\"yes\">et al.</italic> Transunet: Transformers make strong encoders for medical image segmentation. preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2102.04306\">arXiv:2102.04306</ext-link> (2021).</mixed-citation></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Liu, Z. <italic toggle=\"yes\">et al.</italic> Swin transformer: Hierarchical vision transformer using shifted windows. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF international conference on computer vision</italic> 10012&#8211;10022 (2021).</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ou</surname><given-names>J</given-names></name><etal/></person-group><article-title>Restransunet: an effective network combined with transformer and u-net for liver segmentation in ct scans</article-title><source>Comput. Biol. Med.</source><year>2024</year><volume>177</volume><fpage>108625</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2024.108625</pub-id><pub-id pub-id-type=\"pmid\">38823365</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Ou, J. et al. Restransunet: an effective network combined with transformer and u-net for liver segmentation in ct scans. <italic toggle=\"yes\">Comput. Biol. Med.</italic><bold>177</bold>, 108625 (2024).<pub-id pub-id-type=\"pmid\">38823365</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2024.108625</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kumar</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>RV</given-names></name><name name-style=\"western\"><surname>Ranjith</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Jeevakala</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Varun</surname><given-names>SS</given-names></name></person-group><article-title>Grey wolf optimized swinunet based transformer framework for liver segmentation from ct images</article-title><source>Comput. Electr. Eng.</source><year>2024</year><volume>117</volume><fpage>109248</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compeleceng.2024.109248</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Kumar, S., Kumar, R. V., Ranjith, V., Jeevakala, S. &amp; Varun, S. S. Grey wolf optimized swinunet based transformer framework for liver segmentation from ct images. <italic toggle=\"yes\">Comput. Electr. Eng.</italic><bold>117</bold>, 109248 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><mixed-citation publication-type=\"other\">Guo, J. <italic toggle=\"yes\">et al.</italic> Cmt: Convolutional neural networks meet vision transformers. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic> 12175&#8211;12185 (2022).</mixed-citation></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tolstikhin</surname><given-names>IO</given-names></name><etal/></person-group><article-title>Mlp-mixer: An all-mlp architecture for vision</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>24261</fpage><lpage>24272</lpage></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Tolstikhin, I. O. et al. Mlp-mixer: An all-mlp architecture for vision. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>34</bold>, 24261&#8211;24272 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Krizhevsky</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sutskever</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Commun. ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type=\"doi\">10.1145/3065386</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. <italic toggle=\"yes\">Commun. ACM</italic><bold>60</bold>, 84&#8211;90 (2017).</mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Lyu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title>Automatic computed tomography image segmentation method for liver tumor based on a modified tokenized multilayer perceptron and attention mechanism</article-title><source>Quant. Imaging Med. Surg.</source><year>2025</year><volume>15</volume><fpage>2385</fpage><pub-id pub-id-type=\"doi\">10.21037/qims-24-2132</pub-id><pub-id pub-id-type=\"pmid\">40160629</pub-id><pub-id pub-id-type=\"pmcid\">PMC11948385</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Yang, B., Zhang, J., Lyu, Y. &amp; Zhang, J. Automatic computed tomography image segmentation method for liver tumor based on a modified tokenized multilayer perceptron and attention mechanism. <italic toggle=\"yes\">Quant. Imaging Med. Surg.</italic><bold>15</bold>, 2385 (2025).<pub-id pub-id-type=\"pmid\">40160629</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.21037/qims-24-2132</pub-id><pub-id pub-id-type=\"pmcid\">PMC11948385</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>LeCun</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Bottou</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Haffner</surname><given-names>P</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><year>2002</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type=\"doi\">10.1109/5.726791</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. <italic toggle=\"yes\">Proc. IEEE</italic><bold>86</bold>, 2278&#8211;2324 (2002).</mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><citation-alternatives><element-citation id=\"ec-CR23\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Krizhevsky</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sutskever</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Commun. ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>95</lpage><pub-id pub-id-type=\"doi\">10.1145/3065386</pub-id></element-citation><mixed-citation id=\"mc-CR23\" publication-type=\"journal\">Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. <italic toggle=\"yes\">Commun. ACM</italic><bold>60</bold>, 84&#8211;95 (2017).</mixed-citation></citation-alternatives></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1409.1556\">arXiv:1409.1556</ext-link> (2014).</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Szegedy, C. <italic toggle=\"yes\">et al.</italic> Going deeper with convolutions. In <italic toggle=\"yes\">Proceedings of the IEEE conference on computer vision and pattern recognition</italic> 1&#8211;9 (2015).</mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic toggle=\"yes\">Proceedings of the IEEE conference on computer vision and pattern recognition</italic> 770&#8211;778 (2016).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Huang, G., Liu, Z., Van Der Maaten, L. &amp; Weinberger, K.&#160;Q. Densely connected convolutional networks. In <italic toggle=\"yes\">Proceedings of the IEEE conference on computer vision and pattern recognition</italic> 4700&#8211;4708 (2017).</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Asif</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ain</surname><given-names>Q-U</given-names></name><name name-style=\"western\"><surname>Al-Sabri</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Abdullah</surname><given-names>M</given-names></name></person-group><article-title>Litefusionnet: boosting the performance for medical image classification with an intelligent and lightweight feature fusion network</article-title><source>J. Comput. Sci.</source><year>2024</year><volume>80</volume><fpage>102324</fpage><pub-id pub-id-type=\"doi\">10.1016/j.jocs.2024.102324</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Asif, S., Ain, Q.-U., Al-Sabri, R. &amp; Abdullah, M. Litefusionnet: boosting the performance for medical image classification with an intelligent and lightweight feature fusion network. <italic toggle=\"yes\">J. Comput. Sci.</italic><bold>80</bold>, 102324 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Howard, A.&#160;G. <italic toggle=\"yes\">et al.</italic> Mobilenets: Efficient convolutional neural networks for mobile vision applications. preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1704.04861\">arXiv:1704.04861</ext-link> (2017).</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In <italic toggle=\"yes\">Proceedings of the IEEE conference on computer vision and pattern recognition</italic> 4510&#8211;4520 (2018).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y</given-names></name></person-group><article-title>Quality assessment of abdominal ct images: an improved resnet algorithm with dual-attention mechanism</article-title><source>Am. J. Transl. Res.</source><year>2024</year><volume>16</volume><fpage>3099</fpage><pub-id pub-id-type=\"doi\">10.62347/WKNS8633</pub-id><pub-id pub-id-type=\"pmid\">39114678</pub-id><pub-id pub-id-type=\"pmcid\">PMC11301486</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Zhu, B. &amp; Yang, Y. Quality assessment of abdominal ct images: an improved resnet algorithm with dual-attention mechanism. <italic toggle=\"yes\">Am. J. Transl. Res.</italic><bold>16</bold>, 3099 (2024).<pub-id pub-id-type=\"pmid\">39114678</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.62347/WKNS8633</pub-id><pub-id pub-id-type=\"pmcid\">PMC11301486</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Maglogiannis</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Zafiropoulos</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Anagnostopoulos</surname><given-names>I</given-names></name></person-group><article-title>An intelligent system for automated breast cancer diagnosis and prognosis using svm based classifiers</article-title><source>Appl. Intell.</source><year>2009</year><volume>30</volume><fpage>24</fpage><lpage>36</lpage><pub-id pub-id-type=\"doi\">10.1007/s10489-007-0073-z</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Maglogiannis, I., Zafiropoulos, E. &amp; Anagnostopoulos, I. An intelligent system for automated breast cancer diagnosis and prognosis using svm based classifiers. <italic toggle=\"yes\">Appl. Intell.</italic><bold>30</bold>, 24&#8211;36 (2009).</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>T</given-names></name></person-group><article-title>Ci-net: Clinical-inspired network for automated skin lesion recognition</article-title><source>IEEE Trans. Med. Imaging</source><year>2022</year><volume>42</volume><fpage>619</fpage><lpage>632</lpage><pub-id pub-id-type=\"doi\">10.1109/TMI.2022.3215547</pub-id><pub-id pub-id-type=\"pmid\">36279355</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Liu, Z., Xiong, R. &amp; Jiang, T. Ci-net: Clinical-inspired network for automated skin lesion recognition. <italic toggle=\"yes\">IEEE Trans. Med. Imaging</italic><bold>42</bold>, 619&#8211;632 (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2022.3215547</pub-id><pub-id pub-id-type=\"pmid\">36279355</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gedeon</surname><given-names>KK</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name></person-group><article-title>Classification of liver lesions in ct images based on livlesionet, modified multi-scale cnn with bridge scale method</article-title><source>Multimedia Tools Appl.</source><year>2024</year><volume>83</volume><fpage>8911</fpage><lpage>8929</lpage><pub-id pub-id-type=\"doi\">10.1007/s11042-023-15966-x</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Gedeon, K. K. &amp; Liu, Z. Classification of liver lesions in ct images based on livlesionet, modified multi-scale cnn with bridge scale method. <italic toggle=\"yes\">Multimedia Tools Appl.</italic><bold>83</bold>, 8911&#8211;8929 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Manjula, P., Krishnakumar, K., Gl, S., Pandiaraj, S. &amp; Prakash, M. A novel method for detecting liver tumors combining machine learning with medical imaging in ct scans using resunet. In <italic toggle=\"yes\">2024 International Conference on Integrated Circuits and Communication Systems (ICICACS)</italic> 1&#8211;5 (IEEE, 2024).</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Malik</surname><given-names>MGA</given-names></name><name name-style=\"western\"><surname>Saeed</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Shehzad</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Iqbal</surname><given-names>M</given-names></name></person-group><article-title>Def-swine2net: Dual enhanced features guided with multi-model fusion for brain tumor classification using preprocessing optimization</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>100</volume><fpage>107079</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.107079</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Malik, M. G. A., Saeed, A., Shehzad, K. &amp; Iqbal, M. Def-swine2net: Dual enhanced features guided with multi-model fusion for brain tumor classification using preprocessing optimization. <italic toggle=\"yes\">Biomed. Signal Process. Control</italic><bold>100</bold>, 107079 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><citation-alternatives><element-citation id=\"ec-CR37\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pan</surname><given-names>H</given-names></name><etal/></person-group><article-title>A lightweight model for the retinal disease classification using optical coherence tomography</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>101</volume><fpage>107146</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.107146</pub-id></element-citation><mixed-citation id=\"mc-CR37\" publication-type=\"journal\">Pan, H. et al. A lightweight model for the retinal disease classification using optical coherence tomography. <italic toggle=\"yes\">Biomed. Signal Process. Control</italic><bold>101</bold>, 107146 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR38\"><label>38.</label><citation-alternatives><element-citation id=\"ec-CR38\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tanveer</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Akram</surname><given-names>MU</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>AM</given-names></name></person-group><article-title>Transnetv: an optimized hybrid model for enhanced colorectal cancer image classification</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>96</volume><fpage>106579</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.106579</pub-id></element-citation><mixed-citation id=\"mc-CR38\" publication-type=\"journal\">Tanveer, M., Akram, M. U. &amp; Khan, A. M. Transnetv: an optimized hybrid model for enhanced colorectal cancer image classification. <italic toggle=\"yes\">Biomed. Signal Process. Control</italic><bold>96</bold>, 106579 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mahmood</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Wahid</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Hong</surname><given-names>JS</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>SG</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>KR</given-names></name></person-group><article-title>A novel convolution transformer-based network for histopathology-image classification using adaptive convolution and dynamic attention</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>135</volume><fpage>108824</fpage><pub-id pub-id-type=\"doi\">10.1016/j.engappai.2024.108824</pub-id></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Mahmood, T., Wahid, A., Hong, J. S., Kim, S. G. &amp; Park, K. R. A novel convolution transformer-based network for histopathology-image classification using adaptive convolution and dynamic attention. <italic toggle=\"yes\">Eng. Appl. Artif. Intell.</italic><bold>135</bold>, 108824 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR40\"><label>40.</label><citation-alternatives><element-citation id=\"ec-CR40\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cheng</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title>Eg-unetr: an edge-guided liver tumor segmentation network based on cross-level interactive transformer</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>97</volume><fpage>106739</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.106739</pub-id></element-citation><mixed-citation id=\"mc-CR40\" publication-type=\"journal\">Cheng, D., Zhou, Z. &amp; Zhang, J. Eg-unetr: an edge-guided liver tumor segmentation network based on cross-level interactive transformer. <italic toggle=\"yes\">Biomed. Signal Process. Control</italic><bold>97</bold>, 106739 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Zhao, L. <italic toggle=\"yes\">et al.</italic> A hybrid cnn-transformer for focal liver lesion classification. In <italic toggle=\"yes\">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic> 13001&#8211;13005 (IEEE, 2024).</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><citation-alternatives><element-citation id=\"ec-CR42\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Touvron</surname><given-names>H</given-names></name><etal/></person-group><article-title>Resmlp: Feedforward networks for image classification with data-efficient training</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>5314</fpage><lpage>5321</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2022.3206148</pub-id><pub-id pub-id-type=\"pmid\">36094972</pub-id></element-citation><mixed-citation id=\"mc-CR42\" publication-type=\"journal\">Touvron, H. et al. Resmlp: Feedforward networks for image classification with data-efficient training. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>45</bold>, 5314&#8211;5321 (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2022.3206148</pub-id><pub-id pub-id-type=\"pmid\">36094972</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>So</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Le</surname><given-names>QV</given-names></name></person-group><article-title>Pay attention to mlps</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>9204</fpage><lpage>9215</lpage></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Liu, H., Dai, Z., So, D. &amp; Le, Q. V. Pay attention to mlps. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>34</bold>, 9204&#8211;9215 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Li, J., Hassani, A., Walton, S. &amp; Shi, H. Convmlp: Hierarchical convolutional mlps for vision. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 6307&#8211;6316 (2023).</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><citation-alternatives><element-citation id=\"ec-CR45\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>S</given-names></name></person-group><article-title>Mlp-based classification of covid-19 and skin diseases</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>228</volume><fpage>120389</fpage><pub-id pub-id-type=\"doi\">10.1016/j.eswa.2023.120389</pub-id><pub-id pub-id-type=\"pmid\">37193247</pub-id><pub-id pub-id-type=\"pmcid\">PMC10170962</pub-id></element-citation><mixed-citation id=\"mc-CR45\" publication-type=\"journal\">Zhang, R., Wang, L., Cheng, S. &amp; Song, S. Mlp-based classification of covid-19 and skin diseases. <italic toggle=\"yes\">Expert Syst. Appl.</italic><bold>228</bold>, 120389 (2023).<pub-id pub-id-type=\"pmid\">37193247</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.eswa.2023.120389</pub-id><pub-id pub-id-type=\"pmcid\">PMC10170962</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><label>46.</label><mixed-citation publication-type=\"other\">Wang, B. <italic toggle=\"yes\">et al.</italic> Ma-net: A mlp-based attentional deep network for segmentation of liver tumor ablation region from 2d ultrasound image. In <italic toggle=\"yes\">Proceedings of the 2024 7th International Conference on Image and Graphics Processing</italic> 62&#8211;66 (2024).</mixed-citation></ref><ref id=\"CR47\"><label>47.</label><mixed-citation publication-type=\"other\">Zhu, L., Wang, X., Ke, Z., Zhang, W. &amp; Lau, R.&#160;W. Biformer: Vision transformer with bi-level routing attention. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic> 10323&#8211;10333 (2023).</mixed-citation></ref><ref id=\"CR48\"><label>48.</label><citation-alternatives><element-citation id=\"ec-CR48\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ren</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Z</given-names></name></person-group><article-title>Conv-sdmlpmixer: a hybrid medical image classification network based on multi-branch cnn and multi-scale multi-dimensional mlp</article-title><source>Inf. Fusion</source><year>2025</year><volume>2025</volume><fpage>102937</fpage><pub-id pub-id-type=\"doi\">10.1016/j.inffus.2025.102937</pub-id></element-citation><mixed-citation id=\"mc-CR48\" publication-type=\"journal\">Ren, Z., Liu, S., Wang, L. &amp; Guo, Z. Conv-sdmlpmixer: a hybrid medical image classification network based on multi-branch cnn and multi-scale multi-dimensional mlp. <italic toggle=\"yes\">Inf. Fusion</italic><bold>2025</bold>, 102937 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR49\"><label>49.</label><mixed-citation publication-type=\"other\">Chongjian, G. <italic toggle=\"yes\">et al.</italic> Advancing vision transformers with group-mix attention. preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2311.15157\">arXiv:2311.15157</ext-link> (2023).</mixed-citation></ref><ref id=\"CR50\"><label>50.</label><citation-alternatives><element-citation id=\"ec-CR50\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Yue</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Multi-branch cnn and grouping cascade attention for medical image classification</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>15013</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-64982-w</pub-id><pub-id pub-id-type=\"pmid\">38951526</pub-id><pub-id pub-id-type=\"pmcid\">PMC11217469</pub-id></element-citation><mixed-citation id=\"mc-CR50\" publication-type=\"journal\">Liu, S., Yue, W., Guo, Z. &amp; Wang, L. Multi-branch cnn and grouping cascade attention for medical image classification. <italic toggle=\"yes\">Sci. Rep.</italic><bold>14</bold>, 15013 (2024).<pub-id pub-id-type=\"pmid\">38951526</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-64982-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC11217469</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR51\"><label>51.</label><citation-alternatives><element-citation id=\"ec-CR51\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Yue</surname><given-names>W</given-names></name></person-group><article-title>An efficient medical image classification network based on multi-branch cnn, token grouping transformer and mixer mlp</article-title><source>Appl. Soft Comput.</source><year>2024</year><volume>153</volume><fpage>111323</fpage><pub-id pub-id-type=\"doi\">10.1016/j.asoc.2024.111323</pub-id></element-citation><mixed-citation id=\"mc-CR51\" publication-type=\"journal\">Liu, S., Wang, L. &amp; Yue, W. An efficient medical image classification network based on multi-branch cnn, token grouping transformer and mixer mlp. <italic toggle=\"yes\">Appl. Soft Comput.</italic><bold>153</bold>, 111323 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR52\"><label>52.</label><mixed-citation publication-type=\"other\">Yu, W., Zhou, P., Yan, S. &amp; Wang, X. Inceptionnext: When inception meets convnext. In <italic toggle=\"yes\">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</italic> 5672&#8211;5683 (2024).</mixed-citation></ref><ref id=\"CR53\"><label>53.</label><mixed-citation publication-type=\"other\">Chen, J. <italic toggle=\"yes\">et al.</italic> Run, don&#8217;t walk: chasing higher flops for faster neural networks. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic> 12021&#8211;12031 (2023).</mixed-citation></ref><ref id=\"CR54\"><label>54.</label><mixed-citation publication-type=\"other\">Ding, X. <italic toggle=\"yes\">et al.</italic> Repvgg: Making vgg-style convnets great again. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</italic> 13733&#8211;13742 (2021).</mixed-citation></ref><ref id=\"CR55\"><label>55.</label><mixed-citation publication-type=\"other\">Tan, M. &amp; Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In <italic toggle=\"yes\">International conference on machine learning</italic> 6105&#8211;6114 (PMLR, 2019).</mixed-citation></ref><ref id=\"CR56\"><label>56.</label><mixed-citation publication-type=\"other\">Chen, C.-F.&#160;R., Fan, Q. &amp; Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF international conference on computer vision</italic> 357&#8211;366 (2021).</mixed-citation></ref><ref id=\"CR57\"><label>57.</label><mixed-citation publication-type=\"other\">Ding, M. <italic toggle=\"yes\">et al.</italic> Davit: Dual attention vision transformers. In <italic toggle=\"yes\">European conference on computer vision</italic> 74&#8211;92 (Springer, 2022).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12678529 PMC12678529.1 12678529 12678529 41331299 10.1038/s41598-025-17581-2 17581 1 Article A novel liver image classification network for accurate diagnosis of liver diseases He Xiaolei 1 Wang Xilong 2 Wang Yan 1 Li Hui 1 Liu Shuo 1 Wang Jun 1 Feng Yan 1 Wang Qi 3 Chen Jie lamal2000@163.com 1 1 https://ror.org/02r247g67 grid.410644.3 Radiographic Image Center, People&#8217;s Hospital of Xinjiang Uygur Autonomous Region, &#220;r&#252;mqi, Xinjiang 830001 China 2 https://ror.org/01p455v08 grid.13394.3c 0000 0004 1799 3993 Graduate School, Xinjiang Medical University, &#220;r&#252;mqi, 830017 Xinjiang China 3 https://ror.org/01p455v08 grid.13394.3c 0000 0004 1799 3993 PCR Biology Laboratory, Xinjiang Medical University, &#220;r&#252;mqi, 830000 Xinjiang China 2 12 2025 2025 15 478255 43121 18 6 2025 25 8 2025 02 12 2025 06 12 2025 06 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . In medical imaging diagnosis, the identification of normal liver, fatty liver, and cirrhosis is often challenging due to subtle morphological and density differences. Previous studies have used CNNs, MLPs or transformers to extract lesion features. However, CNN&#8217;s global representation is limited, while MLPs and transformers have insufficient local modeling, resulting in insufficient lesion information mining. Therefore, this article proposes a hybrid network CMT-Net, which unifies the local perception of CNNs, high-dimensional mapping of MLPs, and global dependencies of Transformers into a single architecture, significantly improving the accuracy of CT liver three classification. The core components of CMT-Net include an efficient transformer (ET) module, which focuses on extracting local feature details while progressively integrating global information, significantly enhancing the model feature representation and generalization capabilities. Additionally, this paper introduces a Hybrid MLP (HM) module that combines Token-Mixing MLP and Channel-Mixing MLP to achieve deep fusion of spatial and channel information, further improving feature extraction. To validate the proposed algorithm, extensive experiments were conducted on a dataset of three liver diseases collected from the Imaging Department of Urumqi People&#8217;s Hospital. The results demonstrate that CMT-Net achieves outstanding performance in classifying normal liver, fatty liver, and cirrhosis. These findings not only provide an effective tool for precise liver disease diagnosis but also offer new directions for deep learning model design in medical image classification tasks. Subject terms Computer science Information technology The Hospital Level Project 20230118 of Xinjiang Uygur Autonomous Region People&#8217;s Hospital 20230118 20230118 20230118 20230118 20230118 20230118 20230118 20230118 20230118 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Liver diseases are among the leading global health concerns, with fatty liver disease and cirrhosis being two common chronic liver conditions &#160; 1 . Fatty liver disease results from excessive fat accumulation in the liver, while cirrhosis is the outcome of long-term liver damage leading to fibrosis and nodular regeneration &#160; 2 . Normal liver, fatty liver, and cirrhosis represent different stages of liver disease progression, with fatty liver serving as a critical intermediate stage between normal liver and cirrhosis&#160; 3 . Understanding their interrelationships is essential for prevention, diagnosis, and treatment. In early-stage fatty liver, imaging modalities such as ultrasound and CT may reveal only minor changes in liver morphology and density, making differentiation from normal liver difficult&#160; 4 . Due to limited experience or insufficient imaging resolution, clinicians may misdiagnose early fatty liver as normal, delaying timely intervention. Similarly, early cirrhosis may exhibit subtle density variations in CT images, resembling mild fatty liver, leading to potential misclassification&#160; 5 . Recent advances in deep learning have significantly enhanced the role of medical image classification algorithms in liver disease diagnosis. Notably, CNNs have demonstrated remarkable advantages in liver disease classification&#160; 6 . First, CNNs can automatically extract lesion features from images without relying on handcrafted feature extraction methods. Through localized convolutional operations, CNNs effectively capture fine-grained details in CT images, such as uniform texture in normal liver, density variations in fatty liver, and nodular patterns in cirrhosis&#160; 7 . Second, CNN-based models can learn complex nonlinear mappings through training, enabling high-accuracy classification. Furthermore, transfer learning and data augmentation techniques mitigate the challenge of limited medical image data&#160; 8 , allowing deep learning models to perform well even with small datasets. Addressing the issues of effective deep features in liver cancer CT images within CNNs but limited global generalization ability, and fast training speed of Kernel Extreme Learning Machines (KELM) but sensitive kernel function selection, Jesi et al.&#160; 9 proposed an end-to-end &#8220;Differential CNN-KELM&#8221; framework for liver cancer recognition. This algorithm combines the advantages of CNNs and KELM to enhance recognition accuracy and training efficiency. Addressing the issue of liver fibrosis, Abinaya et al.&#160; 10 developed a BiLSTM-CNN hybrid network. This network adopts a serial strategy of &#8220;CNN capturing local features and BiLSTM capturing long-range dependencies&#8221;, achieving an accuracy rate of over 96% in fibrosis staging within a lightweight parameter budget. It provides an efficient and practical deep learning backbone for rapid and non-invasive clinical assessment. Although CNNs have achieved remarkable results in liver disease recognition, their inherent limitations cannot be ignored. Limited by their local receptive fields, CNNs struggle to effectively capture lesion features across different levels or global scope, leading to missed diagnoses of diffuse or small metastatic lesions. In recent years, transformer have achieved great success in natural language processing (NLP), have been increasingly applied to computer vision and medical image analysis &#160; 11 . The self-attention mechanism in transformers enables efficient modeling of global image information, making them particularly suitable for medical image classification &#160; 12 , 13 . In liver disease imaging, transformer can simultaneously analyze overall liver morphology, spatial relationships between lesions and surrounding tissues, and long-range dependencies, improving classification accuracy for complex cases. Additionally, transformer offer architectural flexibility, facilitating extensions and improvements (e.g., TransUNet&#160; 14 , Swin Transformer&#160; 15 ). Addressing the two major issues of insufficient global context in traditional U-Net and excessively large number of parameters in pure Transformer, Ou et al.&#160; 16 proposed a lightweight CNN-Transformer hybrid network to achieve precise and end-to-end automatic segmentation of liver and tumors in CT images. Addressing the issues of pure CNNs or pure Vision Transformers (ViTs), such as difficulty in balancing local and global features, sensitivity to hyperparameters, and susceptibility to overfitting, Kumar et al.&#160; 17 utilized the Grey Wolf Optimization (GWO) algorithm to perform end-to-end tuning on the Swin Transformer-UNet hybrid network, providing a fully automated liver CT segmentation solution that can be deployed immediately for clinical use. Although Transformers have demonstrated excellent global modeling capabilities in liver disease classification, certain issues still persist. Firstly, the self-attention mechanism lacks sufficient focus on fine-grained textures in small liver lesions and low-contrast areas, which can easily lead to missed detection of early or small-volume liver lesions. Secondly, pure Transformers have a large number of parameters, making it difficult to run them in real-time on conventional Picture Archiving and Communication System(PACS) or edge devices&#160; 18 . Fig. 1 Visualization of network parameters and accuracy. Moreover, the emergence of MLP-Mixer, a pure multilayer perceptron (MLP)-based architecture, has demonstrated competitive performance in image classification benchmarks, reigniting interest in MLPs &#160; 19 . Like transformer, MLPs possess global modeling capabilities absent in CNNs, enabling holistic feature extraction from medical images&#160; 20 . MLPs also benefit from positional priors, allowing them to leverage spatial information for tasks such as identifying irregular nodule distributions in cirrhosis. CNNs and ViTs primarily focus on improving segmentation accuracy, often neglecting issues such as segmentation speed. Yang et al.&#160; 21 achieved end-to-end precise segmentation of CT liver tumors using a &#8220;lightweight Tokenized-MLP + dual attention&#8221; approach. In summary, although MLP has advantages in global modeling and positional prior knowledge in liver disease classification, it is still constrained by key bottlenecks such as poor geometric robustness, large parameter count, sensitivity to noise, overfitting in small sample sizes, and insufficient interpretability. Fig. 2 Visualization of network FLOPs and accuracy. To address these limitations and leverage the imaging characteristics of normal liver, fatty liver, and cirrhosis in CT, we propose CMT-Net, a hybrid medical image classification algorithm integrating CNN, MLP, and transformer modules. CMT-Net employs a hierarchical single-branch serial design, incorporating a Hybrid MLP (HM) module and an Efficient Transformer (ET) module. The ET module uses Grouping Cascade Attention (GCA) to partition feature maps into groups, focusing on local details while progressively integrating global information. The HM module combines Token-Mixing MLP and Channel-Mixing MLP to fuse spatial and channel information, enhancing feature representation. In the shallow stages (Stages 1&#8211;3), CNN and ET modules extract features, while the intermediate stage (Stage 4) employs the HM module. Deeper stages (Stages 5&#8211;6) reuse CNN and ET modules to balance local and global feature extraction. Extensive experiments on three liver datasets demonstrate that CMT-Net outperforms existing CNN, ViT, and hybrid-based methods in classification accuracy while maintaining low computational costs (Figs. 1 , 2 ). In summary, our contributions are as follows: We propose an Efficient Transformer (ET) module with a Grouping Cascade Attention (GCA) mechanism that partitions feature maps into groups and processes them in a cascaded manner, enhancing feature representation and generalization. We design a Hybrid MLP (HM) module that integrates Token-Mixing MLP and Channel-Mixing MLP to fuse spatial and channel information effectively. We introduce CMT-Net, a hybrid medical image classification network combining CNN, MLP, and transformer in a single-branch multi-stage architecture. Experiments on three liver datasets show that CMT-Net achieves state-of-the-art classification performance with fewer parameters and FLOPs. Related work Through an in-depth review of existing literature and technological advancements, we observe that while CNNs&#160; 22 , transformers, and multilayer perceptrons (MLPs) have been widely applied in medical image processing, research on algorithms for CT-based liver disease classification remains relatively limited. To advance this field, we systematically analyze existing deep learning-based medical image classification methods, aiming to provide more effective solutions for precise liver disease diagnosis. CNN-based methods Since the widespread adoption of AlexNet&#160; 23 , VGG&#160; 24 , GoogleNet&#160; 25 , ResNet&#160; 26 , and DenseNet&#160; 27 in natural image processing, these milestone achievements have also provided robust technical support for medical image analysis. LitefusionNet&#160; 28 is a lightweight feature fusion network designed to enhance medical image classification performance through intelligent feature fusion. By combining the strengths of MobileNet&#160; 29 and MobileNetV2&#160; 30 architectures, it improves feature discriminability and classification accuracy across diverse medical image datasets. Zhu et al. &#160; 31 proposed an improved ResNet algorithm with a dual-attention mechanism (channel and spatial attention), significantly boosting classification performance while reducing overfitting in abdominal CT image quality assessment. Maglogiannis et al. &#160; 32 , introduced a healthcare framework for breast cancer diagnosis that fuses deep learning architectures with optimized algorithms, achieving higher diagnostic accuracy by combining medical imaging and clinical data. CI-Net &#160; 33 mimics dermatologists&#8217; diagnostic workflows through three modules: a Lesion Area Attention Module to focus on central regions, a Feature Extraction Module, and a Lesion Feature Attention Module to correlate critical local features. This design significantly improves skin lesion recognition accuracy, outperforming existing methods on benchmark datasets. To alleviate the problems of small sample overfitting and scale mismatch, Gedeon et al.&#160; 34 proposed a lightweight LivlesioNet. This network uses parallel 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 3, 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 5, and 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 7 dynamic convolutions to capture multi-scale lesion features, and uses lightweight attention gating to refine fusion, achieving efficient four classification of liver cancer, hemangioma, metastatic lesions, and normal liver. Manjula et al.&#160; 35 proposed an end-to-end CT liver tumor detection framework. On the one hand, ResUNet is used to segment tumor regions, and on the other hand, the segmentation results are input into a random forest classifier along with radiomics features to achieve precise localization and benign/malignant discrimination of lesions. Although CNNs have made significant breakthroughs in the field of liver disease recognition, their application still faces several key challenges. Firstly, CNNs are prone to insufficient generalization performance in small sample data scenarios. Secondly, although convolutional kernels can effectively extract local features, they are difficult to establish global semantic associations; On the other hand, fixed scale convolution operations limit the adaptability of the model to multi-scale lesions, resulting in significant differences in its sensitivity to identifying lesions of different sizes. Transformer-based methods The ViTs demonstrated exceptional performance in natural image classification by capturing global context via self-attention mechanisms&#8211;a capability lacking in CNNs local receptive fields. This advantage has spurred interest in transformer-based medical image classification. For instance, DEF-SwinE2NET &#160; 36 integrates swin transformer (global features) and efficientNet-B0 (local features) for brain tumor classification, employing feature fusion to enhance discriminative power. In retinal disease classification, proposed a lightweight model using OCT images, where adaptive pooling and attention mechanisms optimize both accuracy and clinical applicability&#160; 37 . TransNetV&#160; 38 hybridizes CNNs and transformers for colorectal cancer classification, leveraging CNN-extracted local features and transformer-based global context understanding. Tahir et al. &#160; 39 introduced adaptive convolution (dynamic kernel adjustment) and dynamic attention mechanisms in a convolution-transformer network, achieving state-of-the-art performance in histopathology image classification. To solve the problem of blurred edges and over segmentation of small tumors in CT liver tumors, Cheng et al.&#160; 40 proposed EG-UNETR. Firstly, cross layer skip connections inject multi head attention to capture multi-scale context and denoise it. Secondly, parallel dilated convolution and mixed attention enhance semantics. Finally, edge guided attention is used to refine contours and achieve sub-pixel level segmentation. Zhao et al.&#160; 41 addressed the challenges of significant differences in the scale and class imbalance of liver lesions, limited receptive field of pure CNN, and excessive number of pure ViT parameters. They used lightweight CNN to capture local textures and micro ViT to obtain global context, and combined the two to achieve SOTA accuracy and interpretability in small sample CT five classification tasks. Although Transformer has outstanding global modeling capabilities, it still has limitations in liver disease classification. Firstly, Transformer lacks a variable receptive field, making it difficult to adaptively capture multi-scale features of liver lesions. Secondly, the fixed patch mechanism of Transformer is prone to losing sub-pixel level small tumor details. MLP-based methods MLP-Mixer pioneered an all-MLP architecture for vision tasks&#160; 18 , replacing convolutions and attention with two MLP types: Channel-Mixing MLPs and Token-Mixing MLPs. This design reduces computational costs while maintaining performance on large-scale datasets. ResMLP &#160; 42 further simplified image classification by independently processing patch interactions across channels and proposed &#8220;Class-MLP&#8221; to replace average pooling. The gMLP architecture eliminated self-attention &#160; 43 , using gated MLPs with parameterized spatial-channel projections instead. It scales comparably to transformers with increasing data and compute resources. Li et al.&#160; 44 achieved hierarchical multi-scale representations via lightweight convolutional MLPs, while S2-MLP &#160; 45 replaced token-mixing MLPs with spatial-shift operations, improving efficiency without sacrificing accuracy. Wang et al.&#160; 46 proposed a lightweight full MLP architecture MA Net for real-time segmentation of 2D ultrasound liver cancer ablation areas. Firstly, MA Net divides the image into multi-level tokens. Then, the cascaded MLP Mixer captures local global features in the image. Finally, channel space lightweight attention suppresses artifacts and sharpens ablation boundaries. Although MLP has good scalability in visual tasks due to its fully connected structure, liver disease classification still exposes its deep shortcomings. Firstly, the lack of local receptive fields in MLP results in limited ability to capture the subtle structures of sub-pixel lesions. Secondly, the static weights mixed with tokens are difficult to adapt to the multi-scale changes of lesions. Finally, the efficiency of hierarchical feature fusion is low, and the overall classification accuracy is limited by structural bottlenecks. Methodology This section details the structure and functionality of each module in CMT-Net. Section 3.1 outlines the overall architecture, followed by explanations of the Efficient Transformer (ET) and Hybrid MLP (HM) modules in Sections 3.2 and 3.3, respectively. Section 3.4 introduces the loss function. Overall architecture Figure 3 illustrates the CMT-Net architecture, comprising four key modules: Feature Mining Block (FMB), Efficient Transformer (ET), Hybrid MLP (HM), and Multi-scale Bottleneck Convolution (MBC). The input image resolution is set to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$X_{in} \\in R^{3\\times 224\\times 224}$$\\end{document} . Given the high proportion of background information in raw medical images, Stage 1(Stem) employs a \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$3\\times 3$$\\end{document} convolutional kernel for preliminary feature extraction. This design efficiently suppresses background interference while preserving lesion characteristics. 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} X_1 = Conv2d\\left( X_{i n}\\right) \\quad X_1 \\in R^{24 \\times 112 \\times 112} ; Conv2d \\in \\mathbb {R}^{(in=3 ,out =24, kernel =3, stride =2, padding =1)} \\end{aligned}$$\\end{document} Here, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$X_1 \\in R^{24 \\times 112 \\times 112}$$\\end{document} represents the feature map output by Stage1 (Stem). \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Conv2d \\in \\mathbb {R}^{(in=3,out =24, kernel =3, stride =2, padding =1)}$$\\end{document} represents the specific parameters of the convolution kernel. In Stage 2, the FMB module further analyzes features using sequential \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$3\\times 3$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1\\times 1$$\\end{document} convolutions, coupled with the SiLU activation function. SiLU optimizes gradient propagation during training and retains partial negative inputs, enhancing feature discriminability. 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{2 \\_1}=B N\\left( Conv2d\\left( X_1\\right) \\right) \\quad X_{2 \\_1} \\in R^{64 \\times 112 \\times 112}; Conv2d \\in \\mathbb {R}^{(in =24, out =64, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document} 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{2 \\_2} = SiLU\\left( X_{2 \\_1}\\right) \\quad X_{2 \\_2} \\in R^{64 \\times 112 \\times 112} \\end{aligned}$$\\end{document} 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{2}=B N\\left( Conv2d\\left( X_{2 \\_2}\\right) \\right) \\quad X_{2} \\in R^{64 \\times 112 \\times 112}; Conv2d \\in \\mathbb {R}^{(in =64, out =64, kernel =1, stride =1, padding =0)} \\end{aligned}$$\\end{document} Stage 3 integrates the proposed ET module, which combines local feature extraction with progressive global information fusion (Section 3.2). This significantly improves feature representation and model generalization. 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} X_3=E T\\left( X_2\\right) \\quad X_3 \\in R^{128 \\times 56 \\times 56} \\end{aligned}$$\\end{document} Stage 4 introduces the HM module (Section 3.3), where Token-Mixing MLP and Channel-Mixing MLP synergistically fuse spatial and channel information, refining feature extraction. 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} X_4=HM\\left( X_4\\right) \\quad X_4 \\in R^{256 \\times 28 \\times 28} \\end{aligned}$$\\end{document} In Stage 5, the MBC module leverages multi-scale convolutions to enhance lesion representation. As network depth increases, lesion features dominate the feature maps. MBC hierarchical convolutions capture fine-grained details while enriching feature diversity. 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{5 \\_1}=SiLU\\left( Conv2d\\left( X_4\\right) \\right) \\quad X_{5 \\_1} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =256, out =512, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document} 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{5 \\_2}=Sigmoid\\left( Conv2d\\left( X_{5 \\_1}\\right) \\right) \\quad X_{5 \\_2} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =512, out =512, kernel =1, stride =1, padding =0)} \\end{aligned}$$\\end{document} 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; X_{5}=BN\\left( Conv2d\\left( X_{5 \\_2}\\right) \\right) \\quad X_{5 \\_2} \\in R^{512 \\times 28 \\times 28}; Conv2d \\in \\mathbb {R}^{(in =512, out =512, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document} Finally, Stage 6 reapplies the ET module to consolidate local-global feature interactions, boosting classification accuracy. 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} X_6=E T\\left( X_5\\right) \\quad X_6 \\in R^{1024 \\times 14 \\times 14} \\end{aligned}$$\\end{document} Fig. 3 Example of the overall network architecture of CMT-Net. CMT-Net includes FMB, ET, HM, and MBC modules. Efficient transformer (ET) module As shown in Fig. 4 , the ET module mainly uses patch embedding (composed of convolution kernels), feed forward network (composed of convolution kernels) and GCA module to extract local and global information. First, we introduce the overall workflow of ET module. Next, we will introduce the specific workflow of GCA module in detail. Assume that the size of the feature map input to the ET module is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_{in} \\in R^{C\\times H\\times W}.$$\\end{document} Fig. 4 Flowchart of the ET module. Here, the Patch Embedding and Feed Forward Network utilize convolutional kernels for local feature extraction, while the GCA module is primarily responsible for global information extraction. First, the Patch Embedding module extracts the local details in the input feature map through convolution operation. 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F_{1}=B N\\left( Conv2d\\left( F_{in}\\right) \\right) \\quad F_{1} \\in R^{C \\times H \\times W} ; Conv2d \\in \\mathbb {R}^{(in =C, out = C, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document} Then, the Feed Forward Network maps the number of channels of the feature graph to the target dimension through convolution operation. 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F_{2}=B N\\left( Conv2d\\left( F_{1}\\right) \\right) \\quad F_{2} \\in R^{2C \\times (H/2) \\times (W/2)} ; Conv2d \\in \\mathbb {R}^{(in =C, out = 2C, kernel =1, stride =2, padding =0)} \\end{aligned}$$\\end{document} Next, the feature map is input into the GCA module to extract the global information. 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F_3=GCA\\left( F_2\\right) \\quad F_3 \\in R^{2C \\times (H/2) \\times (W/2)} \\end{aligned}$$\\end{document} Finally, the feature map is extracted by Patch Embedding and Feed Forward Network modules respectively. 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F_{out} = B N\\left( Conv2d_{-} 2\\left( B N\\left( Conv2 d_{-} 1\\left( F_3\\right) \\right) \\right) \\right) \\quad F_{out} \\in R^{2C \\times (H/2) \\times (W/2)} \\end{aligned}$$\\end{document} Among them, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Conv2d_{-} 1 \\in \\mathbb {R}^{(in =2C, out = 2C, kernel =3, stride =1, padding =1)}$$\\end{document} is the convolution kernel in Patch Embedding. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Conv2d_{-} 2 \\in \\mathbb {R}^{(in =2C, out = 2C, kernel =1, stride =1, padding =0)}$$\\end{document} is the convolution kernel in the Feed Forward Network. Next, we will explain the specific workflow of GCA in ET module in detail. To facilitate readers&#8217; understanding, we assume that the size of the feature map input into GCA is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$T_{in} \\in R^{C\\times H\\times W}$$\\end{document} . First, the input feature map is evenly divided into 4 parts along the channel dimension. 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Head^i = Split(T_{in}) \\quad 1\\le i \\le 4; \\ Head^i \\in R^{(C/4)\\times H\\times W} \\end{aligned}$$\\end{document} To reduce the computational complexity in the subsequent weight coefficient solving process, we ingeniously perform regional division on the feature map in the spatial dimension. 16 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Head^i_j = Split(Head^i) \\quad 1\\le i \\le 4; 1\\le j \\le (H/2); Head^i_j \\in R^{(C/4)\\times j \\times j} \\end{aligned}$$\\end{document} Here, the spatial dimension \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$j\\times j$$\\end{document} of each divided feature map block is denoted as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Head^i_j$$\\end{document} . Next, each feature map block is individually fed into the self-attention mechanism to compute the pixel-wise correlations. The specific formula is as follows: 17 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; Q^i_j = Reshape(Conv2d(Head^i_j)) \\quad Q^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1}; Conv2d \\in \\mathbb {R}^{(in =(C/4), out = (C/4), kernel =5, stride =1, padding =2)} \\end{aligned}$$\\end{document} 18 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; K^i_j = Reshape(Head^i_j) \\quad K^i_j \\in R^{(C/4)\\times 1\\times (j\\times j)} \\end{aligned}$$\\end{document} 19 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; V^i_j = Reshape(Head^i_j) \\quad V^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1} \\end{aligned}$$\\end{document} 20 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; Head^i_j = Self - Att\\{Q^i_j, K^i_j, V^i_j\\} \\quad Head^i_j \\in R^{(C/4)\\times (j\\times j)\\times 1} \\end{aligned}$$\\end{document} 21 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; Head^i_j = Reshape(Head^i_j) \\quad Head^i_j \\in R^{(C/4)\\times j\\times j} \\end{aligned}$$\\end{document} The output feature map blocks \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Head^i_j$$\\end{document} are then spatially concatenated to reconstruct a complete feature map \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Head^i$$\\end{document} that incorporates global information. 22 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Head^i = Concat(Head^i_j) \\quad Head^i \\in R^{(C/4)\\times H\\times W} \\end{aligned}$$\\end{document} Finally, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Head^i$$\\end{document} are concatenated along the channel dimension to produce the final feature map. 23 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} T_{Output} = Concat(Head^i) \\quad T_{Output} \\in \\mathbb {R}^{C\\times H\\times W} \\end{aligned}$$\\end{document} In summary, we propose an efficient feature processing module. First, the input feature map is evenly split into four parts along the channel dimension. To reduce computational complexity, we perform spatial partitioning on the feature maps. Each partitioned block is then fed into a self-attention mechanism to compute pixel-wise correlations accurately. The output blocks are spatially reassembled to form a complete feature map integrating global information. Finally, these maps are concatenated along the channel dimension to generate the final output. This pipeline not only optimizes computational efficiency but also enhances feature representation by focusing on local details and progressively integrating global context. Hybrid MLP (HM) module As shown in Fig. 5 , we illustrate the detailed working of the HM module. Suppose the size of the output feature is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_{in} \\in R^{C\\times H\\times W}$$\\end{document} . First, the input feature map passes through a convolutional layer to extract spatial features. Here, the convolution operation does not change the shape of the feature map, and the output feature map still maintains the shape of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_1 \\in R^{C\\times H\\times W}.$$\\end{document} 24 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Q_1 = Conv2d\\left( Q_{i n}\\right) \\quad Q_1 \\in R^{C\\times H \\times W} ; Conv2d \\in \\mathbb {R}^{(in=C ,out =C, kernel =3, stride =1, padding =1)} \\end{aligned}$$\\end{document} The convolved feature map then undergoes a transpose operation, transforming its shape from \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_1$$\\end{document} to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_2 \\in R^{(H \\times W) \\times C}.$$\\end{document} 25 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Q_2 = Transpose(Q_1) \\quad Q_2 \\in R^{(H\\times W)\\times C } \\end{aligned}$$\\end{document} Fig. 5 Flowchart of the HM module, which mainly consists of Channel Mixing MLP and Token Mixing MLP. After the transpose, the features of each channel are integrated into a vector, providing a basis for the Channel Mixing MLP to perform feature interaction and mixing between channels. Next, the transposed feature map is fed into the Channel Mixing MLP. Assuming the weights of the fully connected layer are \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$W_{fc}$$\\end{document} and the bias is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$b_{fc}$$\\end{document} , the output of the Channel Mixing MLP can be expressed as: 26 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Q_{fc} = \\sigma (W_{fc} \\cdot Q_2 + b_{fc}) \\quad Q_{fc} \\in R^{(H\\times W)\\times C }; W_{fc} \\in R^{C\\times C}; b_{fc} \\in R^{C\\times 1} \\end{aligned}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma$$\\end{document} represents the ReLU activation function. We apply the ReLU activation to introduce non-linearity, enabling the model to learn more complex feature representations. In summary, the computational formula for the Channel Mixing MLP can be expressed as: 27 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Q_{out} = \\sigma (W_{fc} \\cdot (Transpose(Conv2d(Q_{in}))) + b_{fc}) \\quad Q_{out} \\in R^{(H\\times W) \\times C } \\end{aligned}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_{in}$$\\end{document} denotes the input feature map, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q_{out}$$\\end{document} denotes the output feature map, Transpose (.) represents reshaping the feature map into a one-dimensional vector, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma$$\\end{document} denotes the activation function. Equation ( 26 ) equation summarizes the core computation of the Channel Mixing MLP, which achieves cross-channel information interaction and feature mixing through the combination of fully connected layers and activation functions. The feature graph output by the channel mixing MLP is first dimensionally transformed to meet the needs of the token mixing MLP. 28 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Y_{in} = Transpose(Q_{out}) \\quad Y_{in} \\in R^{ C \\times (H\\times W)} \\end{aligned}$$\\end{document} The transposed feature map is then fed into the Token Mixing MLP, which performs spatial feature mixing via fully connected layers. The input feature map has a shape of [ C ,&#160; N ], where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$N = H\\times W$$\\end{document} represents the total number of spatial elements and C denotes the number of channels. The computation of the Token Mixing MLP can be expressed as: 29 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Y_{ft} = \\sigma (W_{ft} \\cdot Y_{in} + b_{ft}) \\quad Y_{ft} \\in R^{ C\\times (H\\times W) }; W_{ft} \\in R^{(H\\times W)\\times (H\\times W)}; b_{ft} \\in R^{(H\\times W)\\times 1} \\end{aligned}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma$$\\end{document} is the GELU activation function. Similar to other activation functions, Gelu introduces nonlinearity to enable neural networks to learn and model more complex function mappings. Additionally, the GELU function is continuously differentiable and smooth, which facilitates stable gradient propagation during backpropagation, mitigating issues such as gradient vanishing or exploding. In summary, the computational formula for the Token Mixing MLP can be expressed as: 30 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Y_{out} = \\sigma (W_{ft} \\cdot (Transpose(Q_{out}) + b_{ft})) \\quad Y_{out} \\in R^{C \\times (H\\times W) } \\end{aligned}$$\\end{document} The Token-Mixing MLP enhances the model ability to capture spatial information by mixing features across the spatial dimension. This design allows features from each channel to interact across all spatial locations, thereby enriching the expressiveness of the features. Subsequently, dimensional transformation is performed on the feature graph to reconstruct and generate a complete feature graph. 31 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} P = Transpose(Y_{out}) \\quad P \\in R^{C \\times H \\times W} \\end{aligned}$$\\end{document} Finally, the final feature map is output through convolution operation. 32 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} P_{out}=Conv2d\\left( P\\right) \\quad P_{out} \\in R^{2C \\times (H/2) \\times (W/2)} ; Conv2d \\in \\mathbb {R}^{(in =C, out = 2C, kernel =3, stride =2, padding =1)} \\end{aligned}$$\\end{document} Experiments and analysis In this section, we validate the effectiveness and authenticity of the proposed method through extensive experiments and analyses. Section 4.1 introduces the three medical image classification datasets used for validation, including information such as the number of images and classification categories. Section 4.2 describes the relevant experimental settings, including the environment and hyperparameter configurations used to run the experiments. Section 4.3 presents the evaluation metrics employed in the experiments, along with a detailed explanation of the rationale for using these metrics. Section 4.4 demonstrates the results of the proposed method on three datasets and conducts comparative experiments with state-of-the-art models. Section 4.5 validates the effectiveness of the proposed method through ablation experiments. Datasets All medical images in this article were collected using GE Revolution 256 CT and Siemens Dual Source Spiral CT (Somatom Definition Flash) equipment from the Radiology Imaging Center of Xinjiang Uygur Autonomous Region People&#8217;s Hospital. The patients are all outpatient or inpatient patients who seek treatment at the People&#8217;s Hospital of Xinjiang Uygur Autonomous Region. Note that the selection of the 12th thoracic vertebra level and its upper/lower planes during data collection is due to the relatively large area of the liver in these planes, which facilitates later analysis and diagnosis. The liver disease dataset from twelve cervical layers (Twelve Cervical Layers) : This dataset consists of 3000 liver CT images from 3000 patients. Among them, the number of normal liver, fatty liver, and cirrhosis images is 1000 each. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ . The liver diseases in the upper layer of the twelve cervical vertebrae (The upper layers of the twelve cervical vertebrae) : This dataset consists of 3004 liver CT images from 3004 patients. Among them, there are 1002 cirrhosis, and 1001 images of fatty liver and normal liver images, respectively. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ . The liver diseases in the inferior layer of the twelve cervical vertebrae (The inferior layer of the twelve cervical vertebrae) : This dataset consists of 3004 liver CT images from 3002 patients. Among them, there are 1001 normal liver images, and 1001 images of fatty liver and cirrhosis, respectively. During the experiment, we randomly assigned three types of images into a training set and a testing set in an 8:2 ratio. The dataset can be accessed via https://pan.baidu.com/s/1hy2218tSGiWvso8lj5krgQ . Experimental details To ensure the authenticity and validity of the experimental results, this paper uses standardized and unified experimental settings. In terms of input parameters, the default image resolution of the model is uniformly set to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$224 \\times 224$$\\end{document} , a standard specification widely recognized in the medical image field. The batch size is fixed at 32, a parameter setting that effectively balances computational efficiency and model training effects. During the image preprocessing stage, no complex data augmentation techniques were used; instead, basic operations such as random cropping, random horizontal flipping, and normalization were employed to ensure data diversity while maintaining data distribution consistency. During model training, the Adam optimization algorithm was selected. As a gradient-based optimization method, Adam is highly regarded in the deep learning community for its fast convergence and excellent generalization capabilities. The initial learning rate was set to 0.0001, and a cosine annealing decay strategy was used to dynamically adjust the learning rate. The hyperparameter \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$T_{max}$$\\end{document} was set to 10 to regulate the learning rate decay rate and ensure stable and efficient model training. The default number of training epochs was set to 300. All experiments were completed on a single NVIDIA TITAN RTX 24G GPU to ensure consistency and reproducibility of the experimental environment. Evaluation metrics In medical image classification tasks, a single evaluation metric is often insufficient to fully reflect model performance. To achieve a precise assessment of model performance, this paper constructs a multi-dimensional evaluation metric system. Core evaluation metrics including Accuracy (Acc), Precision, Recall, and F1-score were selected. Accuracy, a key evaluation parameter in classification tasks, intuitively reflects the overall correctness of model classification. Precision focuses on the accuracy of the model positive predictions, while Recall measures the model ability to identify true positive examples. Given the trade-off between Precision and Recall in some scenarios, the F1-score is introduced to comprehensively consider both, providing a more comprehensive evaluation of the model classification performance. Additionally, the Receiver Operating Characteristic (ROC) curve and its Area Under the Curve (AUC) were used as supplementary evaluation metrics. The ROC curve visually demonstrates the model classification performance by plotting the True Positive Rate against the False Positive Rate at different classification thresholds. The AUC value quantifies the quality of the ROC curve, providing an objective basis for comparing the classification performance of different models. Through the comprehensive application of these multi-dimensional evaluation metrics, a comprehensive and accurate assessment of the model&#8217;s performance in medical image classification tasks can be achieved. The specific calculation formulas for each metric are as follows: 33 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; \\text {Accuracy (ACC)} = \\frac{\\text {TP} + \\text {TN}}{\\text {TP} + \\text {FP} + \\text {TN} + \\text {FN}} \\end{aligned}$$\\end{document} 34 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; \\text {Precision} = \\frac{\\text {TP}}{\\text {TP} + \\text {FP}} \\end{aligned}$$\\end{document} 35 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; \\text {Recall} = \\frac{\\text {TP}}{\\text {TP} + \\text {FN}} \\end{aligned}$$\\end{document} 36 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} &amp; \\text {F1 Score} = \\frac{2 * \\text {Precision} * \\text {Recall}}{\\text {Precision} + \\text {Recall}} \\end{aligned}$$\\end{document} where TP is True Positive,FP is False Positive,TN is True Negative, and FN is False Negative. The AUC is calculated as: 37 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\text {AUC} = \\frac{\\sum _{i \\in \\text {positiveClass}} \\text {rank}_i - M(1 + M)/2}{M * N} \\end{aligned}$$\\end{document} where M is the number of positive samples, N is the number of negative samples, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$rank_{i}$$\\end{document} is the model predicted probability for sample i . Experimental results To comprehensively evaluate the performance of CMT-Net, comparative experiments were conducted on three datasets: Twelve cervical layers, The upper layers of the twelve cervical vertebrae, and The inferior layer of the twelve cervical vertebrae. The method was systematically compared with various approaches based on CNNs, MLPs, vision transformers (ViTs), and their hybrid architectures. In the comparison of CNN-based methods, both classical architectures such as ResNet and EfficientNetV2, as well as cutting-edge improved models like RepVGG, ConvNext, InceptionNext, and FasterNet, were included. Meanwhile, to deeply explore the performance of pure MLP architectures and CNN-MLP hybrid networks in medical image classification tasks, this study selected state-of-the-art (SOTA) models for comparative experiments, including Conv-SdMLPMixer and SCNet, where SCNet represents the latest research achievements of CNN+MLP hybrid architectures in the field of medical image classification. Additionally, this paper conducted a comparative analysis of ViT-based models and their hybrid architectures, involving classical models like BiFormer and current top-performing SOTA methods such as GroupMixFormer. Comparative experimental results on the twelve cervical layers dataset Table 1 shows the five key metrics&#8212;-Accuracy (Acc), F1-score, Precision, Recall, and AUC&#8211;of CNN, transformer, MLP-based networks, and their hybrid models on the Twelve cervical layers dataset. Meanwhile, the parameter count (Params) and floating-point operations (FLOPs) of each model are provided to comprehensively evaluate performance and computational complexity. The optimal metrics among the compared models are bolded in the table. Table 1 Comparative experiments on the The Twelve cervical layers dataset. Methods Acc F1 Pre Re Auc Parma FLOPs Swin-S&#160; 15 84.50 84.44 84.75 87.50 88.38 48.8M 8.6G ResNet50&#160; 26 86.17 86.08 86.22 86.47 89.63 23.5M 4.1G SCNet&#160; 45 86.67 86.62 86.62 86.37 90.00 55.1M 12.1G BiFormer-S&#160; 47 86.33 86.31 86.40 86.33 89.75 25.0M 4.2G Conv-SdMLPMixer&#160; 48 86.33 86.49 88.40 86.63 89.78 49.8M 10.8G GroupMixFormer&#160; 49 86.33 86.38 86.64 86.36 89.75 22.1M 5.1G Eff-CTNet&#160; 50 86.17 86.19 86.20 86.47 89.63 25.2M 6.4G Eff-CTM&#160; 51 87.00 87.04 87.21 86.99 90.25 28.7M 10.3G InceptionNext&#160; 52 85.17 85.01 85.26 86.26 88.87 47.1M 8.4G FasterNet&#160; 53 85.50 85.42 85.73 86.50 89.13 13.7M 1.9G RepVGG&#160; 54 86.50 86.38 87.12 86.85 89.88 45.8M 9.9G EfficientNetV2&#160; 55 86.83 86.77 86.94 86.83 90.12 20.2M 2.9G CrossViT&#160; 56 86.21 86.14 86.33 86.24 89.45 26.7M 5.3G DaViT&#160; 57 85.75 85.27 85.36 85.64 88.59 28.3M 4.5G Ours 88.67 88.71 88.87 88.67 91.50 29.5M 5.0G The data in the table show that our CMT-Net demonstrated excellent performance on the Twelve cervical layers dataset, with an Acc of 88.67%, an F1-score of 88.71%, a Precision of 88.87%, a Recall of 88.67%, and an AUC of 91.50%. Among these, Acc, F1-score, Precision, Recall, and AUC all outperformed other comparative methods. Among CNN-based methods, ResNet50 and RepVGG showed relatively superior performance. However, compared with RepVGG, our method achieved significant optimization in model complexity, using only approximately one-third of the parameters and half of the FLOPs, while significantly improving key performance metrics. Specifically, the Accuracy (Acc) increased by 2.17% percentage points, the F1-score increased by 2.33%, the Precision increased by 1.75%, the Recall increased by 1.82%, and the Area Under the Curve (AUC) increased by 1.62%. This indicates that our method significantly enhances the model&#8217;s classification ability while maintaining efficient computation. When compared with transformer-based methods, using the best-performing BiFormer-S as a reference, our method also showed strong advantages. In terms of Accuracy, our method improved by 2.34% percentage points compared with BiFormer-S, the F1-score increased by 2.4%, the Precision increased by 2.47%, the Recall increased by 2.34%, and the AUC increased by 1.75%. These improvements strongly demonstrate that our method has more accurate representation and classification capabilities when processing complex data features. For MLP-based methods, Conv-SdMLPMixer represents the current top performance level. Compared with it, our method improved by 2.34% in Accuracy, significantly increased the F1-score by 4.22%, improved the Precision by 0.47%, and increased the AUC by 1.72%. Notably, our method achieved these performance improvements using only approximately 40% of the parameters of Conv-SdMLPMixer. This result fully demonstrates the effectiveness and innovation of our method in model lightweighting and performance optimization, providing highly valuable new ideas and solutions for research and applications in related fields. In summary, this result not only validates the robustness and scalability of CMT Net in complex medical imaging, but also provides hope for deploying CMT Net to PACS or end devices for high-precision liver disease screening. Comparative experimental results on the upper layers of the twelve cervical vertebrae dataset Table 2 presents the five key performance metrics&#8211;Acc, F1-score, Precision, Recall, and AUC&#8211;of different methods on The upper layers of the twelve cervical vertebrae dataset. On this dataset, our method demonstrated excellent performance. As can be clearly seen from the table data, our method significantly outperformed all comparative methods in all five key evaluation metrics&#8211;Accuracy (Acc), F1-score, Precision, Recall, and Area Under the Curve (AUC)&#8211;ranking first. Specifically, our method achieved an Accuracy of 88.83%, an F1-score of 88.84%, a Precision of 88.99%, a Recall of 88.73%, and an AUC of 91.62%. Table 2 Comparative experiments onThe upper layers of the twelve cervical vertebrae. Methods Acc F1 Pre Re Auc Swin-S&#160; 15 85.50 85.51 85.55 85.20 89.13 Resnet50&#160; 26 86.83 86.82 86.83 86.53 90.13 BiFormer-S&#160; 44 86.83 86.82 86.84 86.33 90.13 SCNet&#160; 45 86.17 86.16 86.30 86.46 89.62 Conv-SdMLPMixer&#160; 48 87.09 87.01 87.46 87.56 90.38 GroupMixFormer&#160; 49 86.17 86.20 87.35 86.47 89.63 Eff-CTNet&#160; 50 87.33 87.33 87.64 87.03 90.50 Eff-CTM&#160; 51 87.00 87.04 87.86 87.49 90.25 InceptionNext&#160; 52 85.50 85.48 85.60 85.57 89.12 Fasternet&#160; 53 85.00 86.03 85.17 85.80 88.75 Repvgg-Big2&#160; 54 87.17 87.19 84.43 87.16 90.38 EfficientNetV2&#160; 55 87.00 87.01 87.16 87.00 90.25 Ours 88.83 88.84 88.99 88.73 91.62 As shown in Table 2 , Compared with other advanced methods, our method showed more obvious advantages in various metrics. Taking Accuracy as an example, compared with the outstanding Eff-CTNet, our method achieved a 1.5% improvement; compared with the classical ResNet50, the Accuracy increased by 2%. In terms of F1-score, the improvement compared with Eff-CTNet was 1.51%, while the improvement compared with InceptionNeXt reached 3.36%, indicating that our method achieved a better balance between Precision and Recall. In the Precision metric, our method improved by 1.53% percentage points compared with Conv-SdMLPMixer, demonstrating stronger positive sample recognition ability; in terms of Recall, it improved by 2.27% compared with SCNet, indicating that our method can more effectively identify actual positive samples; in the AUC metric, it improved by 1.24% percentage points compared with Conv-SdMLPMixer, further proving the superiority of our method in classification performance. In terms of parameter count (Params) and FLOPs, while achieving optimal performance metrics, our method also has certain competitiveness in computational complexity, able to reasonably control computational resource consumption while ensuring high performance, providing a more advantageous choice for practical applications. Overall, CMT-Net leads existing CNN, Transformer, and MLP methods with an accuracy of 88.83% and an AUC of 91.62%, while maintaining a moderate scale in terms of parameter count and FLOPs, balancing accuracy and efficiency. The above results are expected to directly embed the CMT-Net model into existing PACS and continuously optimize it through periodic incremental fine-tuning, significantly reducing misdiagnosis and missed diagnosis, providing an immediately implementable AI solution for early screening, follow-up, and large-scale physical examinations of liver diseases. Comparative experimental results on the inferior layer of the twelve cervical vertebrae Dataset As shown in Table 3 , our method performed excellently on The inferior layer of the twelve cervical vertebrae dataset, with an Acc of 86.67%, an F1-score of 86.37, a Precision of 86.72%, a Recall of 86.97%, and a high AUC of 90.00%. Among these five key metrics, Acc, F1-score, Precision, Recall, and AUC all outperformed all other comparative methods. Table 3 Comparative experiments on The inferior layer of the twelve cervical vertebrae. Methods Acc F1 Pre Re Auc Swin-S &#160; 15 83.50 83.47 83.48 83.65 87.63 Resnet50&#160; 26 83.83 83.92 84.84 83.86 87.88 SCNet&#160; 45 83.83 83.87 84.13 83.83 87.86 BiFormer-S&#160; 47 84.00 84.01 85.22 86.06 88.01 Conv-SdMLPMixer&#160; 48 84.67 84.63 85.51 84.97 88.50 GroupMixFormer&#160; 49 84.67 84.74 85.09 74.67 88.50 Eff-CTNet&#160; 50 84.50 84.49 85.29 84.52 88.38 Eff-CTM &#160; 51 83.83 83.79 83.81 86.83 87.88 InceptionNext&#160; 52 84.50 84.49 84.49 84.82 88.38 FasterNet&#160; 53 83.67 83.72 84.02 83.69 87.75 Repvgg-B1g2&#160; 54 84.33 84.31 84.98 84.33 88.25 EfficientNetV2&#160; 55 84.83 84.87 85.03 84.83 88.63 Ours 86.67 86.37 86.72 86.97 90.00 When compared with traditional CNN methods, our method showed significant advantages. Taking ResNet50 as an example, its Accuracy (Acc) was 83.83%, while our method improved the Accuracy by 2.84% compared with ResNet50; in terms of F1-score, ResNet50 was 83.92%, and our method achieved an improvement of 2.45%. Additionally, our method achieved varying degrees of improvement in key metrics such as Precision, Recall, and AUC, strongly demonstrating its advancement compared with traditional CNN methods. When compared with transformer-based methods, our method also had outstanding advantages in various metrics. Among many transformer-based methods, GroupMixFormer performed relatively well; however, compared with it, our method improved the Accuracy by 2%, the F1-score by 1.63%, the Precision and Recall by 1.63% each, and the AUC by 1.5%, further highlighting the superiority of our method. In the MLP-based method system, Conv-SdMLPMixer is a representative of relatively excellent performance. Even so, our method still showed strong competitiveness when compared with it. Specifically, our method improved the Accuracy by 2%, the F1-score by 2.01%, the Precision by 1.21%, the Recall by 2%, and the AUC by 1.5%. In summary, whether compared with CNN, transformer, or MLP-related methods, our method has significantly surpassed them in multiple key performance metrics, providing a more promising solution for related research fields. In summary, this model maintains high performance while still having controllable computational overhead, and can be directly embedded into existing PACS or edge devices to achieve real-time, high-precision liver disease screening and follow-up, providing a plug and play solution for clinical implementation. Visual analysis The Grad-CAM visualization results of different methods on three datasets are shown in Figure 6 . From the visualization results, there are differences in the activation levels of each method in key regions. Traditional CNN methods such as ResNet, FasterNet, and EfficientNetV2 reflect different emphases on capturing malignant sample features, with obvious activation in specific structural regions of the image, indicating their sensitivity to the features of these regions. BiFormer, a transformer-based method, benefits from the transformer architecture advantage in capturing long-range dependency relationships, and its activation regions may pay more attention to globally correlated features in the samples. Compared with other methods, our method has more precise and concentrated activation in key regions, able to accurately locate the lesion areas in the liver, demonstrating effective capture of the key features of malignant samples. This means that our method can more accurately focus on the core information related to the determination of malignant samples in feature extraction and classification decision-making. The visualization results further verify the accuracy of the metrics in the tables, indicating that the model not only performs excellently in numerical metrics but also has considerable advantages in actual lesion localization capabilities. Fig. 6 The Grad-CAM visualization results of different methods on three datasets. Figure 7 shows the ROC curves of different methods on the three datasets. On The Twelve cervical layers dataset, the ROC curves of traditional CNN models such as ResNet and InceptionNeXt are relatively close, and their AUC values are also similar, indicating that their overall classification performances on this dataset are comparable. For The upper layer of the twelve cervical vertebrae and The inferior layer of the twelve cervical vertebrae datasets, the performance differences between models are more significant. Transformer-based models such as BiFormer showed certain performance advantages, but our method&#8217;s (Ours) ROC curve is located above the others, standing out among many models, indicating better performance in balancing the False Positive Rate (FPR) and True Positive Rate (TPR), and being able to more effectively identify positive and negative samples. Through the analysis of the ROC curves of different models on the three datasets, our method demonstrated excellent performance in medical image classification tasks, showing significant advantages over other classical models. Fig. 7 Visualization of ROC curves for different methods on three datasets. Figure 8 shows the visualization results of the confusion matrices of the proposed model method on the three medical image datasets. In the confusion matrix of The Twelve cervical layers dataset, the elements on the main diagonal represent the number of samples correctly classified by the model, with relatively high values, indicating that the model has a certain overall classification ability. For The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, the confusion matrices also show the classification effects of the model on various categories. However, the &#8220;Carcinoma&#8221; category has a high probability of being misclassified as other categories, which may be due to the model&#8217;s difficulty in distinguishing certain categories with similar features, leading to a decrease in classification accuracy. Fig. 8 Visualization examples of confusion matrices in three different datasets. Ablation studies To deeply explore the roles of the components of the proposed method and the impact of key module parameter settings, this section introduces ablation experiments. Figure 4 shows the results of the ablation experiments. Table 4 Performance comparison across different cervical layers and configurations. Cervical Layers Configuration Method Metrics Acc(M&#177;SD) Acc(CI) F1 Pre Re Auc Parma FLOPs Twelve cervical layers w/o Each Module Baseline 86.83&#177;0.52 [85.33, 87.92] 86.77 86.94 86.83 90.12 20.2M 2.9G Baseline+ET 87.67&#177;0.49 [86.42, 88.86] 87.65 87.75 87.67 90.75 29.7M 3.8G Baseline+HM 88.17&#177;0.50 [86.85, 89.30] 88.13 88.13 88.17 91.13 30.3M 6.7G ours 88.67 &#177; 0.42 [87.63, 89.71] 88.71 88.87 88.67 91.50 29.5M 5.0G Window Size 5 88.00&#177;0.46 [86.86, 89.14] 87.93 87.97 87.99 91.00 29.5M 5.0G 7(ours) 88.67 &#177; 0.32 [87.88, 89.46] 88.71 88.87 88.67 91.50 29.5M 5.0G 9 87.83&#177;0.30 [87.08, 88.58] 87.85 87.92 87.83 90.88 29.5M 5.0G The upper layers of the twelve cervical vertebrae w/o Each Module Baseline 87.00&#177;0.41 [85.98, 88.02] 87.01 87.16 87.00 90.25 &#8211; &#8211; Baseline+ET 88.00&#177;0.26 [87.35, 88.65] 88.02 88.11 87.96 91.00 &#8211; &#8211; Baseline+HM 87.50&#177;0.32 [86.71, 88.29] 87.46 87.50 87.66 90.63 &#8211; &#8211; ours 88.83 &#177; 0.29 [88.11, 89.55] 88.84 88.99 88.73 91.62 &#8211; &#8211; Window Size 5 88.33&#177;0.31 [87.56, 89.10] 88.38 88.53 88.33 91.25 &#8211; &#8211; 7(ours) 88.83 &#177; 0.30 [88.08, 89.58] 88.84 88.99 88.73 91.62 &#8211; &#8211; 9 88.50&#177;0.36 [87.61, 89.39] 88.53 88.77 88.49 91.63 &#8211; &#8211; The inferior layer of the twelve cervical vertebrae w/o Each Module Baseline 84.83&#177;0.45 [83.71, 85.95] 84.87 85.03 84.83 88.63 &#8211; &#8211; Baseline+ET 85.50&#177;0.43 [84.43, 86.57] 85.56 85.81 85.20 89.13 &#8211; &#8211; Baseline+HM 86.00&#177;0.50 [84.76, 87.24] 86.05 86.21 86.00 89.50 &#8211; &#8211; ours 86.67 &#177; 0.44 [85.58, 87.76] 86.67 86.72 86.97 90.00 &#8211; &#8211; Window Size 5 86.33&#177;0.40 [85.34, 87.32] 86.26 86.55 86.63 89.75 &#8211; &#8211; 7(ours) 86.67 &#177; 0.36 [85.78, 87.56] 86.87 86.72 86.97 90.00 &#8211; &#8211; 9 86.50&#177;0.38 [85.56, 87.44] 86.40 86.62 86.50 89.88 &#8211; &#8211; Among them, ACC (M&#177;SD) represents the mean and standard deviation of ACC. ACC (CI) represents the 95% confidence interval of ACC. Impact of each module On the three datasets, using the Baseline model as the foundation, we conducted experiments by separately adding the ET module and the HM module, and compared them with the complete model. As shown in Table 4 , on The Twelve cervical layers dataset, the Baseline model achieved certain results in metrics such as Accuracy and F1-score, with an Accuracy of 86.83%. After adding the ET module, the model&#8217;s metrics improved, with the Accuracy reaching 87.67%, indicating that the ET module can effectively enhance the model&#8217;s ability to extract and process medical image features, thereby improving classification performance. After adding the HM module, the Accuracy further increased to 88.17%, indicating that the HM module played a positive role in feature fusion and classification decision-making. The proposed model, after integrating these modules, achieved an Accuracy of 88.67%, outperforming the cases of adding modules individually in multiple metrics, fully demonstrating the advantages of the collaborative work of each module, which can more comprehensively and accurately extract image features, thereby improving classification accuracy. Similar trends were observed on The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, achieving better classification results. This again proves the positive impact of the ET and HM modules on model performance and the effectiveness of the overall model structure in integrating the advantages of each module. Impact of window size in the GCA module As shown in Table 4 , for the key parameter of the GCA module, which divides the feature map into regions in the spatial dimension, we set different values (5, 7, 9) for experiments. In a paper to be submitted to a top Chinese Academy of Sciences (CAS) Zone 1 journal on the theme of medical image classification, the experimental results on the three datasets show that when the feature map patch size was 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 5, the model showed certain performance in various metrics, such as an Accuracy of 88.00% on The Twelve cervical layers dataset. When the feature map patch size was adjusted to 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 7, the Accuracy increased to 88.67%, with corresponding improvements in other metrics. This indicates that when the feature map patch size is 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 7, the GCA module can more accurately capture key feature information in the image and optimize feature interaction, thereby improving model performance. When the feature map patch size became 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 9, the model Accuracy decreased to 87.83%, indicating that overly large feature map patches may introduce excessive redundant information, interfering with the model extraction of key features and leading to performance degradation. On The upper layers of the twelve cervical vertebrae and The inferior layers of the twelve cervical vertebrae datasets, the results also show that the GCA module performed best with a feature map patch size of 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 7, providing strong support for the overall performance improvement of the model. In summary, the ablation experiment results clearly demonstrate the significant impact of the proposed ET module, HM module, and the feature map patch size in the GCA module on the performance of medical image classification models, strongly supporting the effectiveness and rationality of our proposed method and providing important insights for the further optimization of subsequent models. Conclusion To address the challenge of identifying normal liver, fatty liver, and liver cirrhosis CT images in medical imaging diagnosis&#8211;caused by subtle differences in morphology and density&#8211;this study proposes CMT-Net, a hybrid medical image classification network that integrates the advantages of CNNs, MLPs, and Transformer. The network employs an Efficient Transformer module (ET) to focus on local feature details and gradually integrate global information, while a Hybrid MLP module (HM) enables deep fusion of spatial and channel-wise information, forming a hierarchical single-branch serial feature extraction architecture. Experimental results demonstrate that CMT-Net significantly outperforms existing CNN, ViT, and hybrid-based methods on multiple datasets. Ablation studies validate the effectiveness of the core modules, and visualization analyses show its ability to accurately localize lesion areas. This research not only provides a new tool for the precise diagnosis of liver diseases but also offers novel insights for designing deep learning models in medical image classification. However, this study only focused on the static CT images of three types of liver diseases (normal, fatty liver and liver cirrhosis), and did not involve multiphase images, multiple organ lesions or more complex clinical scenes (such as early screening of liver cancer and fibrosis classification). In addition, this study did not integrate clinical indicators (laboratory examination, medical history) or other modes (MRI, ultrasonic elastography), which may miss complementary diagnostic information. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Xiaolei He and Xilong Wang contributed equally to this work. Author contributions X.H.: Software, Funding acquisition. X.W.: Conceptualization. Y.W.: Validation. H.L.: Writing - review &amp; editing. S.L.: Formal analysis. J.W.: Resources. Y.F.: Supervision. Q.W.: Visualization. J.C.: Writing - original draft. Funding This study was funded by the Hospital Level Project 20230118 of Xinjiang Uygur Autonomous Region People&#8217;s Hospital. Data availability The dataset used in this study was from the Medical Imaging Center of Xinjiang Uygur Autonomous Region People&#8217;s Hospital. The relevant data has been stored in the following link: https://pan.baidu.com/s/1JkmdP2Ukc4iHM3108AYrUw?pwd=hug3 . Other researchers can download and use it normally. If you have any questions while using the data, please contact the corresponding author through the following methods: 13565898386@163.com. Declarations Competing interests The authors declare no competing interests. References 1. Younossi Z Global burden of nafld and nash: trends, predictions, risk factors and prevention Nat. Rev. Gastroenterol. Hepatol. 2018 15 11 20 10.1038/nrgastro.2017.109 28930295 Younossi, Z. et al. Global burden of nafld and nash: trends, predictions, risk factors and prevention. Nat. Rev. Gastroenterol. Hepatol. 15 , 11&#8211;20 (2018). 28930295 10.1038/nrgastro.2017.109 2. Gin&#232;s P Liver cirrhosis The Lancet 2021 398 1359 1376 10.1016/S0140-6736(21)01374-X 34543610 Gin&#232;s, P. et al. Liver cirrhosis. The Lancet 398 , 1359&#8211;1376 (2021). 10.1016/S0140-6736(21)01374-X 34543610 3. Friedman SL Mechanisms of hepatic fibrogenesis Gastroenterology 2008 134 1655 1669 10.1053/j.gastro.2008.03.003 18471545 PMC2888539 Friedman, S. L. Mechanisms of hepatic fibrogenesis. Gastroenterology 134 , 1655&#8211;1669 (2008). 18471545 10.1053/j.gastro.2008.03.003 PMC2888539 4. Lee DH Imaging evaluation of non-alcoholic fatty liver disease: focused on quantification Clin. Mol. Hepatol. 2017 23 290 10.3350/cmh.2017.0042 28994271 PMC5760010 Lee, D. H. Imaging evaluation of non-alcoholic fatty liver disease: focused on quantification. Clin. Mol. Hepatol. 23 , 290 (2017). 28994271 10.3350/cmh.2017.0042 PMC5760010 5. Tang A Accuracy of mr imaging-estimated proton density fat fraction for classification of dichotomized histologic steatosis grades in nonalcoholic fatty liver disease Radiology 2015 274 416 425 10.1148/radiol.14140754 25247408 PMC4314291 Tang, A. et al. Accuracy of mr imaging-estimated proton density fat fraction for classification of dichotomized histologic steatosis grades in nonalcoholic fatty liver disease. Radiology 274 , 416&#8211;425 (2015). 25247408 10.1148/radiol.14140754 PMC4314291 6. Litjens G A survey on deep learning in medical image analysis Med. Image Anal. 2017 42 60 88 10.1016/j.media.2017.07.005 28778026 Litjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42 , 60&#8211;88 (2017). 28778026 10.1016/j.media.2017.07.005 7. Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 234&#8211;241 (Springer, 2015). 8. Tajbakhsh N Convolutional neural networks for medical image analysis: full training or fine tuning? IEEE Trans. Med. Imaging 2016 35 1299 1312 10.1109/TMI.2016.2535302 26978662 Tajbakhsh, N. et al. Convolutional neural networks for medical image analysis: full training or fine tuning?. IEEE Trans. Med. Imaging 35 , 1299&#8211;1312 (2016). 26978662 10.1109/TMI.2016.2535302 9. Jesi PM Daniel VAA Differential cnn and kelm integration for accurate liver cancer detection Biomed. Signal Process. Control 2024 95 106419 10.1016/j.bspc.2024.106419 Jesi, P. M. &amp; Daniel, V. A. A. Differential cnn and kelm integration for accurate liver cancer detection. Biomed. Signal Process. Control 95 , 106419 (2024). 10. Abinaya RJ Rajakumar G Accurate liver fibrosis detection through hybrid mrmr-bilstm-cnn architecture with histogram equalization and optimization J. Imaging Inf. Med. 2024 37 1008 1022 10.1007/s10278-024-00995-1 PMC11169190 38351226 Abinaya, R. J. &amp; Rajakumar, G. Accurate liver fibrosis detection through hybrid mrmr-bilstm-cnn architecture with histogram equalization and optimization. J. Imaging Inf. Med. 37 , 1008&#8211;1022 (2024). 10.1007/s10278-024-00995-1 PMC11169190 38351226 11. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929 (2020). 12. Mir AN Rizvi DR Ahmad MR Enhancing histopathological image analysis: an explainable vision transformer approach with comprehensive interpretation methods and evaluation of explanation quality Eng. Appl. Artif. Intell. 2025 149 110519 10.1016/j.engappai.2025.110519 Mir, A. N., Rizvi, D. R. &amp; Ahmad, M. R. Enhancing histopathological image analysis: an explainable vision transformer approach with comprehensive interpretation methods and evaluation of explanation quality. Eng. Appl. Artif. Intell. 149 , 110519 (2025). 13. Mir AN Rizvi DR Advancements in deep learning and explainable artificial intelligence for enhanced medical image analysis: a comprehensive survey and future directions Eng. Appl. Artif. Intell. 2025 158 111413 10.1016/j.engappai.2025.111413 Mir, A. N. &amp; Rizvi, D. R. Advancements in deep learning and explainable artificial intelligence for enhanced medical image analysis: a comprehensive survey and future directions. Eng. Appl. Artif. Intell. 158 , 111413 (2025). 14. Chen, J. et al. Transunet: Transformers make strong encoders for medical image segmentation. preprint arXiv:2102.04306 (2021). 15. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision 10012&#8211;10022 (2021). 16. Ou J Restransunet: an effective network combined with transformer and u-net for liver segmentation in ct scans Comput. Biol. Med. 2024 177 108625 10.1016/j.compbiomed.2024.108625 38823365 Ou, J. et al. Restransunet: an effective network combined with transformer and u-net for liver segmentation in ct scans. Comput. Biol. Med. 177 , 108625 (2024). 38823365 10.1016/j.compbiomed.2024.108625 17. Kumar S Kumar RV Ranjith V Jeevakala S Varun SS Grey wolf optimized swinunet based transformer framework for liver segmentation from ct images Comput. Electr. Eng. 2024 117 109248 10.1016/j.compeleceng.2024.109248 Kumar, S., Kumar, R. V., Ranjith, V., Jeevakala, S. &amp; Varun, S. S. Grey wolf optimized swinunet based transformer framework for liver segmentation from ct images. Comput. Electr. Eng. 117 , 109248 (2024). 18. Guo, J. et al. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 12175&#8211;12185 (2022). 19. Tolstikhin IO Mlp-mixer: An all-mlp architecture for vision Adv. Neural. Inf. Process. Syst. 2021 34 24261 24272 Tolstikhin, I. O. et al. Mlp-mixer: An all-mlp architecture for vision. Adv. Neural. Inf. Process. Syst. 34 , 24261&#8211;24272 (2021). 20. Krizhevsky A Sutskever I Hinton GE Imagenet classification with deep convolutional neural networks Commun. ACM 2017 60 84 90 10.1145/3065386 Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. Commun. ACM 60 , 84&#8211;90 (2017). 21. Yang B Zhang J Lyu Y Zhang J Automatic computed tomography image segmentation method for liver tumor based on a modified tokenized multilayer perceptron and attention mechanism Quant. Imaging Med. Surg. 2025 15 2385 10.21037/qims-24-2132 40160629 PMC11948385 Yang, B., Zhang, J., Lyu, Y. &amp; Zhang, J. Automatic computed tomography image segmentation method for liver tumor based on a modified tokenized multilayer perceptron and attention mechanism. Quant. Imaging Med. Surg. 15 , 2385 (2025). 40160629 10.21037/qims-24-2132 PMC11948385 22. LeCun Y Bottou L Bengio Y Haffner P Gradient-based learning applied to document recognition Proc. IEEE 2002 86 2278 2324 10.1109/5.726791 LeCun, Y., Bottou, L., Bengio, Y. &amp; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 86 , 2278&#8211;2324 (2002). 23. Krizhevsky A Sutskever I Hinton GE Imagenet classification with deep convolutional neural networks Commun. ACM 2017 60 84 95 10.1145/3065386 Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. Commun. ACM 60 , 84&#8211;95 (2017). 24. Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. preprint arXiv:1409.1556 (2014). 25. Szegedy, C. et al. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition 1&#8211;9 (2015). 26. He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 770&#8211;778 (2016). 27. Huang, G., Liu, Z., Van Der Maaten, L. &amp; Weinberger, K.&#160;Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 4700&#8211;4708 (2017). 28. Asif S Ain Q-U Al-Sabri R Abdullah M Litefusionnet: boosting the performance for medical image classification with an intelligent and lightweight feature fusion network J. Comput. Sci. 2024 80 102324 10.1016/j.jocs.2024.102324 Asif, S., Ain, Q.-U., Al-Sabri, R. &amp; Abdullah, M. Litefusionnet: boosting the performance for medical image classification with an intelligent and lightweight feature fusion network. J. Comput. Sci. 80 , 102324 (2024). 29. Howard, A.&#160;G. et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications. preprint arXiv:1704.04861 (2017). 30. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &amp; Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition 4510&#8211;4520 (2018). 31. Zhu B Yang Y Quality assessment of abdominal ct images: an improved resnet algorithm with dual-attention mechanism Am. J. Transl. Res. 2024 16 3099 10.62347/WKNS8633 39114678 PMC11301486 Zhu, B. &amp; Yang, Y. Quality assessment of abdominal ct images: an improved resnet algorithm with dual-attention mechanism. Am. J. Transl. Res. 16 , 3099 (2024). 39114678 10.62347/WKNS8633 PMC11301486 32. Maglogiannis I Zafiropoulos E Anagnostopoulos I An intelligent system for automated breast cancer diagnosis and prognosis using svm based classifiers Appl. Intell. 2009 30 24 36 10.1007/s10489-007-0073-z Maglogiannis, I., Zafiropoulos, E. &amp; Anagnostopoulos, I. An intelligent system for automated breast cancer diagnosis and prognosis using svm based classifiers. Appl. Intell. 30 , 24&#8211;36 (2009). 33. Liu Z Xiong R Jiang T Ci-net: Clinical-inspired network for automated skin lesion recognition IEEE Trans. Med. Imaging 2022 42 619 632 10.1109/TMI.2022.3215547 36279355 Liu, Z., Xiong, R. &amp; Jiang, T. Ci-net: Clinical-inspired network for automated skin lesion recognition. IEEE Trans. Med. Imaging 42 , 619&#8211;632 (2022). 10.1109/TMI.2022.3215547 36279355 34. Gedeon KK Liu Z Classification of liver lesions in ct images based on livlesionet, modified multi-scale cnn with bridge scale method Multimedia Tools Appl. 2024 83 8911 8929 10.1007/s11042-023-15966-x Gedeon, K. K. &amp; Liu, Z. Classification of liver lesions in ct images based on livlesionet, modified multi-scale cnn with bridge scale method. Multimedia Tools Appl. 83 , 8911&#8211;8929 (2024). 35. Manjula, P., Krishnakumar, K., Gl, S., Pandiaraj, S. &amp; Prakash, M. A novel method for detecting liver tumors combining machine learning with medical imaging in ct scans using resunet. In 2024 International Conference on Integrated Circuits and Communication Systems (ICICACS) 1&#8211;5 (IEEE, 2024). 36. Malik MGA Saeed A Shehzad K Iqbal M Def-swine2net: Dual enhanced features guided with multi-model fusion for brain tumor classification using preprocessing optimization Biomed. Signal Process. Control 2025 100 107079 10.1016/j.bspc.2024.107079 Malik, M. G. A., Saeed, A., Shehzad, K. &amp; Iqbal, M. Def-swine2net: Dual enhanced features guided with multi-model fusion for brain tumor classification using preprocessing optimization. Biomed. Signal Process. Control 100 , 107079 (2025). 37. Pan H A lightweight model for the retinal disease classification using optical coherence tomography Biomed. Signal Process. Control 2025 101 107146 10.1016/j.bspc.2024.107146 Pan, H. et al. A lightweight model for the retinal disease classification using optical coherence tomography. Biomed. Signal Process. Control 101 , 107146 (2025). 38. Tanveer M Akram MU Khan AM Transnetv: an optimized hybrid model for enhanced colorectal cancer image classification Biomed. Signal Process. Control 2024 96 106579 10.1016/j.bspc.2024.106579 Tanveer, M., Akram, M. U. &amp; Khan, A. M. Transnetv: an optimized hybrid model for enhanced colorectal cancer image classification. Biomed. Signal Process. Control 96 , 106579 (2024). 39. Mahmood T Wahid A Hong JS Kim SG Park KR A novel convolution transformer-based network for histopathology-image classification using adaptive convolution and dynamic attention Eng. Appl. Artif. Intell. 2024 135 108824 10.1016/j.engappai.2024.108824 Mahmood, T., Wahid, A., Hong, J. S., Kim, S. G. &amp; Park, K. R. A novel convolution transformer-based network for histopathology-image classification using adaptive convolution and dynamic attention. Eng. Appl. Artif. Intell. 135 , 108824 (2024). 40. Cheng D Zhou Z Zhang J Eg-unetr: an edge-guided liver tumor segmentation network based on cross-level interactive transformer Biomed. Signal Process. Control 2024 97 106739 10.1016/j.bspc.2024.106739 Cheng, D., Zhou, Z. &amp; Zhang, J. Eg-unetr: an edge-guided liver tumor segmentation network based on cross-level interactive transformer. Biomed. Signal Process. Control 97 , 106739 (2024). 41. Zhao, L. et al. A hybrid cnn-transformer for focal liver lesion classification. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 13001&#8211;13005 (IEEE, 2024). 42. Touvron H Resmlp: Feedforward networks for image classification with data-efficient training IEEE Trans. Pattern Anal. Mach. Intell. 2022 45 5314 5321 10.1109/TPAMI.2022.3206148 36094972 Touvron, H. et al. Resmlp: Feedforward networks for image classification with data-efficient training. IEEE Trans. Pattern Anal. Mach. Intell. 45 , 5314&#8211;5321 (2022). 10.1109/TPAMI.2022.3206148 36094972 43. Liu H Dai Z So D Le QV Pay attention to mlps Adv. Neural. Inf. Process. Syst. 2021 34 9204 9215 Liu, H., Dai, Z., So, D. &amp; Le, Q. V. Pay attention to mlps. Adv. Neural. Inf. Process. Syst. 34 , 9204&#8211;9215 (2021). 44. Li, J., Hassani, A., Walton, S. &amp; Shi, H. Convmlp: Hierarchical convolutional mlps for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 6307&#8211;6316 (2023). 45. Zhang R Wang L Cheng S Song S Mlp-based classification of covid-19 and skin diseases Expert Syst. Appl. 2023 228 120389 10.1016/j.eswa.2023.120389 37193247 PMC10170962 Zhang, R., Wang, L., Cheng, S. &amp; Song, S. Mlp-based classification of covid-19 and skin diseases. Expert Syst. Appl. 228 , 120389 (2023). 37193247 10.1016/j.eswa.2023.120389 PMC10170962 46. Wang, B. et al. Ma-net: A mlp-based attentional deep network for segmentation of liver tumor ablation region from 2d ultrasound image. In Proceedings of the 2024 7th International Conference on Image and Graphics Processing 62&#8211;66 (2024). 47. Zhu, L., Wang, X., Ke, Z., Zhang, W. &amp; Lau, R.&#160;W. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 10323&#8211;10333 (2023). 48. Ren Z Liu S Wang L Guo Z Conv-sdmlpmixer: a hybrid medical image classification network based on multi-branch cnn and multi-scale multi-dimensional mlp Inf. Fusion 2025 2025 102937 10.1016/j.inffus.2025.102937 Ren, Z., Liu, S., Wang, L. &amp; Guo, Z. Conv-sdmlpmixer: a hybrid medical image classification network based on multi-branch cnn and multi-scale multi-dimensional mlp. Inf. Fusion 2025 , 102937 (2025). 49. Chongjian, G. et al. Advancing vision transformers with group-mix attention. preprint arXiv:2311.15157 (2023). 50. Liu S Yue W Guo Z Wang L Multi-branch cnn and grouping cascade attention for medical image classification Sci. Rep. 2024 14 15013 10.1038/s41598-024-64982-w 38951526 PMC11217469 Liu, S., Yue, W., Guo, Z. &amp; Wang, L. Multi-branch cnn and grouping cascade attention for medical image classification. Sci. Rep. 14 , 15013 (2024). 38951526 10.1038/s41598-024-64982-w PMC11217469 51. Liu S Wang L Yue W An efficient medical image classification network based on multi-branch cnn, token grouping transformer and mixer mlp Appl. Soft Comput. 2024 153 111323 10.1016/j.asoc.2024.111323 Liu, S., Wang, L. &amp; Yue, W. An efficient medical image classification network based on multi-branch cnn, token grouping transformer and mixer mlp. Appl. Soft Comput. 153 , 111323 (2024). 52. Yu, W., Zhou, P., Yan, S. &amp; Wang, X. Inceptionnext: When inception meets convnext. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition 5672&#8211;5683 (2024). 53. Chen, J. et al. Run, don&#8217;t walk: chasing higher flops for faster neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 12021&#8211;12031 (2023). 54. Ding, X. et al. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 13733&#8211;13742 (2021). 55. Tan, M. &amp; Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning 6105&#8211;6114 (PMLR, 2019). 56. Chen, C.-F.&#160;R., Fan, Q. &amp; Panda, R. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision 357&#8211;366 (2021). 57. Ding, M. et al. Davit: Dual attention vision transformers. In European conference on computer vision 74&#8211;92 (Springer, 2022)."
}