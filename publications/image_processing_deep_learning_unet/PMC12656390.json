{
  "pmcid": "PMC12656390",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:31.307181",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656390",
    "pmid": "41305072",
    "doi": "10.3390/s25226865",
    "title": "CONTI-CrackNet: A Continuity-Aware State-Space Network for Crack Segmentation",
    "year": "2025",
    "month": "11",
    "day": "10",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "10"
    },
    "authors": [
      "Song Wenjie",
      "Zhao Min",
      "Xu Xunqian"
    ],
    "abstract": "Crack segmentation in cluttered scenes with slender and irregular patterns remains difficult, and practical systems must balance accuracy and efficiency. We present CONTI-CrackNet, which is a lightweight visual state-space network that integrates a Multi-Directional Selective Scanning Strategy (MD3S). MD3S performs bidirectional scanning along the horizontal, vertical, and diagonal directions, and it fuses the complementary paths with a Bidirectional Gated Fusion (BiGF) module to strengthen global continuity. To preserve fine details while completing global texture, we propose a Dual-Branch Pixel-Level Global–Local Fusion (DBPGL) module that incorporates a Pixel-Adaptive Pooling (PAP) mechanism to dynamically weight max-pooled responses and average-pooled responses. Evaluated on two public benchmarks, the proposed method achieves an F1 score (F1) of 0.8332 and a mean Intersection over Union (mIoU) of 0.8436 on the TUT dataset, and it achieves an mIoU of 0.7760 on the CRACK500 dataset, surpassing competitive Convolutional Neural Network (CNN), Transformer, and Mamba baselines. With 512 × 512 input, the model requires 24.22 G floating point operations (GFLOPs), 6.01 M parameters (Params), and operates at 42 frames per second (FPS) on an RTX 3090 GPU, delivering a favorable accuracy–efficiency balance. These results show that CONTI-CrackNet improves continuity and edge recovery for thin cracks while keeping computational cost low, and it is lightweight in terms of parameter count and computational cost.",
    "keywords": [
      "crack segmentation",
      "segmentation",
      "Mamba",
      "deep learning",
      "lightweight networks",
      "feature extraction"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656390</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656390.1</article-id><article-id pub-id-type=\"pmcaid\">12656390</article-id><article-id pub-id-type=\"pmcaiid\">12656390</article-id><article-id pub-id-type=\"pmid\">41305072</article-id><article-id pub-id-type=\"doi\">10.3390/s25226865</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-06865</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CONTI-CrackNet: A Continuity-Aware State-Space Network for Crack Segmentation</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names initials=\"W\">Wenjie</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><xref rid=\"af1-sensors-25-06865\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names initials=\"M\">Min</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><xref rid=\"af2-sensors-25-06865\" ref-type=\"aff\">2</xref><xref rid=\"c1-sensors-25-06865\" ref-type=\"corresp\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"X\">Xunqian</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af3-sensors-25-06865\" ref-type=\"aff\">3</xref></contrib></contrib-group><aff id=\"af1-sensors-25-06865\"><label>1</label>School of Information Science and Technology, Nantong University, Nantong 226019, China; <email>wenjiesong@stmail.ntu.edu.cn</email></aff><aff id=\"af2-sensors-25-06865\"><label>2</label>School of Artificial Intelligence and Computer Science, Nantong University, Nantong 226019, China</aff><aff id=\"af3-sensors-25-06865\"><label>3</label>School of Transportation and Civil Engineering, Nantong University, Nantong 226019, China; <email>xunqian_xu@ntu.edu.cn</email></aff><author-notes><corresp id=\"c1-sensors-25-06865\"><label>*</label>Correspondence: <email>zhao.m@ntu.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>10</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>6865</elocation-id><history><date date-type=\"received\"><day>03</day><month>10</month><year>2025</year></date><date date-type=\"rev-recd\"><day>07</day><month>11</month><year>2025</year></date><date date-type=\"accepted\"><day>08</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>10</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-06865.pdf\"/><abstract><p>Crack segmentation in cluttered scenes with slender and irregular patterns remains difficult, and practical systems must balance accuracy and efficiency. We present CONTI-CrackNet, which is a lightweight visual state-space network that integrates a Multi-Directional Selective Scanning Strategy (MD3S). MD3S performs bidirectional scanning along the horizontal, vertical, and diagonal directions, and it fuses the complementary paths with a Bidirectional Gated Fusion (BiGF) module to strengthen global continuity. To preserve fine details while completing global texture, we propose a Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module that incorporates a Pixel-Adaptive Pooling (PAP) mechanism to dynamically weight max-pooled responses and average-pooled responses. Evaluated on two public benchmarks, the proposed method achieves an F1 score (F1) of 0.8332 and a mean Intersection over Union (mIoU) of 0.8436 on the TUT dataset, and it achieves an mIoU of 0.7760 on the CRACK500 dataset, surpassing competitive Convolutional Neural Network (CNN), Transformer, and Mamba baselines. With 512 &#215; 512 input, the model requires 24.22 G floating point operations (GFLOPs), 6.01 M parameters (Params), and operates at 42 frames per second (FPS) on an RTX 3090 GPU, delivering a favorable accuracy&#8211;efficiency balance. These results show that CONTI-CrackNet improves continuity and edge recovery for thin cracks while keeping computational cost low, and it is lightweight in terms of parameter count and computational cost.</p></abstract><kwd-group><kwd>crack segmentation</kwd><kwd>segmentation</kwd><kwd>Mamba</kwd><kwd>deep learning</kwd><kwd>lightweight networks</kwd><kwd>feature extraction</kwd></kwd-group><funding-group><award-group><funding-source>Nantong Natural Science Foundation of China</funding-source><award-id>JC2023073</award-id></award-group><funding-statement>This research was funded by the Nantong Natural Science Foundation of China (JC2023073).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-06865\"><title>1. Introduction</title><p>Cracks are among the most common and harmful defects [<xref rid=\"B1-sensors-25-06865\" ref-type=\"bibr\">1</xref>]. They weaken the load-bearing capacity and durability of structures, increase maintenance costs, and pose public-safety risks. Cracks manifest in diverse materials and structures, such as pavements, bricks, and tiles [<xref rid=\"B2-sensors-25-06865\" ref-type=\"bibr\">2</xref>]. Owing to irregular shapes, large-scale variations, and complex textures, accurate crack recognition remains challenging. Traditional crack detection still relies on manual inspection [<xref rid=\"B3-sensors-25-06865\" ref-type=\"bibr\">3</xref>]. This process is slow, costly, and subjective, which often leads to missed cases and inconsistent results and cannot meet large-scale application needs. Early segmentation methods mainly used digital image processing, such as edge detection, clustering, thresholding, and morphological operations [<xref rid=\"B4-sensors-25-06865\" ref-type=\"bibr\">4</xref>,<xref rid=\"B5-sensors-25-06865\" ref-type=\"bibr\">5</xref>,<xref rid=\"B6-sensors-25-06865\" ref-type=\"bibr\">6</xref>,<xref rid=\"B7-sensors-25-06865\" ref-type=\"bibr\">7</xref>], but their performance is limited in the presence of noise.</p><p>To overcome these limitations, researchers introduced Convolutional Neural Networks (CNNs) for crack segmentation [<xref rid=\"B8-sensors-25-06865\" ref-type=\"bibr\">8</xref>]. Although prior studies expanded the effective receptive field through pyramid pooling [<xref rid=\"B9-sensors-25-06865\" ref-type=\"bibr\">9</xref>], employed deformable convolutions to adaptively focus on relevant regions [<xref rid=\"B10-sensors-25-06865\" ref-type=\"bibr\">10</xref>], and incorporated lightweight attention in SegNeXt [<xref rid=\"B11-sensors-25-06865\" ref-type=\"bibr\">11</xref>] to highlight local features while reducing computational overhead, CNNs remain constrained by local convolutions [<xref rid=\"B12-sensors-25-06865\" ref-type=\"bibr\">12</xref>], which limits the modeling of long-range dependencies and complex semantic relations. In view of these shortcomings, Transformer-based methods have been applied to segmentation tasks, including hierarchical encoders coupled with Multi-layer Perceptron (MLP) decoders for cross-level feature aggregation [<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>] and CNN&#8211;Transformer dual-path architectures that exploit complementary strengths [<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>]. However, Transformers typically involve large parameter scales and substantial computational cost [<xref rid=\"B15-sensors-25-06865\" ref-type=\"bibr\">15</xref>], which restricts deployment on resource-constrained devices. Moreover, MaskSup [<xref rid=\"B16-sensors-25-06865\" ref-type=\"bibr\">16</xref>] performs context modeling for image segmentation via random masking, thereby improving segmentation performance without incurring any additional inference overhead.</p><p>To retain both global and local modeling while controlling computational cost, researchers have begun exploring state-space models for vision tasks [<xref rid=\"B17-sensors-25-06865\" ref-type=\"bibr\">17</xref>]. Mamba is an efficient model in this class, employing selective state spaces to model long sequences in linear time [<xref rid=\"B18-sensors-25-06865\" ref-type=\"bibr\">18</xref>] and thereby delivering strong global-context capture with high computational efficiency. Building on this idea, many studies have extended Mamba to computer vision. For example, VMamba [<xref rid=\"B19-sensors-25-06865\" ref-type=\"bibr\">19</xref>] divides an image into a sequence of patches and employs Visual State-Space (VSS) blocks for multi-directional scanning to better capture complex inter-patch dependencies. PlainMamba [<xref rid=\"B20-sensors-25-06865\" ref-type=\"bibr\">20</xref>] removes the hierarchical structure and adopts continuous 2D scanning, simplifying the network and improving inference efficiency. MambaIR [<xref rid=\"B21-sensors-25-06865\" ref-type=\"bibr\">21</xref>] incorporates local enhancement and channel attention and demonstrates improvements in image restoration. CSMamba [<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>] combines a CNN encoder with a Mamba decoder for remote-sensing image segmentation.</p><p>Overall, these studies indicate that Mamba offers clear advantages for global-information modeling. However, despite progress in image restoration and remote sensing [<xref rid=\"B23-sensors-25-06865\" ref-type=\"bibr\">23</xref>,<xref rid=\"B24-sensors-25-06865\" ref-type=\"bibr\">24</xref>,<xref rid=\"B25-sensors-25-06865\" ref-type=\"bibr\">25</xref>], in-depth investigations of crack segmentation remain limited. A few studies have introduced Mamba into crack segmentation. For example, CrackMamba [<xref rid=\"B26-sensors-25-06865\" ref-type=\"bibr\">26</xref>] adopts a Mamba-based encoder and a Snake-scan mechanism to strengthen global modeling; MambaCrackNet [<xref rid=\"B27-sensors-25-06865\" ref-type=\"bibr\">27</xref>] incorporates novel visual Mamba blocks, effectively capturing global semantic relations while reducing computational cost. Cracks exhibit a slender, irregular morphology with fuzzy boundaries, which requires models to preserve spatial continuity while balancing pixel-level local details and global texture. At present, crack segmentation primarily relies on architectures based on CNNs or Transformers [<xref rid=\"B28-sensors-25-06865\" ref-type=\"bibr\">28</xref>,<xref rid=\"B29-sensors-25-06865\" ref-type=\"bibr\">29</xref>,<xref rid=\"B30-sensors-25-06865\" ref-type=\"bibr\">30</xref>]. The former (e.g., SFIAN [<xref rid=\"B31-sensors-25-06865\" ref-type=\"bibr\">31</xref>]) excels at detailed feature extraction, whereas the latter (e.g., CT-crackseg [<xref rid=\"B32-sensors-25-06865\" ref-type=\"bibr\">32</xref>]) achieves gains in boundary modeling. Yet CNNs are limited by the locality of convolutions, whereas Transformers incur high computational cost due to attention [<xref rid=\"B33-sensors-25-06865\" ref-type=\"bibr\">33</xref>]. Consequently, achieving a favorable balance between segmentation quality and computational efficiency remains an open problem.</p><p>To address the above challenges, we propose CONTI-CrackNet. Our main contributions are outlined below:<list list-type=\"order\"><list-item><p>CONTI-CrackNet architecture: a cascaded and lightweight crack segmentation network that effectively represents complex crack morphology while markedly reducing computation.</p></list-item><list-item><p>Multi-Directional Selective Scanning Strategy (MD3S): efficient long-range modeling that characterizes crack continuity and shape from multiple directions combined with the Bidirectional Gated Fusion (BiGF) module to alleviate directional bias.</p></list-item><list-item><p>Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module: a Pixel-Adaptive Pooling (PAP) mechanism that balances max-pooled and average-pooled features at each pixel, preserving edge fidelity while improving global connectivity.</p></list-item></list></p></sec><sec sec-type=\"methods\" id=\"sec2-sensors-25-06865\"><title>2. Methods</title><sec id=\"sec2dot1-sensors-25-06865\"><title>2.1. Network Architecture</title><p>This paper proposes a new crack-segmentation model, CONTI-CrackNet; its overall structure is shown in <xref rid=\"sensors-25-06865-f001\" ref-type=\"fig\">Figure 1</xref>a. The model addresses multiple crack properties: it enhances shape modeling, captures global information about crack continuity, and preserves the local details of fine cracks. Building on this design, the model employs a Multi-Directional Selective Scanning Strategy (MD3S), a Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module, and a Pixel-Adaptive Pooling (PAP) mechanism to achieve fine-grained segmentation in complex scenes.</p><p>The input is an RGB image of shape <inline-formula><mml:math id=\"mm1\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant=\"normal\">H</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant=\"normal\">W</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where 3 denotes the channel count and <inline-formula><mml:math id=\"mm2\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">H</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm3\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">W</mml:mi></mml:mrow></mml:math></inline-formula> denote the image height and width, respectively. We first partition the image into <inline-formula><mml:math id=\"mm4\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mi mathvariant=\"normal\">H</mml:mi><mml:mi mathvariant=\"normal\">h</mml:mi></mml:mfrac></mml:mstyle><mml:mo>&#183;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mi mathvariant=\"normal\">W</mml:mi><mml:mi mathvariant=\"normal\">w</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> patches, where <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">h</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">w</mml:mi></mml:mrow></mml:math></inline-formula> are the height and width of each patch, and add positional encodings to each patch to preserve spatial information. The patch sequence is then fed into an Structure-Strengthened Visual State-Space (SSVSS) block for feature modeling. As shown in <xref rid=\"sensors-25-06865-f001\" ref-type=\"fig\">Figure 1</xref>b, the SSVSS block first applies a Gated Bottleneck Convolution (GBC) [<xref rid=\"B34-sensors-25-06865\" ref-type=\"bibr\">34</xref>] module to extract detailed features and then splits the stream into two branches. The lower branch expands the channels via a linear layer and applies SiLU. The upper branch applies a linear transform and activation, and then it introduces an MD3S to better capture the crack shape and structure. The outputs of the two branches are fused by element-wise multiplication and projected back to the original channel size via a linear layer.</p><p>To improve both local detail and global semantics, we further design DBPGL. The MD3S-processed features and the corresponding original features are fed into this module. This module employs PAP to fuse max-pooling and average-pooling features at the pixel level, thereby enhancing both global and local representations. Residual connections are added in the SSVSS block to preserve original details during fusion and mitigate gradient vanishing. During feature extraction, the network produces four feature maps at different scales: low-level maps primarily encode spatial details, whereas high-level maps carry richer semantic information. These features gradually integrate spatial structure and contextual information. Afterward, an MLP aligns the four scales to a common channel width, and dynamic upsampling [<xref rid=\"B35-sensors-25-06865\" ref-type=\"bibr\">35</xref>] restores them to the original image resolution. Finally, all feature maps are concatenated and further processed by a convolutional layer and an MLP to yield the final crack-segmentation map.</p></sec><sec id=\"sec2dot2-sensors-25-06865\"><title>2.2. Multi-Directional Selective Scanning Strategy (MD3S)</title><p>In recent years, two-dimensional scanning strategies have demonstrated strong feature-modeling capability in image analysis tasks. However, common scanning modes in visual Mamba models include parallel, diagonal, Z-order, and Hilbert scans [<xref rid=\"B36-sensors-25-06865\" ref-type=\"bibr\">36</xref>,<xref rid=\"B37-sensors-25-06865\" ref-type=\"bibr\">37</xref>]. Each mode follows a single strategy (<xref rid=\"sensors-25-06865-f002\" ref-type=\"fig\">Figure 2</xref>). Parallel scans better capture long-range dependencies along the horizontal and vertical directions; diagonal and Z-order scans are more suitable for modeling global information along diagonal paths; and Hilbert scans preserve spatial continuity to some extent but are weaker in capturing global information. Thus, single-strategy modes are limited when handling complex structures and cannot fully represent slender, irregular cracks across multiple directions [<xref rid=\"B38-sensors-25-06865\" ref-type=\"bibr\">38</xref>].</p><p>To address this limitation, we propose the Multi-Directional Selective Scanning Strategy (MD3S). MD3S scans along four directions: horizontal, vertical, main-diagonal, and anti-diagonal. It performs forward and backward passes in each direction to obtain four bidirectional feature sequences (<xref rid=\"sensors-25-06865-f003\" ref-type=\"fig\">Figure 3</xref>a). To reduce redundancy and emphasize complementary information between the bidirectional sequences [<xref rid=\"B39-sensors-25-06865\" ref-type=\"bibr\">39</xref>], we introduce a Bidirectional Gated Fusion (BiGF) module (<xref rid=\"sensors-25-06865-f003\" ref-type=\"fig\">Figure 3</xref>b), which adaptively assigns weights to the forward and backward paths and fuses them: <disp-formula id=\"FD1-sensors-25-06865\"><label>(1)</label><mml:math id=\"mm7\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi><mml:mo>=</mml:mo><mml:mi>SiLU</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Linear</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Norm</mml:mi><mml:mfenced open=\"(\" close=\")\"><mml:msub><mml:mi mathvariant=\"bold\">T</mml:mi><mml:mrow><mml:mi mathvariant=\"bold\">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn mathvariant=\"bold\">1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-sensors-25-06865\"><label>(2)</label><mml:math id=\"mm8\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">T</mml:mi><mml:mi mathvariant=\"bold\">t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Linear</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">forward</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi></mml:mfenced><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">backward</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">T</mml:mi><mml:mrow><mml:mi mathvariant=\"bold\">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn mathvariant=\"bold\">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">T</mml:mi><mml:mrow><mml:mi mathvariant=\"bold\">t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn mathvariant=\"bold\">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the original feature and <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">T</mml:mi><mml:mi mathvariant=\"bold\">t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the output feature; <inline-formula><mml:math id=\"mm11\" overflow=\"scroll\"><mml:mrow><mml:mi>Norm</mml:mi></mml:mrow></mml:math></inline-formula> denotes normalization, <inline-formula><mml:math id=\"mm12\" overflow=\"scroll\"><mml:mrow><mml:mi>Linear</mml:mi></mml:mrow></mml:math></inline-formula> denotes a linear projection, and <inline-formula><mml:math id=\"mm13\" overflow=\"scroll\"><mml:mrow><mml:mi>SiLU</mml:mi></mml:mrow></mml:math></inline-formula> is the activation function; <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">forward</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">backward</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the features from the forward and backward paths, respectively.</p><p>After the gated fusion, the four fused sequences are passed to the S6 block [<xref rid=\"B18-sensors-25-06865\" ref-type=\"bibr\">18</xref>]. A subsequent scan&#8211;merge operation concatenates the sequences from different directions and maps them back to the original spatial resolution, yielding a representation that preserves multi-directional structural awareness and global-context modeling.</p></sec><sec id=\"sec2dot3-sensors-25-06865\"><title>2.3. Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) Module</title><p>To preserve fine-grained details from the original sequence while fusing the crack texture patterns produced by MD3S, we propose the DBPGL module. DBPGL incorporates a Pixel-Adaptive Pooling (PAP) mechanism within a dual-branch design. It performs dynamic weighting at the pixel level, thereby strengthening the SSVSS module&#8217;s capacity to model and learn crack features.</p><p>As shown in <xref rid=\"sensors-25-06865-f004\" ref-type=\"fig\">Figure 4</xref>a, DBPGL fuses two inputs using a dual-branch structure, focusing on local detail preservation and global texture completion. In the left branch, the input feature passes through a 1 &#215; 1 convolution for channel reduction, which is followed by ReLU to introduce nonlinearity and enhance representation. A 1 &#215; 1 convolution then restores the original channel number. Batch normalization is applied to improve training stability and generalization. The right branch employs PAP to complement global structural information related to crack continuity. Finally, the outputs of the two branches are fused. Element-wise operations compute per channel attention weights via a sigmoid function, which enables adaptive importance assignment to the two branches. The fusion can be written as the equations below.<disp-formula id=\"FD3-sensors-25-06865\"><label>(3)</label><mml:math id=\"mm16\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>BN</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Conv</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>ReLU</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Conv</mml:mi><mml:mfenced open=\"(\" close=\")\"><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-06865\"><label>(4)</label><mml:math id=\"mm17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>BN</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Conv</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>ReLU</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>Conv</mml:mi><mml:mfenced open=\"(\" close=\")\"><mml:mi mathvariant=\"bold\">Y</mml:mi></mml:mfenced></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD5-sensors-25-06865\"><label>(5)</label><mml:math id=\"mm18\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn mathvariant=\"bold\">1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>Sigmoid</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">g</mml:mi></mml:msub></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"normal\">X</mml:mi><mml:mo>+</mml:mo><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>Sigmoid</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">g</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"bold\">Y</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm19\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">Y</mml:mi></mml:mrow></mml:math></inline-formula> denote input a and input b; <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mi>Conv</mml:mi></mml:mrow></mml:math></inline-formula> denotes the 1 &#215; 1 convolution; <inline-formula><mml:math id=\"mm22\" overflow=\"scroll\"><mml:mrow><mml:mi>ReLU</mml:mi></mml:mrow></mml:math></inline-formula> denotes the activation function; <inline-formula><mml:math id=\"mm23\" overflow=\"scroll\"><mml:mrow><mml:mi>BN</mml:mi></mml:mrow></mml:math></inline-formula> denotes batch normalization; <inline-formula><mml:math id=\"mm24\" overflow=\"scroll\"><mml:mrow><mml:mi>Sigmoid</mml:mi></mml:mrow></mml:math></inline-formula> denotes the activation function. <inline-formula><mml:math id=\"mm25\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the output of the left branch of the module, <inline-formula><mml:math id=\"mm26\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the output of the right branch, and <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn mathvariant=\"bold\">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the fused output obtained by weighting and combining the features from both branches.</p></sec><sec id=\"sec2dot4-sensors-25-06865\"><title>2.4. Pixel-Adaptive Pooling (PAP) Mechanism</title><p>In feature extraction, max pooling and average pooling are often used together because they are complementary. Max pooling focuses on high-response regions, whereas average pooling integrates global background information and yields a balanced representation [<xref rid=\"B40-sensors-25-06865\" ref-type=\"bibr\">40</xref>]. To leverage both, we design a pixel-level adaptive fusion strategy within PAP, as illustrated in <xref rid=\"sensors-25-06865-f004\" ref-type=\"fig\">Figure 4</xref>b. Specifically, the input features <inline-formula><mml:math id=\"mm28\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm29\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">Y</mml:mi></mml:mrow></mml:math></inline-formula> are split into two paths and processed by max pooling and average pooling to obtain complementary spatial features. During two-branch feature fusion, a fixed rule cannot adapt to crack structures at the pixel scale. We introduce the Pixel Attention Mechanism (PAM) [<xref rid=\"B41-sensors-25-06865\" ref-type=\"bibr\">41</xref>] to enable adaptive, pixel-level fusion. It assigns weights to the two complementary branches at each spatial location, preserving fine details and maintaining global connectivity. This mechanism applies a 1 &#215; 1 convolution followed by batch normalization to each path, performs element-wise multiplication for interaction, and then applies another 1 &#215; 1 convolution and batch normalization. The output is passed through a sigmoid to produce a gating coefficient <inline-formula><mml:math id=\"mm30\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi></mml:mrow></mml:math></inline-formula>, which encodes the dynamic preference between the two branches. Finally, the pixel attention weights are applied to the two features. Fusion is completed by weighted multiplication and element-wise addition, which is formulated as shown below: <disp-formula id=\"FD6-sensors-25-06865\"><label>(6)</label><mml:math id=\"mm31\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi><mml:mo>=</mml:mo><mml:mi>Sigmoid</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>BNConv</mml:mi><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mi>BNConv</mml:mi><mml:mfenced open=\"(\" close=\")\"><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi mathvariant=\"bold\">a</mml:mi></mml:msub></mml:mfenced><mml:mo>&#183;</mml:mo><mml:mi>BNConv</mml:mi><mml:mfenced open=\"(\" close=\")\"><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi mathvariant=\"bold\">b</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD7-sensors-25-06865\"><label>(7)</label><mml:math id=\"mm32\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn mathvariant=\"bold\">2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">max</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi mathvariant=\"bold-sans-serif\">&#963;</mml:mi></mml:mfenced><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">avg</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm33\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi mathvariant=\"bold\">a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm34\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi mathvariant=\"bold\">b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are pixels from the two-branch feature maps; <inline-formula><mml:math id=\"mm35\" overflow=\"scroll\"><mml:mrow><mml:mi>BNConv</mml:mi></mml:mrow></mml:math></inline-formula> denotes a <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution followed by batch normalization, i.e., <inline-formula><mml:math id=\"mm37\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>BNConv</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>BN</mml:mi><mml:mo>(</mml:mo><mml:mi>Conv</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; <inline-formula><mml:math id=\"mm38\" overflow=\"scroll\"><mml:mrow><mml:mi>Sigmoid</mml:mi></mml:mrow></mml:math></inline-formula> is the activation function; <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">max</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm40\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mi mathvariant=\"bold\">avg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the feature maps produced by max pooling and average pooling, respectively; and <inline-formula><mml:math id=\"mm41\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">Z</mml:mi><mml:mn mathvariant=\"bold\">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the fused output obtained by weighting and combining the features from both branches. With this design, the network fuses important local features with global context and can adapt, at the pixel level, the contribution of different spatial locations, which improves the modeling of complex crack structures.</p></sec></sec><sec sec-type=\"results\" id=\"sec3-sensors-25-06865\"><title>3. Results</title><sec id=\"sec3dot1-sensors-25-06865\"><title>3.1. Datasets</title><p>We validate our approach on two public benchmark datasets for crack segmentation: TUT [<xref rid=\"B42-sensors-25-06865\" ref-type=\"bibr\">42</xref>] and CRACK500 [<xref rid=\"B43-sensors-25-06865\" ref-type=\"bibr\">43</xref>]. TUT covers diverse scenes with complex backgrounds, which helps assess the model&#8217;s robustness and generalization across scenes and under strong noise. CRACK500 is a standard pavement crack benchmark that enables fair comparison with prior methods. <xref rid=\"sensors-25-06865-t001\" ref-type=\"table\">Table 1</xref> reports the main characteristics and split schemes of the datasets, while <xref rid=\"sensors-25-06865-f005\" ref-type=\"fig\">Figure 5</xref> presents representative images and the corresponding ground truth masks.</p><list list-type=\"order\"><list-item><p>TUT [<xref rid=\"B42-sensors-25-06865\" ref-type=\"bibr\">42</xref>]: Unlike datasets with simple backgrounds, TUT contains dense and cluttered scenes with diverse crack shapes. It includes 1408 RGB images from eight representative scenes: bitumen, cement, bricks, runways, tiles, metal, generator blades, and underground pipelines. Of these, 1270 images were collected in-house using mobile phones, and 138 images were sourced from the internet.</p></list-item><list-item><p>CRACK500 [<xref rid=\"B43-sensors-25-06865\" ref-type=\"bibr\">43</xref>]: Proposed by Yang et al., the original dataset contains 500 bitumen crack images at 2000 &#215; 1500 resolution, which were all captured by mobile phones. Because the dataset is small, images are cropped into 16 nonoverlapping patches, and only samples with more than 1000 crack pixels are retained. After this processing, each patch has a resolution of 640 &#215; 320. Data augmentation increases the total to 3368 images, and each sample is paired with a per-pixel binary mask.</p></list-item></list></sec><sec id=\"sec3dot2-sensors-25-06865\"><title>3.2. Evaluation Metrics</title><p>To comprehensively evaluate the proposed segmentation model, we use several common metrics: Optimal Dataset Scale (ODS), Optimal Image Scale (OIS), Precision (P), Recall (R), F1 score (F1), and mean Intersection over Union (mIoU). The ODS evaluates the model on the whole dataset under a single fixed threshold m. The OIS evaluates performance when an optimal threshold n is chosen for each image. The F1, as the harmonic mean of P and R, balances both and is widely used to assess overall robustness and reliability in crack segmentation. In addition, mIoU quantifies spatial accuracy by the overlap between the predicted mask and the ground-truth annotation. They are defined as shown below:<disp-formula id=\"FD8-sensors-25-06865\"><label>(8)</label><mml:math id=\"mm42\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>ODS</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mi>max</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:munder><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">R</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">R</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD9-sensors-25-06865\"><label>(9)</label><mml:math id=\"mm43\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>OIS</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:munderover><mml:munder><mml:mi>max</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi></mml:munder><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">R</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">R</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD10-sensors-25-06865\"><label>(10)</label><mml:math id=\"mm44\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"normal\">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant=\"normal\">R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD11-sensors-25-06865\"><label>(11)</label><mml:math id=\"mm45\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:munderover><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mrow><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">j</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm46\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi></mml:mrow></mml:math></inline-formula> is the number of classes; <inline-formula><mml:math id=\"mm47\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:math></inline-formula> indexes the ground-truth class and <inline-formula><mml:math id=\"mm48\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">j</mml:mi></mml:mrow></mml:math></inline-formula> denotes the predicted class. Let <inline-formula><mml:math id=\"mm49\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the number of pixels of ground-truth class <inline-formula><mml:math id=\"mm50\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:math></inline-formula> predicted as class <inline-formula><mml:math id=\"mm51\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">j</mml:mi></mml:mrow></mml:math></inline-formula> (thus, <inline-formula><mml:math id=\"mm52\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the true positives for class <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"normal\">i</mml:mi></mml:mrow></mml:math></inline-formula>). In this case, <inline-formula><mml:math id=\"mm54\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id=\"sec3dot3-sensors-25-06865\"><title>3.3. Implementation Details and Training</title><p>The proposed network is implemented in PyTorch v1.13.1 and trained on an Intel Xeon Platinum 8336C CPU. AdamW is used with an initial learning rate of 5 &#215; 10<sup>&#8722;4</sup> and the PolyLR scheduler for dynamic adjustment. The weight decay is 0.01, and the random seed is 42. Training runs for 50 epochs. All input images are resized to 512 &#215; 512 pixels, and the batch size is 8. After each epoch, performance on the validation set is measured, and the checkpoint with the best validation score is kept for later testing.</p><p>A joint loss that combines Binary Cross-Entropy (BCE) and Dice loss [<xref rid=\"B44-sensors-25-06865\" ref-type=\"bibr\">44</xref>] is adopted. BCE measures per-pixel classification accuracy, while Dice focuses on the overlap between the predicted mask and the ground truth, improving coherence and completeness. The combined loss is shown below: <disp-formula id=\"FD12-sensors-25-06865\"><label>(12)</label><mml:math id=\"mm56\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi><mml:mspace width=\"4pt\"/><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi>BCE</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced separators=\"\" open=\"(\" close=\")\"><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi></mml:mfenced><mml:mspace width=\"4pt\"/><mml:mo>&#183;</mml:mo><mml:msub><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi>Dice</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm57\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi>BCE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the BEC, <inline-formula><mml:math id=\"mm58\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi>Dice</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the Dice loss, and <inline-formula><mml:math id=\"mm59\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi></mml:mrow></mml:math></inline-formula> controls the weights of the two terms. As shown in <xref rid=\"sensors-25-06865-t002\" ref-type=\"table\">Table 2</xref>, we conduct a sensitivity study of the loss-weight hyperparameter <inline-formula><mml:math id=\"mm60\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi></mml:mrow></mml:math></inline-formula> on the TUT dataset; the results indicate that <inline-formula><mml:math id=\"mm61\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi></mml:mrow></mml:math></inline-formula> = 0.2 achieves the best performance.</p></sec><sec id=\"sec3dot4-sensors-25-06865\"><title>3.4. Experimental Results</title><p>We evaluate the proposed model against seven representative baseline networks: SFIAN [<xref rid=\"B31-sensors-25-06865\" ref-type=\"bibr\">31</xref>], SegNeXt [<xref rid=\"B11-sensors-25-06865\" ref-type=\"bibr\">11</xref>], Crackmer [<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>], SegFormer [<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>], CT-crackseg [<xref rid=\"B32-sensors-25-06865\" ref-type=\"bibr\">32</xref>], CSMamba [<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>], and PlainMamba [<xref rid=\"B20-sensors-25-06865\" ref-type=\"bibr\">20</xref>]. On two public datasets, we make a systematic comparison between these methods and our model. Specifically, SFIAN and SegNeXt are built on CNNs; Crackmer, SegFormer, and CT-crackseg adopt the Transformer architecture; CSMamba and PlainMamba follow the Mamba framework.</p><sec id=\"sec3dot4dot1-sensors-25-06865\"><title>3.4.1. Experimental Results on Dataset TUT</title><p>On the TUT dataset, CONTI-CrackNet achieves the best results across all metrics. As shown in <xref rid=\"sensors-25-06865-t003\" ref-type=\"table\">Table 3</xref>, our model reaches an F1 of 0.8332 and an mIoU of 0.8436, outperforming architectures based on CNNs, Transformers, and Mamba. Compared with SegNeXt, F1 improves by 0.0815, indicating a stronger modeling of thin and low-contrast cracks, with fewer breaks and missed segments along long, slender structures. Compared with the best Transformer baseline, CT-crackseg, R increases by 0.0248, confirming advantages in accuracy. Compared with strong Mamba-based baselines, the proposed method attains higher accuracy; relative to PlainMamba, mIoU increases by 0.0120, demonstrating clear competitiveness within the same architectural class. In addition, a standard deviation analysis was conducted on the TUT dataset. The standard deviations of ODS, OIS, F1, P, R, and mIoU are 0.0114, 0.0106, 0.0065, 0.0043, 0.0102, and 0.0071, respectively. These results demonstrate that the proposed model performs well and exhibits strong engineering reliability.</p><p>Beyond the numbers, our visual results also show clear gains. As shown in <xref rid=\"sensors-25-06865-f006\" ref-type=\"fig\">Figure 6</xref>, in scenes with complex backgrounds and uneven lighting, SegNeXt and SegFormer tend to produce false cracks and false positives, while CONTI-CrackNet suppresses background noise. For very thin cracks or low contrast, CNN and Transformer methods often show broken or missed segments, but our method restores complete crack shapes. For long and curved cracks, PlainMamba still shows blurred edges and local breaks, while CONTI-CrackNet keeps global continuity and sharp boundaries. These improvements come from the MD3S, which models crack continuity, and the DBPGL module, which fuses global and local features. Together, they enable robust segmentation in complex scenes and demonstrate the effectiveness and practical value of our method.</p></sec><sec id=\"sec3dot4dot2-sensors-25-06865\"><title>3.4.2. Experimental Results on Dataset CRACK500</title><p>On the CRACK500 dataset, the results further verify the effectiveness of our method. As shown in <xref rid=\"sensors-25-06865-t004\" ref-type=\"table\">Table 4</xref>, CONTI-CrackNet attains an mIoU of 0.7760, outperforming all compared methods on every metric. Compared with the second-best PlainMamba, our model improves by 0.0181 on mIoU, respectively. Against the CNN and Transformer baselines, the gains are larger, especially on F1 and mIoU, which shows stronger robustness in preserving overall crack structure and regional consistency. On CRACK500, the standard deviations for ODS, OIS, F1, P, R, and mIoU are 0.0057, 0.0102, 0.0091, 0.0146, 0.0052, and 0.0119, respectively. This finding demonstrates the consistency and reliability of the proposed model across diverse evaluation conditions.</p><p>The visual results in <xref rid=\"sensors-25-06865-f007\" ref-type=\"fig\">Figure 7</xref> also show clear advantages. In scenes with complex backgrounds and strong texture noise, CNN and Transformer models such as SegNeXt and SegFormer often produce false positives or fake cracks, as seen in the first and fifth rows of <xref rid=\"sensors-25-06865-f007\" ref-type=\"fig\">Figure 7</xref>. When crack shapes are complex, Mamba methods can still show over segmentation or blurry boundaries, as illustrated in the second row. In contrast, CONTI-CrackNet accurately extracts the main crack skeleton in most cases and maintains sharp edges and global continuity; even under high noise or low contrast, it avoids several false segmentations. Although CONTI-CrackNet does not perfectly recover every tiny crack, its results are closer to the ground truth and contain much less noise overall.</p></sec></sec><sec id=\"sec3dot5-sensors-25-06865\"><title>3.5. Ablation Study</title><p>To verify the effectiveness of the proposed modules, we conduct a systematic ablation study on the TUT dataset. Specifically, we replace, enable, or disable the proposed modules to systematically assess each module&#8217;s impact on segmentation performance. This design quantifies the contributions of each component and the proposed scanning strategy and further confirms the soundness and effectiveness of the approach.</p><sec id=\"sec3dot5dot1-sensors-25-06865\"><title>3.5.1. Ablation Study of Components</title><p>We further conduct ablation studies on the modules introduced in SSVSS. As summarized in <xref rid=\"sensors-25-06865-t005\" ref-type=\"table\">Table 5</xref>, the full configuration integrating MD3S, DBPGL, and PAP attains the best overall metrics, validating its effectiveness for crack segmentation. The baseline excludes all proposed modules and only employs the standard SS2D scanning [<xref rid=\"B17-sensors-25-06865\" ref-type=\"bibr\">17</xref>] with element-wise addition (ElemAdd) for fusion. On this basis, introducing MD3S increases the mIoU to 0.8350, indicating that MD3S effectively captures directional continuity and multi-orientation morphology.</p><p>In the comparison of fusion strategies, both ElemAdd and SKNet [<xref rid=\"B45-sensors-25-06865\" ref-type=\"bibr\">45</xref>] underperform DBPGL. With the addition of PAP, the metrics further improve to 0.8436, suggesting that the combination of DBPGL and PAP enables fine-grained, pixel-adaptive gating between the two branches. Moreover, the global&#8211;local dual-branch design better suits crack scenarios, where max-pooled and average-pooled features are complementarily fused at the pixel level, thereby preserving the connectivity of slender and discontinuous cracks while suppressing texture noise.</p><p>In summary, MD3S, DBPGL, and PAP exhibit clear divisions of labor and synergy: MD3S provides direction-aware global modeling and continuity completion; DBPGL together with PAP delivers complementary global&#8211;local fusion with pixel-level adaptive weighting to refine fusion decisions. Their joint effect leads to substantial performance gains in crack segmentation.</p></sec><sec id=\"sec3dot5dot2-sensors-25-06865\"><title>3.5.2. Ablation on Scanning Strategies</title><p>As shown in <xref rid=\"sensors-25-06865-t006\" ref-type=\"table\">Table 6</xref>, we compare four directional scanning strategies under the same settings. In this experiment, we disable both DBPGL and PAP, and we perform feature fusion using only ElemAdd. Using parallel serpentine scanning (ParaSpn), with forward and backward traversals along the horizontal and vertical directions, the model achieves F1 = 0.8063. Using diagonal serpentine scanning (DiagSpn) [<xref rid=\"B37-sensors-25-06865\" ref-type=\"bibr\">37</xref>], with forward and backward traversals along the main and anti-diagonal directions, the model attains F1 = 0.8077, which remains relatively low. Combining the two (ParaSpn + DiagSpn), employing both parallel and diagonal serpentine scans, raises the F1 to 0.8127 and the mIoU to 0.8260, indicating that multi-directional cues benefit crack segmentation. Furthermore, after introducing the proposed MD3S, the model achieves new best results across all metrics. Compared with the combination of ParaSpn and DiagSpn, F1 and P improve by 0.0141 and 0.0252, respectively. These results indicate that the proposed strategy captures multi-direction features and strengthens the modeling of complex crack structures. Compared with a single scanning strategy, our method builds forward&#8211;backward (bidirectional) sequences in four directions: horizontal, vertical, main diagonal, and anti-diagonal. It then performs direction-wise fusion. This lets the network learn pixel-level adaptive preferences. Compared with simple multi-direction stacking, the per-direction bidirectional fusion better fits the modeling needs of multi-scale and multi-directional geometric features. It yields superior performance on crack.</p></sec><sec id=\"sec3dot5dot3-sensors-25-06865\"><title>3.5.3. Ablation Study of the Attention Mechanism in the PAP Module</title><p>To substantiate the necessity of the proposed Pixel Attention Mechanism (PAM), we conducted a controlled comparison under identical experimental settings (<xref rid=\"sensors-25-06865-t007\" ref-type=\"table\">Table 7</xref>). In this study, MD3S and DBPGL were kept enabled, and only the weight-generation unit in the fusion stage was swapped between the Convolutional Block Attention Module (CBAM) [<xref rid=\"B46-sensors-25-06865\" ref-type=\"bibr\">46</xref>] and PAM. The results show that the model with PAM delivers the best performance, indicating that pixel-level, adaptive gating weights better discriminate thin cracks from background textures than CBAM, effectively reducing false positives and breakages and achieving finer pixel-wise segmentation.</p></sec></sec><sec id=\"sec3dot6-sensors-25-06865\"><title>3.6. Complexity Analysis</title><p><xref rid=\"sensors-25-06865-t008\" ref-type=\"table\">Table 8</xref> reports the comparison under a fixed input size of 512 &#215; 512. Our method requires 24.22 G floating point operations (GFLOPs), has 6.01M parameters (Params), and runs at 42 frames per second (FPS). Compared with the lightweight Crackmer [<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>], our model is slightly heavier but delivers a clear accuracy gain, achieving a better accuracy&#8211;efficiency trade-off. At the same time, compared with larger and more complex networks such as CSMamba [<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>] and SegFormer [<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>], our method uses fewer GFLOPs and Params and offers much faster inference. In sum, CONTI-CrackNet provides high segmentation accuracy with low computational cost and fast runtime, making it suitable for research and practical deployment.</p></sec><sec id=\"sec3dot7-sensors-25-06865\"><title>3.7. Analysis of Failure Cases and Limitations</title><p>To objectively evaluate the performance of CONTI-CrackNet, we analyze the failure cases observed in our experiments. <xref rid=\"sensors-25-06865-f008\" ref-type=\"fig\">Figure 8</xref> illustrates two representative scenarios: extreme noisy backgrounds and complex crack intersections. Experiments indicate that in complex scenarios with strong noise and frequent crack intersections, the continuity of fine cracks remains disturbed. Fine cracks may not be fully recovered because complex backgrounds cause confusion between crack pixels and background textures, and intricate crack geometries tend to induce discontinuities at intersections. Although CONTI-CrackNet may exhibit under-segmentation in a few extreme cases, the proposed architecture improves segmentation accuracy under challenging conditions. In future work, we will further enhance the model to increase noise robustness and multi-scale handling, thereby improving the overall performance.</p></sec></sec><sec sec-type=\"discussion\" id=\"sec4-sensors-25-06865\"><title>4. Discussion and Conclusions</title><p>We propose CONTI-CrackNet, which is a lightweight crack-segmentation network that improves pixel-level fine segmentation under low computational cost. Its performance stems from three aspects: (1) a cascaded lightweight backbone that captures crack morphology while reducing resource usage; (2) MD3S, which aggregates multi-directional information to model the global context of thin and irregular cracks; and (3) DBPGL with a PAP mechanism, which employs dual-branch pixel attention with max and average pooling to enhance local details and overall structural perception. On TUT and CRACK500, the model achieves superior or comparable accuracy with 24.22 G GFLOPs and 6.01 M parameters while maintaining high inference speed. Ablation studies show that MD3S strengthens continuity, and DBPGL with PAP improves segmentation by coupling global dependencies with detail enhancement. Limitations remain: validation is conducted on two public datasets and focuses on static images. Future work will expand data diversity, improve small-crack segmentation, investigate crack-depth quantification, and assess pathways toward efficient edge execution.</p></sec></body><back><ack><title>Acknowledgments</title><p>During the preparation of this manuscript, the authors used ChatGPT (OpenAI, GPT-5, <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.openai.com/chatgpt\">https://www.openai.com/chatgpt</uri>, accessed on 22 September 2025) for language editing. The authors reviewed and edited the output and take full responsibility for the content of this publication.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, W.S. and M.Z.; methodology, W.S.; software, W.S.; validation, W.S., M.Z. and X.X.; formal analysis, W.S.; investigation, W.S.; resources, W.S.; data curation, W.S.; writing&#8212;original draft preparation, W.S.; writing&#8212;review and editing, M.Z. and X.X.; visualization, W.S.; supervision, M.Z.; project administration, M.Z.; funding acquisition, M.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>Publicly available datasets were analyzed in this study. The TUT dataset can be found at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/Karl1109/CrackSCF\">https://github.com/Karl1109/CrackSCF</uri>, accessed on 2 July 2025, and the CRACK500 dataset is available at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/fyangneil/pavement-crack-detection\">https://github.com/fyangneil/pavement-crack-detection</uri>, accessed on 2 July 2025.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-06865\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Withers</surname><given-names>P.J.</given-names></name><name name-style=\"western\"><surname>Beretta</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Kang</surname><given-names>G.</given-names></name></person-group><article-title>Editorial: Tomography traces the growing cracks and defects</article-title><source>Eng. Fract. Mech.</source><year>2023</year><volume>292</volume><fpage>109628</fpage><pub-id pub-id-type=\"doi\">10.1016/j.engfracmech.2023.109628</pub-id></element-citation></ref><ref id=\"B2-sensors-25-06865\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Matarneh</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Elghaish</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Edwards</surname><given-names>D.J.</given-names></name><name name-style=\"western\"><surname>Rahimian</surname><given-names>F.P.</given-names></name><name name-style=\"western\"><surname>Abdellatef</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Ejohwomu</surname><given-names>O.</given-names></name></person-group><article-title>Automatic crack classification on asphalt pavement surfaces using convolutional neural networks and transfer learning</article-title><source>J. Inf. Technol. Constr.</source><year>2024</year><volume>29</volume><fpage>1239</fpage><lpage>1256</lpage><pub-id pub-id-type=\"doi\">10.36680/j.itcon.2024.055</pub-id></element-citation></ref><ref id=\"B3-sensors-25-06865\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Singh</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Baral</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Tummala</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Noori</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yadav</surname><given-names>S.V.</given-names></name><name name-style=\"western\"><surname>Kang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>W.</given-names></name></person-group><article-title>A Hybrid Deep Learning Model for Enhanced Structural Damage Detection: Integrating ResNet50, GoogLeNet, and Attention Mechanisms</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7249</elocation-id><pub-id pub-id-type=\"doi\">10.3390/s24227249</pub-id><pub-id pub-id-type=\"pmid\">39599027</pub-id><pub-id pub-id-type=\"pmcid\">PMC11598465</pub-id></element-citation></ref><ref id=\"B4-sensors-25-06865\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Gui</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ji</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><etal/></person-group><article-title>EU-Net: A segmentation network based on semantic fusion and edge guidance for road crack images</article-title><source>Appl. Intell.</source><year>2024</year><volume>54</volume><fpage>12949</fpage><lpage>12963</lpage><pub-id pub-id-type=\"doi\">10.1007/s10489-024-05788-1</pub-id></element-citation></ref><ref id=\"B5-sensors-25-06865\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H.</given-names></name></person-group><article-title>Clustering and diagnosis of crack images of tunnel linings via graph neural networks</article-title><source>Georisk Assess. Manag. Risk Eng. Syst. Geohazards</source><year>2024</year><volume>18</volume><fpage>825</fpage><lpage>837</lpage><pub-id pub-id-type=\"doi\">10.1080/17499518.2024.2337380</pub-id></element-citation></ref><ref id=\"B6-sensors-25-06865\"><label>6.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lei</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhong</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Joint optimization of crack segmentation with an adaptive dynamic threshold module</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2024</year><volume>25</volume><fpage>6902</fpage><lpage>6916</lpage><pub-id pub-id-type=\"doi\">10.1109/TITS.2023.3348812</pub-id></element-citation></ref><ref id=\"B7-sensors-25-06865\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yeoh</surname><given-names>J.K.W.</given-names></name></person-group><article-title>Robust pixel-wise concrete crack segmentation and properties retrieval using image patches</article-title><source>Autom. Constr.</source><year>2021</year><volume>123</volume><fpage>103535</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2020.103535</pub-id></element-citation></ref><ref id=\"B8-sensors-25-06865\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yamaguchi</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Mizutani</surname><given-names>T.</given-names></name></person-group><article-title>Road crack detection interpreting background images by convolutional neural networks and a self-organizing map</article-title><source>Comput.-Aided Civ. Infrastruct. Eng.</source><year>2024</year><volume>39</volume><fpage>1616</fpage><lpage>1640</lpage><pub-id pub-id-type=\"doi\">10.1111/mice.13132</pub-id></element-citation></ref><ref id=\"B9-sensors-25-06865\"><label>9.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Pyramid Scene Parsing Network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>22&#8211;25 July 2017</conf-date><fpage>2881</fpage><lpage>2890</lpage><pub-id pub-id-type=\"doi\">10.1109/CVPR.2017.660</pub-id></element-citation></ref><ref id=\"B10-sensors-25-06865\"><label>10.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>J.</given-names></name></person-group><article-title>Deformable ConvNets V2: More Deformable, Better Results</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>9308</fpage><lpage>9316</lpage><pub-id pub-id-type=\"doi\">10.1109/CVPR.2019.00954</pub-id></element-citation></ref><ref id=\"B11-sensors-25-06865\"><label>11.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>M.H.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>C.Z.</given-names></name><name name-style=\"western\"><surname>Hou</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>M.M.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>S.M.</given-names></name></person-group><article-title>SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation</article-title><source>Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>28 November&#8211;9 December 2022</conf-date><volume>Volume 35</volume><fpage>1140</fpage><lpage>1156</lpage></element-citation></ref><ref id=\"B12-sensors-25-06865\"><label>12.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>G.</given-names></name></person-group><article-title>ConvTransNet: A CNN-Transformer Network for Change Detection with Multiscale Global-Local Representations</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>5610315</fpage><pub-id pub-id-type=\"doi\">10.1109/TGRS.2023.3272694</pub-id></element-citation></ref><ref id=\"B13-sensors-25-06865\"><label>13.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Anandkumar</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Alvarez</surname><given-names>J.M.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2105.15203</pub-id><pub-id pub-id-type=\"arxiv\">2105.15203</pub-id></element-citation></ref><ref id=\"B14-sensors-25-06865\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Sharma</surname><given-names>P.K.</given-names></name><name name-style=\"western\"><surname>Alfarraj</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Tolba</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L.</given-names></name></person-group><article-title>Dual-path network combining CNN and transformer for pavement crack segmentation</article-title><source>Autom. Constr.</source><year>2024</year><volume>158</volume><fpage>105217</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2023.105217</pub-id></element-citation></ref><ref id=\"B15-sensors-25-06865\"><label>15.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Yao</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Bell</surname><given-names>M.G.H.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name></person-group><article-title>ST-MambaSync: Complement the power of Mamba and Transformer fusion for less computational cost in spatial&#8211;temporal traffic forecasting</article-title><source>Inf. Fusion</source><year>2025</year><volume>117</volume><fpage>102872</fpage><pub-id pub-id-type=\"doi\">10.1016/j.inffus.2024.102872</pub-id></element-citation></ref><ref id=\"B16-sensors-25-06865\"><label>16.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zunair</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Ben Hamza</surname><given-names>A.</given-names></name></person-group><article-title>Masked Supervised Learning for Semantic Segmentation</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2210.00923</pub-id><pub-id pub-id-type=\"arxiv\">2210.00923</pub-id></element-citation></ref><ref id=\"B17-sensors-25-06865\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.09417</pub-id><pub-id pub-id-type=\"arxiv\">2401.09417</pub-id></element-citation></ref><ref id=\"B18-sensors-25-06865\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name></person-group><article-title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2312.00752</pub-id></element-citation></ref><ref id=\"B19-sensors-25-06865\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Jiao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>VMamba: Visual State Space Model</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2401.10166</pub-id></element-citation></ref><ref id=\"B20-sensors-25-06865\"><label>20.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Espinosa</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Ericsson</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Crowley</surname><given-names>E.J.</given-names></name></person-group><article-title>PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2403.17695</pub-id></element-citation></ref><ref id=\"B21-sensors-25-06865\"><label>21.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ouyang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>S.T.</given-names></name></person-group><article-title>MambaIR: A Simple Baseline for Image Restoration with State-Space Model</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><fpage>222</fpage><lpage>241</lpage><pub-id pub-id-type=\"doi\">10.1007/978-3-031-72649-1_13</pub-id></element-citation></ref><ref id=\"B22-sensors-25-06865\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Dan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>CM-UNet: Hybrid CNN-Mamba UNet for Remote Sensing Image Semantic Segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2405.10530</pub-id></element-citation></ref><ref id=\"B23-sensors-25-06865\"><label>23.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Luan</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name></person-group><article-title>FMambaIR: A Hybrid State-Space Model and Frequency Domain for Image Restoration</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2025</year><volume>63</volume><fpage>4201614</fpage><pub-id pub-id-type=\"doi\">10.1109/TGRS.2025.3526927</pub-id></element-citation></ref><ref id=\"B24-sensors-25-06865\"><label>24.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Gou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>X.</given-names></name></person-group><article-title>MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>10&#8211;17 June 2025</conf-date><fpage>7491</fpage><lpage>7501</lpage><pub-id pub-id-type=\"doi\">10.1109/CVPR52734.2025.00702</pub-id></element-citation></ref><ref id=\"B25-sensors-25-06865\"><label>25.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>S.</given-names></name></person-group><article-title>A Novel Mamba Architecture with a Semantic Transformer for Efficient Real-Time Remote Sensing Semantic Segmentation</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>2620</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs16142620</pub-id></element-citation></ref><ref id=\"B26-sensors-25-06865\"><label>26.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zuo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Sheng</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shan</surname><given-names>Y.</given-names></name></person-group><article-title>Topology-aware Mamba for Crack Segmentation in Structures</article-title><source>Autom. Constr.</source><year>2024</year><volume>168</volume><fpage>105845</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2024.105845</pub-id></element-citation></ref><ref id=\"B27-sensors-25-06865\"><label>27.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y.</given-names></name></person-group><article-title>Enhancing Pixel-Level Crack Segmentation with Visual Mamba and Convolutional Networks</article-title><source>Autom. Constr.</source><year>2024</year><volume>168</volume><fpage>105770</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2024.105770</pub-id></element-citation></ref><ref id=\"B28-sensors-25-06865\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>ECSNet: An Accelerated Real-Time Image Segmentation CNN Architecture for Pavement Crack Detection</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>15105</fpage><lpage>15112</lpage><pub-id pub-id-type=\"doi\">10.1109/TITS.2023.3300312</pub-id></element-citation></ref><ref id=\"B29-sensors-25-06865\"><label>29.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>C.</given-names></name></person-group><article-title>Automatic concrete crack segmentation model based on transformer</article-title><source>Autom. Constr.</source><year>2022</year><volume>139</volume><fpage>104275</fpage><pub-id pub-id-type=\"doi\">10.1016/j.autcon.2022.104275</pub-id></element-citation></ref><ref id=\"B30-sensors-25-06865\"><label>30.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fan</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Q.</given-names></name></person-group><article-title>CrackNet: A Hybrid Model for Crack Segmentation with Dynamic Loss Function</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7134</elocation-id><pub-id pub-id-type=\"doi\">10.3390/s24227134</pub-id><pub-id pub-id-type=\"pmid\">39598912</pub-id><pub-id pub-id-type=\"pmcid\">PMC11598754</pub-id></element-citation></ref><ref id=\"B31-sensors-25-06865\"><label>31.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yin</surname><given-names>Y.</given-names></name></person-group><article-title>A Lightweight Selective Feature Fusion and Irregular-Aware Network for Crack Detection Based on Federated Learning</article-title><source>Proceedings of the International Conference on High Performance Big Data and Intelligent Systems (HDIS)</source><conf-loc>Tianjin, China</conf-loc><conf-date>10&#8211;11 December 2022</conf-date><fpage>294</fpage><lpage>298</lpage><pub-id pub-id-type=\"doi\">10.1109/HDIS56859.2022.9991509</pub-id></element-citation></ref><ref id=\"B32-sensors-25-06865\"><label>32.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name></person-group><article-title>A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness</article-title><source>Proceedings of the IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Kuala Lumpur, Malaysia</conf-loc><conf-date>8&#8211;11 October 2023</conf-date><fpage>86</fpage><lpage>90</lpage><pub-id pub-id-type=\"doi\">10.1109/ICIP49359.2023.10222276</pub-id></element-citation></ref><ref id=\"B33-sensors-25-06865\"><label>33.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rahman</surname><given-names>M.M.</given-names></name><name name-style=\"western\"><surname>Tutul</surname><given-names>A.A.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Laishram</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Jung</surname><given-names>S.K.</given-names></name><name name-style=\"western\"><surname>Hammond</surname><given-names>T.</given-names></name></person-group><article-title>Mamba in Vision: A Comprehensive Survey of Techniques and Applications</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2410.03105</pub-id><pub-id pub-id-type=\"arxiv\">2410.03105</pub-id></element-citation></ref><ref id=\"B34-sensors-25-06865\"><label>34.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>10&#8211;17 June 2025</conf-date><fpage>29406</fpage><lpage>29416</lpage><pub-id pub-id-type=\"doi\">10.1109/CVPR52734.2025.02738</pub-id></element-citation></ref><ref id=\"B35-sensors-25-06865\"><label>35.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>You</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Bi</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Bi</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Duan</surname><given-names>Y.</given-names></name></person-group><article-title>A Blueberry Maturity Detection Method Integrating Attention-Driven Multi-Scale Feature Interaction and Dynamic Upsampling</article-title><source>Horticulturae</source><year>2025</year><volume>11</volume><elocation-id>600</elocation-id><pub-id pub-id-type=\"doi\">10.3390/horticulturae11060600</pub-id></element-citation></ref><ref id=\"B36-sensors-25-06865\"><label>36.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Cai</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>L.</given-names></name></person-group><article-title>Rethinking Scanning Strategies With Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2024</year><volume>17</volume><fpage>18223</fpage><lpage>18234</lpage><pub-id pub-id-type=\"doi\">10.1109/JSTARS.2024.3472296</pub-id></element-citation></ref><ref id=\"B37-sensors-25-06865\"><label>37.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Ning</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>An</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Derr</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Q.</given-names></name></person-group><article-title>A Survey of Mamba</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2408.01129</pub-id><pub-id pub-id-type=\"arxiv\">2408.01129</pub-id></element-citation></ref><ref id=\"B38-sensors-25-06865\"><label>38.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Bai</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Ouyang</surname><given-names>W.</given-names></name></person-group><article-title>RS-Mamba for Large Remote Sensing Image Dense Prediction</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2404.02668</pub-id><pub-id pub-id-type=\"doi\">10.1109/TGRS.2024.3425540</pub-id></element-citation></ref><ref id=\"B39-sensors-25-06865\"><label>39.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>X.</given-names></name></person-group><article-title>Remote Sensing Scene Classification by Gated Bidirectional Network</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2020</year><volume>58</volume><fpage>82</fpage><lpage>96</lpage><pub-id pub-id-type=\"doi\">10.1109/TGRS.2019.2931801</pub-id></element-citation></ref><ref id=\"B40-sensors-25-06865\"><label>40.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nirthika</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Manivannan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Ramanan</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>R.</given-names></name></person-group><article-title>Pooling in convolutional neural networks for medical image analysis: A survey and an empirical study</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>5321</fpage><lpage>5347</lpage><pub-id pub-id-type=\"doi\">10.1007/s00521-022-06953-8</pub-id><pub-id pub-id-type=\"pmid\">35125669</pub-id><pub-id pub-id-type=\"pmcid\">PMC8804673</pub-id></element-citation></ref><ref id=\"B41-sensors-25-06865\"><label>41.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xiao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>Z.</given-names></name></person-group><article-title>Multi-Scale Object Detection with the Pixel Attention Mechanism in a Complex Background</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>3969</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs14163969</pub-id></element-citation></ref><ref id=\"B42-sensors-25-06865\"><label>42.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>S.</given-names></name></person-group><article-title>CrackSCF: Lightweight Cascaded Fusion Network for Robust and Efficient Structural Crack Segmentation</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type=\"arxiv\">2408.12815</pub-id></element-citation></ref><ref id=\"B43-sensors-25-06865\"><label>43.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Prokhorov</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Mei</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ling</surname><given-names>H.</given-names></name></person-group><article-title>Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2020</year><volume>21</volume><fpage>1525</fpage><lpage>1535</lpage><pub-id pub-id-type=\"doi\">10.1109/TITS.2019.2910595</pub-id></element-citation></ref><ref id=\"B44-sensors-25-06865\"><label>44.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>A.A.</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhan</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>K.C.P.</given-names></name></person-group><article-title>Robust Semantic Segmentation for Automatic Crack Detection Within Pavement Images Using Multi-Mixing of Global Context and Local Image Features</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2024</year><volume>25</volume><fpage>11282</fpage><lpage>11303</lpage><pub-id pub-id-type=\"doi\">10.1109/TITS.2024.3360263</pub-id></element-citation></ref><ref id=\"B45-sensors-25-06865\"><label>45.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J.</given-names></name></person-group><article-title>Selective Kernel Networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>510</fpage><lpage>519</lpage><pub-id pub-id-type=\"doi\">10.1109/CVPR.2019.00060</pub-id></element-citation></ref><ref id=\"B46-sensors-25-06865\"><label>46.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Woo</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style=\"western\"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>CBAM: Convolutional Block Attention Module</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>3</fpage><lpage>19</lpage><pub-id pub-id-type=\"doi\">10.1007/978-3-030-01234-2_1</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-06865-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Overall framework of CONTI-CrackNet: (<bold>a</bold>) CONTI-CrackNet, (<bold>b</bold>) SSVSS block.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Four common single-strategy scans: (<bold>a</bold>) parallel scans, (<bold>b</bold>) diagonal scans, (<bold>c</bold>) Z-order scans, (<bold>d</bold>) Hilbert scans.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Architecture of MD3S: (<bold>a</bold>) MD3S, (<bold>b</bold>) BiGF.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Structures of DBPGL and PAP: (<bold>a</bold>) DBPGL; (<bold>b</bold>) PAP.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Sample images and ground-truth masks from the datasets used in this study: (<bold>a</bold>) TUT, (<bold>b</bold>) CRACK500.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Typical visual comparison on the TUT dataset using four methods. Red boxes highlight key details; green boxes mark wrongly identified regions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Typical visual comparison on the CRACK500 dataset using four methods. The red boxes highlight key details; the green boxes mark wrongly identified regions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06865-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Visualization of CONTI-CrackNet failure cases. Yellow boxes indicate missed detections.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06865-g008.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-06865-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t001_Table 1</object-id><label>Table 1</label><caption><p>Description of the experimental datasets.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Resolution</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Images</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Training</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Validation</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Test</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">TUT&#160;[<xref rid=\"B42-sensors-25-06865\" ref-type=\"bibr\">42</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"mm62\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:mrow></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1408</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">986</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">141</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">281</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CRACK500&#160;[<xref rid=\"B43-sensors-25-06865\" ref-type=\"bibr\">43</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"mm63\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>640</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>360</mml:mn></mml:mrow></mml:mrow></mml:math>\n</inline-formula>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">3368</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">2358</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">337</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">673</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t002_Table 2</object-id><label>Table 2</label><caption><p>Sensitivity analysis of <inline-formula><mml:math id=\"mm64\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"sans-serif\">&#945;</mml:mi></mml:mrow></mml:math></inline-formula> on the TUT dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-formula>\n<mml:math id=\"mm65\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-sans-serif\">&#945;</mml:mi></mml:mstyle></mml:mrow></mml:math>\n</inline-formula>\n</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8021</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8059</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8279</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8176</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8384</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8368</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8049</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8116</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8292</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8185</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8402</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8371</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8133</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8165</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8332</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8220</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8447</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8436</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8110</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8123</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8313</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8196</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8434</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8420</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.5</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8002</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8017</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8283</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8201</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8366</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8359</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of segmentation results of different models on the TUT dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SFIAN (2022)&#160;[<xref rid=\"B31-sensors-25-06865\" ref-type=\"bibr\">31</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7290</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7513</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7473</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7715</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7247</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7756</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegNeXt (2022)&#160;[<xref rid=\"B11-sensors-25-06865\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7312</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7435</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7517</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7812</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7245</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7785</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Crackmer (2024)&#160;[<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7429</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7501</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7578</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7501</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7656</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7966</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegFormer (2021)&#160;[<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7532</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7612</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7670</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7654</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7688</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8078</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CT-crackseg (2023)&#160;[<xref rid=\"B32-sensors-25-06865\" ref-type=\"bibr\">32</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7940</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7996</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8199</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8202</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8195</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8301</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CSMamba (2024)&#160;[<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7879</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7946</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8146</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7947</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8353</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8263</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PlainMamba (2024)&#160;[<xref rid=\"B20-sensors-25-06865\" ref-type=\"bibr\">20</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7889</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7954</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8154</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7955</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8365</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8316</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8133</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8165</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8332</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8220</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8447</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8436</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of segmentation results of different models on the CRACK500 dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SFIAN (2022)&#160;[<xref rid=\"B31-sensors-25-06865\" ref-type=\"bibr\">31</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6473</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6941</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7204</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6983</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7441</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7315</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegNeXt (2022)&#160;[<xref rid=\"B11-sensors-25-06865\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6488</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6762</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7334</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7134</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7546</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7345</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Crackmer (2024)&#160;[<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6933</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7097</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7267</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6985</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7572</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7421</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegFormer (2021)&#160;[<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6998</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7134</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7245</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7067</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7434</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7456</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CT-crackseg (2023)&#160;[<xref rid=\"B32-sensors-25-06865\" ref-type=\"bibr\">32</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6941</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7059</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7322</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6940</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7748</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7591</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CSMamba (2024)&#160;[<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6931</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7162</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7315</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6858</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7823</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7592</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PlainMamba (2024)&#160;[<xref rid=\"B20-sensors-25-06865\" ref-type=\"bibr\">20</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6574</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.6870</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7422</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7318</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7530</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7579</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7104</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7301</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7587</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7333</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7860</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.7760</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation validating the effectiveness of the proposed modules.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">MD3S</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ElemAdd</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SKNet</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DBPGL</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PAP</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7845</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7921</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8049</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7945</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8156</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8078</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8008</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8074</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8268</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8161</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8377</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8350</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7944</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8064</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8200</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8123</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8279</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8268</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8094</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8168</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8297</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8142</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8459</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8406</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10007;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8133</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8165</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8332</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8220</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8447</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8436</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t006\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t006_Table 6</object-id><label>Table 6</label><caption><p>Ablation validating the effectiveness of the proposed MD3S.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ParaSna</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7880</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7926</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8063</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7898</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8236</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8041</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DiagSna</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7897</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7979</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8077</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7891</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8273</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8059</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ParaSna + DiagSna</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7970</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8071</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8127</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7909</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8359</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8260</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">MD3S</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8008</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8074</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8268</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8161</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8377</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8350</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t007\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t007_Table 7</object-id><label>Table 7</label><caption><p>Ablation validating the effectiveness of the PAM.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">ODS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">OIS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">P</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">R</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CBAM</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8016</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8023</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8250</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8245</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8256</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8277</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">PAM</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8133</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8165</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8332</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8220</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8447</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.8436</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06865-t008\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06865-t008_Table 8</object-id><label>Table 8</label><caption><p>Comparison of CONTI-CrackNet and other methods in Params, GFLOPs, and FPS.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">GFLOPs</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Params</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FPS</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SFIAN (2022)&#160;[<xref rid=\"B31-sensors-25-06865\" ref-type=\"bibr\">31</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.57 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">13.63 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">32</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegNeXt (2022)&#160;[<xref rid=\"B11-sensors-25-06865\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">31.80 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">27.52 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">22</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Crackmer (2024)&#160;[<xref rid=\"B14-sensors-25-06865\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">14.94 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.90 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegFormer (2021)&#160;[<xref rid=\"B13-sensors-25-06865\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">30.80 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">28.20 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">21</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CT-crackseg (2023)&#160;[<xref rid=\"B32-sensors-25-06865\" ref-type=\"bibr\">32</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">39.47 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">22.88 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">28</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CSMamba (2024)&#160;[<xref rid=\"B22-sensors-25-06865\" ref-type=\"bibr\">22</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">145.84 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">35.95 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">19</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PlainMamba (2024)&#160;[<xref rid=\"B20-sensors-25-06865\" ref-type=\"bibr\">20</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.36 G</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">16.72 M</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">24.22 G</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">6.01 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">42</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656390 PMC12656390.1 12656390 12656390 41305072 10.3390/s25226865 sensors-25-06865 1 Article CONTI-CrackNet: A Continuity-Aware State-Space Network for Crack Segmentation Song Wenjie Conceptualization Methodology Software Validation Formal analysis Investigation Resources Data curation Visualization 1 Zhao Min Conceptualization Validation Writing &#8211; review &amp; editing Supervision Project administration Funding acquisition 2 * Xu Xunqian Validation Writing &#8211; review &amp; editing 3 1 School of Information Science and Technology, Nantong University, Nantong 226019, China; wenjiesong@stmail.ntu.edu.cn 2 School of Artificial Intelligence and Computer Science, Nantong University, Nantong 226019, China 3 School of Transportation and Civil Engineering, Nantong University, Nantong 226019, China; xunqian_xu@ntu.edu.cn * Correspondence: zhao.m@ntu.edu.cn 10 11 2025 11 2025 25 22 501335 6865 03 10 2025 07 11 2025 08 11 2025 10 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Crack segmentation in cluttered scenes with slender and irregular patterns remains difficult, and practical systems must balance accuracy and efficiency. We present CONTI-CrackNet, which is a lightweight visual state-space network that integrates a Multi-Directional Selective Scanning Strategy (MD3S). MD3S performs bidirectional scanning along the horizontal, vertical, and diagonal directions, and it fuses the complementary paths with a Bidirectional Gated Fusion (BiGF) module to strengthen global continuity. To preserve fine details while completing global texture, we propose a Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module that incorporates a Pixel-Adaptive Pooling (PAP) mechanism to dynamically weight max-pooled responses and average-pooled responses. Evaluated on two public benchmarks, the proposed method achieves an F1 score (F1) of 0.8332 and a mean Intersection over Union (mIoU) of 0.8436 on the TUT dataset, and it achieves an mIoU of 0.7760 on the CRACK500 dataset, surpassing competitive Convolutional Neural Network (CNN), Transformer, and Mamba baselines. With 512 &#215; 512 input, the model requires 24.22 G floating point operations (GFLOPs), 6.01 M parameters (Params), and operates at 42 frames per second (FPS) on an RTX 3090 GPU, delivering a favorable accuracy&#8211;efficiency balance. These results show that CONTI-CrackNet improves continuity and edge recovery for thin cracks while keeping computational cost low, and it is lightweight in terms of parameter count and computational cost. crack segmentation segmentation Mamba deep learning lightweight networks feature extraction Nantong Natural Science Foundation of China JC2023073 This research was funded by the Nantong Natural Science Foundation of China (JC2023073). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Cracks are among the most common and harmful defects [ 1 ]. They weaken the load-bearing capacity and durability of structures, increase maintenance costs, and pose public-safety risks. Cracks manifest in diverse materials and structures, such as pavements, bricks, and tiles [ 2 ]. Owing to irregular shapes, large-scale variations, and complex textures, accurate crack recognition remains challenging. Traditional crack detection still relies on manual inspection [ 3 ]. This process is slow, costly, and subjective, which often leads to missed cases and inconsistent results and cannot meet large-scale application needs. Early segmentation methods mainly used digital image processing, such as edge detection, clustering, thresholding, and morphological operations [ 4 , 5 , 6 , 7 ], but their performance is limited in the presence of noise. To overcome these limitations, researchers introduced Convolutional Neural Networks (CNNs) for crack segmentation [ 8 ]. Although prior studies expanded the effective receptive field through pyramid pooling [ 9 ], employed deformable convolutions to adaptively focus on relevant regions [ 10 ], and incorporated lightweight attention in SegNeXt [ 11 ] to highlight local features while reducing computational overhead, CNNs remain constrained by local convolutions [ 12 ], which limits the modeling of long-range dependencies and complex semantic relations. In view of these shortcomings, Transformer-based methods have been applied to segmentation tasks, including hierarchical encoders coupled with Multi-layer Perceptron (MLP) decoders for cross-level feature aggregation [ 13 ] and CNN&#8211;Transformer dual-path architectures that exploit complementary strengths [ 14 ]. However, Transformers typically involve large parameter scales and substantial computational cost [ 15 ], which restricts deployment on resource-constrained devices. Moreover, MaskSup [ 16 ] performs context modeling for image segmentation via random masking, thereby improving segmentation performance without incurring any additional inference overhead. To retain both global and local modeling while controlling computational cost, researchers have begun exploring state-space models for vision tasks [ 17 ]. Mamba is an efficient model in this class, employing selective state spaces to model long sequences in linear time [ 18 ] and thereby delivering strong global-context capture with high computational efficiency. Building on this idea, many studies have extended Mamba to computer vision. For example, VMamba [ 19 ] divides an image into a sequence of patches and employs Visual State-Space (VSS) blocks for multi-directional scanning to better capture complex inter-patch dependencies. PlainMamba [ 20 ] removes the hierarchical structure and adopts continuous 2D scanning, simplifying the network and improving inference efficiency. MambaIR [ 21 ] incorporates local enhancement and channel attention and demonstrates improvements in image restoration. CSMamba [ 22 ] combines a CNN encoder with a Mamba decoder for remote-sensing image segmentation. Overall, these studies indicate that Mamba offers clear advantages for global-information modeling. However, despite progress in image restoration and remote sensing [ 23 , 24 , 25 ], in-depth investigations of crack segmentation remain limited. A few studies have introduced Mamba into crack segmentation. For example, CrackMamba [ 26 ] adopts a Mamba-based encoder and a Snake-scan mechanism to strengthen global modeling; MambaCrackNet [ 27 ] incorporates novel visual Mamba blocks, effectively capturing global semantic relations while reducing computational cost. Cracks exhibit a slender, irregular morphology with fuzzy boundaries, which requires models to preserve spatial continuity while balancing pixel-level local details and global texture. At present, crack segmentation primarily relies on architectures based on CNNs or Transformers [ 28 , 29 , 30 ]. The former (e.g., SFIAN [ 31 ]) excels at detailed feature extraction, whereas the latter (e.g., CT-crackseg [ 32 ]) achieves gains in boundary modeling. Yet CNNs are limited by the locality of convolutions, whereas Transformers incur high computational cost due to attention [ 33 ]. Consequently, achieving a favorable balance between segmentation quality and computational efficiency remains an open problem. To address the above challenges, we propose CONTI-CrackNet. Our main contributions are outlined below: CONTI-CrackNet architecture: a cascaded and lightweight crack segmentation network that effectively represents complex crack morphology while markedly reducing computation. Multi-Directional Selective Scanning Strategy (MD3S): efficient long-range modeling that characterizes crack continuity and shape from multiple directions combined with the Bidirectional Gated Fusion (BiGF) module to alleviate directional bias. Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module: a Pixel-Adaptive Pooling (PAP) mechanism that balances max-pooled and average-pooled features at each pixel, preserving edge fidelity while improving global connectivity. 2. Methods 2.1. Network Architecture This paper proposes a new crack-segmentation model, CONTI-CrackNet; its overall structure is shown in Figure 1 a. The model addresses multiple crack properties: it enhances shape modeling, captures global information about crack continuity, and preserves the local details of fine cracks. Building on this design, the model employs a Multi-Directional Selective Scanning Strategy (MD3S), a Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) module, and a Pixel-Adaptive Pooling (PAP) mechanism to achieve fine-grained segmentation in complex scenes. The input is an RGB image of shape ( 3 , &#160; H , &#160; W ) , where 3 denotes the channel count and H and W denote the image height and width, respectively. We first partition the image into N = H h &#183; W w patches, where h and w are the height and width of each patch, and add positional encodings to each patch to preserve spatial information. The patch sequence is then fed into an Structure-Strengthened Visual State-Space (SSVSS) block for feature modeling. As shown in Figure 1 b, the SSVSS block first applies a Gated Bottleneck Convolution (GBC) [ 34 ] module to extract detailed features and then splits the stream into two branches. The lower branch expands the channels via a linear layer and applies SiLU. The upper branch applies a linear transform and activation, and then it introduces an MD3S to better capture the crack shape and structure. The outputs of the two branches are fused by element-wise multiplication and projected back to the original channel size via a linear layer. To improve both local detail and global semantics, we further design DBPGL. The MD3S-processed features and the corresponding original features are fed into this module. This module employs PAP to fuse max-pooling and average-pooling features at the pixel level, thereby enhancing both global and local representations. Residual connections are added in the SSVSS block to preserve original details during fusion and mitigate gradient vanishing. During feature extraction, the network produces four feature maps at different scales: low-level maps primarily encode spatial details, whereas high-level maps carry richer semantic information. These features gradually integrate spatial structure and contextual information. Afterward, an MLP aligns the four scales to a common channel width, and dynamic upsampling [ 35 ] restores them to the original image resolution. Finally, all feature maps are concatenated and further processed by a convolutional layer and an MLP to yield the final crack-segmentation map. 2.2. Multi-Directional Selective Scanning Strategy (MD3S) In recent years, two-dimensional scanning strategies have demonstrated strong feature-modeling capability in image analysis tasks. However, common scanning modes in visual Mamba models include parallel, diagonal, Z-order, and Hilbert scans [ 36 , 37 ]. Each mode follows a single strategy ( Figure 2 ). Parallel scans better capture long-range dependencies along the horizontal and vertical directions; diagonal and Z-order scans are more suitable for modeling global information along diagonal paths; and Hilbert scans preserve spatial continuity to some extent but are weaker in capturing global information. Thus, single-strategy modes are limited when handling complex structures and cannot fully represent slender, irregular cracks across multiple directions [ 38 ]. To address this limitation, we propose the Multi-Directional Selective Scanning Strategy (MD3S). MD3S scans along four directions: horizontal, vertical, main-diagonal, and anti-diagonal. It performs forward and backward passes in each direction to obtain four bidirectional feature sequences ( Figure 3 a). To reduce redundancy and emphasize complementary information between the bidirectional sequences [ 39 ], we introduce a Bidirectional Gated Fusion (BiGF) module ( Figure 3 b), which adaptively assigns weights to the forward and backward paths and fuses them: (1) &#963; = SiLU Linear Norm T t &#8722; 1 (2) T t = Linear &#963; &#183; Y forward + 1 &#8722; &#963; &#183; Y backward + T t &#8722; 1 where T t &#8722; 1 is the original feature and T t is the output feature; Norm denotes normalization, Linear denotes a linear projection, and SiLU is the activation function; Y forward and Y backward denote the features from the forward and backward paths, respectively. After the gated fusion, the four fused sequences are passed to the S6 block [ 18 ]. A subsequent scan&#8211;merge operation concatenates the sequences from different directions and maps them back to the original spatial resolution, yielding a representation that preserves multi-directional structural awareness and global-context modeling. 2.3. Dual-Branch Pixel-Level Global&#8211;Local Fusion (DBPGL) Module To preserve fine-grained details from the original sequence while fusing the crack texture patterns produced by MD3S, we propose the DBPGL module. DBPGL incorporates a Pixel-Adaptive Pooling (PAP) mechanism within a dual-branch design. It performs dynamic weighting at the pixel level, thereby strengthening the SSVSS module&#8217;s capacity to model and learn crack features. As shown in Figure 4 a, DBPGL fuses two inputs using a dual-branch structure, focusing on local detail preservation and global texture completion. In the left branch, the input feature passes through a 1 &#215; 1 convolution for channel reduction, which is followed by ReLU to introduce nonlinearity and enhance representation. A 1 &#215; 1 convolution then restores the original channel number. Batch normalization is applied to improve training stability and generalization. The right branch employs PAP to complement global structural information related to crack continuity. Finally, the outputs of the two branches are fused. Element-wise operations compute per channel attention weights via a sigmoid function, which enables adaptive importance assignment to the two branches. The fusion can be written as the equations below. (3) X l = BN Conv ReLU Conv X (4) Y g = BN Conv ReLU Conv Y (5) Z 1 = Sigmoid X l + Y g &#183; X + 1 &#8722; Sigmoid X l + Y g &#183; Y where X and Y denote input a and input b; Conv denotes the 1 &#215; 1 convolution; ReLU denotes the activation function; BN denotes batch normalization; Sigmoid denotes the activation function. X l denotes the output of the left branch of the module, Y g represents the output of the right branch, and Z 1 is the fused output obtained by weighting and combining the features from both branches. 2.4. Pixel-Adaptive Pooling (PAP) Mechanism In feature extraction, max pooling and average pooling are often used together because they are complementary. Max pooling focuses on high-response regions, whereas average pooling integrates global background information and yields a balanced representation [ 40 ]. To leverage both, we design a pixel-level adaptive fusion strategy within PAP, as illustrated in Figure 4 b. Specifically, the input features X and Y are split into two paths and processed by max pooling and average pooling to obtain complementary spatial features. During two-branch feature fusion, a fixed rule cannot adapt to crack structures at the pixel scale. We introduce the Pixel Attention Mechanism (PAM) [ 41 ] to enable adaptive, pixel-level fusion. It assigns weights to the two complementary branches at each spatial location, preserving fine details and maintaining global connectivity. This mechanism applies a 1 &#215; 1 convolution followed by batch normalization to each path, performs element-wise multiplication for interaction, and then applies another 1 &#215; 1 convolution and batch normalization. The output is passed through a sigmoid to produce a gating coefficient &#963; , which encodes the dynamic preference between the two branches. Finally, the pixel attention weights are applied to the two features. Fusion is completed by weighted multiplication and element-wise addition, which is formulated as shown below: (6) &#963; = Sigmoid BNConv BNConv V a &#183; BNConv V b (7) Z 2 = &#963; &#183; X max + 1 &#8722; &#963; &#183; Y avg where V a and V b are pixels from the two-branch feature maps; BNConv denotes a 1 &#215; 1 convolution followed by batch normalization, i.e., BNConv ( ) = BN ( Conv ( ) ) ; Sigmoid is the activation function; X max and Y avg are the feature maps produced by max pooling and average pooling, respectively; and Z 2 is the fused output obtained by weighting and combining the features from both branches. With this design, the network fuses important local features with global context and can adapt, at the pixel level, the contribution of different spatial locations, which improves the modeling of complex crack structures. 3. Results 3.1. Datasets We validate our approach on two public benchmark datasets for crack segmentation: TUT [ 42 ] and CRACK500 [ 43 ]. TUT covers diverse scenes with complex backgrounds, which helps assess the model&#8217;s robustness and generalization across scenes and under strong noise. CRACK500 is a standard pavement crack benchmark that enables fair comparison with prior methods. Table 1 reports the main characteristics and split schemes of the datasets, while Figure 5 presents representative images and the corresponding ground truth masks. TUT [ 42 ]: Unlike datasets with simple backgrounds, TUT contains dense and cluttered scenes with diverse crack shapes. It includes 1408 RGB images from eight representative scenes: bitumen, cement, bricks, runways, tiles, metal, generator blades, and underground pipelines. Of these, 1270 images were collected in-house using mobile phones, and 138 images were sourced from the internet. CRACK500 [ 43 ]: Proposed by Yang et al., the original dataset contains 500 bitumen crack images at 2000 &#215; 1500 resolution, which were all captured by mobile phones. Because the dataset is small, images are cropped into 16 nonoverlapping patches, and only samples with more than 1000 crack pixels are retained. After this processing, each patch has a resolution of 640 &#215; 320. Data augmentation increases the total to 3368 images, and each sample is paired with a per-pixel binary mask. 3.2. Evaluation Metrics To comprehensively evaluate the proposed segmentation model, we use several common metrics: Optimal Dataset Scale (ODS), Optimal Image Scale (OIS), Precision (P), Recall (R), F1 score (F1), and mean Intersection over Union (mIoU). The ODS evaluates the model on the whole dataset under a single fixed threshold m. The OIS evaluates performance when an optimal threshold n is chosen for each image. The F1, as the harmonic mean of P and R, balances both and is widely used to assess overall robustness and reliability in crack segmentation. In addition, mIoU quantifies spatial accuracy by the overlap between the predicted mask and the ground-truth annotation. They are defined as shown below: (8) ODS = max m 2 &#183; P m &#183; R m P m + R m (9) OIS = 1 N &#8721; i = 1 N max n 2 &#183; P n , i &#183; R n , i P n , i + R n , i (10) F 1 = 2 &#183; P &#183; R P + R (11) mIoU = 1 N + 1 &#8721; i = 0 N p i i &#8721; j = 0 N p i j + &#8721; j = 0 N p j i &#8722; p i i where N is the number of classes; i indexes the ground-truth class and j denotes the predicted class. Let p i j denote the number of pixels of ground-truth class i predicted as class j (thus, p i i are the true positives for class i ). In this case, N = 1 . 3.3. Implementation Details and Training The proposed network is implemented in PyTorch v1.13.1 and trained on an Intel Xeon Platinum 8336C CPU. AdamW is used with an initial learning rate of 5 &#215; 10 &#8722;4 and the PolyLR scheduler for dynamic adjustment. The weight decay is 0.01, and the random seed is 42. Training runs for 50 epochs. All input images are resized to 512 &#215; 512 pixels, and the batch size is 8. After each epoch, performance on the validation set is measured, and the checkpoint with the best validation score is kept for later testing. A joint loss that combines Binary Cross-Entropy (BCE) and Dice loss [ 44 ] is adopted. BCE measures per-pixel classification accuracy, while Dice focuses on the overlap between the predicted mask and the ground truth, improving coherence and completeness. The combined loss is shown below: (12) L = &#945; &#183; L BCE + 1 &#8722; &#945; &#183; L Dice where L BCE is the BEC, L Dice is the Dice loss, and &#945; controls the weights of the two terms. As shown in Table 2 , we conduct a sensitivity study of the loss-weight hyperparameter &#945; on the TUT dataset; the results indicate that &#945; = 0.2 achieves the best performance. 3.4. Experimental Results We evaluate the proposed model against seven representative baseline networks: SFIAN [ 31 ], SegNeXt [ 11 ], Crackmer [ 14 ], SegFormer [ 13 ], CT-crackseg [ 32 ], CSMamba [ 22 ], and PlainMamba [ 20 ]. On two public datasets, we make a systematic comparison between these methods and our model. Specifically, SFIAN and SegNeXt are built on CNNs; Crackmer, SegFormer, and CT-crackseg adopt the Transformer architecture; CSMamba and PlainMamba follow the Mamba framework. 3.4.1. Experimental Results on Dataset TUT On the TUT dataset, CONTI-CrackNet achieves the best results across all metrics. As shown in Table 3 , our model reaches an F1 of 0.8332 and an mIoU of 0.8436, outperforming architectures based on CNNs, Transformers, and Mamba. Compared with SegNeXt, F1 improves by 0.0815, indicating a stronger modeling of thin and low-contrast cracks, with fewer breaks and missed segments along long, slender structures. Compared with the best Transformer baseline, CT-crackseg, R increases by 0.0248, confirming advantages in accuracy. Compared with strong Mamba-based baselines, the proposed method attains higher accuracy; relative to PlainMamba, mIoU increases by 0.0120, demonstrating clear competitiveness within the same architectural class. In addition, a standard deviation analysis was conducted on the TUT dataset. The standard deviations of ODS, OIS, F1, P, R, and mIoU are 0.0114, 0.0106, 0.0065, 0.0043, 0.0102, and 0.0071, respectively. These results demonstrate that the proposed model performs well and exhibits strong engineering reliability. Beyond the numbers, our visual results also show clear gains. As shown in Figure 6 , in scenes with complex backgrounds and uneven lighting, SegNeXt and SegFormer tend to produce false cracks and false positives, while CONTI-CrackNet suppresses background noise. For very thin cracks or low contrast, CNN and Transformer methods often show broken or missed segments, but our method restores complete crack shapes. For long and curved cracks, PlainMamba still shows blurred edges and local breaks, while CONTI-CrackNet keeps global continuity and sharp boundaries. These improvements come from the MD3S, which models crack continuity, and the DBPGL module, which fuses global and local features. Together, they enable robust segmentation in complex scenes and demonstrate the effectiveness and practical value of our method. 3.4.2. Experimental Results on Dataset CRACK500 On the CRACK500 dataset, the results further verify the effectiveness of our method. As shown in Table 4 , CONTI-CrackNet attains an mIoU of 0.7760, outperforming all compared methods on every metric. Compared with the second-best PlainMamba, our model improves by 0.0181 on mIoU, respectively. Against the CNN and Transformer baselines, the gains are larger, especially on F1 and mIoU, which shows stronger robustness in preserving overall crack structure and regional consistency. On CRACK500, the standard deviations for ODS, OIS, F1, P, R, and mIoU are 0.0057, 0.0102, 0.0091, 0.0146, 0.0052, and 0.0119, respectively. This finding demonstrates the consistency and reliability of the proposed model across diverse evaluation conditions. The visual results in Figure 7 also show clear advantages. In scenes with complex backgrounds and strong texture noise, CNN and Transformer models such as SegNeXt and SegFormer often produce false positives or fake cracks, as seen in the first and fifth rows of Figure 7 . When crack shapes are complex, Mamba methods can still show over segmentation or blurry boundaries, as illustrated in the second row. In contrast, CONTI-CrackNet accurately extracts the main crack skeleton in most cases and maintains sharp edges and global continuity; even under high noise or low contrast, it avoids several false segmentations. Although CONTI-CrackNet does not perfectly recover every tiny crack, its results are closer to the ground truth and contain much less noise overall. 3.5. Ablation Study To verify the effectiveness of the proposed modules, we conduct a systematic ablation study on the TUT dataset. Specifically, we replace, enable, or disable the proposed modules to systematically assess each module&#8217;s impact on segmentation performance. This design quantifies the contributions of each component and the proposed scanning strategy and further confirms the soundness and effectiveness of the approach. 3.5.1. Ablation Study of Components We further conduct ablation studies on the modules introduced in SSVSS. As summarized in Table 5 , the full configuration integrating MD3S, DBPGL, and PAP attains the best overall metrics, validating its effectiveness for crack segmentation. The baseline excludes all proposed modules and only employs the standard SS2D scanning [ 17 ] with element-wise addition (ElemAdd) for fusion. On this basis, introducing MD3S increases the mIoU to 0.8350, indicating that MD3S effectively captures directional continuity and multi-orientation morphology. In the comparison of fusion strategies, both ElemAdd and SKNet [ 45 ] underperform DBPGL. With the addition of PAP, the metrics further improve to 0.8436, suggesting that the combination of DBPGL and PAP enables fine-grained, pixel-adaptive gating between the two branches. Moreover, the global&#8211;local dual-branch design better suits crack scenarios, where max-pooled and average-pooled features are complementarily fused at the pixel level, thereby preserving the connectivity of slender and discontinuous cracks while suppressing texture noise. In summary, MD3S, DBPGL, and PAP exhibit clear divisions of labor and synergy: MD3S provides direction-aware global modeling and continuity completion; DBPGL together with PAP delivers complementary global&#8211;local fusion with pixel-level adaptive weighting to refine fusion decisions. Their joint effect leads to substantial performance gains in crack segmentation. 3.5.2. Ablation on Scanning Strategies As shown in Table 6 , we compare four directional scanning strategies under the same settings. In this experiment, we disable both DBPGL and PAP, and we perform feature fusion using only ElemAdd. Using parallel serpentine scanning (ParaSpn), with forward and backward traversals along the horizontal and vertical directions, the model achieves F1 = 0.8063. Using diagonal serpentine scanning (DiagSpn) [ 37 ], with forward and backward traversals along the main and anti-diagonal directions, the model attains F1 = 0.8077, which remains relatively low. Combining the two (ParaSpn + DiagSpn), employing both parallel and diagonal serpentine scans, raises the F1 to 0.8127 and the mIoU to 0.8260, indicating that multi-directional cues benefit crack segmentation. Furthermore, after introducing the proposed MD3S, the model achieves new best results across all metrics. Compared with the combination of ParaSpn and DiagSpn, F1 and P improve by 0.0141 and 0.0252, respectively. These results indicate that the proposed strategy captures multi-direction features and strengthens the modeling of complex crack structures. Compared with a single scanning strategy, our method builds forward&#8211;backward (bidirectional) sequences in four directions: horizontal, vertical, main diagonal, and anti-diagonal. It then performs direction-wise fusion. This lets the network learn pixel-level adaptive preferences. Compared with simple multi-direction stacking, the per-direction bidirectional fusion better fits the modeling needs of multi-scale and multi-directional geometric features. It yields superior performance on crack. 3.5.3. Ablation Study of the Attention Mechanism in the PAP Module To substantiate the necessity of the proposed Pixel Attention Mechanism (PAM), we conducted a controlled comparison under identical experimental settings ( Table 7 ). In this study, MD3S and DBPGL were kept enabled, and only the weight-generation unit in the fusion stage was swapped between the Convolutional Block Attention Module (CBAM) [ 46 ] and PAM. The results show that the model with PAM delivers the best performance, indicating that pixel-level, adaptive gating weights better discriminate thin cracks from background textures than CBAM, effectively reducing false positives and breakages and achieving finer pixel-wise segmentation. 3.6. Complexity Analysis Table 8 reports the comparison under a fixed input size of 512 &#215; 512. Our method requires 24.22 G floating point operations (GFLOPs), has 6.01M parameters (Params), and runs at 42 frames per second (FPS). Compared with the lightweight Crackmer [ 14 ], our model is slightly heavier but delivers a clear accuracy gain, achieving a better accuracy&#8211;efficiency trade-off. At the same time, compared with larger and more complex networks such as CSMamba [ 22 ] and SegFormer [ 13 ], our method uses fewer GFLOPs and Params and offers much faster inference. In sum, CONTI-CrackNet provides high segmentation accuracy with low computational cost and fast runtime, making it suitable for research and practical deployment. 3.7. Analysis of Failure Cases and Limitations To objectively evaluate the performance of CONTI-CrackNet, we analyze the failure cases observed in our experiments. Figure 8 illustrates two representative scenarios: extreme noisy backgrounds and complex crack intersections. Experiments indicate that in complex scenarios with strong noise and frequent crack intersections, the continuity of fine cracks remains disturbed. Fine cracks may not be fully recovered because complex backgrounds cause confusion between crack pixels and background textures, and intricate crack geometries tend to induce discontinuities at intersections. Although CONTI-CrackNet may exhibit under-segmentation in a few extreme cases, the proposed architecture improves segmentation accuracy under challenging conditions. In future work, we will further enhance the model to increase noise robustness and multi-scale handling, thereby improving the overall performance. 4. Discussion and Conclusions We propose CONTI-CrackNet, which is a lightweight crack-segmentation network that improves pixel-level fine segmentation under low computational cost. Its performance stems from three aspects: (1) a cascaded lightweight backbone that captures crack morphology while reducing resource usage; (2) MD3S, which aggregates multi-directional information to model the global context of thin and irregular cracks; and (3) DBPGL with a PAP mechanism, which employs dual-branch pixel attention with max and average pooling to enhance local details and overall structural perception. On TUT and CRACK500, the model achieves superior or comparable accuracy with 24.22 G GFLOPs and 6.01 M parameters while maintaining high inference speed. Ablation studies show that MD3S strengthens continuity, and DBPGL with PAP improves segmentation by coupling global dependencies with detail enhancement. Limitations remain: validation is conducted on two public datasets and focuses on static images. Future work will expand data diversity, improve small-crack segmentation, investigate crack-depth quantification, and assess pathways toward efficient edge execution. Acknowledgments During the preparation of this manuscript, the authors used ChatGPT (OpenAI, GPT-5, https://www.openai.com/chatgpt , accessed on 22 September 2025) for language editing. The authors reviewed and edited the output and take full responsibility for the content of this publication. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, W.S. and M.Z.; methodology, W.S.; software, W.S.; validation, W.S., M.Z. and X.X.; formal analysis, W.S.; investigation, W.S.; resources, W.S.; data curation, W.S.; writing&#8212;original draft preparation, W.S.; writing&#8212;review and editing, M.Z. and X.X.; visualization, W.S.; supervision, M.Z.; project administration, M.Z.; funding acquisition, M.Z. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement Publicly available datasets were analyzed in this study. The TUT dataset can be found at https://github.com/Karl1109/CrackSCF , accessed on 2 July 2025, and the CRACK500 dataset is available at https://github.com/fyangneil/pavement-crack-detection , accessed on 2 July 2025. Conflicts of Interest The authors declare no conflicts of interest. References 1. Wu S. Withers P.J. Beretta S. Kang G. Editorial: Tomography traces the growing cracks and defects Eng. Fract. Mech. 2023 292 109628 10.1016/j.engfracmech.2023.109628 2. Matarneh S. Elghaish F. Edwards D.J. Rahimian F.P. Abdellatef E. Ejohwomu O. Automatic crack classification on asphalt pavement surfaces using convolutional neural networks and transfer learning J. Inf. Technol. Constr. 2024 29 1239 1256 10.36680/j.itcon.2024.055 3. Singh V. Baral A. Kumar R. Tummala S. Noori M. Yadav S.V. Kang S. Zhao W. A Hybrid Deep Learning Model for Enhanced Structural Damage Detection: Integrating ResNet50, GoogLeNet, and Attention Mechanisms Sensors 2024 24 7249 10.3390/s24227249 39599027 PMC11598465 4. Gao J. Gui Y. Ji W. Wen J. Zhou Y. Huang X. Wang Q. Wei C. Huang Z. Wang C. EU-Net: A segmentation network based on semantic fusion and edge guidance for road crack images Appl. Intell. 2024 54 12949 12963 10.1007/s10489-024-05788-1 5. Zhou Z. Zhou S. Zheng Y. Yan L. Yang H. Clustering and diagnosis of crack images of tunnel linings via graph neural networks Georisk Assess. Manag. Risk Eng. Syst. Geohazards 2024 18 825 837 10.1080/17499518.2024.2337380 6. Lei Q. Zhong J. Wang C. Joint optimization of crack segmentation with an adaptive dynamic threshold module IEEE Trans. Intell. Transp. Syst. 2024 25 6902 6916 10.1109/TITS.2023.3348812 7. Liu Y. Yeoh J.K.W. Robust pixel-wise concrete crack segmentation and properties retrieval using image patches Autom. Constr. 2021 123 103535 10.1016/j.autcon.2020.103535 8. Yamaguchi T. Mizutani T. Road crack detection interpreting background images by convolutional neural networks and a self-organizing map Comput.-Aided Civ. Infrastruct. Eng. 2024 39 1616 1640 10.1111/mice.13132 9. Zhao H. Shi J. Qi X. Wang X. Jia J. Pyramid Scene Parsing Network Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Honolulu, HI, USA 22&#8211;25 July 2017 2881 2890 10.1109/CVPR.2017.660 10. Zhu X. Hu H. Lin S. Dai J. Deformable ConvNets V2: More Deformable, Better Results Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Long Beach, CA, USA 15&#8211;20 June 2019 9308 9316 10.1109/CVPR.2019.00954 11. Guo M.H. Lu C.Z. Hou Q. Liu Z. Cheng M.M. Hu S.M. SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation Proceedings of the Advances in Neural Information Processing Systems (NeurIPS) New Orleans, LA, USA 28 November&#8211;9 December 2022 Volume 35 1140 1156 12. Li W. Xue L. Wang X. Li G. ConvTransNet: A CNN-Transformer Network for Change Detection with Multiscale Global-Local Representations IEEE Trans. Geosci. Remote Sens. 2023 61 5610315 10.1109/TGRS.2023.3272694 13. Xie E. Wang W. Yu Z. Anandkumar A. Alvarez J.M. Luo P. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers arXiv 2021 10.48550/arXiv.2105.15203 2105.15203 14. Wang J. Zeng Z. Sharma P.K. Alfarraj O. Tolba A. Zhang J. Wang L. Dual-path network combining CNN and transformer for pavement crack segmentation Autom. Constr. 2024 158 105217 10.1016/j.autcon.2023.105217 15. Shao Z. Wang Z. Yao X. Bell M.G.H. Gao J. ST-MambaSync: Complement the power of Mamba and Transformer fusion for less computational cost in spatial&#8211;temporal traffic forecasting Inf. Fusion 2025 117 102872 10.1016/j.inffus.2024.102872 16. Zunair H. Ben Hamza A. Masked Supervised Learning for Semantic Segmentation arXiv 2022 10.48550/arXiv.2210.00923 2210.00923 17. Zhu L. Liao B. Zhang Q. Wang X. Liu W. Wang X. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model arXiv 2024 10.48550/arXiv.2401.09417 2401.09417 18. Gu A. Dao T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces arXiv 2024 2312.00752 19. Liu Y. Tian Y. Zhao Y. Yu H. Xie L. Wang Y. Ye Q. Jiao J. Liu Y. VMamba: Visual State Space Model arXiv 2024 2401.10166 20. Yang C. Chen Z. Espinosa M. Ericsson L. Wang Z. Liu J. Crowley E.J. PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition arXiv 2024 2403.17695 21. Guo H. Li J. Dai T. Ouyang Z. Ren X. Xia S.T. MambaIR: A Simple Baseline for Image Restoration with State-Space Model Proceedings of the European Conference on Computer Vision (ECCV) Milan, Italy 29 September&#8211;4 October 2024 222 241 10.1007/978-3-031-72649-1_13 22. Liu M. Dan J. Lu Z. Yu Y. Li Y. Li X. CM-UNet: Hybrid CNN-Mamba UNet for Remote Sensing Image Semantic Segmentation arXiv 2024 2405.10530 23. Luan X. Fan H. Wang Q. Yang N. Liu S. Li X. Tang Y. FMambaIR: A Hybrid State-Space Model and Frequency Domain for Image Restoration IEEE Trans. Geosci. Remote Sens. 2025 63 4201614 10.1109/TGRS.2025.3526927 24. Li B. Zhao H. Wang W. Hu P. Gou Y. Peng X. MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Nashville, TN, USA 10&#8211;17 June 2025 7491 7501 10.1109/CVPR52734.2025.00702 25. Ding H. Xia B. Liu W. Zhang Z. Zhang J. Wang X. Xu S. A Novel Mamba Architecture with a Semantic Transformer for Efficient Real-Time Remote Sensing Semantic Segmentation Remote Sens. 2024 16 2620 10.3390/rs16142620 26. Zuo X. Sheng Y. Shen J. Shan Y. Topology-aware Mamba for Crack Segmentation in Structures Autom. Constr. 2024 168 105845 10.1016/j.autcon.2024.105845 27. Han C. Yang H. Yang Y. Enhancing Pixel-Level Crack Segmentation with Visual Mamba and Convolutional Networks Autom. Constr. 2024 168 105770 10.1016/j.autcon.2024.105770 28. Zhang T. Wang D. Lu Y. ECSNet: An Accelerated Real-Time Image Segmentation CNN Architecture for Pavement Crack Detection IEEE Trans. Intell. Transp. Syst. 2023 24 15105 15112 10.1109/TITS.2023.3300312 29. Wang W. Su C. Automatic concrete crack segmentation model based on transformer Autom. Constr. 2022 139 104275 10.1016/j.autcon.2022.104275 30. Fan Y. Hu Z. Li Q. Sun Y. Chen J. Zhou Q. CrackNet: A Hybrid Model for Crack Segmentation with Dynamic Loss Function Sensors 2024 24 7134 10.3390/s24227134 39598912 PMC11598754 31. He T. Shi F. Zhao M. Yin Y. A Lightweight Selective Feature Fusion and Irregular-Aware Network for Crack Detection Based on Federated Learning Proceedings of the International Conference on High Performance Big Data and Intelligent Systems (HDIS) Tianjin, China 10&#8211;11 December 2022 294 298 10.1109/HDIS56859.2022.9991509 32. Tao H. Liu B. Cui J. Zhang H. A Convolutional-Transformer Network for Crack Segmentation with Boundary Awareness Proceedings of the IEEE International Conference on Image Processing (ICIP) Kuala Lumpur, Malaysia 8&#8211;11 October 2023 86 90 10.1109/ICIP49359.2023.10222276 33. Rahman M.M. Tutul A.A. Nath A. Laishram L. Jung S.K. Hammond T. Mamba in Vision: A Comprehensive Survey of Techniques and Applications arXiv 2024 10.48550/arXiv.2410.03105 2410.03105 34. Liu H. Jia C. Shi F. Cheng X. Chen S. SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Nashville, TN, USA 10&#8211;17 June 2025 29406 29416 10.1109/CVPR52734.2025.02738 35. You H. Li Z. Wei Z. Zhang L. Bi X. Bi C. Li X. Duan Y. A Blueberry Maturity Detection Method Integrating Attention-Driven Multi-Scale Feature Interaction and Dynamic Upsampling Horticulturae 2025 11 600 10.3390/horticulturae11060600 36. Zhu Q. Fang Y. Cai Y. Chen C. Fan L. Rethinking Scanning Strategies With Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2024 17 18223 18234 10.1109/JSTARS.2024.3472296 37. Qu H. Ning L. An R. Fan W. Derr T. Liu H. Xu X. Li Q. A Survey of Mamba arXiv 2025 10.48550/arXiv.2408.01129 2408.01129 38. Zhao S. Chen H. Zhang X. Xiao P. Bai L. Ouyang W. RS-Mamba for Large Remote Sensing Image Dense Prediction arXiv 2024 2404.02668 10.1109/TGRS.2024.3425540 39. Sun H. Li S. Zheng X. Lu X. Remote Sensing Scene Classification by Gated Bidirectional Network IEEE Trans. Geosci. Remote Sens. 2020 58 82 96 10.1109/TGRS.2019.2931801 40. Nirthika R. Manivannan S. Ramanan A. Wang R. Pooling in convolutional neural networks for medical image analysis: A survey and an empirical study Neural Comput. Appl. 2022 34 5321 5347 10.1007/s00521-022-06953-8 35125669 PMC8804673 41. Xiao J. Guo H. Yao Y. Zhang S. Zhou J. Jiang Z. Multi-Scale Object Detection with the Pixel Attention Mechanism in a Complex Background Remote Sens. 2022 14 3969 10.3390/rs14163969 42. Liu H. Jia C. Shi F. Cheng X. Wang M. Chen S. CrackSCF: Lightweight Cascaded Fusion Network for Robust and Efficient Structural Crack Segmentation arXiv 2025 2408.12815 43. Yang F. Zhang L. Yu S. Prokhorov D. Mei X. Ling H. Feature Pyramid and Hierarchical Boosting Network for Pavement Crack Detection IEEE Trans. Intell. Transp. Syst. 2020 21 1525 1535 10.1109/TITS.2019.2910595 44. Zhang H. Zhang A.A. Dong Z. He A. Liu Y. Zhan Y. Wang K.C.P. Robust Semantic Segmentation for Automatic Crack Detection Within Pavement Images Using Multi-Mixing of Global Context and Local Image Features IEEE Trans. Intell. Transp. Syst. 2024 25 11282 11303 10.1109/TITS.2024.3360263 45. Li X. Wang W. Hu X. Yang J. Selective Kernel Networks Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Long Beach, CA, USA 15&#8211;20 June 2019 510 519 10.1109/CVPR.2019.00060 46. Woo S. Park J. Lee J.Y. Kweon I.S. CBAM: Convolutional Block Attention Module Proceedings of the European Conference on Computer Vision (ECCV) Munich, Germany 8&#8211;14 September 2018 3 19 10.1007/978-3-030-01234-2_1 Figure 1 Overall framework of CONTI-CrackNet: ( a ) CONTI-CrackNet, ( b ) SSVSS block. Figure 2 Four common single-strategy scans: ( a ) parallel scans, ( b ) diagonal scans, ( c ) Z-order scans, ( d ) Hilbert scans. Figure 3 Architecture of MD3S: ( a ) MD3S, ( b ) BiGF. Figure 4 Structures of DBPGL and PAP: ( a ) DBPGL; ( b ) PAP. Figure 5 Sample images and ground-truth masks from the datasets used in this study: ( a ) TUT, ( b ) CRACK500. Figure 6 Typical visual comparison on the TUT dataset using four methods. Red boxes highlight key details; green boxes mark wrongly identified regions. Figure 7 Typical visual comparison on the CRACK500 dataset using four methods. The red boxes highlight key details; the green boxes mark wrongly identified regions. Figure 8 Visualization of CONTI-CrackNet failure cases. Yellow boxes indicate missed detections. sensors-25-06865-t001_Table 1 Table 1 Description of the experimental datasets. Dataset Resolution Images Training Validation Test TUT&#160;[ 42 ] 640 &#215; 640 1408 986 141 281 CRACK500&#160;[ 43 ] 640 &#215; 360 3368 2358 337 673 sensors-25-06865-t002_Table 2 Table 2 Sensitivity analysis of &#945; on the TUT dataset. &#945; ODS OIS F1 P R mIoU 0 0.8021 0.8059 0.8279 0.8176 0.8384 0.8368 0.1 0.8049 0.8116 0.8292 0.8185 0.8402 0.8371 0.2 0.8133 0.8165 0.8332 0.8220 0.8447 0.8436 0.3 0.8110 0.8123 0.8313 0.8196 0.8434 0.8420 0.5 0.8002 0.8017 0.8283 0.8201 0.8366 0.8359 sensors-25-06865-t003_Table 3 Table 3 Comparison of segmentation results of different models on the TUT dataset. Methods ODS OIS F1 P R mIoU SFIAN (2022)&#160;[ 31 ] 0.7290 0.7513 0.7473 0.7715 0.7247 0.7756 SegNeXt (2022)&#160;[ 11 ] 0.7312 0.7435 0.7517 0.7812 0.7245 0.7785 Crackmer (2024)&#160;[ 14 ] 0.7429 0.7501 0.7578 0.7501 0.7656 0.7966 SegFormer (2021)&#160;[ 13 ] 0.7532 0.7612 0.7670 0.7654 0.7688 0.8078 CT-crackseg (2023)&#160;[ 32 ] 0.7940 0.7996 0.8199 0.8202 0.8195 0.8301 CSMamba (2024)&#160;[ 22 ] 0.7879 0.7946 0.8146 0.7947 0.8353 0.8263 PlainMamba (2024)&#160;[ 20 ] 0.7889 0.7954 0.8154 0.7955 0.8365 0.8316 Ours 0.8133 0.8165 0.8332 0.8220 0.8447 0.8436 sensors-25-06865-t004_Table 4 Table 4 Comparison of segmentation results of different models on the CRACK500 dataset. Methods ODS OIS F1 P R mIoU SFIAN (2022)&#160;[ 31 ] 0.6473 0.6941 0.7204 0.6983 0.7441 0.7315 SegNeXt (2022)&#160;[ 11 ] 0.6488 0.6762 0.7334 0.7134 0.7546 0.7345 Crackmer (2024)&#160;[ 14 ] 0.6933 0.7097 0.7267 0.6985 0.7572 0.7421 SegFormer (2021)&#160;[ 13 ] 0.6998 0.7134 0.7245 0.7067 0.7434 0.7456 CT-crackseg (2023)&#160;[ 32 ] 0.6941 0.7059 0.7322 0.6940 0.7748 0.7591 CSMamba (2024)&#160;[ 22 ] 0.6931 0.7162 0.7315 0.6858 0.7823 0.7592 PlainMamba (2024)&#160;[ 20 ] 0.6574 0.6870 0.7422 0.7318 0.7530 0.7579 Ours 0.7104 0.7301 0.7587 0.7333 0.7860 0.7760 sensors-25-06865-t005_Table 5 Table 5 Ablation validating the effectiveness of the proposed modules. MD3S ElemAdd SKNet DBPGL PAP ODS OIS F1 P R mIoU &#10007; &#10003; &#10007; &#10007; &#10007; 0.7845 0.7921 0.8049 0.7945 0.8156 0.8078 &#10003; &#10003; &#10007; &#10007; &#10007; 0.8008 0.8074 0.8268 0.8161 0.8377 0.8350 &#10003; &#10007; &#10003; &#10007; &#10007; 0.7944 0.8064 0.8200 0.8123 0.8279 0.8268 &#10003; &#10007; &#10007; &#10003; &#10007; 0.8094 0.8168 0.8297 0.8142 0.8459 0.8406 &#10003; &#10007; &#10007; &#10003; &#10003; 0.8133 0.8165 0.8332 0.8220 0.8447 0.8436 sensors-25-06865-t006_Table 6 Table 6 Ablation validating the effectiveness of the proposed MD3S. Method ODS OIS F1 P R mIoU ParaSna 0.7880 0.7926 0.8063 0.7898 0.8236 0.8041 DiagSna 0.7897 0.7979 0.8077 0.7891 0.8273 0.8059 ParaSna + DiagSna 0.7970 0.8071 0.8127 0.7909 0.8359 0.8260 MD3S 0.8008 0.8074 0.8268 0.8161 0.8377 0.8350 sensors-25-06865-t007_Table 7 Table 7 Ablation validating the effectiveness of the PAM. Methods ODS OIS F1 P R mIoU CBAM 0.8016 0.8023 0.8250 0.8245 0.8256 0.8277 PAM 0.8133 0.8165 0.8332 0.8220 0.8447 0.8436 sensors-25-06865-t008_Table 8 Table 8 Comparison of CONTI-CrackNet and other methods in Params, GFLOPs, and FPS. Methods GFLOPs Params FPS SFIAN (2022)&#160;[ 31 ] 84.57 G 13.63 M 32 SegNeXt (2022)&#160;[ 11 ] 31.80 G 27.52 M 22 Crackmer (2024)&#160;[ 14 ] 14.94 G 5.90 M 33 SegFormer (2021)&#160;[ 13 ] 30.80 G 28.20 M 21 CT-crackseg (2023)&#160;[ 32 ] 39.47 G 22.88 M 28 CSMamba (2024)&#160;[ 22 ] 145.84 G 35.95 M 19 PlainMamba (2024)&#160;[ 20 ] 73.36 G 16.72 M 33 Ours 24.22 G 6.01 M 42"
}