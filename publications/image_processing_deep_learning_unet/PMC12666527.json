{
  "pmcid": "PMC12666527",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.740130",
  "metadata": {
    "journal_title": "Frontiers in Oncology",
    "journal_nlm_ta": "Front Oncol",
    "journal_iso_abbrev": "Front Oncol",
    "journal": "Frontiers in Oncology",
    "pmcid": "PMC12666527",
    "pmid": "41333203",
    "doi": "10.3389/fonc.2025.1679826",
    "title": "Real-time colonoscopic detection and precise segmentation of colorectal polyps via PESNet",
    "authors": [
      "Yu Jing",
      "Zhu Jianchun",
      "Gu Qi",
      "Sun Yuhan",
      "Wang Qin",
      "Sun Pengcheng",
      "Gu Liugen"
    ],
    "abstract": "Introduction Precise and timely visual assistance is critical for detecting and completely removing colorectal cancer precursor polyps, a key step in preventing interval cancer and reducing patient morbidity. Current endoscopic workflows lack real-time, integrated solutions for simultaneous polyp diagnosis and segmentation, creating unmet needs in improving adenoma detection rates and resection precision. Methods We propose PESNet, a real-time assistance framework for standard endoscopy workstations. It simultaneously performs frame-level polyp diagnosis and pixel-level polyp outlining at 225 FPS, with minimal additional latency and no specialized hardware. PESNet dynamically injects a “presence of polyp” prompt into the segmentation stream, refines lesion boundaries in real time, and compensates for lighting/mucosal texture changes via a lightweight adaptive module. Evaluations were conducted on PolypDiag, CVC-12K benchmark datasets, and replay resection scenarios. Latency was measured using TensorRT FP16 on an RTX 6000 Ada GPU. Results On PolypDiag and CVC-12K, PESNet improved diagnostic F1 from 95.0% to 97.2% and segmentation Dice from 85.4% to 89.1%. This translated to a 26% reduction in missed flat polyps and a 15% reduction in residual tumor margins after cold snare resection. End-to-end latency (1080p) was 12.6 ± 0.3 ms per frame, with segmentation (4.4 ms), prompt fusion (0.6 ms), and prototype lookup (< 0.2 ms) all satisfying a 40 ms clinical budget with > 3× headroom. Discussion These clinically significant improvements demonstrate PESNet’s potential to enhance adenoma detection rates, support cleaner resection margins, and ultimately reduce colorectal cancer incidence during routine endoscopic examinations. Its real-time performance and hardware compatibility make it feasible for integration into standard endoscopic workflows, addressing critical gaps in polyp management.",
    "keywords": [
      "colorectal polyp",
      "state-space network",
      "prompt learning",
      "segmentation",
      "prototype memory"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Front Oncol</journal-id><journal-id journal-id-type=\"iso-abbrev\">Front Oncol</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1755</journal-id><journal-id journal-id-type=\"pmc-domain\">frontonco</journal-id><journal-id journal-id-type=\"publisher-id\">Front. Oncol.</journal-id><journal-title-group><journal-title>Frontiers in Oncology</journal-title></journal-title-group><issn pub-type=\"epub\">2234-943X</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12666527</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12666527.1</article-id><article-id pub-id-type=\"pmcaid\">12666527</article-id><article-id pub-id-type=\"pmcaiid\">12666527</article-id><article-id pub-id-type=\"pmid\">41333203</article-id><article-id pub-id-type=\"doi\">10.3389/fonc.2025.1679826</article-id><article-version-alternatives><article-version article-version-type=\"pmc-version\">1</article-version><article-version article-version-type=\"Version of Record\" vocab=\"NISO-RP-8-2008\"/></article-version-alternatives><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Real-time colonoscopic detection and precise segmentation of colorectal polyps via PESNet</article-title></title-group><contrib-group><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Yu</surname><given-names initials=\"J\">Jing</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"fn003\" ref-type=\"author-notes\">\n<sup>&#8224;</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3153624/overview\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project-administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Zhu</surname><given-names initials=\"J\">Jianchun</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">\n<sup>3</sup>\n</xref><xref rid=\"fn003\" ref-type=\"author-notes\">\n<sup>&#8224;</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names initials=\"Q\">Qi</given-names></name><xref rid=\"aff4\" ref-type=\"aff\">\n<sup>4</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names initials=\"Y\">Yuhan</given-names></name><xref rid=\"aff4\" ref-type=\"aff\">\n<sup>4</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"Q\">Qin</given-names></name><xref rid=\"aff5\" ref-type=\"aff\">\n<sup>5</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Sun</surname><given-names initials=\"P\">Pengcheng</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">\n<sup>3</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Gu</surname><given-names initials=\"L\">Liugen</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib></contrib-group><aff id=\"aff1\"><label>1</label><institution>Department of Gastroenterology, The Southeast University Affiliated Nantong First People&#8217;s Hospital</institution>, <city>Nantong</city>,&#160;<country country=\"cn\">China</country></aff><aff id=\"aff2\"><label>2</label><institution>Department of Gastroenterology, the First People&#8217;s Hospital of Nantong</institution>, <city>Nantong</city>,&#160;<country country=\"cn\">China</country></aff><aff id=\"aff3\"><label>3</label><institution>Suzhou Xiangcheng People&#8217;s Hospital</institution>, <city>Suzhou</city>,&#160;<country country=\"cn\">China</country></aff><aff id=\"aff4\"><label>4</label><institution>School of Medicine, Nantong University</institution>, <city>Nantong</city>,&#160;<country country=\"cn\">China</country></aff><aff id=\"aff5\"><label>5</label><institution>Affiliated Nantong Hospital 3 of Nantong University</institution>, <city>Nantong</city>,&#160;<country country=\"cn\">China</country></aff><author-notes><corresp id=\"c001\"><label>*</label>Correspondence: Pengcheng Sun, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:sunpengcheng8723@163.com\">sunpengcheng8723@163.com</email>; Liugen Gu, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:guliugen@sina.com\">guliugen@sina.com</email></corresp><fn fn-type=\"equal\" id=\"fn003\"><label>&#8224;</label><p>These authors have contributed equally to this work and share first authorship</p></fn></author-notes><pub-date publication-format=\"electronic\" date-type=\"pub\" iso-8601-date=\"2025-11-17\"><day>17</day><month>11</month><year>2025</year></pub-date><pub-date publication-format=\"electronic\" date-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">480898</issue-id><elocation-id>1679826</elocation-id><history><date date-type=\"received\"><day>12</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>20</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>17</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>02</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-03 09:25:14.567\"><day>03</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Yu, Zhu, Gu, Sun, Wang, Sun and Gu.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yu, Zhu, Gu, Sun, Wang, Sun and Gu</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\" start_date=\"2025-11-13\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"fonc-15-1679826.pdf\"/><abstract><sec><title>Introduction</title><p>Precise and timely visual assistance is critical for detecting and completely removing colorectal cancer precursor polyps, a key step in preventing interval cancer and reducing patient morbidity. Current endoscopic workflows lack real-time, integrated solutions for simultaneous polyp diagnosis and segmentation, creating unmet needs in improving adenoma detection rates and resection precision.</p></sec><sec><title>Methods</title><p>We propose PESNet, a real-time assistance framework for standard endoscopy workstations. It simultaneously performs frame-level polyp diagnosis and pixel-level polyp outlining at 225 FPS, with minimal additional latency and no specialized hardware. PESNet dynamically injects a &#8220;presence of polyp&#8221; prompt into the segmentation stream, refines lesion boundaries in real time, and compensates for lighting/mucosal texture changes via a lightweight adaptive module. Evaluations were conducted on PolypDiag, CVC-12K benchmark datasets, and replay resection scenarios. Latency was measured using TensorRT FP16 on an RTX 6000 Ada GPU.</p></sec><sec><title>Results</title><p>On PolypDiag and CVC-12K, PESNet improved diagnostic F1 from 95.0% to 97.2% and segmentation Dice from 85.4% to 89.1%. This translated to a 26% reduction in missed flat polyps and a 15% reduction in residual tumor margins after cold snare resection. End-to-end latency (1080p) was 12.6 &#177; 0.3 ms per frame, with segmentation (4.4 ms), prompt fusion (0.6 ms), and prototype lookup (&lt; 0.2 ms) all satisfying a 40 ms clinical budget with &gt; 3&#215; headroom.</p></sec><sec><title>Discussion</title><p>These clinically significant improvements demonstrate PESNet&#8217;s potential to enhance adenoma detection rates, support cleaner resection margins, and ultimately reduce colorectal cancer incidence during routine endoscopic examinations. Its real-time performance and hardware compatibility make it feasible for integration into standard endoscopic workflows, addressing critical gaps in polyp management.</p></sec></abstract><kwd-group><kwd>colorectal polyp</kwd><kwd>state-space network</kwd><kwd>prompt learning</kwd><kwd>segmentation</kwd><kwd>prototype memory</kwd></kwd-group><funding-group><funding-statement>The author(s) declare financial support was received for the research and/or publication of this article. Nantong Municipal Science and Technology Bureau Social Livelihood Science and Technology Funding Project (Grant No. MSZ21066); Scientific Research Project Funded by Nantong Municipal Health Commission (Grant No. MS2024065).</funding-statement></funding-group><counts><fig-count count=\"5\"/><table-count count=\"3\"/><equation-count count=\"7\"/><ref-count count=\"26\"/><page-count count=\"10\"/><word-count count=\"4637\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Gastrointestinal Cancers: Colorectal Cancer</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"s1\"><label>1</label><title>Introduction</title><p>Colorectal cancer (CRC) continues to rank within the global top three for both incidence and cancer-related mortality (<xref rid=\"B1\" ref-type=\"bibr\">1</xref>, <xref rid=\"B2\" ref-type=\"bibr\">2</xref>, <xref rid=\"B3\" ref-type=\"bibr\">3</xref>). Population-based registries now confirm a further &#8220;left-shift&#8221; toward diagnoses in adults&lt; 50 years, underscoring modifiable lifestyle and environmental risks (<xref rid=\"B4\" ref-type=\"bibr\">4</xref>). Optical colonoscopy remains the gold-standard screening test because it couples direct mucosal inspection with same-session endoscopic mucosal resection (EMR) of premalignant polyps (<xref rid=\"B5\" ref-type=\"bibr\">5</xref>). Yet large tandem-procedure meta-analyses still find that conventional white-light colonoscopy misses &#8776; 25% of adenomas. Multiple randomised and real-world trials published in 2024&#8211;2025 now show that computer-aided detection (CADe) raises the mean adenoma-detection rate (ADR) by 20&#8211;30% and cuts miss rates nearly in half&#8212;even in community hospitals and national health-care systems (<xref rid=\"B6\" ref-type=\"bibr\">6</xref>, <xref rid=\"B7\" ref-type=\"bibr\">7</xref>). Reflecting this momentum, both the European Society of Gastrointestinal Endoscopy (ESGE) and the American Gastroenterological Association (AGA) issued <italic toggle=\"yes\">2025</italic> guidance on CADe-assisted colonoscopy (<xref rid=\"B8\" ref-type=\"bibr\">8</xref>); while ESGE endorses its use to improve quality indicators, the AGA Living Guideline judged the long-term outcome evidence &#8220;very low certainty&#8221; and therefore made no formal recommendation pending further data (<xref rid=\"B9\" ref-type=\"bibr\">9</xref>, <xref rid=\"B10\" ref-type=\"bibr\">10</xref>).</p><p>Three inter-related bedside bottlenecks still limit such deployment. First, stringent latency thresholds dominate engineering design: 1080p video streams at 25&#8211;30fps allow &#8776; 40ms per frame for <italic toggle=\"yes\">all</italic> AI processing; many 3-D CNN or Vision-Transformer stacks still deliver&lt; 10fps, and even state-space backbones approach the limit once a full-resolution segmentation decoder is attachedSedeh and Sharifian (<xref rid=\"B11\" ref-type=\"bibr\">11</xref>, <xref rid=\"B12\" ref-type=\"bibr\">12</xref>). Second, severe data imbalance persists: pixel-level annotated frames number only in the low thousands, whereas image-level labels are an order of magnitude more plentiful, so models can decide &#8220;polyp present&#8221; with high confidence yet delineate flat or sessile-serrated lesions poorly (<xref rid=\"B13\" ref-type=\"bibr\">13</xref>, <xref rid=\"B14\" ref-type=\"bibr\">14</xref>, <xref rid=\"B15\" ref-type=\"bibr\">15</xref>). Third, inter-institutional variability erodes generalisability: shifts in illumination spectra, colour balance, optical filters and vendor-specific post-processing mean that a high-performing model in one centre may suffer a marked Dice-score drop in another; routine site-specific retraining is impractical for both workflow and regulatory reasons (<xref rid=\"B16\" ref-type=\"bibr\">16</xref>, <xref rid=\"B17\" ref-type=\"bibr\">17</xref>).</p><p>To address these hurdles we introduce PESNet, a cross-task prompt-learning framework that couples the real-time efficiency of a state-space video backbone with the parameter-sparse adaptability of an SVD-based <italic toggle=\"yes\">Segment-Anything</italic> adaptor (<xref rid=\"f2\" ref-type=\"fig\"><bold>Figure&#160;2</bold></xref>). A discriminative token learned from the clinical-grade <italic toggle=\"yes\">PolypDiag</italic> dataset is verbalised on-the-fly into a &#8220;polyp present/absent&#8221; prompt, which tightens pixel boundaries in the segmentation branch; the resultant mask area feeds back to stabilise the diagnostic head (<xref rid=\"B10\" ref-type=\"bibr\">10</xref>). Adaptation is confined to the singular spectra of every spatial and temporal weight matrix via a dual-axis S-LoRA scheme, adding only &#8776; 0.57% (136k) new parameters yet sustaining 225fps on a single RTX 6000 Ada GPU&#8212;comfortably within workstation latency budgets. A 256-vector prototype memory executes a single cosine lookup in&lt;0.05ms, auto-calibrating logit bias and mitigating illumination or colour drift without retraining.</p><p>Collectively, these modules lift the Dice coefficient on <italic toggle=\"yes\">CVC-12K</italic> by +3.7percentage points and the <italic toggle=\"yes\">F</italic><sub>1</sub> score on <italic toggle=\"yes\">PolypDiag</italic> by +2.2percentage points. Clinically, this translates to a 26% reduction in missed flat lesions and a 15% decrease in residual-tumour margins during replayed cold-snare resections&#8212;achieved on workstation-class hardware without extra annotation or equipment costs. PESNet therefore delivers a guideline-concordant, interactive and genuinely real-time CADe solution poised to improve ADR, secure cleaner resection margins, and ultimately lower CRC incidence in everyday practice.</p></sec><sec id=\"s2\"><label>2</label><title>Broader related work and positioning</title><p>Beyond colonoscopy, prompt-aware or attention-enhanced vision models have advanced diverse medical tasks. For example, EEG-based epilepsy detection benefits from entropy-driven deep or CNN-based pipelines that marry non-linear complexity measures with learnable feature extractors (<xref rid=\"B18\" ref-type=\"bibr\">18</xref>, <xref rid=\"B19\" ref-type=\"bibr\">19</xref>). In neuro-oncology, hybrid attention CNNs and Transformer-augmented pipelines improve MR brain-tumor analysis (<xref rid=\"B20\" ref-type=\"bibr\">20</xref>, <xref rid=\"B21\" ref-type=\"bibr\">21</xref>), while RepVGG style enhanced backbones and their dual-encoder variants (e.g., ViT+RepVGG) provide deployment friendly speed/accuracy trade-offs for multimodal tumor segmentation (<xref rid=\"B22\" ref-type=\"bibr\">22</xref>, <xref rid=\"B23\" ref-type=\"bibr\">23</xref>). These trends motivate lightweight attention and adaptor designs that transfer well to endoscopy. We therefore situate PESNet among recent Transformer-hybrids Jia and Shu (<xref rid=\"B24\" ref-type=\"bibr\">24</xref>), self-supervised/pre-training and PEFT practices for SAM-family adaptation (<xref rid=\"B25\" ref-type=\"bibr\">25</xref>), and multi-modal fusion approaches (<xref rid=\"B26\" ref-type=\"bibr\">26</xref>), emphasizing parameter-efficient prompting/adapters as a practical bridge from foundation models to real-time clinical use.</p></sec><sec id=\"s3\"><label>3</label><title>Method</title><p>A visual overview of the dataset is presented in <xref rid=\"f1\" ref-type=\"fig\"><bold>Figure&#160;1</bold></xref>, which includes representative colonoscopic images covering normal mucosa and various polyp types, laying a foundation for diverse model training. Our framework performs simultaneous frame-level diagnosis and pixel-accurate delineation while 75 remaining within the strict 40 ms latency budget imposed by modern endoscopy workstations.</p><fig position=\"float\" id=\"f1\" orientation=\"portrait\"><label>Figure&#160;1</label><caption><p>Visual overview of dataset. <bold>(a)</bold> original endoscopic images of colorectal mucosa, for observing polyp morphology and surroundings; <bold>(b)</bold> polyp area mask annotation (white), defining polyp boundaries; <bold>(c)</bold> gridded/contoured polyp areas for algorithmic recognition and segmentation; <bold>(d)</bold> close-ups of polyps with distinct pathological types.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g001.jpg\"><alt-text content-type=\"machine-generated\">A series of endoscopic images showing the interior of a colon. Each row contains different views and conditions, with varying appearances of the bowel lining. Some images highlight particular areas with overlays or enhancements, possibly indicating abnormalities or points of interest. The images are labeled with letters (b, c, d, e) for reference.</alt-text></graphic></fig><fig position=\"float\" id=\"f2\" orientation=\"portrait\"><label>Figure&#160;2</label><caption><p>Overview of the proposed model architecture. Intuitive view: the diagnosis head answers &#8220;is there a polyp now?&#8221;, then its yes/no signal is distilled into a simple prompt that sharpens the segmentation mask, while a tiny memory block keeps predictions stable under illumination drift.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g002.jpg\"><alt-text content-type=\"machine-generated\">Flowchart of a medical image processing system involving colonoscopy frames. Frames are processed through a Spatial Bidirectional Mamba block, creating latent representations via patch embedding. The process splits into a discriminative token leading to a classification head and S-SAM segmentation head. They contribute to BCE, Dice, and Distillation Loss calculations. A modulation loop includes Dual-Axis S-LoRA Spectral Scales linking to Prototype Memory.</alt-text></graphic></fig><p>Our framework performs <italic toggle=\"yes\">simultaneous</italic> frame-level diagnosis and pixel-accurate delineation while remaining within the strict 40 ms latency budget imposed by modern endoscopy workstations. It couples (i) a state-space video backbone, (ii) a prompt-aware segmentation adaptor, and (iii) an ultra-lightweight prototype memory, all optimised end-to-end under a single learning objective. The following subsections present the theoretical motivation, algorithmic details and computational consequences of each component in continuous prose.</p><sec id=\"s3_1\"><label>3.1</label><title>Pseudocode of online inference</title><p>The pseudocode for online inference is presented in <xref rid=\"f6\" ref-type=\"fig\"><bold>Algorithm 1</bold></xref>.</p><fig position=\"float\" id=\"f6\" orientation=\"portrait\"><label>Algorithm 1</label><caption><p>PESNet Online Inference (per-frame at 1080p) Pseudocode of online inference.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g006.jpg\"/></fig><p><xref rid=\"f6\" ref-type=\"fig\"><bold>Algorithm 1</bold></xref> details the online inference process of PESNet, covering the entire workflow from input frame processing to outputting diagnostic results and segmentation masks. This process ensures real-time execution at 1080p resolution.</p></sec><sec id=\"s3_2\"><label>3.2</label><title>State-space backbone</title><p>Given a colonoscopy clip <inline-formula>\n<mml:math id=\"im9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> with <inline-formula>\n<mml:math id=\"im10\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mo>&#8712;</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, every frame is first divided into <inline-formula>\n<mml:math id=\"im11\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> non-overlapping patches, yielding a length- <inline-formula>\n<mml:math id=\"im12\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy=\"false\">/</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> token sequence. Each frame is then processed by a <italic toggle=\"yes\">bidirectional</italic> Mamba block whose implicit recurrence offers <italic toggle=\"yes\">linear</italic>, rather than quadratic, token-interaction cost. The resulting spatial representation <inline-formula>\n<mml:math id=\"im13\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>h</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is forwarded to a <italic toggle=\"yes\">causal</italic> temporal Mamba, which maintains a hidden state <inline-formula>\n<mml:math id=\"im14\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and updates</p><disp-formula id=\"eq1\"><label>(1)</label><mml:math id=\"M1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>h</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>s</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Mamba&#160;</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>h</mml:mi></mml:mstyle><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq1\" ref-type=\"disp-formula\">Equation 1</xref> describes the update process of spatial representation and hidden state by the Mamba module. Because the Mamba kernel is convolutional and pre-computed, the full spatio-temporal pipeline scales as <inline-formula>\n<mml:math id=\"im15\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in runtime and <inline-formula>\n<mml:math id=\"im16\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in memory, permitting <inline-formula>\n<mml:math id=\"im17\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1080</mml:mn><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> inference at 30 fps on an NVIDIA<sup>&#174;</sup> Jetson NX. A <italic toggle=\"yes\">discriminative</italic> token <inline-formula>\n<mml:math id=\"im18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is appended to each temporal step; its final state <inline-formula>\n<mml:math id=\"im19\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> drives a logistic classifier</p><disp-formula id=\"eq2\"><label>(2)</label><mml:math id=\"M2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msubsup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>&#8868;</mml:mo></mml:msubsup><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq2\" ref-type=\"disp-formula\">Equation 2</xref> maps discriminative tokens to polyp existence probability via a logistic classifie, where <inline-formula>\n<mml:math id=\"im20\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> denotes &#8220;polyp present&#8221;. In this way, the backbone sustains real-time throughput while retaining long-range temporal context&#8212;an essential prerequisite for reliable, clinic-ready CADe.</p></sec><sec id=\"s3_3\"><label>3.3</label><title>Cross-task prompt distillation</title><p><italic toggle=\"yes\">PolypDiag</italic> provides accurate frame labels but no masks, whereas <italic toggle=\"yes\">CVC-12K</italic> supplies high-quality masks yet lacks labels. Cross-Task Prompt Distillation reconciles this asymmetry by converting the discriminative token <bold>d</bold><italic toggle=\"yes\"><sub>T</sub></italic> into a text-like prompt. A linear projection.</p><disp-formula id=\"eq3\"><label>(3)</label><mml:math id=\"M3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>p</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>W</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>d</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>b</mml:mi></mml:mstyle><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq3\" ref-type=\"disp-formula\">Equation 3</xref> implements linear projection of discriminative tokens into the prompt space. Maps the token into prompt space; <inline-formula>\n<mml:math id=\"im21\" display=\"inline\" overflow=\"scroll\"><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> is embedded into the fixed template &#8220; <inline-formula>\n<mml:math id=\"im22\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#9001;</mml:mo><mml:mrow><mml:mtext>SOS</mml:mtext></mml:mrow><mml:mo>&#9002;</mml:mo></mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>p</mml:mi></mml:mstyle><mml:mtext>&#160;</mml:mtext><mml:mrow><mml:mo>&#9001;</mml:mo><mml:mrow><mml:mtext>EOS</mml:mtext></mml:mrow><mml:mo>&#9002;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>&#8216;&#8216; and injected into the text encoder of an SVD-adapted Segment-Anything head (<italic toggle=\"yes\">S-SAM</italic>). Conditioned on the backbone visual tokens <inline-formula>\n<mml:math id=\"im23\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>h</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, S-SAM yields the dense mask</p><disp-formula id=\"eq4\"><label>(4)</label><mml:math id=\"M4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>S</mml:mtext><mml:mo>&#8722;</mml:mo><mml:mtext>SAM</mml:mtext></mml:mrow></mml:msub><mml:mtext>&#160;</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>h</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>p</mml:mi></mml:mstyle></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy=\"false\">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq4\" ref-type=\"disp-formula\">Equation 4</xref> illustrates the process by which S-SAM generates dense masks based on visual tokens and prompts. Coherence between diagnosis and delineation is enforced by matching the expected mask area to the classification probability:</p><disp-formula id=\"eq5\"><label>(5)</label><mml:math id=\"M5\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#8466;</mml:mi><mml:mrow><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mtext>mean&#160;</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msup><mml:mo>)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq5\" ref-type=\"disp-formula\">Equation 5</xref> constrains the consistency between diagnosis and segmentation via distillation loss. Minimising <inline-formula>\n<mml:math id=\"im24\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#8466;</mml:mi><mml:mrow><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> tightens an upper bound on the conditional mutual information <inline-formula>\n<mml:math id=\"im25\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>I</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>;</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mtext>&#160;</mml:mtext><mml:mo>|</mml:mo><mml:mtext>&#160;</mml:mtext><mml:mi>X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, empirically reducing mask entropy and sharpening lesion borders <italic toggle=\"yes\">without</italic> extra pixel-level annotation.</p></sec><sec id=\"s3_4\"><label>3.4</label><title>Dual-axis S-LoRA</title><p>Full fine-tuning of the backbone is infeasible within clinical memory budgets, and conventional low-rank adapters still incur quadratic products at inference. In Dual-Axis S-LoRA, all original weights remain frozen; only their singular spectra are modulated. For a frozen weight matrix <inline-formula>\n<mml:math id=\"im26\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>U</mml:mi></mml:mstyle><mml:mtext>diag&#160;</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>&#963;</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#8868;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> we learn scale&#8211;shift vectors <inline-formula>\n<mml:math id=\"im27\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and re-parameterise</p><disp-formula id=\"eq6\"><label>(6)</label><mml:math id=\"M6\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>W</mml:mi><mml:mo>&#732;</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>U</mml:mi></mml:mstyle><mml:mtext>diag&#160;</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8857;</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>V</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#8868;</mml:mo></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq6\" ref-type=\"disp-formula\">Equation 6</xref> enables the modulation of frozen weights by Dual-axis S-LoRA. A single pair (<italic toggle=\"yes\">&#945;,&#946;</italic>) is shared by every spatial Bi-Mamba and temporal Mamba layer, limiting new parameters to 2<italic toggle=\"yes\">r</italic>&#8212;about 0.25% of the backbone. The spectra-sharing regularises high-frequency noise, enhancing robustness to motion blur and electronic artefacts while preserving the vanilla backbone&#8217;s 46 fps throughput.</p></sec><sec id=\"s3_5\"><label>3.5</label><title>Prototype-memory adaptation</title><p>Variation in illumination, colour balance and vendor post-processing induces systematic logit shifts. We counter this drift with a prototype memory <inline-formula>\n<mml:math id=\"im28\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>m</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula>\n<mml:math id=\"im29\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>, of unit-norm vectors. At inference, the normalised discriminative token <inline-formula>\n<mml:math id=\"im30\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>d</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mstyle><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is compared to the bank via cosine similarity, producing weights <inline-formula>\n<mml:math id=\"im31\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>m</mml:mi></mml:mstyle><mml:mi>k</mml:mi><mml:mo>&#8868;</mml:mo></mml:msubsup><mml:msub><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mover accent=\"true\"><mml:mi>d</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mstyle><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Softmax-normalised weights then form a bias vector <inline-formula>\n<mml:math id=\"im32\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mtext>&#916;</mml:mtext><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>B</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula>, with learnable <inline-formula>\n<mml:math id=\"im33\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>B</mml:mi></mml:mstyle><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The adjusted logits <inline-formula>\n<mml:math id=\"im34\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>z</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mtext>&#916;</mml:mtext><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>z</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula> feed directly into the sigmoid, adding <inline-formula>\n<mml:math id=\"im35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> ms latency on embedded GPUs. During training, prototypes track class-conditioned token means by exponential moving average, while an orthogonality penalty <inline-formula>\n<mml:math id=\"im36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo>&#8214;</mml:mo><mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>M</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#8868;</mml:mo></mml:mrow></mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>M</mml:mi></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>I</mml:mi></mml:mstyle><mml:msubsup><mml:mo>&#8214;</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> discourages redundancy. Removing the memory reduces Dice by over two points under illumination shift, confirming its clinical value.</p></sec><sec id=\"s3_6\"><label>3.6</label><title>Loss function and optimisation</title><p>The total loss combines binary cross-entropy for diagnosis, soft-Dice for segmentation, the distillation term above and the prototype orthogonality regulariser:</p><disp-formula id=\"eq7\"><label>(7)</label><mml:math id=\"M7\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>&#8466;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>&#8466;</mml:mi><mml:mrow><mml:mtext>BCE</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#8466;</mml:mi><mml:mrow><mml:mtext>Dice</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>&#8466;</mml:mi><mml:mrow><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mtext>mem</mml:mtext></mml:mrow></mml:msub><mml:mo>&#8214;</mml:mo><mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>M</mml:mi></mml:mstyle><mml:mrow><mml:mo>&#8868;</mml:mo></mml:mrow></mml:msup><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>M</mml:mi></mml:mstyle><mml:mo>&#8722;</mml:mo><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>I</mml:mi></mml:mstyle><mml:msubsup><mml:mo>&#8214;</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p><xref rid=\"eq7\" ref-type=\"disp-formula\">Equation 7</xref> defines the model&#8217;s total loss function with <inline-formula>\n<mml:math id=\"im37\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula>\n<mml:math id=\"im38\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#955;</mml:mi><mml:mrow><mml:mtext>mem</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>. All modules are trained jointly using AdamW (initial learning rate <inline-formula>\n<mml:math id=\"im39\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, cosine decay, weight decay 0.05). Convergence is reached in 35 k iterations on two RTX 6000 Ada GPUs. redHyperparameters were selected by a coarse-to-fine search (Optuna, 50 trials; search ranges in <xref rid=\"T1\" ref-type=\"table\"><bold>Table&#160;1</bold></xref>), then fixed across all datasets and seeds for fair comparison. The final network&#8212;including frozen backbone, spectral scale&#8211;shift vectors, prompt projector and prototype memory&#8212;occupies 820 MB of VRAM yet maintains 46 fps <inline-formula>\n<mml:math id=\"im40\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1080</mml:mn><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> inference on an NVIDIA<sup>&#174;</sup> Jetson Xavier NX, thereby satisfying real-time clinical constraints while materially improving both diagnostic accuracy and delineation fidelity.</p><table-wrap position=\"float\" id=\"T1\" orientation=\"portrait\"><label>Table&#160;1</label><caption><p>Shared hyperparameter search (Optuna, 50 trials) and selected values.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Hyperparameter</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Range</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">PESNet</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Applied to baselines</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Learning rate</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">[1&#215;10<sup>&#8722;5</sup>,3&#215;10<sup>&#8722;3</sup>]</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">3&#215;10<sup>&#8722;4</sup></td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">grid within range</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Weight decay</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">[0,0.1]</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.05</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">matched best per model</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Batch size</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">{8,12,16}</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">12</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">as memory allows</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Prompt distill <italic toggle=\"yes\">&#955;</italic><sub>dist</sub></td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">[0.05,0.4]</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">n/a</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Mem. orthogonality <italic toggle=\"yes\">&#955;</italic><sub>mem</sub></td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">[0.001,0.05]</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.01</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">n/a</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">S-LoRA rank <italic toggle=\"yes\">r</italic></td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">{8,12,16}</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">12</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">n/a</td></tr></tbody></table></table-wrap></sec></sec><sec id=\"s4\"><label>4</label><title>Experimental results</title><sec id=\"s4_1\"><label>4.1</label><title>Implementation details</title><p>All experiments were conducted on two NVIDIA<sup>&#174;</sup> RTX6000 Ada GPUs. One card executes the forward and backward passes, whereas the second handles asynchronous data streaming; consequently, all throughput figures <italic toggle=\"yes\">reflect a single</italic> RTX6000 Ada.</p><p>In all experiments we rely on two public benchmarks&#8212;PolypDiag and CVC-12K &#8212;to ensure a fair, reproducible evaluation (<xref rid=\"f1\" ref-type=\"fig\"><bold>Figure&#160;1</bold></xref>). PolypDiag fuses Hyper-Kvasir, LDPolypVideo and other endoscopy sources, yielding 253 short gastroscopy clips (5 s each, 30 fps; 485561 frames in total) that carry only video-level binary labels (Polyp vs. Normal, 63% positive). Following the authors&#8217; protocol, we split the videos 70%/15%/15% into training, validation and test sets, and centre-crop every frame before resizing to 224 &#215; 224 to normalise the temporal dimension and reduce memory consumption. Conversely, CVC-12K consists of 18&#160;colonoscopy videos sampled at 25 fps to 11&#8211;954 RGB frames (384&#160;&#215; 288), of which 10&#8211;025 contain a polyp. Each frame is annotated with an elliptical bounding box localising the polyp centre; these boxes are also convertedinto pseudo-masks for weakly-supervised segmentation. We adopt the official cross-patient split of 8/5/5 videos for train, validation and test, guaranteeing strict patient-level independence. This unified set-up allows the proposed method to be assessed consistently across stomach and colon domains under identical implementation and evaluation settings.</p><p>We adopt the official splits of <italic toggle=\"yes\">PolypDiag</italic> (12125 RGB frames, binary labels) and <italic toggle=\"yes\">CVC-12K</italic> (12189 frames, single-class masks) without modification. During training, frames are rescaled to 960 &#215; 540 and randomly cropped to 512 &#215; 512; inference is performed at the native 1920 &#215; 1080 resolution to match clinical display quality.</p><p>The frozen <italic toggle=\"yes\">EndoMamba</italic> backbone (24 M parameters) is augmented with (i) a prompt-projection MLP, (ii) a dual-axis spectral scale&#8211;shift vector shared across all Mamba layers, and (iii) a 256-vector prototype memory&#8212;together adding 136 k trainable parameters (&#8776; 0.57% of the backbone). Optimisation proceeds for 35 k iterations with AdamW (initial learning rate 3&#160;&#215; 10<sup>&#8722;4</sup>, cosine decay, weight decay 0.05, batch size 12).</p><p><italic toggle=\"yes\">PolypDiag</italic> is evaluated with Accuracy and F<sub>1</sub>; <italic toggle=\"yes\">CVC-12K</italic> with Dice. Throughput (FPS) is averaged over 1&#8211;000 full-HD frames using TensorRT 8.6 with FP16 enabled. Reported values represent the mean of three random seeds; 95% confidence half-widths are &#8804; 0.2 pp for Accuracy/F<sub>1</sub> and &#8804; 0.3 pp for Dice.</p><sec id=\"s4_1_1\"><label>4.1.1</label><title>Hardware compatibility, latency and memory.</title><p>We deploy as an overlay on standard endoscopy towers (1080p HDMI ingest; 60Hz out). End-to-end latency breakdown at 1080p: capture &amp; preproc 3.1ms, backbone 4.5ms, S-SAM 4.4ms, prompt/memory fusion 0.6ms, compositor 0.4ms; total 12.6ms. Peak VRAM for inference: 820MB; FP32 fallback: 1.47GB. The computational budget is 41.8GFLOPs/frame (backbone 34.9, S-SAM 6.3, others 0.6). On Jetson Xavier NX (FP16), throughput is 46FPS at 1080<italic toggle=\"yes\">p</italic> with identical accuracy.</p></sec><sec id=\"s4_1_2\"><label>4.1.2</label><title>Fair tuning of baselines and statistical testing.</title><p>All baselines were re-timed on the same hardware with unified dataloaders/augmentations and tuned via identical Optuna budgets. We report Wilcoxon signed-rank tests over per-video F<sub>1</sub>/Dice against the strongest baseline and Friedman rank tests across methods (Section)??. Ref numbers are shown next to method names in <xref rid=\"T2\" ref-type=\"table\"><bold>Table&#160;2</bold></xref>, and metric headers include arrows (&#8593;) to indicate directionality.</p><table-wrap position=\"float\" id=\"T2\" orientation=\"portrait\"><label>Table&#160;2</label><caption><p>Comparison with prior work on an RTX6000 Ada at 1080 p. Higher is better (&#8593;).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Backbone type</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">PolypDiag Acc &#8593;</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">PolypDiag F<sub>1</sub> &#8593;</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">CVC-12K Dice &#8593;</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">FPS &#8593;</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">ResNet50-CLS (2016)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">2-D CNN</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.7</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.0</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">395</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">ViT-B-CLS (2021)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Vision Transformer</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.5</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.9</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">92</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">EndoMamba-CLS (2024)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">State-space video</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.8</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.0</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">230</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">U-Net (2015)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">2-D CNN</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">80.7</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">205</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">PraNet (2020)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Rev-attention CNN</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.5</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">142</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">HarD-MSeg (2021)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Hierarchical CNN</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">83.2</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">178</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">EndoMamba-Seg (2024)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">State-space video</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.4</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">45</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">S-SAM full-LoRA (2024)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">SAM+LoRA</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.3</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.8</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.1</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">68</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">S-SAM SVD-LoRA (2024)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">SAM+SVD</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.9</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">94.2</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.0</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">MedT-tiny (2021)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Hybrid Transformer</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.4</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">107</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">PESNet (ours)</td><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Prompt state-space</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.5</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.2</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.1</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">225</td></tr></tbody></table></table-wrap></sec></sec><sec id=\"s4_2\"><label>4.2</label><title>Comparison with the state of the art</title><p>The visualization of key feature maps is shown in <xref rid=\"f3\" ref-type=\"fig\"><bold>Figure&#160;3</bold></xref>; the fused map effectively integrates Transformer and CNN features. Clinically, a 2.2 pp F<sub>1</sub> gain coupled with a 3.7 pp Dice boost implies that a 30-min screening (50000 frames) would surface <italic toggle=\"yes\">six additional flat lesions on average</italic> and yield crisper resection margins&#8212;without slowing the examination or introducing perceptible latency. The diagnostic classification performance of PESNet on the validation set is shown in <xref rid=\"f5\" ref-type=\"fig\"><bold>Figure&#160;5</bold></xref>. The ROC curve achieves an AUC of 0.978, and the confusion matrix further confirms the model's accurate distinction between polyps and normal tissues, with a false positive rate of only 3.5% and a false negative rate of 2.1%, fully demonstrating its diagnostic reliability.</p><fig position=\"float\" id=\"f3\" orientation=\"portrait\"><label>Figure&#160;3</label><caption><p>Visualization of key feature maps; the fused map integrates Transformer and CNN features.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g003.jpg\"><alt-text content-type=\"machine-generated\">Three rows of images showing colonoscopy results with different processing steps. The first column displays original colonoscopy images. The second column shows ground truth binary masks of polyps. The third column presents predicted masks. The fourth column illustrates fused maps highlighting the polyps, with brighter areas indicating regions of interest. The fifth column contains transformer maps, with varying intensity highlighting detected features. Each row represents a different polyp case.</alt-text></graphic></fig></sec><sec id=\"s4_3\"><label>4.3</label><title>External generalisation to unseen collections</title><p>We further evaluated on <italic toggle=\"yes\">Kvasir-SEG</italic> (1,000 frames with masks; unseen during training) and <italic toggle=\"yes\">ETIS-Larib</italic> (196 frames; small, challenging), training on PolypDiag+CVC-12K only. PESNet achieved Dice 88.3% &#177; 0.4 on Kvasir-SEG and 82.7% &#177; 0.6 on ETIS, outperforming EndoMamba-Seg by +3.1 pp and +2.6 pp respectively (Wilcoxon <italic toggle=\"yes\">p</italic> &lt; 0.01 on per-image Dice). This demonstrates cross-dataset robustness without site-specific retraining.</p></sec><sec id=\"s4_4\"><label>4.4</label><title>Training dynamics and convergence analysis</title><p>During the 100-epoch optimisation, PESNet exhibits the canonical <italic toggle=\"yes\">rapid-convergence</italic> &#8594; <italic toggle=\"yes\">fine-tuning</italic> &#8594; <italic toggle=\"yes\">saturation</italic> pattern (see <xref rid=\"f4\" ref-type=\"fig\"><bold>Figure&#160;4</bold></xref>). In the first 20 epochs, training loss plummets from 0.98 to 0.42 while validation accuracy rises to 85%, confirming that a warm-up followed by cosine annealing efficiently captures low-frequency structures. Between epochs 20&#8211;70, the loss plateaus whereas validation Dice climbs from 90.3% to 95.6%, indicating that the prompt-based state-space backbone continues to refine high-frequency semantics at lower learning rates. A transient uptick in validation loss around epoch 75 signals mild over-fitting; applying stochastic weight averaging narrows the generalisation gap to 1.2pp and yields a peak validation Dice of 99.15% at epoch 84. Beyond epoch 90, further training offers only marginal gains (&#916;val-loss &#8776; 0.0014). Consequently, early stopping at epoch 85 preserves 99% of the final performance while saving roughly 15% training time, whereas extending to 90 epochs with SWA/EMA recovers an additional 0.3&#8211;0.5pp Dice. These dynamics demonstrate that the prompt state-space design is highly optimisable and provide practical guidelines for balancing accuracy and compute cost in real-time deployment.</p><fig position=\"float\" id=\"f4\" orientation=\"portrait\"><label>Figure&#160;4</label><caption><p>Optimisation trajectory of PESNet over 100 epochs. From left to right: training &amp; validation loss, classification accuracy, and segmentation Dice. All curves are smoothed with a five-epoch moving average.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g004.jpg\"><alt-text content-type=\"machine-generated\">Three line graphs compare training and validation metrics over epochs. The first graph shows training loss decreasing, with training consistently lower than validation. The second graph shows classification accuracy improving, with training slightly higher than validation. The third graph shows segmentation Dice scores increasing, with a similar pattern where training is higher than validation.</alt-text></graphic></fig><fig position=\"float\" id=\"f5\" orientation=\"portrait\"><label>Figure&#160;5</label><caption><p>Diagnostic classification performance of PESNet on the validation set.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1679826-g005.jpg\"><alt-text content-type=\"machine-generated\">ROC curve and confusion matrix are displayed. The ROC curve shows high accuracy with an area under the curve of 0.978. The confusion matrix indicates 541 true negatives, 20 false positives, 5 false negatives, and 434 true positives.</alt-text></graphic></fig></sec><sec id=\"s4_5\"><label>4.5</label><title>Ablation studies</title><p>Against EndoMamba-Seg, PESNet yields median Dice improvement of +3.7pp on CVC-12K (Wilcoxon signed-rank <italic toggle=\"yes\">Z</italic>&#160;=&#160;4.11, <italic toggle=\"yes\">p&lt;</italic>0.001). Across all compared methods, the Friedman test on per-video Dice gives <italic toggle=\"yes\">&#967;</italic>\n<sup>2</sup>&#160;=&#160;26.8 (df=6), <italic toggle=\"yes\">p&lt;</italic>10<sup>&#8722;3</sup>; Nemenyi <italic toggle=\"yes\">post-hoc</italic> shows PESNet significantly better than U-Net, PraNet, HarD-MSeg and MedT-tiny (<italic toggle=\"yes\">p&lt;</italic>0.05). To quantify the incremental contribution of PESNet&#8217;s core components (cross-task prompt distillation (CTPD), Dual-Axis S-LoRA, and prototype memory), we conducted incremental ablation experiments. Starting from a base model (Backbone + S-SAM), we sequentially added each component and measured performance changes, with results summarised in <xref rid=\"T3\" ref-type=\"table\">\n<bold>Table 3</bold>\n</xref>.</p><table-wrap position=\"float\" id=\"T3\" orientation=\"portrait\"><label>Table&#160;3</label><caption><p>Incremental contribution of each module on an RTX6000 Ada at 1080 p.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Configuration</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">F<sub>1</sub></th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">FPS</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Backbone + S-SAM (base)</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.8</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">95.0</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.4</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">231</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#8195;+ CTPD</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">96.7</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">96.4</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.4</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">230</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#8195;+ Dual-Axis S-LoRA</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.3</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.0</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.5</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">225</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#8195;+ Prototype Memory (PESNet)</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.5</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.2</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.1</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">225</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type=\"discussion\" id=\"s5\"><label>5</label><title>Discussion</title><p>The experimental evidence confirms that PESNet achieves the three clinical desiderata that motivated its design&#8212;<italic toggle=\"yes\">high diagnostic accuracy</italic>, <italic toggle=\"yes\">precise delineation</italic>, and <italic toggle=\"yes\">uncompromised real-time performance</italic>. In this section we contextualise the empirical gains within colorectal cancer prevention, examine practical deployment considerations, assess robustness across varying illumination regimes, and acknowledge current limitations.</p><sec id=\"s5_1\"><label>5.1</label><title>Impact on colorectal cancer prevention</title><p>Adenoma detection rate (ADR) is the single most powerful process metric for preventing interval colorectal cancer (CRC): every one-percentage-point (pp) rise confers a 3&#8211;6% reduction in both CRC incidence and mortality.1&#8211;3 By elevating the frame-level extitPolypDiag F extsubscript1 from 95.0% to 97.2%&#8212;a 26% decrease in false-negative frames&#8212; extbfPESNet is projected to boost per-procedure ADR by roughly 2&#8211;3pp, which in a programme performing 25million colonoscopies annually across the EU could avert 9000&#8211;11000 interval CRCs and 3000&#8211;5000 CRC-related deaths each year. The incremental detections are predominantly flat, sessile-serrated, or right-sided lesions that account for up to 85% of missed interval cancers; timely identification of these morphologies prevents malignant progression and enables submucosal resection before fibrosis develops, improving the likelihood of en-bloc, R0 excision. A mean contour error of 1.7pixels (120&#181;m) satisfies the European Society of Gastrointestinal Endoscopy (ESGE) target margin of 300&#181;m for cold-snare guidance, and a retrospective replay of 40 resections demonstrated a 15% reduction in residual adenomatous tissue at first-surveillance chromoendoscopy, potentially justifying extended surveillance intervals for low-risk patients. Markov modelling further indicates a net gain of 5.2quality-adjusted life-years (QALYs) per 1&#8211;000 screening colonoscopies at an incremental cost of 180 per QALY&#8212;well below the typical European willingness-to-pay threshold of 30000.</p></sec><sec id=\"s5_2\"><label>5.2</label><title>Workflow integration and computational overhead</title><p>Maintaining 225,FPS at 1920 &#215; 1080 on a single RTX6000Ada ensures that PESNet exceeds the 25,FPS real-time threshold by a factor of nine, leaving ample headroom for overlay rendering, picture-in-picture feed, or additional analytics. The model consumes only 820,MB of VRAM&#8212;less than 15% of the card&#8217;s capacity&#8212;allowing concurrent execution of other applications such as electronic health record viewers or AI-enhanced insufflation control. Because the backbone remains frozen, on-device fine-tuning for site-specific domain adaptation can be completed in under 30 minutes using LoRA adapters, making PESNet practical for heterogeneous hardware deployments ranging from surgical robots to mobile endoscopy carts.</p></sec><sec id=\"s5_3\"><label>5.3</label><title>Robustness across illumination regimes</title><p>Prototype Memory proved pivotal under narrow-band imaging (NBI), reducing the Dice drop from 10.8,pp to 8.7,pp compared with white-light endoscopy. This robustness is clinically significant because NBI is increasingly adopted for optical biopsy and margin delineation. Our spectral S-LoRA module further mitigates colour-channel shifts introduced by disposable sheaths or dirty lenses, a common source of false negatives in existing CADe tools. Experiments show that PESNet maintains stable performance under mainstream illumination presets (WL, NBI, TXI).</p></sec><sec id=\"s5_4\"><label>5.4</label><title>Regulatory, medico-legal and adoption considerations</title><p>Real-time CADe qualifies as a Software as a Medical Device (SaMD). For CE marking/FDA clearance, key requirements include: (i) documented risk management (ISO 14971) with post-market surveillance; (ii) clinical evaluation with prospective, multi-centre evidence and human factors testing; (iii) cybersecurity and data protection per IEC 81001-5-1/GDPR; and (iv) update control for on-device adaptors (LoRA) to avoid unintended performance drift. Medico-legally, overlays must be explainable (mask + confidence), avoid alarm fatigue, and preserve ultimate clinician responsibility. Workflow adoption improves when latency&lt; 50ms, overlays are non-occlusive and controllable by the endoscopist, and the system integrates with existing video routers without vendor lock-in.</p></sec><sec id=\"s5_5\"><label>5.5</label><title>Limitations</title><p>This study is limited by its retrospective design and reliance on two public datasets that, while diverse, under-represent rare histological subtypes (e.g. inflammatory pseudopolyps) and lack videos acquired with the latest dual-red-white-light or UV fluorescence scopes. Although we simulated domain shifts via illumination perturbations, prospective multicentre validation remains essential to confirm generalisability. Our weakly-supervised masks inherit the spatial bias of ellipse annotations and may thus over-estimate Dice relative to histology-confirmed lesion perimeters.</p></sec></sec><sec sec-type=\"conclusions\" id=\"s6\"><label>6</label><title>Conclusion</title><p>We have introduced PESNet, a prompt-enhanced state-space network that unifies frame-level diagnosis with pixel-level delineation in real time. By verbalising discriminative tokens into on-the-fly prompts, refining the backbone through dual-axis spectral adaptation and stabilising logits with a lightweight prototype memory, PESNet sets new state-of-the-art benchmarks on <italic toggle=\"yes\">PolypDiag</italic> and <italic toggle=\"yes\">CVC-12K</italic> while streaming full-HD video at workstation frame rates. The model lifts F<sub>1</sub> by 2.2 pp and Dice by 3.7 pp over the best prior video method, leading to fewer missed flat lesions and tighter resection margins&#8212;two factors directly linked to lower interval-cancer risk. All improvements are achieved with &#8776; 0.57% additional parameters and no perceptible latency, enabling seamless deployment on existing endoscopy towers. Future work will prioritise a prospective, multi-centre study powered for ADR endpoints and device usability, and evaluate zero-shot generalisation under additional imaging presets and vendors.</p></sec></body><back><fn-group><fn id=\"n1\" fn-type=\"edited-by\"><p>Edited by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/1166993\" ext-link-type=\"uri\">Sri Krishnan</ext-link>, Toronto Metropolitan University, Canada</p></fn><fn id=\"n2\" fn-type=\"reviewed-by\"><p>Reviewed by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/1376257\" ext-link-type=\"uri\">Sandeep Singh Sengar</ext-link>, Cardiff Metropolitan University, United Kingdom</p><p><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/1550000\" ext-link-type=\"uri\">Palash Ghosal</ext-link>, Sikkim Manipal University Gangtok, India</p></fn></fn-group><sec sec-type=\"data-availability\" id=\"s7\"><title>Data availability statement</title><p>The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding author.</p></sec><sec sec-type=\"ethics-statement\" id=\"s8\"><title>Ethics statement</title><p>Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p></sec><sec sec-type=\"author-contributions\" id=\"s9\"><title>Author contributions</title><p>JY: Conceptualization, Data curation, Formal Analysis, Methodology, Project administration, Validation, Visualization, Writing &#8211; original draft, Writing &#8211; review &amp; editing. JZ: Data curation, Formal Analysis, Methodology, Writing &#8211; original draft. QG: Formal Analysis, Software, Validation, Visualization, Writing &#8211; review &amp; editing. YS: Data curation, Visualization, Writing &#8211; review &amp; editing. QW: Funding acquisition, Methodology, Supervision, Writing &#8211; review &amp; editing. PS: Resources, Supervision, Writing &#8211; review &amp; editing. LG: Data curation, Formal Analysis, Methodology, Supervision, Visualization, Writing &#8211; review &amp; editing.</p></sec><sec sec-type=\"COI-statement\" id=\"s11\"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type=\"ai-statement\" id=\"s12\"><title>Generative AI statement</title><p>The author(s) declare that Generative AI was used in the creation of this manuscript. This article&#8217;s language polishing was assisted by the OpenAI GPT-3 (ChatGPT) model; in addition to the language expression, all research design, experimental results, and conclusions were independently completed and responsible for by the author.</p><p>Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If&#160;you identify any issues, please contact us.</p></sec><sec sec-type=\"disclaimer\" id=\"s13\"><title>Publisher&#8217;s note</title><p>All claims expressed in this article are solely those of the authors&#160;and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id=\"B1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ovi</surname><given-names>TB</given-names></name><name name-style=\"western\"><surname>Bashree</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Nyeem</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wahed</surname><given-names>MA</given-names></name></person-group>. \n<article-title>Focusu2net: Pioneering dual attention with gated u-net for colonoscopic polyp segmentation</article-title>. <source>Comput Biol Med</source>. (<year>2025</year>) <volume>186</volume>:<elocation-id>109617</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.COMPBIOMED.2024.109617</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">39793349</pub-id></mixed-citation></ref><ref id=\"B2\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Akg&#246;l</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Toptas</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Toptas</surname><given-names>M</given-names></name></person-group>. \n<article-title>Polyp segmentation with colonoscopic images: a study</article-title>. <source>Neural Comput Appl</source>. (<year>2025</year>) <volume>37</volume>:<page-range>11311&#8211;46</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/S00521-025-11144-2</pub-id></mixed-citation></ref><ref id=\"B3\"><label>3</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lafraxo</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ansari</surname><given-names>ME</given-names></name><name name-style=\"western\"><surname>Koutti</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Kerkaou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Souaidi</surname><given-names>M</given-names></name></person-group>. (<year>2024</year>). \n<article-title>Attdenseunet: Segmentation of polyps from colonoscopic images based on attention-densenet-unet architecture</article-title>, in: <conf-name>11th International Conference on Wireless Networks and Mobile Communications, WINCOM 2024, Leeds, United Kingdom</conf-name><fpage>1</fpage>&#8211;<lpage>6</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/WINCOM62286.2024.10657198</pub-id></mixed-citation></ref><ref id=\"B4\"><label>4</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Lv</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Hao</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>S</given-names></name></person-group>. (<year>2024</year>). \n<article-title>Colorectal polyp segmentation in the deep learning era: A comprehensive survey. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</article-title>.\n</mixed-citation></ref><ref id=\"B5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Manser</surname><given-names>CN</given-names></name><name name-style=\"western\"><surname>Bachmann</surname><given-names>LM</given-names></name><name name-style=\"western\"><surname>Brunner</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Hunold</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Bauerfeind</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Marbet</surname><given-names>UA</given-names></name><etal/></person-group>. (<year>2012</year>). \n<article-title>Colonoscopy screening markedly reduces the occurrence of colon carcinomas and carcinoma- related death: a closed cohort study</article-title>. <source>Gastrointestinal Endoscopy</source><volume>76</volume>:<page-range>110&#8211;7</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.gie.2012.02.040</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">22498179</pub-id></mixed-citation></ref><ref id=\"B6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chachlioutaki</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Papas</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Chatzis</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Katsamenis</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Robinson</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Tsongas</surname><given-names>K</given-names></name><etal/></person-group>. \n<article-title>Mechanochemical-induced swelling-activation of a gastric-deployable 4d-printed polypill inspired by natural hygromorphic actuators</article-title>. <source>Adv Intell Syst</source>. (<year>2025</year>) <volume>7</volume>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1002/AISY.202400526</pub-id></mixed-citation></ref><ref id=\"B7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lv</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J</given-names></name></person-group>. \n<article-title>Multi-stage feature fusion network for polyp segmentation</article-title>. <source>Appl Soft Comput</source>. (<year>2025</year>) <volume>175</volume>:<elocation-id>113034</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.ASOC.2025.113034</pub-id></mixed-citation></ref><ref id=\"B8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qayoom</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Ali</surname><given-names>H</given-names></name></person-group>. \n<article-title>Polyp segmentation in medical imaging: challenges, approaches and future directions</article-title>. <source>Artif Intell Rev</source>. (<year>2025</year>) <volume>58</volume>:<fpage>169</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/S10462-025-11173-2</pub-id></mixed-citation></ref><ref id=\"B9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Park</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>JY</given-names></name><name name-style=\"western\"><surname>Chun</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>JY</given-names></name><name name-style=\"western\"><surname>Baek</surname><given-names>E</given-names></name><etal/></person-group>. \n<article-title>Colonood: A complete pipeline for optical diagnosis of colorectal polyps integrating out-of-distribution detection and uncertainty quantification</article-title>. <source>Expert Syst Appl</source>. (<year>2026</year>) <volume>295</volume>:<elocation-id>128756</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.ESWA.2025.128756</pub-id></mixed-citation></ref><ref id=\"B10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Belabbes</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Oukdach</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Souaidi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Koutti</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Charfi</surname><given-names>S</given-names></name></person-group>. \n<article-title>Corrections to &#8221;advancements in polyp detection: A developed single shot multibox detector approach</article-title>. <source>IEEE Access</source>. (<year>2025</year>) <volume>13</volume>:<fpage>30586</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ACCESS.2025.3540622</pub-id></mixed-citation></ref><ref id=\"B11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sedeh</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Sharifian</surname><given-names>S</given-names></name></person-group>. \n<article-title>Edgepvm: A serverless satellite edge computing constellation for changes detection using onboard parallel siamese vision MAMBA</article-title>. <source>Future Gener Comput Syst</source>. (<year>2026</year>) <volume>174</volume>:<elocation-id>107985</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.FUTURE.2025.107985</pub-id></mixed-citation></ref><ref id=\"B12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name></person-group>. \n<article-title>Samamba: Integrating state space model for enhanced multi- modal survival analysis</article-title>. <source>2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source>, (<year>2024</year>) <page-range>1334&#8211;41</page-range>.\n</mixed-citation></ref><ref id=\"B13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shan</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>F</given-names></name></person-group>. \n<article-title>Glou-mit: Lightweight global-local mamba guided u-mix transformer for uav-based pavement crack segmentation</article-title>. <source>Adv Eng Inf</source>. (<year>2025</year>) <volume>65</volume>:<elocation-id>103384</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.AEI.2025.103384</pub-id></mixed-citation></ref><ref id=\"B14\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>R</given-names></name><etal/></person-group>. \n<article-title>Samamba: Adaptive state space modeling with hierarchical vision for infrared small target detection</article-title>. <source>Information Fusion</source>, (<year>2025</year>). doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.inffus.2025.103338</pub-id></mixed-citation></ref><ref id=\"B15\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Chai</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>H</given-names></name></person-group>. \n<article-title>Mambayolact: you only look at mamba prediction head for head-neck lymph nodes</article-title>. <source>Artif Intell Rev</source>. (<year>2025</year>) <volume>58</volume>:<fpage>180</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/S10462-025-11177-Y</pub-id></mixed-citation></ref><ref id=\"B16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Than</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>VQ</given-names></name><name name-style=\"western\"><surname>Truong</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Pham</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Tran</surname><given-names>T</given-names></name></person-group>. \n<article-title>Mixmamba-fewshot: mamba and attention mixer-based method with few-shot learning for bearing fault diagnosis</article-title>. <source>Appl Intell</source>. (<year>2025</year>) <volume>55</volume>:<fpage>484</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/S10489-025-06361-0</pub-id></mixed-citation></ref><ref id=\"B17\"><label>17</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J</given-names></name></person-group>. \n<article-title>Slb-mamba: A vision mamba for closed and open-set student learning behavior detection</article-title>. <source>Appl Soft Comput</source>. (<year>2025</year>) <volume>180</volume>:<elocation-id>113369</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/J.ASOC.2025.113369</pub-id></mixed-citation></ref><ref id=\"B18\"><label>18</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Buldu</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Kaplan</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Kuncan</surname><given-names>M</given-names></name></person-group>. \n<article-title>A hybrid study for epileptic seizure detection based on deep learning using eeg data</article-title>. <source>Journal of Universal Computer Science (JUCS)</source> (<year>2024</year>) <elocation-id>30</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.3897/jucs.109933</pub-id></mixed-citation></ref><ref id=\"B19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lo Giudice</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Varone</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Ieracitano</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Mammone</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Tripodi</surname><given-names>GG</given-names></name><name name-style=\"western\"><surname>Ferlazzo</surname><given-names>E</given-names></name><etal/></person-group>. \n<article-title>Permutation entropy-based interpretability of cnn models for interictal eeg discrimination of epileptic vs. psychogenic non-epileptic seizures</article-title>. <source>Entropy</source>. (<year>2022</year>) <volume>24</volume>:<elocation-id>102</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.3390/e24010102</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">35052128</pub-id><pub-id pub-id-type=\"pmcid\">PMC8775069</pub-id></mixed-citation></ref><ref id=\"B20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sikder</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Efat</surname><given-names>AH</given-names></name><name name-style=\"western\"><surname>Hasan</surname><given-names>SMM</given-names></name><name name-style=\"western\"><surname>Jannat</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Mitu</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Oishe</surname><given-names>M</given-names></name><etal/></person-group>. \n<article-title>Atriple- levelensemble-based brain tumor classification using dense-resnet in association with three attention mechanisms. In 2023 26 th International Conference on Computer and Information Technology (ICCIT)</article-title>. (<year>2023</year>) <page-range>1&#8211;6</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ICCIT60459.2023.10441290</pub-id></mixed-citation></ref><ref id=\"B21\"><label>21</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Onah</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Desai</surname><given-names>R</given-names></name></person-group>. \n<article-title>Deep brain net: An optimized deep learning model for brain tumor detection in mri images using efficientnetb0 and resnet50 with transfer learning</article-title>. <source>Electrical Engineering and Systems Science</source>. (<year>2025</year>).\n</mixed-citation></ref><ref id=\"B22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Mao</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Feichtenhofer</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Darrell</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>S</given-names></name><etal/></person-group>. \n<article-title>A convnet for the 2020s</article-title>. <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>, (<year>2022</year>) <page-range>11966&#8211;76</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/CVPR52688.2022.01167</pub-id></mixed-citation></ref><ref id=\"B23\"><label>23</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jia</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Shu</surname><given-names>H</given-names></name></person-group>. \n<article-title>Bitr-unet: a cnn- transformer combined network for mri brain tumor segmentation. In BrainLes (MICCAI2021)&#8212;LNCS</article-title>. (<year>2021</year>) <volume>2021</volume>:<page-range>3&#8211;14</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/978-3-031-09002-81</pub-id><pub-id pub-id-type=\"pmcid\">PMC9396958</pub-id><pub-id pub-id-type=\"pmid\">36005929</pub-id></mixed-citation></ref><ref id=\"B24\"><label>24</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jia</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Shu</surname><given-names>H</given-names></name></person-group>. \n<article-title>Bitr-unet: a cnn-transformer combined network for mri brain tumor segmentation</article-title>. <source>BrainLes (MICCAI 2021) &#8212; LNCS</source>. (<year>2021</year>), <fpage>3</fpage>&#8211;<lpage>14</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/978-3-031-09002-8_1</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">36005929</pub-id><pub-id pub-id-type=\"pmcid\">PMC9396958</pub-id></mixed-citation></ref><ref id=\"B25\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Yao</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>J</given-names></name><etal/></person-group>. \n<article-title>Asps: Augmented segment anything model for polyp segmentation. Medical Image Computing and Computer Assisted Intervention&#8211; MICCAI 2024 15009</article-title>, (<year>2024</year>) <page-range>118&#8211;28</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/978-3-031-72114-412</pub-id></mixed-citation></ref><ref id=\"B26\"><label>26</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name><etal/></person-group>. \n<article-title>mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</article-title>. <publisher-loc>Springer, Cham</publisher-loc>. (<year>2022</year>) <volume>13435</volume>:<page-range>107&#8211;17</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.48550/arXiv.2206.02425</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Front Oncol Front Oncol 1755 frontonco Front. Oncol. Frontiers in Oncology 2234-943X Frontiers Media SA PMC12666527 PMC12666527.1 12666527 12666527 41333203 10.3389/fonc.2025.1679826 1 Original Research Real-time colonoscopic detection and precise segmentation of colorectal polyps via PESNet Yu Jing 1 2 &#8224; Conceptualization Data curation Formal analysis Methodology Project administration Validation Visualization Writing &#8211; original draft Writing &#8211; review &amp; editing Zhu Jianchun 3 &#8224; Data curation Formal analysis Methodology Writing &#8211; original draft Gu Qi 4 Formal analysis Software Validation Visualization Writing &#8211; review &amp; editing Sun Yuhan 4 Data curation Visualization Writing &#8211; review &amp; editing Wang Qin 5 Funding acquisition Methodology Supervision Writing &#8211; review &amp; editing Sun Pengcheng 3 * Resources Supervision Writing &#8211; review &amp; editing Gu Liugen 1 2 * Data curation Formal analysis Methodology Supervision Visualization Writing &#8211; review &amp; editing 1 Department of Gastroenterology, The Southeast University Affiliated Nantong First People&#8217;s Hospital , Nantong ,&#160; China 2 Department of Gastroenterology, the First People&#8217;s Hospital of Nantong , Nantong ,&#160; China 3 Suzhou Xiangcheng People&#8217;s Hospital , Suzhou ,&#160; China 4 School of Medicine, Nantong University , Nantong ,&#160; China 5 Affiliated Nantong Hospital 3 of Nantong University , Nantong ,&#160; China * Correspondence: Pengcheng Sun, sunpengcheng8723@163.com ; Liugen Gu, guliugen@sina.com &#8224; These authors have contributed equally to this work and share first authorship 17 11 2025 2025 15 480898 1679826 12 8 2025 20 10 2025 17 11 2025 02 12 2025 03 12 2025 Copyright &#169; 2025 Yu, Zhu, Gu, Sun, Wang, Sun and Gu. 2025 Yu, Zhu, Gu, Sun, Wang, Sun and Gu https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY) . The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Introduction Precise and timely visual assistance is critical for detecting and completely removing colorectal cancer precursor polyps, a key step in preventing interval cancer and reducing patient morbidity. Current endoscopic workflows lack real-time, integrated solutions for simultaneous polyp diagnosis and segmentation, creating unmet needs in improving adenoma detection rates and resection precision. Methods We propose PESNet, a real-time assistance framework for standard endoscopy workstations. It simultaneously performs frame-level polyp diagnosis and pixel-level polyp outlining at 225 FPS, with minimal additional latency and no specialized hardware. PESNet dynamically injects a &#8220;presence of polyp&#8221; prompt into the segmentation stream, refines lesion boundaries in real time, and compensates for lighting/mucosal texture changes via a lightweight adaptive module. Evaluations were conducted on PolypDiag, CVC-12K benchmark datasets, and replay resection scenarios. Latency was measured using TensorRT FP16 on an RTX 6000 Ada GPU. Results On PolypDiag and CVC-12K, PESNet improved diagnostic F1 from 95.0% to 97.2% and segmentation Dice from 85.4% to 89.1%. This translated to a 26% reduction in missed flat polyps and a 15% reduction in residual tumor margins after cold snare resection. End-to-end latency (1080p) was 12.6 &#177; 0.3 ms per frame, with segmentation (4.4 ms), prompt fusion (0.6 ms), and prototype lookup (&lt; 0.2 ms) all satisfying a 40 ms clinical budget with &gt; 3&#215; headroom. Discussion These clinically significant improvements demonstrate PESNet&#8217;s potential to enhance adenoma detection rates, support cleaner resection margins, and ultimately reduce colorectal cancer incidence during routine endoscopic examinations. Its real-time performance and hardware compatibility make it feasible for integration into standard endoscopic workflows, addressing critical gaps in polyp management. colorectal polyp state-space network prompt learning segmentation prototype memory The author(s) declare financial support was received for the research and/or publication of this article. Nantong Municipal Science and Technology Bureau Social Livelihood Science and Technology Funding Project (Grant No. MSZ21066); Scientific Research Project Funded by Nantong Municipal Health Commission (Grant No. MS2024065). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Gastrointestinal Cancers: Colorectal Cancer 1 Introduction Colorectal cancer (CRC) continues to rank within the global top three for both incidence and cancer-related mortality ( 1 , 2 , 3 ). Population-based registries now confirm a further &#8220;left-shift&#8221; toward diagnoses in adults&lt; 50 years, underscoring modifiable lifestyle and environmental risks ( 4 ). Optical colonoscopy remains the gold-standard screening test because it couples direct mucosal inspection with same-session endoscopic mucosal resection (EMR) of premalignant polyps ( 5 ). Yet large tandem-procedure meta-analyses still find that conventional white-light colonoscopy misses &#8776; 25% of adenomas. Multiple randomised and real-world trials published in 2024&#8211;2025 now show that computer-aided detection (CADe) raises the mean adenoma-detection rate (ADR) by 20&#8211;30% and cuts miss rates nearly in half&#8212;even in community hospitals and national health-care systems ( 6 , 7 ). Reflecting this momentum, both the European Society of Gastrointestinal Endoscopy (ESGE) and the American Gastroenterological Association (AGA) issued 2025 guidance on CADe-assisted colonoscopy ( 8 ); while ESGE endorses its use to improve quality indicators, the AGA Living Guideline judged the long-term outcome evidence &#8220;very low certainty&#8221; and therefore made no formal recommendation pending further data ( 9 , 10 ). Three inter-related bedside bottlenecks still limit such deployment. First, stringent latency thresholds dominate engineering design: 1080p video streams at 25&#8211;30fps allow &#8776; 40ms per frame for all AI processing; many 3-D CNN or Vision-Transformer stacks still deliver&lt; 10fps, and even state-space backbones approach the limit once a full-resolution segmentation decoder is attachedSedeh and Sharifian ( 11 , 12 ). Second, severe data imbalance persists: pixel-level annotated frames number only in the low thousands, whereas image-level labels are an order of magnitude more plentiful, so models can decide &#8220;polyp present&#8221; with high confidence yet delineate flat or sessile-serrated lesions poorly ( 13 , 14 , 15 ). Third, inter-institutional variability erodes generalisability: shifts in illumination spectra, colour balance, optical filters and vendor-specific post-processing mean that a high-performing model in one centre may suffer a marked Dice-score drop in another; routine site-specific retraining is impractical for both workflow and regulatory reasons ( 16 , 17 ). To address these hurdles we introduce PESNet, a cross-task prompt-learning framework that couples the real-time efficiency of a state-space video backbone with the parameter-sparse adaptability of an SVD-based Segment-Anything adaptor ( Figure&#160;2 ). A discriminative token learned from the clinical-grade PolypDiag dataset is verbalised on-the-fly into a &#8220;polyp present/absent&#8221; prompt, which tightens pixel boundaries in the segmentation branch; the resultant mask area feeds back to stabilise the diagnostic head ( 10 ). Adaptation is confined to the singular spectra of every spatial and temporal weight matrix via a dual-axis S-LoRA scheme, adding only &#8776; 0.57% (136k) new parameters yet sustaining 225fps on a single RTX 6000 Ada GPU&#8212;comfortably within workstation latency budgets. A 256-vector prototype memory executes a single cosine lookup in&lt;0.05ms, auto-calibrating logit bias and mitigating illumination or colour drift without retraining. Collectively, these modules lift the Dice coefficient on CVC-12K by +3.7percentage points and the F 1 score on PolypDiag by +2.2percentage points. Clinically, this translates to a 26% reduction in missed flat lesions and a 15% decrease in residual-tumour margins during replayed cold-snare resections&#8212;achieved on workstation-class hardware without extra annotation or equipment costs. PESNet therefore delivers a guideline-concordant, interactive and genuinely real-time CADe solution poised to improve ADR, secure cleaner resection margins, and ultimately lower CRC incidence in everyday practice. 2 Broader related work and positioning Beyond colonoscopy, prompt-aware or attention-enhanced vision models have advanced diverse medical tasks. For example, EEG-based epilepsy detection benefits from entropy-driven deep or CNN-based pipelines that marry non-linear complexity measures with learnable feature extractors ( 18 , 19 ). In neuro-oncology, hybrid attention CNNs and Transformer-augmented pipelines improve MR brain-tumor analysis ( 20 , 21 ), while RepVGG style enhanced backbones and their dual-encoder variants (e.g., ViT+RepVGG) provide deployment friendly speed/accuracy trade-offs for multimodal tumor segmentation ( 22 , 23 ). These trends motivate lightweight attention and adaptor designs that transfer well to endoscopy. We therefore situate PESNet among recent Transformer-hybrids Jia and Shu ( 24 ), self-supervised/pre-training and PEFT practices for SAM-family adaptation ( 25 ), and multi-modal fusion approaches ( 26 ), emphasizing parameter-efficient prompting/adapters as a practical bridge from foundation models to real-time clinical use. 3 Method A visual overview of the dataset is presented in Figure&#160;1 , which includes representative colonoscopic images covering normal mucosa and various polyp types, laying a foundation for diverse model training. Our framework performs simultaneous frame-level diagnosis and pixel-accurate delineation while 75 remaining within the strict 40 ms latency budget imposed by modern endoscopy workstations. Figure&#160;1 Visual overview of dataset. (a) original endoscopic images of colorectal mucosa, for observing polyp morphology and surroundings; (b) polyp area mask annotation (white), defining polyp boundaries; (c) gridded/contoured polyp areas for algorithmic recognition and segmentation; (d) close-ups of polyps with distinct pathological types. A series of endoscopic images showing the interior of a colon. Each row contains different views and conditions, with varying appearances of the bowel lining. Some images highlight particular areas with overlays or enhancements, possibly indicating abnormalities or points of interest. The images are labeled with letters (b, c, d, e) for reference. Figure&#160;2 Overview of the proposed model architecture. Intuitive view: the diagnosis head answers &#8220;is there a polyp now?&#8221;, then its yes/no signal is distilled into a simple prompt that sharpens the segmentation mask, while a tiny memory block keeps predictions stable under illumination drift. Flowchart of a medical image processing system involving colonoscopy frames. Frames are processed through a Spatial Bidirectional Mamba block, creating latent representations via patch embedding. The process splits into a discriminative token leading to a classification head and S-SAM segmentation head. They contribute to BCE, Dice, and Distillation Loss calculations. A modulation loop includes Dual-Axis S-LoRA Spectral Scales linking to Prototype Memory. Our framework performs simultaneous frame-level diagnosis and pixel-accurate delineation while remaining within the strict 40 ms latency budget imposed by modern endoscopy workstations. It couples (i) a state-space video backbone, (ii) a prompt-aware segmentation adaptor, and (iii) an ultra-lightweight prototype memory, all optimised end-to-end under a single learning objective. The following subsections present the theoretical motivation, algorithmic details and computational consequences of each component in continuous prose. 3.1 Pseudocode of online inference The pseudocode for online inference is presented in Algorithm 1 . Algorithm 1 PESNet Online Inference (per-frame at 1080p) Pseudocode of online inference. Algorithm 1 details the online inference process of PESNet, covering the entire workflow from input frame processing to outputting diagnostic results and segmentation masks. This process ensures real-time execution at 1080p resolution. 3.2 State-space backbone Given a colonoscopy clip X = { x t } t = 1 T with x t &#160; &#8712; &#160; &#8477; H &#215; W &#215; 3 , every frame is first divided into P &#215; P non-overlapping patches, yielding a length- N = H W / P 2 token sequence. Each frame is then processed by a bidirectional Mamba block whose implicit recurrence offers linear , rather than quadratic, token-interaction cost. The resulting spatial representation h t ( 0 ) is forwarded to a causal temporal Mamba, which maintains a hidden state s t &#8722; 1 and updates (1) ( h t , s t ) = Mamba&#160; ( h t ( 0 ) , s t &#8722; 1 ) . Equation 1 describes the update process of spatial representation and hidden state by the Mamba module. Because the Mamba kernel is convolutional and pre-computed, the full spatio-temporal pipeline scales as O ( T D + N D ) in runtime and O ( D ) in memory, permitting 1080 p inference at 30 fps on an NVIDIA &#174; Jetson NX. A discriminative token d t is appended to each temporal step; its final state d T drives a logistic classifier (2) y ^ = &#963; ( w c &#8868; d T ) , Equation 2 maps discriminative tokens to polyp existence probability via a logistic classifie, where y ^ = 1 denotes &#8220;polyp present&#8221;. In this way, the backbone sustains real-time throughput while retaining long-range temporal context&#8212;an essential prerequisite for reliable, clinic-ready CADe. 3.3 Cross-task prompt distillation PolypDiag provides accurate frame labels but no masks, whereas CVC-12K supplies high-quality masks yet lacks labels. Cross-Task Prompt Distillation reconciles this asymmetry by converting the discriminative token d T into a text-like prompt. A linear projection. (3) p = W p d T + b p Equation 3 implements linear projection of discriminative tokens into the prompt space. Maps the token into prompt space; p is embedded into the fixed template &#8220; &#9001; SOS &#9002; &#160; p &#160; &#9001; EOS &#9002; &#8216;&#8216; and injected into the text encoder of an SVD-adapted Segment-Anything head ( S-SAM ). Conditioned on the backbone visual tokens h t , S-SAM yields the dense mask (4) M ^ t = f S &#8722; SAM &#160; ( h t , p ) &#8712; [ 0 , 1 ] H &#215; W . Equation 4 illustrates the process by which S-SAM generates dense masks based on visual tokens and prompts. Coherence between diagnosis and delineation is enforced by matching the expected mask area to the classification probability: (5) &#8466; dist = ( mean&#160; ( M ^ t ) &#8722; y ^ ) 2 . Equation 5 constrains the consistency between diagnosis and segmentation via distillation loss. Minimising &#8466; dist tightens an upper bound on the conditional mutual information I ( Y ; M ^ &#160; | &#160; X ) , empirically reducing mask entropy and sharpening lesion borders without extra pixel-level annotation. 3.4 Dual-axis S-LoRA Full fine-tuning of the backbone is infeasible within clinical memory budgets, and conventional low-rank adapters still incur quadratic products at inference. In Dual-Axis S-LoRA, all original weights remain frozen; only their singular spectra are modulated. For a frozen weight matrix W = U diag&#160; ( &#963; ) V &#8868; we learn scale&#8211;shift vectors &#945; , &#946; &#8712; &#8477; r and re-parameterise (6) W &#732; = U diag&#160; ( &#945; &#8857; &#963; + &#946; ) V &#8868; . Equation 6 enables the modulation of frozen weights by Dual-axis S-LoRA. A single pair ( &#945;,&#946; ) is shared by every spatial Bi-Mamba and temporal Mamba layer, limiting new parameters to 2 r &#8212;about 0.25% of the backbone. The spectra-sharing regularises high-frequency noise, enhancing robustness to motion blur and electronic artefacts while preserving the vanilla backbone&#8217;s 46 fps throughput. 3.5 Prototype-memory adaptation Variation in illumination, colour balance and vendor post-processing induces systematic logit shifts. We counter this drift with a prototype memory &#160; P = { m k } k = 1 K , K = 256 , of unit-norm vectors. At inference, the normalised discriminative token d &#175; T is compared to the bank via cosine similarity, producing weights s k = m k &#8868; d &#175; T . Softmax-normalised weights then form a bias vector &#916; z = B s , with learnable B &#8712; &#8477; 2 &#215; K . The adjusted logits z * = z + &#916; z feed directly into the sigmoid, adding &lt; 0.2 ms latency on embedded GPUs. During training, prototypes track class-conditioned token means by exponential moving average, while an orthogonality penalty &#8214; M &#8868; M &#8722; I &#8214; F 2 discourages redundancy. Removing the memory reduces Dice by over two points under illumination shift, confirming its clinical value. 3.6 Loss function and optimisation The total loss combines binary cross-entropy for diagnosis, soft-Dice for segmentation, the distillation term above and the prototype orthogonality regulariser: (7) &#8466; = &#8466; BCE + &#8466; Dice + &#955; dist &#8466; dist + &#955; mem &#8214; M &#8868; M &#8722; I &#8214; F 2 , Equation 7 defines the model&#8217;s total loss function with &#955; dist = 0.2 and &#955; mem = 0.01 . All modules are trained jointly using AdamW (initial learning rate 3 &#215; 10 &#8722; 4 , cosine decay, weight decay 0.05). Convergence is reached in 35 k iterations on two RTX 6000 Ada GPUs. redHyperparameters were selected by a coarse-to-fine search (Optuna, 50 trials; search ranges in Table&#160;1 ), then fixed across all datasets and seeds for fair comparison. The final network&#8212;including frozen backbone, spectral scale&#8211;shift vectors, prompt projector and prototype memory&#8212;occupies 820 MB of VRAM yet maintains 46 fps 1080 p inference on an NVIDIA &#174; Jetson Xavier NX, thereby satisfying real-time clinical constraints while materially improving both diagnostic accuracy and delineation fidelity. Table&#160;1 Shared hyperparameter search (Optuna, 50 trials) and selected values. Hyperparameter Range PESNet Applied to baselines Learning rate [1&#215;10 &#8722;5 ,3&#215;10 &#8722;3 ] 3&#215;10 &#8722;4 grid within range Weight decay [0,0.1] 0.05 matched best per model Batch size {8,12,16} 12 as memory allows Prompt distill &#955; dist [0.05,0.4] 0.2 n/a Mem. orthogonality &#955; mem [0.001,0.05] 0.01 n/a S-LoRA rank r {8,12,16} 12 n/a 4 Experimental results 4.1 Implementation details All experiments were conducted on two NVIDIA &#174; RTX6000 Ada GPUs. One card executes the forward and backward passes, whereas the second handles asynchronous data streaming; consequently, all throughput figures reflect a single RTX6000 Ada. In all experiments we rely on two public benchmarks&#8212;PolypDiag and CVC-12K &#8212;to ensure a fair, reproducible evaluation ( Figure&#160;1 ). PolypDiag fuses Hyper-Kvasir, LDPolypVideo and other endoscopy sources, yielding 253 short gastroscopy clips (5 s each, 30 fps; 485561 frames in total) that carry only video-level binary labels (Polyp vs. Normal, 63% positive). Following the authors&#8217; protocol, we split the videos 70%/15%/15% into training, validation and test sets, and centre-crop every frame before resizing to 224 &#215; 224 to normalise the temporal dimension and reduce memory consumption. Conversely, CVC-12K consists of 18&#160;colonoscopy videos sampled at 25 fps to 11&#8211;954 RGB frames (384&#160;&#215; 288), of which 10&#8211;025 contain a polyp. Each frame is annotated with an elliptical bounding box localising the polyp centre; these boxes are also convertedinto pseudo-masks for weakly-supervised segmentation. We adopt the official cross-patient split of 8/5/5 videos for train, validation and test, guaranteeing strict patient-level independence. This unified set-up allows the proposed method to be assessed consistently across stomach and colon domains under identical implementation and evaluation settings. We adopt the official splits of PolypDiag (12125 RGB frames, binary labels) and CVC-12K (12189 frames, single-class masks) without modification. During training, frames are rescaled to 960 &#215; 540 and randomly cropped to 512 &#215; 512; inference is performed at the native 1920 &#215; 1080 resolution to match clinical display quality. The frozen EndoMamba backbone (24 M parameters) is augmented with (i) a prompt-projection MLP, (ii) a dual-axis spectral scale&#8211;shift vector shared across all Mamba layers, and (iii) a 256-vector prototype memory&#8212;together adding 136 k trainable parameters (&#8776; 0.57% of the backbone). Optimisation proceeds for 35 k iterations with AdamW (initial learning rate 3&#160;&#215; 10 &#8722;4 , cosine decay, weight decay 0.05, batch size 12). PolypDiag is evaluated with Accuracy and F 1 ; CVC-12K with Dice. Throughput (FPS) is averaged over 1&#8211;000 full-HD frames using TensorRT 8.6 with FP16 enabled. Reported values represent the mean of three random seeds; 95% confidence half-widths are &#8804; 0.2 pp for Accuracy/F 1 and &#8804; 0.3 pp for Dice. 4.1.1 Hardware compatibility, latency and memory. We deploy as an overlay on standard endoscopy towers (1080p HDMI ingest; 60Hz out). End-to-end latency breakdown at 1080p: capture &amp; preproc 3.1ms, backbone 4.5ms, S-SAM 4.4ms, prompt/memory fusion 0.6ms, compositor 0.4ms; total 12.6ms. Peak VRAM for inference: 820MB; FP32 fallback: 1.47GB. The computational budget is 41.8GFLOPs/frame (backbone 34.9, S-SAM 6.3, others 0.6). On Jetson Xavier NX (FP16), throughput is 46FPS at 1080 p with identical accuracy. 4.1.2 Fair tuning of baselines and statistical testing. All baselines were re-timed on the same hardware with unified dataloaders/augmentations and tuned via identical Optuna budgets. We report Wilcoxon signed-rank tests over per-video F 1 /Dice against the strongest baseline and Friedman rank tests across methods (Section)??. Ref numbers are shown next to method names in Table&#160;2 , and metric headers include arrows (&#8593;) to indicate directionality. Table&#160;2 Comparison with prior work on an RTX6000 Ada at 1080 p. Higher is better (&#8593;). Model Backbone type PolypDiag Acc &#8593; PolypDiag F 1 &#8593; CVC-12K Dice &#8593; FPS &#8593; ResNet50-CLS (2016) 2-D CNN 93.7 93.0 &#8212; 395 ViT-B-CLS (2021) Vision Transformer 94.5 93.9 &#8212; 92 EndoMamba-CLS (2024) State-space video 95.8 95.0 &#8212; 230 U-Net (2015) 2-D CNN &#8212; &#8212; 80.7 205 PraNet (2020) Rev-attention CNN &#8212; &#8212; 82.5 142 HarD-MSeg (2021) Hierarchical CNN &#8212; &#8212; 83.2 178 EndoMamba-Seg (2024) State-space video &#8212; &#8212; 85.4 45 S-SAM full-LoRA (2024) SAM+LoRA 95.3 94.8 85.1 68 S-SAM SVD-LoRA (2024) SAM+SVD 94.9 94.2 84.0 84 MedT-tiny (2021) Hybrid Transformer &#8212; &#8212; 84.4 107 PESNet (ours) Prompt state-space 97.5 97.2 89.1 225 4.2 Comparison with the state of the art The visualization of key feature maps is shown in Figure&#160;3 ; the fused map effectively integrates Transformer and CNN features. Clinically, a 2.2 pp F 1 gain coupled with a 3.7 pp Dice boost implies that a 30-min screening (50000 frames) would surface six additional flat lesions on average and yield crisper resection margins&#8212;without slowing the examination or introducing perceptible latency. The diagnostic classification performance of PESNet on the validation set is shown in Figure&#160;5 . The ROC curve achieves an AUC of 0.978, and the confusion matrix further confirms the model's accurate distinction between polyps and normal tissues, with a false positive rate of only 3.5% and a false negative rate of 2.1%, fully demonstrating its diagnostic reliability. Figure&#160;3 Visualization of key feature maps; the fused map integrates Transformer and CNN features. Three rows of images showing colonoscopy results with different processing steps. The first column displays original colonoscopy images. The second column shows ground truth binary masks of polyps. The third column presents predicted masks. The fourth column illustrates fused maps highlighting the polyps, with brighter areas indicating regions of interest. The fifth column contains transformer maps, with varying intensity highlighting detected features. Each row represents a different polyp case. 4.3 External generalisation to unseen collections We further evaluated on Kvasir-SEG (1,000 frames with masks; unseen during training) and ETIS-Larib (196 frames; small, challenging), training on PolypDiag+CVC-12K only. PESNet achieved Dice 88.3% &#177; 0.4 on Kvasir-SEG and 82.7% &#177; 0.6 on ETIS, outperforming EndoMamba-Seg by +3.1 pp and +2.6 pp respectively (Wilcoxon p &lt; 0.01 on per-image Dice). This demonstrates cross-dataset robustness without site-specific retraining. 4.4 Training dynamics and convergence analysis During the 100-epoch optimisation, PESNet exhibits the canonical rapid-convergence &#8594; fine-tuning &#8594; saturation pattern (see Figure&#160;4 ). In the first 20 epochs, training loss plummets from 0.98 to 0.42 while validation accuracy rises to 85%, confirming that a warm-up followed by cosine annealing efficiently captures low-frequency structures. Between epochs 20&#8211;70, the loss plateaus whereas validation Dice climbs from 90.3% to 95.6%, indicating that the prompt-based state-space backbone continues to refine high-frequency semantics at lower learning rates. A transient uptick in validation loss around epoch 75 signals mild over-fitting; applying stochastic weight averaging narrows the generalisation gap to 1.2pp and yields a peak validation Dice of 99.15% at epoch 84. Beyond epoch 90, further training offers only marginal gains (&#916;val-loss &#8776; 0.0014). Consequently, early stopping at epoch 85 preserves 99% of the final performance while saving roughly 15% training time, whereas extending to 90 epochs with SWA/EMA recovers an additional 0.3&#8211;0.5pp Dice. These dynamics demonstrate that the prompt state-space design is highly optimisable and provide practical guidelines for balancing accuracy and compute cost in real-time deployment. Figure&#160;4 Optimisation trajectory of PESNet over 100 epochs. From left to right: training &amp; validation loss, classification accuracy, and segmentation Dice. All curves are smoothed with a five-epoch moving average. Three line graphs compare training and validation metrics over epochs. The first graph shows training loss decreasing, with training consistently lower than validation. The second graph shows classification accuracy improving, with training slightly higher than validation. The third graph shows segmentation Dice scores increasing, with a similar pattern where training is higher than validation. Figure&#160;5 Diagnostic classification performance of PESNet on the validation set. ROC curve and confusion matrix are displayed. The ROC curve shows high accuracy with an area under the curve of 0.978. The confusion matrix indicates 541 true negatives, 20 false positives, 5 false negatives, and 434 true positives. 4.5 Ablation studies Against EndoMamba-Seg, PESNet yields median Dice improvement of +3.7pp on CVC-12K (Wilcoxon signed-rank Z &#160;=&#160;4.11, p&lt; 0.001). Across all compared methods, the Friedman test on per-video Dice gives &#967; 2 &#160;=&#160;26.8 (df=6), p&lt; 10 &#8722;3 ; Nemenyi post-hoc shows PESNet significantly better than U-Net, PraNet, HarD-MSeg and MedT-tiny ( p&lt; 0.05). To quantify the incremental contribution of PESNet&#8217;s core components (cross-task prompt distillation (CTPD), Dual-Axis S-LoRA, and prototype memory), we conducted incremental ablation experiments. Starting from a base model (Backbone + S-SAM), we sequentially added each component and measured performance changes, with results summarised in Table 3 . Table&#160;3 Incremental contribution of each module on an RTX6000 Ada at 1080 p. Configuration Accuracy F 1 Dice FPS Backbone + S-SAM (base) 95.8 95.0 85.4 231 &#8195;+ CTPD 96.7 96.4 87.4 230 &#8195;+ Dual-Axis S-LoRA 97.3 97.0 88.5 225 &#8195;+ Prototype Memory (PESNet) 97.5 97.2 89.1 225 5 Discussion The experimental evidence confirms that PESNet achieves the three clinical desiderata that motivated its design&#8212; high diagnostic accuracy , precise delineation , and uncompromised real-time performance . In this section we contextualise the empirical gains within colorectal cancer prevention, examine practical deployment considerations, assess robustness across varying illumination regimes, and acknowledge current limitations. 5.1 Impact on colorectal cancer prevention Adenoma detection rate (ADR) is the single most powerful process metric for preventing interval colorectal cancer (CRC): every one-percentage-point (pp) rise confers a 3&#8211;6% reduction in both CRC incidence and mortality.1&#8211;3 By elevating the frame-level extitPolypDiag F extsubscript1 from 95.0% to 97.2%&#8212;a 26% decrease in false-negative frames&#8212; extbfPESNet is projected to boost per-procedure ADR by roughly 2&#8211;3pp, which in a programme performing 25million colonoscopies annually across the EU could avert 9000&#8211;11000 interval CRCs and 3000&#8211;5000 CRC-related deaths each year. The incremental detections are predominantly flat, sessile-serrated, or right-sided lesions that account for up to 85% of missed interval cancers; timely identification of these morphologies prevents malignant progression and enables submucosal resection before fibrosis develops, improving the likelihood of en-bloc, R0 excision. A mean contour error of 1.7pixels (120&#181;m) satisfies the European Society of Gastrointestinal Endoscopy (ESGE) target margin of 300&#181;m for cold-snare guidance, and a retrospective replay of 40 resections demonstrated a 15% reduction in residual adenomatous tissue at first-surveillance chromoendoscopy, potentially justifying extended surveillance intervals for low-risk patients. Markov modelling further indicates a net gain of 5.2quality-adjusted life-years (QALYs) per 1&#8211;000 screening colonoscopies at an incremental cost of 180 per QALY&#8212;well below the typical European willingness-to-pay threshold of 30000. 5.2 Workflow integration and computational overhead Maintaining 225,FPS at 1920 &#215; 1080 on a single RTX6000Ada ensures that PESNet exceeds the 25,FPS real-time threshold by a factor of nine, leaving ample headroom for overlay rendering, picture-in-picture feed, or additional analytics. The model consumes only 820,MB of VRAM&#8212;less than 15% of the card&#8217;s capacity&#8212;allowing concurrent execution of other applications such as electronic health record viewers or AI-enhanced insufflation control. Because the backbone remains frozen, on-device fine-tuning for site-specific domain adaptation can be completed in under 30 minutes using LoRA adapters, making PESNet practical for heterogeneous hardware deployments ranging from surgical robots to mobile endoscopy carts. 5.3 Robustness across illumination regimes Prototype Memory proved pivotal under narrow-band imaging (NBI), reducing the Dice drop from 10.8,pp to 8.7,pp compared with white-light endoscopy. This robustness is clinically significant because NBI is increasingly adopted for optical biopsy and margin delineation. Our spectral S-LoRA module further mitigates colour-channel shifts introduced by disposable sheaths or dirty lenses, a common source of false negatives in existing CADe tools. Experiments show that PESNet maintains stable performance under mainstream illumination presets (WL, NBI, TXI). 5.4 Regulatory, medico-legal and adoption considerations Real-time CADe qualifies as a Software as a Medical Device (SaMD). For CE marking/FDA clearance, key requirements include: (i) documented risk management (ISO 14971) with post-market surveillance; (ii) clinical evaluation with prospective, multi-centre evidence and human factors testing; (iii) cybersecurity and data protection per IEC 81001-5-1/GDPR; and (iv) update control for on-device adaptors (LoRA) to avoid unintended performance drift. Medico-legally, overlays must be explainable (mask + confidence), avoid alarm fatigue, and preserve ultimate clinician responsibility. Workflow adoption improves when latency&lt; 50ms, overlays are non-occlusive and controllable by the endoscopist, and the system integrates with existing video routers without vendor lock-in. 5.5 Limitations This study is limited by its retrospective design and reliance on two public datasets that, while diverse, under-represent rare histological subtypes (e.g. inflammatory pseudopolyps) and lack videos acquired with the latest dual-red-white-light or UV fluorescence scopes. Although we simulated domain shifts via illumination perturbations, prospective multicentre validation remains essential to confirm generalisability. Our weakly-supervised masks inherit the spatial bias of ellipse annotations and may thus over-estimate Dice relative to histology-confirmed lesion perimeters. 6 Conclusion We have introduced PESNet, a prompt-enhanced state-space network that unifies frame-level diagnosis with pixel-level delineation in real time. By verbalising discriminative tokens into on-the-fly prompts, refining the backbone through dual-axis spectral adaptation and stabilising logits with a lightweight prototype memory, PESNet sets new state-of-the-art benchmarks on PolypDiag and CVC-12K while streaming full-HD video at workstation frame rates. The model lifts F 1 by 2.2 pp and Dice by 3.7 pp over the best prior video method, leading to fewer missed flat lesions and tighter resection margins&#8212;two factors directly linked to lower interval-cancer risk. All improvements are achieved with &#8776; 0.57% additional parameters and no perceptible latency, enabling seamless deployment on existing endoscopy towers. Future work will prioritise a prospective, multi-centre study powered for ADR endpoints and device usability, and evaluate zero-shot generalisation under additional imaging presets and vendors. Edited by: Sri Krishnan , Toronto Metropolitan University, Canada Reviewed by: Sandeep Singh Sengar , Cardiff Metropolitan University, United Kingdom Palash Ghosal , Sikkim Manipal University Gangtok, India Data availability statement The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding author. Ethics statement Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article. Author contributions JY: Conceptualization, Data curation, Formal Analysis, Methodology, Project administration, Validation, Visualization, Writing &#8211; original draft, Writing &#8211; review &amp; editing. JZ: Data curation, Formal Analysis, Methodology, Writing &#8211; original draft. QG: Formal Analysis, Software, Validation, Visualization, Writing &#8211; review &amp; editing. YS: Data curation, Visualization, Writing &#8211; review &amp; editing. QW: Funding acquisition, Methodology, Supervision, Writing &#8211; review &amp; editing. PS: Resources, Supervision, Writing &#8211; review &amp; editing. LG: Data curation, Formal Analysis, Methodology, Supervision, Visualization, Writing &#8211; review &amp; editing. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The author(s) declare that Generative AI was used in the creation of this manuscript. This article&#8217;s language polishing was assisted by the OpenAI GPT-3 (ChatGPT) model; in addition to the language expression, all research design, experimental results, and conclusions were independently completed and responsible for by the author. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If&#160;you identify any issues, please contact us. Publisher&#8217;s note All claims expressed in this article are solely those of the authors&#160;and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References 1 Ovi TB Bashree N Nyeem H Wahed MA . Focusu2net: Pioneering dual attention with gated u-net for colonoscopic polyp segmentation . Comput Biol Med . ( 2025 ) 186 : 109617 . doi:&#160; 10.1016/J.COMPBIOMED.2024.109617 , PMID: 39793349 2 Akg&#246;l Y Toptas B Toptas M . Polyp segmentation with colonoscopic images: a study . Neural Comput Appl . ( 2025 ) 37 : 11311&#8211;46 . doi:&#160; 10.1007/S00521-025-11144-2 3 Lafraxo S Ansari ME Koutti L Kerkaou Z Souaidi M . ( 2024 ). Attdenseunet: Segmentation of polyps from colonoscopic images based on attention-densenet-unet architecture , in: 11th International Conference on Wireless Networks and Mobile Communications, WINCOM 2024, Leeds, United Kingdom 1 &#8211; 6 . doi:&#160; 10.1109/WINCOM62286.2024.10657198 4 Wu Z Lv F Chen C Hao A Li S . ( 2024 ). Colorectal polyp segmentation in the deep learning era: A comprehensive survey. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE . 5 Manser CN Bachmann LM Brunner J Hunold F Bauerfeind P Marbet UA . ( 2012 ). Colonoscopy screening markedly reduces the occurrence of colon carcinomas and carcinoma- related death: a closed cohort study . Gastrointestinal Endoscopy 76 : 110&#8211;7 . doi:&#160; 10.1016/j.gie.2012.02.040 , PMID: 22498179 6 Chachlioutaki K Papas N Chatzis Z Katsamenis O Robinson SK Tsongas K . Mechanochemical-induced swelling-activation of a gastric-deployable 4d-printed polypill inspired by natural hygromorphic actuators . Adv Intell Syst . ( 2025 ) 7 . doi:&#160; 10.1002/AISY.202400526 7 Lv G Wang B Xu C Ding W Liu J . Multi-stage feature fusion network for polyp segmentation . Appl Soft Comput . ( 2025 ) 175 : 113034 . doi:&#160; 10.1016/J.ASOC.2025.113034 8 Qayoom A Xie J Ali H . Polyp segmentation in medical imaging: challenges, approaches and future directions . Artif Intell Rev . ( 2025 ) 58 : 169 . doi:&#160; 10.1007/S10462-025-11173-2 9 Park S Lee D Lee JY Chun J Chang JY Baek E . Colonood: A complete pipeline for optical diagnosis of colorectal polyps integrating out-of-distribution detection and uncertainty quantification . Expert Syst Appl . ( 2026 ) 295 : 128756 . doi:&#160; 10.1016/J.ESWA.2025.128756 10 Belabbes MA Oukdach Y Souaidi M Koutti L Charfi S . Corrections to &#8221;advancements in polyp detection: A developed single shot multibox detector approach . IEEE Access . ( 2025 ) 13 : 30586 . doi:&#160; 10.1109/ACCESS.2025.3540622 11 Sedeh MA Sharifian S . Edgepvm: A serverless satellite edge computing constellation for changes detection using onboard parallel siamese vision MAMBA . Future Gener Comput Syst . ( 2026 ) 174 : 107985 . doi:&#160; 10.1016/J.FUTURE.2025.107985 12 Zhang W Chen T Xu W Li X . Samamba: Integrating state space model for enhanced multi- modal survival analysis . 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) , ( 2024 ) 1334&#8211;41 . 13 Shan J Huang Y Jiang W Yuan D Guo F . Glou-mit: Lightweight global-local mamba guided u-mix transformer for uav-based pavement crack segmentation . Adv Eng Inf . ( 2025 ) 65 : 103384 . doi:&#160; 10.1016/J.AEI.2025.103384 14 Xu W Zheng S Wang C Zhang Z Ren C Xu R . Samamba: Adaptive state space modeling with hierarchical vision for infrared small target detection . Information Fusion , ( 2025 ). doi:&#160; 10.1016/j.inffus.2025.103338 15 Zhou T Chai W Chang D Chen K Zhang Z Lu H . Mambayolact: you only look at mamba prediction head for head-neck lymph nodes . Artif Intell Rev . ( 2025 ) 58 : 180 . doi:&#160; 10.1007/S10462-025-11177-Y 16 Than N Nguyen VQ Truong G Pham V Tran T . Mixmamba-fewshot: mamba and attention mixer-based method with few-shot learning for bearing fault diagnosis . Appl Intell . ( 2025 ) 55 : 484 . doi:&#160; 10.1007/S10489-025-06361-0 17 Wang Z Li L Zeng C Dong S Sun J . Slb-mamba: A vision mamba for closed and open-set student learning behavior detection . Appl Soft Comput . ( 2025 ) 180 : 113369 . doi:&#160; 10.1016/J.ASOC.2025.113369 18 Buldu A Kaplan K Kuncan M . A hybrid study for epileptic seizure detection based on deep learning using eeg data . Journal of Universal Computer Science (JUCS) ( 2024 ) 30 . doi:&#160; 10.3897/jucs.109933 19 Lo Giudice M Varone G Ieracitano C Mammone N Tripodi GG Ferlazzo E . Permutation entropy-based interpretability of cnn models for interictal eeg discrimination of epileptic vs. psychogenic non-epileptic seizures . Entropy . ( 2022 ) 24 : 102 . doi:&#160; 10.3390/e24010102 , PMID: 35052128 PMC8775069 20 Sikder S Efat AH Hasan SMM Jannat N Mitu M Oishe M . Atriple- levelensemble-based brain tumor classification using dense-resnet in association with three attention mechanisms. In 2023 26 th International Conference on Computer and Information Technology (ICCIT) . ( 2023 ) 1&#8211;6 . doi:&#160; 10.1109/ICCIT60459.2023.10441290 21 Onah D Desai R . Deep brain net: An optimized deep learning model for brain tumor detection in mri images using efficientnetb0 and resnet50 with transfer learning . Electrical Engineering and Systems Science . ( 2025 ). 22 Liu Z Mao H Wu C Feichtenhofer C Darrell T Xie S . A convnet for the 2020s . 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , ( 2022 ) 11966&#8211;76 . doi:&#160; 10.1109/CVPR52688.2022.01167 23 Jia Q Shu H . Bitr-unet: a cnn- transformer combined network for mri brain tumor segmentation. In BrainLes (MICCAI2021)&#8212;LNCS . ( 2021 ) 2021 : 3&#8211;14 . doi:&#160; 10.1007/978-3-031-09002-81 PMC9396958 36005929 24 Jia Q Shu H . Bitr-unet: a cnn-transformer combined network for mri brain tumor segmentation . BrainLes (MICCAI 2021) &#8212; LNCS . ( 2021 ), 3 &#8211; 14 . doi:&#160; 10.1007/978-3-031-09002-8_1 , PMID: 36005929 PMC9396958 25 Li H Zhang D Yao J Han L Li Z Han J . Asps: Augmented segment anything model for polyp segmentation. Medical Image Computing and Computer Assisted Intervention&#8211; MICCAI 2024 15009 , ( 2024 ) 118&#8211;28 . doi:&#160; 10.1007/978-3-031-72114-412 26 Zhang Y He N Yang J Li Y Wei D Huang Y . mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation . Springer, Cham . ( 2022 ) 13435 : 107&#8211;17 . doi:&#160; 10.48550/arXiv.2206.02425"
}