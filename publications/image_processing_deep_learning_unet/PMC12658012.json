{
  "pmcid": "PMC12658012",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.270742",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12658012",
    "pmid": "41291031",
    "doi": "10.1038/s41598-025-29130-y",
    "title": "Automated segmentation of the fibula from CT imaging using two-stepped deep learning in 3D U-Net architectures",
    "year": "2025",
    "month": "11",
    "day": "25",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "25"
    },
    "authors": [
      "Nascimento Jônatas de Souza",
      "Pankert Tobias",
      "Peters Florian",
      "Hölzle Frank",
      "Modabber Ali",
      "Wien Mathias",
      "Raith Stefan"
    ],
    "abstract": "This study proposes a fully automatic segmentation of the fibula bone from CT images for application in pre-operative planning of reconstructive surgery. The objective is to make use of new developments in the image segmentation field to optimize and reduce the costs of patient-specific surgery planning. Two different approaches are proposed to perform the fibula bone segmentation, both based on a two-step segmentation method using a 3D-UNet architecture. To account for the symmetry of the left and right fibula bones, input images of the right fibula are mirrored to the left side. The accuracy of the trained models is measured using common evaluation metrics, together with specific metrics focused on facial reconstructive surgery. Both of the described approaches achieve high-accuracy results. For the best-trained model, an average Dice score of 0.95 and Average Surface Distances below 0.31 mm is measured on the test set in the region of interest for the surgery. Both approaches are robust segmentation techniques and permit data pre-processing for further application in the context of preoperative surgical planning of procedures for facial reconstruction with bony transplants.",
    "keywords": [
      "Anatomy",
      "Computational biology and bioinformatics",
      "Engineering",
      "Health care",
      "Mathematics and computing",
      "Medical research"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12658012</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12658012.1</article-id><article-id pub-id-type=\"pmcaid\">12658012</article-id><article-id pub-id-type=\"pmcaiid\">12658012</article-id><article-id pub-id-type=\"pmid\">41291031</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-29130-y</article-id><article-id pub-id-type=\"publisher-id\">29130</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Automated segmentation of the fibula from CT imaging using two-stepped deep learning in 3D U-Net architectures</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Nascimento</surname><given-names initials=\"JDS\">J&#244;natas de Souza</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Pankert</surname><given-names initials=\"T\">Tobias</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Peters</surname><given-names initials=\"F\">Florian</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>H&#246;lzle</surname><given-names initials=\"F\">Frank</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Modabber</surname><given-names initials=\"A\">Ali</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wien</surname><given-names initials=\"M\">Mathias</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Raith</surname><given-names initials=\"S\">Stefan</given-names></name><address><email>sraith@ukaachen.de</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04xfq0f34</institution-id><institution-id institution-id-type=\"GRID\">grid.1957.a</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0728 696X</institution-id><institution>Department of Oral and Maxillofacial Surgery, </institution><institution>RWTH Aachen University Hospital, </institution></institution-wrap>Aachen, Germany </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04xfq0f34</institution-id><institution-id institution-id-type=\"GRID\">grid.1957.a</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0728 696X</institution-id><institution>Institute for Imaging and Computer Vision, </institution><institution>RWTH Aachen University, </institution></institution-wrap>Aachen, Germany </aff><aff id=\"Aff3\"><label>3</label>Inzipio GmbH, Aachen, Germany </aff></contrib-group><pub-date pub-type=\"epub\"><day>25</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>42020</elocation-id><history><date date-type=\"received\"><day>24</day><month>7</month><year>2025</year></date><date date-type=\"accepted\"><day>14</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>25</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 14:25:12.873\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_29130.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">This study proposes a fully automatic segmentation of the fibula bone from CT images for application in pre-operative planning of reconstructive surgery. The objective is to make use of new developments in the image segmentation field to optimize and reduce the costs of patient-specific surgery planning. Two different approaches are proposed to perform the fibula bone segmentation, both based on a two-step segmentation method using a 3D-UNet architecture. To account for the symmetry of the left and right fibula bones, input images of the right fibula are mirrored to the left side. The accuracy of the trained models is measured using common evaluation metrics, together with specific metrics focused on facial reconstructive surgery. Both of the described approaches achieve high-accuracy results. For the best-trained model, an average Dice score of 0.95 and Average Surface Distances below 0.31 mm is measured on the test set in the region of interest for the surgery. Both approaches are robust segmentation techniques and permit data pre-processing for further application in the context of preoperative surgical planning of procedures for facial reconstruction with bony transplants.</p></abstract><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Anatomy</kwd><kwd>Computational biology and bioinformatics</kwd><kwd>Engineering</kwd><kwd>Health care</kwd><kwd>Mathematics and computing</kwd><kwd>Medical research</kwd></kwd-group><funding-group><award-group><funding-source><institution>Universit&#228;tsklinikum RWTH Aachen (8915)</institution></funding-source></award-group><open-access><p>Open Access funding enabled and organized by Projekt DEAL.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par5\">In recent years, Convolutional Neural Networks (CNNs) have been used for different tasks in the medical field, including detecting tumors<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, COVID-19 diagnosis<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>, and improving patient-specific individualization of treatments<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>. These exemplary applications show the enormous potential of using Deep Learning methods to improve different tasks in the medical field. There are, however, several applications in this domain for which the possibilities of using Artificial Intelligence-based techniques have not been explored yet. The planning of facial reconstructive surgery is an example where essential tasks could be potentially improved with such novel methods. Until now, few works on that topic have been published in literature, with only one publication known so far that deals with the segmentation of the iliac crest (i.e. a part of the pelvic bone) with the aim of use in reconstructive facial surgery<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. There has been a remarkable increase in the use of computer tools to assist surgeons in the surgery planning procedure, which is leading to essential transformations in the field<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref>,<xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. The use of Deep Learning techniques, commonly used in medical imaging, has remained scarcely studied with respect to an application in a surgical context. This can be seen in the facial reconstructive surgery planning context. Even though the use of 3D models of the donor bones during the planning procedures is shown to yield positive outcomes<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>, the usual process to acquire those 3D models is still highly manual, and time-consuming<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>, and could potentially be automated with Neural Network-based techniques. Even though fibular segmentation may appear less challenging compared to more complex anatomical structures, such as the mandible, an accurate representation of the fibular bone from CT scans is of paramount importance for a precise patient individual planning of reconstructive surgery. Although manual segmentation is feasible, it remains time-consuming and specifically subject to inter-operator variability, and thus bottleneck in an otherwise largely automated process routine of digital surgical planning. Thus, an automatic and hence repeatable and fast segmentation method, is highly valuable for clinical application.</p><p id=\"Par6\">While various fully automatic CNN-based approaches for the segmentation of different bones are described in literature<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>, segmentation of the fibula bone has not been investigated, despite its major significance as a donor bone for grafting procedures<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref>,<xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>.</p><p id=\"Par7\">In this study, we propose a fully automatic CNN-based approach to perform the segmentation of the fibula bone from CT imaging using a two-step, 3D-Unet approach. This two-step based approach using 3D U-Nets was chosen given its showed robustness and applicability in a variety of usage scenarios in medical image processing. Its application proved to be well-suited for the segmentation of other anatomical structures, including mandible<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>, the carpal bone<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, and the pelvic bone<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>.</p><p id=\"Par8\">Thus, the contribution of automated segmentation of the fibular bone is a closing link to the process chain of a fully automated pipeline of surgical planning comprising mandibular segmentation<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>, graft design<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> and finally the design of surgical guides<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> to enable a high-quality digital workflow in mandibular reconstruction using fibula-free flaps.</p><sec id=\"Sec2\"><title>Virtual planning in reconstructive surgery</title><p id=\"Par9\">Preoperative planning is an essential part of complex surgical processes. Recently, there has been increasing research attention on this step<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>, recognizing that well-planned procedures positively impact the surgery as a whole<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. This focus has guided the growth in the search for patient-specific planning solutions in the field of facial reconstructive surgery in recent years. Consequently, the use of computer-assisted tools in surgical preparation has gained special attention, particularly in mandibular reconstruction surgeries<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. With the assistance of 3D modeling of the bones, a patient-specific cutting guide can be developed beforehand, along with osteosynthesis plates<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>. These advancements optimize the surgery in ways that were not possible employing just a freehand surgical approach.</p><p id=\"Par10\">Various researchers compared the mandibular reconstruction surgery when made with and without the assistance of virtual planning guides, where 3D models of the relevant bones are made based on CT images. Results showed significant improvements in operative time and the overall surgery together with better comfort to the surgeon<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>, better reliability<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>, and increased symmetry compared to freehand procedures<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>. Other studies also made systematic reviews of the impact of computer-assisted surgery in maxillary reconstruction based on available literature<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>,<xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. While there are relevant aspects of criticism on the heterogeneity of the evaluation metrics used to measure the success of the procedures<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>, they also highlight the advantages of computed assisted surgeries on the field<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref>,<xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>.</p></sec><sec id=\"Sec3\"><title>Medical image segmentation</title><p id=\"Par11\">It is noted that, even though the use of virtual surgery planning has been showing great advantages in mandibular reconstruction surgery in general, the key limitation to this approach is the preparation of the 3D models to be used in the procedure. The first step of this process is to acquire three-dimensional image data of the anatomical regions of interest, typically with CT for bony structures, and then perform segmentation to annotate the bone to be modeled before the surgery. For instance, from the CT scan of the bottom region of the body, extract only the fibula that will be used as a donor bone for the surgery. The simplest and yet most commonly used segmentation method in the surgery planning context is threshold segmentation<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>. It consists of a selection of a threshold of intensity and asserting the value one for every pixel in the image above the intensity threshold, or zero otherwise<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>. The threshold can be selected either empirically or using a quantitative technique. Van Eijnatten et al.<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup> made an overview of segmentation techniques for additive manufacturing of anatomical models. Their analysis shows that most segmentation applications used either a direct global threshold or some variation of this approach. However, even though this method was proven to be capable of achieving favorable results with decent geometric accuracy, the method includes an extensive manual step. This is because the cited method has a series of inconsistencies, such as not taking CT artifacts and noise into account or intensity variations between different CT scanners<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>. So far, the only way around such inconsistencies when creating 3D models suitable for surgery planning procedures or similar tasks includes manual post-processing, which is usually expensive, time-consuming, and requires a skilled and trained expert. Even in the best cases, due to subjectivity or personal variance, the final results of threshold-based segmentation may vary significantly.</p><p id=\"Par12\">To overcome these problems of purely threshold-based segmentations, altas-based approaches showed remarkable performance, e.g. for femur segmentation<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>. Deep learning techniques are more recently widely studied in the context of medical image segmentation. Different studies proposed CNN-based techniques to segment bones from CT<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref>,<xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup> or MRI images<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref>,<xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>. Yosinski et al.<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup> showed that initializing weights of deep neural networks based on an already trained model improves the generalization performance of the model, even for non-related tasks. This feature is called Transfer Learning and was already used in the medical field, e.g., to train networks to identify COVID infection based on chest X-rays<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>, to classify brain tumor MRI images<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup>, and also to classify endoscopic colonoscopy images<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>. Zhang et al. could show that even segmentation of different facial bones and corresponding landmark detection based on a heatmap approach is feasible<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>.</p><p id=\"Par13\">Focusing on facial reconstructive surgery planning, Pankert et al.<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup> developed a U-Net-based two-step segmentation approach to segment the mandibular bone. They conducted an extensive discussion on the advantages of using a two-step approach instead of only one U-Net and reached Dice scores of over 0.94 and Average Surface Distances lower than 0.36 mm. The present work intends to investigate the possibility of adapting this two-step segmentation method to the donor bones of facial reconstructive surgery. This way contributes to the current main issue of the planning procedure, which is the high costs and time consumption of the segmentation step.</p></sec></sec><sec id=\"Sec4\"><title>Materials and methods</title><sec id=\"Sec5\"><title>Dataset overview</title><p id=\"Par14\">The RWTH Aachen University Hospital provided CT scans of 89 patients for this study along with corresponding manual segmentations of the fibula bone that were used in real surgery planning scenarios. Institutional approval (EK 260/20) of the local ethics committee of RWTH Aachen University Hospital was obtained. All methods were carried out in accordance with relevant guidelines and regulations.</p><p id=\"Par15\">These segmentations are referred to as ground truth in the following. The age of the patients ranged from 11 to 80 years with a mean of 55.9 and a median of 59 years. 37 patients were male and 52 female. The dataset provided was not fully homogeneous and partially contained inconsistencies, so the data was carefully investigated for those flaws, and sets were excluded from the subsequent study in case they did not meet the inclusion criteria, that was defined as a complete manual segmentation with correct spatial alignment to the corresponding CT imaging data. Additionally, not all patients had both fibula bones segmented, but only one side. In these cases, the available side was chosen for subsequent work. The included datasets were subsequently used in the convolutional neural networks training that is detailed in further sections.</p><p id=\"Par16\">After the exclusion of the inconsistent cases, there were 13 data sets with only the right fibula, 26 data sets with only the left fibula, and 36 data sets with both fibulae segmented, summing up to 111 fibulae from 75 subjects. From these, 10 data sets with both fibulae segmented were held back as an independent test set, to evaluate the proposed segmentation pipelines. This dataset was deemed to be sufficiently large to produce satisfying results, as other studies with two-staged approaches showed to be applicable with smaller datasets, e.g. on carpal bones<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup> and in a preliminary study on training set sizes on the mandibular bone<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. However, considering the wide range of ages and the inclusion of both sexes, the relatively small dataset size is a limitation of the study.</p></sec><sec id=\"Sec6\"><title>Segmentation approach</title><p id=\"Par17\">In this work, we present two different methods to automatically segment the fibula bone from the CT data. The development of these two different approaches was done because of the dataset specificity that the bones of interest appear twice in the field of view. Both of these propositions are modifications of methods previously studied in literature. Wang et al., for instance, proposed a two-step segmentation framework to segment organs at risk of the head<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>. The method uses a two-step segmentation process that starts with a localization step that is applied to the whole field of view of the imaging data. The purpose of this initial step is to locate the region of interest around the anatomical structures of relevance in order to crop the field of view around this detected region and to allow for a subsequent detailed segmentation. Subsequently, a second step, that may be called a refinement step, is applied that can focus on the structures of choice and allows to segment these particular structures in detail with high precision by optimally using the available model capability. This method proved to be beneficial in a variety of different use cases, such as the segmentation of mandibular bones<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>, the pelvic bone<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup> or the wrist bone<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, as shown in previous research. In this application, two U-nets are independently trained for each step of the task. For both steps, the same input and output resolution was used.</p><p id=\"Par18\">Therefore, the present study proposes to address the fibula bone segmentation on the available dataset. The baseline method is similar to the approaches described previously: Two U-Nets are trained to perform two different tasks. The purpose of the first U-Net is to segment the bone of interest from the full CT image, while the second is trained to segment the bone only from the bounding box where the bone is present.</p><sec id=\"Sec7\"><title>Bilateral segmentation</title><p id=\"Par19\">The first proposed method consists of a two-step segmentation, where the first step is responsible for identifying both of the fibulae at the same time. Thus, the first step is trained on the whole CT image, down-sampled to the resolution <inline-formula id=\"IEq1\"><tex-math id=\"d33e471\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$144 \\times 144 \\times 288$$\\end{document}</tex-math></inline-formula> voxels, and the output is the left and the right fibulae, which will be used for the second step of segmentation. To identify the left and the right fibulae, the output of the U-Net is converted into 3D surface objects using the marching cubes algorithm<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>. We assume that the two largest output meshes are the fibulae bones, while the rest are discarded. Based on their orientation in space, we label the fibulae meshes as left and right.</p><p id=\"Par20\">Similar to<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup> and<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>, the neural networks for the second step are trained only on the region of interest of the CT, for this application resampled to <inline-formula id=\"IEq2\"><tex-math id=\"d33e489\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$80 \\times 80 \\times 960$$\\end{document}</tex-math></inline-formula>, not on the whole data. However, since there are two fibulae, one on the right side and one on the left, two different U-Nets would need to be trained for this step. To circumvent this issue, we propose to mirror all right fibulae along the mid-plane of the field of view to the left side to train the second-step segmentation model on all fibulae simultaneously.</p><p id=\"Par21\">Hence, the entire pipeline consists of the following: Initially, the first step of the segmentation identifies both fibulae based on the full CT. Then the output results are post-processed and transformed into 3D meshes. Then the original CT is cropped to the bounding box of both right and left outputs, and the right fibula and cropped CT are mirrored. Lastly, the second-step model segments each of the fibulae from the cropped CT, then once more the output is post-processed and the right fibula output is re-mirrored. A diagram of this approach is shown in Fig. <xref rid=\"Fig1\" ref-type=\"fig\">1</xref>.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Bilateral segmentation approach overview: The first step takes the original CT data (shown in the diagram as threshold segmentations for visual clarity) and segments both fibula bones from the whole CT, then the second step segments each fibula bone from the CT cropped to the region of interest.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig1_HTML.jpg\"/></fig></p></sec><sec id=\"Sec8\"><title>Unilateral segmentation</title><p id=\"Par22\">Even though the method presented above is solid, and produces remarkable results, as will be shown in the Results chapter, a relevant issue with this implementation is that for the first step model to be trained, ground truth data for both left and right fibula need to be available, otherwise, it can not be used for training. This is specifically problematic for the available dataset since there are only 36 patients with both fibulae segmented in the whole set, due to the nature of the data coming from real clinical cases for surgical planning. As the second step is trained for each fibula individually, all the data can be used in the training procedure, and no samples have to be discarded. For the first step of the bilateral approach, data from 39 patients were excluded, which is 52% of the complete set.</p><p id=\"Par23\">Therefore, we propose an additional approach for the first step, where potentially all the data can be used for training the U-Net models. The proposed method consists of using all the fibulae individually for training, similar to what is already done in the second step. Instead of segmenting both fibulae from the complete CT scan, the proposition is to split the CT in half along the mid-plane of the field of view, with each half containing only one fibula.This does not imply an exact alignment with the anatomical mid-sagittal plane, but in our examples, as all patients were lying centric enough on the bench with sufficient distance between their legs, thus this approach showed to be valid on the given data. Subsequently, the first step segmentation model is trained to segment the fibula based only on this half section of the image. To avoid training two different neural networks for the same task, the same mirroring technique used in the second step is also applied to this modified first step. Hence, the right side of the CT, which was cropped, is also mirrored along with the ground truth in this step and only one neural network is trained in this step to identify left fibulae from the left part of CTs. A different resolution is also necessary for this first step, since the region that will be input for the U-Net is no longer based on the full CT scan. To maintain the relative proportion, the proportion of voxels from the y-axis to the x-axis is doubled, and the proportion from the y-axis to the z-axis is kept the same. The chosen resolution therefore is <inline-formula id=\"IEq3\"><tex-math id=\"d33e511\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$96 \\times 192 \\times 320$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par24\">In conclusion, the method presented here involves first splitting the CT data in half on the lateral axis, then using the same first-step model to segment both the left and the mirrored right side of the fibulae. Finally, to apply the second step model to segment both fibulae from the CT images cropped only to the bounding box of the fibulae found in the first step. Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref> presents an overview of this approach.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Unilateral segmentation approach overview: The CT dataset is divided into left and right sections (shown in the diagram as threshold segmentations for visual clarity), and then the first step segments each fibula bone from its respective side, and finally the second step segments each fibula bone from the CT cropped to the region of interest.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig2_HTML.jpg\"/></fig></p></sec><sec id=\"Sec9\"><title>Dataset distribution</title><p id=\"Par25\">After preprocessing of the raw imaging data and a technical check for data integrity, all available data was used for the training of the respective networks and divided into a training and validation set. For the training of the bilateral first step of this model, 22 could be made available, split with a 9 to 1 ratio into training set (n=20) and a validation set (n=2). The first step of the unilateral segmentation could use a database of 87 fibulae, where 80 could be used in training and 7 for validation. The second stage could rely on dataset of 85 fibulae, split into a dataset training (n=75) and validation (n=7).</p></sec></sec><sec id=\"Sec10\"><title>Transfer learning</title><p id=\"Par26\">As transfer learning has shown to be beneficial in related research<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>, both approaches proposed in this study use it. In this application, first, the second-step U-Net is trained from scratch, as this is expected to be a computationally easier task. Subsequently, transfer learning to the first step is used, as this showed superior results in preliminary work on other segmentation tasks. This allows for expanding the more detailed training results of the second step to the whole field of view of the CT, hence this approach is referred to as Expansion Transfer Learning (ETL)<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>.</p></sec><sec id=\"Sec11\"><title>U-Net architecture</title><p id=\"Par27\">The U-Net architecture used for the experiments is a variation of the three-dimensional U-Net described in<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>. It has ten convolutional layers in the encoding path, with four max pooling layers in between. Following a common pattern seen in the literature<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref>,<xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>, it doubles the number of feature maps for every pooling step. The network uses <inline-formula id=\"IEq4\"><tex-math id=\"d33e561\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3 \\times 3 \\times 3$$\\end{document}</tex-math></inline-formula> receptive fields and <inline-formula id=\"IEq5\"><tex-math id=\"d33e565\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2 \\times 2 \\times 2$$\\end{document}</tex-math></inline-formula> max pooling layers. Furthermore, the decoding path consists of nine convolutional layers and five upsampling layers, which simply repeat the elements of the input tensor by an upsampling factor in every dimension. The upsampling factor used in the described architecture was <inline-formula id=\"IEq6\"><tex-math id=\"d33e569\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2 \\times 2 \\times 2$$\\end{document}</tex-math></inline-formula>, which means the output of every upsampling layer is double its input for every dimension. An overview of the 3D U-net described above can be seen in Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref>. All networks described in this paper were trained using an Adam optimizer<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup> and a Dice loss. We did not implement data augmentation techniques in our presented methodology. While preliminary experiments were conducted with various augmentation approaches including spatial deformation, scaling, intensity variations, and noise addition, we did not observe significant improvements in model performance that would have justified their inclusion in our final implementation.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>3D U-Net architecture.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"/><th align=\"left\" colspan=\"1\" rowspan=\"1\">Type</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Output feature maps</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"7\" colspan=\"1\">Encoding path</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Input layer</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">16</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Pool + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">32</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Pool + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">64</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Pool + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">128</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Pool + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">256</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">512</td></tr><tr><td align=\"left\" rowspan=\"7\" colspan=\"1\">Decoding path</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">512</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Upsampling + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">256</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Upsampling + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">128</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Upsampling + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">64</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv + Upsampling + Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">32</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Conv</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Output Layer</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td></tr></tbody></table></table-wrap></p><p id=\"Par28\">No individual image normalization was used to standardize intensity values. However, the standard range of Hounsfield units (from -1024 to +3071 HU) was clipped and mapped to floating point numbers in the range from 0.0 to 1.0 for further processing in the neural network.</p></sec><sec id=\"Sec12\"><title>Post-processing</title><p id=\"Par29\">Both proposed segmentation methods include a post-processing step of the model&#8217;s predictions. This stage occurs after all the neural networks outputs and transform the binary image output to the 3D model mesh, that is subsequently used in the surgical procedure. The first step of post-processing is a binary erosion of the data<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup> and is followed by the application of the Gaussian filter<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup> to add smoothness. Finally, the binary data is transformed to a mesh using the marching cubes algorithm on the predicted data<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>.</p></sec><sec id=\"Sec13\"><title>Evaluation metrics</title><p id=\"Par30\">To evaluate the trained models in this study, we use three metrics commonly used in medical image analysis in similar applications. The first metric is the Dice Score (DS)<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref></sup>, an overlapping metric that compares the result segmentation and the ground truth. For a ground truth segmentation represented by the set of voxels <italic toggle=\"yes\">A</italic> and a predicted segmentation represented by the set of voxels <italic toggle=\"yes\">B</italic>, the Dice Score can be formalized according to Equation <xref rid=\"Equ1\" ref-type=\"disp-formula\">1</xref>.<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e708\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\mathscr{D}\\mathscr{S} = 2 \\cdot \\frac{A \\cap B}{ A + B } \\end{aligned}$$\\end{document}</tex-math></disp-formula>Additionally, we use two distance metrics, the Average Surface Distance (ASD)<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup>, and the 95% Hausdorff Distance (95% HD)<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup> which measure the average, and the 95% highest distance between the ground truth and the model prediction respectively.</p><sec id=\"Sec14\"><title>Restricted evaluation of transplantable bone</title><p id=\"Par31\">During the facial reconstructive surgery with the use of the fibula-free flap, 6 cm or 7 cm from the top and bottom of the fibula are preserved to avoid knee or ankle instability<sup><xref ref-type=\"bibr\" rid=\"CR46\">46</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup>. Therefore, only the central part of the fibula is effectively used in the procedure and needs to be as well segmented as possible for the present application. Consequently, the proposition here is to evaluate with the same metrics as defined beforehand, removing the top and bottom 7 cm of the fibula bone. This removal can be seen in Fig. <xref rid=\"Fig3\" ref-type=\"fig\">3</xref>. This removal is done on both ground truth segmentation and the output of the proposed framework. Then the cropped meshes are again compared using Dice Score, Average Surface Distance, and 95% Hausdorff Distance. In this study, these metrics are additionally labeled as Region of Interest (ROI) metrics to differentiate from the standard implementations on the whole fibular bone.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Fibula bone cropping example: the image shows the top and bottom 7cm being removed from the evaluation procedure.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig3_HTML.jpg\"/></fig></p></sec></sec></sec><sec id=\"Sec15\"><title>Results</title><p id=\"Par32\">The results section is split into a qualitative part and a quantitative part. Both refer to the final prediction, i.e.&#160;the output of the respective second U-Nets.</p><sec id=\"Sec16\"><title>Qualitative results</title><sec id=\"Sec17\"><title>Bilateral segmentation approach</title><p id=\"Par33\">Figure <xref rid=\"Fig4\" ref-type=\"fig\">4</xref> shows predictions from the trained models, each aside from its respective ground truth. The two neural networks combined succeed in identifying not just the fibula bone region and a fibula-like object, but the fibulae are visually close to the corresponding ground truth segmentation.</p><p id=\"Par34\">Visually, all the bottom edges and central regions look well segmented, whereas the top parts are not perfect for all cases. The first and second fibulae, especially, have flaws in the segmentation of its top regions, where the ground truth has it well segmented.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Bilateral Segmentation approach: Predictions (green) and ground truths (red) examples.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig4_HTML.jpg\"/></fig></p></sec><sec id=\"Sec18\"><title>Unilateral segmentation approach</title><p id=\"Par35\">The predictions obtained using the unilateral segmentation approach can be seen in Fig. <xref rid=\"Fig5\" ref-type=\"fig\">5</xref>. These results were generated for the same fibulae as the ones in Fig. <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>, therefore a direct comparison is possible.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Unilateral segmentation approach: predictions (green) and ground truths (red) examples.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig5_HTML.jpg\"/></fig></p><p id=\"Par36\">Visually, the predictions are closely aligned with the ground truths, especially on the central region, which is the one of biggest importance for facial reconstructive surgery, and also for the bottom region. The first and second fibulae show minor flaws in the top region segmentation, even though the segmentation looks relatively better than the ones achieved using the bilateral segmentation approach. Figure <xref rid=\"Fig6\" ref-type=\"fig\">6</xref> displays a closer view of one example of prediction and highlights the poor results on the top region and the low deviation results on the central and bottom regions.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Detail view of a prediction in relation to ground truth data in color-coded 3D surface comparison. (left: proximal region of the fibula (head), middle: shaft of the fibula, right: lateral region of the fibula (ankle). The image shows an example result of the bilateral approach.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig6_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec19\"><title>Quantitative results</title><p id=\"Par37\">Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> displays the quantitative evaluation of the trained models on the test set.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Evaluation of all models on the whole test set based on Dice Score (DS), Average Surface Distance (ASD) 95% Hausdorff Distance (HD) both on the whole fibular bone, and restricted to the Region of Interest (ROI).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Approach</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">DS</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ASD</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">95% HD</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ROI DS</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ROI ASD</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ROI 95% HD</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bilateral segmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq7\"><tex-math id=\"d33e836\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.85 \\pm 0.09$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq8\"><tex-math id=\"d33e841\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.91 \\pm 1.55$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq9\"><tex-math id=\"d33e846\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$5.45 \\pm 14.81$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq10\"><tex-math id=\"d33e851\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.94 \\pm 0.02$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq11\"><tex-math id=\"d33e856\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.36 \\pm 0.12$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq12\"><tex-math id=\"d33e861\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.89 \\pm 0.59$$\\end{document}</tex-math></inline-formula></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>Unilateral segmentation</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq13\"><tex-math id=\"d33e871\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.87 \\pm 0.05$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq14\"><tex-math id=\"d33e876\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.58 \\pm 0.25$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq15\"><tex-math id=\"d33e881\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2.15 \\pm 0.84$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq16\"><tex-math id=\"d33e886\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.95 \\pm 0.02$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq17\"><tex-math id=\"d33e891\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.31 \\pm 0.13$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq18\"><tex-math id=\"d33e896\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$0.79 \\pm 0.63$$\\end{document}</tex-math></inline-formula></td></tr></tbody></table></table-wrap></p><p id=\"Par38\">The unilateral approach performs overall better in all metrics. When evaluating the metrics on the whole fibula, the achieved Dice Scores are 0.85 and 0.87 for the bilateral segmentation and the unilateral method respectively, while the Average Surface Distance and 95% Hausdorff Distance show an even bigger difference between the two approaches. It is also important to note the high standard deviation of the bilateral segmentation evaluation.</p><p id=\"Par39\">However, for the Region of Interest (ROI) metrics, the difference between the two approaches is less significant. Both methods achieve high DS values of 0.94 (bilateral approach) and 0.95 (unilateral approach), the ASD and 95% HD metrics are at 0.36&#160;mm to 0.31&#160;mm and 0.89&#160;mm to 0.79&#160;mm, respectively (Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref>).</p><p id=\"Par40\">The bilateral approach suffers from a robustness problem visible in the high standard deviations. For one of the test cases, the first step of the bilateral approach failed to predict one of the fibulae as a continuous bone. Due to filtering in the post-processing, the inferior part of the fibula was discarded, resulting in a too small field of view for the second segmentation step (Fig. <xref rid=\"Fig7\" ref-type=\"fig\">7</xref>).<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>The low accuracy of the first-step segmentation in the bilateral approach leading to an incorrect bounding box for the second segmentation step. The outputs are displayed after conversion to mesh representations and before filtering out unconnected parts. Note: this is an exceptionally bad case and not the standard of our study.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig7_HTML.jpg\"/></fig></p><p id=\"Par41\">Figure <xref rid=\"Fig8\" ref-type=\"fig\">8</xref> shows the statistical distribution of the ASD evaluation on the test set for both the bi- and unilateral approaches. It is important to note that evaluating the same prediction only in the region of interest provides better results.<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Distribution of the average surface distances (ASD) on the full bone or within the region of interest (ROI) for bi- and unilateral segmentations. For the histogram of the bilateral segmentations (in red), the outlier at 7.43 mm has been omitted.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_29130_Fig8_HTML.jpg\"/></fig></p><p id=\"Par42\">The unilateral approach solves the problem of high differences in the outliers. This variant achieves an average ASD and 95% HD value of 0.58&#160;mm and 2.15&#160;mm, respectively. Both standard deviations are drastically lower as well, which are 0.25&#160;mm and 0.84&#160;mm for the whole bone. Running the segmentations, including preprocessing of the DICOM datasets and all post-processing steps took on average 50&#160;s for the bilateral and 58&#160;s for the unilateral approach per dataset on a PC with an <italic toggle=\"yes\">Nvidia GeForce RTX 2080 Ti</italic> and an <italic toggle=\"yes\">Intel Core i9-7900X</italic>.</p></sec></sec><sec id=\"Sec20\"><title>Discussion</title><p id=\"Par43\">Even though the fibular bone is an important anatomy for diverse applications in the medical field, it is still not covered by research works on automatic segmentation of this bone, especially using AI techniques.</p><p id=\"Par44\">Though targeting a long bone of the lower extremity, the present work is particularly relevant for the field of maxillofacial reconstruction, where the fibula is commonly used as a donor site for autologous bone transplantation, and fast and reliable segmentation can significantly improve the preoperative workflow. Hence, the experiments conducted in this work are of high value to this clinical domain, as the proven applicability of this method in the given context demonstrates the efficiency for segmenting the desired parts of the fibula from CT images.</p><p id=\"Par45\">Therefore, the experiments conducted in this work are of high value to the field, since their remarkable results attest to the efficiency of the proposed methods in segmenting the desired bone from CT images. These results are especially good not just for the use case but have the potential to be used in different applications, such as fracture detection<sup><xref ref-type=\"bibr\" rid=\"CR49\">49</xref></sup><sup><xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup> or disease prediction<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref></sup>. Both methods proposed are modifications of the approach described in<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref>,<xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>. Overall, the results showed a high quality of the segmentation for both proposed approaches. When evaluating the predictions on the whole bone, the average of the Dice Score metric found for the two cases is over 0.84. Overall, the bilateral approach showed decreased robustness, as visible in the high standard deviations, due to the presence of outliers. The unilateral segmentation approach, by contrast, solves this problem and shows a significantly improved robustness.</p><p id=\"Par46\">This result is a strong indication of how the unilateral approach deals better with difficult cases than the bilateral segmentation approach. Even in CT scans for which the bilateral approach has difficulties in performing the first step segmentation, this method provides a segmentation that is sufficient to correctly detect the region of interest for the second step input. That the proximal part of the fibula shows weaker segmentation accuracy can be treated as irrelevant for the intended purpose of autologous transplantation. However, for potential application of this segmentation to other medical indications, the head of the fibula could deserve a more careful investigation, either by improving the ground truth segmentations in this particular region or by specific post-processing with shape-based refinement</p><p id=\"Par47\">Therefore, the visual analysis corroborates the statistics, which attest to an overall improvement of the metrics when using the unilateral approach when evaluating the whole bone. This is expected for two main reasons: The first one is directly related to the relative resolution of the down-sampling for the first step. Since the CT scan is divided in half for the unilateral approach, and the resolution used is kept proportionally, the relative down-sampling is lower for this case. Thus, for the bilateral method, more information is lost, which affects directly the training procedure. The second reason is related to the dataset availability. As described beforehand, not every patient in the data used for training has both fibulae segmented as a ground truth. This is a problem for the bilateral approach since it needs both bones to have ground truths to allow better training. Thus, the number of samples used for training in this method is 67.5% lower in comparison to the unilateral method. These factors combined explain how the split approach can deal better with the outliers, and have overall better results, even though the second step network is the same.</p><p id=\"Par48\">All these results together show that the unilateral approach is overall a better method to perform the fibula bone segmentation task than the bilateral approach, especially because of its higher robustness, and its remarkably better results for the metrics that evaluate the whole bone. Nevertheless, both the quantitative and qualitative analyses suggest that the main difference overall between the two methods is outside the region of interest. When evaluating both approaches only on the region of interest, the difference between them is shown to be significantly lower. Even the worst results of both metrics show Average Surface Distances below 1&#160;mm in the fibula region used for the surgery. The other metrics show similar results, since the average of the Dice Score and 95% Hausdorff distance metrics are respectively 0.94 and below 1&#160;mm for both approaches, and even the worst outliers are within a good range considering the task. These errors are all within the order of magnitude of the resolution of the underlying CT scans.</p><p id=\"Par49\">Thus, in summary, both two-step approaches showed to be applicable and yield remarkable accuracy, leveraging the full potential of the available hardware by applying a coarse segmentation in a first stage, followed by a detailed segmentation around the detected area of interest. In this approach, the advantages of using one common model architecture and training dataset could be combined with the gain in relative resolution in the focused second step.</p><p id=\"Par50\">For the application of the presented segmentation method in the context of surgical planning, the required accuracy depends on many different factors. Generally, the more accurate the segmentation the better for the surgical outcome. Accuracy is limited by several factors, including imaging, segmentation and manufacturing of parts created based on this data. Van den Broeck et al. report a mean error of 0.48&#160;mm between CT scans and optical scans of the tibial bone diaphysis<sup><xref ref-type=\"bibr\" rid=\"CR52\">52</xref></sup>, showing that the imaging already contains some errors in comparison to the real clinical application. Wallner et al. report an inter-observer Dice score of 0.94 between the manual segmentation of the mandible bone from CT imaging from two different human annotators<sup><xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup>. Dice scores much higher than this might indicate an overfitting of the AI models to a specific human annotator instead of correctly generalizing to the segmentation problem. In a work by Kim et al., the manufacturing accuracy of surgical guides based on different manufacturing techniques was measured, showing mean errors ranging from 0.09&#160;mm for SLA printing to 0.31&#160;mm for Multijet printing (MJP)<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup>.</p><p id=\"Par51\">These results are of high importance for the present work, since they show that even though the bilateral segmentation approach has problems regarding robustness when considering only the region that is considered in the facial reconstructive surgery, the results are outstanding for all cases. Even though there are no studies that apply a similar approach to perform the segmentation of fibular bones, it is possible to compare the achieved results to other bone segmentation from volumetric images. For instance, when performing the vertebral cortical segmentation from chest CT scans using a 3D U-Net-based approach,<sup><xref ref-type=\"bibr\" rid=\"CR55\">55</xref></sup> obtains Dice Scores of only 0.71, which is considerably lower compared to the results achieved in the present work.<sup><xref ref-type=\"bibr\" rid=\"CR56\">56</xref></sup> proposed a cascaded approach to segment condyles from CT scans, also using 3D U-Nets. The results achieved in this study were of high quality, achieving over 0.93 Dice Score values and Hausdorff Distances lower than 2.45&#160;mm. Focusing on the reconstructive facial surgery planning application, Pankert et al.&#160;developed a fully automatic framework to segment the mandibular bone and obtained averages of 0.94, 0.35&#160;mm, and 0.97&#160;mm for the Dice Score, Average Surface Distance, and 95% Hausdorff Distance respectively, and attested its significance and possible applicability in surgical planning scenarios<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. All the discussed comparisons have to be done carefully since even though the metrics are the same, the segmented bones and the available datasets are significantly different. However, literature results attest to what is a good segmentation in the medical image field. In fact, for the region of interest, the results obtained for this work are either better or at least similar for both presented methods.</p><p id=\"Par52\">The complete process of segmentation (including pre- and post-processing) took less than one minute on average on a workstation PC with contemporary hardware. Even though the fibular bone might be an easier task for manual segmentation than other anatomical structures, the time for manual segmentation is expected to be much longer. Though there is no published data available on the average times for manual segmentation of the fibula from CT scans, and comparisons with the segmentation of other anatomical structures are not directly applicable. Based on internal consultation with personnel experienced in manual segmentation, an estimated duration of 5 to 10 minutes per fibula was reported. It is important to note that this estimate refers to trained professionals with substantial experience; for less experienced users or those performing the task only occasionally, a significant learning curve can result in considerably longer times. While this working time is lower than for more complex anatomical structures or regions with extensive imaging artifacts, it still represents a relevant burden in an already packed clinical workflow. Moreover, with an automated segmentation using a CNN, any inter-observer differences may be omitted.</p><p id=\"Par53\">Thus, the AI approach presented in this work, enables the integration into a comprehensive process chain of surgical planning, which involves more cumbersome tasks, and profits significantly from an objective and reproducible acquisition of bony geometry from CT data. The segmented virtual representation of fibular bones can be directly integrated into computer-assisted planning workflows, such as virtual preoperative planning of surgical reconstruction of the mandible or midface bones in continuity defects, as is required e.g. after tumor removal, bone necrosis, or excessive trauma, where an autologous transplant from the fibular bone in a so-called fibula free flap proved its clinical value<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref>,<xref ref-type=\"bibr\" rid=\"CR7\">7</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>,<xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. The geometric accuracy of the virtual surfaces of the digital bone models is essential for the planning of the transplant to match the defect dimensions and thus allow for optimal functional and aesthetic rehabilitation.</p><p id=\"Par54\">This work focuses on the surgical planning of fibula bone transplants in reconstructive surgery. A critical factor for clinical success is the accurate identification and integration of supporting blood vessels, as visible in CT angiography. While the present study concentrates on bone geometry, a natural extension of the automated segmentation approach would thus be the integration of vascular structures. Enabling automatic detection of relevant vessels would allow this information to directly inform clinical decision-making without requiring additional manual input. However, this task is considerably more challenging due to the high anatomical variability of vascular structures and would likely necessitate a different study design, potentially with access to larger and more diverse datasets.</p><p id=\"Par55\">It is important to note some limitations of the study. The used dataset was prepared to focus on a facial reconstructive surgery context, therefore flaws in unimportant regions for the surgery were ignored when modeling the segmentations. This directly affected the predictions achieved by the trained models. Furthermore, the low number of data available for training, especially on the bilateral segmentation approach, was probably one of the causes of the model&#8217;s lack of robustness for this method. Thus, we expect the overall results to improve significantly when more data is available for training. More data could also be generated by applying data augmentation to the existing datasets. Even though within the limitation of the preliminary studies conducted prior to the present work did not yield a discernible improvement in performance, a further ablation on augmentations techniques and settings could achieve an improvement of the training performance. In future work, we intend to apply similar methods to segment other donor bones used in the facial reconstructive surgery procedure. Furthermore, comparisons of our CNN-based approach to Atlas-based methods<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup> might yield relevant insights to the technical specifics of the different approaches in this particular field.</p></sec><sec id=\"Sec21\"><title>Conclusion</title><p id=\"Par56\">This study presented two methods to automatically segment fibula bones from full-resolution CT scans. Both propositions are based on two-step segmentations, where the second step predicts the output based on the region of interest found by the first step. The bilateral approach&#8217;s first step does a coarse segmentation of both fibulae from a full-resolution CT at once, while the unilateral approach first splits the CT into two lateral halves to simplify the task by increasing the relative image resolution and the usable training data.</p><p id=\"Par57\">Both alternatives presented in this paper have the potential to be used in clinical surgical planning scenarios after integration into approved software as a medical device. The framework is capable of producing accurate segmentation of the fibula bone from the CT scans with a significant improvement in speed in comparison to established (semi-) manual segmentation techniques, and can be directly used as input data for the computer-assisted planning procedure.</p><p id=\"Par58\">Even eventual imprecision can be easily manually corrected before the procedure, which would still be a better and overall cheaper approach than the state-of-the-art threshold segmentation-based techniques.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>S.R. and T.P. designed the study. S.R., T.P., and M.W. elaborated the methodology. J.S.N. performed technical implementation of the two-stage 3D U-Net architecture. F.P., F.H., and A.M. supervised the clinical aspects of the study, ensuring the relevance to surgical applications. M.W. provided guidance on data analysis. S.R. and J.S.N. wrote the main manuscript text, while S.R., M.W. and S.J. provided critical revisions. J.S.N and S.R. prepared the figures. All authors reviewed and approved the final manuscript. S.R., T.P., and A.M. are co-founders of Inzipio, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL. The authors declare that no funds, grants, or other support were received during the preparation of this manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p> The raw medical imaging datasets analyzed during this study are not publicly available due to privacy concerns, as per the guidelines of the ethics committee, which permits access to the original data only for a specific group of named researchers. However, evaluation data and statistical analyses derived from it are available from the corresponding author upon reasonable request.</p></notes><notes notes-type=\"data-availability\"><title>Code availability</title><p>The code for evaluation of the geometry data is available as part of the supplementary material of this publication. The base implementation of the utilized 3D U-Net is available under <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/smatzek/3DUnetCNN\">https://github.com/smatzek/3DUnetCNN</ext-link>. Further details of the implementation and methodological description, as well as specific code parts used in this study for the training and inference pipelines, are available from the corresponding author upon reasonable request.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Conflict of interest</title><p id=\"Par63\">S.R., T.P., and A.M. are co-founders of Inzipio GmbH, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital. J.S.N., S.J., M.W., F.P., and F.H. have no financial or proprietary interests in any material discussed in this article.</p></notes><notes id=\"FPar2\"><title>Ethical approval</title><p id=\"Par64\">Institutional approval (EK 260/20) of the local ethics committee of RWTH Aachen University Hospital was obtained. All methods were carried out in accordance with relevant guidelines and regulations.</p></notes><notes id=\"FPar3\"><title>Informed consent</title><p id=\"Par65\">Due to the retrospective nature of the study, the Independent Ethics Committee of the Faculty of Medicine of RWTH Aachen University Hospital waived the need of obtaining informed consent.</p></notes><notes id=\"FPar4\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par66\">S.R., T.P., and A.M. are co-founders of Inzipio, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mahmud</surname><given-names>MI</given-names></name><name name-style=\"western\"><surname>Mamun</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Abdelgawad</surname><given-names>A</given-names></name></person-group><article-title>A deep analysis of brain tumor detection from MR images using deep learning networks</article-title><source>Algorithms</source><year>2023</year><volume>16</volume><fpage>176</fpage><pub-id pub-id-type=\"doi\">10.3390/a16040176</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Mahmud, M. I., Mamun, M. &amp; Abdelgawad, A. A deep analysis of brain tumor detection from MR images using deep learning networks. <italic toggle=\"yes\">Algorithms</italic><bold>16</bold>, 176. 10.3390/a16040176 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Poongodi</surname><given-names>M</given-names></name><etal/></person-group><article-title>Diagnosis and combating COVID-19 using wearable oura smart ring with deep learning methods</article-title><source>Pers. Ubiquit. Comput.</source><year>2021</year><volume>26</volume><fpage>25</fpage><lpage>35</lpage><pub-id pub-id-type=\"doi\">10.1007/s00779-021-01541-4</pub-id><pub-id pub-id-type=\"pmcid\">PMC7908947</pub-id><pub-id pub-id-type=\"pmid\">33654480</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Poongodi, M. et al. Diagnosis and combating COVID-19 using wearable oura smart ring with deep learning methods. <italic toggle=\"yes\">Pers. Ubiquit. Comput.</italic><bold>26</bold>, 25&#8211;35. 10.1007/s00779-021-01541-4 (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s00779-021-01541-4</pub-id><pub-id pub-id-type=\"pmcid\">PMC7908947</pub-id><pub-id pub-id-type=\"pmid\">33654480</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wen</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Deep learning in digital pathology for personalized treatment plans of cancer patients</article-title><source>Semin. Diagn. Pathol.</source><year>2023</year><volume>40</volume><fpage>109</fpage><lpage>119</lpage><pub-id pub-id-type=\"doi\">10.1053/j.semdp.2023.02.003</pub-id><pub-id pub-id-type=\"pmid\">36890029</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Wen, Z. et al. Deep learning in digital pathology for personalized treatment plans of cancer patients. <italic toggle=\"yes\">Semin. Diagn. Pathol.</italic><bold>40</bold>, 109&#8211;119. 10.1053/j.semdp.2023.02.003 (2023).<pub-id pub-id-type=\"pmid\">36890029</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1053/j.semdp.2023.02.003</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raith</surname><given-names>S</given-names></name><etal/></person-group><article-title>Segmentation of the iliac crest from CT-data for virtual surgical planning of facial reconstruction surgery using deep learning</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>1097</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-83031-0</pub-id><pub-id pub-id-type=\"pmid\">39773990</pub-id><pub-id pub-id-type=\"pmcid\">PMC11707128</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Raith, S. et al. Segmentation of the iliac crest from CT-data for virtual surgical planning of facial reconstruction surgery using deep learning. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold>, 1097 (2025).<pub-id pub-id-type=\"pmid\">39773990</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-83031-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC11707128</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>van Baar</surname><given-names>GJ</given-names></name><etal/></person-group><article-title>Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review</article-title><source>J. Clin. Med.</source><year>2021</year><volume>10</volume><fpage>1226</fpage><pub-id pub-id-type=\"doi\">10.3390/jcm10061226</pub-id><pub-id pub-id-type=\"pmid\">33809600</pub-id><pub-id pub-id-type=\"pmcid\">PMC8002284</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">van Baar, G. J. et al. Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review. <italic toggle=\"yes\">J. Clin. Med.</italic><bold>10</bold>, 1226 (2021).<pub-id pub-id-type=\"pmid\">33809600</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/jcm10061226</pub-id><pub-id pub-id-type=\"pmcid\">PMC8002284</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chan</surname><given-names>TJ</given-names></name><name name-style=\"western\"><surname>Long</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Prisman</surname><given-names>E</given-names></name></person-group><article-title>The state of virtual surgical planning in maxillary reconstruction: A systematic review</article-title><source>Oral Oncol.</source><year>2022</year><volume>133</volume><fpage>106058</fpage><pub-id pub-id-type=\"doi\">10.1016/j.oraloncology.2022.106058</pub-id><pub-id pub-id-type=\"pmid\">35952582</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Chan, T. J., Long, C., Wang, E. &amp; Prisman, E. The state of virtual surgical planning in maxillary reconstruction: A systematic review. <italic toggle=\"yes\">Oral Oncol.</italic><bold>133</bold>, 106058 (2022).<pub-id pub-id-type=\"pmid\">35952582</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.oraloncology.2022.106058</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><citation-alternatives><element-citation id=\"ec-CR7\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Culi&#233;</surname><given-names>D</given-names></name><etal/></person-group><article-title>Virtual planning and guided surgery in fibular free-flap mandibular reconstruction: A 29-case series</article-title><source>Eur. Ann. Otorhinolaryngol. Head Neck Dis.</source><year>2016</year><volume>133</volume><fpage>175</fpage><lpage>178</lpage><pub-id pub-id-type=\"doi\">10.1016/j.anorl.2016.01.009</pub-id><pub-id pub-id-type=\"pmid\">26876743</pub-id></element-citation><mixed-citation id=\"mc-CR7\" publication-type=\"journal\">Culi&#233;, D. et al. Virtual planning and guided surgery in fibular free-flap mandibular reconstruction: A 29-case series. <italic toggle=\"yes\">Eur. Ann. Otorhinolaryngol. Head Neck Dis.</italic><bold>133</bold>, 175&#8211;178. 10.1016/j.anorl.2016.01.009 (2016).<pub-id pub-id-type=\"pmid\">26876743</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.anorl.2016.01.009</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>van Eijnatten</surname><given-names>M</given-names></name><etal/></person-group><article-title>Ct image segmentation methods for bone used in medical additive manufacturing</article-title><source>Med. Eng. Phys.</source><year>2017</year><volume>10</volume><fpage>24</fpage><lpage>25</lpage><pub-id pub-id-type=\"doi\">10.1016/j.medengphy.2017.10.008</pub-id><pub-id pub-id-type=\"pmid\">29096986</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">van Eijnatten, M. et al. Ct image segmentation methods for bone used in medical additive manufacturing. <italic toggle=\"yes\">Med. Eng. Phys.</italic><bold>10</bold>, 24&#8211;25. 10.1016/j.medengphy.2017.10.008 (2017).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.medengphy.2017.10.008</pub-id><pub-id pub-id-type=\"pmid\">29096986</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pankert</surname><given-names>T</given-names></name><etal/></person-group><article-title>Mandible segmentation from CT data for virtual surgical planning using an augmented two-stepped convolutional neural network</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2023</year><pub-id pub-id-type=\"doi\">10.1007/s11548-022-02830-w</pub-id><pub-id pub-id-type=\"pmid\">36637748</pub-id><pub-id pub-id-type=\"pmcid\">PMC10363055</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Pankert, T. et al. Mandible segmentation from CT data for virtual surgical planning using an augmented two-stepped convolutional neural network. <italic toggle=\"yes\">Int. J. Comput. Assist. Radiol. Surg.</italic>10.1007/s11548-022-02830-w (2023).<pub-id pub-id-type=\"pmid\">36637748</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11548-022-02830-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC10363055</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Minnema</surname><given-names>J</given-names></name><etal/></person-group><article-title>Ct image segmentation of bone for medical additive manufacturing using a convolutional neural network</article-title><source>Comput. Biol. Med.</source><year>2018</year><volume>103</volume><fpage>130</fpage><lpage>139</lpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2018.10.012</pub-id><pub-id pub-id-type=\"pmid\">30366309</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Minnema, J. et al. Ct image segmentation of bone for medical additive manufacturing using a convolutional neural network. <italic toggle=\"yes\">Comput. Biol. Med.</italic><bold>103</bold>, 130&#8211;139. 10.1016/j.compbiomed.2018.10.012 (2018).<pub-id pub-id-type=\"pmid\">30366309</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2018.10.012</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raudaschl</surname><given-names>PF</given-names></name><etal/></person-group><article-title>Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015</article-title><source>Med. Phys.</source><year>2017</year><volume>44</volume><fpage>2020</fpage><lpage>2036</lpage><pub-id pub-id-type=\"doi\">10.1002/mp.12197</pub-id><pub-id pub-id-type=\"pmid\">28273355</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Raudaschl, P. F. et al. Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015. <italic toggle=\"yes\">Med. Phys.</italic><bold>44</bold>, 2020&#8211;2036 (2017).<pub-id pub-id-type=\"pmid\">28273355</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/mp.12197</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hidalgo</surname><given-names>DA</given-names></name><name name-style=\"western\"><surname>Pusic</surname><given-names>AL</given-names></name></person-group><article-title>Free-flap mandibular reconstruction: A 10-year follow-up study</article-title><source>Plast. Reconstr. Surg.</source><year>2002</year><volume>110</volume><fpage>438</fpage><lpage>449</lpage><pub-id pub-id-type=\"doi\">10.1097/00006534-200208000-00010</pub-id><pub-id pub-id-type=\"pmid\">12142657</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Hidalgo, D. A. &amp; Pusic, A. L. Free-flap mandibular reconstruction: A 10-year follow-up study. <italic toggle=\"yes\">Plast. Reconstr. Surg.</italic><bold>110</bold>, 438&#8211;449. 10.1097/00006534-200208000-00010 (2002).<pub-id pub-id-type=\"pmid\">12142657</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1097/00006534-200208000-00010</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>H&#246;lzle</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Franz</surname><given-names>EP</given-names></name><name name-style=\"western\"><surname>von Diepenbroick</surname><given-names>VH</given-names></name><name name-style=\"western\"><surname>Wolff</surname><given-names>K-D</given-names></name></person-group><article-title>Evaluation der unterschenkelarterien vor mikrochirurgischem fibulatransfer</article-title><source>Mund Kiefer Gesichtschir.</source><year>2003</year><volume>7</volume><fpage>246</fpage><lpage>253</lpage><pub-id pub-id-type=\"doi\">10.1007/s10006-003-0486-8</pub-id><pub-id pub-id-type=\"pmid\">12961076</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">H&#246;lzle, F., Franz, E. P., von Diepenbroick, V. H. &amp; Wolff, K.-D. Evaluation der unterschenkelarterien vor mikrochirurgischem fibulatransfer. <italic toggle=\"yes\">Mund Kiefer Gesichtschir.</italic><bold>7</bold>, 246&#8211;253. 10.1007/s10006-003-0486-8 (2003).<pub-id pub-id-type=\"pmid\">12961076</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s10006-003-0486-8</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Raith, S. et al. Multi-label segmentation of carpal bones in mri using expansion transfer learning. <italic toggle=\"yes\">Phys. Med. Biol.</italic> (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1088/1361-6560/adabae</pub-id><pub-id pub-id-type=\"pmid\">39823747</pub-id></mixed-citation></ref><ref id=\"CR15\"><label>15.</label><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raith</surname><given-names>S</given-names></name><etal/></person-group><article-title>Introduction of an algorithm for planning of autologous fibular transfer in mandibular reconstruction based on individual bone curvatures</article-title><source>Int. J. Med. Robot. Comput. Assist. Surg.</source><year>2018</year><volume>14</volume><fpage>e1894</fpage><pub-id pub-id-type=\"doi\">10.1002/rcs.1894</pub-id><pub-id pub-id-type=\"pmid\">29423929</pub-id></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Raith, S. et al. Introduction of an algorithm for planning of autologous fibular transfer in mandibular reconstruction based on individual bone curvatures. <italic toggle=\"yes\">Int. J. Med. Robot. Comput. Assist. Surg.</italic><bold>14</bold>, e1894. 10.1002/rcs.1894 (2018).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/rcs.1894</pub-id><pub-id pub-id-type=\"pmid\">29423929</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Modabber</surname><given-names>A</given-names></name><etal/></person-group><article-title>Evaluation of a novel algorithm for automating virtual surgical planning in mandibular reconstruction using fibula flaps</article-title><source>J. Craniomaxillofac. Surg.</source><year>2019</year><volume>47</volume><fpage>1378</fpage><lpage>1386</lpage><pub-id pub-id-type=\"doi\">10.1016/j.jcms.2019.06.013</pub-id><pub-id pub-id-type=\"pmid\">31331845</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Modabber, A. et al. Evaluation of a novel algorithm for automating virtual surgical planning in mandibular reconstruction using fibula flaps. <italic toggle=\"yes\">J. Craniomaxillofac. Surg.</italic><bold>47</bold>, 1378&#8211;1386. 10.1016/j.jcms.2019.06.013 (2019).<pub-id pub-id-type=\"pmid\">31331845</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.jcms.2019.06.013</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><mixed-citation publication-type=\"other\">Hagen, N. et al. A user-friendly software for automated knowledge-based virtual surgical planning in mandibular reconstruction. <italic toggle=\"yes\">Journal of Clinical Medicine</italic><bold>14</bold>, 10.3390/jcm14134508 (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/jcm14134508</pub-id><pub-id pub-id-type=\"pmcid\">PMC12249504</pub-id><pub-id pub-id-type=\"pmid\">40648882</pub-id></mixed-citation></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Automated planning of mandible reconstruction with fibula free flap based on shape completion and morphometric descriptors</article-title><source>Med. Image Anal.</source><year>2025</year><volume>102</volume><fpage>103544</fpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2024.103544</pub-id><pub-id pub-id-type=\"pmid\">40132366</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Guo, Y. et al. Automated planning of mandible reconstruction with fibula free flap based on shape completion and morphometric descriptors. <italic toggle=\"yes\">Med. Image Anal.</italic><bold>102</bold>, 103544. 10.1016/j.media.2024.103544 (2025).<pub-id pub-id-type=\"pmid\">40132366</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2025.103544</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>&#352;imi&#263;</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Kopa&#269;in</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Mumlek</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Butkovi&#263;</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zub&#269;i&#263;</surname><given-names>V</given-names></name></person-group><article-title>Improved technique of personalised surgical guides generation for mandibular free flap reconstruction using an open-source tool</article-title><source>Eur. Radiol. Exp.</source><year>2021</year><volume>5</volume><fpage>30</fpage><pub-id pub-id-type=\"doi\">10.1186/s41747-021-00229-x</pub-id><pub-id pub-id-type=\"pmid\">34318382</pub-id><pub-id pub-id-type=\"pmcid\">PMC8316532</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">&#352;imi&#263;, L., Kopa&#269;in, V., Mumlek, I., Butkovi&#263;, J. &amp; Zub&#269;i&#263;, V. Improved technique of personalised surgical guides generation for mandibular free flap reconstruction using an open-source tool. <italic toggle=\"yes\">Eur. Radiol. Exp.</italic><bold>5</bold>, 30. 10.1186/s41747-021-00229-x (2021).<pub-id pub-id-type=\"pmid\">34318382</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s41747-021-00229-x</pub-id><pub-id pub-id-type=\"pmcid\">PMC8316532</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">van Baar, G. J.&#160;C. et al. Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review. <italic toggle=\"yes\">J. Clin. Med.</italic><bold>10</bold>, 10.3390/jcm10061226 (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/jcm10061226</pub-id><pub-id pub-id-type=\"pmcid\">PMC8002284</pub-id><pub-id pub-id-type=\"pmid\">33809600</pub-id></mixed-citation></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Powcharoen</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>W-F</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>KY</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>Y-X</given-names></name></person-group><article-title>Computer-assisted versus conventional freehand mandibular reconstruction with fibula free flap: a systematic review and meta-analysis</article-title><source>Plast. Reconstr. Surg.</source><year>2019</year><volume>144</volume><fpage>1417</fpage><lpage>1428</lpage><pub-id pub-id-type=\"doi\">10.1097/PRS.0000000000006261</pub-id><pub-id pub-id-type=\"pmid\">31764662</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Powcharoen, W., Yang, W.-F., Li, K. Y., Zhu, W. &amp; Su, Y.-X. Computer-assisted versus conventional freehand mandibular reconstruction with fibula free flap: a systematic review and meta-analysis. <italic toggle=\"yes\">Plast. Reconstr. Surg.</italic><bold>144</bold>, 1417&#8211;1428 (2019).<pub-id pub-id-type=\"pmid\">31764662</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1097/PRS.0000000000006261</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wilde</surname><given-names>F</given-names></name><etal/></person-group><article-title>Multicenter study on the use of patient-specific cad/cam reconstruction plates for mandibular reconstruction</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2015</year><volume>10</volume><fpage>2035</fpage><lpage>2051</lpage><pub-id pub-id-type=\"doi\">10.1007/s11548-015-1235-2</pub-id><pub-id pub-id-type=\"pmid\">25843949</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Wilde, F. et al. Multicenter study on the use of patient-specific cad/cam reconstruction plates for mandibular reconstruction. <italic toggle=\"yes\">Int. J. Comput. Assist. Radiol. Surg.</italic><bold>10</bold>, 2035&#8211;2051. 10.1007/s11548-015-1235-2 (2015).<pub-id pub-id-type=\"pmid\">25843949</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11548-015-1193-2</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Al-Sabahi, M.&#160;E. et al. Aesthetic reconstruction of onco-surgical mandibular defects using free fibular flap with and without CAD/CAM customized osteotomy guide: A randomized controlled clinical trial. <italic toggle=\"yes\">BMC Cancer</italic><bold>22</bold>, 10.1186/s12885-022-10322-y (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s12885-022-10322-y</pub-id><pub-id pub-id-type=\"pmcid\">PMC9717507</pub-id><pub-id pub-id-type=\"pmid\">36460978</pub-id></mixed-citation></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bartier</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Mazzaschi</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Benichou</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Sauvaget</surname><given-names>E</given-names></name></person-group><article-title>Computer-assisted versus traditional technique in fibular free-flap mandibular reconstruction: A ct symmetry study</article-title><source>Eur. Ann. Otorhinolaryngol. Head Neck Dis.</source><year>2021</year><volume>138</volume><fpage>23</fpage><lpage>27</lpage><pub-id pub-id-type=\"doi\">10.1016/j.anorl.2020.06.011</pub-id><pub-id pub-id-type=\"pmid\">32620425</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Bartier, S., Mazzaschi, O., Benichou, L. &amp; Sauvaget, E. Computer-assisted versus traditional technique in fibular free-flap mandibular reconstruction: A ct symmetry study. <italic toggle=\"yes\">Eur. Ann. Otorhinolaryngol. Head Neck Dis.</italic><bold>138</bold>, 23&#8211;27. 10.1016/j.anorl.2020.06.011 (2021).<pub-id pub-id-type=\"pmid\">32620425</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.anorl.2020.06.011</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Al-Amri, S.&#160;S., Kalyankar, N.&#160;V. et al. Image segmentation by using threshold techniques. <italic toggle=\"yes\">arXiv preprint</italic><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1005.4020\">arXiv:1005.4020</ext-link> (2010).</mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Besler, B.&#160;A., Michalski, A.&#160;S., Forkert, N.&#160;D. &amp; Boyd, S.&#160;K. Automatic full femur segmentation from computed tomography datasets using an atlas-based approach. In <italic toggle=\"yes\">Computational Methods and Clinical Applications in Musculoskeletal Imaging</italic>, vol. 10734 of <italic toggle=\"yes\">Lecture Notes in Computer Science</italic>, 120&#8211;132, 10.1007/978-3-319-74113-0_11 (Springer, 2018).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>P</given-names></name><etal/></person-group><article-title>Deep learning to segment pelvic bones: large-scale CT datasets and baseline models</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2021</year><volume>16</volume><fpage>749</fpage><lpage>756</lpage><pub-id pub-id-type=\"doi\">10.1007/s11548-021-02363-8</pub-id><pub-id pub-id-type=\"pmid\">33864189</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Liu, P. et al. Deep learning to segment pelvic bones: large-scale CT datasets and baseline models. <italic toggle=\"yes\">Int. J. Comput. Assist. Radiol. Surg.</italic><bold>16</bold>, 749&#8211;756 (2021).<pub-id pub-id-type=\"pmid\">33864189</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11548-021-02363-8</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hemke</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Buckless</surname><given-names>CG</given-names></name><name name-style=\"western\"><surname>Tsao</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Torriani</surname><given-names>M</given-names></name></person-group><article-title>Deep learning for automated segmentation of pelvic muscles, fat, and bone from CT studies for body composition assessment</article-title><source>Skeletal Radiol.</source><year>2019</year><volume>49</volume><fpage>387</fpage><lpage>395</lpage><pub-id pub-id-type=\"doi\">10.1007/s00256-019-03289-8</pub-id><pub-id pub-id-type=\"pmid\">31396667</pub-id><pub-id pub-id-type=\"pmcid\">PMC6980503</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Hemke, R., Buckless, C. G., Tsao, A., Wang, B. &amp; Torriani, M. Deep learning for automated segmentation of pelvic muscles, fat, and bone from CT studies for body composition assessment. <italic toggle=\"yes\">Skeletal Radiol.</italic><bold>49</bold>, 387&#8211;395 (2019).<pub-id pub-id-type=\"pmid\">31396667</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s00256-019-03289-8</pub-id><pub-id pub-id-type=\"pmcid\">PMC6980503</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Liu, X. et al. Fully automated pelvic bone segmentation in multiparameteric mri using a 3d convolutional neural network. <italic toggle=\"yes\">Insights into Imaging</italic><bold>12</bold> (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s13244-021-01044-z</pub-id><pub-id pub-id-type=\"pmcid\">PMC8263843</pub-id><pub-id pub-id-type=\"pmid\">34232404</pub-id></mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In <italic toggle=\"yes\">2016 Fourth International Conference on 3D Vision (3DV)</italic>, 565&#8211;571 (IEEE, 2016).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Yosinski, J., Clune, J., Bengio, Y. &amp; Lipson, H. How transferable are features in deep neural networks? <italic toggle=\"yes\">Adv. Neural Inform. Process. Syst.</italic><bold>27</bold> (2014).</mixed-citation></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Das</surname><given-names>NN</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Kaur</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Singh</surname><given-names>D</given-names></name></person-group><article-title>Automated deep transfer learning-based approach for detection of COVID-19 infection in chest x-rays</article-title><source>IRBM</source><year>2022</year><volume>43</volume><fpage>114</fpage><lpage>119</lpage><pub-id pub-id-type=\"doi\">10.1016/j.irbm.2020.07.001</pub-id><pub-id pub-id-type=\"pmid\">32837679</pub-id><pub-id pub-id-type=\"pmcid\">PMC7333623</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Das, N. N., Kumar, N., Kaur, M., Kumar, V. &amp; Singh, D. Automated deep transfer learning-based approach for detection of COVID-19 infection in chest x-rays. <italic toggle=\"yes\">IRBM</italic><bold>43</bold>, 114&#8211;119. 10.1016/j.irbm.2020.07.001 (2022).<pub-id pub-id-type=\"pmid\">32837679</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.irbm.2020.07.001</pub-id><pub-id pub-id-type=\"pmcid\">PMC7333623</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Deepak</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ameer</surname><given-names>P</given-names></name></person-group><article-title>Brain tumor classification using deep CNN features via transfer learning</article-title><source>Comput. Biol. Med.</source><year>2019</year><volume>111</volume><fpage>103345</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2019.103345</pub-id><pub-id pub-id-type=\"pmid\">31279167</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Deepak, S. &amp; Ameer, P. Brain tumor classification using deep CNN features via transfer learning. <italic toggle=\"yes\">Comput. Biol. Med.</italic><bold>111</bold>, 103345. 10.1016/j.compbiomed.2019.103345 (2019).<pub-id pub-id-type=\"pmid\">31279167</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2019.103345</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Feng</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S</given-names></name></person-group><article-title>Multiclassification of endoscopic colonoscopy images based on deep transfer learning</article-title><source>Comput. Math. Methods Med.</source><year>2021</year><volume>1&#8211;12</volume><fpage>2021</fpage><pub-id pub-id-type=\"doi\">10.1155/2021/2485934</pub-id><pub-id pub-id-type=\"pmcid\">PMC8272675</pub-id><pub-id pub-id-type=\"pmid\">34306173</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Wang, Y., Feng, Z., Song, L., Liu, X. &amp; Liu, S. Multiclassification of endoscopic colonoscopy images based on deep transfer learning. <italic toggle=\"yes\">Comput. Math. Methods Med.</italic><bold>1&#8211;12</bold>, 2021. 10.1155/2021/2485934 (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1155/2021/2485934</pub-id><pub-id pub-id-type=\"pmcid\">PMC8272675</pub-id><pub-id pub-id-type=\"pmid\">34306173</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><citation-alternatives><element-citation id=\"ec-CR35\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>R</given-names></name><etal/></person-group><article-title>Craniomaxillofacial bone segmentation and landmark detection using semantic segmentation networks and an unbiased heatmap</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2024</year><volume>28</volume><fpage>427</fpage><lpage>437</lpage><pub-id pub-id-type=\"doi\">10.1109/JBHI.2023.3337546</pub-id><pub-id pub-id-type=\"pmid\">38019620</pub-id></element-citation><mixed-citation id=\"mc-CR35\" publication-type=\"journal\">Zhang, R. et al. Craniomaxillofacial bone segmentation and landmark detection using semantic segmentation networks and an unbiased heatmap. <italic toggle=\"yes\">IEEE J. Biomed. Health Inform.</italic><bold>28</bold>, 427&#8211;437. 10.1109/JBHI.2023.3337546 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JBHI.2023.3337546</pub-id><pub-id pub-id-type=\"pmid\">38019620</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>Z</given-names></name></person-group><article-title>Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>144591</fpage><lpage>144602</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2019.2944958</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Wang, Y., Zhao, L., Wang, M. &amp; Song, Z. Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net. <italic toggle=\"yes\">IEEE Access</italic><bold>7</bold>, 144591&#8211;144602. 10.1109/ACCESS.2019.2944958 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><citation-alternatives><element-citation id=\"ec-CR37\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lewiner</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Lopes</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Vieira</surname><given-names>AW</given-names></name><name name-style=\"western\"><surname>Tavares</surname><given-names>G</given-names></name></person-group><article-title>Efficient implementation of marching cubes&#8217; cases with topological guarantees</article-title><source>J. Graph. Tools</source><year>2003</year><volume>8</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type=\"doi\">10.1080/10867651.2003.10487582</pub-id></element-citation><mixed-citation id=\"mc-CR37\" publication-type=\"journal\">Lewiner, T., Lopes, H., Vieira, A. W. &amp; Tavares, G. Efficient implementation of marching cubes&#8217; cases with topological guarantees. <italic toggle=\"yes\">J. Graph. Tools</italic><bold>8</bold>, 1&#8211;15. 10.1080/10867651.2003.10487582 (2003).</mixed-citation></citation-alternatives></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">&#199;i&#231;ek, &#214;., Abdulkadir, A., Lienkamp, S.&#160;S., Brox, T. &amp; Ronneberger, O. 3d u-net: learning dense volumetric segmentation from sparse annotation. In <italic toggle=\"yes\">Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19</italic>, 424&#8211;432 (Springer, 2016).</mixed-citation></ref><ref id=\"CR39\"><label>39.</label><mixed-citation publication-type=\"other\">Wang, C., MacGillivray, T., Macnaught, G., Yang, G. &amp; Newby, D. A two-stage 3d unet framework for multi-class segmentation on full resolution image. <italic toggle=\"yes\">arXiv preprint</italic><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1804.04341\">arXiv:1804.04341</ext-link> (2018).</mixed-citation></ref><ref id=\"CR40\"><label>40.</label><mixed-citation publication-type=\"other\">Kingma, D.&#160;P. &amp; Ba, J. Adam: A method for stochastic optimization. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/https://arxiv.org/abs/1412.6980\">arxiv:https://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">JI, L., Piper, J. &amp; Tang, J.-Y. Erosion and dilation of binary images by arbitrary structuring elements using interval coding. <italic toggle=\"yes\">Pattern Recognition Letters</italic><bold>9</bold>, 201&#8211;209, 10.1016/0167-8655(89)90055-x (1989).</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><mixed-citation publication-type=\"other\">Deng, G. &amp; Cahill, L. An adaptive gaussian filter for noise reduction and edge detection. In <italic toggle=\"yes\">1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference</italic>, vol. 3, 1615&#8211;1619, 10.1109/NSSMIC.1993.373563 (1993).</mixed-citation></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dice</surname><given-names>LR</given-names></name></person-group><article-title>Measures of the amount of ecologic association between species</article-title><source>Ecology</source><year>1945</year><volume>26</volume><fpage>297</fpage><lpage>302</lpage><pub-id pub-id-type=\"doi\">10.2307/1932409</pub-id></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Dice, L. R. Measures of the amount of ecologic association between species. <italic toggle=\"yes\">Ecology</italic><bold>26</bold>, 297&#8211;302. 10.2307/1932409 (1945).</mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><citation-alternatives><element-citation id=\"ec-CR44\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Heimann</surname><given-names>T</given-names></name><etal/></person-group><article-title>Comparison and evaluation of methods for liver segmentation from CT datasets</article-title><source>IEEE Trans. Med. Imaging</source><year>2009</year><volume>28</volume><fpage>1251</fpage><lpage>1265</lpage><pub-id pub-id-type=\"doi\">10.1109/tmi.2009.2013851</pub-id><pub-id pub-id-type=\"pmid\">19211338</pub-id></element-citation><mixed-citation id=\"mc-CR44\" publication-type=\"journal\">Heimann, T. et al. Comparison and evaluation of methods for liver segmentation from CT datasets. <italic toggle=\"yes\">IEEE Trans. Med. Imaging</italic><bold>28</bold>, 1251&#8211;1265. 10.1109/tmi.2009.2013851 (2009).<pub-id pub-id-type=\"pmid\">19211338</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2009.2013851</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR45\"><label>45.</label><citation-alternatives><element-citation id=\"ec-CR45\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huttenlocher</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Klanderman</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Rucklidge</surname><given-names>W</given-names></name></person-group><article-title>Comparing images using the hausdorff distance</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1993</year><volume>15</volume><fpage>850</fpage><lpage>863</lpage><pub-id pub-id-type=\"doi\">10.1109/34.232073</pub-id></element-citation><mixed-citation id=\"mc-CR45\" publication-type=\"journal\">Huttenlocher, D., Klanderman, G. &amp; Rucklidge, W. Comparing images using the hausdorff distance. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>15</bold>, 850&#8211;863. 10.1109/34.232073 (1993).</mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><label>46.</label><citation-alternatives><element-citation id=\"ec-CR46\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wolff</surname><given-names>K-D</given-names></name><name name-style=\"western\"><surname>Ervens</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Herzog</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Hoffmeister</surname><given-names>B</given-names></name></person-group><article-title>Experience with the osteocutaneous fibula flap: an analysis of 24 consecutive reconstructions of composite mandibular defects</article-title><source>J. Cranio-Maxillofac. Surg.</source><year>1996</year><volume>24</volume><fpage>330</fpage><lpage>338</lpage><pub-id pub-id-type=\"doi\">10.1016/S1010-5182(96)80033-3</pub-id><pub-id pub-id-type=\"pmid\">9032600</pub-id></element-citation><mixed-citation id=\"mc-CR46\" publication-type=\"journal\">Wolff, K.-D., Ervens, J., Herzog, K. &amp; Hoffmeister, B. Experience with the osteocutaneous fibula flap: an analysis of 24 consecutive reconstructions of composite mandibular defects. <italic toggle=\"yes\">J. Cranio-Maxillofac. Surg.</italic><bold>24</bold>, 330&#8211;338 (1996).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/s1010-5182(96)80033-3</pub-id><pub-id pub-id-type=\"pmid\">9032600</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR47\"><label>47.</label><citation-alternatives><element-citation id=\"ec-CR47\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shpitzer</surname><given-names>T</given-names></name><etal/></person-group><article-title>Leg morbidity and function following fibular free flap harvest</article-title><source>Ann. Plast. Surg.</source><year>1997</year><volume>38</volume><fpage>460</fpage><lpage>464</lpage><pub-id pub-id-type=\"doi\">10.1097/00000637-199705000-00005</pub-id><pub-id pub-id-type=\"pmid\">9160127</pub-id></element-citation><mixed-citation id=\"mc-CR47\" publication-type=\"journal\">Shpitzer, T. et al. Leg morbidity and function following fibular free flap harvest. <italic toggle=\"yes\">Ann. Plast. Surg.</italic><bold>38</bold>, 460&#8211;464 (1997).<pub-id pub-id-type=\"pmid\">9160127</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1097/00000637-199705000-00005</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR48\"><label>48.</label><citation-alternatives><element-citation id=\"ec-CR48\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chhabra</surname><given-names>A</given-names></name><etal/></person-group><article-title>Treatment of chronic nonunions of the humerus with free vascularized fibula transfer: A report of thirteen cases</article-title><source>J. Reconstr. Microsurg.</source><year>2008</year><volume>25</volume><fpage>117</fpage><lpage>124</lpage><pub-id pub-id-type=\"doi\">10.1055/s-0028-1090624</pub-id><pub-id pub-id-type=\"pmid\">18925551</pub-id></element-citation><mixed-citation id=\"mc-CR48\" publication-type=\"journal\">Chhabra, A. et al. Treatment of chronic nonunions of the humerus with free vascularized fibula transfer: A report of thirteen cases. <italic toggle=\"yes\">J. Reconstr. Microsurg.</italic><bold>25</bold>, 117&#8211;124. 10.1055/s-0028-1090624 (2008).<pub-id pub-id-type=\"pmid\">18925551</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1055/s-0028-1090624</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR49\"><label>49.</label><mixed-citation publication-type=\"other\">Ruikar, D.&#160;D., Santosh, K.&#160;C. &amp; Hegadi, R.&#160;S. Automated fractured bone segmentation and labeling from CT images. <italic toggle=\"yes\">Journal of Medical Systems</italic><bold>43</bold>, 10.1007/s10916-019-1176-x (2019).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s10916-019-1176-x</pub-id><pub-id pub-id-type=\"pmid\">30710217</pub-id></mixed-citation></ref><ref id=\"CR50\"><label>50.</label><citation-alternatives><element-citation id=\"ec-CR50\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>Y</given-names></name></person-group><article-title>Bone fracture detection through the two-stage system of crack-sensitive convolutional neural network</article-title><source>Inform. Med. Unlocked</source><year>2021</year><volume>22</volume><fpage>100452</fpage><pub-id pub-id-type=\"doi\">10.1016/j.imu.2020.100452</pub-id></element-citation><mixed-citation id=\"mc-CR50\" publication-type=\"journal\">Ma, Y. &amp; Luo, Y. Bone fracture detection through the two-stage system of crack-sensitive convolutional neural network. <italic toggle=\"yes\">Inform. Med. Unlocked</italic><bold>22</bold>, 100452. 10.1016/j.imu.2020.100452 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR51\"><label>51.</label><mixed-citation publication-type=\"other\">Kim, H., Lee, K., Lee, D.-C. &amp; Baek, N. 3d reconstruction of leg bones from x-ray images using cnn-based feature analysis. <italic toggle=\"yes\">2019 International Conference on Information and Communication Technology Convergence (ICTC)</italic> 669&#8211;672 (2019).</mixed-citation></ref><ref id=\"CR52\"><label>52.</label><citation-alternatives><element-citation id=\"ec-CR52\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>den Broeck</surname><given-names>JV</given-names></name><name name-style=\"western\"><surname>Vereecke</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Wirix-Speetjens</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Sloten</surname><given-names>JV</given-names></name></person-group><article-title>Segmentation accuracy of long bones</article-title><source>Med. Eng. Phys.</source><year>2014</year><volume>36</volume><fpage>949</fpage><lpage>953</lpage><pub-id pub-id-type=\"doi\">10.1016/j.medengphy.2014.03.016</pub-id><pub-id pub-id-type=\"pmid\">24768087</pub-id></element-citation><mixed-citation id=\"mc-CR52\" publication-type=\"journal\">den Broeck, J. V., Vereecke, E., Wirix-Speetjens, R. &amp; Sloten, J. V. Segmentation accuracy of long bones. <italic toggle=\"yes\">Med. Eng. Phys.</italic><bold>36</bold>, 949&#8211;953. 10.1016/j.medengphy.2014.03.016 (2014).<pub-id pub-id-type=\"pmid\">24768087</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.medengphy.2014.03.016</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR53\"><label>53.</label><citation-alternatives><element-citation id=\"ec-CR53\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wallner</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Mischak</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Egger</surname><given-names>J</given-names></name></person-group><article-title>Computed tomography data collection of the complete human mandible and valid clinical ground truth models</article-title><source>Scientific Data 2019 6:1</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type=\"doi\">10.1038/sdata.2019.3</pub-id><pub-id pub-id-type=\"pmcid\">PMC6350631</pub-id><pub-id pub-id-type=\"pmid\">30694227</pub-id></element-citation><mixed-citation id=\"mc-CR53\" publication-type=\"journal\">Wallner, J., Mischak, I. &amp; Egger, J. Computed tomography data collection of the complete human mandible and valid clinical ground truth models. <italic toggle=\"yes\">Scientific Data 2019 6:1</italic><bold>6</bold>, 1&#8211;14. 10.1038/sdata.2019.3 (2019).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/sdata.2019.3</pub-id><pub-id pub-id-type=\"pmcid\">PMC6350631</pub-id><pub-id pub-id-type=\"pmid\">30694227</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR54\"><label>54.</label><citation-alternatives><element-citation id=\"ec-CR54\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>T</given-names></name><etal/></person-group><article-title>Accuracy of a simplified 3D-printed implant surgical guide</article-title><source>J. Prosthet. Dent.</source><year>2020</year><volume>124</volume><fpage>195</fpage><lpage>201.e2</lpage><pub-id pub-id-type=\"doi\">10.1016/j.prosdent.2019.06.006</pub-id><pub-id pub-id-type=\"pmid\">31753464</pub-id></element-citation><mixed-citation id=\"mc-CR54\" publication-type=\"journal\">Kim, T. et al. Accuracy of a simplified 3D-printed implant surgical guide. <italic toggle=\"yes\">J. Prosthet. Dent.</italic><bold>124</bold>, 195-201.e2. 10.1016/j.prosdent.2019.06.006 (2020).<pub-id pub-id-type=\"pmid\">31753464</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.prosdent.2019.06.006</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR55\"><label>55.</label><citation-alternatives><element-citation id=\"ec-CR55\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Automated segmentation of vertebral cortex with 3d u-net-based deep convolutional neural network</article-title><source>Front. Bioeng. Biotechnol.</source><year>2022</year><volume>10</volume><fpage>996723</fpage><pub-id pub-id-type=\"doi\">10.3389/fbioe.2022.996723</pub-id><pub-id pub-id-type=\"pmid\">36338129</pub-id><pub-id pub-id-type=\"pmcid\">PMC9626964</pub-id></element-citation><mixed-citation id=\"mc-CR55\" publication-type=\"journal\">Li, Y. et al. Automated segmentation of vertebral cortex with 3d u-net-based deep convolutional neural network. <italic toggle=\"yes\">Front. Bioeng. Biotechnol.</italic><bold>10</bold>, 996723 (2022).<pub-id pub-id-type=\"pmid\">36338129</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fbioe.2022.996723</pub-id><pub-id pub-id-type=\"pmcid\">PMC9626964</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR56\"><label>56.</label><citation-alternatives><element-citation id=\"ec-CR56\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jha</surname><given-names>N</given-names></name><etal/></person-group><article-title>Fully automated condyle segmentation using 3D convolutional neural networks</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>20590</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-022-24164-y</pub-id><pub-id pub-id-type=\"pmid\">36446860</pub-id><pub-id pub-id-type=\"pmcid\">PMC9709043</pub-id></element-citation><mixed-citation id=\"mc-CR56\" publication-type=\"journal\">Jha, N. et al. Fully automated condyle segmentation using 3D convolutional neural networks. <italic toggle=\"yes\">Sci. Rep.</italic><bold>12</bold>, 20590 (2022).<pub-id pub-id-type=\"pmid\">36446860</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-022-24164-y</pub-id><pub-id pub-id-type=\"pmcid\">PMC9709043</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12658012 PMC12658012.1 12658012 12658012 41291031 10.1038/s41598-025-29130-y 29130 1 Article Automated segmentation of the fibula from CT imaging using two-stepped deep learning in 3D U-Net architectures Nascimento J&#244;natas de Souza 2 3 Pankert Tobias 1 3 Peters Florian 1 H&#246;lzle Frank 1 Modabber Ali 1 3 Wien Mathias 2 Raith Stefan sraith@ukaachen.de 1 3 1 https://ror.org/04xfq0f34 grid.1957.a 0000 0001 0728 696X Department of Oral and Maxillofacial Surgery, RWTH Aachen University Hospital, Aachen, Germany 2 https://ror.org/04xfq0f34 grid.1957.a 0000 0001 0728 696X Institute for Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany 3 Inzipio GmbH, Aachen, Germany 25 11 2025 2025 15 478255 42020 24 7 2025 14 11 2025 25 11 2025 28 11 2025 28 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . This study proposes a fully automatic segmentation of the fibula bone from CT images for application in pre-operative planning of reconstructive surgery. The objective is to make use of new developments in the image segmentation field to optimize and reduce the costs of patient-specific surgery planning. Two different approaches are proposed to perform the fibula bone segmentation, both based on a two-step segmentation method using a 3D-UNet architecture. To account for the symmetry of the left and right fibula bones, input images of the right fibula are mirrored to the left side. The accuracy of the trained models is measured using common evaluation metrics, together with specific metrics focused on facial reconstructive surgery. Both of the described approaches achieve high-accuracy results. For the best-trained model, an average Dice score of 0.95 and Average Surface Distances below 0.31 mm is measured on the test set in the region of interest for the surgery. Both approaches are robust segmentation techniques and permit data pre-processing for further application in the context of preoperative surgical planning of procedures for facial reconstruction with bony transplants. Subject terms Anatomy Computational biology and bioinformatics Engineering Health care Mathematics and computing Medical research Universit&#228;tsklinikum RWTH Aachen (8915) Open Access funding enabled and organized by Projekt DEAL. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction In recent years, Convolutional Neural Networks (CNNs) have been used for different tasks in the medical field, including detecting tumors 1 , COVID-19 diagnosis 2 , and improving patient-specific individualization of treatments 3 . These exemplary applications show the enormous potential of using Deep Learning methods to improve different tasks in the medical field. There are, however, several applications in this domain for which the possibilities of using Artificial Intelligence-based techniques have not been explored yet. The planning of facial reconstructive surgery is an example where essential tasks could be potentially improved with such novel methods. Until now, few works on that topic have been published in literature, with only one publication known so far that deals with the segmentation of the iliac crest (i.e. a part of the pelvic bone) with the aim of use in reconstructive facial surgery 4 . There has been a remarkable increase in the use of computer tools to assist surgeons in the surgery planning procedure, which is leading to essential transformations in the field 5 , 6 . The use of Deep Learning techniques, commonly used in medical imaging, has remained scarcely studied with respect to an application in a surgical context. This can be seen in the facial reconstructive surgery planning context. Even though the use of 3D models of the donor bones during the planning procedures is shown to yield positive outcomes 7 , the usual process to acquire those 3D models is still highly manual, and time-consuming 8 , and could potentially be automated with Neural Network-based techniques. Even though fibular segmentation may appear less challenging compared to more complex anatomical structures, such as the mandible, an accurate representation of the fibular bone from CT scans is of paramount importance for a precise patient individual planning of reconstructive surgery. Although manual segmentation is feasible, it remains time-consuming and specifically subject to inter-operator variability, and thus bottleneck in an otherwise largely automated process routine of digital surgical planning. Thus, an automatic and hence repeatable and fast segmentation method, is highly valuable for clinical application. While various fully automatic CNN-based approaches for the segmentation of different bones are described in literature 9 &#8211; 11 , segmentation of the fibula bone has not been investigated, despite its major significance as a donor bone for grafting procedures 12 , 13 . In this study, we propose a fully automatic CNN-based approach to perform the segmentation of the fibula bone from CT imaging using a two-step, 3D-Unet approach. This two-step based approach using 3D U-Nets was chosen given its showed robustness and applicability in a variety of usage scenarios in medical image processing. Its application proved to be well-suited for the segmentation of other anatomical structures, including mandible 9 , the carpal bone 14 , and the pelvic bone 4 . Thus, the contribution of automated segmentation of the fibular bone is a closing link to the process chain of a fully automated pipeline of surgical planning comprising mandibular segmentation 9 , graft design 15 &#8211; 18 and finally the design of surgical guides 19 to enable a high-quality digital workflow in mandibular reconstruction using fibula-free flaps. Virtual planning in reconstructive surgery Preoperative planning is an essential part of complex surgical processes. Recently, there has been increasing research attention on this step 6 , 20 , recognizing that well-planned procedures positively impact the surgery as a whole 7 . This focus has guided the growth in the search for patient-specific planning solutions in the field of facial reconstructive surgery in recent years. Consequently, the use of computer-assisted tools in surgical preparation has gained special attention, particularly in mandibular reconstruction surgeries 21 . With the assistance of 3D modeling of the bones, a patient-specific cutting guide can be developed beforehand, along with osteosynthesis plates 22 . These advancements optimize the surgery in ways that were not possible employing just a freehand surgical approach. Various researchers compared the mandibular reconstruction surgery when made with and without the assistance of virtual planning guides, where 3D models of the relevant bones are made based on CT images. Results showed significant improvements in operative time and the overall surgery together with better comfort to the surgeon 7 , better reliability 23 , and increased symmetry compared to freehand procedures 24 . Other studies also made systematic reviews of the impact of computer-assisted surgery in maxillary reconstruction based on available literature 6 , 20 , 21 . While there are relevant aspects of criticism on the heterogeneity of the evaluation metrics used to measure the success of the procedures 6 , they also highlight the advantages of computed assisted surgeries on the field 20 , 21 . Medical image segmentation It is noted that, even though the use of virtual surgery planning has been showing great advantages in mandibular reconstruction surgery in general, the key limitation to this approach is the preparation of the 3D models to be used in the procedure. The first step of this process is to acquire three-dimensional image data of the anatomical regions of interest, typically with CT for bony structures, and then perform segmentation to annotate the bone to be modeled before the surgery. For instance, from the CT scan of the bottom region of the body, extract only the fibula that will be used as a donor bone for the surgery. The simplest and yet most commonly used segmentation method in the surgery planning context is threshold segmentation 8 . It consists of a selection of a threshold of intensity and asserting the value one for every pixel in the image above the intensity threshold, or zero otherwise 25 . The threshold can be selected either empirically or using a quantitative technique. Van Eijnatten et al. 8 made an overview of segmentation techniques for additive manufacturing of anatomical models. Their analysis shows that most segmentation applications used either a direct global threshold or some variation of this approach. However, even though this method was proven to be capable of achieving favorable results with decent geometric accuracy, the method includes an extensive manual step. This is because the cited method has a series of inconsistencies, such as not taking CT artifacts and noise into account or intensity variations between different CT scanners 10 . So far, the only way around such inconsistencies when creating 3D models suitable for surgery planning procedures or similar tasks includes manual post-processing, which is usually expensive, time-consuming, and requires a skilled and trained expert. Even in the best cases, due to subjectivity or personal variance, the final results of threshold-based segmentation may vary significantly. To overcome these problems of purely threshold-based segmentations, altas-based approaches showed remarkable performance, e.g. for femur segmentation 26 . Deep learning techniques are more recently widely studied in the context of medical image segmentation. Different studies proposed CNN-based techniques to segment bones from CT 27 , 28 or MRI images 29 , 30 . Yosinski et al. 31 showed that initializing weights of deep neural networks based on an already trained model improves the generalization performance of the model, even for non-related tasks. This feature is called Transfer Learning and was already used in the medical field, e.g., to train networks to identify COVID infection based on chest X-rays 32 , to classify brain tumor MRI images 33 , and also to classify endoscopic colonoscopy images 34 . Zhang et al. could show that even segmentation of different facial bones and corresponding landmark detection based on a heatmap approach is feasible 35 . Focusing on facial reconstructive surgery planning, Pankert et al. 9 developed a U-Net-based two-step segmentation approach to segment the mandibular bone. They conducted an extensive discussion on the advantages of using a two-step approach instead of only one U-Net and reached Dice scores of over 0.94 and Average Surface Distances lower than 0.36 mm. The present work intends to investigate the possibility of adapting this two-step segmentation method to the donor bones of facial reconstructive surgery. This way contributes to the current main issue of the planning procedure, which is the high costs and time consumption of the segmentation step. Materials and methods Dataset overview The RWTH Aachen University Hospital provided CT scans of 89 patients for this study along with corresponding manual segmentations of the fibula bone that were used in real surgery planning scenarios. Institutional approval (EK 260/20) of the local ethics committee of RWTH Aachen University Hospital was obtained. All methods were carried out in accordance with relevant guidelines and regulations. These segmentations are referred to as ground truth in the following. The age of the patients ranged from 11 to 80 years with a mean of 55.9 and a median of 59 years. 37 patients were male and 52 female. The dataset provided was not fully homogeneous and partially contained inconsistencies, so the data was carefully investigated for those flaws, and sets were excluded from the subsequent study in case they did not meet the inclusion criteria, that was defined as a complete manual segmentation with correct spatial alignment to the corresponding CT imaging data. Additionally, not all patients had both fibula bones segmented, but only one side. In these cases, the available side was chosen for subsequent work. The included datasets were subsequently used in the convolutional neural networks training that is detailed in further sections. After the exclusion of the inconsistent cases, there were 13 data sets with only the right fibula, 26 data sets with only the left fibula, and 36 data sets with both fibulae segmented, summing up to 111 fibulae from 75 subjects. From these, 10 data sets with both fibulae segmented were held back as an independent test set, to evaluate the proposed segmentation pipelines. This dataset was deemed to be sufficiently large to produce satisfying results, as other studies with two-staged approaches showed to be applicable with smaller datasets, e.g. on carpal bones 14 and in a preliminary study on training set sizes on the mandibular bone 9 . However, considering the wide range of ages and the inclusion of both sexes, the relatively small dataset size is a limitation of the study. Segmentation approach In this work, we present two different methods to automatically segment the fibula bone from the CT data. The development of these two different approaches was done because of the dataset specificity that the bones of interest appear twice in the field of view. Both of these propositions are modifications of methods previously studied in literature. Wang et al., for instance, proposed a two-step segmentation framework to segment organs at risk of the head 36 . The method uses a two-step segmentation process that starts with a localization step that is applied to the whole field of view of the imaging data. The purpose of this initial step is to locate the region of interest around the anatomical structures of relevance in order to crop the field of view around this detected region and to allow for a subsequent detailed segmentation. Subsequently, a second step, that may be called a refinement step, is applied that can focus on the structures of choice and allows to segment these particular structures in detail with high precision by optimally using the available model capability. This method proved to be beneficial in a variety of different use cases, such as the segmentation of mandibular bones 9 , the pelvic bone 4 or the wrist bone 14 , as shown in previous research. In this application, two U-nets are independently trained for each step of the task. For both steps, the same input and output resolution was used. Therefore, the present study proposes to address the fibula bone segmentation on the available dataset. The baseline method is similar to the approaches described previously: Two U-Nets are trained to perform two different tasks. The purpose of the first U-Net is to segment the bone of interest from the full CT image, while the second is trained to segment the bone only from the bounding box where the bone is present. Bilateral segmentation The first proposed method consists of a two-step segmentation, where the first step is responsible for identifying both of the fibulae at the same time. Thus, the first step is trained on the whole CT image, down-sampled to the resolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$144 \\times 144 \\times 288$$\\end{document} voxels, and the output is the left and the right fibulae, which will be used for the second step of segmentation. To identify the left and the right fibulae, the output of the U-Net is converted into 3D surface objects using the marching cubes algorithm 37 . We assume that the two largest output meshes are the fibulae bones, while the rest are discarded. Based on their orientation in space, we label the fibulae meshes as left and right. Similar to 36 and 9 , the neural networks for the second step are trained only on the region of interest of the CT, for this application resampled to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$80 \\times 80 \\times 960$$\\end{document} , not on the whole data. However, since there are two fibulae, one on the right side and one on the left, two different U-Nets would need to be trained for this step. To circumvent this issue, we propose to mirror all right fibulae along the mid-plane of the field of view to the left side to train the second-step segmentation model on all fibulae simultaneously. Hence, the entire pipeline consists of the following: Initially, the first step of the segmentation identifies both fibulae based on the full CT. Then the output results are post-processed and transformed into 3D meshes. Then the original CT is cropped to the bounding box of both right and left outputs, and the right fibula and cropped CT are mirrored. Lastly, the second-step model segments each of the fibulae from the cropped CT, then once more the output is post-processed and the right fibula output is re-mirrored. A diagram of this approach is shown in Fig. 1 . Fig. 1 Bilateral segmentation approach overview: The first step takes the original CT data (shown in the diagram as threshold segmentations for visual clarity) and segments both fibula bones from the whole CT, then the second step segments each fibula bone from the CT cropped to the region of interest. Unilateral segmentation Even though the method presented above is solid, and produces remarkable results, as will be shown in the Results chapter, a relevant issue with this implementation is that for the first step model to be trained, ground truth data for both left and right fibula need to be available, otherwise, it can not be used for training. This is specifically problematic for the available dataset since there are only 36 patients with both fibulae segmented in the whole set, due to the nature of the data coming from real clinical cases for surgical planning. As the second step is trained for each fibula individually, all the data can be used in the training procedure, and no samples have to be discarded. For the first step of the bilateral approach, data from 39 patients were excluded, which is 52% of the complete set. Therefore, we propose an additional approach for the first step, where potentially all the data can be used for training the U-Net models. The proposed method consists of using all the fibulae individually for training, similar to what is already done in the second step. Instead of segmenting both fibulae from the complete CT scan, the proposition is to split the CT in half along the mid-plane of the field of view, with each half containing only one fibula.This does not imply an exact alignment with the anatomical mid-sagittal plane, but in our examples, as all patients were lying centric enough on the bench with sufficient distance between their legs, thus this approach showed to be valid on the given data. Subsequently, the first step segmentation model is trained to segment the fibula based only on this half section of the image. To avoid training two different neural networks for the same task, the same mirroring technique used in the second step is also applied to this modified first step. Hence, the right side of the CT, which was cropped, is also mirrored along with the ground truth in this step and only one neural network is trained in this step to identify left fibulae from the left part of CTs. A different resolution is also necessary for this first step, since the region that will be input for the U-Net is no longer based on the full CT scan. To maintain the relative proportion, the proportion of voxels from the y-axis to the x-axis is doubled, and the proportion from the y-axis to the z-axis is kept the same. The chosen resolution therefore is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$96 \\times 192 \\times 320$$\\end{document} . In conclusion, the method presented here involves first splitting the CT data in half on the lateral axis, then using the same first-step model to segment both the left and the mirrored right side of the fibulae. Finally, to apply the second step model to segment both fibulae from the CT images cropped only to the bounding box of the fibulae found in the first step. Figure 2 presents an overview of this approach. Fig. 2 Unilateral segmentation approach overview: The CT dataset is divided into left and right sections (shown in the diagram as threshold segmentations for visual clarity), and then the first step segments each fibula bone from its respective side, and finally the second step segments each fibula bone from the CT cropped to the region of interest. Dataset distribution After preprocessing of the raw imaging data and a technical check for data integrity, all available data was used for the training of the respective networks and divided into a training and validation set. For the training of the bilateral first step of this model, 22 could be made available, split with a 9 to 1 ratio into training set (n=20) and a validation set (n=2). The first step of the unilateral segmentation could use a database of 87 fibulae, where 80 could be used in training and 7 for validation. The second stage could rely on dataset of 85 fibulae, split into a dataset training (n=75) and validation (n=7). Transfer learning As transfer learning has shown to be beneficial in related research 31 &#8211; 34 , both approaches proposed in this study use it. In this application, first, the second-step U-Net is trained from scratch, as this is expected to be a computationally easier task. Subsequently, transfer learning to the first step is used, as this showed superior results in preliminary work on other segmentation tasks. This allows for expanding the more detailed training results of the second step to the whole field of view of the CT, hence this approach is referred to as Expansion Transfer Learning (ETL) 14 . U-Net architecture The U-Net architecture used for the experiments is a variation of the three-dimensional U-Net described in 38 . It has ten convolutional layers in the encoding path, with four max pooling layers in between. Following a common pattern seen in the literature 9 , 39 , it doubles the number of feature maps for every pooling step. The network uses \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$3 \\times 3 \\times 3$$\\end{document} receptive fields and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2 \\times 2 \\times 2$$\\end{document} max pooling layers. Furthermore, the decoding path consists of nine convolutional layers and five upsampling layers, which simply repeat the elements of the input tensor by an upsampling factor in every dimension. The upsampling factor used in the described architecture was \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2 \\times 2 \\times 2$$\\end{document} , which means the output of every upsampling layer is double its input for every dimension. An overview of the 3D U-net described above can be seen in Table 1 . All networks described in this paper were trained using an Adam optimizer 40 and a Dice loss. We did not implement data augmentation techniques in our presented methodology. While preliminary experiments were conducted with various augmentation approaches including spatial deformation, scaling, intensity variations, and noise addition, we did not observe significant improvements in model performance that would have justified their inclusion in our final implementation. Table 1 3D U-Net architecture. Type Output feature maps Encoding path Input layer 1 Conv 16 Conv + Pool + Conv 32 Conv + Pool + Conv 64 Conv + Pool + Conv 128 Conv + Pool + Conv 256 Conv 512 Decoding path Conv 512 Conv + Upsampling + Conv 256 Conv + Upsampling + Conv 128 Conv + Upsampling + Conv 64 Conv + Upsampling + Conv 32 Conv 1 Output Layer 1 No individual image normalization was used to standardize intensity values. However, the standard range of Hounsfield units (from -1024 to +3071 HU) was clipped and mapped to floating point numbers in the range from 0.0 to 1.0 for further processing in the neural network. Post-processing Both proposed segmentation methods include a post-processing step of the model&#8217;s predictions. This stage occurs after all the neural networks outputs and transform the binary image output to the 3D model mesh, that is subsequently used in the surgical procedure. The first step of post-processing is a binary erosion of the data 41 and is followed by the application of the Gaussian filter 42 to add smoothness. Finally, the binary data is transformed to a mesh using the marching cubes algorithm on the predicted data 37 . Evaluation metrics To evaluate the trained models in this study, we use three metrics commonly used in medical image analysis in similar applications. The first metric is the Dice Score (DS) 43 , an overlapping metric that compares the result segmentation and the ground truth. For a ground truth segmentation represented by the set of voxels A and a predicted segmentation represented by the set of voxels B , the Dice Score can be formalized according to Equation 1 . 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\mathscr{D}\\mathscr{S} = 2 \\cdot \\frac{A \\cap B}{ A + B } \\end{aligned}$$\\end{document} Additionally, we use two distance metrics, the Average Surface Distance (ASD) 44 , and the 95% Hausdorff Distance (95% HD) 45 which measure the average, and the 95% highest distance between the ground truth and the model prediction respectively. Restricted evaluation of transplantable bone During the facial reconstructive surgery with the use of the fibula-free flap, 6 cm or 7 cm from the top and bottom of the fibula are preserved to avoid knee or ankle instability 46 &#8211; 48 . Therefore, only the central part of the fibula is effectively used in the procedure and needs to be as well segmented as possible for the present application. Consequently, the proposition here is to evaluate with the same metrics as defined beforehand, removing the top and bottom 7 cm of the fibula bone. This removal can be seen in Fig. 3 . This removal is done on both ground truth segmentation and the output of the proposed framework. Then the cropped meshes are again compared using Dice Score, Average Surface Distance, and 95% Hausdorff Distance. In this study, these metrics are additionally labeled as Region of Interest (ROI) metrics to differentiate from the standard implementations on the whole fibular bone. Fig. 3 Fibula bone cropping example: the image shows the top and bottom 7cm being removed from the evaluation procedure. Results The results section is split into a qualitative part and a quantitative part. Both refer to the final prediction, i.e.&#160;the output of the respective second U-Nets. Qualitative results Bilateral segmentation approach Figure 4 shows predictions from the trained models, each aside from its respective ground truth. The two neural networks combined succeed in identifying not just the fibula bone region and a fibula-like object, but the fibulae are visually close to the corresponding ground truth segmentation. Visually, all the bottom edges and central regions look well segmented, whereas the top parts are not perfect for all cases. The first and second fibulae, especially, have flaws in the segmentation of its top regions, where the ground truth has it well segmented. Fig. 4 Bilateral Segmentation approach: Predictions (green) and ground truths (red) examples. Unilateral segmentation approach The predictions obtained using the unilateral segmentation approach can be seen in Fig. 5 . These results were generated for the same fibulae as the ones in Fig. 4 , therefore a direct comparison is possible. Fig. 5 Unilateral segmentation approach: predictions (green) and ground truths (red) examples. Visually, the predictions are closely aligned with the ground truths, especially on the central region, which is the one of biggest importance for facial reconstructive surgery, and also for the bottom region. The first and second fibulae show minor flaws in the top region segmentation, even though the segmentation looks relatively better than the ones achieved using the bilateral segmentation approach. Figure 6 displays a closer view of one example of prediction and highlights the poor results on the top region and the low deviation results on the central and bottom regions. Fig. 6 Detail view of a prediction in relation to ground truth data in color-coded 3D surface comparison. (left: proximal region of the fibula (head), middle: shaft of the fibula, right: lateral region of the fibula (ankle). The image shows an example result of the bilateral approach. Quantitative results Table 2 displays the quantitative evaluation of the trained models on the test set. Table 2 Evaluation of all models on the whole test set based on Dice Score (DS), Average Surface Distance (ASD) 95% Hausdorff Distance (HD) both on the whole fibular bone, and restricted to the Region of Interest (ROI). Approach DS ASD 95% HD ROI DS ROI ASD ROI 95% HD Bilateral segmentation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.85 \\pm 0.09$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.91 \\pm 1.55$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$5.45 \\pm 14.81$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.94 \\pm 0.02$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.36 \\pm 0.12$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.89 \\pm 0.59$$\\end{document} Unilateral segmentation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.87 \\pm 0.05$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.58 \\pm 0.25$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2.15 \\pm 0.84$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.95 \\pm 0.02$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.31 \\pm 0.13$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0.79 \\pm 0.63$$\\end{document} The unilateral approach performs overall better in all metrics. When evaluating the metrics on the whole fibula, the achieved Dice Scores are 0.85 and 0.87 for the bilateral segmentation and the unilateral method respectively, while the Average Surface Distance and 95% Hausdorff Distance show an even bigger difference between the two approaches. It is also important to note the high standard deviation of the bilateral segmentation evaluation. However, for the Region of Interest (ROI) metrics, the difference between the two approaches is less significant. Both methods achieve high DS values of 0.94 (bilateral approach) and 0.95 (unilateral approach), the ASD and 95% HD metrics are at 0.36&#160;mm to 0.31&#160;mm and 0.89&#160;mm to 0.79&#160;mm, respectively (Table 2 ). The bilateral approach suffers from a robustness problem visible in the high standard deviations. For one of the test cases, the first step of the bilateral approach failed to predict one of the fibulae as a continuous bone. Due to filtering in the post-processing, the inferior part of the fibula was discarded, resulting in a too small field of view for the second segmentation step (Fig. 7 ). Fig. 7 The low accuracy of the first-step segmentation in the bilateral approach leading to an incorrect bounding box for the second segmentation step. The outputs are displayed after conversion to mesh representations and before filtering out unconnected parts. Note: this is an exceptionally bad case and not the standard of our study. Figure 8 shows the statistical distribution of the ASD evaluation on the test set for both the bi- and unilateral approaches. It is important to note that evaluating the same prediction only in the region of interest provides better results. Fig. 8 Distribution of the average surface distances (ASD) on the full bone or within the region of interest (ROI) for bi- and unilateral segmentations. For the histogram of the bilateral segmentations (in red), the outlier at 7.43 mm has been omitted. The unilateral approach solves the problem of high differences in the outliers. This variant achieves an average ASD and 95% HD value of 0.58&#160;mm and 2.15&#160;mm, respectively. Both standard deviations are drastically lower as well, which are 0.25&#160;mm and 0.84&#160;mm for the whole bone. Running the segmentations, including preprocessing of the DICOM datasets and all post-processing steps took on average 50&#160;s for the bilateral and 58&#160;s for the unilateral approach per dataset on a PC with an Nvidia GeForce RTX 2080 Ti and an Intel Core i9-7900X . Discussion Even though the fibular bone is an important anatomy for diverse applications in the medical field, it is still not covered by research works on automatic segmentation of this bone, especially using AI techniques. Though targeting a long bone of the lower extremity, the present work is particularly relevant for the field of maxillofacial reconstruction, where the fibula is commonly used as a donor site for autologous bone transplantation, and fast and reliable segmentation can significantly improve the preoperative workflow. Hence, the experiments conducted in this work are of high value to this clinical domain, as the proven applicability of this method in the given context demonstrates the efficiency for segmenting the desired parts of the fibula from CT images. Therefore, the experiments conducted in this work are of high value to the field, since their remarkable results attest to the efficiency of the proposed methods in segmenting the desired bone from CT images. These results are especially good not just for the use case but have the potential to be used in different applications, such as fracture detection 49 50 or disease prediction 51 . Both methods proposed are modifications of the approach described in 9 , 36 . Overall, the results showed a high quality of the segmentation for both proposed approaches. When evaluating the predictions on the whole bone, the average of the Dice Score metric found for the two cases is over 0.84. Overall, the bilateral approach showed decreased robustness, as visible in the high standard deviations, due to the presence of outliers. The unilateral segmentation approach, by contrast, solves this problem and shows a significantly improved robustness. This result is a strong indication of how the unilateral approach deals better with difficult cases than the bilateral segmentation approach. Even in CT scans for which the bilateral approach has difficulties in performing the first step segmentation, this method provides a segmentation that is sufficient to correctly detect the region of interest for the second step input. That the proximal part of the fibula shows weaker segmentation accuracy can be treated as irrelevant for the intended purpose of autologous transplantation. However, for potential application of this segmentation to other medical indications, the head of the fibula could deserve a more careful investigation, either by improving the ground truth segmentations in this particular region or by specific post-processing with shape-based refinement Therefore, the visual analysis corroborates the statistics, which attest to an overall improvement of the metrics when using the unilateral approach when evaluating the whole bone. This is expected for two main reasons: The first one is directly related to the relative resolution of the down-sampling for the first step. Since the CT scan is divided in half for the unilateral approach, and the resolution used is kept proportionally, the relative down-sampling is lower for this case. Thus, for the bilateral method, more information is lost, which affects directly the training procedure. The second reason is related to the dataset availability. As described beforehand, not every patient in the data used for training has both fibulae segmented as a ground truth. This is a problem for the bilateral approach since it needs both bones to have ground truths to allow better training. Thus, the number of samples used for training in this method is 67.5% lower in comparison to the unilateral method. These factors combined explain how the split approach can deal better with the outliers, and have overall better results, even though the second step network is the same. All these results together show that the unilateral approach is overall a better method to perform the fibula bone segmentation task than the bilateral approach, especially because of its higher robustness, and its remarkably better results for the metrics that evaluate the whole bone. Nevertheless, both the quantitative and qualitative analyses suggest that the main difference overall between the two methods is outside the region of interest. When evaluating both approaches only on the region of interest, the difference between them is shown to be significantly lower. Even the worst results of both metrics show Average Surface Distances below 1&#160;mm in the fibula region used for the surgery. The other metrics show similar results, since the average of the Dice Score and 95% Hausdorff distance metrics are respectively 0.94 and below 1&#160;mm for both approaches, and even the worst outliers are within a good range considering the task. These errors are all within the order of magnitude of the resolution of the underlying CT scans. Thus, in summary, both two-step approaches showed to be applicable and yield remarkable accuracy, leveraging the full potential of the available hardware by applying a coarse segmentation in a first stage, followed by a detailed segmentation around the detected area of interest. In this approach, the advantages of using one common model architecture and training dataset could be combined with the gain in relative resolution in the focused second step. For the application of the presented segmentation method in the context of surgical planning, the required accuracy depends on many different factors. Generally, the more accurate the segmentation the better for the surgical outcome. Accuracy is limited by several factors, including imaging, segmentation and manufacturing of parts created based on this data. Van den Broeck et al. report a mean error of 0.48&#160;mm between CT scans and optical scans of the tibial bone diaphysis 52 , showing that the imaging already contains some errors in comparison to the real clinical application. Wallner et al. report an inter-observer Dice score of 0.94 between the manual segmentation of the mandible bone from CT imaging from two different human annotators 53 . Dice scores much higher than this might indicate an overfitting of the AI models to a specific human annotator instead of correctly generalizing to the segmentation problem. In a work by Kim et al., the manufacturing accuracy of surgical guides based on different manufacturing techniques was measured, showing mean errors ranging from 0.09&#160;mm for SLA printing to 0.31&#160;mm for Multijet printing (MJP) 54 . These results are of high importance for the present work, since they show that even though the bilateral segmentation approach has problems regarding robustness when considering only the region that is considered in the facial reconstructive surgery, the results are outstanding for all cases. Even though there are no studies that apply a similar approach to perform the segmentation of fibular bones, it is possible to compare the achieved results to other bone segmentation from volumetric images. For instance, when performing the vertebral cortical segmentation from chest CT scans using a 3D U-Net-based approach, 55 obtains Dice Scores of only 0.71, which is considerably lower compared to the results achieved in the present work. 56 proposed a cascaded approach to segment condyles from CT scans, also using 3D U-Nets. The results achieved in this study were of high quality, achieving over 0.93 Dice Score values and Hausdorff Distances lower than 2.45&#160;mm. Focusing on the reconstructive facial surgery planning application, Pankert et al.&#160;developed a fully automatic framework to segment the mandibular bone and obtained averages of 0.94, 0.35&#160;mm, and 0.97&#160;mm for the Dice Score, Average Surface Distance, and 95% Hausdorff Distance respectively, and attested its significance and possible applicability in surgical planning scenarios 9 . All the discussed comparisons have to be done carefully since even though the metrics are the same, the segmented bones and the available datasets are significantly different. However, literature results attest to what is a good segmentation in the medical image field. In fact, for the region of interest, the results obtained for this work are either better or at least similar for both presented methods. The complete process of segmentation (including pre- and post-processing) took less than one minute on average on a workstation PC with contemporary hardware. Even though the fibular bone might be an easier task for manual segmentation than other anatomical structures, the time for manual segmentation is expected to be much longer. Though there is no published data available on the average times for manual segmentation of the fibula from CT scans, and comparisons with the segmentation of other anatomical structures are not directly applicable. Based on internal consultation with personnel experienced in manual segmentation, an estimated duration of 5 to 10 minutes per fibula was reported. It is important to note that this estimate refers to trained professionals with substantial experience; for less experienced users or those performing the task only occasionally, a significant learning curve can result in considerably longer times. While this working time is lower than for more complex anatomical structures or regions with extensive imaging artifacts, it still represents a relevant burden in an already packed clinical workflow. Moreover, with an automated segmentation using a CNN, any inter-observer differences may be omitted. Thus, the AI approach presented in this work, enables the integration into a comprehensive process chain of surgical planning, which involves more cumbersome tasks, and profits significantly from an objective and reproducible acquisition of bony geometry from CT data. The segmented virtual representation of fibular bones can be directly integrated into computer-assisted planning workflows, such as virtual preoperative planning of surgical reconstruction of the mandible or midface bones in continuity defects, as is required e.g. after tumor removal, bone necrosis, or excessive trauma, where an autologous transplant from the fibular bone in a so-called fibula free flap proved its clinical value 6 , 7 , 20 , 21 . The geometric accuracy of the virtual surfaces of the digital bone models is essential for the planning of the transplant to match the defect dimensions and thus allow for optimal functional and aesthetic rehabilitation. This work focuses on the surgical planning of fibula bone transplants in reconstructive surgery. A critical factor for clinical success is the accurate identification and integration of supporting blood vessels, as visible in CT angiography. While the present study concentrates on bone geometry, a natural extension of the automated segmentation approach would thus be the integration of vascular structures. Enabling automatic detection of relevant vessels would allow this information to directly inform clinical decision-making without requiring additional manual input. However, this task is considerably more challenging due to the high anatomical variability of vascular structures and would likely necessitate a different study design, potentially with access to larger and more diverse datasets. It is important to note some limitations of the study. The used dataset was prepared to focus on a facial reconstructive surgery context, therefore flaws in unimportant regions for the surgery were ignored when modeling the segmentations. This directly affected the predictions achieved by the trained models. Furthermore, the low number of data available for training, especially on the bilateral segmentation approach, was probably one of the causes of the model&#8217;s lack of robustness for this method. Thus, we expect the overall results to improve significantly when more data is available for training. More data could also be generated by applying data augmentation to the existing datasets. Even though within the limitation of the preliminary studies conducted prior to the present work did not yield a discernible improvement in performance, a further ablation on augmentations techniques and settings could achieve an improvement of the training performance. In future work, we intend to apply similar methods to segment other donor bones used in the facial reconstructive surgery procedure. Furthermore, comparisons of our CNN-based approach to Atlas-based methods 26 might yield relevant insights to the technical specifics of the different approaches in this particular field. Conclusion This study presented two methods to automatically segment fibula bones from full-resolution CT scans. Both propositions are based on two-step segmentations, where the second step predicts the output based on the region of interest found by the first step. The bilateral approach&#8217;s first step does a coarse segmentation of both fibulae from a full-resolution CT at once, while the unilateral approach first splits the CT into two lateral halves to simplify the task by increasing the relative image resolution and the usable training data. Both alternatives presented in this paper have the potential to be used in clinical surgical planning scenarios after integration into approved software as a medical device. The framework is capable of producing accurate segmentation of the fibula bone from the CT scans with a significant improvement in speed in comparison to established (semi-) manual segmentation techniques, and can be directly used as input data for the computer-assisted planning procedure. Even eventual imprecision can be easily manually corrected before the procedure, which would still be a better and overall cheaper approach than the state-of-the-art threshold segmentation-based techniques. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions S.R. and T.P. designed the study. S.R., T.P., and M.W. elaborated the methodology. J.S.N. performed technical implementation of the two-stage 3D U-Net architecture. F.P., F.H., and A.M. supervised the clinical aspects of the study, ensuring the relevance to surgical applications. M.W. provided guidance on data analysis. S.R. and J.S.N. wrote the main manuscript text, while S.R., M.W. and S.J. provided critical revisions. J.S.N and S.R. prepared the figures. All authors reviewed and approved the final manuscript. S.R., T.P., and A.M. are co-founders of Inzipio, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital. Funding Open Access funding enabled and organized by Projekt DEAL. The authors declare that no funds, grants, or other support were received during the preparation of this manuscript. Data availability The raw medical imaging datasets analyzed during this study are not publicly available due to privacy concerns, as per the guidelines of the ethics committee, which permits access to the original data only for a specific group of named researchers. However, evaluation data and statistical analyses derived from it are available from the corresponding author upon reasonable request. Code availability The code for evaluation of the geometry data is available as part of the supplementary material of this publication. The base implementation of the utilized 3D U-Net is available under https://github.com/smatzek/3DUnetCNN . Further details of the implementation and methodological description, as well as specific code parts used in this study for the training and inference pipelines, are available from the corresponding author upon reasonable request. Declarations Conflict of interest S.R., T.P., and A.M. are co-founders of Inzipio GmbH, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital. J.S.N., S.J., M.W., F.P., and F.H. have no financial or proprietary interests in any material discussed in this article. Ethical approval Institutional approval (EK 260/20) of the local ethics committee of RWTH Aachen University Hospital was obtained. All methods were carried out in accordance with relevant guidelines and regulations. Informed consent Due to the retrospective nature of the study, the Independent Ethics Committee of the Faculty of Medicine of RWTH Aachen University Hospital waived the need of obtaining informed consent. Competing interests S.R., T.P., and A.M. are co-founders of Inzipio, a spin-off from the department of Oral and Maxillofacial Surgery at RWTH Aachen University Hospital. References 1. Mahmud MI Mamun M Abdelgawad A A deep analysis of brain tumor detection from MR images using deep learning networks Algorithms 2023 16 176 10.3390/a16040176 Mahmud, M. I., Mamun, M. &amp; Abdelgawad, A. A deep analysis of brain tumor detection from MR images using deep learning networks. Algorithms 16 , 176. 10.3390/a16040176 (2023). 2. Poongodi M Diagnosis and combating COVID-19 using wearable oura smart ring with deep learning methods Pers. Ubiquit. Comput. 2021 26 25 35 10.1007/s00779-021-01541-4 PMC7908947 33654480 Poongodi, M. et al. Diagnosis and combating COVID-19 using wearable oura smart ring with deep learning methods. Pers. Ubiquit. Comput. 26 , 25&#8211;35. 10.1007/s00779-021-01541-4 (2021). 10.1007/s00779-021-01541-4 PMC7908947 33654480 3. Wen Z Deep learning in digital pathology for personalized treatment plans of cancer patients Semin. Diagn. Pathol. 2023 40 109 119 10.1053/j.semdp.2023.02.003 36890029 Wen, Z. et al. Deep learning in digital pathology for personalized treatment plans of cancer patients. Semin. Diagn. Pathol. 40 , 109&#8211;119. 10.1053/j.semdp.2023.02.003 (2023). 36890029 10.1053/j.semdp.2023.02.003 4. Raith S Segmentation of the iliac crest from CT-data for virtual surgical planning of facial reconstruction surgery using deep learning Sci. Rep. 2025 15 1097 10.1038/s41598-024-83031-0 39773990 PMC11707128 Raith, S. et al. Segmentation of the iliac crest from CT-data for virtual surgical planning of facial reconstruction surgery using deep learning. Sci. Rep. 15 , 1097 (2025). 39773990 10.1038/s41598-024-83031-0 PMC11707128 5. van Baar GJ Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review J. Clin. Med. 2021 10 1226 10.3390/jcm10061226 33809600 PMC8002284 van Baar, G. J. et al. Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review. J. Clin. Med. 10 , 1226 (2021). 33809600 10.3390/jcm10061226 PMC8002284 6. Chan TJ Long C Wang E Prisman E The state of virtual surgical planning in maxillary reconstruction: A systematic review Oral Oncol. 2022 133 106058 10.1016/j.oraloncology.2022.106058 35952582 Chan, T. J., Long, C., Wang, E. &amp; Prisman, E. The state of virtual surgical planning in maxillary reconstruction: A systematic review. Oral Oncol. 133 , 106058 (2022). 35952582 10.1016/j.oraloncology.2022.106058 7. Culi&#233; D Virtual planning and guided surgery in fibular free-flap mandibular reconstruction: A 29-case series Eur. Ann. Otorhinolaryngol. Head Neck Dis. 2016 133 175 178 10.1016/j.anorl.2016.01.009 26876743 Culi&#233;, D. et al. Virtual planning and guided surgery in fibular free-flap mandibular reconstruction: A 29-case series. Eur. Ann. Otorhinolaryngol. Head Neck Dis. 133 , 175&#8211;178. 10.1016/j.anorl.2016.01.009 (2016). 26876743 10.1016/j.anorl.2016.01.009 8. van Eijnatten M Ct image segmentation methods for bone used in medical additive manufacturing Med. Eng. Phys. 2017 10 24 25 10.1016/j.medengphy.2017.10.008 29096986 van Eijnatten, M. et al. Ct image segmentation methods for bone used in medical additive manufacturing. Med. Eng. Phys. 10 , 24&#8211;25. 10.1016/j.medengphy.2017.10.008 (2017). 10.1016/j.medengphy.2017.10.008 29096986 9. Pankert T Mandible segmentation from CT data for virtual surgical planning using an augmented two-stepped convolutional neural network Int. J. Comput. Assist. Radiol. Surg. 2023 10.1007/s11548-022-02830-w 36637748 PMC10363055 Pankert, T. et al. Mandible segmentation from CT data for virtual surgical planning using an augmented two-stepped convolutional neural network. Int. J. Comput. Assist. Radiol. Surg. 10.1007/s11548-022-02830-w (2023). 36637748 10.1007/s11548-022-02830-w PMC10363055 10. Minnema J Ct image segmentation of bone for medical additive manufacturing using a convolutional neural network Comput. Biol. Med. 2018 103 130 139 10.1016/j.compbiomed.2018.10.012 30366309 Minnema, J. et al. Ct image segmentation of bone for medical additive manufacturing using a convolutional neural network. Comput. Biol. Med. 103 , 130&#8211;139. 10.1016/j.compbiomed.2018.10.012 (2018). 30366309 10.1016/j.compbiomed.2018.10.012 11. Raudaschl PF Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015 Med. Phys. 2017 44 2020 2036 10.1002/mp.12197 28273355 Raudaschl, P. F. et al. Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015. Med. Phys. 44 , 2020&#8211;2036 (2017). 28273355 10.1002/mp.12197 12. Hidalgo DA Pusic AL Free-flap mandibular reconstruction: A 10-year follow-up study Plast. Reconstr. Surg. 2002 110 438 449 10.1097/00006534-200208000-00010 12142657 Hidalgo, D. A. &amp; Pusic, A. L. Free-flap mandibular reconstruction: A 10-year follow-up study. Plast. Reconstr. Surg. 110 , 438&#8211;449. 10.1097/00006534-200208000-00010 (2002). 12142657 10.1097/00006534-200208000-00010 13. H&#246;lzle F Franz EP von Diepenbroick VH Wolff K-D Evaluation der unterschenkelarterien vor mikrochirurgischem fibulatransfer Mund Kiefer Gesichtschir. 2003 7 246 253 10.1007/s10006-003-0486-8 12961076 H&#246;lzle, F., Franz, E. P., von Diepenbroick, V. H. &amp; Wolff, K.-D. Evaluation der unterschenkelarterien vor mikrochirurgischem fibulatransfer. Mund Kiefer Gesichtschir. 7 , 246&#8211;253. 10.1007/s10006-003-0486-8 (2003). 12961076 10.1007/s10006-003-0486-8 14. Raith, S. et al. Multi-label segmentation of carpal bones in mri using expansion transfer learning. Phys. Med. Biol. (2025). 10.1088/1361-6560/adabae 39823747 15. Raith S Introduction of an algorithm for planning of autologous fibular transfer in mandibular reconstruction based on individual bone curvatures Int. J. Med. Robot. Comput. Assist. Surg. 2018 14 e1894 10.1002/rcs.1894 29423929 Raith, S. et al. Introduction of an algorithm for planning of autologous fibular transfer in mandibular reconstruction based on individual bone curvatures. Int. J. Med. Robot. Comput. Assist. Surg. 14 , e1894. 10.1002/rcs.1894 (2018). 10.1002/rcs.1894 29423929 16. Modabber A Evaluation of a novel algorithm for automating virtual surgical planning in mandibular reconstruction using fibula flaps J. Craniomaxillofac. Surg. 2019 47 1378 1386 10.1016/j.jcms.2019.06.013 31331845 Modabber, A. et al. Evaluation of a novel algorithm for automating virtual surgical planning in mandibular reconstruction using fibula flaps. J. Craniomaxillofac. Surg. 47 , 1378&#8211;1386. 10.1016/j.jcms.2019.06.013 (2019). 31331845 10.1016/j.jcms.2019.06.013 17. Hagen, N. et al. A user-friendly software for automated knowledge-based virtual surgical planning in mandibular reconstruction. Journal of Clinical Medicine 14 , 10.3390/jcm14134508 (2025). 10.3390/jcm14134508 PMC12249504 40648882 18. Guo Y Automated planning of mandible reconstruction with fibula free flap based on shape completion and morphometric descriptors Med. Image Anal. 2025 102 103544 10.1016/j.media.2024.103544 40132366 Guo, Y. et al. Automated planning of mandible reconstruction with fibula free flap based on shape completion and morphometric descriptors. Med. Image Anal. 102 , 103544. 10.1016/j.media.2024.103544 (2025). 40132366 10.1016/j.media.2025.103544 19. &#352;imi&#263; L Kopa&#269;in V Mumlek I Butkovi&#263; J Zub&#269;i&#263; V Improved technique of personalised surgical guides generation for mandibular free flap reconstruction using an open-source tool Eur. Radiol. Exp. 2021 5 30 10.1186/s41747-021-00229-x 34318382 PMC8316532 &#352;imi&#263;, L., Kopa&#269;in, V., Mumlek, I., Butkovi&#263;, J. &amp; Zub&#269;i&#263;, V. Improved technique of personalised surgical guides generation for mandibular free flap reconstruction using an open-source tool. Eur. Radiol. Exp. 5 , 30. 10.1186/s41747-021-00229-x (2021). 34318382 10.1186/s41747-021-00229-x PMC8316532 20. van Baar, G. J.&#160;C. et al. Accuracy of computer-assisted surgery in maxillary reconstruction: A systematic review. J. Clin. Med. 10 , 10.3390/jcm10061226 (2021). 10.3390/jcm10061226 PMC8002284 33809600 21. Powcharoen W Yang W-F Li KY Zhu W Su Y-X Computer-assisted versus conventional freehand mandibular reconstruction with fibula free flap: a systematic review and meta-analysis Plast. Reconstr. Surg. 2019 144 1417 1428 10.1097/PRS.0000000000006261 31764662 Powcharoen, W., Yang, W.-F., Li, K. Y., Zhu, W. &amp; Su, Y.-X. Computer-assisted versus conventional freehand mandibular reconstruction with fibula free flap: a systematic review and meta-analysis. Plast. Reconstr. Surg. 144 , 1417&#8211;1428 (2019). 31764662 10.1097/PRS.0000000000006261 22. Wilde F Multicenter study on the use of patient-specific cad/cam reconstruction plates for mandibular reconstruction Int. J. Comput. Assist. Radiol. Surg. 2015 10 2035 2051 10.1007/s11548-015-1235-2 25843949 Wilde, F. et al. Multicenter study on the use of patient-specific cad/cam reconstruction plates for mandibular reconstruction. Int. J. Comput. Assist. Radiol. Surg. 10 , 2035&#8211;2051. 10.1007/s11548-015-1235-2 (2015). 25843949 10.1007/s11548-015-1193-2 23. Al-Sabahi, M.&#160;E. et al. Aesthetic reconstruction of onco-surgical mandibular defects using free fibular flap with and without CAD/CAM customized osteotomy guide: A randomized controlled clinical trial. BMC Cancer 22 , 10.1186/s12885-022-10322-y (2022). 10.1186/s12885-022-10322-y PMC9717507 36460978 24. Bartier S Mazzaschi O Benichou L Sauvaget E Computer-assisted versus traditional technique in fibular free-flap mandibular reconstruction: A ct symmetry study Eur. Ann. Otorhinolaryngol. Head Neck Dis. 2021 138 23 27 10.1016/j.anorl.2020.06.011 32620425 Bartier, S., Mazzaschi, O., Benichou, L. &amp; Sauvaget, E. Computer-assisted versus traditional technique in fibular free-flap mandibular reconstruction: A ct symmetry study. Eur. Ann. Otorhinolaryngol. Head Neck Dis. 138 , 23&#8211;27. 10.1016/j.anorl.2020.06.011 (2021). 32620425 10.1016/j.anorl.2020.06.011 25. Al-Amri, S.&#160;S., Kalyankar, N.&#160;V. et al. Image segmentation by using threshold techniques. arXiv preprint arXiv:1005.4020 (2010). 26. Besler, B.&#160;A., Michalski, A.&#160;S., Forkert, N.&#160;D. &amp; Boyd, S.&#160;K. Automatic full femur segmentation from computed tomography datasets using an atlas-based approach. In Computational Methods and Clinical Applications in Musculoskeletal Imaging , vol. 10734 of Lecture Notes in Computer Science , 120&#8211;132, 10.1007/978-3-319-74113-0_11 (Springer, 2018). 27. Liu P Deep learning to segment pelvic bones: large-scale CT datasets and baseline models Int. J. Comput. Assist. Radiol. Surg. 2021 16 749 756 10.1007/s11548-021-02363-8 33864189 Liu, P. et al. Deep learning to segment pelvic bones: large-scale CT datasets and baseline models. Int. J. Comput. Assist. Radiol. Surg. 16 , 749&#8211;756 (2021). 33864189 10.1007/s11548-021-02363-8 28. Hemke R Buckless CG Tsao A Wang B Torriani M Deep learning for automated segmentation of pelvic muscles, fat, and bone from CT studies for body composition assessment Skeletal Radiol. 2019 49 387 395 10.1007/s00256-019-03289-8 31396667 PMC6980503 Hemke, R., Buckless, C. G., Tsao, A., Wang, B. &amp; Torriani, M. Deep learning for automated segmentation of pelvic muscles, fat, and bone from CT studies for body composition assessment. Skeletal Radiol. 49 , 387&#8211;395 (2019). 31396667 10.1007/s00256-019-03289-8 PMC6980503 29. Liu, X. et al. Fully automated pelvic bone segmentation in multiparameteric mri using a 3d convolutional neural network. Insights into Imaging 12 (2021). 10.1186/s13244-021-01044-z PMC8263843 34232404 30. Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 Fourth International Conference on 3D Vision (3DV) , 565&#8211;571 (IEEE, 2016). 31. Yosinski, J., Clune, J., Bengio, Y. &amp; Lipson, H. How transferable are features in deep neural networks? Adv. Neural Inform. Process. Syst. 27 (2014). 32. Das NN Kumar N Kaur M Kumar V Singh D Automated deep transfer learning-based approach for detection of COVID-19 infection in chest x-rays IRBM 2022 43 114 119 10.1016/j.irbm.2020.07.001 32837679 PMC7333623 Das, N. N., Kumar, N., Kaur, M., Kumar, V. &amp; Singh, D. Automated deep transfer learning-based approach for detection of COVID-19 infection in chest x-rays. IRBM 43 , 114&#8211;119. 10.1016/j.irbm.2020.07.001 (2022). 32837679 10.1016/j.irbm.2020.07.001 PMC7333623 33. Deepak S Ameer P Brain tumor classification using deep CNN features via transfer learning Comput. Biol. Med. 2019 111 103345 10.1016/j.compbiomed.2019.103345 31279167 Deepak, S. &amp; Ameer, P. Brain tumor classification using deep CNN features via transfer learning. Comput. Biol. Med. 111 , 103345. 10.1016/j.compbiomed.2019.103345 (2019). 31279167 10.1016/j.compbiomed.2019.103345 34. Wang Y Feng Z Song L Liu X Liu S Multiclassification of endoscopic colonoscopy images based on deep transfer learning Comput. Math. Methods Med. 2021 1&#8211;12 2021 10.1155/2021/2485934 PMC8272675 34306173 Wang, Y., Feng, Z., Song, L., Liu, X. &amp; Liu, S. Multiclassification of endoscopic colonoscopy images based on deep transfer learning. Comput. Math. Methods Med. 1&#8211;12 , 2021. 10.1155/2021/2485934 (2021). 10.1155/2021/2485934 PMC8272675 34306173 35. Zhang R Craniomaxillofacial bone segmentation and landmark detection using semantic segmentation networks and an unbiased heatmap IEEE J. Biomed. Health Inform. 2024 28 427 437 10.1109/JBHI.2023.3337546 38019620 Zhang, R. et al. Craniomaxillofacial bone segmentation and landmark detection using semantic segmentation networks and an unbiased heatmap. IEEE J. Biomed. Health Inform. 28 , 427&#8211;437. 10.1109/JBHI.2023.3337546 (2024). 10.1109/JBHI.2023.3337546 38019620 36. Wang Y Zhao L Wang M Song Z Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net IEEE Access 2019 7 144591 144602 10.1109/ACCESS.2019.2944958 Wang, Y., Zhao, L., Wang, M. &amp; Song, Z. Organ at risk segmentation in head and neck ct images using a two-stage segmentation framework based on 3d u-net. IEEE Access 7 , 144591&#8211;144602. 10.1109/ACCESS.2019.2944958 (2019). 37. Lewiner T Lopes H Vieira AW Tavares G Efficient implementation of marching cubes&#8217; cases with topological guarantees J. Graph. Tools 2003 8 1 15 10.1080/10867651.2003.10487582 Lewiner, T., Lopes, H., Vieira, A. W. &amp; Tavares, G. Efficient implementation of marching cubes&#8217; cases with topological guarantees. J. Graph. Tools 8 , 1&#8211;15. 10.1080/10867651.2003.10487582 (2003). 38. &#199;i&#231;ek, &#214;., Abdulkadir, A., Lienkamp, S.&#160;S., Brox, T. &amp; Ronneberger, O. 3d u-net: learning dense volumetric segmentation from sparse annotation. In Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19 , 424&#8211;432 (Springer, 2016). 39. Wang, C., MacGillivray, T., Macnaught, G., Yang, G. &amp; Newby, D. A two-stage 3d unet framework for multi-class segmentation on full resolution image. arXiv preprint arXiv:1804.04341 (2018). 40. Kingma, D.&#160;P. &amp; Ba, J. Adam: A method for stochastic optimization. arxiv:https://arxiv.org/abs/1412.6980 (2014). 41. JI, L., Piper, J. &amp; Tang, J.-Y. Erosion and dilation of binary images by arbitrary structuring elements using interval coding. Pattern Recognition Letters 9 , 201&#8211;209, 10.1016/0167-8655(89)90055-x (1989). 42. Deng, G. &amp; Cahill, L. An adaptive gaussian filter for noise reduction and edge detection. In 1993 IEEE Conference Record Nuclear Science Symposium and Medical Imaging Conference , vol. 3, 1615&#8211;1619, 10.1109/NSSMIC.1993.373563 (1993). 43. Dice LR Measures of the amount of ecologic association between species Ecology 1945 26 297 302 10.2307/1932409 Dice, L. R. Measures of the amount of ecologic association between species. Ecology 26 , 297&#8211;302. 10.2307/1932409 (1945). 44. Heimann T Comparison and evaluation of methods for liver segmentation from CT datasets IEEE Trans. Med. Imaging 2009 28 1251 1265 10.1109/tmi.2009.2013851 19211338 Heimann, T. et al. Comparison and evaluation of methods for liver segmentation from CT datasets. IEEE Trans. Med. Imaging 28 , 1251&#8211;1265. 10.1109/tmi.2009.2013851 (2009). 19211338 10.1109/TMI.2009.2013851 45. Huttenlocher D Klanderman G Rucklidge W Comparing images using the hausdorff distance IEEE Trans. Pattern Anal. Mach. Intell. 1993 15 850 863 10.1109/34.232073 Huttenlocher, D., Klanderman, G. &amp; Rucklidge, W. Comparing images using the hausdorff distance. IEEE Trans. Pattern Anal. Mach. Intell. 15 , 850&#8211;863. 10.1109/34.232073 (1993). 46. Wolff K-D Ervens J Herzog K Hoffmeister B Experience with the osteocutaneous fibula flap: an analysis of 24 consecutive reconstructions of composite mandibular defects J. Cranio-Maxillofac. Surg. 1996 24 330 338 10.1016/S1010-5182(96)80033-3 9032600 Wolff, K.-D., Ervens, J., Herzog, K. &amp; Hoffmeister, B. Experience with the osteocutaneous fibula flap: an analysis of 24 consecutive reconstructions of composite mandibular defects. J. Cranio-Maxillofac. Surg. 24 , 330&#8211;338 (1996). 10.1016/s1010-5182(96)80033-3 9032600 47. Shpitzer T Leg morbidity and function following fibular free flap harvest Ann. Plast. Surg. 1997 38 460 464 10.1097/00000637-199705000-00005 9160127 Shpitzer, T. et al. Leg morbidity and function following fibular free flap harvest. Ann. Plast. Surg. 38 , 460&#8211;464 (1997). 9160127 10.1097/00000637-199705000-00005 48. Chhabra A Treatment of chronic nonunions of the humerus with free vascularized fibula transfer: A report of thirteen cases J. Reconstr. Microsurg. 2008 25 117 124 10.1055/s-0028-1090624 18925551 Chhabra, A. et al. Treatment of chronic nonunions of the humerus with free vascularized fibula transfer: A report of thirteen cases. J. Reconstr. Microsurg. 25 , 117&#8211;124. 10.1055/s-0028-1090624 (2008). 18925551 10.1055/s-0028-1090624 49. Ruikar, D.&#160;D., Santosh, K.&#160;C. &amp; Hegadi, R.&#160;S. Automated fractured bone segmentation and labeling from CT images. Journal of Medical Systems 43 , 10.1007/s10916-019-1176-x (2019). 10.1007/s10916-019-1176-x 30710217 50. Ma Y Luo Y Bone fracture detection through the two-stage system of crack-sensitive convolutional neural network Inform. Med. Unlocked 2021 22 100452 10.1016/j.imu.2020.100452 Ma, Y. &amp; Luo, Y. Bone fracture detection through the two-stage system of crack-sensitive convolutional neural network. Inform. Med. Unlocked 22 , 100452. 10.1016/j.imu.2020.100452 (2021). 51. Kim, H., Lee, K., Lee, D.-C. &amp; Baek, N. 3d reconstruction of leg bones from x-ray images using cnn-based feature analysis. 2019 International Conference on Information and Communication Technology Convergence (ICTC) 669&#8211;672 (2019). 52. den Broeck JV Vereecke E Wirix-Speetjens R Sloten JV Segmentation accuracy of long bones Med. Eng. Phys. 2014 36 949 953 10.1016/j.medengphy.2014.03.016 24768087 den Broeck, J. V., Vereecke, E., Wirix-Speetjens, R. &amp; Sloten, J. V. Segmentation accuracy of long bones. Med. Eng. Phys. 36 , 949&#8211;953. 10.1016/j.medengphy.2014.03.016 (2014). 24768087 10.1016/j.medengphy.2014.03.016 53. Wallner J Mischak I Egger J Computed tomography data collection of the complete human mandible and valid clinical ground truth models Scientific Data 2019 6:1 2019 6 1 14 10.1038/sdata.2019.3 PMC6350631 30694227 Wallner, J., Mischak, I. &amp; Egger, J. Computed tomography data collection of the complete human mandible and valid clinical ground truth models. Scientific Data 2019 6:1 6 , 1&#8211;14. 10.1038/sdata.2019.3 (2019). 10.1038/sdata.2019.3 PMC6350631 30694227 54. Kim T Accuracy of a simplified 3D-printed implant surgical guide J. Prosthet. Dent. 2020 124 195 201.e2 10.1016/j.prosdent.2019.06.006 31753464 Kim, T. et al. Accuracy of a simplified 3D-printed implant surgical guide. J. Prosthet. Dent. 124 , 195-201.e2. 10.1016/j.prosdent.2019.06.006 (2020). 31753464 10.1016/j.prosdent.2019.06.006 55. Li Y Automated segmentation of vertebral cortex with 3d u-net-based deep convolutional neural network Front. Bioeng. Biotechnol. 2022 10 996723 10.3389/fbioe.2022.996723 36338129 PMC9626964 Li, Y. et al. Automated segmentation of vertebral cortex with 3d u-net-based deep convolutional neural network. Front. Bioeng. Biotechnol. 10 , 996723 (2022). 36338129 10.3389/fbioe.2022.996723 PMC9626964 56. Jha N Fully automated condyle segmentation using 3D convolutional neural networks Sci. Rep. 2022 12 20590 10.1038/s41598-022-24164-y 36446860 PMC9709043 Jha, N. et al. Fully automated condyle segmentation using 3D convolutional neural networks. Sci. Rep. 12 , 20590 (2022). 36446860 10.1038/s41598-022-24164-y PMC9709043"
}