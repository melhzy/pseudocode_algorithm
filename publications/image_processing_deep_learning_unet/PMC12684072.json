{
  "pmcid": "PMC12684072",
  "source": "PMC",
  "download_date": "2025-12-09T16:06:40.833394",
  "metadata": {
    "journal_title": "Biomedical Optics Express",
    "journal_nlm_ta": "Biomed Opt Express",
    "journal_iso_abbrev": "Biomed Opt Express",
    "journal": "Biomedical Optics Express",
    "pmcid": "PMC12684072",
    "doi": "10.1364/BOE.564842",
    "title": "Three-dimensional transformer-enhanced multi-scale global co-attention network for precise diabetic macular edema segmentation in OCT volumes",
    "year": "2025",
    "month": "8",
    "day": "14",
    "pub_date": {
      "year": "2025",
      "month": "8",
      "day": "14"
    },
    "authors": [
      "Hu Jiangting",
      "Li Chunxiu",
      "Lin Shuaichen",
      "Qin Mohan",
      "Wu Renxiong",
      "Zhong Jie",
      "Liu Yong",
      "Ni Guangming"
    ],
    "abstract": "Diabetic macular edema (DME) has emerged as one of the leading causes of visual impairment worldwide, and optical coherence tomography (OCT) plays a pivotal role in detecting DME. Automatic and accurate segmentation of lesions in retinal OCT images is essential for early clinical diagnosis of DME, but most recent deep-learning methods are two-dimensional (2D) segmentation and fail to fully extract the lesionsâ€™ critical three-dimensional (3D) information contained in OCT images. Here we proposed a novel 3D deep-learning network characterized by combining a transformer encoder with multi-scale feature aggregation, a non-local module, and global channel-spatial joint attention to obtain accurate 3D segmentation of DME and reveal their 3D morphological characteristics. Extensive experimental results demonstrate that our proposed method not only achieves commendable 3D segmentation performance with robust generalization capabilities in challenging cases, but also offers valuable insights into ophthalmic diseases, enhancing the convenience of clinical diagnosis and treatment of DME."
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Biomed Opt Express</journal-id><journal-id journal-id-type=\"iso-abbrev\">Biomed Opt Express</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1350</journal-id><journal-id journal-id-type=\"pmc-domain\">boe</journal-id><journal-id journal-id-type=\"publisher-id\">BOE</journal-id><journal-title-group><journal-title>Biomedical Optics Express</journal-title></journal-title-group><issn pub-type=\"epub\">2156-7085</issn><publisher><publisher-name>Optica Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12684072</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12684072.1</article-id><article-id pub-id-type=\"pmcaid\">12684072</article-id><article-id pub-id-type=\"pmcaiid\">12684072</article-id><article-id pub-id-type=\"doi\">10.1364/BOE.564842</article-id><article-id pub-id-type=\"publisher-id\">boe-16-9-3608</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Three-dimensional transformer-enhanced multi-scale global co-attention network for precise diabetic macular edema segmentation in OCT volumes</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names initials=\"J\">Jiangting</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref><xref rid=\"n1\" ref-type=\"author-notes\">&#8224;</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"C\">Chunxiu</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">2</xref><xref rid=\"n1\" ref-type=\"author-notes\">&#8224;</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names initials=\"S\">Shuaichen</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names initials=\"M\">Mohan</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"R\">Renxiong</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhong</surname><given-names initials=\"J\">Jie</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"Y\">Yong</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Ni</surname><given-names initials=\"G\">Guangming</given-names></name><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0002-4516-256X</contrib-id><xref rid=\"aff1\" ref-type=\"aff\">1</xref><xref rid=\"cor1\" ref-type=\"corresp\">*</xref></contrib><aff id=\"aff1\">\n<label>1</label>School of Optoelectronic Science and Engineering, \n<institution>University of Electronic Science and Technology of China</institution>, Chengdu 611731, \n<country country=\"CN\">China</country></aff><aff id=\"aff2\">\n<label>2</label>School of Medicine, \n<institution>University of Electronic Science and Technology of China</institution>, Chengdu 610054, \n<country country=\"CN\">China</country></aff></contrib-group><author-notes><fn id=\"n1\"><label>&#8224;</label><p>These authors contributed equally.</p></fn><corresp id=\"cor1\"><label>*</label><email>guangmingni@uestc.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>14</day><month>8</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><day>01</day><month>9</month><year>2025</year></pub-date><volume>16</volume><issue>9</issue><issue-id pub-id-type=\"pmc-issue-id\">502064</issue-id><fpage>3608</fpage><lpage>3622</lpage><history><date date-type=\"received\"><day>10</day><month>4</month><year>2025</year></date><date date-type=\"rev-recd\"><day>03</day><month>7</month><year>2025</year></date><date date-type=\"accepted\"><day>25</day><month>7</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>14</day><month>08</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 10:25:13.050\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 Optica Publishing Group</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Optica Publishing Group</copyright-holder><license><license-p>https://doi.org/10.1364/OA_License_v2#VOR-OA</license-p></license><license><license-p>&#169; 2025 Optica Publishing Group under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://doi.org/10.1364/OA_License_v2#VOR-OA\">Optica Open Access Publishing Agreement</ext-link></license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"boe-16-9-3608.pdf\"/><abstract><p>Diabetic macular edema (DME) has emerged as one of the leading causes of visual impairment worldwide, and optical coherence tomography (OCT) plays a pivotal role in detecting DME. Automatic and accurate segmentation of lesions in retinal OCT images is essential for early clinical diagnosis of DME, but most recent deep-learning methods are two-dimensional (2D) segmentation and fail to fully extract the lesions&#8217; critical three-dimensional (3D) information contained in OCT images. Here we proposed a novel 3D deep-learning network characterized by combining a transformer encoder with multi-scale feature aggregation, a non-local module, and global channel-spatial joint attention to obtain accurate 3D segmentation of DME and reveal their 3D morphological characteristics. Extensive experimental results demonstrate that our proposed method not only achieves commendable 3D segmentation performance with robust generalization capabilities in challenging cases, but also offers valuable insights into ophthalmic diseases, enhancing the convenience of clinical diagnosis and treatment of DME.</p></abstract><funding-group><award-group id=\"sp1\"><funding-source country=\"CN\">National Natural Science Foundation of China\n<named-content content-type=\"doi\">10.13039/501100001809</named-content></funding-source><award-id>61905036</award-id></award-group><award-group id=\"sp2\"><funding-source country=\"CN\">China Postdoctoral Science Foundation\n<named-content content-type=\"doi\">10.13039/501100002858</named-content></funding-source><award-id>2021T140090</award-id><award-id>2019M663465</award-id></award-group><award-group id=\"sp3\"><funding-source country=\"CN\">Fundamental Research Funds for the Central Universities\n<named-content content-type=\"doi\">10.13039/501100012226</named-content></funding-source><award-id>ZYGX2021J012</award-id></award-group><award-group id=\"sp4\"><funding-source country=\"CN\">Medico-Engineering Cooperation Funds from University of Electronic Science and Technology of China</funding-source><award-id>ZYGX2021YGCX019</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"introduction\" id=\"sec1\"><label>1.</label><title>Introduction</title><p>Diabetic macular edema (DME), characterized by the accumulation of fluid in the macula caused by increased vascular permeability [<xref rid=\"r1\" ref-type=\"bibr\">1</xref>&#8211;<xref rid=\"r4\" ref-type=\"bibr\">4</xref>], is currently the leading cause of blindness and associated functional impairment in the working-age population of most developed countries [<xref rid=\"r5\" ref-type=\"bibr\">5</xref>]. According to authoritative statistics, global estimates predict a significant rise in diabetes mellitus and macular edema diagnosis [<xref rid=\"r4\" ref-type=\"bibr\">4</xref>,<xref rid=\"r6\" ref-type=\"bibr\">6</xref>,<xref rid=\"r7\" ref-type=\"bibr\">7</xref>]. As a severe and prevalent complication of diabetes mellitus occurring in any phase of diabetic retinopathy (DR) progression [<xref rid=\"r3\" ref-type=\"bibr\">3</xref>,<xref rid=\"r8\" ref-type=\"bibr\">8</xref>], DME can trigger progressive retinal dysfunction, leading to irreversible and permanent vision loss [<xref rid=\"r9\" ref-type=\"bibr\">9</xref>]. To effectively prevent its progression to blindness, quantitative assessment of DME lesions is crucial to better understand the ophthalmic diseases and improve the corresponding diagnosis and treatment of DME.</p><p>Optical coherence tomography (OCT) imaging, a non-invasive three-dimensional (3D) imaging technology [<xref rid=\"r10\" ref-type=\"bibr\">10</xref>], is now widely used for progress monitoring and damage assessment of DME. Manual segmentation of DME lesions in OCT images by professional ophthalmologists is time-consuming and inefficient; as a result, automatic and accurate segmentation methods are of substantial clinical significance. To analyze macular fluids automatically and accurately, different image processing and machine learning algorithms have been developed [<xref rid=\"r11\" ref-type=\"bibr\">11</xref>]. Previous approaches to segmenting lesions of medical images are mainly divided into several kinds: deformable models based on level sets [<xref rid=\"r12\" ref-type=\"bibr\">12</xref>], learning-based approaches [<xref rid=\"r13\" ref-type=\"bibr\">13</xref>], and pixel classification-based approaches [<xref rid=\"r14\" ref-type=\"bibr\">14</xref>].</p><p>In particular, deep-learning methods using convolutional neural networks (CNNs) have achieved impressive improvements in a variety of automated medical image segmentation tasks and provide a general approach to extracting the feature [<xref rid=\"r13\" ref-type=\"bibr\">13</xref>]. Over the past few years, several networks have been proposed and applied to medical imaging segmentation tasks, including U-Net [<xref rid=\"r15\" ref-type=\"bibr\">15</xref>], U-Net++ [<xref rid=\"r16\" ref-type=\"bibr\">16</xref>], V-Net [<xref rid=\"r17\" ref-type=\"bibr\">17</xref>], Deeplabv3+ [<xref rid=\"r18\" ref-type=\"bibr\">18</xref>], and Seg-Net [<xref rid=\"r19\" ref-type=\"bibr\">19</xref>]. Inspired by these networks, several specialists have proposed some other networks that have great accuracy and robustness in DME segmentation tasks. For example, R. Gu et al. [<xref rid=\"r20\" ref-type=\"bibr\">20</xref>] proposed CANet, which used ResNet50 with various attention modules to detect diabetic retinopathy and macular edema for diabetes. Roy et al. [<xref rid=\"r21\" ref-type=\"bibr\">21</xref>] proposed a new fully convolutional deep architecture termed ReLayNet, which uses a contracting path of convolutional blocks for semantic segmentation. W. Liu et al. [<xref rid=\"r22\" ref-type=\"bibr\">22</xref>] proposed a 2D fully convolutional network called MDAN-UNet for DME segmentation in OCT images, and they incorporated re-designed skip pathways, multi-scale input, multi-scale side output, and attention mechanisms into U-Net++ to enhance performance. X. Liu et al. [<xref rid=\"r23\" ref-type=\"bibr\">23</xref>] introduced the attention gates and dense skip connection into UNet to automatically locate the suspicious areas when segmenting DME lesions. Hassan et al. [<xref rid=\"r24\" ref-type=\"bibr\">24</xref>] proposed RFS-Net, which integrates ASPP modules with the inception module and residual learning to perform effectively in retinal fluid lesions segmentation tasks on OCT scans. Y. Bai et al. [<xref rid=\"r25\" ref-type=\"bibr\">25</xref>] introduced an improved ASPP module into MobileNetV2 to avoid grid effects and better extract high-level features of DME. George et al. [<xref rid=\"r26\" ref-type=\"bibr\">26</xref>] integrated a modified VGG16 network with a UNet model for the classification and segmentation of DME lesions. Wu et al. [<xref rid=\"r27\" ref-type=\"bibr\">27</xref>] proposed RCU-Net, which combined U-Net with residual structures and a convolutional block attention module to enhance gradient propagation and extract more abundant space and useful features on the channel for better DME segmentation results.</p><p>Although these networks have demonstrated impressive accuracy and robustness in macular edema segmentation tasks, most of them are fully convolutional neural networks that focus solely on the two-dimensional (2D) properties of OCT images. As a result, these networks fail to fully leverage the 3D information present in retinal OCT volumes and cannot learn long-range spatial dependencies, which leads to a lack of 3D spatial consistency in the long-axis direction. Moreover, variations in lesion size and shape, blurred boundaries, background noise interference, and the low contrast of OCT imaging can lead to incorrect segmentation in areas that do not correspond to lesions, posing a significant challenge for the accurate segmentation of DME [<xref rid=\"r28\" ref-type=\"bibr\">28</xref>].</p><p>To address these limitations, we proposed a novel 3D transformer-enhanced multi-scale global co-attention network that can effectively explore DME features from OCT 3D volumes, thereby capturing the accurate 3D morphology of DME lesions. Instead of using a fully convolutional neural network, we employed transformers as the backbone, which not only maximizes the use of 3D structural and spatial information in OCT volumes but also effectively extracts global, multi-scale information with long-range dependencies. Specifically, we introduced a 3D multi-scale non-local feature extractor (3D-MsNLFE) and a 3D semantic context aggregator (3D-SCA) within the bottleneck to mitigate the effects of variable DME lesion sizes and shapes, as well as speckle noise. Additionally, we incorporated a 3D multi-scale global channel and spatial joint attention (3D-MsGCS) into skip connections to enhance the model&#8217;s ability to learn multi-semantic global contextual features across both channel and spatial dimensions, while suppressing irrelevant local features. Furthermore, a 3D multi-scale feature aggregation (3D-MsFA) module is introduced in the final layer to capture finer semantic details and further enhance segmentation accuracy.</p></sec><sec sec-type=\"methods\" id=\"sec2\"><label>2.</label><title>Methods</title><sec id=\"sec2-1\"><label>2.1.</label><title>Datasets</title><p>This study was approved by the Institutional Review Board (IRB) of Sichuan Provincial People&#8217;s Hospital (IRB-2022-258). A swept-source OCT setup (BM-400 K BMizar, TowardPi, China) was used to obtain the datasets comprising OCT volumes of patients with DME. This system utilizes a vertical-cavity surface-emitting laser with a wavelength of 1060 nm and a scanning speed of 400000 A-scans per second. OCT scanning mode can obtain OCT images covering a <inline-formula>\n<mml:math id=\"m1\" display=\"inline\" overflow=\"scroll\"><mml:mn>6</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>6</mml:mn></mml:math>\n</inline-formula> mm area centered on the macula. Within this <inline-formula>\n<mml:math id=\"m2\" display=\"inline\" overflow=\"scroll\"><mml:mn>6</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>6</mml:mn></mml:math>\n</inline-formula> mm scan range, each retinal OCT scan consists of 512 A-lines per B-scan, resulting in a cube size of <inline-formula>\n<mml:math id=\"m3\" display=\"inline\" overflow=\"scroll\"><mml:mn>512</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>1024</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>512</mml:mn></mml:math>\n</inline-formula> pixels <inline-formula>\n<mml:math id=\"m4\" display=\"inline\" overflow=\"scroll\"><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#215;\n</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#215;\n</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>.</p><p>Our dataset included 49 cases of DME volumes and corresponding manual pixel-level annotation from specialized ophthalmologists, and 2 cases of DME volumes without manual annotation remained for the clinic trial test. All participants underwent a comprehensive ocular examination including refraction and best-corrected visual acuity, non-contact intraocular pressure (IOP), ocular axis, slit lamp, and wide-angle fundus OCT imaging. This study was approved by the Ethics Committee of Sichuan Provincial People&#8217;s Hospital and adhered to the Declaration of Helsinki. Among the annotated data, 80% of the data was randomly chosen for training, 10% for validation, and the remaining 10% was reserved for testing.</p><p><xref rid=\"g001\" ref-type=\"fig\">Figure&#160;1</xref> shows the 3D visualization and the corresponding B-scans of our datasets. <xref rid=\"g001\" ref-type=\"fig\">Figure&#160;1(a)</xref> illustrates the raw 3D retinal OCT image with DME. <xref rid=\"g001\" ref-type=\"fig\">Figure&#160;1(b)</xref> demonstrates the 3D visualization of DME lesions. <xref rid=\"g001\" ref-type=\"fig\">Figure&#160;1(c)</xref> presents the B-scan image corresponding to the green slice of the raw data in <xref rid=\"g001\" ref-type=\"fig\">Fig.&#160;1(a)</xref>. <xref rid=\"g001\" ref-type=\"fig\">Figure&#160;1(d)</xref> demonstrates the label image of the DME lesions cross-section.</p><fig position=\"float\" id=\"g001\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 1.</label><caption><p>3D visualization and B-scans of OCT images with DME datasets. (a) represents the raw 3D retinal OCT images with DME. (b) demonstrates the 3D visualization of DME lesions. (c) represents the B-scan image corresponding to the green slice of raw data in (a). (d) represents the label image of the DME lesion cross-section.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g001.jpg\"/></fig></sec><sec id=\"sec2-2\"><label>2.2.</label><title>Overview architecture</title><p>To obtain automatic and accurate 3D segmentation results of DME lesions in retinal OCT images, we proposed a 3D transformer-enhanced multi-scale global co-attention network. As shown in <xref rid=\"g002\" ref-type=\"fig\">Fig.&#160;2</xref>, the network adopts a U-shaped architecture with a contracting-expanding path, in which a stack of 12 transformer blocks is used as the encoder. Unlike conventional fully convolutional networks, transformers encode volumetric data as a sequence of 1D patch embeddings and apply the self-attention mechanism [<xref rid=\"r29\" ref-type=\"bibr\">29</xref>] to model long-range dependencies and capture global contextual information [<xref rid=\"r30\" ref-type=\"bibr\">30</xref>,<xref rid=\"r31\" ref-type=\"bibr\">31</xref>], which is essential for volumetric medical imaging tasks.</p><fig position=\"float\" id=\"g002\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 2.</label><caption><p>Schematic overview of the proposed 3D segmentation network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g002.jpg\"/></fig><p>When a 3D OCT volume is input into the encoder, it is first divided into non-overlapping patches. Each patch is flattened and projected into a fixed-dimensional embedding space via a linear layer, followed by the addition of a learnable positional embedding to preserve spatial information. The embedded sequence is then processed by a series of transformer blocks composed of multi-head self-attention (MSA) and multi-layer perceptron (MLP) to extract high-level representations [<xref rid=\"r32\" ref-type=\"bibr\">32</xref>].</p><p>Due to the small proportion and complex variability of DME lesions in OCT images, as well as the large background regions, it is essential to enhance lesion-related features while suppressing irrelevant information. To handle 3D volumetric OCT data and obtain precise segmentation results, we introduced and restructured a series of 3D attention-based modules adapted from previously validated 2D designs [<xref rid=\"r28\" ref-type=\"bibr\">28</xref>]. As shown in <xref rid=\"g003\" ref-type=\"fig\">Fig.&#160;3(a), (b), and (c)</xref>, we incorporate the 3D multi-semantic global channel and spatial joint attention-based skip connection (3D-MsGCS) module, the 3D multi-scale non-local feature extractor (3D-MsNLFE) module, the 3D semantic context aggregator (3D-SCA) module, and the 3D multi-scale feature aggregation module (3D-MsFA) into our network architecture.</p><fig position=\"float\" id=\"g003\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 3.</label><caption><p>Details of 3D-MsGCS module, 3D-MsNLFE module and 3D-SCA module. (a) is 3D-MsGCS module, (b) is 3D-MsNLFE module, (c) is 3D-SCA module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g003.jpg\"/></fig><p>As illustrated in <xref rid=\"g003\" ref-type=\"fig\">Fig.&#160;3</xref>, the 3D-MsGCS module is used in skip connections to bridge the encoder and decoder. It enables enhanced multi-semantic feature fusion in both channel and spatial dimensions, helping the decoder recover detailed structures while suppressing irrelevant local features [<xref rid=\"r28\" ref-type=\"bibr\">28</xref>]. In the bottleneck of the network, we utilized the 3D-MsNLFE module and 3D-SCA module to capture multi-scale and non-local features [<xref rid=\"r28\" ref-type=\"bibr\">28</xref>]. By aggregating outputs from multiple transformer layers with different receptive fields, they improve robustness against the variation in lesion size, shape, and speckle noise.</p><p>Specifically, in the above three modules, we use a global learnable heatmap generated by learnable vectors to capture the multi-semantic global information. This type of position coding method has been commonly used in 2D image processing methods to make the self-attention operation sensitive to spatial positions of features [<xref rid=\"r33\" ref-type=\"bibr\">33</xref>]. In our work, we extend this 2D positional encoding by incorporating an additional depth dimension, enabling it to better model positional features in 3D volumetric data. This enhancement allows the network to capture long-range dependencies along all three spatial axes and focus more effectively on the positional relationships within the volume, which may help facilitate more accurate 3D structure modeling, particularly for small and irregular lesions in OCT volumes.</p><p>In the final stage of the network, we employ a 3D multi-scale feature aggregation (3D-MsFA) module to enhance the segmentation results. Feature maps from multiple decoder levels are first compressed to a consistent channel dimension and refined using an attention mechanism to highlight informative features. These refined features are then fused across scales to integrate contextual information from different spatial resolutions. This multi-scale fusion is particularly beneficial for capturing the heterogeneous size and shape of DME lesions in volumetric OCT data.</p><p>Finally, the aggregated output is passed through a convolutional layer and a sigmoid activation to produce the final 3D segmentation prediction.</p></sec><sec id=\"sec2-3\"><label>2.3.</label><title>Loss function</title><p>As background noise and blurred boundaries make up a large portion of the obtained retinal image, precise segmentation of DME lesions is facing a significant challenge. To efficiently optimize the proposed model and mitigate the negative effects of the imbalance between foreground and background in a sample, we adopted a joint loss function to train our model, which can be defined as Eq.&#160;(<xref rid=\"e1\" ref-type=\"disp-formula\">1</xref>): \n<disp-formula id=\"e1\"><label>(1)</label><mml:math id=\"m5\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#955;\n</mml:mi><mml:mo>&#8901;\n</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>&#945;\n</mml:mi><mml:mo>,</mml:mo><mml:mi>&#946;\n</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;\n</mml:mo><mml:mi>&#955;\n</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8901;\n</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <inline-formula>\n<mml:math id=\"m6\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mrow><mml:mi>&#945;\n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mspace width=\"thickmathspace\"/><mml:mtext>&#160;</mml:mtext><mml:mi>&#946;\n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> represents the Tversky loss [<xref rid=\"r34\" ref-type=\"bibr\">34</xref>], serving as the supervised loss to evaluate the quality of the model output on labeled inputs. It can be expressed as Eq.&#160;(<xref rid=\"e2\" ref-type=\"disp-formula\">2</xref>): \n<disp-formula id=\"e2\"><label>(2)</label><mml:math id=\"m7\" display=\"block\" overflow=\"scroll\"><mml:mi>T</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>&#945;\n</mml:mi><mml:mo>,</mml:mo><mml:mi>&#946;\n</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo movablelimits=\"false\">&#8721;\n</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo movablelimits=\"false\">&#8721;\n</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#945;\n</mml:mi><mml:msubsup><mml:mo movablelimits=\"false\">&#8721;\n</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#946;\n</mml:mi><mml:msubsup><mml:mo movablelimits=\"false\">&#8721;\n</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula> where <inline-formula>\n<mml:math id=\"m8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>\n</inline-formula> represents the probability of voxel i being a lesion and <inline-formula>\n<mml:math id=\"m9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>\n</inline-formula> represents the probability of being a non-lesion. And the <inline-formula>\n<mml:math id=\"m10\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>\n</inline-formula> is 1 for a lesion voxel and 0 for a non-lesion voxel while <inline-formula>\n<mml:math id=\"m11\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>\n</inline-formula> is 0 for a lesion voxel and 1 for a non-lesion voxel. Compared with the Dice loss, the Tversky loss introduces two coefficients <inline-formula>\n<mml:math id=\"m12\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#945;\n</mml:mi></mml:math>\n</inline-formula> and <inline-formula>\n<mml:math id=\"m13\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#946;\n</mml:mi></mml:math>\n</inline-formula> to better balance false negatives and false positives and thus achieves better performance than the Dice loss in multi-task segmentation. For our experiment, <inline-formula>\n<mml:math id=\"m14\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#945;\n</mml:mi></mml:math>\n</inline-formula> is set to 0.2, <inline-formula>\n<mml:math id=\"m15\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#946;\n</mml:mi></mml:math>\n</inline-formula> is set to 0.8 and <inline-formula>\n<mml:math id=\"m16\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#955;\n</mml:mi></mml:math>\n</inline-formula> is set to 0.8 for better performance.</p><p>As expressed in Eq.&#160;(<xref rid=\"e3\" ref-type=\"disp-formula\">3</xref>), <inline-formula>\n<mml:math id=\"m17\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>\n</inline-formula> represents the binary cross-entropy loss, and <inline-formula>\n<mml:math id=\"m18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>\n</inline-formula> represents the annotated ground truth label for pixel i, which is 1 for a lesion voxel and 0 for a non-lesion voxel, and <inline-formula>\n<mml:math id=\"m19\" display=\"inline\" overflow=\"scroll\"><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> represents the probability of the prediction label for pixel i. \n<disp-formula id=\"e3\"><label>(3)</label><mml:math id=\"m20\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>&#8722;\n</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo movablelimits=\"false\">&#8721;\n</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#8901;\n</mml:mo><mml:mi>log</mml:mi><mml:mo>&#8289;\n</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;\n</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8901;\n</mml:mo><mml:mi>log</mml:mi><mml:mo>&#8289;\n</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;\n</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec><sec sec-type=\"other1\" id=\"sec3\"><label>3.</label><title>Experiment setup</title><sec id=\"sec3-1\"><label>3.1.</label><title>Implementation details</title><p>Since all the B-scans contained a lot of background area without information, to make full use of GPU resources and improve training efficiency, we cropped and resized the image patch into seven volumes with the size of <inline-formula>\n<mml:math id=\"m21\" display=\"inline\" overflow=\"scroll\"><mml:mn>224</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>224</mml:mn><mml:mo>&#215;\n</mml:mo><mml:mn>64</mml:mn></mml:math>\n</inline-formula> pixels <inline-formula>\n<mml:math id=\"m22\" display=\"inline\" overflow=\"scroll\"><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#215;\n</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#215;\n</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>. We enhanced volume by flipping, rotating, and rolling at random vertical or horizontal directions to obtain four augmented volumes for each volumetric data in the training datasets. The proposed model was implemented in TensorFlow v2.5.0 with NVIDIA RTX A6000 (48 G) GPUs, and Python 3.8.8. In all comparative experiments. We use 2 GPUs to distribute the dataset as our training method, and the batch size for each GPU is 1. We set the maximum number of epochs and learning rate as 135 and 0.00005, respectively, and use Adam as the optimizer.</p></sec><sec id=\"sec3-2\"><label>3.2.</label><title>Performance measures</title><p>To ensure accuracy and fairness when comparing the performance of different networks, we employ some classical metrics to quantitatively evaluate the obtained segmentation results, including the Dice similarity coefficient (DSC), balanced accuracy (ACC), precision, recall, specificity, and intersection over union (IOU). The range of these metrics is between 0 and 1, and if the value of the metrics is higher, the segmentation result is better. These metrics are obtained through the following equations: \n<disp-formula id=\"e4\"><label>(4)</label><mml:math id=\"m23\" display=\"block\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>\n<disp-formula id=\"e5\"><label>(5)</label><mml:math id=\"m24\" display=\"block\" overflow=\"scroll\"><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>\n<disp-formula id=\"e6\"><label>(6)</label><mml:math id=\"m25\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>\n<disp-formula id=\"e7\"><label>(7)</label><mml:math id=\"m26\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>\n<disp-formula id=\"e8\"><label>(8)</label><mml:math id=\"m27\" display=\"block\" overflow=\"scroll\"><mml:mi>I</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula> where TP, TN, FP, and FN are true positive, true negative, false positive, and false negative for pixel classification, respectively.</p></sec></sec><sec sec-type=\"results\" id=\"sec4\"><label>4.</label><title>Results</title><p><xref rid=\"g004\" ref-type=\"fig\">Figure&#160;4</xref> presents the 3D segmentation results of DME lesions in OCT images using our proposed network and ground truth. The original 3D OCT images consist of massive B-scans, the corresponding ground truth, and the 3D segmentation results predicted by our network are presented from left to right, respectively.</p><fig position=\"float\" id=\"g004\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 4.</label><caption><p>3D visualization of original OCT volumes, original annotation, and segmentation results of validation sets. (a), (d), and (g) represent OCT 3D volumes consisting of massive B-scans from three subjects, (b), (e), and (h) represent the corresponding manually annotated 3D images, (c), (f), and (i) represent the 3D segmentation results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g004.jpg\"/></fig><p>As shown in <xref rid=\"g004\" ref-type=\"fig\">Fig.&#160;4</xref>, the 3D segmentation results generated by our network closely resemble the actual morphology of DME lesions, which can assist ophthalmologists by reducing the burden of manual annotation. Despite the considerable variability in lesion shape, size, and number across patients, our method effectively captures these differences, which are often difficult to identify using traditional 2D segmentation methods. As a result, compared with 2D methods, 3D segmentation provides more intuitive and comprehensive morphological information, offering valuable insights into retinal disease progression.</p><p>To further illustrate the performance of our model, we have done consistency analysis and Bland-Altman agreement analysis on 20 groups of 3D segmentation results and corresponding manual annotations. As depicted in <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(a) and (c)</xref>, the lesions&#8217; volume and surface area were independently computed, and Pearson correlation analysis was employed to evaluate the agreement between the manual annotation and the prediction generated by the proposed network. In <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(a)</xref>, Volume GT denotes the lesions&#8217; volume derived from manual annotation, while Volume Pre represents the corresponding volume predicted by the network. Pearson correlation analysis between Volume GT and Volume Pre yielded a correlation coefficient of r&#8201;=&#8201;0.97828 (p&#8201;&lt;&#8201;0.0001). Similarly, in <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(c)</xref>, Area GT and Area Pre refer to the surface area obtained from manual annotation and model prediction, respectively, with Pearson analysis results showing r&#8201;=&#8201;0.98283 (p&#8201;&lt;&#8201;0.0001).</p><fig position=\"float\" id=\"g005\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 5.</label><caption><p>Consistency analysis and Bland-Altman agreement analysis of 20 groups of 3D segmentation results and corresponding manual annotations. (a), (b) are consistency analysis and Bland-Altman agreement analysis of lesion volume, respectively. (c), (d) are consistency analysis and Bland-Altman agreement analysis of lesions&#8217; surface area, respectively.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g005.jpg\"/></fig><p><xref rid=\"g005\" ref-type=\"fig\">Figure&#160;5(b)</xref> and <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(d)</xref> present Bland-Altman analysis for lesion volume and surface area, respectively. The solid blue line indicates the mean bias, and the dashed blue lines correspond to the upper and lower 95% limits of agreement. As shown in <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(b)</xref>, the mean bias for volume estimation is <inline-formula>\n<mml:math id=\"m28\" display=\"inline\" overflow=\"scroll\"><mml:mo>&#8722;\n</mml:mo><mml:mn>0.17131</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula>, with 95% limits ranging from <inline-formula>\n<mml:math id=\"m29\" display=\"inline\" overflow=\"scroll\"><mml:mo>&#8722;\n</mml:mo><mml:mn>0.93271</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula> to <inline-formula>\n<mml:math id=\"m30\" display=\"inline\" overflow=\"scroll\"><mml:mn>0.59079</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula>. In <xref rid=\"g005\" ref-type=\"fig\">Fig.&#160;5(d)</xref>, the mean bias for surface area estimation is <inline-formula>\n<mml:math id=\"m31\" display=\"inline\" overflow=\"scroll\"><mml:mo>&#8722;\n</mml:mo><mml:mn>3.48005</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula>, with 95% limits ranging from <inline-formula>\n<mml:math id=\"m32\" display=\"inline\" overflow=\"scroll\"><mml:mo>&#8722;\n</mml:mo><mml:mn>17.24880</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula> to <inline-formula>\n<mml:math id=\"m33\" display=\"inline\" overflow=\"scroll\"><mml:mn>10.27597</mml:mn><mml:mspace width=\"thickmathspace\"/><mml:mrow><mml:mtext>&#160;m</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mtext>m</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math>\n</inline-formula>. These results demonstrate a high degree of consistency between the predicted and manually annotated lesion measurements.</p><p>To ensure the reliability of our experiment, we use randomly selected data for testing, which are not seen when training and validating. <xref rid=\"g006\" ref-type=\"fig\">Figure&#160;6</xref> shows the 3D segmentation results of the testing set predicted by our network. The first column represents the original OCT volume to be segmented along with its corresponding B-scans, the second column displays the corresponding manually annotated 3D shapes of the lesions, and the last column represents the 3D segmentation results of our network. It can be demonstrated from the figure that our network consistently provides reliable segmentation results under the variation of number, shape, and size of DME lesions, while clearly showing the 3D morphology of the lesions in the final segmentation results.</p><fig position=\"float\" id=\"g006\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 6.</label><caption><p>3D visualization of original OCT images, original annotation, and segmentation results of testing sets. (a), (d), and (g) represent OCT 3D volumes consisting of massive B-scans from three subjects, (b), (e), and (h) represent the corresponding manually annotated 3D images, (c), (f), and (i) represent the 3D segmentation results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g006.jpg\"/></fig></sec><sec sec-type=\"discussions\" id=\"sec5\"><label>5.</label><title>Discussion</title><sec id=\"sec5-1\"><label>5.1.</label><title>Comparison experiments</title><p>In this section, we conducted a comprehensive comparison between our proposed network and\n\n five segmentation methods on our testing set, including 3D-UNet [<xref rid=\"r35\" ref-type=\"bibr\">35</xref>], 3D-UNet++ [<xref rid=\"r16\" ref-type=\"bibr\">16</xref>], 3D-AttentionUNet [<xref rid=\"r36\" ref-type=\"bibr\">36</xref>], 3D-MultiResUNet [<xref rid=\"r37\" ref-type=\"bibr\">37</xref>], and UNetr [<xref rid=\"r29\" ref-type=\"bibr\">29</xref>]. To thoroughly evaluate the 3D segmentation performance of our proposed network, we conducted a quantitative analysis using the average and standard deviation of key metrics, including DSC, ACC, precision, recall, and IOU.</p><p><xref rid=\"t001\" ref-type=\"table\">Table&#160;1</xref> presents the comparison results using five evaluation metrics, reported as mean&#8201;&#177;&#8201;standard deviation. The proposed method achieved average scores of 86.8% for DSC, 99.0% for ACC, 88.1% for precision, 86.6% for recall, and 77.2% for IOU. Compared with 3D-UNet [<xref rid=\"r35\" ref-type=\"bibr\">35</xref>], 3D-UNet++ [<xref rid=\"r16\" ref-type=\"bibr\">16</xref>], 3D-AttentionUNet [<xref rid=\"r36\" ref-type=\"bibr\">36</xref>], 3D-MultiResUNet [<xref rid=\"r37\" ref-type=\"bibr\">37</xref>], and UNetr [<xref rid=\"r29\" ref-type=\"bibr\">29</xref>], our method yielded a higher average DSC by 4.0%, 4.7%, 6.9%, 5.7%, and 4.4%, respectively. In terms of variability, the standard deviations were 7.4% for DSC, 0.3% for ACC, 6.6% for precision, 9.0% for recall, and 10.0% for IOU. Among these, our method exhibited the lowest standard deviations for DSC, ACC, and IOU across the compared models, indicating more stable performance on these metrics.</p><table-wrap position=\"float\" id=\"t001\" orientation=\"portrait\"><label>Table 1.</label><caption><title>Quantitative results of different methods in DME segmentation (mean&#8201;&#177;&#8201;standard deviation)</title></caption><table frame=\"hsides\" rules=\"all\"><colgroup span=\"1\"><col align=\"left\" width=\"17%\" span=\"1\"/><col align=\"center\" width=\"17%\" span=\"1\"/><col align=\"center\" width=\"17%\" span=\"1\"/><col align=\"center\" width=\"17%\" span=\"1\"/><col align=\"center\" width=\"17%\" span=\"1\"/><col align=\"center\" width=\"17%\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">DSC</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">ACC</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">precision</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">recall</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">IOU</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>3D-Unet</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.828&#8201;&#177;&#8201;0.097</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.985&#8201;&#177;&#8201;0.006</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.754&#8201;&#177;&#8201;0.125</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.936</bold>&#8201;&#177;&#8201;<bold>0.066</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.691&#8201;&#177;&#8201;0.128</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>3D-UNet++</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.821&#8201;&#177;&#8201;0.086</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.985&#8201;&#177;&#8201;0.005</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.905&#8201;&#177;&#8201;0.065</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.764&#8201;&#177;&#8201;0.109</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.717&#8201;&#177;&#8201;0.110</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>3D-AttentionUNet</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.799&#8201;&#177;&#8201;0.125</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.987&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.931</bold>&#8201;&#177;&#8201;<bold>0.007</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.722&#8201;&#177;&#8201;0.145</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.709&#8201;&#177;&#8201;0.145</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>3D-MultiResUNet</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.811&#8201;&#177;&#8201;0.104</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.985&#8201;&#177;&#8201;0.005</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.779&#8201;&#177;&#8201;0.109</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.870&#8201;&#177;&#8201;0.104</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.689&#8201;&#177;&#8201;0.124</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>UNetr</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.824&#8201;&#177;&#8201;0.088</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.986&#8201;&#177;&#8201;0.005</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.795&#8201;&#177;&#8201;0.109</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.877&#8201;&#177;&#8201;0.091</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.700&#8201;&#177;&#8201;0.119</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Our method</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.868</bold>&#8201;&#177;&#8201;<bold>0.074</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.990</bold>&#8201;&#177;&#8201;<bold>0.003</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.881&#8201;&#177;&#8201;0.066</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.866&#8201;&#177;&#8201;0.090</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.772</bold>&#8201;&#177;&#8201;<bold>0.100</bold></td></tr></tbody></table></table-wrap><p><xref rid=\"g007\" ref-type=\"fig\">Figure&#160;7</xref> shows one of the 3D visualizations of DME 3D segmentation results from different methods over the DME datasets and detailed segmentation results in the corresponding B-scan images. In challenging cases, such as scanned OCT images where frame-to-frame jumps occur due to the unconscious movement of patients, and areas that are interfering with segmentation, it can be difficult to get a good result, which always leads to incorrect segmentation. In <xref rid=\"g007\" ref-type=\"fig\">Fig.&#160;7</xref>, some methods, such as 3D-UNet, 3D-MultiResUNet, and UNetr generate fragments that do not belong to lesions of DME, especially parts in the blue dashed box and green dashed box.</p><fig position=\"float\" id=\"g007\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 7.</label><caption><p>3D visualization of DME 3D segmentation results from different methods over the DME datasets and detailed segmentation results in the corresponding B-scan images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g007.jpg\"/></fig><p>Although other methods like 3D-UNet++ and 3D-AttentionUNet produce less error segmentation, they still have the problem of insufficient ability to extract the details of the lesion area and over-segmentation. Compared with those methods, it can be found that our method can still achieve reliable segmentation results in the face of more difficult parts without exhibiting over-segmentation. These results demonstrate the effectiveness and robustness of our proposed method when performing 3D segmentation of DME, especially in challenging cases.</p></sec><sec id=\"sec5-2\"><label>5.2.</label><title>Ablation experiments</title><p><xref rid=\"t002\" ref-type=\"table\">Table&#160;2</xref> presents detailed ablation experiments conducted on the testing set to evaluate the effectiveness of 3D-MsGCS, 3D-MsNLFE, 3D-SCA, and 3D-MsFA. Results are reported as mean&#8201;&#177;&#8201;standard deviation. We chose UNetr [<xref rid=\"r29\" ref-type=\"bibr\">29</xref>] as a baseline, and then individually incorporated each block into the baseline to obtain Model 1, Model 2, and Model 3. Moreover, we combined these three modules in pairs to get Model 4, Model 5, and Model 6, in which Model 4 is 3D-MsGCS combined with 3D-MsNLFE and 3D-SCA, Model 5 is 3D-MsGCS combined with 3D-MsFA, while Model 6 is 3D-MsNLFE and 3D-SCA combined with 3D-MsFA. The last column in <xref rid=\"t002\" ref-type=\"table\">Table&#160;2</xref> shows the segmentation effect of the baseline with all modules. As shown in <xref rid=\"t002\" ref-type=\"table\">Table&#160;2</xref>, in Model 7, the average value of DSC improved by 4.4% compared with the baseline, while the standard deviation of ACC and precision reached 0.3% and 6.6% respectively.</p><table-wrap position=\"float\" id=\"t002\" orientation=\"portrait\"><label>Table 2.</label><caption><title>Ablation experiment results over our dataset (mean&#8201;&#177;&#8201;standard deviation)</title></caption><table frame=\"hsides\" rules=\"all\"><colgroup span=\"1\"><col align=\"left\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/><col align=\"center\" width=\"13%\" span=\"1\"/></colgroup><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"2\" colspan=\"1\">Network</th><th valign=\"top\" align=\"center\" colspan=\"3\" rowspan=\"1\">Module</th><th valign=\"top\" align=\"center\" colspan=\"4\" rowspan=\"1\">Metrics</th></tr><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3D-MsGCS</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3D-MsNLFE+3D-SCA</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3D-MsFA</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">DSC</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">ACC</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">precision</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">IOU</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Baseline</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.824&#8201;&#177;&#8201;0.088</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.986&#8201;&#177;&#8201;0.005</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.795&#8201;&#177;&#8201;0.109</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.700&#8201;&#177;&#8201;0.119</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.848&#8201;&#177;&#8201;0.070</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.987&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.800&#8201;&#177;&#8201;0.098</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.722&#8201;&#177;&#8201;0.104</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.849&#8201;&#177;&#8201;0.062</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.990&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.823&#8201;&#177;&#8201;0.087</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.727&#8201;&#177;&#8201;0.094</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 3</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.850&#8201;&#177;&#8201;0.073</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.988&#8201;&#177;&#8201;0.005</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.847&#8201;&#177;&#8201;0.073</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.735&#8201;&#177;&#8201;0.097</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.858&#8201;&#177;&#8201;0.069</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.988&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.825&#8201;&#177;&#8201;0.085</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.741&#8201;&#177;&#8201;0.100</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 5</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.861&#8201;&#177;&#8201;0.078</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.990&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.837&#8201;&#177;&#8201;0.092</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.751&#8201;&#177;&#8201;0.106</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 6</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.853&#8201;&#177;&#8201;0.079</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.989&#8201;&#177;&#8201;0.004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.870&#8201;&#177;&#8201;0.077</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.752&#8201;&#177;&#8201;0.105</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model 7</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8730;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.868</bold>&#8201;&#177;&#8201;<bold>0.074</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.990</bold>&#8201;&#177;&#8201;<bold>0.003</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.881</bold>&#8201;&#177;&#8201;<bold>0.066</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.772</bold>&#8201;&#177;&#8201;<bold>0.100</bold></td></tr></tbody></table></table-wrap><p>To further demonstrate the contributions of the modules added to the baseline, <xref rid=\"g008\" ref-type=\"fig\">Fig.&#160;8</xref> presents a representative segmentation result, including its 3D visualization and corresponding B-scan results. As shown, compared with the baseline, the inclusion of 3D-MsGCS, 3D-MsNLFE, 3D-SCA, and 3D-MsFA helps to mitigate both segmentation errors and over-segmentation to varying degrees, especially parts in the blue dashed box, green dashed box, and yellow dashed box.</p><fig position=\"float\" id=\"g008\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 8.</label><caption><p>3D visualization and detailed segmentation results in the corresponding B-scan images of manual annotation (GT), baseline (UNetr), Model 1 (baseline with only 3D-MsGCS), Model 2 (baseline with only 3D-MsNLFE and 3D-SCA), Model 3 (baseline with only 3D-MsFA), and Model 7 (baseline with all the modules).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g008.jpg\"/></fig></sec><sec id=\"sec5-3\"><label>5.3.</label><title>Clinical trial results</title><p>We can measure the central macular thickness (CMT) based on the volumetric visualization of fluid to evaluate the efficacy of treatments against macular edema [<xref rid=\"r38\" ref-type=\"bibr\">38</xref>]. <xref rid=\"g009\" ref-type=\"fig\">Figure&#160;9</xref> illustrates the progress of the DME patients over eight months, during which time the patient received six intravitreal injections. These pictures were arranged by time from left to right, representing different stages of treatment. The first row shows the 3D segmentation results of DME in 3D OCT images, the second row shows the B-scan images corresponding to the same scanning positions, and the third row shows the heatmap of CMT in the three stages. In the macular thickness heatmap of the three stages, the green parts represent the thickness at the normal value and critical value, respectively. The red part represents the thicker part. If the thickness is thicker, the color is redder. As shown in <xref rid=\"g009\" ref-type=\"fig\">Fig.&#160;9(a), (d), (g)</xref>, before initial treatment, the patient&#8217;s OCT scanning revealed an obvious abnormality in central macular thickness (CMT), which is measured to be 655 <inline-formula>\n<mml:math id=\"m34\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#956;\n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:mrow></mml:math>\n</inline-formula>. After four injections, the CMT dropped to 421 <inline-formula>\n<mml:math id=\"m35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#956;\n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:mrow></mml:math>\n</inline-formula>, which decreased significantly compared to before.</p><fig position=\"float\" id=\"g009\" fig-type=\"figure\" orientation=\"portrait\"><label>Fig. 9.</label><caption><p>Condition monitoring of a patient with DME at different stages of treatment. (a), (b) and (c) are the 3D segmentation results of the DME lesions of this patient before, during, and after treatment, respectively. (d), (e) and (f) are the B-scan images corresponding to the same scanning positions in different sections. (g), (h) and (i) are the corresponding macular thickness heatmaps in different sections.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"boe-16-9-3608-g009.jpg\"/></fig><p>After completing six initial treatments, as shown in <xref rid=\"g009\" ref-type=\"fig\">Fig.&#160;9(c), (f), and (i)</xref>, CMT was reduced further, which is measured to be 386 <inline-formula>\n<mml:math id=\"m36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#956;\n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:mrow></mml:math>\n</inline-formula>. The 3D segmentation results, B-scan images, and macular thickness heatmaps show the same trend of changes in DME lesions throughout treatment, which further demonstrates the accuracy of our 3D segmentation results in clinical use. The above experiment shows that the 3D segmentation results produced by our method can accurately and visually reflect changes in the fundus conditions of patients with DME. These results not only align with the diagnostic findings obtained through current clinical methods but also provide more detailed and comprehensive information about DME. Therefore, this method greatly simplifies the diagnosis and follow-up of the treatment effects of DME patients, while improving their acceptance and trust in the diagnostic results. Furthermore, the 3D segmentation results of DME lesions offer retina specialists an intuitive way to present more visualized information to patients, facilitating better communication.</p></sec></sec><sec sec-type=\"conclusions\" id=\"sec6\"><label>6.</label><title>Conclusion</title><p>In this paper, we proposed a novel 3D transformer-enhanced multi-scale global co-attention network for the automatic and accurate segmentation of DME and for revealing its 3D morphological characteristics. We integrated a transformer encoder with multi-scale feature aggregation, a non-local module, and global channel-spatial joint attention, enabling it to fully leverage the 3D information in OCT images and effectively extract global, multi-scale features with long-range dependencies. A 3D-MsNLFE and a 3D-SCA are introduced in the bottleneck to further extract multi-scale non-local features and suppress the interference of speckle noise and the variable shape of DME lesions. Additionally, the 3D-MsGCS module is incorporated into skip connections to enhance the model&#8217;s ability to learn multi-semantic global contextual features and suppress irrelevant local features. To further improve segmentation accuracy, the 3D-MsFA module is added in the final layer to capture more detailed semantic information.</p><p>Extensive experimental results demonstrated that our proposed method delivered outstanding 3D segmentation performance and robust generalization capabilities, potentially alleviating the time-consuming manual annotation by professional ophthalmologists and offering significant convenience for the clinical diagnosis and treatment of DME. Compared with other state-of-the-art methods, our approach achieves superior performance in 3D segmentation of DME lesions in OCT images, with DSC surpassing existing methods by 4.0%, 4.7%, 6.9%, 5.7%, and 4.4%, respectively. Even in challenging cases, our method consistently produced reliable 3D segmentation results and accurately visualized the 3D morphological characteristics of DME lesions. These findings indicated that our method not only achieved exceptional 3D segmentation performance but also exhibited strong generalization capabilities, ultimately offering significant benefits for the diagnosis and treatment of DME and providing valuable insights into ophthalmic diseases.</p></sec></body><back><sec sec-type=\"funding\" id=\"sec7\"><title>Funding</title><p>\n<funding-source rid=\"sp1\">National Natural Science Foundation of China\n<named-content content-type=\"doi\">10.13039/501100001809</named-content></funding-source> (\n<award-id rid=\"sp1\">61905036</award-id>); \n<funding-source rid=\"sp2\">China Postdoctoral Science Foundation\n<named-content content-type=\"doi\">10.13039/501100002858</named-content></funding-source> (\n<award-id rid=\"sp2\">2021T140090</award-id>, \n<award-id rid=\"sp2\">2019M663465</award-id>); \n<funding-source rid=\"sp3\">Fundamental Research Funds for the Central Universities\n<named-content content-type=\"doi\">10.13039/501100012226</named-content></funding-source> (\n<award-id rid=\"sp3\">ZYGX2021J012</award-id>); \n<funding-source rid=\"sp4\">Medico-Engineering Cooperation Funds from University of Electronic Science and Technology of China</funding-source> (\n<award-id rid=\"sp4\">ZYGX2021YGCX019</award-id>).</p></sec><sec sec-type=\"COI-statement\" id=\"sec8\"><title>Disclosures</title><p>The authors declare no conflicts of interest.</p></sec><sec sec-type=\"data-availability\" id=\"sec9\"><title>Data availability</title><p>Data underlying the results presented in this paper are not publicly available at this time but may be obtained from the authors upon reasonable request.</p></sec><ref-list><title>References</title><ref id=\"r1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ciulla</surname><given-names>T. A.</given-names></name><name name-style=\"western\"><surname>Amador</surname><given-names>A. G.</given-names></name><name name-style=\"western\"><surname>Zinman</surname><given-names>B.</given-names></name></person-group>, &#8220;<article-title>Diabetic Retinopathy and Diabetic Macular Edema: Pathophysiology, screening, and novel therapies</article-title>,&#8221; <source>Diabetes Care</source><volume>26</volume>(<issue>9</issue>), <fpage>2653</fpage>&#8211;<lpage>2664</lpage> (<year>2003</year>).<pub-id pub-id-type=\"doi\">10.2337/diacare.26.9.2653</pub-id><pub-id pub-id-type=\"pmid\">12941734</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mondal</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Nandi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Pramanik</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Application of deep learning algorithm for judicious use of anti-VEGF in diabetic macular edema</article-title>,&#8221; <source>Sci. Rep.</source><volume>15</volume>(<issue>1</issue>), <fpage>4569</fpage> (<year>2025</year>).<pub-id pub-id-type=\"doi\">10.1038/s41598-025-87290-3</pub-id><pub-id pub-id-type=\"pmid\">39915516</pub-id><pub-id pub-id-type=\"pmcid\">PMC11802850</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chung</surname><given-names>Y.-R.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>Y. H.</given-names></name><name name-style=\"western\"><surname>Ha</surname><given-names>S. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Role of inflammation in classification of diabetic macular edema by optical coherence tomography</article-title>,&#8221; <source>J. Diabetes Res.</source><volume>2019</volume>, <fpage>1</fpage>&#8211;<lpage>8</lpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1155/2019/8164250</pub-id><pub-id pub-id-type=\"pmcid\">PMC6939426</pub-id><pub-id pub-id-type=\"pmid\">31930145</pub-id></mixed-citation></ref><ref id=\"r4\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>F.</given-names></name></person-group>, &#8220;<article-title>Efficacy and safety of dexamethasone or triamcinolone in combination with anti-vascular endothelial growth factor therapy for diabetic macular edema: A systematic review and meta-analysis with trial sequential analysis</article-title>,&#8221; <source>PLoS One</source><volume>20</volume>(<issue>2</issue>), <fpage>e0318373</fpage> (<year>2025</year>).<pub-id pub-id-type=\"doi\">10.1371/journal.pone.0318373</pub-id><pub-id pub-id-type=\"pmid\">39919066</pub-id><pub-id pub-id-type=\"pmcid\">PMC11805578</pub-id></mixed-citation></ref><ref id=\"r5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Musat</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Cernat</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Labib</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Diabetic Macular Edema</article-title>,&#8221; <source>Rom. J. Ophthalmol.</source><volume>59</volume>, <fpage>133</fpage>&#8211;<lpage>136</lpage> (<year>2015</year>).<pub-id pub-id-type=\"pmid\">26978879</pub-id><pub-id pub-id-type=\"pmcid\">PMC5712956</pub-id></mixed-citation></ref><ref id=\"r6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saeedi</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Petersohn</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Salpea</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Global and regional diabetes prevalence estimates for 2019 and projections for 2030 and 2045: Results from the International Diabetes Federation Diabetes Atlas</article-title>,&#8221; <source>Diabetes Res. Clin. Pract.</source><volume>157</volume>, <fpage>107843</fpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1016/j.diabres.2019.107843</pub-id><pub-id pub-id-type=\"pmid\">31518657</pub-id></mixed-citation></ref><ref id=\"r7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lois</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Campbell</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Waugh</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Diabetic macular edema and diode subthreshold micropulse laser: a randomized double-masked noninferiority clinical trial</article-title>,&#8221; <source>Ophthalmology</source><volume>130</volume>(<issue>1</issue>), <fpage>14</fpage>&#8211;<lpage>27</lpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.1016/j.ophtha.2022.08.012</pub-id><pub-id pub-id-type=\"pmid\">35973593</pub-id></mixed-citation></ref><ref id=\"r8\"><label>8</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sabeena</surname><given-names>A. S.</given-names></name><name name-style=\"western\"><surname>Jeyakumar</surname><given-names>M. K.</given-names></name></person-group>, &#8220;<article-title>A hybrid model for diabetic retinopathy and diabetic macular edema severity grade classification</article-title>,&#8221; Int. J. Diabetes Dev. Ctries. (<year>2025</year>).</mixed-citation></ref><ref id=\"r9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brown</surname><given-names>D. M.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>Q. D.</given-names></name><name name-style=\"western\"><surname>Marcus</surname><given-names>D. M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Long-term outcomes of ranibizumab therapy for diabetic macular edema: the 36-month results from two phase III trials: RISE and RIDE</article-title>,&#8221; <source>Ophthalmology</source><volume>120</volume>(<issue>10</issue>), <fpage>2013</fpage>&#8211;<lpage>2022</lpage> (<year>2013</year>).<pub-id pub-id-type=\"doi\">10.1016/j.ophtha.2013.02.034</pub-id><pub-id pub-id-type=\"pmid\">23706949</pub-id></mixed-citation></ref><ref id=\"r10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Swanson</surname><given-names>E. A.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>C. P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optical coherence tomography</article-title>,&#8221; <source>Science</source><volume>254</volume>(<issue>5035</issue>), <fpage>1178</fpage>&#8211;<lpage>1181</lpage> (<year>1991</year>).<pub-id pub-id-type=\"doi\">10.1126/science.1957169</pub-id><pub-id pub-id-type=\"pmid\">1957169</pub-id><pub-id pub-id-type=\"pmcid\">PMC4638169</pub-id></mixed-citation></ref><ref id=\"r11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rasti</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Biglari</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Rezapourian</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>RetiFluidNet: A Self-Adaptive and Multi-Attention Deep Convolutional Network for Retinal OCT Fluid Segmentation</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>42</volume>(<issue>5</issue>), <fpage>1413</fpage>&#8211;<lpage>1423</lpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2022.3228285</pub-id><pub-id pub-id-type=\"pmid\">37015695</pub-id></mixed-citation></ref><ref id=\"r12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tsai</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Yezzi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Wells</surname><given-names>W.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A shape-based approach to the segmentation of medical imagery using level sets</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>22</volume>(<issue>2</issue>), <fpage>137</fpage>&#8211;<lpage>154</lpage> (<year>2003</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2002.808355</pub-id><pub-id pub-id-type=\"pmid\">12715991</pub-id></mixed-citation></ref><ref id=\"r13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>CE-Net: Context Encoder Network for 2D Medical Image Segmentation</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>38</volume>(<issue>10</issue>), <fpage>2281</fpage>&#8211;<lpage>2292</lpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2019.2903562</pub-id><pub-id pub-id-type=\"pmid\">30843824</pub-id></mixed-citation></ref><ref id=\"r14\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cheng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Superpixel Classification Based Optic Disc and Optic Cup Segmentation for Glaucoma Screening</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>32</volume>(<issue>6</issue>), <fpage>1019</fpage>&#8211;<lpage>1032</lpage> (<year>2013</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2013.2247770</pub-id><pub-id pub-id-type=\"pmid\">23434609</pub-id></mixed-citation></ref><ref id=\"r15\"><label>15</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>,&#8221; in <conf-name>Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</conf-name> (<publisher-name>Springer</publisher-name><year>2015</year>), pp. <fpage>234</fpage>&#8211;<lpage>241</lpage>.</mixed-citation></ref><ref id=\"r16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Siddiquee</surname><given-names>M. M. R.</given-names></name><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>39</volume>(<issue>6</issue>), <fpage>1856</fpage>&#8211;<lpage>1867</lpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2019.2959609</pub-id><pub-id pub-id-type=\"pmid\">31841402</pub-id><pub-id pub-id-type=\"pmcid\">PMC7357299</pub-id></mixed-citation></ref><ref id=\"r17\"><label>17</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Milletari</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Navab</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Ahmadi</surname><given-names>S. A.</given-names></name></person-group>, &#8220;<article-title>V-Net: Fully convolutional neural networks for volumetric medical image segmentation</article-title>,&#8221; <conf-name>the Proce. 4th Int. Conf. 3D Vision</conf-name> (Oct <year>2016</year>).</mixed-citation></ref><ref id=\"r18\"><label>18</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liang-Chieh Chen</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Papandreou</surname></name><etal>et al.</etal></person-group>, &#8220;<article-title>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</article-title>,&#8221; <source>arXiv</source> (<year>2018</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.1802.02611</pub-id></mixed-citation></ref><ref id=\"r19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group>, &#8220;<article-title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</article-title>,&#8221; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>39</volume>(<issue>12</issue>), <fpage>2481</fpage>&#8211;<lpage>2495</lpage> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type=\"pmid\">28060704</pub-id></mixed-citation></ref><ref id=\"r20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>CA-Net: comprehensive attention convolutional neural networks for explainable medical image segmentation</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>40</volume>(<issue>2</issue>), <fpage>699</fpage>&#8211;<lpage>711</lpage> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2020.3035253</pub-id><pub-id pub-id-type=\"pmid\">33136540</pub-id><pub-id pub-id-type=\"pmcid\">PMC7611411</pub-id></mixed-citation></ref><ref id=\"r21\"><label>21</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roy</surname><given-names>A. G.</given-names></name><name name-style=\"western\"><surname>Conjeti</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Karri</surname><given-names>S. P. K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks</article-title>,&#8221; <source>Biomed Opt Express</source><volume>8</volume>(<issue>8</issue>), <fpage>3627</fpage>&#8211;<lpage>3642</lpage> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.1364/BOE.8.003627</pub-id><pub-id pub-id-type=\"pmid\">28856040</pub-id><pub-id pub-id-type=\"pmcid\">PMC5560830</pub-id></mixed-citation></ref><ref id=\"r22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ji</surname><given-names>Q.</given-names></name></person-group>, &#8220;<article-title>MDAN-UNet: multi-scale and dual attention enhanced nested U-Net architecture for segmentation of optical coherence tomography images</article-title>,&#8221; <source>Algorithms</source><volume>13</volume>(<issue>3</issue>), <fpage>60</fpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.3390/a13030060</pub-id></mixed-citation></ref><ref id=\"r23\"><label>23</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Automatic fluid segmentation in retinal optical coherence tomography images using attention based deep learning</article-title>,&#8221; <source>Neurocomputing</source><volume>452</volume>, <fpage>576</fpage>&#8211;<lpage>591</lpage> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.1016/j.neucom.2020.07.143</pub-id></mixed-citation></ref><ref id=\"r24\"><label>24</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hassan</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Qin</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Ahmed</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep learning based joint segmentation and characterization of multi-class retinal fluid lesions on OCT scans for clinical use in anti-VEGF therapy</article-title>,&#8221; <source>Comput. Biol. Med.</source><volume>136</volume>, <fpage>104727</fpage> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2021.104727</pub-id><pub-id pub-id-type=\"pmid\">34385089</pub-id></mixed-citation></ref><ref id=\"r25\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bai</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>DME-DeepLabV3+: a lightweight model for diabetic macular edema extraction based on DeepLabV3+ architecture</article-title>,&#8221; <source>Front. Med.</source><volume>10</volume>, <fpage>1150295</fpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.3389/fmed.2023.1150295</pub-id><pub-id pub-id-type=\"pmcid\">PMC10515718</pub-id><pub-id pub-id-type=\"pmid\">37746086</pub-id></mixed-citation></ref><ref id=\"r26\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>George</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Shine</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>N</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A two-stage CNN model for the classification and severity analysis of retinal and choroidal diseases in OCT images</article-title>,&#8221; <source>International Journal of Intelligent Networks</source><volume>5</volume>, <fpage>10</fpage>&#8211;<lpage>18</lpage> (<year>2024</year>).<pub-id pub-id-type=\"doi\">10.1016/j.ijin.2024.01.002</pub-id></mixed-citation></ref><ref id=\"r27\"><label>27</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Automated segmentation of diabetic macular edema in OCT B-scan images based on RCU-Net</article-title>,&#8221; <source>Int. J. Imaging Syst. Tech.</source><volume>33</volume>(<issue>1</issue>), <fpage>299</fpage>&#8211;<lpage>311</lpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.1002/ima.22788</pub-id></mixed-citation></ref><ref id=\"r28\"><label>28</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>F.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>MsTGANet: Automatic Drusen Segmentation From Retinal OCT Images</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>41</volume>(<issue>2</issue>), <fpage>394</fpage>&#8211;<lpage>406</lpage> (<year>2022</year>).<pub-id pub-id-type=\"doi\">10.1109/TMI.2021.3112716</pub-id><pub-id pub-id-type=\"pmid\">34520349</pub-id></mixed-citation></ref><ref id=\"r29\"><label>29</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>UNETR: Transformers for 3D Medical Image Segmentation</article-title>,&#8221; in <conf-name>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</conf-name> (<year>2022</year>), pp. <fpage>1748</fpage>&#8211;<lpage>1758</lpage>.</mixed-citation></ref><ref id=\"r30\"><label>30</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>An Image is Worth 16&#8201;&#215;&#8201;16 Words: Transformers for Image Recognition at Scale</article-title>,&#8221; <source>arXiv</source> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.2010.11929</pub-id></mixed-citation></ref><ref id=\"r31\"><label>31</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bello</surname><given-names>I.</given-names></name></person-group>, &#8220;<article-title>Lambdanetworks: Modeling long-range interactions without attention</article-title>,&#8221; <source>arXiv</source> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.2102.08602</pub-id></mixed-citation></ref><ref id=\"r32\"><label>32</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>Z.</given-names></name></person-group>, &#8220;<article-title>XLNet: Generalized Autoregressive Pretraining for Language Understanding</article-title>,&#8221; <source>arXiv</source> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.1906.08237</pub-id></mixed-citation></ref><ref id=\"r33\"><label>33</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ashish Vaswani</surname><given-names>N. S.</given-names></name><name name-style=\"western\"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Uszkoreit</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Attention Is All You Need</article-title>,&#8221; <source>arXiv</source> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.1706.03762</pub-id></mixed-citation></ref><ref id=\"r34\"><label>34</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Salehi</surname><given-names>S. S. M.</given-names></name><name name-style=\"western\"><surname>Erdogmus</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Gholipour</surname><given-names>A.</given-names></name></person-group>, &#8220;<article-title>Tversky loss function for image segmentation using 3D fully convolutional deep networks</article-title>,&#8221; <source>arXiv</source> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.1706.05721</pub-id></mixed-citation></ref><ref id=\"r35\"><label>35</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title>,&#8221; LNCS (<year>2015</year>).</mixed-citation></ref><ref id=\"r36\"><label>36</label><mixed-citation publication-type=\"preprint\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oktay</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Schlemper</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Folgoc</surname><given-names>L. L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Attention U-Net: Learning Where to Look for the Pancreas</article-title>,&#8221; <source>arXiv</source> (<year>2018</year>).<pub-id pub-id-type=\"doi\">10.48550/arXiv.1804.03999</pub-id></mixed-citation></ref><ref id=\"r37\"><label>37</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ibtehaz</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Rahman</surname><given-names>M. S.</given-names></name></person-group>, &#8220;<article-title>MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation</article-title>,&#8221; <source>Neural Networks</source><volume>121</volume>, <fpage>74</fpage>&#8211;<lpage>87</lpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.1016/j.neunet.2019.08.025</pub-id><pub-id pub-id-type=\"pmid\">31536901</pub-id></mixed-citation></ref><ref id=\"r38\"><label>38</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Maggio</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Sartore</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Attanasio</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Anti-vascular endothelial growth factor treatment for diabetic macular edema in a real-world clinical setting</article-title>,&#8221; <source>Am. J. Ophthalmol.</source><volume>195</volume>, <fpage>209</fpage>&#8211;<lpage>222</lpage> (<year>2018</year>).<pub-id pub-id-type=\"doi\">10.1016/j.ajo.2018.08.004</pub-id><pub-id pub-id-type=\"pmid\">30098350</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Biomed Opt Express Biomed Opt Express 1350 boe BOE Biomedical Optics Express 2156-7085 Optica Publishing Group PMC12684072 PMC12684072.1 12684072 12684072 10.1364/BOE.564842 boe-16-9-3608 1 Article Three-dimensional transformer-enhanced multi-scale global co-attention network for precise diabetic macular edema segmentation in OCT volumes Hu Jiangting 1 &#8224; Li Chunxiu 2 &#8224; Lin Shuaichen 1 Qin Mohan 1 Wu Renxiong 1 Zhong Jie 2 Liu Yong 1 Ni Guangming https://orcid.org/0000-0002-4516-256X 1 * 1 School of Optoelectronic Science and Engineering, University of Electronic Science and Technology of China , Chengdu 611731, China 2 School of Medicine, University of Electronic Science and Technology of China , Chengdu 610054, China &#8224; These authors contributed equally. * guangmingni@uestc.edu.cn 14 8 2025 01 9 2025 16 9 502064 3608 3622 10 4 2025 03 7 2025 25 7 2025 14 08 2025 09 12 2025 09 12 2025 &#169; 2025 Optica Publishing Group 2025 Optica Publishing Group https://doi.org/10.1364/OA_License_v2#VOR-OA &#169; 2025 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement Diabetic macular edema (DME) has emerged as one of the leading causes of visual impairment worldwide, and optical coherence tomography (OCT) plays a pivotal role in detecting DME. Automatic and accurate segmentation of lesions in retinal OCT images is essential for early clinical diagnosis of DME, but most recent deep-learning methods are two-dimensional (2D) segmentation and fail to fully extract the lesions&#8217; critical three-dimensional (3D) information contained in OCT images. Here we proposed a novel 3D deep-learning network characterized by combining a transformer encoder with multi-scale feature aggregation, a non-local module, and global channel-spatial joint attention to obtain accurate 3D segmentation of DME and reveal their 3D morphological characteristics. Extensive experimental results demonstrate that our proposed method not only achieves commendable 3D segmentation performance with robust generalization capabilities in challenging cases, but also offers valuable insights into ophthalmic diseases, enhancing the convenience of clinical diagnosis and treatment of DME. National Natural Science Foundation of China 10.13039/501100001809 61905036 China Postdoctoral Science Foundation 10.13039/501100002858 2021T140090 2019M663465 Fundamental Research Funds for the Central Universities 10.13039/501100012226 ZYGX2021J012 Medico-Engineering Cooperation Funds from University of Electronic Science and Technology of China ZYGX2021YGCX019 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Diabetic macular edema (DME), characterized by the accumulation of fluid in the macula caused by increased vascular permeability [ 1 &#8211; 4 ], is currently the leading cause of blindness and associated functional impairment in the working-age population of most developed countries [ 5 ]. According to authoritative statistics, global estimates predict a significant rise in diabetes mellitus and macular edema diagnosis [ 4 , 6 , 7 ]. As a severe and prevalent complication of diabetes mellitus occurring in any phase of diabetic retinopathy (DR) progression [ 3 , 8 ], DME can trigger progressive retinal dysfunction, leading to irreversible and permanent vision loss [ 9 ]. To effectively prevent its progression to blindness, quantitative assessment of DME lesions is crucial to better understand the ophthalmic diseases and improve the corresponding diagnosis and treatment of DME. Optical coherence tomography (OCT) imaging, a non-invasive three-dimensional (3D) imaging technology [ 10 ], is now widely used for progress monitoring and damage assessment of DME. Manual segmentation of DME lesions in OCT images by professional ophthalmologists is time-consuming and inefficient; as a result, automatic and accurate segmentation methods are of substantial clinical significance. To analyze macular fluids automatically and accurately, different image processing and machine learning algorithms have been developed [ 11 ]. Previous approaches to segmenting lesions of medical images are mainly divided into several kinds: deformable models based on level sets [ 12 ], learning-based approaches [ 13 ], and pixel classification-based approaches [ 14 ]. In particular, deep-learning methods using convolutional neural networks (CNNs) have achieved impressive improvements in a variety of automated medical image segmentation tasks and provide a general approach to extracting the feature [ 13 ]. Over the past few years, several networks have been proposed and applied to medical imaging segmentation tasks, including U-Net [ 15 ], U-Net++ [ 16 ], V-Net [ 17 ], Deeplabv3+ [ 18 ], and Seg-Net [ 19 ]. Inspired by these networks, several specialists have proposed some other networks that have great accuracy and robustness in DME segmentation tasks. For example, R. Gu et al. [ 20 ] proposed CANet, which used ResNet50 with various attention modules to detect diabetic retinopathy and macular edema for diabetes. Roy et al. [ 21 ] proposed a new fully convolutional deep architecture termed ReLayNet, which uses a contracting path of convolutional blocks for semantic segmentation. W. Liu et al. [ 22 ] proposed a 2D fully convolutional network called MDAN-UNet for DME segmentation in OCT images, and they incorporated re-designed skip pathways, multi-scale input, multi-scale side output, and attention mechanisms into U-Net++ to enhance performance. X. Liu et al. [ 23 ] introduced the attention gates and dense skip connection into UNet to automatically locate the suspicious areas when segmenting DME lesions. Hassan et al. [ 24 ] proposed RFS-Net, which integrates ASPP modules with the inception module and residual learning to perform effectively in retinal fluid lesions segmentation tasks on OCT scans. Y. Bai et al. [ 25 ] introduced an improved ASPP module into MobileNetV2 to avoid grid effects and better extract high-level features of DME. George et al. [ 26 ] integrated a modified VGG16 network with a UNet model for the classification and segmentation of DME lesions. Wu et al. [ 27 ] proposed RCU-Net, which combined U-Net with residual structures and a convolutional block attention module to enhance gradient propagation and extract more abundant space and useful features on the channel for better DME segmentation results. Although these networks have demonstrated impressive accuracy and robustness in macular edema segmentation tasks, most of them are fully convolutional neural networks that focus solely on the two-dimensional (2D) properties of OCT images. As a result, these networks fail to fully leverage the 3D information present in retinal OCT volumes and cannot learn long-range spatial dependencies, which leads to a lack of 3D spatial consistency in the long-axis direction. Moreover, variations in lesion size and shape, blurred boundaries, background noise interference, and the low contrast of OCT imaging can lead to incorrect segmentation in areas that do not correspond to lesions, posing a significant challenge for the accurate segmentation of DME [ 28 ]. To address these limitations, we proposed a novel 3D transformer-enhanced multi-scale global co-attention network that can effectively explore DME features from OCT 3D volumes, thereby capturing the accurate 3D morphology of DME lesions. Instead of using a fully convolutional neural network, we employed transformers as the backbone, which not only maximizes the use of 3D structural and spatial information in OCT volumes but also effectively extracts global, multi-scale information with long-range dependencies. Specifically, we introduced a 3D multi-scale non-local feature extractor (3D-MsNLFE) and a 3D semantic context aggregator (3D-SCA) within the bottleneck to mitigate the effects of variable DME lesion sizes and shapes, as well as speckle noise. Additionally, we incorporated a 3D multi-scale global channel and spatial joint attention (3D-MsGCS) into skip connections to enhance the model&#8217;s ability to learn multi-semantic global contextual features across both channel and spatial dimensions, while suppressing irrelevant local features. Furthermore, a 3D multi-scale feature aggregation (3D-MsFA) module is introduced in the final layer to capture finer semantic details and further enhance segmentation accuracy. 2. Methods 2.1. Datasets This study was approved by the Institutional Review Board (IRB) of Sichuan Provincial People&#8217;s Hospital (IRB-2022-258). A swept-source OCT setup (BM-400 K BMizar, TowardPi, China) was used to obtain the datasets comprising OCT volumes of patients with DME. This system utilizes a vertical-cavity surface-emitting laser with a wavelength of 1060 nm and a scanning speed of 400000 A-scans per second. OCT scanning mode can obtain OCT images covering a 6 &#215; 6 mm area centered on the macula. Within this 6 &#215; 6 mm scan range, each retinal OCT scan consists of 512 A-lines per B-scan, resulting in a cube size of 512 &#215; 1024 &#215; 512 pixels ( X &#215; Y &#215; Z ) . Our dataset included 49 cases of DME volumes and corresponding manual pixel-level annotation from specialized ophthalmologists, and 2 cases of DME volumes without manual annotation remained for the clinic trial test. All participants underwent a comprehensive ocular examination including refraction and best-corrected visual acuity, non-contact intraocular pressure (IOP), ocular axis, slit lamp, and wide-angle fundus OCT imaging. This study was approved by the Ethics Committee of Sichuan Provincial People&#8217;s Hospital and adhered to the Declaration of Helsinki. Among the annotated data, 80% of the data was randomly chosen for training, 10% for validation, and the remaining 10% was reserved for testing. Figure&#160;1 shows the 3D visualization and the corresponding B-scans of our datasets. Figure&#160;1(a) illustrates the raw 3D retinal OCT image with DME. Figure&#160;1(b) demonstrates the 3D visualization of DME lesions. Figure&#160;1(c) presents the B-scan image corresponding to the green slice of the raw data in Fig.&#160;1(a) . Figure&#160;1(d) demonstrates the label image of the DME lesions cross-section. Fig. 1. 3D visualization and B-scans of OCT images with DME datasets. (a) represents the raw 3D retinal OCT images with DME. (b) demonstrates the 3D visualization of DME lesions. (c) represents the B-scan image corresponding to the green slice of raw data in (a). (d) represents the label image of the DME lesion cross-section. 2.2. Overview architecture To obtain automatic and accurate 3D segmentation results of DME lesions in retinal OCT images, we proposed a 3D transformer-enhanced multi-scale global co-attention network. As shown in Fig.&#160;2 , the network adopts a U-shaped architecture with a contracting-expanding path, in which a stack of 12 transformer blocks is used as the encoder. Unlike conventional fully convolutional networks, transformers encode volumetric data as a sequence of 1D patch embeddings and apply the self-attention mechanism [ 29 ] to model long-range dependencies and capture global contextual information [ 30 , 31 ], which is essential for volumetric medical imaging tasks. Fig. 2. Schematic overview of the proposed 3D segmentation network. When a 3D OCT volume is input into the encoder, it is first divided into non-overlapping patches. Each patch is flattened and projected into a fixed-dimensional embedding space via a linear layer, followed by the addition of a learnable positional embedding to preserve spatial information. The embedded sequence is then processed by a series of transformer blocks composed of multi-head self-attention (MSA) and multi-layer perceptron (MLP) to extract high-level representations [ 32 ]. Due to the small proportion and complex variability of DME lesions in OCT images, as well as the large background regions, it is essential to enhance lesion-related features while suppressing irrelevant information. To handle 3D volumetric OCT data and obtain precise segmentation results, we introduced and restructured a series of 3D attention-based modules adapted from previously validated 2D designs [ 28 ]. As shown in Fig.&#160;3(a), (b), and (c) , we incorporate the 3D multi-semantic global channel and spatial joint attention-based skip connection (3D-MsGCS) module, the 3D multi-scale non-local feature extractor (3D-MsNLFE) module, the 3D semantic context aggregator (3D-SCA) module, and the 3D multi-scale feature aggregation module (3D-MsFA) into our network architecture. Fig. 3. Details of 3D-MsGCS module, 3D-MsNLFE module and 3D-SCA module. (a) is 3D-MsGCS module, (b) is 3D-MsNLFE module, (c) is 3D-SCA module. As illustrated in Fig.&#160;3 , the 3D-MsGCS module is used in skip connections to bridge the encoder and decoder. It enables enhanced multi-semantic feature fusion in both channel and spatial dimensions, helping the decoder recover detailed structures while suppressing irrelevant local features [ 28 ]. In the bottleneck of the network, we utilized the 3D-MsNLFE module and 3D-SCA module to capture multi-scale and non-local features [ 28 ]. By aggregating outputs from multiple transformer layers with different receptive fields, they improve robustness against the variation in lesion size, shape, and speckle noise. Specifically, in the above three modules, we use a global learnable heatmap generated by learnable vectors to capture the multi-semantic global information. This type of position coding method has been commonly used in 2D image processing methods to make the self-attention operation sensitive to spatial positions of features [ 33 ]. In our work, we extend this 2D positional encoding by incorporating an additional depth dimension, enabling it to better model positional features in 3D volumetric data. This enhancement allows the network to capture long-range dependencies along all three spatial axes and focus more effectively on the positional relationships within the volume, which may help facilitate more accurate 3D structure modeling, particularly for small and irregular lesions in OCT volumes. In the final stage of the network, we employ a 3D multi-scale feature aggregation (3D-MsFA) module to enhance the segmentation results. Feature maps from multiple decoder levels are first compressed to a consistent channel dimension and refined using an attention mechanism to highlight informative features. These refined features are then fused across scales to integrate contextual information from different spatial resolutions. This multi-scale fusion is particularly beneficial for capturing the heterogeneous size and shape of DME lesions in volumetric OCT data. Finally, the aggregated output is passed through a convolutional layer and a sigmoid activation to produce the final 3D segmentation prediction. 2.3. Loss function As background noise and blurred boundaries make up a large portion of the obtained retinal image, precise segmentation of DME lesions is facing a significant challenge. To efficiently optimize the proposed model and mitigate the negative effects of the imbalance between foreground and background in a sample, we adopted a joint loss function to train our model, which can be defined as Eq.&#160;( 1 ): (1) L t o t a l = &#955; &#8901; T ( &#945; , &#946; ) + ( 1 &#8722; &#955; ) &#8901; L B C E , where T ( &#945; , &#160; &#946; ) represents the Tversky loss [ 34 ], serving as the supervised loss to evaluate the quality of the model output on labeled inputs. It can be expressed as Eq.&#160;( 2 ): (2) T ( &#945; , &#946; ) = &#8721; i = 1 N p 0 i g 0 i &#8721; i = 1 N p 0 i g 0 i + &#945; &#8721; i = 1 N p 0 i g 1 i + &#946; &#8721; i = 1 N p 1 i g 0 i , where p 0 i represents the probability of voxel i being a lesion and p 1 i represents the probability of being a non-lesion. And the g 0 i is 1 for a lesion voxel and 0 for a non-lesion voxel while g 1 i is 0 for a lesion voxel and 1 for a non-lesion voxel. Compared with the Dice loss, the Tversky loss introduces two coefficients &#945; and &#946; to better balance false negatives and false positives and thus achieves better performance than the Dice loss in multi-task segmentation. For our experiment, &#945; is set to 0.2, &#946; is set to 0.8 and &#955; is set to 0.8 for better performance. As expressed in Eq.&#160;( 3 ), L B C E represents the binary cross-entropy loss, and y i represents the annotated ground truth label for pixel i, which is 1 for a lesion voxel and 0 for a non-lesion voxel, and p ( y i ) represents the probability of the prediction label for pixel i. (3) L B C E = &#8722; 1 N &#8721; i = 1 N y i &#8901; log &#8289; ( p ( y i ) ) + ( 1 &#8722; y i ) &#8901; log &#8289; ( 1 &#8722; p ( y i ) ) 3. Experiment setup 3.1. Implementation details Since all the B-scans contained a lot of background area without information, to make full use of GPU resources and improve training efficiency, we cropped and resized the image patch into seven volumes with the size of 224 &#215; 224 &#215; 64 pixels ( X &#215; Y &#215; Z ) . We enhanced volume by flipping, rotating, and rolling at random vertical or horizontal directions to obtain four augmented volumes for each volumetric data in the training datasets. The proposed model was implemented in TensorFlow v2.5.0 with NVIDIA RTX A6000 (48 G) GPUs, and Python 3.8.8. In all comparative experiments. We use 2 GPUs to distribute the dataset as our training method, and the batch size for each GPU is 1. We set the maximum number of epochs and learning rate as 135 and 0.00005, respectively, and use Adam as the optimizer. 3.2. Performance measures To ensure accuracy and fairness when comparing the performance of different networks, we employ some classical metrics to quantitatively evaluate the obtained segmentation results, including the Dice similarity coefficient (DSC), balanced accuracy (ACC), precision, recall, specificity, and intersection over union (IOU). The range of these metrics is between 0 and 1, and if the value of the metrics is higher, the segmentation result is better. These metrics are obtained through the following equations: (4) D S C = 2 | T P | 2 | T P | + | F P | + | F N | , (5) A C C = 1 2 ( T P T P + F N + T N T N + F P ) , (6) P r e c i s i o n = T P T P + F P , (7) R e c a l l = T P T P + F N , (8) I O U = T P T P + FP + F N , where TP, TN, FP, and FN are true positive, true negative, false positive, and false negative for pixel classification, respectively. 4. Results Figure&#160;4 presents the 3D segmentation results of DME lesions in OCT images using our proposed network and ground truth. The original 3D OCT images consist of massive B-scans, the corresponding ground truth, and the 3D segmentation results predicted by our network are presented from left to right, respectively. Fig. 4. 3D visualization of original OCT volumes, original annotation, and segmentation results of validation sets. (a), (d), and (g) represent OCT 3D volumes consisting of massive B-scans from three subjects, (b), (e), and (h) represent the corresponding manually annotated 3D images, (c), (f), and (i) represent the 3D segmentation results. As shown in Fig.&#160;4 , the 3D segmentation results generated by our network closely resemble the actual morphology of DME lesions, which can assist ophthalmologists by reducing the burden of manual annotation. Despite the considerable variability in lesion shape, size, and number across patients, our method effectively captures these differences, which are often difficult to identify using traditional 2D segmentation methods. As a result, compared with 2D methods, 3D segmentation provides more intuitive and comprehensive morphological information, offering valuable insights into retinal disease progression. To further illustrate the performance of our model, we have done consistency analysis and Bland-Altman agreement analysis on 20 groups of 3D segmentation results and corresponding manual annotations. As depicted in Fig.&#160;5(a) and (c) , the lesions&#8217; volume and surface area were independently computed, and Pearson correlation analysis was employed to evaluate the agreement between the manual annotation and the prediction generated by the proposed network. In Fig.&#160;5(a) , Volume GT denotes the lesions&#8217; volume derived from manual annotation, while Volume Pre represents the corresponding volume predicted by the network. Pearson correlation analysis between Volume GT and Volume Pre yielded a correlation coefficient of r&#8201;=&#8201;0.97828 (p&#8201;&lt;&#8201;0.0001). Similarly, in Fig.&#160;5(c) , Area GT and Area Pre refer to the surface area obtained from manual annotation and model prediction, respectively, with Pearson analysis results showing r&#8201;=&#8201;0.98283 (p&#8201;&lt;&#8201;0.0001). Fig. 5. Consistency analysis and Bland-Altman agreement analysis of 20 groups of 3D segmentation results and corresponding manual annotations. (a), (b) are consistency analysis and Bland-Altman agreement analysis of lesion volume, respectively. (c), (d) are consistency analysis and Bland-Altman agreement analysis of lesions&#8217; surface area, respectively. Figure&#160;5(b) and Fig.&#160;5(d) present Bland-Altman analysis for lesion volume and surface area, respectively. The solid blue line indicates the mean bias, and the dashed blue lines correspond to the upper and lower 95% limits of agreement. As shown in Fig.&#160;5(b) , the mean bias for volume estimation is &#8722; 0.17131 &#160;m m 3 , with 95% limits ranging from &#8722; 0.93271 &#160;m m 3 to 0.59079 &#160;m m 3 . In Fig.&#160;5(d) , the mean bias for surface area estimation is &#8722; 3.48005 &#160;m m 2 , with 95% limits ranging from &#8722; 17.24880 &#160;m m 2 to 10.27597 &#160;m m 2 . These results demonstrate a high degree of consistency between the predicted and manually annotated lesion measurements. To ensure the reliability of our experiment, we use randomly selected data for testing, which are not seen when training and validating. Figure&#160;6 shows the 3D segmentation results of the testing set predicted by our network. The first column represents the original OCT volume to be segmented along with its corresponding B-scans, the second column displays the corresponding manually annotated 3D shapes of the lesions, and the last column represents the 3D segmentation results of our network. It can be demonstrated from the figure that our network consistently provides reliable segmentation results under the variation of number, shape, and size of DME lesions, while clearly showing the 3D morphology of the lesions in the final segmentation results. Fig. 6. 3D visualization of original OCT images, original annotation, and segmentation results of testing sets. (a), (d), and (g) represent OCT 3D volumes consisting of massive B-scans from three subjects, (b), (e), and (h) represent the corresponding manually annotated 3D images, (c), (f), and (i) represent the 3D segmentation results. 5. Discussion 5.1. Comparison experiments In this section, we conducted a comprehensive comparison between our proposed network and five segmentation methods on our testing set, including 3D-UNet [ 35 ], 3D-UNet++ [ 16 ], 3D-AttentionUNet [ 36 ], 3D-MultiResUNet [ 37 ], and UNetr [ 29 ]. To thoroughly evaluate the 3D segmentation performance of our proposed network, we conducted a quantitative analysis using the average and standard deviation of key metrics, including DSC, ACC, precision, recall, and IOU. Table&#160;1 presents the comparison results using five evaluation metrics, reported as mean&#8201;&#177;&#8201;standard deviation. The proposed method achieved average scores of 86.8% for DSC, 99.0% for ACC, 88.1% for precision, 86.6% for recall, and 77.2% for IOU. Compared with 3D-UNet [ 35 ], 3D-UNet++ [ 16 ], 3D-AttentionUNet [ 36 ], 3D-MultiResUNet [ 37 ], and UNetr [ 29 ], our method yielded a higher average DSC by 4.0%, 4.7%, 6.9%, 5.7%, and 4.4%, respectively. In terms of variability, the standard deviations were 7.4% for DSC, 0.3% for ACC, 6.6% for precision, 9.0% for recall, and 10.0% for IOU. Among these, our method exhibited the lowest standard deviations for DSC, ACC, and IOU across the compared models, indicating more stable performance on these metrics. Table 1. Quantitative results of different methods in DME segmentation (mean&#8201;&#177;&#8201;standard deviation) Method DSC ACC precision recall IOU 3D-Unet 0.828&#8201;&#177;&#8201;0.097 0.985&#8201;&#177;&#8201;0.006 0.754&#8201;&#177;&#8201;0.125 0.936 &#8201;&#177;&#8201; 0.066 0.691&#8201;&#177;&#8201;0.128 3D-UNet++ 0.821&#8201;&#177;&#8201;0.086 0.985&#8201;&#177;&#8201;0.005 0.905&#8201;&#177;&#8201;0.065 0.764&#8201;&#177;&#8201;0.109 0.717&#8201;&#177;&#8201;0.110 3D-AttentionUNet 0.799&#8201;&#177;&#8201;0.125 0.987&#8201;&#177;&#8201;0.004 0.931 &#8201;&#177;&#8201; 0.007 0.722&#8201;&#177;&#8201;0.145 0.709&#8201;&#177;&#8201;0.145 3D-MultiResUNet 0.811&#8201;&#177;&#8201;0.104 0.985&#8201;&#177;&#8201;0.005 0.779&#8201;&#177;&#8201;0.109 0.870&#8201;&#177;&#8201;0.104 0.689&#8201;&#177;&#8201;0.124 UNetr 0.824&#8201;&#177;&#8201;0.088 0.986&#8201;&#177;&#8201;0.005 0.795&#8201;&#177;&#8201;0.109 0.877&#8201;&#177;&#8201;0.091 0.700&#8201;&#177;&#8201;0.119 Our method 0.868 &#8201;&#177;&#8201; 0.074 0.990 &#8201;&#177;&#8201; 0.003 0.881&#8201;&#177;&#8201;0.066 0.866&#8201;&#177;&#8201;0.090 0.772 &#8201;&#177;&#8201; 0.100 Figure&#160;7 shows one of the 3D visualizations of DME 3D segmentation results from different methods over the DME datasets and detailed segmentation results in the corresponding B-scan images. In challenging cases, such as scanned OCT images where frame-to-frame jumps occur due to the unconscious movement of patients, and areas that are interfering with segmentation, it can be difficult to get a good result, which always leads to incorrect segmentation. In Fig.&#160;7 , some methods, such as 3D-UNet, 3D-MultiResUNet, and UNetr generate fragments that do not belong to lesions of DME, especially parts in the blue dashed box and green dashed box. Fig. 7. 3D visualization of DME 3D segmentation results from different methods over the DME datasets and detailed segmentation results in the corresponding B-scan images. Although other methods like 3D-UNet++ and 3D-AttentionUNet produce less error segmentation, they still have the problem of insufficient ability to extract the details of the lesion area and over-segmentation. Compared with those methods, it can be found that our method can still achieve reliable segmentation results in the face of more difficult parts without exhibiting over-segmentation. These results demonstrate the effectiveness and robustness of our proposed method when performing 3D segmentation of DME, especially in challenging cases. 5.2. Ablation experiments Table&#160;2 presents detailed ablation experiments conducted on the testing set to evaluate the effectiveness of 3D-MsGCS, 3D-MsNLFE, 3D-SCA, and 3D-MsFA. Results are reported as mean&#8201;&#177;&#8201;standard deviation. We chose UNetr [ 29 ] as a baseline, and then individually incorporated each block into the baseline to obtain Model 1, Model 2, and Model 3. Moreover, we combined these three modules in pairs to get Model 4, Model 5, and Model 6, in which Model 4 is 3D-MsGCS combined with 3D-MsNLFE and 3D-SCA, Model 5 is 3D-MsGCS combined with 3D-MsFA, while Model 6 is 3D-MsNLFE and 3D-SCA combined with 3D-MsFA. The last column in Table&#160;2 shows the segmentation effect of the baseline with all modules. As shown in Table&#160;2 , in Model 7, the average value of DSC improved by 4.4% compared with the baseline, while the standard deviation of ACC and precision reached 0.3% and 6.6% respectively. Table 2. Ablation experiment results over our dataset (mean&#8201;&#177;&#8201;standard deviation) Network Module Metrics 3D-MsGCS 3D-MsNLFE+3D-SCA 3D-MsFA DSC ACC precision IOU Baseline 0.824&#8201;&#177;&#8201;0.088 0.986&#8201;&#177;&#8201;0.005 0.795&#8201;&#177;&#8201;0.109 0.700&#8201;&#177;&#8201;0.119 Model 1 &#8730; 0.848&#8201;&#177;&#8201;0.070 0.987&#8201;&#177;&#8201;0.004 0.800&#8201;&#177;&#8201;0.098 0.722&#8201;&#177;&#8201;0.104 Model 2 &#8730; 0.849&#8201;&#177;&#8201;0.062 0.990&#8201;&#177;&#8201;0.004 0.823&#8201;&#177;&#8201;0.087 0.727&#8201;&#177;&#8201;0.094 Model 3 &#8730; 0.850&#8201;&#177;&#8201;0.073 0.988&#8201;&#177;&#8201;0.005 0.847&#8201;&#177;&#8201;0.073 0.735&#8201;&#177;&#8201;0.097 Model 4 &#8730; &#8730; 0.858&#8201;&#177;&#8201;0.069 0.988&#8201;&#177;&#8201;0.004 0.825&#8201;&#177;&#8201;0.085 0.741&#8201;&#177;&#8201;0.100 Model 5 &#8730; &#8730; 0.861&#8201;&#177;&#8201;0.078 0.990&#8201;&#177;&#8201;0.004 0.837&#8201;&#177;&#8201;0.092 0.751&#8201;&#177;&#8201;0.106 Model 6 &#8730; &#8730; 0.853&#8201;&#177;&#8201;0.079 0.989&#8201;&#177;&#8201;0.004 0.870&#8201;&#177;&#8201;0.077 0.752&#8201;&#177;&#8201;0.105 Model 7 &#8730; &#8730; &#8730; 0.868 &#8201;&#177;&#8201; 0.074 0.990 &#8201;&#177;&#8201; 0.003 0.881 &#8201;&#177;&#8201; 0.066 0.772 &#8201;&#177;&#8201; 0.100 To further demonstrate the contributions of the modules added to the baseline, Fig.&#160;8 presents a representative segmentation result, including its 3D visualization and corresponding B-scan results. As shown, compared with the baseline, the inclusion of 3D-MsGCS, 3D-MsNLFE, 3D-SCA, and 3D-MsFA helps to mitigate both segmentation errors and over-segmentation to varying degrees, especially parts in the blue dashed box, green dashed box, and yellow dashed box. Fig. 8. 3D visualization and detailed segmentation results in the corresponding B-scan images of manual annotation (GT), baseline (UNetr), Model 1 (baseline with only 3D-MsGCS), Model 2 (baseline with only 3D-MsNLFE and 3D-SCA), Model 3 (baseline with only 3D-MsFA), and Model 7 (baseline with all the modules). 5.3. Clinical trial results We can measure the central macular thickness (CMT) based on the volumetric visualization of fluid to evaluate the efficacy of treatments against macular edema [ 38 ]. Figure&#160;9 illustrates the progress of the DME patients over eight months, during which time the patient received six intravitreal injections. These pictures were arranged by time from left to right, representing different stages of treatment. The first row shows the 3D segmentation results of DME in 3D OCT images, the second row shows the B-scan images corresponding to the same scanning positions, and the third row shows the heatmap of CMT in the three stages. In the macular thickness heatmap of the three stages, the green parts represent the thickness at the normal value and critical value, respectively. The red part represents the thicker part. If the thickness is thicker, the color is redder. As shown in Fig.&#160;9(a), (d), (g) , before initial treatment, the patient&#8217;s OCT scanning revealed an obvious abnormality in central macular thickness (CMT), which is measured to be 655 &#956; m . After four injections, the CMT dropped to 421 &#956; m , which decreased significantly compared to before. Fig. 9. Condition monitoring of a patient with DME at different stages of treatment. (a), (b) and (c) are the 3D segmentation results of the DME lesions of this patient before, during, and after treatment, respectively. (d), (e) and (f) are the B-scan images corresponding to the same scanning positions in different sections. (g), (h) and (i) are the corresponding macular thickness heatmaps in different sections. After completing six initial treatments, as shown in Fig.&#160;9(c), (f), and (i) , CMT was reduced further, which is measured to be 386 &#956; m . The 3D segmentation results, B-scan images, and macular thickness heatmaps show the same trend of changes in DME lesions throughout treatment, which further demonstrates the accuracy of our 3D segmentation results in clinical use. The above experiment shows that the 3D segmentation results produced by our method can accurately and visually reflect changes in the fundus conditions of patients with DME. These results not only align with the diagnostic findings obtained through current clinical methods but also provide more detailed and comprehensive information about DME. Therefore, this method greatly simplifies the diagnosis and follow-up of the treatment effects of DME patients, while improving their acceptance and trust in the diagnostic results. Furthermore, the 3D segmentation results of DME lesions offer retina specialists an intuitive way to present more visualized information to patients, facilitating better communication. 6. Conclusion In this paper, we proposed a novel 3D transformer-enhanced multi-scale global co-attention network for the automatic and accurate segmentation of DME and for revealing its 3D morphological characteristics. We integrated a transformer encoder with multi-scale feature aggregation, a non-local module, and global channel-spatial joint attention, enabling it to fully leverage the 3D information in OCT images and effectively extract global, multi-scale features with long-range dependencies. A 3D-MsNLFE and a 3D-SCA are introduced in the bottleneck to further extract multi-scale non-local features and suppress the interference of speckle noise and the variable shape of DME lesions. Additionally, the 3D-MsGCS module is incorporated into skip connections to enhance the model&#8217;s ability to learn multi-semantic global contextual features and suppress irrelevant local features. To further improve segmentation accuracy, the 3D-MsFA module is added in the final layer to capture more detailed semantic information. Extensive experimental results demonstrated that our proposed method delivered outstanding 3D segmentation performance and robust generalization capabilities, potentially alleviating the time-consuming manual annotation by professional ophthalmologists and offering significant convenience for the clinical diagnosis and treatment of DME. Compared with other state-of-the-art methods, our approach achieves superior performance in 3D segmentation of DME lesions in OCT images, with DSC surpassing existing methods by 4.0%, 4.7%, 6.9%, 5.7%, and 4.4%, respectively. Even in challenging cases, our method consistently produced reliable 3D segmentation results and accurately visualized the 3D morphological characteristics of DME lesions. These findings indicated that our method not only achieved exceptional 3D segmentation performance but also exhibited strong generalization capabilities, ultimately offering significant benefits for the diagnosis and treatment of DME and providing valuable insights into ophthalmic diseases. Funding National Natural Science Foundation of China 10.13039/501100001809 ( 61905036 ); China Postdoctoral Science Foundation 10.13039/501100002858 ( 2021T140090 , 2019M663465 ); Fundamental Research Funds for the Central Universities 10.13039/501100012226 ( ZYGX2021J012 ); Medico-Engineering Cooperation Funds from University of Electronic Science and Technology of China ( ZYGX2021YGCX019 ). Disclosures The authors declare no conflicts of interest. Data availability Data underlying the results presented in this paper are not publicly available at this time but may be obtained from the authors upon reasonable request. References 1 Ciulla T. A. Amador A. G. Zinman B. , &#8220; Diabetic Retinopathy and Diabetic Macular Edema: Pathophysiology, screening, and novel therapies ,&#8221; Diabetes Care 26 ( 9 ), 2653 &#8211; 2664 ( 2003 ). 10.2337/diacare.26.9.2653 12941734 2 Mondal A. Nandi A. Pramanik S. et al. , &#8220; Application of deep learning algorithm for judicious use of anti-VEGF in diabetic macular edema ,&#8221; Sci. Rep. 15 ( 1 ), 4569 ( 2025 ). 10.1038/s41598-025-87290-3 39915516 PMC11802850 3 Chung Y.-R. Kim Y. H. Ha S. J. et al. , &#8220; Role of inflammation in classification of diabetic macular edema by optical coherence tomography ,&#8221; J. Diabetes Res. 2019 , 1 &#8211; 8 ( 2019 ). 10.1155/2019/8164250 PMC6939426 31930145 4 Zhou B. Liu H. Xiong F. , &#8220; Efficacy and safety of dexamethasone or triamcinolone in combination with anti-vascular endothelial growth factor therapy for diabetic macular edema: A systematic review and meta-analysis with trial sequential analysis ,&#8221; PLoS One 20 ( 2 ), e0318373 ( 2025 ). 10.1371/journal.pone.0318373 39919066 PMC11805578 5 Musat O. Cernat C. Labib M. et al. , &#8220; Diabetic Macular Edema ,&#8221; Rom. J. Ophthalmol. 59 , 133 &#8211; 136 ( 2015 ). 26978879 PMC5712956 6 Saeedi P. Petersohn I. Salpea P. et al. , &#8220; Global and regional diabetes prevalence estimates for 2019 and projections for 2030 and 2045: Results from the International Diabetes Federation Diabetes Atlas ,&#8221; Diabetes Res. Clin. Pract. 157 , 107843 ( 2019 ). 10.1016/j.diabres.2019.107843 31518657 7 Lois N. Campbell C. Waugh N. et al. , &#8220; Diabetic macular edema and diode subthreshold micropulse laser: a randomized double-masked noninferiority clinical trial ,&#8221; Ophthalmology 130 ( 1 ), 14 &#8211; 27 ( 2023 ). 10.1016/j.ophtha.2022.08.012 35973593 8 Sabeena A. S. Jeyakumar M. K. , &#8220; A hybrid model for diabetic retinopathy and diabetic macular edema severity grade classification ,&#8221; Int. J. Diabetes Dev. Ctries. ( 2025 ). 9 Brown D. M. Nguyen Q. D. Marcus D. M. et al. , &#8220; Long-term outcomes of ranibizumab therapy for diabetic macular edema: the 36-month results from two phase III trials: RISE and RIDE ,&#8221; Ophthalmology 120 ( 10 ), 2013 &#8211; 2022 ( 2013 ). 10.1016/j.ophtha.2013.02.034 23706949 10 Huang D. Swanson E. A. Lin C. P. et al. , &#8220; Optical coherence tomography ,&#8221; Science 254 ( 5035 ), 1178 &#8211; 1181 ( 1991 ). 10.1126/science.1957169 1957169 PMC4638169 11 Rasti R. Biglari A. Rezapourian M. et al. , &#8220; RetiFluidNet: A Self-Adaptive and Multi-Attention Deep Convolutional Network for Retinal OCT Fluid Segmentation ,&#8221; IEEE Trans. Med. Imaging 42 ( 5 ), 1413 &#8211; 1423 ( 2023 ). 10.1109/TMI.2022.3228285 37015695 12 Tsai A. Yezzi A. Wells W. et al. , &#8220; A shape-based approach to the segmentation of medical imagery using level sets ,&#8221; IEEE Trans. Med. Imaging 22 ( 2 ), 137 &#8211; 154 ( 2003 ). 10.1109/TMI.2002.808355 12715991 13 Gu Z. Cheng J. Fu H. et al. , &#8220; CE-Net: Context Encoder Network for 2D Medical Image Segmentation ,&#8221; IEEE Trans. Med. Imaging 38 ( 10 ), 2281 &#8211; 2292 ( 2019 ). 10.1109/TMI.2019.2903562 30843824 14 Cheng J. Liu J. Xu Y. et al. , &#8220; Superpixel Classification Based Optic Disc and Optic Cup Segmentation for Glaucoma Screening ,&#8221; IEEE Trans. Med. Imaging 32 ( 6 ), 1019 &#8211; 1032 ( 2013 ). 10.1109/TMI.2013.2247770 23434609 15 Ronneberger O. Fischer P. Brox T. , &#8220; U-net: Convolutional networks for biomedical image segmentation ,&#8221; in Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 ( Springer 2015 ), pp. 234 &#8211; 241 . 16 Zhou Z. Siddiquee M. M. R. Tajbakhsh N. et al. , &#8220; UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation ,&#8221; IEEE Trans. Med. Imaging 39 ( 6 ), 1856 &#8211; 1867 ( 2020 ). 10.1109/TMI.2019.2959609 31841402 PMC7357299 17 Milletari F. Navab N. Ahmadi S. A. , &#8220; V-Net: Fully convolutional neural networks for volumetric medical image segmentation ,&#8221; the Proce. 4th Int. Conf. 3D Vision (Oct 2016 ). 18 Liang-Chieh Chen Y. Zhu G. Papandreou et al. , &#8220; Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation ,&#8221; arXiv ( 2018 ). 10.48550/arXiv.1802.02611 19 Badrinarayanan V. Kendall A. Cipolla R. , &#8220; SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation ,&#8221; IEEE Trans. Pattern Anal. Mach. Intell. 39 ( 12 ), 2481 &#8211; 2495 ( 2017 ). 10.1109/TPAMI.2016.2644615 28060704 20 Gu R. Wang G. Song T. et al. , &#8220; CA-Net: comprehensive attention convolutional neural networks for explainable medical image segmentation ,&#8221; IEEE Trans. Med. Imaging 40 ( 2 ), 699 &#8211; 711 ( 2021 ). 10.1109/TMI.2020.3035253 33136540 PMC7611411 21 Roy A. G. Conjeti S. Karri S. P. K. et al. , &#8220; ReLayNet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks ,&#8221; Biomed Opt Express 8 ( 8 ), 3627 &#8211; 3642 ( 2017 ). 10.1364/BOE.8.003627 28856040 PMC5560830 22 Liu W. Sun Y. Ji Q. , &#8220; MDAN-UNet: multi-scale and dual attention enhanced nested U-Net architecture for segmentation of optical coherence tomography images ,&#8221; Algorithms 13 ( 3 ), 60 ( 2020 ). 10.3390/a13030060 23 Liu X. Wang S. Zhang Y. et al. , &#8220; Automatic fluid segmentation in retinal optical coherence tomography images using attention based deep learning ,&#8221; Neurocomputing 452 , 576 &#8211; 591 ( 2021 ). 10.1016/j.neucom.2020.07.143 24 Hassan B. Qin S. Ahmed R. et al. , &#8220; Deep learning based joint segmentation and characterization of multi-class retinal fluid lesions on OCT scans for clinical use in anti-VEGF therapy ,&#8221; Comput. Biol. Med. 136 , 104727 ( 2021 ). 10.1016/j.compbiomed.2021.104727 34385089 25 Bai Y. Li J. Shi L. et al. , &#8220; DME-DeepLabV3+: a lightweight model for diabetic macular edema extraction based on DeepLabV3+ architecture ,&#8221; Front. Med. 10 , 1150295 ( 2023 ). 10.3389/fmed.2023.1150295 PMC10515718 37746086 26 George N. Shine L. N A. et al. , &#8220; A two-stage CNN model for the classification and severity analysis of retinal and choroidal diseases in OCT images ,&#8221; International Journal of Intelligent Networks 5 , 10 &#8211; 18 ( 2024 ). 10.1016/j.ijin.2024.01.002 27 Wu J. Zhang Y. Xiao Z. et al. , &#8220; Automated segmentation of diabetic macular edema in OCT B-scan images based on RCU-Net ,&#8221; Int. J. Imaging Syst. Tech. 33 ( 1 ), 299 &#8211; 311 ( 2023 ). 10.1002/ima.22788 28 Wang M. Zhu W. Shi F. et al. , &#8220; MsTGANet: Automatic Drusen Segmentation From Retinal OCT Images ,&#8221; IEEE Trans. Med. Imaging 41 ( 2 ), 394 &#8211; 406 ( 2022 ). 10.1109/TMI.2021.3112716 34520349 29 Hatamizadeh A. Tang Y. Nath V. et al. , &#8220; UNETR: Transformers for 3D Medical Image Segmentation ,&#8221; in 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) ( 2022 ), pp. 1748 &#8211; 1758 . 30 Dosovitskiy A. Beyer L. Kolesnikov A. et al. , &#8220; An Image is Worth 16&#8201;&#215;&#8201;16 Words: Transformers for Image Recognition at Scale ,&#8221; arXiv ( 2020 ). 10.48550/arXiv.2010.11929 31 Bello I. , &#8220; Lambdanetworks: Modeling long-range interactions without attention ,&#8221; arXiv ( 2021 ). 10.48550/arXiv.2102.08602 32 Yang Z. , &#8220; XLNet: Generalized Autoregressive Pretraining for Language Understanding ,&#8221; arXiv ( 2019 ). 10.48550/arXiv.1906.08237 33 Ashish Vaswani N. S. Parmar N. Uszkoreit J. et al. , &#8220; Attention Is All You Need ,&#8221; arXiv ( 2023 ). 10.48550/arXiv.1706.03762 34 Salehi S. S. M. Erdogmus D. Gholipour A. , &#8220; Tversky loss function for image segmentation using 3D fully convolutional deep networks ,&#8221; arXiv ( 2017 ). 10.48550/arXiv.1706.05721 35 Ronneberger O. Fischer P. Brox T. , &#8220; U-Net: Convolutional Networks for Biomedical Image Segmentation ,&#8221; LNCS ( 2015 ). 36 Oktay O. Schlemper J. Folgoc L. L. et al. , &#8220; Attention U-Net: Learning Where to Look for the Pancreas ,&#8221; arXiv ( 2018 ). 10.48550/arXiv.1804.03999 37 Ibtehaz N. Rahman M. S. , &#8220; MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation ,&#8221; Neural Networks 121 , 74 &#8211; 87 ( 2020 ). 10.1016/j.neunet.2019.08.025 31536901 38 Maggio E. Sartore M. Attanasio M. et al. , &#8220; Anti-vascular endothelial growth factor treatment for diabetic macular edema in a real-world clinical setting ,&#8221; Am. J. Ophthalmol. 195 , 209 &#8211; 222 ( 2018 ). 10.1016/j.ajo.2018.08.004 30098350"
}