{
  "pmcid": "PMC12673127",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.111263",
  "metadata": {
    "journal_title": "NPJ Digital Medicine",
    "journal_nlm_ta": "NPJ Digit Med",
    "journal_iso_abbrev": "NPJ Digit Med",
    "journal": "NPJ Digital Medicine",
    "pmcid": "PMC12673127",
    "pmid": "41310368",
    "doi": "10.1038/s41746-025-02041-y",
    "title": "ShapeField-lung: continuous shape embedding for early lung cancer detection via pulmonary nodule segmentation",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Gu Xuyu",
      "Zhu Yifei",
      "Li Chuangqi",
      "Xu Xinnan",
      "Jin Kaiqi",
      "Xu Li"
    ],
    "abstract": "Accurate segmentation of pulmonary nodules in low-dose CT (LDCT) is vital for early lung cancer detection. Existing voxel-based methods often fail to capture irregular nodule boundaries, especially under noisy, low-contrast conditions. We propose ShapeField-Nodule, a continuous shape embedding framework that models nodule geometry as a signed distance field (SDF), enabling sub-voxel precision and anatomically coherent contours. Our method integrates a lightweight MLP-based implicit head with a 3D U-Net backbone to predict dense SDF values, and introduces a shape-aware refinement loss that aligns SDF gradients with image edges. Unlike discrete masks, our representation enforces boundary smoothness, topology regularization, and robustness to perturbations. Evaluations on LIDC-IDRI, LUNA16, and Tianchi datasets show state-of-the-art Dice and surface metrics. Extensive experiments demonstrate superior generalization, robustness under noise, and inference efficiency, highlighting the potential of continuous implicit fields as a principled alternative for medical image segmentation.",
    "keywords": [
      "Cancer",
      "Computational biology and bioinformatics",
      "Mathematics and computing",
      "Medical research"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">NPJ Digit Med</journal-id><journal-id journal-id-type=\"iso-abbrev\">NPJ Digit Med</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3605</journal-id><journal-id journal-id-type=\"pmc-domain\">npjdigitmed</journal-id><journal-title-group><journal-title>NPJ Digital Medicine</journal-title></journal-title-group><issn pub-type=\"epub\">2398-6352</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12673127</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12673127.1</article-id><article-id pub-id-type=\"pmcaid\">12673127</article-id><article-id pub-id-type=\"pmcaiid\">12673127</article-id><article-id pub-id-type=\"pmid\">41310368</article-id><article-id pub-id-type=\"doi\">10.1038/s41746-025-02041-y</article-id><article-id pub-id-type=\"publisher-id\">2041</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ShapeField-lung: continuous shape embedding for early lung cancer detection via pulmonary nodule segmentation</article-title></title-group><contrib-group><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Gu</surname><given-names initials=\"X\">Xuyu</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Zhu</surname><given-names initials=\"Y\">Yifei</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"C\">Chuangqi</given-names></name><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"X\">Xinnan</given-names></name><address><email>xxn@tongji.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Jin</surname><given-names initials=\"K\">Kaiqi</given-names></name><address><email>kaiqijin@tongji.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"L\">Li</given-names></name><address><email>xl_shp@tongji.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/033nbnf69</institution-id><institution-id institution-id-type=\"GRID\">grid.412532.3</institution-id><institution>Department of Oncology, </institution><institution>Shanghai Pulmonary Hospital, </institution></institution-wrap>Shanghai, China </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00my25942</institution-id><institution-id institution-id-type=\"GRID\">grid.452404.3</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1808 0942</institution-id><institution>Cancer Institute, </institution><institution>Fudan University Shanghai Cancer Center, </institution></institution-wrap>Shanghai, China </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01zntxs11</institution-id><institution-id institution-id-type=\"GRID\">grid.11841.3d</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0619 8943</institution-id><institution>Department of Oncology, </institution><institution>Shanghai Medical College of Fudan University, </institution></institution-wrap>Shanghai, China </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03cve4549</institution-id><institution-id institution-id-type=\"GRID\">grid.12527.33</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0662 3178</institution-id><institution>School of Vehicle and Mobility, </institution><institution>Tsinghua University, </institution></institution-wrap>Beijing, China </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/033nbnf69</institution-id><institution-id institution-id-type=\"GRID\">grid.412532.3</institution-id><institution>Department of Thoracic Surgery, </institution><institution>Shanghai Pulmonary Hospital, </institution></institution-wrap>Shanghai, China </aff></contrib-group><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>8</volume><issue-id pub-id-type=\"pmc-issue-id\">478273</issue-id><elocation-id>736</elocation-id><history><date date-type=\"received\"><day>4</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>25</day><month>9</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>04</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-04 00:25:12.810\"><day>04</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41746_2025_Article_2041.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Accurate segmentation of pulmonary nodules in low-dose CT (LDCT) is vital for early lung cancer detection. Existing voxel-based methods often fail to capture irregular nodule boundaries, especially under noisy, low-contrast conditions. We propose ShapeField-Nodule, a continuous shape embedding framework that models nodule geometry as a signed distance field (SDF), enabling sub-voxel precision and anatomically coherent contours. Our method integrates a lightweight MLP-based implicit head with a 3D U-Net backbone to predict dense SDF values, and introduces a shape-aware refinement loss that aligns SDF gradients with image edges. Unlike discrete masks, our representation enforces boundary smoothness, topology regularization, and robustness to perturbations. Evaluations on LIDC-IDRI, LUNA16, and Tianchi datasets show state-of-the-art Dice and surface metrics. Extensive experiments demonstrate superior generalization, robustness under noise, and inference efficiency, highlighting the potential of continuous implicit fields as a principled alternative for medical image segmentation.</p></abstract><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Cancer</kwd><kwd>Computational biology and bioinformatics</kwd><kwd>Mathematics and computing</kwd><kwd>Medical research</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\" sec-type=\"introduction\"><title>Introduction</title><p id=\"Par2\">Lung cancer remains the leading cause of cancer-related mortality worldwide, and early detection is paramount for improving patient survival rates<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref>,<xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. Low-dose computed tomography (LDCT) has emerged as a clinically viable modality for large-scale screening due to its reduced radiation burden and its capacity to detect small pulmonary nodules<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref>,<xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. However, interpreting LDCT scans is challenging even for radiologists, particularly when nodules present with subtle, irregular morphologies under noisy and low-contrast conditions. Accurate delineation of pulmonary nodules is essential not only for diagnostic purposes, but also for downstream malignancy risk estimation, longitudinal tracking, and surgical planning<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref>,<xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>.</p><sec id=\"Sec2\"><title>Automated segmentation of pulmonary nodules</title><p id=\"Par3\">has therefore been a long-standing goal in medical image computing. Existing methods predominantly formulate this task as a voxel-wise binary classification problem, mapping each voxel to either foreground (nodule) or background labels using convolutional neural networks (CNNs). Architectures such as 3D U-Net<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup> and attention-based extensions have achieved significant progress, yet they often struggle to model the fine geometric structures of nodules. Specifically, discrete voxel masks suffer from limited resolution, boundary discontinuity, and lack of shape regularity, particularly in the presence of partial volume effects and acquisition artifacts<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>.</p><p id=\"Par4\">Moreover, such discrete representations make it difficult to enforce anatomical plausibility, as they do not explicitly encode continuity or topological constraints. For pulmonary nodules-&#8220;which can vary greatly in size, attachment to vessels or pleura, and internal texture-&#8221; this becomes a major bottleneck. These limitations have motivated the search for alternative formulations that model shape in a more continuous and structured fashion.</p></sec><sec id=\"Sec3\"><title>Recent advances in implicit neural representations</title><p id=\"Par5\">Such as Signed Distance Functions (SDFs), have demonstrated remarkable capabilities in capturing detailed, continuous geometry in various vision tasks<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>. Instead of predicting binary occupancy, these methods learn continuous scalar fields that represent the distance from each spatial location to an object&#8217;s surface, offering sub-voxel precision and natural boundary regularization. While initially developed for 3D shape modeling, SDFs have since been applied to human pose estimation<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, medical image registration<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>, and shape completion<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>. However, their integration into volumetric medical image segmentation pipelines remains limited.</p></sec><sec id=\"Sec4\"><title>In this paper, we propose ShapeField-Nodule</title><p id=\"Par6\">A novel implicit shape embedding framework for pulmonary nodule segmentation in LDCT. Our method models the nodule boundary as a continuous signed distance field, regressed by a lightweight MLP head attached to a 3D U-Net backbone. By predicting SDF values across the input volume, our method enables smooth shape reconstruction with sub-voxel accuracy, inherently encoding shape continuity and topological regularity.</p><p id=\"Par7\">To ensure the SDF captures anatomically meaningful boundaries, we introduce a <italic toggle=\"yes\">shape-aware refinement loss</italic> that aligns the predicted SDF gradients with edge evidence extracted from the image. This loss encourages the zero-level set (i.e., the predicted boundary) to coincide with strong intensity gradients, thereby improving edge adherence and mitigating over-smoothing. Unlike previous works that rely on post-hoc surface extraction, our approach is end-to-end trainable and requires no external surface fitting.</p><p id=\"Par8\">We evaluate ShapeField-Nodule on the publicly available LIDC-IDRI dataset<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, which contains a diverse range of pulmonary nodules annotated by multiple radiologists. Our method achieves superior performance compared to state-of-the-art voxel-based segmentation models, both in overlap-based metrics (Dice score) and boundary-based metrics (average surface distance, Hausdorff distance). Qualitative results further demonstrate improved boundary smoothness, topological fidelity, and robustness under challenging conditions.</p><p id=\"Par9\">To summarize, our contributions are:<list list-type=\"bullet\"><list-item><p id=\"Par10\">We introduce <bold>ShapeField-Nodule</bold>, the first continuous SDF-based segmentation framework tailored for pulmonary nodule analysis in LDCT, enabling sub-voxel geometric modeling with topological consistency.</p></list-item><list-item><p id=\"Par11\">We design a <bold>hybrid implicit-explicit architecture</bold> that integrates an MLP SDF decoder with a 3D U-Net backbone, combining dense feature extraction with continuous shape representation.</p></list-item><list-item><p id=\"Par12\">We propose a novel <bold>shape-aware refinement loss</bold> that aligns SDF gradients with image edge cues, significantly enhancing boundary sharpness and anatomical plausibility.</p></list-item><list-item><p id=\"Par13\">We conduct extensive quantitative and qualitative evaluations on the LIDC-IDRI benchmark, demonstrating that ShapeField-Nodule outperforms conventional voxel-based baselines in both accuracy and shape coherence.</p></list-item></list></p></sec><sec id=\"Sec5\"><title>Related work</title><sec id=\"Sec6\"><title>Pulmonary nodule segmentation in LDCT</title><p id=\"Par14\">Pulmonary nodule segmentation in low-dose CT (LDCT) remains a critical challenge in medical image analysis due to the subtle boundaries and morphological diversity of nodules. Early works relied on handcrafted features and classical image processing<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>, while deep learning-based methods have now become dominant. 3D convolutional neural networks, especially U-Net variants, are widely adopted due to their ability to model volumetric context<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>,<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>,<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>.</p><p id=\"Par15\">Several studies focused on enhancing representation power via attention mechanisms<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, multi-scale processing<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>, or residual connections<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>, yet most of them still rely on discrete voxel-wise segmentation. While accurate on coarse metrics like Dice, these methods often produce irregular and noisy boundaries, especially for juxtapleural or spiculated nodules. More recent approaches introduce uncertainty modeling<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup> and radiomics-informed networks<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>, yet struggle to produce geometrically coherent outputs. More recently, the landscape of medical image segmentation has been further advanced by the introduction of Vision Transformers (ViTs) and their variants. Architectures such as Swin-Unet and UNETR have demonstrated remarkable success by combining the hierarchical feature extraction capabilities of CNNs with the long-range dependency modeling of transformers. These hybrid models have set new state-of-the-art benchmarks in various segmentation tasks by effectively capturing both local texture details and global contextual information. While these methods excel at voxel-level classification accuracy, they still produce discrete outputs and may not inherently enforce the geometric smoothness and topological consistency that our continuous SDF-based approach is designed to address. Our work is positioned as an orthogonal improvement, focusing on the nature of the shape representation itself rather than solely on the feature extraction backbone.</p></sec><sec id=\"Sec7\"><title>Implicit shape representations in vision</title><p id=\"Par16\">Implicit neural representations have gained significant attention in computer vision and graphics for modeling 3D geometry with sub-pixel precision. These approaches typically represent a shape as a continuous field-\"e.g., signed distance functions (SDFs) or occupancy fields-&#8221;predicted via coordinate-based MLPs<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR13\">13</xref>,<xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>. This continuous formulation enables smooth surface reconstruction and differentiable geometry extraction via level-set operations.</p><p id=\"Par17\">DeepSDF<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup> introduced the use of neural networks to regress SDF values from 3D coordinates, enabling accurate modeling of complex object surfaces. Follow-up works enhanced this idea by incorporating shape priors<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, semantic conditioning<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup>, or camera supervision<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup>. Although implicit fields have been extensively explored for shape reconstruction, pose estimation, and neural rendering, their integration into semantic segmentation tasks-\"especially in medical domains-&#8221; remains limited.</p></sec><sec id=\"Sec8\"><title>Signed distance fields in medical imaging</title><p id=\"Par18\">The utility of SDFs in medical imaging has only recently gained traction. One line of work uses SDFs as intermediate representations to guide surface extraction or as self-supervision for registration tasks. For example, SDFReg<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup> leverages SDFs for unsupervised deformable registration, while<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> applies implicit fields for organ completion. Others integrate SDF constraints into network training to regularize boundary smoothness<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup>.</p><p id=\"Par19\">However, few methods predict SDFs directly from input volumes in an end-to-end learnable way. Notably,<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup> propose an SDF-supervised segmentation for cardiac MRI, but rely on post-processing steps for final boundary reconstruction. Our method departs from this by directly regressing a volumetric SDF field and using its level-set as a continuous and anatomically meaningful segmentation output.</p></sec><sec id=\"Sec9\"><title>Topology-aware and shape-consistent segmentation</title><p id=\"Par20\">Traditional segmentation networks often neglect topological properties such as continuity, smoothness, or anatomical validity. This limitation has motivated shape-aware segmentation frameworks that impose priors or regularizers during training. For example,<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup> introduces differentiable topology constraints, while<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup> uses conditional random fields (CRFs) to enforce boundary adherence.</p><p id=\"Par21\">In 3D medical imaging, several efforts integrate surface-based losses (e.g., surface Dice<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup>) or Laplacian constraints to improve boundary alignment. Yet, these approaches remain fundamentally discrete. Implicit field-based formulations offer a more principled alternative by modeling boundaries continuously. Our work builds on this by proposing a shape refinement loss that aligns SDF gradients with image-derived edge cues, effectively guiding the learning process towards shape-consistent predictions.</p></sec></sec></sec><sec id=\"Sec10\" sec-type=\"results\"><title>Results</title><p id=\"Par22\">We conduct extensive experiments to evaluate the performance of our proposed <bold>ShapeField-Nodule</bold> framework on the task of pulmonary nodule segmentation in low-dose CT. This section describes the dataset and preprocessing strategies used, the evaluation metrics adopted, and the implementation details. We further present comprehensive quantitative and qualitative comparisons with state-of-the-art baselines, followed by ablation studies that assess the contribution of each architectural component and loss design. Our results demonstrate the superiority of ShapeField-Nodule in both segmentation accuracy and anatomical plausibility.</p><sec id=\"Sec11\"><title>Dataset and preprocessing</title><p id=\"Par23\">To validate the effectiveness and generalizability of <bold>ShapeField-Nodule</bold>, we conduct experiments on three publicly available CT datasets for lung nodule segmentation: (1) LIDC-IDRI<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, (2) LUNA16<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>, and (3) the Tianchi Lung Nodule dataset<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>. These datasets vary in imaging quality, labeling granularity, and annotation styles, allowing us to comprehensively evaluate segmentation accuracy, robustness, and domain transferability.</p><sec id=\"Sec12\"><title>LIDC-IDRI</title><p id=\"Par24\">This dataset contains 1,018 low-dose CT scans annotated by up to four expert radiologists. Only nodules larger than 3 mm are labeled with pixel-wise segmentations. To ensure reliable supervision, we select nodules annotated by at least three radiologists and compute a consensus mask using majority voting. 3D patches of size 64 &#215; 64 &#215; 64 centered at nodule centroids are extracted to serve as inputs. All scans are resampled to an isotropic spacing of 1.0 mm. Hounsfield Unit (HU) intensities are clipped to [&#8722; 1000, 400] and normalized to zero mean and unit variance. This dataset is used as our primary benchmark for both training and evaluation.</p></sec><sec id=\"Sec13\"><title>LUNA16</title><p id=\"Par25\">LUNA16 is a curated subset of LIDC-IDRI with 888 scans and higher-quality annotations. It includes only nodules with consistent agreement and excludes scans with severe artifacts. Since pixel-wise masks are not directly available, we synthesize segmentation labels by generating binary spherical masks based on the provided nodule center coordinates and diameters, following the protocol in ref. <sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. This allows us to evaluate our method under weak supervision and test shape recovery capabilities.</p></sec><sec id=\"Sec14\"><title>Tianchi lung nodule dataset</title><p id=\"Par26\">This dataset was released as part of the Alibaba Tianchi Medical AI Challenge and contains more than 1000 chest CT scans with nodule annotations in the form of bounding boxes. While segmentation masks are not officially provided, we generate pseudo-ground truth masks using a pre-trained ShapeField-Nodule model on LIDC-IDRI, similar to pseudo-labeling pipelines in semi-supervised learning. This dataset is used to test the domain transferability of our model in a weakly-labeled, distribution-shifted environment.</p></sec><sec id=\"Sec15\"><title>Preprocessing and augmentation</title><p id=\"Par27\">All scans are resampled to 1.0 mm isotropic resolution and cropped into patches of size 64 &#215; 64 &#215; 64. We apply the same preprocessing pipeline across all datasets for consistency. During training, we employ standard 3D data augmentation techniques including random rotations (&#177; 15<sup>&#8728;</sup>), flipping along axial/coronal/sagittal planes, intensity scaling, gamma adjustment, and elastic deformations using B-spline transformations.</p></sec><sec id=\"Sec16\"><title>Ground truth SDF generation</title><p id=\"Par28\">For all datasets where binary segmentation masks are available or synthesized, we compute ground truth signed distance fields (SDF) using the standard Euclidean transform. Voxels inside the mask are assigned negative distances, voxels outside positive distances, and zero-crossing represents the boundary surface. These SDF volumes serve as regression targets for our implicit shape decoder.</p></sec><sec id=\"Sec17\"><title>Cross-validation protocol</title><p id=\"Par29\">For all datasets, we adopt a 5-fold patient-level cross-validation strategy to evaluate performance and generalization. In each fold, 60% of nodules are used for training, 20% for validation, and 20% for testing, ensuring no patient overlap across splits.</p></sec></sec><sec id=\"Sec18\"><title>Evaluation Metrics</title><p id=\"Par30\">To comprehensively assess the performance of our proposed ShapeField-Nodule framework, we employ a combination of region-based and boundary-based segmentation metrics. These metrics are widely used in medical image segmentation benchmarks and are particularly suited for evaluating nodule-level accuracy and shape fidelity.</p><sec id=\"Sec19\"><title>Dice Similarity Coefficient (DSC)</title><p id=\"Par31\">The Dice score measures the volumetric overlap between the predicted segmentation <inline-formula id=\"IEq1\"><alternatives><tex-math id=\"d33e518\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{M}$$\\end{document}</tex-math><mml:math id=\"d33e521\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math></alternatives></inline-formula> and the ground truth mask <italic toggle=\"yes\">M</italic>. It is defined as:<disp-formula id=\"Equ1\"><label>1</label><alternatives><tex-math id=\"d33e532\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{Dice}}\\,=\\frac{2| {\\hat{M}}\\cap M|}{|\\hat{M}| +| M| }$$\\end{document}</tex-math><mml:math id=\"d33e536\"><mml:mrow><mml:mstyle><mml:mtext>Dice</mml:mtext></mml:mstyle><mml:mspace width=\"0.25em\"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#8739;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8745;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#8739;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8739;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8739;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>M</mml:mi><mml:mo>&#8739;</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par32\">This metric ranges from 0 (no overlap) to 1 (perfect match). Dice is sensitive to class imbalance, making it suitable for small structures such as pulmonary nodules.</p></sec><sec id=\"Sec20\"><title>Average Surface Distance (ASD)</title><p id=\"Par33\">ASD quantifies the mean distance between the surface voxels of the prediction and those of the ground truth:<disp-formula id=\"Equ2\"><label>2</label><alternatives><tex-math id=\"d33e573\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{ASD}}\\,(A,B)=\\frac{1}{|{S}_{A}| +| {S}_{B}| }\\left(\\mathop{\\sum}\\limits_{a\\in {S}_{A}}\\mathop{\\min }\\limits_{b\\in {S}_{B}}\\Vert a-b\\Vert +\\mathop{\\sum}\\limits_{b\\in {S}_{B}}\\mathop{\\min }\\limits_{a\\in {S}_{A}}\\Vert b-a\\Vert \\right)$$\\end{document}</tex-math><mml:math id=\"d33e577\"><mml:mrow><mml:mstyle><mml:mtext>ASD</mml:mtext></mml:mstyle><mml:mspace width=\"0.25em\"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8739;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo></mml:mrow></mml:mfrac><mml:mfenced close=\")\" open=\"(\"><mml:mrow><mml:munder><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>&#8741;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8741;</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>&#8741;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par34\">where <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">A</italic></sub> and <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">B</italic></sub> denote the sets of surface voxels in the predicted and ground truth masks, respectively. A lower ASD indicates better boundary alignment and smoother segmentation.</p></sec><sec id=\"Sec21\"><title>95th Percentile Hausdorff Distance (HD95)</title><p id=\"Par35\">To measure the worst-case surface error while being robust to outliers, we use the 95th percentile of the Hausdorff Distance:<disp-formula id=\"Equ3\"><label>3</label><alternatives><tex-math id=\"d33e686\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{HD}}_{95}(A,B)=\\max {\\left\\{\\mathop{\\sup }\\limits_{a\\in {S}_{A}}\\mathop{\\inf }\\limits_{b\\in {S}_{B}}\\parallel a-b\\parallel ,\\mathop{\\sup }\\limits_{b\\in {S}_{B}}\\mathop{\\inf }\\limits_{a\\in {S}_{A}}\\parallel b-a\\parallel \\right\\}}_{95\\text{th percentile}}$$\\end{document}</tex-math><mml:math id=\"d33e690\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">HD</mml:mi></mml:mrow><mml:mrow><mml:mn>95</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:msub><mml:mrow><mml:mfenced close=\"}\" open=\"{\"><mml:mrow><mml:munder><mml:mrow><mml:mi>sup</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>inf</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>&#8741;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8741;</mml:mo><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi>sup</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>inf</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>&#8741;</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mi mathvariant=\"normal\">th percentile</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par36\">This metric reflects how far the boundary of the prediction deviates from the ground truth in the worst 5% of the cases. It is especially important for detecting segmentation failures near critical structures.</p></sec><sec id=\"Sec22\"><title>Signed Distance Field Mean Error (SDF-ME)</title><p id=\"Par37\">Since our model predicts continuous signed distance fields, we additionally evaluate the mean absolute error between the predicted SDF <italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">&#952;</italic></sub>(<bold>x</bold>) and ground truth SDF <inline-formula id=\"IEq2\"><alternatives><tex-math id=\"d33e787\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{f}({\\bf{x}})$$\\end{document}</tex-math><mml:math id=\"d33e790\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> over all sampled coordinates:<disp-formula id=\"Equ4\"><label>4</label><alternatives><tex-math id=\"d33e805\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{SDF-ME}}\\,=\\frac{1}{N}\\mathop{\\sum}\\limits_{i=1}^{N}\\left\\vert {f}_{\\theta }({{\\bf{x}}}_{i})-\\hat{f}({{\\bf{x}}}_{i})\\right\\vert$$\\end{document}</tex-math><mml:math id=\"d33e809\"><mml:mrow><mml:mstyle><mml:mtext>SDF-ME</mml:mtext></mml:mstyle><mml:mspace width=\"0.25em\"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent=\"false\" accentunder=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=\"&#x2223;\" open=\"&#x2223;\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par38\">This metric captures the geometric accuracy of the learned implicit surface.</p></sec><sec id=\"Sec23\"><title>Interpretation</title><p id=\"Par39\">Dice measures overall region overlap, while ASD and HD95 provide geometric insights into boundary quality. SDF-ME directly evaluates our method&#8217;s core prediction: the implicit continuous surface. Together, these metrics offer a complete view of both classification accuracy and geometric plausibility.</p></sec></sec><sec id=\"Sec24\"><title>Implementation details</title><p id=\"Par40\">Our ShapeField-Nodule framework is implemented in PyTorch and built upon the MONAI medical imaging library for volumetric data handling. We describe the key implementation components below.</p><sec id=\"Sec25\"><title>Network backbone</title><p id=\"Par41\">We use a 3D U-Net architecture as our feature extractor, consisting of four downsampling and four upsampling blocks with skip connections. Each encoder block contains two convolutional layers with kernel size 3 &#215; 3 &#215; 3, instance normalization, and ReLU activations, followed by a 2 &#215; 2 &#215; 2 max pooling layer. The number of channels increases from 32 to 256 across the encoding path. The decoder mirrors the encoder with deconvolutions and feature concatenation. The final decoder output is a dense volumetric feature map <italic toggle=\"yes\">&#981;</italic>(<bold>x</bold>).</p></sec><sec id=\"Sec26\"><title>Implicit MLP head</title><p id=\"Par42\">The SDF decoder is a fully connected multi-layer perceptron (MLP) with four hidden layers of width 128 and ReLU activation. Skip connections are added from the input layer to the third hidden layer, similar to DeepSDF<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>. The MLP takes as input a concatenation of the interpolated volumetric feature vector at coordinate <bold>x</bold> and its positional encoding <italic toggle=\"yes\">&#947;</italic>(<bold>x</bold>), and outputs a single signed distance scalar <italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">&#952;</italic></sub>(<bold>x</bold>).</p></sec><sec id=\"Sec27\"><title>Positional encoding</title><p id=\"Par43\">To enable spatial generalization and capture high-frequency detail, we use a sinusoidal positional encoding scheme similar to NeRF. Each 3D coordinate <bold>x</bold> is encoded as:<disp-formula id=\"Equ5\"><label>5</label><alternatives><tex-math id=\"d33e917\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\gamma ({\\bf{x}})=\\left[\\sin ({2}^{0}\\pi {\\bf{x}}),\\cos ({2}^{0}\\pi {\\bf{x}}),\\ldots ,\\sin ({2}^{L-1}\\pi {\\bf{x}}),\\cos ({2}^{L-1}\\pi {\\bf{x}})\\right]$$\\end{document}</tex-math><mml:math id=\"d33e921\"><mml:mrow><mml:mi>&#947;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close=\"]\" open=\"[\"><mml:mrow><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mi>&#960;</mml:mi><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mi>&#960;</mml:mi><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>&#960;</mml:mi><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>&#960;</mml:mi><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par44\">We set <italic toggle=\"yes\">L</italic> = 6 in all experiments, resulting in a 36-dimensional positional embedding per spatial location.</p></sec><sec id=\"Sec28\"><title>Sampling strategy</title><p id=\"Par45\">During training, we uniformly sample <italic toggle=\"yes\">N</italic> = 8192 spatial coordinates per input patch. To prioritize learning near the surface, 70% of the points are sampled from a narrow band around the ground truth level set (i.e., within 5 voxels of the surface), and 30% are sampled randomly across the full patch. This hybrid sampling scheme ensures both fine boundary learning and global shape structure.</p></sec><sec id=\"Sec29\"><title>Optimization and training</title><p id=\"Par46\">We train all models using the Adam optimizer with initial learning rate 1 &#215; 10<sup>&#8722;4</sup>, <italic toggle=\"yes\">&#946;</italic><sub>1</sub> = 0.9, <italic toggle=\"yes\">&#946;</italic><sub>2</sub> = 0.999, and no weight decay. The learning rate is reduced by a factor of 0.5 every 50 epochs. We use a batch size of 2 and train for 300 epochs on each fold. The loss weight for the shape-aware refinement loss <inline-formula id=\"IEq3\"><alternatives><tex-math id=\"d33e1022\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document}</tex-math><mml:math id=\"d33e1025\"><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is set to <italic toggle=\"yes\">&#955;</italic> = 0.1.</p></sec><sec id=\"Sec30\"><title>Hardware</title><p id=\"Par47\">All experiments are conducted on an NVIDIA A100 GPU with 40 GB memory. Inference time per 3D patch is approximately 0.11 seconds, which is measured end-to-end, encompassing all steps from coordinate sampling and feature extraction to the final SDF-to-mask conversion via zero-level set thresholding. All models converge within 24 hours of training under our settings.</p></sec><sec id=\"Sec31\"><title>Reproducibility</title><p id=\"Par48\">Our codebase, including data preprocessing scripts, SDF generation code, and trained models, will be made publicly available upon acceptance to facilitate reproducibility and further research.</p></sec></sec><sec id=\"Sec32\"><title>Main Results</title><p id=\"Par49\">Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref> presents a comprehensive comparison between our proposed ShapeField-Nodule and a range of state-of-the-art segmentation baselines on the LIDC-IDRI dataset. We report three widely used metrics: Dice similarity coefficient (Dice), average surface distance (ASD), and the 95th percentile Hausdorff distance (HD95). Results are averaged across 5-fold cross-validation splits, and all models are trained and evaluated under identical preprocessing and patch extraction protocols.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Quantitative comparison with state-of-the-art baselines on the LIDC-IDRI dataset</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Method</th><th colspan=\"1\" rowspan=\"1\">Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">ASD (mm) <italic toggle=\"yes\">&#8595;</italic></th><th colspan=\"1\" rowspan=\"1\">HD95 (mm) <italic toggle=\"yes\">&#8595;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">3D U-Net<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup></td><td colspan=\"1\" rowspan=\"1\">82.4 &#177; 2.1</td><td colspan=\"1\" rowspan=\"1\">1.42 &#177; 0.15</td><td colspan=\"1\" rowspan=\"1\">4.21 &#177; 0.37</td></tr><tr><td colspan=\"1\" rowspan=\"1\">V-Net<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup></td><td colspan=\"1\" rowspan=\"1\">83.1 &#177; 2.0</td><td colspan=\"1\" rowspan=\"1\">1.37 &#177; 0.14</td><td colspan=\"1\" rowspan=\"1\">4.05 &#177; 0.41</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Attention U-Net<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup></td><td colspan=\"1\" rowspan=\"1\">83.7 &#177; 1.8</td><td colspan=\"1\" rowspan=\"1\">1.30 &#177; 0.12</td><td colspan=\"1\" rowspan=\"1\">3.85 &#177; 0.36</td></tr><tr><td colspan=\"1\" rowspan=\"1\">nnU-Net<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup></td><td colspan=\"1\" rowspan=\"1\">84.9 &#177; 1.7</td><td colspan=\"1\" rowspan=\"1\">1.28 &#177; 0.11</td><td colspan=\"1\" rowspan=\"1\">3.62 &#177; 0.32</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UNet++<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup></td><td colspan=\"1\" rowspan=\"1\">83.8 &#177; 1.9</td><td colspan=\"1\" rowspan=\"1\">1.34 &#177; 0.13</td><td colspan=\"1\" rowspan=\"1\">3.95 &#177; 0.33</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SegResNet<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup></td><td colspan=\"1\" rowspan=\"1\">84.3 &#177; 1.5</td><td colspan=\"1\" rowspan=\"1\">1.26 &#177; 0.11</td><td colspan=\"1\" rowspan=\"1\">3.71 &#177; 0.30</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Residual Attention U-Net<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup></td><td colspan=\"1\" rowspan=\"1\">85.2 &#177; 1.6</td><td colspan=\"1\" rowspan=\"1\">1.24 &#177; 0.10</td><td colspan=\"1\" rowspan=\"1\">3.60 &#177; 0.29</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Boundary-Aware U-Net<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup></td><td colspan=\"1\" rowspan=\"1\">85.4 &#177; 1.5</td><td colspan=\"1\" rowspan=\"1\">1.21 &#177; 0.09</td><td colspan=\"1\" rowspan=\"1\">3.55 &#177; 0.27</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SDF-Seg (implicit baseline)<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup></td><td colspan=\"1\" rowspan=\"1\">86.1 &#177; 1.4</td><td colspan=\"1\" rowspan=\"1\">1.16 &#177; 0.08</td><td colspan=\"1\" rowspan=\"1\">3.39 &#177; 0.25</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>ShapeField-Nodule (Ours)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold> &#177; <bold>1.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold> &#177; <bold>0.07</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold> &#177; <bold>0.23</bold></td></tr></tbody></table><table-wrap-foot><p>Metrics include Dice Similarity Coefficient (Dice<italic toggle=\"yes\">&#8593;</italic>), Average Surface Distance (ASD <italic toggle=\"yes\">&#8595;</italic>), and 95th percentile Hausdorff Distance (HD95 <italic toggle=\"yes\">&#8595;</italic>). Best results are highlighted in bold.</p></table-wrap-foot></table-wrap></p><p id=\"Par50\">Our method achieves the highest performance across all three metrics, with a Dice score of <bold>87.3%</bold>, an ASD of <bold>1.03 mm</bold>, and an HD95 of <bold>2.95 mm</bold>, outperforming all voxel-based baselines by a clear margin. This confirms that our implicit representation enables more precise and topologically coherent segmentation compared to traditional mask-based methods.</p><p id=\"Par51\">Among the voxel-based architectures, nnU-Net<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup> and Residual Attention U-Net<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup> perform strongly, achieving Dice scores above 84.9%. However, their boundary precision remains limited, as evidenced by ASD values exceeding 1.2 mm. While these methods benefit from sophisticated architectural design and dynamic configuration (as in nnU-Net), they inherently lack the ability to model smooth sub-voxel transitions and regularized shape structure.</p><p id=\"Par52\">UNet++<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup> and SegResNet<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup> offer improvements in multi-scale feature fusion and residual learning but still trail behind our method in both overlap-based and boundary-based metrics. Notably, our ShapeField-Nodule reduces HD95 by over 0.6 mm compared to the next best baseline, indicating superior handling of outlier segmentation errors and irregular boundaries.</p><p id=\"Par53\">SDF-Seg<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>, a recent method using implicit shape modeling in medical segmentation, achieves strong results and validates the advantage of SDF-based formulations. However, their method does not fully leverage coordinate-aware sampling or edge-gradient alignment, which we introduce. As a result, our model improves over SDF-Seg by more than 1.2% in Dice and over 0.4 mm in ASD, demonstrating the efficacy of our hybrid implicit-explicit design and refinement loss. To precisely delineate the sources of this improvement, we provide a direct comparison of the key methodological differences in Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref>. Unlike SDF-Seg, which relies on a more standard implicit learning setup, our framework introduces three critical enhancements. First, our coordinate-aware hybrid sampling strategy (70% near-surface) focuses the model&#8217;s capacity on the decisive boundary region, whereas uniform sampling treats all locations equally. Second, the proposed shape-aware refinement loss provides explicit supervision from image gradients, forcing the predicted zero-level set to align with anatomical edges&#8212;a mechanism absent in the baseline. Finally, the integration of high-frequency positional encoding allows our MLP decoder to capture fine geometric details that are often missed without it. These components collectively account for the observed performance gains, moving beyond a simple SDF regression to a truly shape-aware representation.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Comparison of key methodological differences between the SDF-Seg baseline and our proposed ShapeField-Nodule</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Feature</th><th colspan=\"1\" rowspan=\"1\">SDF-Seg [28]</th><th colspan=\"1\" rowspan=\"1\">ShapeField-Nodule (Ours)</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">Coordinate Sampling</td><td colspan=\"1\" rowspan=\"1\">Standard Uniform Sampling</td><td colspan=\"1\" rowspan=\"1\"><bold>Hybrid Near-Surface Oversampling</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Boundary Supervision</td><td colspan=\"1\" rowspan=\"1\">Standard SDF Regression Loss</td><td colspan=\"1\" rowspan=\"1\"><bold>SDF Loss + Edge-Gradient Alignment Loss</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Spatial Coordinate Input</td><td colspan=\"1\" rowspan=\"1\">Direct Coordinates</td><td colspan=\"1\" rowspan=\"1\"><bold>High-Frequency Positional Encoding</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Topological Regularization</td><td colspan=\"1\" rowspan=\"1\">Implicit via SDF</td><td colspan=\"1\" rowspan=\"1\"><bold>Implicit and Explicitly Refined</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par54\">Another key observation is the correlation between surface-based metrics and visual shape quality. Models with high Dice but poor ASD (e.g., V-Net) often yield segmentations with jagged or disconnected boundaries, while our approach maintains both volumetric accuracy and boundary continuity. This supports our core claim: that continuous implicit representations better preserve anatomical plausibility in noisy or ambiguous regions.</p><p id=\"Par55\">In summary, the consistent performance gains across metrics indicate that ShapeField-Nodule not only segments lung nodules more accurately, but also reconstructs their geometry in a smoother and more robust manner, even in the challenging context of low-dose CT. To verify that these performance gains are not the result of random variation, we conducted statistical significance testing. Specifically, we performed a two-tailed paired t-test on the Dice scores obtained from our 5-fold cross-validation. The improvements of ShapeField-Nodule over the strongest implicit baseline, SDF-Seg, were found to be statistically significant (<italic toggle=\"yes\">p</italic> &lt; 0.01). Furthermore, when compared to the best-performing voxel-based baseline, Boundary-Aware U-Net, the improvements were also significant (<italic toggle=\"yes\">p</italic> &lt; 0.05). This statistical evidence validates that our proposed framework yields a tangible and reliable improvement in segmentation accuracy.</p></sec><sec id=\"Sec33\"><title>Qualitative results</title><p id=\"Par56\">Figure <xref rid=\"Fig1\" ref-type=\"fig\">1</xref> presents qualitative comparisons of segmentation results from our method.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><title>Qualitative comparison of segmentation results on representative pulmonary nodules from the LIDC-IDRI dataset.</title><p>The results demonstrate consistent and accurate segmentation performance across diverse nodule morphologies, highlighting the model's capability to generate smooth and anatomically faithful contours regardless of shape variations.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1357\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig1_HTML.jpg\"/></fig></p><p id=\"Par57\">Our method generates smoother and anatomically plausible contours, better aligned with the true nodule boundary.</p><p id=\"Par58\">This advantage is particularly noticeable in difficult cases, such as juxtapleural nodules and irregularly shaped lesions. ShapeField-Nodule successfully preserves contour continuity and enforces geometric regularity through its implicit representation. Notably, our zero-level set predictions smoothly interpolate across noisy boundaries, avoiding unnatural holes or spurious protrusions often seen in voxel-wise segmentations.</p><p id=\"Par59\">These visual results validate the quantitative improvements reported earlier and highlight the interpretability and clinical reliability of our implicit modeling approach.</p><p id=\"Par60\">Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref> illustrates segmentation results from three particularly challenging cases selected from the LIDC-IDRI dataset. Each case reflects a different type of difficulty often encountered in clinical practice: boundary ambiguity, intensity inhomogeneity, or topological complexity.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><title>Segmentation results on challenging pulmonary nodule cases.</title><p>The columns from left to right correspond to the predictions generated by UNet++, nnU-Net, and our proposed ShapeField-Nodule (Ours), respectively. While our method generally demonstrates superior boundary smoothness and topological consistency, it is not immune to limitations. An example of a failure case is observed in the bottom image of the last column, where the predicted contour fails to accurately align with the nodule boundary.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1376\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig2_HTML.jpg\"/></fig></p><p id=\"Par61\">Overall, these qualitative examples reinforce our quantitative findings: ShapeField-Nodule yields anatomically plausible, topologically coherent, and noise-robust segmentations in clinically challenging conditions where voxel-based approaches struggle.</p><p id=\"Par62\">Figure <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> illustrates the predicted signed distance field (SDF) and reconstructed zero-level set contour from our ShapeField-Nodule model. This visualization highlights the central innovation of our approach: learning a continuous and differentiable shape representation from sparse voxel inputs.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><title>Visualization of the predicted Signed Distance Field (SDF) and zero-level set contour from ShapeField-Nodule.</title><p>From left to right: <bold>a</bold> Input LDCT slice with a pulmonary nodule; <bold>b</bold> Ground truth SDF computed from the binary mask using Euclidean distance transform; <bold>c</bold> Predicted SDF by our model, showing smooth transition from negative (inside) to positive (outside) regions; <bold>d</bold> Overlay of the predicted zero-level set contour on the original image. The continuous and differentiable nature of our implicit surface allows precise boundary modeling even under fuzzy or low-contrast conditions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1404\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig3_HTML.jpg\"/></fig></p><p id=\"Par63\">The ground truth SDF in the second panel is computed via a signed Euclidean transform of the binary nodule mask. It exhibits sharp transitions across the boundary, but still captures a smooth distance decay. Our predicted SDF (panel 3) closely mimics this geometry while remaining smooth and continuous across the domain, thanks to the MLP-based implicit decoder and sub-voxel coordinate encoding.</p><p id=\"Par64\">Most notably, the final panel shows the extracted zero-level set (i.e., <italic toggle=\"yes\">&#981;</italic>(<bold>x</bold>) = 0) overlaid on the original CT slice. This contour aligns precisely with the true boundary, even in areas with partial volume effects or intensity ambiguity. Unlike voxel-based segmentations, our implicit surface formulation avoids blocky artifacts and yields a boundary that is both anatomically faithful and visually interpretable.</p><p id=\"Par65\">Such a representation not only improves segmentation performance but also facilitates downstream tasks like surface-based analysis or 3D mesh extraction, offering a pathway to geometrically coherent clinical interpretation.</p></sec><sec id=\"Sec34\"><title>Ablation study</title><p id=\"Par66\">To understand the contribution of each component in ShapeField-Nodule, we conduct a detailed ablation study on the LIDC-IDRI dataset. Table <xref rid=\"Tab3\" ref-type=\"table\">3</xref> summarizes the quantitative performance changes when specific modules or design choices are removed or altered.<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Ablation study on ShapeField-Nodule components</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Model Variant</th><th colspan=\"1\" rowspan=\"1\">Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">ASD (mm) <italic toggle=\"yes\">&#8595;</italic></th><th colspan=\"1\" rowspan=\"1\">HD95 (mm) <italic toggle=\"yes\">&#8595;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\"><bold>Full Model (ShapeField-Nodule)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">w/o SDF loss</td><td colspan=\"1\" rowspan=\"1\">85.7</td><td colspan=\"1\" rowspan=\"1\">1.18</td><td colspan=\"1\" rowspan=\"1\">3.21</td></tr><tr><td colspan=\"1\" rowspan=\"1\">w/o shape-aware edge refinement loss</td><td colspan=\"1\" rowspan=\"1\">85.9</td><td colspan=\"1\" rowspan=\"1\">1.14</td><td colspan=\"1\" rowspan=\"1\">3.19</td></tr><tr><td colspan=\"1\" rowspan=\"1\">w/o positional encoding</td><td colspan=\"1\" rowspan=\"1\">84.5</td><td colspan=\"1\" rowspan=\"1\">1.29</td><td colspan=\"1\" rowspan=\"1\">3.42</td></tr><tr><td colspan=\"1\" rowspan=\"1\">replace MLP with 1 &#215; 1 conv head</td><td colspan=\"1\" rowspan=\"1\">84.1</td><td colspan=\"1\" rowspan=\"1\">1.33</td><td colspan=\"1\" rowspan=\"1\">3.60</td></tr><tr><td colspan=\"1\" rowspan=\"1\">replace SDF with binary mask prediction</td><td colspan=\"1\" rowspan=\"1\">83.4</td><td colspan=\"1\" rowspan=\"1\">1.45</td><td colspan=\"1\" rowspan=\"1\">3.75</td></tr></tbody></table><table-wrap-foot><p>We evaluate the contribution of each key module by disabling it from the full model. Performance is measured on the LIDC-IDRI dataset using 5-fold cross-validation. The full model achieves the best performance across all metrics. Removing the SDF, edge loss, or positional encoding results in substantial degradation.</p><p>The bold means the best result.</p></table-wrap-foot></table-wrap></p><sec id=\"Sec35\"><title>SDF vs. binary mask</title><p id=\"Par67\">Replacing our signed distance function (SDF) output with a binary voxel mask prediction leads to the most severe degradation across all metrics (&#8722;3.9% Dice, +0.42 mm HD95), confirming that the continuous implicit representation is central to the performance gain. It demonstrates the benefit of learning a differentiable shape field over discrete voxel classification.</p></sec><sec id=\"Sec36\"><title>Shape-aware refinement loss</title><p id=\"Par68\">Disabling the edge alignment loss <inline-formula id=\"IEq4\"><alternatives><tex-math id=\"d33e1527\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document}</tex-math><mml:math id=\"d33e1530\"><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> causes a drop of 1.4% in Dice and increases the average surface distance by over 0.1 mm. This suggests that explicitly enforcing consistency between SDF gradients and image boundaries significantly improves spatial precision.</p></sec><sec id=\"Sec37\"><title>SDF regression loss</title><p id=\"Par69\">Removing the regression supervision on the signed distance values and relying only on classification-level supervision causes noticeable boundary drift. The surface becomes poorly aligned with anatomy, especially near fuzzy boundaries.</p></sec><sec id=\"Sec38\"><title>Positional encoding</title><p id=\"Par70\">When removing the Fourier positional encoding, performance declines sharply. This confirms that encoding sub-voxel spatial coordinates is essential for the MLP to generalize local geometry effectively.</p></sec><sec id=\"Sec39\"><title>Implicit decoder head</title><p id=\"Par71\">Substituting our multi-layer perceptron (MLP) with a simple 1 &#215; 1 &#215; 1 convolution layer causes an additional performance loss, due to the lack of non-linear spatial modeling and coordinate awareness.</p><p id=\"Par72\">Overall, these results validate that each component of our architecture is carefully designed to contribute to geometric accuracy, anatomical smoothness, and robustness under LDCT noise. The full model configuration consistently achieves the best Dice, surface accuracy, and worst-case error.</p></sec><sec id=\"Sec40\"><title>Hyperparameter sensitivity analysis</title><p id=\"Par73\">To further assess the robustness and reproducibility of our framework, we conducted a sensitivity analysis on two key hyperparameters: the weight <italic toggle=\"yes\">&#955;</italic> of the shape-aware refinement loss (Eq. 4) and the proportion of points sampled near the boundary in our hybrid sampling strategy. The results, summarized in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, demonstrate that our chosen parameters provide an optimal balance for segmentation performance.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Hyperparameter sensitivity analysis on the LIDC-IDRI dataset</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Configuration</th><th colspan=\"1\" rowspan=\"1\">Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">ASD (mm) <italic toggle=\"yes\">&#8595;</italic></th><th colspan=\"1\" rowspan=\"1\">HD95 (mm) <italic toggle=\"yes\">&#8595;</italic></th></tr></thead><tbody><tr><td colspan=\"4\" rowspan=\"1\"><italic toggle=\"yes\">Varying Edge Refinement Weight</italic> (<italic toggle=\"yes\">&#955;</italic>)</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">&#955;</italic> = 0.01</td><td colspan=\"1\" rowspan=\"1\">86.7</td><td colspan=\"1\" rowspan=\"1\">1.09</td><td colspan=\"1\" rowspan=\"1\">3.08</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>&#955; = 0.1</bold><bold>(Ours)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">&#955;</italic> = 0.5</td><td colspan=\"1\" rowspan=\"1\">86.9</td><td colspan=\"1\" rowspan=\"1\">1.08</td><td colspan=\"1\" rowspan=\"1\">3.02</td></tr><tr><td colspan=\"4\" rowspan=\"1\"><italic toggle=\"yes\">Varying Near-Surface Sampling Proportion</italic></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Proportion = 50%</td><td colspan=\"1\" rowspan=\"1\">86.4</td><td colspan=\"1\" rowspan=\"1\">1.11</td><td colspan=\"1\" rowspan=\"1\">3.15</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>Proportion = 70% (Ours)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Proportion = 90%</td><td colspan=\"1\" rowspan=\"1\">86.8</td><td colspan=\"1\" rowspan=\"1\">1.06</td><td colspan=\"1\" rowspan=\"1\">3.01</td></tr></tbody></table><table-wrap-foot><p>We report performance while varying the edge refinement loss weight (<italic toggle=\"yes\">&#955;</italic>) and the proportion of near-surface samples. The parameters used in our final model are highlighted in <bold>bold.</bold></p></table-wrap-foot></table-wrap></p></sec><sec id=\"Sec41\"><title>Impact of Edge Refinement Weight (<italic toggle=\"yes\">&#955;</italic>)</title><p id=\"Par74\">We varied <italic toggle=\"yes\">&#955;</italic> from 0.01 to 0.5. A small weight (<italic toggle=\"yes\">&#955;</italic> = 0.01) diminishes the contribution of the edge alignment term, resulting in slightly less sharp boundaries and lower performance. Conversely, a large weight (<italic toggle=\"yes\">&#955;</italic> = 0.5) can lead to overfitting on noisy image gradients, causing the predicted surface to become irregular and slightly degrading overall accuracy. Our selected value of <italic toggle=\"yes\">&#955;</italic> = 0.1 achieves the best trade-off, effectively leveraging image edges without sacrificing geometric stability.</p></sec><sec id=\"Sec42\"><title>Impact of near-surface sampling proportion</title><p id=\"Par75\">We also evaluated the proportion of the 8192 sampled points that are drawn from the narrow band around the nodule surface. As shown in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, a lower proportion (e.g., 50%) does not provide sufficient focus on the critical boundary region, leading to poorer surface metrics. A very high proportion (e.g., 90%), on the other hand, neglects the global context of the SDF, which can also impair performance. The 70% proportion used in our main experiments provides the best results across all metrics, confirming it as an effective choice.</p><p id=\"Par76\">Beyond architectural modules, we also ablate two important design elements in our training pipeline: point sampling strategy and loss formulation.</p></sec><sec id=\"Sec43\"><title>Sampling strategy</title><p id=\"Par77\">As shown in Table <xref rid=\"Tab5\" ref-type=\"table\">5</xref>, training the implicit field using only uniform random sampling across the 3D patch results in suboptimal accuracy (Dice = 85.2%). While this method covers the full context, it lacks sufficient focus on the critical decision boundary near the nodule surface. Conversely, sampling only near the boundary band (within 5-voxel distance) improves boundary metrics but leads to poor global shape regularity.<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Impact of sampling strategies and loss design</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Configuration</th><th colspan=\"1\" rowspan=\"1\">Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">ASD (mm) <italic toggle=\"yes\">&#8595;</italic></th><th colspan=\"1\" rowspan=\"1\">HD95 (mm) <italic toggle=\"yes\">&#8595;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">Uniform sampling only</td><td colspan=\"1\" rowspan=\"1\">85.2</td><td colspan=\"1\" rowspan=\"1\">1.24</td><td colspan=\"1\" rowspan=\"1\">3.33</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Boundary band sampling only</td><td colspan=\"1\" rowspan=\"1\">86.4</td><td colspan=\"1\" rowspan=\"1\">1.12</td><td colspan=\"1\" rowspan=\"1\">3.10</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>Hybrid sampling (Ours)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">No edge refinement loss</td><td colspan=\"1\" rowspan=\"1\">85.9</td><td colspan=\"1\" rowspan=\"1\">1.14</td><td colspan=\"1\" rowspan=\"1\">3.19</td></tr><tr><td colspan=\"1\" rowspan=\"1\">With edge refinement loss (Ours)</td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>1.03</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr></tbody></table><table-wrap-foot><p>We evaluate different combinations of point sampling methods and auxiliary loss terms on SDF prediction. Our proposed hybrid sampling and edge-aligned refinement loss together yield the best accuracy and boundary precision.</p><p>The bold means the best result.</p></table-wrap-foot></table-wrap></p><p id=\"Par78\">Our proposed hybrid sampling strategy&#8212;drawing 70% of training points near the boundary and 30% randomly&#8212;achieves the best overall balance. It encourages accurate zero-level set learning while preserving coarse-to-fine spatial structure. This is especially beneficial in noisy regions or under CT imaging artifacts.</p></sec><sec id=\"Sec44\"><title>Edge-aware refinement loss</title><p id=\"Par79\">We further analyze the contribution of the auxiliary gradient-alignment loss <inline-formula id=\"IEq5\"><alternatives><tex-math id=\"d33e1829\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document}</tex-math><mml:math id=\"d33e1832\"><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>, which encourages consistency between the SDF gradient &#8711; <italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">&#952;</italic></sub>(<bold>x</bold>) and the image-derived boundary cues. When this term is omitted, performance drops across all metrics, with a notable 0.11 mm increase in ASD. Including this term improves surface adherence and discourages disconnected or over-smoothed shapes.</p><p id=\"Par80\">Together, these results demonstrate that both sampling and regularization are critical for stable and anatomically coherent SDF learning. The implicit formulation, while powerful, benefits significantly from carefully chosen supervision signals and training dynamics.</p><p id=\"Par81\">Fig. <xref rid=\"Fig4\" ref-type=\"fig\">4</xref> presents a qualitative comparison of different ablated versions of ShapeField-Nodule on a representative pulmonary nodule.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><title>Qualitative ablation study on key components of ShapeField-Nodule.</title><p>Shown are segmentation results overlaid on the same axial CT slice. From left to right: full model (ours), model without edge refinement loss, without SDF regression loss, and a voxel-based binary mask baseline. The full model produces the smoothest and most anatomically faithful boundary, while ablating the SDF or edge loss leads to noisy or misaligned contours. The binary mask baseline results in staircase-like artifacts and loss of geometric continuity.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1865\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig4_HTML.jpg\"/></fig></p><p id=\"Par82\">The full model (leftmost) accurately captures the smooth and compact boundary of the nodule, aligning well with anatomical structures. When the edge refinement loss <inline-formula id=\"IEq6\"><alternatives><tex-math id=\"d33e1868\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document}</tex-math><mml:math id=\"d33e1871\"><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is removed, the boundary becomes slightly irregular, particularly along weak edges where image gradients are ambiguous. Removing the SDF regression loss causes further degradation&#8212;noticeable in the form of shape drift and contour bulging&#8212;highlighting the importance of direct geometric supervision.</p><p id=\"Par83\">The rightmost column shows the result using a binary voxel-based segmentation head, which produces a visibly staircase-like, blocky boundary due to its discrete nature. Compared to this, the zero-level set contour of our implicit field is significantly smoother and more precise.</p><p id=\"Par84\">These visual comparisons reaffirm the quantitative findings in Tables <xref rid=\"Tab3\" ref-type=\"table\">3</xref> and <xref rid=\"Tab5\" ref-type=\"table\">5</xref>, demonstrating that each component contributes to more stable, anatomically faithful, and clinically interpretable segmentation results.</p></sec></sec><sec id=\"Sec45\"><title>Robustness and generalization</title><p id=\"Par85\">In real-world deployment, segmentation models must remain stable under various forms of image perturbations such as noise, motion artifacts, and contrast variations&#8212;especially in low-dose CT (LDCT) scenarios. To assess the robustness of ShapeField-Nodule, we synthetically perturb the LIDC-IDRI test images with three common degradations and evaluate model performance without retraining.</p><p id=\"Par86\">As shown in Table <xref rid=\"Tab6\" ref-type=\"table\">6</xref>, our method consistently outperforms voxel-based approaches under all tested perturbations. While minor performance degradation is observed (e.g., Dice drops from 87.3% to 85.6% with Gaussian noise), the boundary quality remains high, and HD95 increases only marginally. This stability is primarily due to the implicit surface representation, which acts as a spatial prior and smooths out noise-sensitive voxel inconsistencies.<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Robustness to image perturbations</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Perturbation Type</th><th colspan=\"1\" rowspan=\"1\">Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">HD95 (mm) <italic toggle=\"yes\">&#8595;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">No perturbation (clean)</td><td colspan=\"1\" rowspan=\"1\"><bold>87.3</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>2.95</bold></td></tr><tr><td colspan=\"1\" rowspan=\"1\">+ Gaussian noise (<italic toggle=\"yes\">&#963;</italic> = 0.05)</td><td colspan=\"1\" rowspan=\"1\">85.6</td><td colspan=\"1\" rowspan=\"1\">3.24</td></tr><tr><td colspan=\"1\" rowspan=\"1\">+ Motion blur (3 &#215; 3 kernel)</td><td colspan=\"1\" rowspan=\"1\">84.9</td><td colspan=\"1\" rowspan=\"1\">3.37</td></tr><tr><td colspan=\"1\" rowspan=\"1\">+ Contrast shift (-20%)</td><td colspan=\"1\" rowspan=\"1\">85.3</td><td colspan=\"1\" rowspan=\"1\">3.30</td></tr></tbody></table><table-wrap-foot><p>We evaluate segmentation accuracy under common low-dose CT perturbations. Dice and HD95 are reported on the LIDC-IDRI test set with each perturbation applied at test time only. Our method maintains strong performance despite noise, blur, or contrast shifts, demonstrating enhanced resilience.</p><p>The bold means the best result.</p></table-wrap-foot></table-wrap></p><p id=\"Par87\">Moreover, the SDF formulation allows the model to interpolate over missing or weak edge evidence, making it less reliant on high-frequency textures or sharp gradients. Even under motion blur and contrast shifts, ShapeField-Nodule retains sub-voxel contour accuracy and shape plausibility.</p><p id=\"Par88\">These results highlight that continuous shape modeling offers not only improved accuracy under ideal conditions but also superior generalization to imperfect imaging environments, which is critical for clinical applicability.</p><sec id=\"Sec46\"><title>Analysis of generalization sources</title><p id=\"Par89\">To better quantify the source of the improved generalization observed in our cross-dataset evaluation, we conducted an additional experiment as suggested by the reviewer. The goal is to disentangle the contribution of our proposed training strategies (i.e., hybrid sampling and the edge refinement loss) from the contribution of our network architecture (i.e., the specific MLP head with positional encoding). To achieve this, we created a new baseline, which we term <bold>SDF-Seg+</bold>, by training the original SDF-Seg architecture using our exact training pipeline.</p><p id=\"Par90\">The results of this analysis are presented in Table <xref rid=\"Tab7\" ref-type=\"table\">7</xref>. When comparing the standard SDF-Seg with SDF-Seg+, we observe a substantial performance increase of approximately 1.2&#8211;1.3% in Dice score on both external datasets. This clearly indicates that our training methodology, particularly the near-surface sampling and edge-aware loss, is a major contributor to robust generalization. However, our full ShapeField-Nodule model still consistently outperforms the SDF-Seg+ baseline. This remaining performance gap confirms the efficacy of our architectural design. We conclude that the superior generalization of ShapeField-Nodule is a result of both factors: the advanced training strategy provides a strong foundation for robustness, while the carefully designed implicit decoder architecture further refines the model&#8217;s ability to learn a truly generalizable shape representation.<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Analysis of generalization sources</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Method</th><th colspan=\"1\" rowspan=\"1\">LUNA16 Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">Tianchi Dice (%)<italic toggle=\"yes\">&#8593;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">SDF-Seg [36] (Standard Training)</td><td colspan=\"1\" rowspan=\"1\">84.0</td><td colspan=\"1\" rowspan=\"1\">83.2</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SDF-Seg+ (Our Training Strategy)</td><td colspan=\"1\" rowspan=\"1\">85.3</td><td colspan=\"1\" rowspan=\"1\">84.5</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>ShapeField-Nodule (Ours, Full Model)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>85.8</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>85.2</bold></td></tr></tbody></table><table-wrap-foot><p>We compare our full model to the SDF-Seg baseline and an enhanced baseline (SDF-Seg+) trained with our proposed methods. All models are trained on LIDC-IDRI and tested on unseen datasets.</p><p>The bold means the best result.</p></table-wrap-foot></table-wrap></p><p id=\"Par91\">To further assess generalization beyond synthetic perturbations, we evaluate the performance of ShapeField-Nodule on two external datasets&#8212;LUNA16 and Tianchi&#8212;without any domain adaptation or fine-tuning. These datasets differ significantly from LIDC-IDRI in terms of patient demographics, CT acquisition protocols, and nodule annotation standards.</p><p id=\"Par92\">As shown in Table <xref rid=\"Tab8\" ref-type=\"table\">8</xref>, our method consistently achieves the highest Dice scores across both target domains, outperforming U-Net, nnU-Net, and the SDF-Seg baseline. Specifically, ShapeField-Nodule obtains 85.8% Dice on LUNA16 and 85.2% on Tianchi, with a margin of up to 2.6% over the best voxel-based alternative.<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Cross-dataset generalization performance</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Method</th><th colspan=\"1\" rowspan=\"1\">LUNA16 Dice (%)<italic toggle=\"yes\">&#8593;</italic></th><th colspan=\"1\" rowspan=\"1\">Tianchi Dice (%)<italic toggle=\"yes\">&#8593;</italic></th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">U-Net (baseline)</td><td colspan=\"1\" rowspan=\"1\">80.6</td><td colspan=\"1\" rowspan=\"1\">78.9</td></tr><tr><td colspan=\"1\" rowspan=\"1\">nnU-Net<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup></td><td colspan=\"1\" rowspan=\"1\">82.4</td><td colspan=\"1\" rowspan=\"1\">81.1</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SDF-Seg<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup></td><td colspan=\"1\" rowspan=\"1\">84.0</td><td colspan=\"1\" rowspan=\"1\">83.2</td></tr><tr><td colspan=\"1\" rowspan=\"1\"><bold>ShapeField-Nodule (Ours)</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>85.8</bold></td><td colspan=\"1\" rowspan=\"1\"><bold>85.2</bold></td></tr></tbody></table><table-wrap-foot><p>We train the model on LIDC-IDRI and test directly on two unseen datasets: LUNA16 and Tianchi, without any fine-tuning. Our ShapeField-Nodule exhibits superior generalization ability compared to voxel-based baselines, showing consistent segmentation quality across diverse patient cohorts and scanning protocols.</p><p>The bold means the best result.</p></table-wrap-foot></table-wrap></p><p id=\"Par93\">This improvement stems from our implicit geometric modeling, which encodes shape priors independent of voxel-grid alignment. By learning signed distance fields and zero-level set boundaries, the model becomes less sensitive to voxel size, contrast scale, and scanner-specific variations. This cross-dataset stability indicates strong potential for real-world clinical deployment where domain shift is unavoidable.</p><p id=\"Par94\">Together with the perturbation results in Table <xref rid=\"Tab6\" ref-type=\"table\">6</xref>, these findings confirm that ShapeField-Nodule not only excels under clean settings but also adapts robustly across domains and noise conditions.</p></sec></sec><sec id=\"Sec47\"><title>Inference efficiency</title><p id=\"Par95\">Figure <xref rid=\"Fig5\" ref-type=\"fig\">5</xref> illustrates the trade-offs between segmentation accuracy, inference latency, and model size across different baseline methods.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><title>Inference efficiency of ShapeField-Nodule.</title><p>Left: Dice score vs. inference time per volume (seconds). Right: Dice score vs. model size (in millions of parameters). Our method consistently achieves the highest segmentation accuracy while maintaining lower latency and a compact architecture compared to voxel-based baselines.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2130\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig5_HTML.jpg\"/></fig></p><p id=\"Par96\">In the left panel, we compare Dice score against inference time per volume on the LIDC-IDRI test set. Despite operating on continuous representations, ShapeField-Nodule achieves the best Dice score with only marginal latency overhead compared to voxel-based models. It outperforms SDF-Seg and nnU-Net both in accuracy and speed, indicating that our lightweight MLP-based implicit head does not introduce a significant computational burden.</p><p id=\"Par97\">The right panel shows Dice score versus model size. While U-Net and nnU-Net require over 20-30 million parameters to reach reasonable accuracy, ShapeField-Nodule matches or surpasses their performance with fewer than 10 million parameters. This suggests that our method encodes geometric priors more effectively, allowing compact models to generalize well without overfitting.</p><p id=\"Par98\">Together, these plots highlight that ShapeField-Nodule provides a highly favorable balance between accuracy, speed, and model size&#8212;crucial for real-time and embedded deployment in clinical settings.</p></sec></sec><sec id=\"Sec48\" sec-type=\"discussion\"><title>Discussion</title><p id=\"Par99\">We presented <bold>ShapeField-Nodule</bold>, a continuous shape embedding framework for pulmonary nodule segmentation in low-dose CT. Unlike conventional voxel-based approaches, our method models nodule boundaries via signed distance functions (SDFs), allowing sub-voxel accuracy, topological smoothness, and enhanced robustness under low signal-to-noise imaging conditions.</p><p id=\"Par100\">Built upon a 3D U-Net backbone with an implicit MLP decoder, ShapeField-Nodule enables boundary-aware learning and regularized shape representation. We further introduced a shape-aware refinement loss to align the predicted SDF gradient with edge cues, resulting in more anatomically faithful contours. Extensive experiments across LIDC-IDRI, LUNA16, and Tianchi datasets demonstrate that our method achieves state-of-the-art performance while maintaining high efficiency and generalization.</p><p id=\"Par101\">This work highlights the advantages of continuous shape modeling in medical image segmentation, particularly in scenarios with sparse, noisy, or ambiguous visual evidence. In future work, we plan to extend this implicit representation to multi-object segmentation, integrate uncertainty modeling into the SDF field, and explore its use for 3D mesh reconstruction and radiomics-driven downstream analysis.</p><p id=\"Par102\">Overall, ShapeField-Nodule offers a principled and interpretable alternative to voxel masks, with strong potential for deployment in real-world clinical pipelines for early lung cancer screening. Beyond improving segmentation accuracy, the continuous shape representation offered by ShapeField-Nodule holds significant promise for direct clinical applications. In computer-aided diagnosis (CAD) pipelines, our method can provide highly precise and anatomically plausible inputs for downstream tasks, such as malignancy classification or volumetric growth tracking, potentially reducing errors caused by noisy, voxelated boundaries. The differentiable and smooth nature of the predicted SDF is particularly advantageous for radiomics, as it allows for the stable and robust extraction of geometry-based features (e.g., surface area, sphericity, margin sharpness) that are often unreliable when computed from discrete masks. Furthermore, the ability to generate high-quality 3D meshes directly from the zero-level set can aid in pre-operative surgical planning, offering clinicians a more intuitive visualization of the nodule&#8217;s morphology and its spatial relationship with adjacent structures like vessels and airways Fig. <xref rid=\"Fig6\" ref-type=\"fig\">6</xref>.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><title>Examples of challenging cases and limitations of ShapeField-Nodule.</title><p>(Left) An extremely small nodule where partial volume effects lead to an over-smoothed and slightly inaccurate segmentation. (Right) A case with severe motion artifacts, where the corrupted image gradients result in a misaligned boundary prediction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2160\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig6_HTML.jpg\"/></fig></p></sec><sec id=\"Sec49\"><title>Methods</title><p id=\"Par103\">In this section, we present <bold>ShapeField-Nodule</bold>, a continuous shape embedding framework for pulmonary nodule segmentation in low-dose CT (LDCT). Our method aims to overcome the limitations of discrete voxel-based segmentation by modeling the nodule boundary as an implicit signed distance function (SDF). The key idea is to regress a continuous volumetric field whose zero level set represents the anatomical boundary of the nodule. This allows for sub-voxel precision, smoother shape continuity, and better topological regularity. Fig. <xref rid=\"Fig7\" ref-type=\"fig\">7</xref> illustrates the full pipeline.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><title>Overview of the ShapeField-Nodule framework.</title><p>The model takes a 3D LDCT volume as input and extracts hierarchical features using a 3D U-Net. A coordinate-conditioned MLP predicts signed distance values at sampled spatial locations using both features and positional encoding. The final segmentation is obtained by thresholding the zero level-set of the predicted signed distance field (SDF). A shape-aware loss aligns the gradient of the predicted SDF with the image edge field.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2178\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig7_HTML.jpg\"/></fig></p><sec id=\"Sec50\"><title>Overview and motivation</title><p id=\"Par104\">Conventional segmentation networks output binary masks by classifying each voxel independently. While effective in many cases, such predictions are discrete and lack the capacity to capture smooth geometric transitions or enforce structural regularity. These issues are amplified in LDCT, where image noise and low contrast impair voxel-wise decisions.</p><p id=\"Par105\">To address this, we propose a continuous formulation where the segmentation target is a <italic toggle=\"yes\">signed distance field</italic><inline-formula id=\"IEq7\"><alternatives><tex-math id=\"d33e2187\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${f}_{\\theta }:{{\\mathbb{R}}}^{3}\\to {\\mathbb{R}}$$\\end{document}</tex-math><mml:math id=\"d33e2190\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>&#8594;</mml:mo><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> parameterized by a neural network. This field encodes the shortest distance of any 3D point to the underlying nodule surface, with negative values inside the object, positive values outside, and zero on the boundary. The continuous nature of SDFs allows us to reconstruct smooth iso-surfaces and sidestep voxelization artifacts.</p></sec><sec id=\"Sec51\"><title>Network architecture</title><p id=\"Par106\">The overall architecture of ShapeField-Nodule is illustrated in Fig. <xref rid=\"Fig8\" ref-type=\"fig\">8</xref>. It consists of two main components:<list list-type=\"bullet\"><list-item><p id=\"Par107\">A <bold>3D U-Net backbone</bold> that extracts multi-scale volumetric features from the input LDCT scan <inline-formula id=\"IEq8\"><alternatives><tex-math id=\"d33e2223\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\mathcal{X}}\\in {{\\mathbb{R}}}^{D\\times H\\times W}$$\\end{document}</tex-math><mml:math id=\"d33e2226\"><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p></list-item><list-item><p id=\"Par108\">A <bold>coordinate-conditioned MLP head</bold> that predicts the SDF value at any 3D location <inline-formula id=\"IEq9\"><alternatives><tex-math id=\"d33e2249\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\bf{x}}\\in {{\\mathbb{R}}}^{3}$$\\end{document}</tex-math><mml:math id=\"d33e2252\"><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> using both the location and the contextual features from the U-Net encoder.</p></list-item></list><fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><title>Detailed illustration of the internal modules of ShapeField-Nodule.</title><p>The network consists of a multi-scale 3D U-Net, coordinate sampling and positional encoding modules, and an MLP head for SDF prediction. A gradient alignment branch compares predicted SDF gradients with image-derived edge vectors to refine boundary localization.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2272\" position=\"float\" orientation=\"portrait\" xlink:href=\"41746_2025_2041_Fig8_HTML.jpg\"/></fig></p><p id=\"Par109\">Specifically, given a sampled coordinate <bold>x</bold> in the input volume, we bilinearly interpolate the feature vector <italic toggle=\"yes\">&#981;</italic>(<bold>x</bold>) from the U-Net&#8217;s feature map, and concatenate it with a positional encoding <italic toggle=\"yes\">&#947;</italic>(<bold>x</bold>) to form the input to the MLP:<disp-formula id=\"Equ6\"><label>6</label><alternatives><tex-math id=\"d33e2291\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${f}_{\\theta }({\\bf{x}})=\\,\\text{MLP}\\,([\\phi ({\\bf{x}});\\gamma ({\\bf{x}})])$$\\end{document}</tex-math><mml:math id=\"d33e2295\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width=\"0.25em\"/><mml:mi mathvariant=\"normal\">MLP</mml:mi><mml:mspace width=\"0.25em\"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>&#981;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>&#947;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par110\">Here, <italic toggle=\"yes\">&#947;</italic>( &#8901; ) denotes a sinusoidal positional encoding similar to that used in NeRF<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>, and <italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">&#952;</italic></sub>( &#8901; ) is the predicted signed distance value at <bold>x</bold>.</p></sec><sec id=\"Sec52\"><title>Continuous shape embedding via SDF</title><p id=\"Par111\">To learn the implicit surface, we construct supervision signals in the form of ground-truth signed distances. Given a binary segmentation mask <italic toggle=\"yes\">M</italic>(<bold>x</bold>) &#8712; {0, 1}, we compute its Euclidean signed distance transform <inline-formula id=\"IEq10\"><alternatives><tex-math id=\"d33e2365\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{f}({\\bf{x}})$$\\end{document}</tex-math><mml:math id=\"d33e2368\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where negative values indicate foreground (inside the nodule) and positive values indicate background.</p><p id=\"Par112\">We then minimize an <italic toggle=\"yes\">L</italic><sub>1</sub> regression loss over a set of sampled spatial coordinates {<bold>x</bold><sub><italic toggle=\"yes\">i</italic></sub>}:<disp-formula id=\"Equ7\"><label>7</label><alternatives><tex-math id=\"d33e2395\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{SDF}}}=\\frac{1}{N}\\mathop{\\sum }\\limits_{i=1}^{N}\\left\\vert {f}_{\\theta }({{\\bf{x}}}_{i})-\\hat{f}({{\\bf{x}}}_{i})\\right\\vert$$\\end{document}</tex-math><mml:math id=\"d33e2399\"><mml:mrow><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">SDF</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent=\"false\" accentunder=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=\"&#x2223;\" open=\"&#x2223;\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par113\">The field is evaluated only at sampled voxels, avoiding full-volume regression and keeping computation efficient. In practice, we oversample near-boundary voxels (within a narrow band around the level set) to emphasize learning accurate shape transitions.</p></sec><sec id=\"Sec53\"><title>Shape-aware refinement loss</title><p id=\"Par114\">While the SDF regression ensures geometric fidelity, it does not directly encourage alignment with actual image intensity edges. To improve edge adherence, we introduce a <bold>shape-aware refinement loss</bold> based on the gradient field of the SDF.</p><p id=\"Par115\">We first compute the predicted gradient &#8711; <italic toggle=\"yes\">f</italic><sub><italic toggle=\"yes\">&#952;</italic></sub>(<bold>x</bold>) using auto-differentiation. Then, we enforce alignment between this gradient and the image-derived edge vector <bold>g</bold>(<bold>x</bold>), which can be computed via Sobel or Laplacian filters:<disp-formula id=\"Equ8\"><label>8</label><alternatives><tex-math id=\"d33e2483\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}=\\frac{1}{N}\\mathop{\\sum }\\limits_{i=1}^{N}\\left(1-\\cos \\angle (\\nabla {f}_{\\theta }({{\\bf{x}}}_{i}),{\\bf{g}}({{\\bf{x}}}_{i}))\\right)$$\\end{document}</tex-math><mml:math id=\"d33e2487\"><mml:mrow><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent=\"false\" accentunder=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=\")\" open=\"(\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>cos</mml:mi><mml:mi>&#8736;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#8711;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par116\">This loss penalizes angular mismatch between the SDF&#8217;s directional gradient and actual image edges, encouraging the zero level-set to lie on strong boundaries.</p></sec><sec id=\"Sec54\"><title>Training objective and optimization</title><p id=\"Par117\">The total training loss combines the SDF regression and the edge-guided refinement term:<disp-formula id=\"Equ9\"><label>9</label><alternatives><tex-math id=\"d33e2557\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{\\mathcal{L}}}_{{\\rm{total}}}={{\\mathcal{L}}}_{{\\rm{SDF}}}+\\lambda \\cdot {{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document}</tex-math><mml:math id=\"d33e2561\"><mml:mrow><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">total</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">SDF</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">edge</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par118\">where <italic toggle=\"yes\">&#955;</italic> is a scalar weight (typically set to 0.1) balancing shape alignment and distance accuracy.</p><p id=\"Par119\">During inference, the final segmentation mask is obtained by thresholding the predicted SDF field at zero level:<disp-formula id=\"Equ10\"><label>10</label><alternatives><tex-math id=\"d33e2596\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{M}({\\bf{x}})={\\mathbb{I}}[{f}_{\\theta }({\\bf{x}})\\le 0]$$\\end{document}</tex-math><mml:math id=\"d33e2600\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant=\"double-struck\">I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant=\"bold\">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8804;</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p><p id=\"Par120\">This produces a binary mask with sub-voxel surface continuity.</p></sec><sec id=\"Sec55\"><title>Implementation details</title><p id=\"Par121\">We implement our model in PyTorch using MONAI for medical imaging operations. The 3D U-Net backbone consists of four downsampling blocks with 32-256 channels. The MLP head contains 4 fully connected layers with ReLU activations and skip connections. We use a learning rate of 1 &#215; 10<sup>&#8722;4</sup> and Adam optimizer. Training is conducted for 300 epochs on LIDC-IDRI with a batch size of 2. Spatial coordinates are sampled dynamically per batch with higher density near boundaries.</p><sec id=\"Sec56\"><title>Ethics approval and consent to participate</title><p id=\"Par122\">This study utilized only publicly available and anonymized datasets (LIDC-IDRI, LUNA16, Tianchi), which do not require additional ethical approval or informed consent under current guidelines.</p></sec></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Xuyu Gu, Yifei Zhu.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This project did not receive any financial support.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>X.G. and Y.Z. had full access to all the data and take responsibility for the integrity of the data and the accuracy of the analysis. CL contributed to the conception and design of the study. XG and YZ contributed to data acquisition, statistical analysis, and interpretation. K.J., X.X. and L.X. contributed to the drafting of the manuscript. All authors were responsible for the critical revision of the manuscript for important content.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The datasets used in this study are publicly accessible: - LIDC-IDRI: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI-\">https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI-</ext-link> LUNA16: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://luna16.grand-challenge.org/-\">https://luna16.grand-challenge.org/-</ext-link> Tianchi Lung Nodule Dataset: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://tianchi.aliyun.com/competition/entrance/231601/information\">https://tianchi.aliyun.com/competition/entrance/231601/information</ext-link>.</p></notes><notes notes-type=\"data-availability\"><title>Code availability</title><p>The source code for ShapeField-Nodule and all experiments will be released publicly upon accept :<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://anonymous.4open.science/r/ShapeField-B6CD/Readme.md\">https://anonymous.4open.science/r/ShapeField-B6CD/Readme.md</ext-link>.</p></notes><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par123\">The authors declare no competing interests.</p></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Armato</surname><given-names>SG</given-names></name><etal/></person-group><article-title>The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on CT scans</article-title><source>Med. Phys.</source><year>2011</year><volume>38</volume><fpage>915</fpage><lpage>931</lpage><pub-id pub-id-type=\"doi\">10.1118/1.3528204</pub-id><pub-id pub-id-type=\"pmid\">21452728</pub-id><pub-id pub-id-type=\"pmcid\">PMC3041807</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Armato, S. G. et al. The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on CT scans. <italic toggle=\"yes\">Med. Phys.</italic><bold>38</bold>, 915&#8211;931 (2011).<pub-id pub-id-type=\"pmid\">21452728</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1118/1.3528204</pub-id><pub-id pub-id-type=\"pmcid\">PMC3041807</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cherezov</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Saha</surname><given-names>PK</given-names></name></person-group><article-title>Treatment decision strategies and outcomes in early-stage lung cancer</article-title><source>Clin. Lung Cancer</source><year>2016</year><volume>17</volume><fpage>e1</fpage><lpage>e10</lpage></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Cherezov, D., Lee, B. &amp; Saha, P. K. Treatment decision strategies and outcomes in early-stage lung cancer. <italic toggle=\"yes\">Clin. Lung Cancer</italic><bold>17</bold>, e1&#8211;e10 (2016).26837474\n</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Setio</surname><given-names>AAA</given-names></name><etal/></person-group><article-title>Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The luna16 challenge</article-title><source>Med. image Anal.</source><year>2017</year><volume>42</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2017.06.015</pub-id><pub-id pub-id-type=\"pmid\">28732268</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Setio, A. A. A. et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The luna16 challenge. <italic toggle=\"yes\">Med. image Anal.</italic><bold>42</bold>, 1&#8211;13 (2017).<pub-id pub-id-type=\"pmid\">28732268</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2017.06.015</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A review of automatic segmentation algorithms for lung nodules from computed tomography images</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>37306</fpage><lpage>37330</lpage></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Zhou, Y. et al. A review of automatic segmentation algorithms for lung nodules from computed tomography images. <italic toggle=\"yes\">IEEE Access</italic><bold>9</bold>, 37306&#8211;37330 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liao</surname><given-names>F</given-names></name><etal/></person-group><article-title>Evaluate pulmonary nodules with radiomics and deep learning</article-title><source>Transl. Lung Cancer Res.</source><year>2019</year><volume>8</volume><fpage>288</fpage></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Liao, F. et al. Evaluate pulmonary nodules with radiomics and deep learning. <italic toggle=\"yes\">Transl. Lung Cancer Res.</italic><bold>8</bold>, 288 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Ding, J.-Y. et al. Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks. In <italic toggle=\"yes\">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic>, 559&#8211;567 (Springer, 2017).</mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic toggle=\"yes\">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic>, 234&#8211;241 (Springer, 2015).</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><mixed-citation publication-type=\"other\">Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In <italic toggle=\"yes\">3D Vision (3DV)</italic>, 565&#8211;571 (IEEE, 2016).</mixed-citation></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isensee</surname><given-names>F</given-names></name><etal/></person-group><article-title>nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nat. Methods</source><year>2021</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type=\"doi\">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type=\"pmid\">33288961</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Isensee, F. et al. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. <italic toggle=\"yes\">Nat. Methods</italic><bold>18</bold>, 203&#8211;211 (2021).<pub-id pub-id-type=\"pmid\">33288961</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41592-020-01008-z</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>X-S</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y-Z</given-names></name><name name-style=\"western\"><surname>Hospedales</surname><given-names>TM</given-names></name></person-group><article-title>A survey on deep learning-based fine-grained image classification</article-title><source>Pattern Recognit.</source><year>2021</year><volume>110</volume><fpage>107613</fpage></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Zhang, X.-S., Xie, X., Yang, Y.-Z. &amp; Hospedales, T. M. A survey on deep learning-based fine-grained image classification. <italic toggle=\"yes\">Pattern Recognit.</italic><bold>110</bold>, 107613 (2021).32868956\n</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><mixed-citation publication-type=\"other\">Park, J. J., Florence, P., Straub, J., Newcombe, R. &amp; Lovegrove, S. Deepsdf: Learning continuous signed distance functions for shape representation. In <italic toggle=\"yes\">CVPR</italic>, 165&#8211;174 (2019).</mixed-citation></ref><ref id=\"CR12\"><label>12.</label><mixed-citation publication-type=\"other\">Gropp, A., Yariv, L., Haim, N., Atzmon, M. &amp; Lipman, Y. Implicit geometric regularization for learning shapes. In <italic toggle=\"yes\">International Conference on Machine Learning (ICML)</italic>, 3789&#8211;3799 (2020).</mixed-citation></ref><ref id=\"CR13\"><label>13.</label><mixed-citation publication-type=\"other\">Chen, Z. &amp; Zhang, H. Learning implicit fields for generative shape modeling. In <italic toggle=\"yes\">CVPR</italic>, 5939&#8211;5948 (2019).</mixed-citation></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Atzmon, M. &amp; Lipman, Y. Sal: Sign agnostic learning of shapes from raw data. In <italic toggle=\"yes\">CVPR</italic>, 2565&#8211;2574 (2020).</mixed-citation></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Xiao, X. et al. Visual instance-aware prompt tuning (2025). <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://arxiv.org/abs/2507.07796\">https://arxiv.org/abs/2507.07796</ext-link>. 2507.07796.</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><mixed-citation publication-type=\"other\">Lin, K. et al. Learning signed distance fields for 3d human shape reconstruction. In <italic toggle=\"yes\">CVPR</italic>, 12316&#8211;12325 (2021).</mixed-citation></ref><ref id=\"CR17\"><label>17.</label><mixed-citation publication-type=\"other\">Ma, K., Zhang, Y. &amp; Wang, Y. Sdfreg: Signed distance function-based registration of medical images. In <italic toggle=\"yes\">MICCAI</italic>, 204&#8211;214 (2022).</mixed-citation></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Davies</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Lim</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Glocker</surname><given-names>B</given-names></name></person-group><article-title>Organ shape completion using implicit surface representations</article-title><source>Med. Image Anal.</source><year>2021</year><volume>71</volume><fpage>102037</fpage></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Davies, R., Lim, Y. &amp; Glocker, B. Organ shape completion using implicit surface representations. <italic toggle=\"yes\">Med. Image Anal.</italic><bold>71</bold>, 102037 (2021).33910110\n</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><mixed-citation publication-type=\"other\">Xiao, X. et al. Describe anything in medical images (2025). <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://arxiv.org/abs/2505.05804\">https://arxiv.org/abs/2505.05804</ext-link>. 2505.05804.</mixed-citation></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">Wang, W. et al. Multi-dimensional transformer with attention-based filtering for medical image segmentation. In <italic toggle=\"yes\">2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)</italic>, 632&#8211;639 (2024).</mixed-citation></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Xiao, X. et al. Hgtdp-dta: Hybrid graph-transformer with dynamic prompt for drug-target binding affinity prediction. In <italic toggle=\"yes\">Neural Information Processing</italic> (eds Mahmud, M. et al.) 340&#8211;354 (Springer Nature Singapore, Singapore, 2025).</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">Oktay, O. et al. Attention U-Net: Learning where to look for the pancreas. In <italic toggle=\"yes\">MICCAI</italic> (2018).</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Zhou, Z. et al. Unet++: Redesigning skip connections to exploit multiscale features in image segmentation. In <italic toggle=\"yes\">MICCAI</italic> (2018).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2019.2959609</pub-id><pub-id pub-id-type=\"pmcid\">PMC7357299</pub-id><pub-id pub-id-type=\"pmid\">31841402</pub-id></mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Zhang, H. et al. Residual attention network for pulmonary nodule segmentation from low-dose ct images. <italic toggle=\"yes\">IEEE Trans. Med. Imaging</italic> (2021).</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Li, X. et al. Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation. <italic toggle=\"yes\">Med. Image Anal.</italic> (2020).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2020.101766</pub-id><pub-id pub-id-type=\"pmid\">32623276</pub-id></mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Xiao, X. et al. Hgtdp-dta: Hybrid graph-transformer with dynamic prompt for drug-target binding affinity prediction. In <italic toggle=\"yes\">International Conference on Neural Information Processing</italic>, 340&#8211;354 (Springer, 2024).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Xu, Q. et al. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In <italic toggle=\"yes\">Advances in Neural Information Processing Systems (NeurIPS)</italic>, vol. 32 (2019).</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Yariv, L. et al. Volume rendering of neural implicit surfaces. In <italic toggle=\"yes\">Advances in Neural Information Processing Systems (NeurIPS)</italic>, vol. 34, 4805&#8211;4815 (2021).</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Lu, Y. et al. Volumetric shape regularization for surface-aware 3d medical image segmentation. <italic toggle=\"yes\">MICCAI</italic> (2021).</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Huang, Z. et al. Shape-aware implicit neural representation for medical image segmentation. <italic toggle=\"yes\">Med. Image Anal.</italic> (2022).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Hu, X. et al. Topology-preserving deep image segmentation via Euler characteristic loss. <italic toggle=\"yes\">Adv. Neural Inf. Process. Syst. (NeurIPS)</italic> (2021).</mixed-citation></ref><ref id=\"CR32\"><label>32.</label><mixed-citation publication-type=\"other\">Bai, W. et al. Semi-supervised learning for network-based cardiac MR image segmentation. <italic toggle=\"yes\">Med. Image Anal.</italic> (2017).</mixed-citation></ref><ref id=\"CR33\"><label>33.</label><mixed-citation publication-type=\"other\">Karimi, D. &amp; Salcudean, S. E. Surface dice-similarity coefficient: An improved metric for segmentation evaluation. <italic toggle=\"yes\">MICCAI</italic> (2020).</mixed-citation></ref><ref id=\"CR34\"><label>34.</label><mixed-citation publication-type=\"other\">Cloud, A. Tianchi medical AI challenge: Lung nodule detection and classification. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://tianchi.aliyun.com/competition/entrance/231601/information\">https://tianchi.aliyun.com/competition/entrance/231601/information</ext-link> (2017).</mixed-citation></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Myronenko, A. 3D MRI brain tumor segmentation using autoencoder regularization. In <italic toggle=\"yes\">MICCAI BRATS</italic> (2018).</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mildenhall</surname><given-names>B</given-names></name><etal/></person-group><article-title>Nerf: representing scenes as neural radiance fields for view synthesis</article-title><source>Commun. ACM</source><year>2021</year><volume>65</volume><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type=\"doi\">10.1145/3503250</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Mildenhall, B. et al. Nerf: representing scenes as neural radiance fields for view synthesis. <italic toggle=\"yes\">Commun. ACM</italic><bold>65</bold>, 99&#8211;106 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">&#199;i&#231;ek, O., Abdulkadir, A., Lienkamp, S., Brox, T. &amp; Ronneberger, O. 3d U-Net: Learning dense volumetric segmentation from sparse annotation (2016).</mixed-citation></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Kervadec, H. et al. Boundary loss for highly unbalanced segmentation. In <italic toggle=\"yes\">MICCAI</italic> (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2020.101851</pub-id><pub-id pub-id-type=\"pmid\">33080507</pub-id></mixed-citation></ref><ref id=\"CR39\"><label>39.</label><mixed-citation publication-type=\"other\">Lin, T., Chen, Z., Yan, Z., Yu, W. &amp; Zheng, F. Stable diffusion segmentation for biomedical images with single-step reverse process. In <italic toggle=\"yes\">Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2024</italic>, 656&#8211;666 (Springer Nature Switzerland, Cham, 2024).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc NPJ Digit Med NPJ Digit Med 3605 npjdigitmed NPJ Digital Medicine 2398-6352 Nature Publishing Group PMC12673127 PMC12673127.1 12673127 12673127 41310368 10.1038/s41746-025-02041-y 2041 1 Article ShapeField-lung: continuous shape embedding for early lung cancer detection via pulmonary nodule segmentation Gu Xuyu 1 Zhu Yifei 2 3 Li Chuangqi 4 Xu Xinnan xxn@tongji.edu.cn 5 Jin Kaiqi kaiqijin@tongji.edu.cn 5 Xu Li xl_shp@tongji.edu.cn 5 1 https://ror.org/033nbnf69 grid.412532.3 Department of Oncology, Shanghai Pulmonary Hospital, Shanghai, China 2 https://ror.org/00my25942 grid.452404.3 0000 0004 1808 0942 Cancer Institute, Fudan University Shanghai Cancer Center, Shanghai, China 3 https://ror.org/01zntxs11 grid.11841.3d 0000 0004 0619 8943 Department of Oncology, Shanghai Medical College of Fudan University, Shanghai, China 4 https://ror.org/03cve4549 grid.12527.33 0000 0001 0662 3178 School of Vehicle and Mobility, Tsinghua University, Beijing, China 5 https://ror.org/033nbnf69 grid.412532.3 Department of Thoracic Surgery, Shanghai Pulmonary Hospital, Shanghai, China 27 11 2025 2025 8 478273 736 4 8 2025 25 9 2025 27 11 2025 04 12 2025 04 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Accurate segmentation of pulmonary nodules in low-dose CT (LDCT) is vital for early lung cancer detection. Existing voxel-based methods often fail to capture irregular nodule boundaries, especially under noisy, low-contrast conditions. We propose ShapeField-Nodule, a continuous shape embedding framework that models nodule geometry as a signed distance field (SDF), enabling sub-voxel precision and anatomically coherent contours. Our method integrates a lightweight MLP-based implicit head with a 3D U-Net backbone to predict dense SDF values, and introduces a shape-aware refinement loss that aligns SDF gradients with image edges. Unlike discrete masks, our representation enforces boundary smoothness, topology regularization, and robustness to perturbations. Evaluations on LIDC-IDRI, LUNA16, and Tianchi datasets show state-of-the-art Dice and surface metrics. Extensive experiments demonstrate superior generalization, robustness under noise, and inference efficiency, highlighting the potential of continuous implicit fields as a principled alternative for medical image segmentation. Subject terms Cancer Computational biology and bioinformatics Mathematics and computing Medical research pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Lung cancer remains the leading cause of cancer-related mortality worldwide, and early detection is paramount for improving patient survival rates 1 , 2 . Low-dose computed tomography (LDCT) has emerged as a clinically viable modality for large-scale screening due to its reduced radiation burden and its capacity to detect small pulmonary nodules 3 , 4 . However, interpreting LDCT scans is challenging even for radiologists, particularly when nodules present with subtle, irregular morphologies under noisy and low-contrast conditions. Accurate delineation of pulmonary nodules is essential not only for diagnostic purposes, but also for downstream malignancy risk estimation, longitudinal tracking, and surgical planning 5 , 6 . Automated segmentation of pulmonary nodules has therefore been a long-standing goal in medical image computing. Existing methods predominantly formulate this task as a voxel-wise binary classification problem, mapping each voxel to either foreground (nodule) or background labels using convolutional neural networks (CNNs). Architectures such as 3D U-Net 7 &#8211; 9 and attention-based extensions have achieved significant progress, yet they often struggle to model the fine geometric structures of nodules. Specifically, discrete voxel masks suffer from limited resolution, boundary discontinuity, and lack of shape regularity, particularly in the presence of partial volume effects and acquisition artifacts 10 . Moreover, such discrete representations make it difficult to enforce anatomical plausibility, as they do not explicitly encode continuity or topological constraints. For pulmonary nodules-&#8220;which can vary greatly in size, attachment to vessels or pleura, and internal texture-&#8221; this becomes a major bottleneck. These limitations have motivated the search for alternative formulations that model shape in a more continuous and structured fashion. Recent advances in implicit neural representations Such as Signed Distance Functions (SDFs), have demonstrated remarkable capabilities in capturing detailed, continuous geometry in various vision tasks 11 &#8211; 15 . Instead of predicting binary occupancy, these methods learn continuous scalar fields that represent the distance from each spatial location to an object&#8217;s surface, offering sub-voxel precision and natural boundary regularization. While initially developed for 3D shape modeling, SDFs have since been applied to human pose estimation 16 , medical image registration 17 , and shape completion 18 . However, their integration into volumetric medical image segmentation pipelines remains limited. In this paper, we propose ShapeField-Nodule A novel implicit shape embedding framework for pulmonary nodule segmentation in LDCT. Our method models the nodule boundary as a continuous signed distance field, regressed by a lightweight MLP head attached to a 3D U-Net backbone. By predicting SDF values across the input volume, our method enables smooth shape reconstruction with sub-voxel accuracy, inherently encoding shape continuity and topological regularity. To ensure the SDF captures anatomically meaningful boundaries, we introduce a shape-aware refinement loss that aligns the predicted SDF gradients with edge evidence extracted from the image. This loss encourages the zero-level set (i.e., the predicted boundary) to coincide with strong intensity gradients, thereby improving edge adherence and mitigating over-smoothing. Unlike previous works that rely on post-hoc surface extraction, our approach is end-to-end trainable and requires no external surface fitting. We evaluate ShapeField-Nodule on the publicly available LIDC-IDRI dataset 1 , which contains a diverse range of pulmonary nodules annotated by multiple radiologists. Our method achieves superior performance compared to state-of-the-art voxel-based segmentation models, both in overlap-based metrics (Dice score) and boundary-based metrics (average surface distance, Hausdorff distance). Qualitative results further demonstrate improved boundary smoothness, topological fidelity, and robustness under challenging conditions. To summarize, our contributions are: We introduce ShapeField-Nodule , the first continuous SDF-based segmentation framework tailored for pulmonary nodule analysis in LDCT, enabling sub-voxel geometric modeling with topological consistency. We design a hybrid implicit-explicit architecture that integrates an MLP SDF decoder with a 3D U-Net backbone, combining dense feature extraction with continuous shape representation. We propose a novel shape-aware refinement loss that aligns SDF gradients with image edge cues, significantly enhancing boundary sharpness and anatomical plausibility. We conduct extensive quantitative and qualitative evaluations on the LIDC-IDRI benchmark, demonstrating that ShapeField-Nodule outperforms conventional voxel-based baselines in both accuracy and shape coherence. Related work Pulmonary nodule segmentation in LDCT Pulmonary nodule segmentation in low-dose CT (LDCT) remains a critical challenge in medical image analysis due to the subtle boundaries and morphological diversity of nodules. Early works relied on handcrafted features and classical image processing 3 , while deep learning-based methods have now become dominant. 3D convolutional neural networks, especially U-Net variants, are widely adopted due to their ability to model volumetric context 7 &#8211; 9 , 19 , 19 &#8211; 21 . Several studies focused on enhancing representation power via attention mechanisms 22 , multi-scale processing 23 , or residual connections 24 , yet most of them still rely on discrete voxel-wise segmentation. While accurate on coarse metrics like Dice, these methods often produce irregular and noisy boundaries, especially for juxtapleural or spiculated nodules. More recent approaches introduce uncertainty modeling 25 and radiomics-informed networks 5 , yet struggle to produce geometrically coherent outputs. More recently, the landscape of medical image segmentation has been further advanced by the introduction of Vision Transformers (ViTs) and their variants. Architectures such as Swin-Unet and UNETR have demonstrated remarkable success by combining the hierarchical feature extraction capabilities of CNNs with the long-range dependency modeling of transformers. These hybrid models have set new state-of-the-art benchmarks in various segmentation tasks by effectively capturing both local texture details and global contextual information. While these methods excel at voxel-level classification accuracy, they still produce discrete outputs and may not inherently enforce the geometric smoothness and topological consistency that our continuous SDF-based approach is designed to address. Our work is positioned as an orthogonal improvement, focusing on the nature of the shape representation itself rather than solely on the feature extraction backbone. Implicit shape representations in vision Implicit neural representations have gained significant attention in computer vision and graphics for modeling 3D geometry with sub-pixel precision. These approaches typically represent a shape as a continuous field-\"e.g., signed distance functions (SDFs) or occupancy fields-&#8221;predicted via coordinate-based MLPs 11 &#8211; 13 , 26 . This continuous formulation enables smooth surface reconstruction and differentiable geometry extraction via level-set operations. DeepSDF 11 introduced the use of neural networks to regress SDF values from 3D coordinates, enabling accurate modeling of complex object surfaces. Follow-up works enhanced this idea by incorporating shape priors 14 , semantic conditioning 27 , or camera supervision 28 . Although implicit fields have been extensively explored for shape reconstruction, pose estimation, and neural rendering, their integration into semantic segmentation tasks-\"especially in medical domains-&#8221; remains limited. Signed distance fields in medical imaging The utility of SDFs in medical imaging has only recently gained traction. One line of work uses SDFs as intermediate representations to guide surface extraction or as self-supervision for registration tasks. For example, SDFReg 17 leverages SDFs for unsupervised deformable registration, while 18 applies implicit fields for organ completion. Others integrate SDF constraints into network training to regularize boundary smoothness 29 . However, few methods predict SDFs directly from input volumes in an end-to-end learnable way. Notably, 30 propose an SDF-supervised segmentation for cardiac MRI, but rely on post-processing steps for final boundary reconstruction. Our method departs from this by directly regressing a volumetric SDF field and using its level-set as a continuous and anatomically meaningful segmentation output. Topology-aware and shape-consistent segmentation Traditional segmentation networks often neglect topological properties such as continuity, smoothness, or anatomical validity. This limitation has motivated shape-aware segmentation frameworks that impose priors or regularizers during training. For example, 31 introduces differentiable topology constraints, while 32 uses conditional random fields (CRFs) to enforce boundary adherence. In 3D medical imaging, several efforts integrate surface-based losses (e.g., surface Dice 33 ) or Laplacian constraints to improve boundary alignment. Yet, these approaches remain fundamentally discrete. Implicit field-based formulations offer a more principled alternative by modeling boundaries continuously. Our work builds on this by proposing a shape refinement loss that aligns SDF gradients with image-derived edge cues, effectively guiding the learning process towards shape-consistent predictions. Results We conduct extensive experiments to evaluate the performance of our proposed ShapeField-Nodule framework on the task of pulmonary nodule segmentation in low-dose CT. This section describes the dataset and preprocessing strategies used, the evaluation metrics adopted, and the implementation details. We further present comprehensive quantitative and qualitative comparisons with state-of-the-art baselines, followed by ablation studies that assess the contribution of each architectural component and loss design. Our results demonstrate the superiority of ShapeField-Nodule in both segmentation accuracy and anatomical plausibility. Dataset and preprocessing To validate the effectiveness and generalizability of ShapeField-Nodule , we conduct experiments on three publicly available CT datasets for lung nodule segmentation: (1) LIDC-IDRI 1 , (2) LUNA16 3 , and (3) the Tianchi Lung Nodule dataset 34 . These datasets vary in imaging quality, labeling granularity, and annotation styles, allowing us to comprehensively evaluate segmentation accuracy, robustness, and domain transferability. LIDC-IDRI This dataset contains 1,018 low-dose CT scans annotated by up to four expert radiologists. Only nodules larger than 3 mm are labeled with pixel-wise segmentations. To ensure reliable supervision, we select nodules annotated by at least three radiologists and compute a consensus mask using majority voting. 3D patches of size 64 &#215; 64 &#215; 64 centered at nodule centroids are extracted to serve as inputs. All scans are resampled to an isotropic spacing of 1.0 mm. Hounsfield Unit (HU) intensities are clipped to [&#8722; 1000, 400] and normalized to zero mean and unit variance. This dataset is used as our primary benchmark for both training and evaluation. LUNA16 LUNA16 is a curated subset of LIDC-IDRI with 888 scans and higher-quality annotations. It includes only nodules with consistent agreement and excludes scans with severe artifacts. Since pixel-wise masks are not directly available, we synthesize segmentation labels by generating binary spherical masks based on the provided nodule center coordinates and diameters, following the protocol in ref. 4 . This allows us to evaluate our method under weak supervision and test shape recovery capabilities. Tianchi lung nodule dataset This dataset was released as part of the Alibaba Tianchi Medical AI Challenge and contains more than 1000 chest CT scans with nodule annotations in the form of bounding boxes. While segmentation masks are not officially provided, we generate pseudo-ground truth masks using a pre-trained ShapeField-Nodule model on LIDC-IDRI, similar to pseudo-labeling pipelines in semi-supervised learning. This dataset is used to test the domain transferability of our model in a weakly-labeled, distribution-shifted environment. Preprocessing and augmentation All scans are resampled to 1.0 mm isotropic resolution and cropped into patches of size 64 &#215; 64 &#215; 64. We apply the same preprocessing pipeline across all datasets for consistency. During training, we employ standard 3D data augmentation techniques including random rotations (&#177; 15 &#8728; ), flipping along axial/coronal/sagittal planes, intensity scaling, gamma adjustment, and elastic deformations using B-spline transformations. Ground truth SDF generation For all datasets where binary segmentation masks are available or synthesized, we compute ground truth signed distance fields (SDF) using the standard Euclidean transform. Voxels inside the mask are assigned negative distances, voxels outside positive distances, and zero-crossing represents the boundary surface. These SDF volumes serve as regression targets for our implicit shape decoder. Cross-validation protocol For all datasets, we adopt a 5-fold patient-level cross-validation strategy to evaluate performance and generalization. In each fold, 60% of nodules are used for training, 20% for validation, and 20% for testing, ensuring no patient overlap across splits. Evaluation Metrics To comprehensively assess the performance of our proposed ShapeField-Nodule framework, we employ a combination of region-based and boundary-based segmentation metrics. These metrics are widely used in medical image segmentation benchmarks and are particularly suited for evaluating nodule-level accuracy and shape fidelity. Dice Similarity Coefficient (DSC) The Dice score measures the volumetric overlap between the predicted segmentation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{M}$$\\end{document} M ^ and the ground truth mask M . It is defined as: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{Dice}}\\,=\\frac{2| {\\hat{M}}\\cap M|}{|\\hat{M}| +| M| }$$\\end{document} Dice = 2 &#8739; M ^ &#8745; M &#8739; &#8739; M ^ &#8739; + &#8739; M &#8739; This metric ranges from 0 (no overlap) to 1 (perfect match). Dice is sensitive to class imbalance, making it suitable for small structures such as pulmonary nodules. Average Surface Distance (ASD) ASD quantifies the mean distance between the surface voxels of the prediction and those of the ground truth: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{ASD}}\\,(A,B)=\\frac{1}{|{S}_{A}| +| {S}_{B}| }\\left(\\mathop{\\sum}\\limits_{a\\in {S}_{A}}\\mathop{\\min }\\limits_{b\\in {S}_{B}}\\Vert a-b\\Vert +\\mathop{\\sum}\\limits_{b\\in {S}_{B}}\\mathop{\\min }\\limits_{a\\in {S}_{A}}\\Vert b-a\\Vert \\right)$$\\end{document} ASD ( A , B ) = 1 &#8739; S A &#8739; + &#8739; S B &#8739; &#8721; a &#8712; S A min b &#8712; S B &#8741; a &#8722; b &#8741; + &#8721; b &#8712; S B min a &#8712; S A &#8741; b &#8722; a &#8741; where S A and S B denote the sets of surface voxels in the predicted and ground truth masks, respectively. A lower ASD indicates better boundary alignment and smoother segmentation. 95th Percentile Hausdorff Distance (HD95) To measure the worst-case surface error while being robust to outliers, we use the 95th percentile of the Hausdorff Distance: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{HD}}_{95}(A,B)=\\max {\\left\\{\\mathop{\\sup }\\limits_{a\\in {S}_{A}}\\mathop{\\inf }\\limits_{b\\in {S}_{B}}\\parallel a-b\\parallel ,\\mathop{\\sup }\\limits_{b\\in {S}_{B}}\\mathop{\\inf }\\limits_{a\\in {S}_{A}}\\parallel b-a\\parallel \\right\\}}_{95\\text{th percentile}}$$\\end{document} HD 95 ( A , B ) = max sup a &#8712; S A inf b &#8712; S B &#8741; a &#8722; b &#8741; , sup b &#8712; S B inf a &#8712; S A &#8741; b &#8722; a &#8741; 95 th percentile This metric reflects how far the boundary of the prediction deviates from the ground truth in the worst 5% of the cases. It is especially important for detecting segmentation failures near critical structures. Signed Distance Field Mean Error (SDF-ME) Since our model predicts continuous signed distance fields, we additionally evaluate the mean absolute error between the predicted SDF f &#952; ( x ) and ground truth SDF \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{f}({\\bf{x}})$$\\end{document} f ^ ( x ) over all sampled coordinates: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{SDF-ME}}\\,=\\frac{1}{N}\\mathop{\\sum}\\limits_{i=1}^{N}\\left\\vert {f}_{\\theta }({{\\bf{x}}}_{i})-\\hat{f}({{\\bf{x}}}_{i})\\right\\vert$$\\end{document} SDF-ME = 1 N &#8721; i = 1 N f &#952; ( x i ) &#8722; f ^ ( x i ) This metric captures the geometric accuracy of the learned implicit surface. Interpretation Dice measures overall region overlap, while ASD and HD95 provide geometric insights into boundary quality. SDF-ME directly evaluates our method&#8217;s core prediction: the implicit continuous surface. Together, these metrics offer a complete view of both classification accuracy and geometric plausibility. Implementation details Our ShapeField-Nodule framework is implemented in PyTorch and built upon the MONAI medical imaging library for volumetric data handling. We describe the key implementation components below. Network backbone We use a 3D U-Net architecture as our feature extractor, consisting of four downsampling and four upsampling blocks with skip connections. Each encoder block contains two convolutional layers with kernel size 3 &#215; 3 &#215; 3, instance normalization, and ReLU activations, followed by a 2 &#215; 2 &#215; 2 max pooling layer. The number of channels increases from 32 to 256 across the encoding path. The decoder mirrors the encoder with deconvolutions and feature concatenation. The final decoder output is a dense volumetric feature map &#981; ( x ). Implicit MLP head The SDF decoder is a fully connected multi-layer perceptron (MLP) with four hidden layers of width 128 and ReLU activation. Skip connections are added from the input layer to the third hidden layer, similar to DeepSDF 11 . The MLP takes as input a concatenation of the interpolated volumetric feature vector at coordinate x and its positional encoding &#947; ( x ), and outputs a single signed distance scalar f &#952; ( x ). Positional encoding To enable spatial generalization and capture high-frequency detail, we use a sinusoidal positional encoding scheme similar to NeRF. Each 3D coordinate x is encoded as: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\gamma ({\\bf{x}})=\\left[\\sin ({2}^{0}\\pi {\\bf{x}}),\\cos ({2}^{0}\\pi {\\bf{x}}),\\ldots ,\\sin ({2}^{L-1}\\pi {\\bf{x}}),\\cos ({2}^{L-1}\\pi {\\bf{x}})\\right]$$\\end{document} &#947; ( x ) = sin ( 2 0 &#960; x ) , cos ( 2 0 &#960; x ) , &#8230; , sin ( 2 L &#8722; 1 &#960; x ) , cos ( 2 L &#8722; 1 &#960; x ) We set L = 6 in all experiments, resulting in a 36-dimensional positional embedding per spatial location. Sampling strategy During training, we uniformly sample N = 8192 spatial coordinates per input patch. To prioritize learning near the surface, 70% of the points are sampled from a narrow band around the ground truth level set (i.e., within 5 voxels of the surface), and 30% are sampled randomly across the full patch. This hybrid sampling scheme ensures both fine boundary learning and global shape structure. Optimization and training We train all models using the Adam optimizer with initial learning rate 1 &#215; 10 &#8722;4 , &#946; 1 = 0.9, &#946; 2 = 0.999, and no weight decay. The learning rate is reduced by a factor of 0.5 every 50 epochs. We use a batch size of 2 and train for 300 epochs on each fold. The loss weight for the shape-aware refinement loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document} L edge is set to &#955; = 0.1. Hardware All experiments are conducted on an NVIDIA A100 GPU with 40 GB memory. Inference time per 3D patch is approximately 0.11 seconds, which is measured end-to-end, encompassing all steps from coordinate sampling and feature extraction to the final SDF-to-mask conversion via zero-level set thresholding. All models converge within 24 hours of training under our settings. Reproducibility Our codebase, including data preprocessing scripts, SDF generation code, and trained models, will be made publicly available upon acceptance to facilitate reproducibility and further research. Main Results Table 1 presents a comprehensive comparison between our proposed ShapeField-Nodule and a range of state-of-the-art segmentation baselines on the LIDC-IDRI dataset. We report three widely used metrics: Dice similarity coefficient (Dice), average surface distance (ASD), and the 95th percentile Hausdorff distance (HD95). Results are averaged across 5-fold cross-validation splits, and all models are trained and evaluated under identical preprocessing and patch extraction protocols. Table 1 Quantitative comparison with state-of-the-art baselines on the LIDC-IDRI dataset Method Dice (%) &#8593; ASD (mm) &#8595; HD95 (mm) &#8595; 3D U-Net 37 82.4 &#177; 2.1 1.42 &#177; 0.15 4.21 &#177; 0.37 V-Net 8 83.1 &#177; 2.0 1.37 &#177; 0.14 4.05 &#177; 0.41 Attention U-Net 22 83.7 &#177; 1.8 1.30 &#177; 0.12 3.85 &#177; 0.36 nnU-Net 9 84.9 &#177; 1.7 1.28 &#177; 0.11 3.62 &#177; 0.32 UNet++ 23 83.8 &#177; 1.9 1.34 &#177; 0.13 3.95 &#177; 0.33 SegResNet 35 84.3 &#177; 1.5 1.26 &#177; 0.11 3.71 &#177; 0.30 Residual Attention U-Net 24 85.2 &#177; 1.6 1.24 &#177; 0.10 3.60 &#177; 0.29 Boundary-Aware U-Net 38 85.4 &#177; 1.5 1.21 &#177; 0.09 3.55 &#177; 0.27 SDF-Seg (implicit baseline) 30 86.1 &#177; 1.4 1.16 &#177; 0.08 3.39 &#177; 0.25 ShapeField-Nodule (Ours) 87.3 &#177; 1.3 1.03 &#177; 0.07 2.95 &#177; 0.23 Metrics include Dice Similarity Coefficient (Dice &#8593; ), Average Surface Distance (ASD &#8595; ), and 95th percentile Hausdorff Distance (HD95 &#8595; ). Best results are highlighted in bold. Our method achieves the highest performance across all three metrics, with a Dice score of 87.3% , an ASD of 1.03 mm , and an HD95 of 2.95 mm , outperforming all voxel-based baselines by a clear margin. This confirms that our implicit representation enables more precise and topologically coherent segmentation compared to traditional mask-based methods. Among the voxel-based architectures, nnU-Net 9 and Residual Attention U-Net 24 perform strongly, achieving Dice scores above 84.9%. However, their boundary precision remains limited, as evidenced by ASD values exceeding 1.2 mm. While these methods benefit from sophisticated architectural design and dynamic configuration (as in nnU-Net), they inherently lack the ability to model smooth sub-voxel transitions and regularized shape structure. UNet++ 23 and SegResNet 35 offer improvements in multi-scale feature fusion and residual learning but still trail behind our method in both overlap-based and boundary-based metrics. Notably, our ShapeField-Nodule reduces HD95 by over 0.6 mm compared to the next best baseline, indicating superior handling of outlier segmentation errors and irregular boundaries. SDF-Seg 30 , a recent method using implicit shape modeling in medical segmentation, achieves strong results and validates the advantage of SDF-based formulations. However, their method does not fully leverage coordinate-aware sampling or edge-gradient alignment, which we introduce. As a result, our model improves over SDF-Seg by more than 1.2% in Dice and over 0.4 mm in ASD, demonstrating the efficacy of our hybrid implicit-explicit design and refinement loss. To precisely delineate the sources of this improvement, we provide a direct comparison of the key methodological differences in Table 2 . Unlike SDF-Seg, which relies on a more standard implicit learning setup, our framework introduces three critical enhancements. First, our coordinate-aware hybrid sampling strategy (70% near-surface) focuses the model&#8217;s capacity on the decisive boundary region, whereas uniform sampling treats all locations equally. Second, the proposed shape-aware refinement loss provides explicit supervision from image gradients, forcing the predicted zero-level set to align with anatomical edges&#8212;a mechanism absent in the baseline. Finally, the integration of high-frequency positional encoding allows our MLP decoder to capture fine geometric details that are often missed without it. These components collectively account for the observed performance gains, moving beyond a simple SDF regression to a truly shape-aware representation. Table 2 Comparison of key methodological differences between the SDF-Seg baseline and our proposed ShapeField-Nodule Feature SDF-Seg [28] ShapeField-Nodule (Ours) Coordinate Sampling Standard Uniform Sampling Hybrid Near-Surface Oversampling Boundary Supervision Standard SDF Regression Loss SDF Loss + Edge-Gradient Alignment Loss Spatial Coordinate Input Direct Coordinates High-Frequency Positional Encoding Topological Regularization Implicit via SDF Implicit and Explicitly Refined Another key observation is the correlation between surface-based metrics and visual shape quality. Models with high Dice but poor ASD (e.g., V-Net) often yield segmentations with jagged or disconnected boundaries, while our approach maintains both volumetric accuracy and boundary continuity. This supports our core claim: that continuous implicit representations better preserve anatomical plausibility in noisy or ambiguous regions. In summary, the consistent performance gains across metrics indicate that ShapeField-Nodule not only segments lung nodules more accurately, but also reconstructs their geometry in a smoother and more robust manner, even in the challenging context of low-dose CT. To verify that these performance gains are not the result of random variation, we conducted statistical significance testing. Specifically, we performed a two-tailed paired t-test on the Dice scores obtained from our 5-fold cross-validation. The improvements of ShapeField-Nodule over the strongest implicit baseline, SDF-Seg, were found to be statistically significant ( p &lt; 0.01). Furthermore, when compared to the best-performing voxel-based baseline, Boundary-Aware U-Net, the improvements were also significant ( p &lt; 0.05). This statistical evidence validates that our proposed framework yields a tangible and reliable improvement in segmentation accuracy. Qualitative results Figure 1 presents qualitative comparisons of segmentation results from our method. Fig. 1 Qualitative comparison of segmentation results on representative pulmonary nodules from the LIDC-IDRI dataset. The results demonstrate consistent and accurate segmentation performance across diverse nodule morphologies, highlighting the model's capability to generate smooth and anatomically faithful contours regardless of shape variations. Our method generates smoother and anatomically plausible contours, better aligned with the true nodule boundary. This advantage is particularly noticeable in difficult cases, such as juxtapleural nodules and irregularly shaped lesions. ShapeField-Nodule successfully preserves contour continuity and enforces geometric regularity through its implicit representation. Notably, our zero-level set predictions smoothly interpolate across noisy boundaries, avoiding unnatural holes or spurious protrusions often seen in voxel-wise segmentations. These visual results validate the quantitative improvements reported earlier and highlight the interpretability and clinical reliability of our implicit modeling approach. Figure 2 illustrates segmentation results from three particularly challenging cases selected from the LIDC-IDRI dataset. Each case reflects a different type of difficulty often encountered in clinical practice: boundary ambiguity, intensity inhomogeneity, or topological complexity. Fig. 2 Segmentation results on challenging pulmonary nodule cases. The columns from left to right correspond to the predictions generated by UNet++, nnU-Net, and our proposed ShapeField-Nodule (Ours), respectively. While our method generally demonstrates superior boundary smoothness and topological consistency, it is not immune to limitations. An example of a failure case is observed in the bottom image of the last column, where the predicted contour fails to accurately align with the nodule boundary. Overall, these qualitative examples reinforce our quantitative findings: ShapeField-Nodule yields anatomically plausible, topologically coherent, and noise-robust segmentations in clinically challenging conditions where voxel-based approaches struggle. Figure 3 illustrates the predicted signed distance field (SDF) and reconstructed zero-level set contour from our ShapeField-Nodule model. This visualization highlights the central innovation of our approach: learning a continuous and differentiable shape representation from sparse voxel inputs. Fig. 3 Visualization of the predicted Signed Distance Field (SDF) and zero-level set contour from ShapeField-Nodule. From left to right: a Input LDCT slice with a pulmonary nodule; b Ground truth SDF computed from the binary mask using Euclidean distance transform; c Predicted SDF by our model, showing smooth transition from negative (inside) to positive (outside) regions; d Overlay of the predicted zero-level set contour on the original image. The continuous and differentiable nature of our implicit surface allows precise boundary modeling even under fuzzy or low-contrast conditions. The ground truth SDF in the second panel is computed via a signed Euclidean transform of the binary nodule mask. It exhibits sharp transitions across the boundary, but still captures a smooth distance decay. Our predicted SDF (panel 3) closely mimics this geometry while remaining smooth and continuous across the domain, thanks to the MLP-based implicit decoder and sub-voxel coordinate encoding. Most notably, the final panel shows the extracted zero-level set (i.e., &#981; ( x ) = 0) overlaid on the original CT slice. This contour aligns precisely with the true boundary, even in areas with partial volume effects or intensity ambiguity. Unlike voxel-based segmentations, our implicit surface formulation avoids blocky artifacts and yields a boundary that is both anatomically faithful and visually interpretable. Such a representation not only improves segmentation performance but also facilitates downstream tasks like surface-based analysis or 3D mesh extraction, offering a pathway to geometrically coherent clinical interpretation. Ablation study To understand the contribution of each component in ShapeField-Nodule, we conduct a detailed ablation study on the LIDC-IDRI dataset. Table 3 summarizes the quantitative performance changes when specific modules or design choices are removed or altered. Table 3 Ablation study on ShapeField-Nodule components Model Variant Dice (%) &#8593; ASD (mm) &#8595; HD95 (mm) &#8595; Full Model (ShapeField-Nodule) 87.3 1.03 2.95 w/o SDF loss 85.7 1.18 3.21 w/o shape-aware edge refinement loss 85.9 1.14 3.19 w/o positional encoding 84.5 1.29 3.42 replace MLP with 1 &#215; 1 conv head 84.1 1.33 3.60 replace SDF with binary mask prediction 83.4 1.45 3.75 We evaluate the contribution of each key module by disabling it from the full model. Performance is measured on the LIDC-IDRI dataset using 5-fold cross-validation. The full model achieves the best performance across all metrics. Removing the SDF, edge loss, or positional encoding results in substantial degradation. The bold means the best result. SDF vs. binary mask Replacing our signed distance function (SDF) output with a binary voxel mask prediction leads to the most severe degradation across all metrics (&#8722;3.9% Dice, +0.42 mm HD95), confirming that the continuous implicit representation is central to the performance gain. It demonstrates the benefit of learning a differentiable shape field over discrete voxel classification. Shape-aware refinement loss Disabling the edge alignment loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document} L edge causes a drop of 1.4% in Dice and increases the average surface distance by over 0.1 mm. This suggests that explicitly enforcing consistency between SDF gradients and image boundaries significantly improves spatial precision. SDF regression loss Removing the regression supervision on the signed distance values and relying only on classification-level supervision causes noticeable boundary drift. The surface becomes poorly aligned with anatomy, especially near fuzzy boundaries. Positional encoding When removing the Fourier positional encoding, performance declines sharply. This confirms that encoding sub-voxel spatial coordinates is essential for the MLP to generalize local geometry effectively. Implicit decoder head Substituting our multi-layer perceptron (MLP) with a simple 1 &#215; 1 &#215; 1 convolution layer causes an additional performance loss, due to the lack of non-linear spatial modeling and coordinate awareness. Overall, these results validate that each component of our architecture is carefully designed to contribute to geometric accuracy, anatomical smoothness, and robustness under LDCT noise. The full model configuration consistently achieves the best Dice, surface accuracy, and worst-case error. Hyperparameter sensitivity analysis To further assess the robustness and reproducibility of our framework, we conducted a sensitivity analysis on two key hyperparameters: the weight &#955; of the shape-aware refinement loss (Eq. 4) and the proportion of points sampled near the boundary in our hybrid sampling strategy. The results, summarized in Table 4 , demonstrate that our chosen parameters provide an optimal balance for segmentation performance. Table 4 Hyperparameter sensitivity analysis on the LIDC-IDRI dataset Configuration Dice (%) &#8593; ASD (mm) &#8595; HD95 (mm) &#8595; Varying Edge Refinement Weight ( &#955; ) &#955; = 0.01 86.7 1.09 3.08 &#955; = 0.1 (Ours) 87.3 1.03 2.95 &#955; = 0.5 86.9 1.08 3.02 Varying Near-Surface Sampling Proportion Proportion = 50% 86.4 1.11 3.15 Proportion = 70% (Ours) 87.3 1.03 2.95 Proportion = 90% 86.8 1.06 3.01 We report performance while varying the edge refinement loss weight ( &#955; ) and the proportion of near-surface samples. The parameters used in our final model are highlighted in bold. Impact of Edge Refinement Weight ( &#955; ) We varied &#955; from 0.01 to 0.5. A small weight ( &#955; = 0.01) diminishes the contribution of the edge alignment term, resulting in slightly less sharp boundaries and lower performance. Conversely, a large weight ( &#955; = 0.5) can lead to overfitting on noisy image gradients, causing the predicted surface to become irregular and slightly degrading overall accuracy. Our selected value of &#955; = 0.1 achieves the best trade-off, effectively leveraging image edges without sacrificing geometric stability. Impact of near-surface sampling proportion We also evaluated the proportion of the 8192 sampled points that are drawn from the narrow band around the nodule surface. As shown in Table 4 , a lower proportion (e.g., 50%) does not provide sufficient focus on the critical boundary region, leading to poorer surface metrics. A very high proportion (e.g., 90%), on the other hand, neglects the global context of the SDF, which can also impair performance. The 70% proportion used in our main experiments provides the best results across all metrics, confirming it as an effective choice. Beyond architectural modules, we also ablate two important design elements in our training pipeline: point sampling strategy and loss formulation. Sampling strategy As shown in Table 5 , training the implicit field using only uniform random sampling across the 3D patch results in suboptimal accuracy (Dice = 85.2%). While this method covers the full context, it lacks sufficient focus on the critical decision boundary near the nodule surface. Conversely, sampling only near the boundary band (within 5-voxel distance) improves boundary metrics but leads to poor global shape regularity. Table 5 Impact of sampling strategies and loss design Configuration Dice (%) &#8593; ASD (mm) &#8595; HD95 (mm) &#8595; Uniform sampling only 85.2 1.24 3.33 Boundary band sampling only 86.4 1.12 3.10 Hybrid sampling (Ours) 87.3 1.03 2.95 No edge refinement loss 85.9 1.14 3.19 With edge refinement loss (Ours) 87.3 1.03 2.95 We evaluate different combinations of point sampling methods and auxiliary loss terms on SDF prediction. Our proposed hybrid sampling and edge-aligned refinement loss together yield the best accuracy and boundary precision. The bold means the best result. Our proposed hybrid sampling strategy&#8212;drawing 70% of training points near the boundary and 30% randomly&#8212;achieves the best overall balance. It encourages accurate zero-level set learning while preserving coarse-to-fine spatial structure. This is especially beneficial in noisy regions or under CT imaging artifacts. Edge-aware refinement loss We further analyze the contribution of the auxiliary gradient-alignment loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document} L edge , which encourages consistency between the SDF gradient &#8711; f &#952; ( x ) and the image-derived boundary cues. When this term is omitted, performance drops across all metrics, with a notable 0.11 mm increase in ASD. Including this term improves surface adherence and discourages disconnected or over-smoothed shapes. Together, these results demonstrate that both sampling and regularization are critical for stable and anatomically coherent SDF learning. The implicit formulation, while powerful, benefits significantly from carefully chosen supervision signals and training dynamics. Fig. 4 presents a qualitative comparison of different ablated versions of ShapeField-Nodule on a representative pulmonary nodule. Fig. 4 Qualitative ablation study on key components of ShapeField-Nodule. Shown are segmentation results overlaid on the same axial CT slice. From left to right: full model (ours), model without edge refinement loss, without SDF regression loss, and a voxel-based binary mask baseline. The full model produces the smoothest and most anatomically faithful boundary, while ablating the SDF or edge loss leads to noisy or misaligned contours. The binary mask baseline results in staircase-like artifacts and loss of geometric continuity. The full model (leftmost) accurately captures the smooth and compact boundary of the nodule, aligning well with anatomical structures. When the edge refinement loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document} L edge is removed, the boundary becomes slightly irregular, particularly along weak edges where image gradients are ambiguous. Removing the SDF regression loss causes further degradation&#8212;noticeable in the form of shape drift and contour bulging&#8212;highlighting the importance of direct geometric supervision. The rightmost column shows the result using a binary voxel-based segmentation head, which produces a visibly staircase-like, blocky boundary due to its discrete nature. Compared to this, the zero-level set contour of our implicit field is significantly smoother and more precise. These visual comparisons reaffirm the quantitative findings in Tables 3 and 5 , demonstrating that each component contributes to more stable, anatomically faithful, and clinically interpretable segmentation results. Robustness and generalization In real-world deployment, segmentation models must remain stable under various forms of image perturbations such as noise, motion artifacts, and contrast variations&#8212;especially in low-dose CT (LDCT) scenarios. To assess the robustness of ShapeField-Nodule, we synthetically perturb the LIDC-IDRI test images with three common degradations and evaluate model performance without retraining. As shown in Table 6 , our method consistently outperforms voxel-based approaches under all tested perturbations. While minor performance degradation is observed (e.g., Dice drops from 87.3% to 85.6% with Gaussian noise), the boundary quality remains high, and HD95 increases only marginally. This stability is primarily due to the implicit surface representation, which acts as a spatial prior and smooths out noise-sensitive voxel inconsistencies. Table 6 Robustness to image perturbations Perturbation Type Dice (%) &#8593; HD95 (mm) &#8595; No perturbation (clean) 87.3 2.95 + Gaussian noise ( &#963; = 0.05) 85.6 3.24 + Motion blur (3 &#215; 3 kernel) 84.9 3.37 + Contrast shift (-20%) 85.3 3.30 We evaluate segmentation accuracy under common low-dose CT perturbations. Dice and HD95 are reported on the LIDC-IDRI test set with each perturbation applied at test time only. Our method maintains strong performance despite noise, blur, or contrast shifts, demonstrating enhanced resilience. The bold means the best result. Moreover, the SDF formulation allows the model to interpolate over missing or weak edge evidence, making it less reliant on high-frequency textures or sharp gradients. Even under motion blur and contrast shifts, ShapeField-Nodule retains sub-voxel contour accuracy and shape plausibility. These results highlight that continuous shape modeling offers not only improved accuracy under ideal conditions but also superior generalization to imperfect imaging environments, which is critical for clinical applicability. Analysis of generalization sources To better quantify the source of the improved generalization observed in our cross-dataset evaluation, we conducted an additional experiment as suggested by the reviewer. The goal is to disentangle the contribution of our proposed training strategies (i.e., hybrid sampling and the edge refinement loss) from the contribution of our network architecture (i.e., the specific MLP head with positional encoding). To achieve this, we created a new baseline, which we term SDF-Seg+ , by training the original SDF-Seg architecture using our exact training pipeline. The results of this analysis are presented in Table 7 . When comparing the standard SDF-Seg with SDF-Seg+, we observe a substantial performance increase of approximately 1.2&#8211;1.3% in Dice score on both external datasets. This clearly indicates that our training methodology, particularly the near-surface sampling and edge-aware loss, is a major contributor to robust generalization. However, our full ShapeField-Nodule model still consistently outperforms the SDF-Seg+ baseline. This remaining performance gap confirms the efficacy of our architectural design. We conclude that the superior generalization of ShapeField-Nodule is a result of both factors: the advanced training strategy provides a strong foundation for robustness, while the carefully designed implicit decoder architecture further refines the model&#8217;s ability to learn a truly generalizable shape representation. Table 7 Analysis of generalization sources Method LUNA16 Dice (%) &#8593; Tianchi Dice (%) &#8593; SDF-Seg [36] (Standard Training) 84.0 83.2 SDF-Seg+ (Our Training Strategy) 85.3 84.5 ShapeField-Nodule (Ours, Full Model) 85.8 85.2 We compare our full model to the SDF-Seg baseline and an enhanced baseline (SDF-Seg+) trained with our proposed methods. All models are trained on LIDC-IDRI and tested on unseen datasets. The bold means the best result. To further assess generalization beyond synthetic perturbations, we evaluate the performance of ShapeField-Nodule on two external datasets&#8212;LUNA16 and Tianchi&#8212;without any domain adaptation or fine-tuning. These datasets differ significantly from LIDC-IDRI in terms of patient demographics, CT acquisition protocols, and nodule annotation standards. As shown in Table 8 , our method consistently achieves the highest Dice scores across both target domains, outperforming U-Net, nnU-Net, and the SDF-Seg baseline. Specifically, ShapeField-Nodule obtains 85.8% Dice on LUNA16 and 85.2% on Tianchi, with a margin of up to 2.6% over the best voxel-based alternative. Table 8 Cross-dataset generalization performance Method LUNA16 Dice (%) &#8593; Tianchi Dice (%) &#8593; U-Net (baseline) 80.6 78.9 nnU-Net 9 82.4 81.1 SDF-Seg 39 84.0 83.2 ShapeField-Nodule (Ours) 85.8 85.2 We train the model on LIDC-IDRI and test directly on two unseen datasets: LUNA16 and Tianchi, without any fine-tuning. Our ShapeField-Nodule exhibits superior generalization ability compared to voxel-based baselines, showing consistent segmentation quality across diverse patient cohorts and scanning protocols. The bold means the best result. This improvement stems from our implicit geometric modeling, which encodes shape priors independent of voxel-grid alignment. By learning signed distance fields and zero-level set boundaries, the model becomes less sensitive to voxel size, contrast scale, and scanner-specific variations. This cross-dataset stability indicates strong potential for real-world clinical deployment where domain shift is unavoidable. Together with the perturbation results in Table 6 , these findings confirm that ShapeField-Nodule not only excels under clean settings but also adapts robustly across domains and noise conditions. Inference efficiency Figure 5 illustrates the trade-offs between segmentation accuracy, inference latency, and model size across different baseline methods. Fig. 5 Inference efficiency of ShapeField-Nodule. Left: Dice score vs. inference time per volume (seconds). Right: Dice score vs. model size (in millions of parameters). Our method consistently achieves the highest segmentation accuracy while maintaining lower latency and a compact architecture compared to voxel-based baselines. In the left panel, we compare Dice score against inference time per volume on the LIDC-IDRI test set. Despite operating on continuous representations, ShapeField-Nodule achieves the best Dice score with only marginal latency overhead compared to voxel-based models. It outperforms SDF-Seg and nnU-Net both in accuracy and speed, indicating that our lightweight MLP-based implicit head does not introduce a significant computational burden. The right panel shows Dice score versus model size. While U-Net and nnU-Net require over 20-30 million parameters to reach reasonable accuracy, ShapeField-Nodule matches or surpasses their performance with fewer than 10 million parameters. This suggests that our method encodes geometric priors more effectively, allowing compact models to generalize well without overfitting. Together, these plots highlight that ShapeField-Nodule provides a highly favorable balance between accuracy, speed, and model size&#8212;crucial for real-time and embedded deployment in clinical settings. Discussion We presented ShapeField-Nodule , a continuous shape embedding framework for pulmonary nodule segmentation in low-dose CT. Unlike conventional voxel-based approaches, our method models nodule boundaries via signed distance functions (SDFs), allowing sub-voxel accuracy, topological smoothness, and enhanced robustness under low signal-to-noise imaging conditions. Built upon a 3D U-Net backbone with an implicit MLP decoder, ShapeField-Nodule enables boundary-aware learning and regularized shape representation. We further introduced a shape-aware refinement loss to align the predicted SDF gradient with edge cues, resulting in more anatomically faithful contours. Extensive experiments across LIDC-IDRI, LUNA16, and Tianchi datasets demonstrate that our method achieves state-of-the-art performance while maintaining high efficiency and generalization. This work highlights the advantages of continuous shape modeling in medical image segmentation, particularly in scenarios with sparse, noisy, or ambiguous visual evidence. In future work, we plan to extend this implicit representation to multi-object segmentation, integrate uncertainty modeling into the SDF field, and explore its use for 3D mesh reconstruction and radiomics-driven downstream analysis. Overall, ShapeField-Nodule offers a principled and interpretable alternative to voxel masks, with strong potential for deployment in real-world clinical pipelines for early lung cancer screening. Beyond improving segmentation accuracy, the continuous shape representation offered by ShapeField-Nodule holds significant promise for direct clinical applications. In computer-aided diagnosis (CAD) pipelines, our method can provide highly precise and anatomically plausible inputs for downstream tasks, such as malignancy classification or volumetric growth tracking, potentially reducing errors caused by noisy, voxelated boundaries. The differentiable and smooth nature of the predicted SDF is particularly advantageous for radiomics, as it allows for the stable and robust extraction of geometry-based features (e.g., surface area, sphericity, margin sharpness) that are often unreliable when computed from discrete masks. Furthermore, the ability to generate high-quality 3D meshes directly from the zero-level set can aid in pre-operative surgical planning, offering clinicians a more intuitive visualization of the nodule&#8217;s morphology and its spatial relationship with adjacent structures like vessels and airways Fig. 6 . Fig. 6 Examples of challenging cases and limitations of ShapeField-Nodule. (Left) An extremely small nodule where partial volume effects lead to an over-smoothed and slightly inaccurate segmentation. (Right) A case with severe motion artifacts, where the corrupted image gradients result in a misaligned boundary prediction. Methods In this section, we present ShapeField-Nodule , a continuous shape embedding framework for pulmonary nodule segmentation in low-dose CT (LDCT). Our method aims to overcome the limitations of discrete voxel-based segmentation by modeling the nodule boundary as an implicit signed distance function (SDF). The key idea is to regress a continuous volumetric field whose zero level set represents the anatomical boundary of the nodule. This allows for sub-voxel precision, smoother shape continuity, and better topological regularity. Fig. 7 illustrates the full pipeline. Fig. 7 Overview of the ShapeField-Nodule framework. The model takes a 3D LDCT volume as input and extracts hierarchical features using a 3D U-Net. A coordinate-conditioned MLP predicts signed distance values at sampled spatial locations using both features and positional encoding. The final segmentation is obtained by thresholding the zero level-set of the predicted signed distance field (SDF). A shape-aware loss aligns the gradient of the predicted SDF with the image edge field. Overview and motivation Conventional segmentation networks output binary masks by classifying each voxel independently. While effective in many cases, such predictions are discrete and lack the capacity to capture smooth geometric transitions or enforce structural regularity. These issues are amplified in LDCT, where image noise and low contrast impair voxel-wise decisions. To address this, we propose a continuous formulation where the segmentation target is a signed distance field \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${f}_{\\theta }:{{\\mathbb{R}}}^{3}\\to {\\mathbb{R}}$$\\end{document} f &#952; : R 3 &#8594; R parameterized by a neural network. This field encodes the shortest distance of any 3D point to the underlying nodule surface, with negative values inside the object, positive values outside, and zero on the boundary. The continuous nature of SDFs allows us to reconstruct smooth iso-surfaces and sidestep voxelization artifacts. Network architecture The overall architecture of ShapeField-Nodule is illustrated in Fig. 8 . It consists of two main components: A 3D U-Net backbone that extracts multi-scale volumetric features from the input LDCT scan \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\mathcal{X}}\\in {{\\mathbb{R}}}^{D\\times H\\times W}$$\\end{document} X &#8712; R D &#215; H &#215; W . A coordinate-conditioned MLP head that predicts the SDF value at any 3D location \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\bf{x}}\\in {{\\mathbb{R}}}^{3}$$\\end{document} x &#8712; R 3 using both the location and the contextual features from the U-Net encoder. Fig. 8 Detailed illustration of the internal modules of ShapeField-Nodule. The network consists of a multi-scale 3D U-Net, coordinate sampling and positional encoding modules, and an MLP head for SDF prediction. A gradient alignment branch compares predicted SDF gradients with image-derived edge vectors to refine boundary localization. Specifically, given a sampled coordinate x in the input volume, we bilinearly interpolate the feature vector &#981; ( x ) from the U-Net&#8217;s feature map, and concatenate it with a positional encoding &#947; ( x ) to form the input to the MLP: 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${f}_{\\theta }({\\bf{x}})=\\,\\text{MLP}\\,([\\phi ({\\bf{x}});\\gamma ({\\bf{x}})])$$\\end{document} f &#952; ( x ) = MLP ( [ &#981; ( x ) ; &#947; ( x ) ] ) Here, &#947; ( &#8901; ) denotes a sinusoidal positional encoding similar to that used in NeRF 36 , and f &#952; ( &#8901; ) is the predicted signed distance value at x . Continuous shape embedding via SDF To learn the implicit surface, we construct supervision signals in the form of ground-truth signed distances. Given a binary segmentation mask M ( x ) &#8712; {0, 1}, we compute its Euclidean signed distance transform \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{f}({\\bf{x}})$$\\end{document} f ^ ( x ) , where negative values indicate foreground (inside the nodule) and positive values indicate background. We then minimize an L 1 regression loss over a set of sampled spatial coordinates { x i }: 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{SDF}}}=\\frac{1}{N}\\mathop{\\sum }\\limits_{i=1}^{N}\\left\\vert {f}_{\\theta }({{\\bf{x}}}_{i})-\\hat{f}({{\\bf{x}}}_{i})\\right\\vert$$\\end{document} L SDF = 1 N &#8721; i = 1 N f &#952; ( x i ) &#8722; f ^ ( x i ) The field is evaluated only at sampled voxels, avoiding full-volume regression and keeping computation efficient. In practice, we oversample near-boundary voxels (within a narrow band around the level set) to emphasize learning accurate shape transitions. Shape-aware refinement loss While the SDF regression ensures geometric fidelity, it does not directly encourage alignment with actual image intensity edges. To improve edge adherence, we introduce a shape-aware refinement loss based on the gradient field of the SDF. We first compute the predicted gradient &#8711; f &#952; ( x ) using auto-differentiation. Then, we enforce alignment between this gradient and the image-derived edge vector g ( x ), which can be computed via Sobel or Laplacian filters: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{edge}}}=\\frac{1}{N}\\mathop{\\sum }\\limits_{i=1}^{N}\\left(1-\\cos \\angle (\\nabla {f}_{\\theta }({{\\bf{x}}}_{i}),{\\bf{g}}({{\\bf{x}}}_{i}))\\right)$$\\end{document} L edge = 1 N &#8721; i = 1 N 1 &#8722; cos &#8736; ( &#8711; f &#952; ( x i ) , g ( x i ) ) This loss penalizes angular mismatch between the SDF&#8217;s directional gradient and actual image edges, encouraging the zero level-set to lie on strong boundaries. Training objective and optimization The total training loss combines the SDF regression and the edge-guided refinement term: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{\\mathcal{L}}}_{{\\rm{total}}}={{\\mathcal{L}}}_{{\\rm{SDF}}}+\\lambda \\cdot {{\\mathcal{L}}}_{{\\rm{edge}}}$$\\end{document} L total = L SDF + &#955; &#8901; L edge where &#955; is a scalar weight (typically set to 0.1) balancing shape alignment and distance accuracy. During inference, the final segmentation mask is obtained by thresholding the predicted SDF field at zero level: 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{M}({\\bf{x}})={\\mathbb{I}}[{f}_{\\theta }({\\bf{x}})\\le 0]$$\\end{document} M ^ ( x ) = I [ f &#952; ( x ) &#8804; 0 ] This produces a binary mask with sub-voxel surface continuity. Implementation details We implement our model in PyTorch using MONAI for medical imaging operations. The 3D U-Net backbone consists of four downsampling blocks with 32-256 channels. The MLP head contains 4 fully connected layers with ReLU activations and skip connections. We use a learning rate of 1 &#215; 10 &#8722;4 and Adam optimizer. Training is conducted for 300 epochs on LIDC-IDRI with a batch size of 2. Spatial coordinates are sampled dynamically per batch with higher density near boundaries. Ethics approval and consent to participate This study utilized only publicly available and anonymized datasets (LIDC-IDRI, LUNA16, Tianchi), which do not require additional ethical approval or informed consent under current guidelines. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. These authors contributed equally: Xuyu Gu, Yifei Zhu. Acknowledgements This project did not receive any financial support. Author contributions X.G. and Y.Z. had full access to all the data and take responsibility for the integrity of the data and the accuracy of the analysis. CL contributed to the conception and design of the study. XG and YZ contributed to data acquisition, statistical analysis, and interpretation. K.J., X.X. and L.X. contributed to the drafting of the manuscript. All authors were responsible for the critical revision of the manuscript for important content. Data availability The datasets used in this study are publicly accessible: - LIDC-IDRI: https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI- LUNA16: https://luna16.grand-challenge.org/- Tianchi Lung Nodule Dataset: https://tianchi.aliyun.com/competition/entrance/231601/information . Code availability The source code for ShapeField-Nodule and all experiments will be released publicly upon accept : https://anonymous.4open.science/r/ShapeField-B6CD/Readme.md . Competing interests The authors declare no competing interests. References 1. Armato SG The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on CT scans Med. Phys. 2011 38 915 931 10.1118/1.3528204 21452728 PMC3041807 Armato, S. G. et al. The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on CT scans. Med. Phys. 38 , 915&#8211;931 (2011). 21452728 10.1118/1.3528204 PMC3041807 2. Cherezov D Lee B Saha PK Treatment decision strategies and outcomes in early-stage lung cancer Clin. Lung Cancer 2016 17 e1 e10 Cherezov, D., Lee, B. &amp; Saha, P. K. Treatment decision strategies and outcomes in early-stage lung cancer. Clin. Lung Cancer 17 , e1&#8211;e10 (2016).26837474 3. Setio AAA Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The luna16 challenge Med. image Anal. 2017 42 1 13 10.1016/j.media.2017.06.015 28732268 Setio, A. A. A. et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The luna16 challenge. Med. image Anal. 42 , 1&#8211;13 (2017). 28732268 10.1016/j.media.2017.06.015 4. Zhou Y A review of automatic segmentation algorithms for lung nodules from computed tomography images IEEE Access 2021 9 37306 37330 Zhou, Y. et al. A review of automatic segmentation algorithms for lung nodules from computed tomography images. IEEE Access 9 , 37306&#8211;37330 (2021). 5. Liao F Evaluate pulmonary nodules with radiomics and deep learning Transl. Lung Cancer Res. 2019 8 288 Liao, F. et al. Evaluate pulmonary nodules with radiomics and deep learning. Transl. Lung Cancer Res. 8 , 288 (2019). 6. Ding, J.-Y. et al. Accurate pulmonary nodule detection in computed tomography images using deep convolutional neural networks. In Medical Image Computing and Computer-Assisted Intervention (MICCAI) , 559&#8211;567 (Springer, 2017). 7. Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI) , 234&#8211;241 (Springer, 2015). 8. Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV) , 565&#8211;571 (IEEE, 2016). 9. Isensee F nnu-net: a self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 Isensee, F. et al. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nat. Methods 18 , 203&#8211;211 (2021). 33288961 10.1038/s41592-020-01008-z 10. Zhang X-S Xie X Yang Y-Z Hospedales TM A survey on deep learning-based fine-grained image classification Pattern Recognit. 2021 110 107613 Zhang, X.-S., Xie, X., Yang, Y.-Z. &amp; Hospedales, T. M. A survey on deep learning-based fine-grained image classification. Pattern Recognit. 110 , 107613 (2021).32868956 11. Park, J. J., Florence, P., Straub, J., Newcombe, R. &amp; Lovegrove, S. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR , 165&#8211;174 (2019). 12. Gropp, A., Yariv, L., Haim, N., Atzmon, M. &amp; Lipman, Y. Implicit geometric regularization for learning shapes. In International Conference on Machine Learning (ICML) , 3789&#8211;3799 (2020). 13. Chen, Z. &amp; Zhang, H. Learning implicit fields for generative shape modeling. In CVPR , 5939&#8211;5948 (2019). 14. Atzmon, M. &amp; Lipman, Y. Sal: Sign agnostic learning of shapes from raw data. In CVPR , 2565&#8211;2574 (2020). 15. Xiao, X. et al. Visual instance-aware prompt tuning (2025). https://arxiv.org/abs/2507.07796 . 2507.07796. 16. Lin, K. et al. Learning signed distance fields for 3d human shape reconstruction. In CVPR , 12316&#8211;12325 (2021). 17. Ma, K., Zhang, Y. &amp; Wang, Y. Sdfreg: Signed distance function-based registration of medical images. In MICCAI , 204&#8211;214 (2022). 18. Davies R Lim Y Glocker B Organ shape completion using implicit surface representations Med. Image Anal. 2021 71 102037 Davies, R., Lim, Y. &amp; Glocker, B. Organ shape completion using implicit surface representations. Med. Image Anal. 71 , 102037 (2021).33910110 19. Xiao, X. et al. Describe anything in medical images (2025). https://arxiv.org/abs/2505.05804 . 2505.05804. 20. Wang, W. et al. Multi-dimensional transformer with attention-based filtering for medical image segmentation. In 2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI) , 632&#8211;639 (2024). 21. Xiao, X. et al. Hgtdp-dta: Hybrid graph-transformer with dynamic prompt for drug-target binding affinity prediction. In Neural Information Processing (eds Mahmud, M. et al.) 340&#8211;354 (Springer Nature Singapore, Singapore, 2025). 22. Oktay, O. et al. Attention U-Net: Learning where to look for the pancreas. In MICCAI (2018). 23. Zhou, Z. et al. Unet++: Redesigning skip connections to exploit multiscale features in image segmentation. In MICCAI (2018). 10.1109/TMI.2019.2959609 PMC7357299 31841402 24. Zhang, H. et al. Residual attention network for pulmonary nodule segmentation from low-dose ct images. IEEE Trans. Med. Imaging (2021). 25. Li, X. et al. Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation. Med. Image Anal. (2020). 10.1016/j.media.2020.101766 32623276 26. Xiao, X. et al. Hgtdp-dta: Hybrid graph-transformer with dynamic prompt for drug-target binding affinity prediction. In International Conference on Neural Information Processing , 340&#8211;354 (Springer, 2024). 27. Xu, Q. et al. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In Advances in Neural Information Processing Systems (NeurIPS) , vol. 32 (2019). 28. Yariv, L. et al. Volume rendering of neural implicit surfaces. In Advances in Neural Information Processing Systems (NeurIPS) , vol. 34, 4805&#8211;4815 (2021). 29. Lu, Y. et al. Volumetric shape regularization for surface-aware 3d medical image segmentation. MICCAI (2021). 30. Huang, Z. et al. Shape-aware implicit neural representation for medical image segmentation. Med. Image Anal. (2022). 31. Hu, X. et al. Topology-preserving deep image segmentation via Euler characteristic loss. Adv. Neural Inf. Process. Syst. (NeurIPS) (2021). 32. Bai, W. et al. Semi-supervised learning for network-based cardiac MR image segmentation. Med. Image Anal. (2017). 33. Karimi, D. &amp; Salcudean, S. E. Surface dice-similarity coefficient: An improved metric for segmentation evaluation. MICCAI (2020). 34. Cloud, A. Tianchi medical AI challenge: Lung nodule detection and classification. https://tianchi.aliyun.com/competition/entrance/231601/information (2017). 35. Myronenko, A. 3D MRI brain tumor segmentation using autoencoder regularization. In MICCAI BRATS (2018). 36. Mildenhall B Nerf: representing scenes as neural radiance fields for view synthesis Commun. ACM 2021 65 99 106 10.1145/3503250 Mildenhall, B. et al. Nerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM 65 , 99&#8211;106 (2021). 37. &#199;i&#231;ek, O., Abdulkadir, A., Lienkamp, S., Brox, T. &amp; Ronneberger, O. 3d U-Net: Learning dense volumetric segmentation from sparse annotation (2016). 38. Kervadec, H. et al. Boundary loss for highly unbalanced segmentation. In MICCAI (2021). 10.1016/j.media.2020.101851 33080507 39. Lin, T., Chen, Z., Yan, Z., Yu, W. &amp; Zheng, F. Stable diffusion segmentation for biomedical images with single-step reverse process. In Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2024 , 656&#8211;666 (Springer Nature Switzerland, Cham, 2024)."
}