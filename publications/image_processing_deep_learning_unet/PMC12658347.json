{
  "pmcid": "PMC12658347",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:29.985028",
  "metadata": {
    "journal_title": "Journal of Applied Clinical Medical Physics",
    "journal_nlm_ta": "J Appl Clin Med Phys",
    "journal_iso_abbrev": "J Appl Clin Med Phys",
    "journal": "Journal of Applied Clinical Medical Physics",
    "pmcid": "PMC12658347",
    "pmid": "41306077",
    "doi": "10.1002/acm2.70385",
    "title": "HCViT‐Net: Hybrid CNN and multi scale query transformer network for dermatological image segmentation",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Jiao Wei",
      "Xu Jianghui",
      "Fang Yijiao",
      "Huang Jiaojiao",
      "Zhu Yujie",
      "Ling Dandan"
    ],
    "abstract": "Abstract Background Dermoscopic lesion segmentation is crucial for dermatology, yet existing methods struggle to integrate global context with local details under the efficiency constraints required for clinical use. Purpose We aim to develop a lightweight model that simultaneously captures long‐range spatial dependencies and preserves fine‐grained boundary details for dermoscopic lesions. The method is designed to achieve a favorable accuracy–efficiency trade‐off, thereby improving segmentation performance and ensuring potential for practical clinical deployment. Methods Proposing a lightweight hybrid model, HCViT‐Net, featuring an encoder–decoder architecture. It incorporates a multi‐scale query transformer (MSQFormer) into each stage of its convolutional encoder to efficiently capture global, multi‐scale context. Furthermore, a wavelet‐guided attention refinement module (WARM) is introduced on the highest‐resolution skip connection to selectively enhance high‐frequency boundary details and bridge the semantic gap between the encoder and decoder, thus improving model performance. Results Evaluated on ISIC 2017 and 2018, our model achieved mean intersection‐over‐union (mIoU) of 87.76% and 87.45%, respectively. With only 5.76M parameters and 7.51 GFLOPs, it demonstrates performance competitive with existing methods at a significantly lower computational cost. Conclusions HCViT‐Net achieves an excellent accuracy–efficiency trade‐off. It improves segmentation accuracy with a low computational footprint, showing strong potential for practical deployment in dermatology workflows.",
    "keywords": [
      "boundary refinement",
      "medical image segmentation",
      "multi‐scale features",
      "skin lesion"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" id=\"acm270385\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">J Appl Clin Med Phys</journal-id><journal-id journal-id-type=\"iso-abbrev\">J Appl Clin Med Phys</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3200</journal-id><journal-id journal-id-type=\"pmc-domain\">jacmp</journal-id><journal-id journal-id-type=\"publisher-id\">ACM2</journal-id><journal-title-group><journal-title>Journal of Applied Clinical Medical Physics</journal-title></journal-title-group><issn pub-type=\"epub\">1526-9914</issn><publisher><publisher-name>Wiley</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12658347</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12658347.1</article-id><article-id pub-id-type=\"pmcaid\">12658347</article-id><article-id pub-id-type=\"pmcaiid\">12658347</article-id><article-id pub-id-type=\"pmid\">41306077</article-id><article-id pub-id-type=\"doi\">10.1002/acm2.70385</article-id><article-id pub-id-type=\"publisher-id\">ACM270385</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"overline\"><subject>Research Article</subject></subj-group><subj-group subj-group-type=\"heading\"><subject>IMAGING PHYSICS</subject><subj-group subj-group-type=\"heading\"><subject>Research Article</subject></subj-group></subj-group></article-categories><title-group><article-title>HCViT&#8208;Net: Hybrid CNN and multi scale query transformer network for dermatological image segmentation</article-title><alt-title alt-title-type=\"left-running-head\">JIAO <sc>et&#160;al.</sc></alt-title></title-group><contrib-group><contrib id=\"acm270385-cr-0001\" contrib-type=\"author\"><name name-style=\"western\"><surname>Jiao</surname><given-names initials=\"W\">Wei</given-names></name><xref rid=\"acm270385-aff-0001\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib id=\"acm270385-cr-0002\" contrib-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"J\">Jianghui</given-names></name><xref rid=\"acm270385-aff-0001\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib id=\"acm270385-cr-0003\" contrib-type=\"author\"><name name-style=\"western\"><surname>Fang</surname><given-names initials=\"Y\">Yijiao</given-names></name><xref rid=\"acm270385-aff-0001\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib id=\"acm270385-cr-0004\" contrib-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names initials=\"J\">Jiaojiao</given-names></name><xref rid=\"acm270385-aff-0001\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib id=\"acm270385-cr-0005\" contrib-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names initials=\"Y\">Yujie</given-names></name><xref rid=\"acm270385-aff-0002\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"acm270385-aff-0003\" ref-type=\"aff\">\n<sup>3</sup>\n</xref></contrib><contrib id=\"acm270385-cr-0006\" contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Ling</surname><given-names initials=\"D\">Dandan</given-names></name><xref rid=\"acm270385-aff-0001\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><address><email>ldd202506@163.com</email></address></contrib></contrib-group><aff id=\"acm270385-aff-0001\">\n<label>\n<sup>1</sup>\n</label>\n<named-content content-type=\"organisation-division\">Department of Anesthesiology</named-content>\n<institution>Fudan University Shanghai Cancer Center</institution>\n<city>Shanghai</city>\n<country country=\"CN\">China</country>\n</aff><aff id=\"acm270385-aff-0002\">\n<label>\n<sup>2</sup>\n</label>\n<named-content content-type=\"organisation-division\">Department of Dermatology</named-content>\n<institution>Shanghai Ninth People's Hospital</institution>\n<city>Shanghai</city>\n<country country=\"CN\">China</country>\n</aff><aff id=\"acm270385-aff-0003\">\n<label>\n<sup>3</sup>\n</label>\n<named-content content-type=\"organisation-division\">School of Medicine</named-content>\n<institution>Shanghai Jiaotong University</institution>\n<city>Shanghai</city>\n<country country=\"CN\">China</country>\n</aff><author-notes><corresp id=\"correspondenceTo\"><label>*</label><bold>Correspondence</bold><break/>\nDandan Ling, Department of Anesthesiology, Fudan University Shanghai Cancer Center, Shanghai, China.<break/> Email: <email>ldd202506@163.com</email><break/></corresp></author-notes><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>12</month><year>2025</year></pub-date><volume>26</volume><issue seq=\"4656\">12</issue><issue-id pub-id-type=\"pmc-issue-id\">500804</issue-id><issue-id pub-id-type=\"doi\">10.1002/acm2.v26.12</issue-id><elocation-id>e70385</elocation-id><history><date date-type=\"rev-recd\"><day>14</day><month>10</month><year>2025</year></date><date date-type=\"received\"><day>15</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>21</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 16:25:13.263\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement content-type=\"article-copyright\">&#169; 2025 The Author(s). <italic toggle=\"yes\">Journal of Applied Clinical Medical Physics</italic> published by Wiley Periodicals, LLC on behalf of The American Association of Physicists in Medicine.</copyright-statement><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"ACM2-26-e70385.pdf\"/><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pdf\" xlink:href=\"file:ACM2-26-e70385.pdf\"/><abstract><title>Abstract</title><sec id=\"acm270385-sec-0010\"><title>Background</title><p>Dermoscopic lesion segmentation is crucial for dermatology, yet existing methods struggle to integrate global context with local details under the efficiency constraints required for clinical&#160;use.</p></sec><sec id=\"acm270385-sec-0020\"><title>Purpose</title><p>We aim to develop a lightweight model that simultaneously captures long&#8208;range spatial dependencies and preserves fine&#8208;grained boundary details for dermoscopic lesions. The method is designed to achieve a favorable accuracy&#8211;efficiency trade&#8208;off, thereby improving segmentation performance and ensuring potential for practical clinical&#160;deployment.</p></sec><sec id=\"acm270385-sec-0030\"><title>Methods</title><p>Proposing a lightweight hybrid model, HCViT&#8208;Net, featuring an encoder&#8211;decoder architecture. It incorporates a multi&#8208;scale query transformer (MSQFormer) into each stage of its convolutional encoder to efficiently capture global, multi&#8208;scale context. Furthermore, a wavelet&#8208;guided attention refinement module (WARM) is introduced on the highest&#8208;resolution skip connection to selectively enhance high&#8208;frequency boundary details and bridge the semantic gap between the encoder and decoder, thus improving model&#160;performance.</p></sec><sec id=\"acm270385-sec-0040\"><title>Results</title><p>Evaluated on ISIC 2017 and 2018, our model achieved mean intersection&#8208;over&#8208;union (mIoU) of 87.76% and 87.45%, respectively. With only 5.76M parameters and 7.51 GFLOPs, it demonstrates performance competitive with existing methods at a significantly lower computational cost.</p></sec><sec id=\"acm270385-sec-0050\"><title>Conclusions</title><p>HCViT&#8208;Net achieves an excellent accuracy&#8211;efficiency trade&#8208;off. It improves segmentation accuracy with a low computational footprint, showing strong potential for practical deployment in dermatology workflows.</p></sec></abstract><kwd-group kwd-group-type=\"author-generated\"><kwd id=\"acm270385-kwd-0001\">boundary refinement</kwd><kwd id=\"acm270385-kwd-0002\">medical image segmentation</kwd><kwd id=\"acm270385-kwd-0003\">multi&#8208;scale features</kwd><kwd id=\"acm270385-kwd-0004\">skin lesion</kwd></kwd-group><counts><fig-count count=\"13\"/><table-count count=\"8\"/><page-count count=\"18\"/><word-count count=\"8467\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>source-schema-version-number</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>December 2025</meta-value></custom-meta><custom-meta><meta-name>details-of-publishers-convertor</meta-name><meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.6.6 mode:remove_FC converted:27.11.2025</meta-value></custom-meta></custom-meta-group></article-meta><notes><p content-type=\"self-citation\">\n<mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0035\"><string-name name-style=\"western\"><surname>Jiao</surname><given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Xu</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Fang</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Huang</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhu</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Ling</surname><given-names>D</given-names></string-name>. <article-title>HCViT&#8208;Net: Hybrid CNN and multi scale query transformer network for dermatological image segmentation</article-title>. <source>J Appl Clin Med Phys</source>. <year>2025</year>;<volume>26</volume>:<elocation-id>e70385</elocation-id>. <pub-id pub-id-type=\"doi\">10.1002/acm2.70385</pub-id><pub-id pub-id-type=\"pmid\">41306077</pub-id></mixed-citation>\n</p><fn-group><fn fn-type=\"equal\" id=\"acm270385-note-0001\"><p>Wei Jiao, Jianhui Xu and Yijiao Fang are contributed equally to this work.</p></fn></fn-group></notes></front><body><sec id=\"acm270385-sec-0060\"><label>1</label><title>INTRODUCTION</title><p>Melanoma, characterized by aggressive invasiveness, high metastatic potential, and ele vated mortality, has become one of the fastest&#8208;growing malignancies worldwide.<xref rid=\"acm270385-bib-0001\" ref-type=\"bibr\">\n<sup>1</sup>\n</xref> In clinical settings, dermatologists must manually identify and delineate lesions via dermatoscopic imaging&#8212;a diagnostic procedure critically dependent on clinician experience and technical proficiency. Substantial evidence demonstrates that automated lesion segmentation in skin imaging enhances the accuracy of abnormality detection by both clinicians and AI diagnostic systems, thereby providing an objective foundation for early screening and differential diagnosis.<xref rid=\"acm270385-bib-0002\" ref-type=\"bibr\">\n<sup>2</sup>\n</xref>, <xref rid=\"acm270385-bib-0003\" ref-type=\"bibr\">\n<sup>3</sup>\n</xref>\n</p><p>The rapid evolution of deep learning and computer vision<xref rid=\"acm270385-bib-0004\" ref-type=\"bibr\">\n<sup>4</sup>\n</xref> has revolutionized medical image segmentation, yielding unprecedented gains in accuracy. The fully convolutional network (FCN)<xref rid=\"acm270385-bib-0005\" ref-type=\"bibr\">\n<sup>5</sup>\n</xref> pioneered pixel&#8208;level CNN segmentation, and subsequent models&#8212;for example, DeepLab<xref rid=\"acm270385-bib-0006\" ref-type=\"bibr\">\n<sup>6</sup>\n</xref> with atrous spatial pyramid pooling (ASPP) for larger receptive fields, and U&#8208;Net<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref> with encoder&#8211;decoder skip connections&#8212;have refined multi&#8208;scale context capture and fine&#8208;detail&#160;recovery.</p><p>While these CNNs&#8208;based segmentation models<xref rid=\"acm270385-bib-0005\" ref-type=\"bibr\">\n<sup>5</sup>\n</xref>, <xref rid=\"acm270385-bib-0006\" ref-type=\"bibr\">\n<sup>6</sup>\n</xref>, <xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref> have achieved remarkable success, the inherently local receptive fields of convolutional layers limit global dependency modeling, a key drawback in skin lesion segmentation. As illustrated in the first two columns of Figure&#160;<xref rid=\"acm270385-fig-0001\" ref-type=\"fig\">1</xref>, skin lesions in dermoscopic images often exhibit significant spatial occupancy that may exceed the local receptive fields of CNNs. The intrinsic locality of convolution operations fundamentally limits the model's capacity to capture global morphological characteristics of lesions. Furthermore, the latter two columns of Figure&#160;<xref rid=\"acm270385-fig-0001\" ref-type=\"fig\">1</xref> demonstrate that skin lesions can present textural similarities to benign skin features. Establishing global feature dependencies across image regions proves crucial for enhancing the model's discriminative power against lesion&#8208;mimicking artifacts, thereby improving segmentation&#160;precision.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0001\" orientation=\"portrait\"><label>FIGURE 1</label><caption><p>Illustration of the global (blue curves) and local (green curves) contextual information in dermatological images. By establishing global feature dependencies across image regions, the model can capture the global morphological characteristics of lesions and enhance its discriminative power against lesion&#8208;mimicking artifacts, thereby improving segmentation precision.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-1\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g004.jpg\"/></fig><p>Therefore, due to the superior performance of ViTs<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref>, <xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref> architectures in establishing long&#8208;range dependencies, some works<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref>, <xref rid=\"acm270385-bib-0011\" ref-type=\"bibr\">\n<sup>11</sup>\n</xref>, <xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref> have introduced them into segmentation models to fully extract global information from images. For example, SwinUNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref> employs Swin Transformer<xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref> as the backbone network to construct a U&#8208;shaped medical image segmentation network composed entirely of&#160;ViT.</p><p>These pure&#8208;ViT&#8208;based segmentation methods can fully extract global information from images. However, limited by the computational mechanism of self&#8208;attention, ViTs typically incur a quadratic computational cost in terms of token quantity, while also lacking the ability to preserve local details during feature extraction. Thus, some works combining CNN and ViT within a single model to overcome this issue. For instance, TransUNet<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref> and TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref> integrates ViT&#8208;R50<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref> as the backbone network for feature extraction, enabling the network to simultaneously capture global and local features. Nevertheless, constrained by the high computational cost of ViTs, these methods can only incorporate ViT modules in regions with smaller feature maps. Studies in non&#8208;local<xref rid=\"acm270385-bib-0014\" ref-type=\"bibr\">\n<sup>14</sup>\n</xref> have shown that integrating the global information module into the model's shallow layers&#8212;where feature maps typically have larger spatial dimensions&#8212;yields greater performance gains. Moreover, inserting global feature modeling modules at multiple stages can progressively yield better results. In addition, due to the fixed window size and token dimension, existing advanced vision transformer modules<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref>, <xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref> often lack internal multi&#8208;scale information, which is critical for the accuracy of segmentation results.<xref rid=\"acm270385-bib-0015\" ref-type=\"bibr\">\n<sup>15</sup>\n</xref>, <xref rid=\"acm270385-bib-0016\" ref-type=\"bibr\">\n<sup>16</sup>\n</xref>\n</p><p>To address above problems of existing ViT modules,<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref>, <xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref>, <xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref> we propose a novel architecture named HCViT&#8208;Net that systematically integrates both CNN and ViT components across all stages of the model. Figure&#160;<xref rid=\"acm270385-fig-0002\" ref-type=\"fig\">2</xref> illustrates the architectural distinctions between our proposed method and existing pure CNN&#8208;based, pure ViT&#8208;based, and hybrid CNN&#8208;ViT models. Our method uniquely enables comprehensive learning of both local and global features at every stage of the model, addressing limitations of prior works that often prioritize one type of information over the other. We design a lightweight self&#8208;attention approach featuring multi&#8208;scale key&#8208;value (K&#8208;V) reduction. This solution not only significantly reduces the computational overhead of standard self&#8208;attention (maintaining our HCViT&#8208;Net's advantage in model complexity), but also constructs intrinsic multi&#8208;scale representations within ViT, thereby compensating for the absence of internal multi&#8208;scale information in competitive ViT&#160;architectures.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0002\" orientation=\"portrait\"><label>FIGURE 2</label><caption><p>Illustration of the differences in model architecture between previous methods and ours. We combine CNN and ViT at all stages of the model to fully exploit the advantages of these two computing paradigms. ViT, vision transformer.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-3\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g001.jpg\"/></fig><p>Additionally, in the skip connections of UNet<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref> architecture, the shallow features generated by the first encoder layer preserve rich spatial details but lack sufficient semantic representation, while the corresponding deep decoder features contain abundant semantic information at the cost of spatial detail loss due to repeated downsampling. This inherent semantic gap leads to significant performance degradation when using conventional fusion methods direct summation or channel concatenation. To address this issue, we propose a feature refinement module named wavelet&#8208;attention refinement module (WARM), which can adaptively establishes cross&#8208;level feature correlations, effectively bridging the semantic gap and consequently improving segmentation&#160;accuracy.</p><p>Consequently, the contributions in this work can be summarized as follows:\n<list list-type=\"simple\" id=\"acm270385-list-0001\"><list-item id=\"acm270385-li-0001\"><label>1.</label><p>To combine the local detail extraction capabilities of CNNs with the global context modeling strengths of ViTs, we introduce HCViT&#8208;Net, a novel hybrid CNN&#8211;ViT architecture that integrates both CNNs and ViTs at all stages of the model&#8212;in contrast to existing approaches which typically insert ViT modules only in a few stages&#8212;thereby enabling the full extraction of local and global contextual information.</p></list-item><list-item id=\"acm270385-li-0002\"><label>2.</label><p>To address the high computational cost and the lack of internal multi&#8208;scale information in existing ViT modules, we propose the multi&#8208;scale query transformer (MSQFormer), which compresses key and value tensors at multiple resolutions, thereby enabling global and multi&#8208;scale self&#8208;attention with significantly reduced computational complexity. This characteristic is pivotal in maintaining the low computational overhead of our proposed HCViT&#8208;Net.</p></list-item><list-item id=\"acm270385-li-0003\"><label>3.</label><p>To reduce the semantic gap between early encoder features and late decoder features, we propose the wavelet&#8208;attention refinement module (WARM), which based on wavelet transform attention. WARM decomposes the encoder's feature maps using wavelet transform and then leverages high&#8208;level semantic information from the decoder to guide the extraction of effective low&#8208;level features, thereby improving the final segmentation accuracy.</p></list-item><list-item id=\"acm270385-li-0004\"><label>4.</label><p>We conduct extensive evaluations on ISIC 2017 and ISIC 2018 benchmarks, demonstrating that HCViT&#8208;Net consistently outperforms pure CNNs, pure ViTs, and existing hybrid methods in segmentation accuracy while maintaining competitive model size and computational cost. This dual achievement underscores the model's significant potential for seamless integration into clinical workflows, offering a powerful tool to assist dermatologists in performing more timely and precise diagnostic assessments.</p></list-item></list>\n</p></sec><sec id=\"acm270385-sec-0070\"><label>2</label><title>RELATED WORKS</title><sec id=\"acm270385-sec-0080\"><label>2.1</label><title>Pure CNN&#8208;based segmentation methods</title><p>Since the advent of fully convolutional networks (FCN),<xref rid=\"acm270385-bib-0005\" ref-type=\"bibr\">\n<sup>5</sup>\n</xref> end&#8208;to&#8208;end convolutional models have become the cornerstone of modern semantic segmentation. The DeepLab<xref rid=\"acm270385-bib-0006\" ref-type=\"bibr\">\n<sup>6</sup>\n</xref> family of models enhances contextual modeling by introducing atrous (dilated) convolutions, which enlarge receptive fields without downsampling, and by employing atrous spatial pyramid pooling (ASPP) for multi&#8208;scale context aggregation. In the medical imaging domain, the U&#8208;Net<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref>, <xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref> architecture addresses challenges of limited data and the need for precise localization through a symmetric encoder&#8211;decoder structure with skip connections. This design directly shuttles encoder features to the decoder, preserving spatial details that might be lost during downsampling while enabling hierarchical feature&#160;extraction.</p><p>These CNN&#8208;based segmentation models<xref rid=\"acm270385-bib-0005\" ref-type=\"bibr\">\n<sup>5</sup>\n</xref>, <xref rid=\"acm270385-bib-0006\" ref-type=\"bibr\">\n<sup>6</sup>\n</xref>, <xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref>, <xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref> have achieved remarkable success. However, limited by their local receptive fields, standard convolutional operations struggle to model long&#8208;range&#160;dependencies.</p></sec><sec id=\"acm270385-sec-0090\"><label>2.2</label><title>Pure ViT based segmentation methods</title><p>The advent of the vision transformer (ViT)<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref> has spurred researchers to explore extending its powerful global modeling capabilities to dense prediction tasks. ViT's core concept involves dividing an image into a sequence of non&#8208;overlapping patches, successfully adapting the standard Transformer<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref> architecture for image classification. In the realm of semantic segmentation, early applications of ViT often treated it as a pixel&#8208;wise encoder. For instance, the SETR<xref rid=\"acm270385-bib-0018\" ref-type=\"bibr\">\n<sup>18</sup>\n</xref> model utilizes a ViT encoder followed by upsampling and refinement stages to recover high&#8208;resolution masks. Similarly, in medical image segmentation, Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref> adeptly employs the Swin Transformer<xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref> to construct a U&#8208;shaped segmentation network, achieving leading segmentation&#160;results.</p><p>These pure ViT&#8208;based methods can effectively excavate global dependencies in images, but are constrained by the limitations of existing ViT modules. They fail to preserve adequate local details during feature extraction,<xref rid=\"acm270385-bib-0019\" ref-type=\"bibr\">\n<sup>19</sup>\n</xref> and additionally lack multi&#8208;scale query capability within their internal&#160;modules.</p></sec><sec id=\"acm270385-sec-0100\"><label>2.3</label><title>Hybrid CNN and ViT segmentation methods</title><p>In medical image segmentation, hybrid CNN&#8211;ViT architectures have emerged to leverage the local feature extraction of convolutions alongside the global context modeling of self&#8208;attention. TransUNet<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref> pioneered this line by embedding a ViT module in the bottleneck of a U&#8208;shaped CNN, allowing long&#8208;range dependencies to guide upsampling and yielding strong performance on abdominal and cardiac CT tasks. Building on this idea, CoTr<xref rid=\"acm270385-bib-0020\" ref-type=\"bibr\">\n<sup>20</sup>\n</xref> introduced a deformable self&#8208;attention mechanism that sparsely attends to salient medical features, reducing both memory footprint and computational overhead while preserving fine&#8208;grained&#160;structures.</p><p>These methods integrate CNNs and ViTs within one model, but due to the high computational cost of self&#8208;attention mechanisms, these ViT modules can typically only be inserted at the deeper stages of the model. This inherently limits the model's ability to perform global information modeling at other stages and consequently restricts further improvements in segmentation&#160;accuracy.</p></sec></sec><sec id=\"acm270385-sec-0110\"><label>3</label><title>METHODS</title><sec id=\"acm270385-sec-0120\"><label>3.1</label><title>Overview</title><p>Figure&#160;<xref rid=\"acm270385-fig-0003\" ref-type=\"fig\">3</xref> illustrates the detailed architecture of our proposed model. The input image is progressively downsampled by the encoder and then restored to its original resolution by the decoder. Our baseline is a U&#8208;shaped network composed solely of CNN blocks. To enrich each stage with global context, we insert the proposed multi&#8208;scale query transformer (MSQFormer) block immediately after every CNN block. Multiple skip connections are utilized to pass low&#8208;level details from the encoder to the decoder. For the two smaller&#8208;sized skip connections, features are fused with the upsampled decoder output via element&#8208;wise addition. Notably, the largest feature map from the earliest encoder stage contains minimal semantic content and significant noise. We first refine these features with our proposed wavelet&#8208;attention refinement module (WARM), and then fuse them with the upsampled decoder output using the same additive&#160;method.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0003\" orientation=\"portrait\"><label>FIGURE 3</label><caption><p>The architecture of the proposed HCViT&#8208;Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-5\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g002.jpg\"/></fig></sec><sec id=\"acm270385-sec-0130\"><label>3.2</label><title>CNN block</title><p>Driven by considerations for computational complexity and parameter efficiency, we built our custom CNN block drawing inspiration from the design philosophy of the classic lightweight network, MobileNet.<xref rid=\"acm270385-bib-0021\" ref-type=\"bibr\">\n<sup>21</sup>\n</xref> As shown in Figure&#160;<xref rid=\"acm270385-fig-0004\" ref-type=\"fig\">4</xref>, this module consists of a <mml:math id=\"jats-math-1\" display=\"inline\"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math> depthwise convolution (DWConv) layer followed by a <mml:math id=\"jats-math-2\" display=\"inline\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math> standard convolution layer. The <mml:math id=\"jats-math-3\" display=\"inline\"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math> depthwise convolution captures spatial contextual information across a wide receptive field at a very low computational cost, while the <mml:math id=\"jats-math-4\" display=\"inline\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math> standard convolution performs information interaction and recombination across the channel dimension to generate new, more expressive feature representations. Furthermore, each convolution layer is succeeded by a batch normalization (BN) layer and a rectified linear unit (ReLU) activation&#160;function.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0004\" orientation=\"portrait\"><label>FIGURE 4</label><caption><p>The architecture of the CNN Block in HCViT&#8208;Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-7\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g006.jpg\"/></fig><p>Let the input feature map be <mml:math id=\"jats-math-5\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>, where <mml:math id=\"jats-math-6\" display=\"inline\"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math> is the batch size, <mml:math id=\"jats-math-7\" display=\"inline\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math> is the channel dimension, <mml:math id=\"jats-math-8\" display=\"inline\"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math> is the height and <mml:math id=\"jats-math-9\" display=\"inline\"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math> is the width, the above steps can be represented by formulas as:\n<disp-formula id=\"acm270385-disp-0001\"><label>(1)</label><mml:math id=\"jats-math-10\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">H</mml:mi><mml:mn mathvariant=\"bold\">1</mml:mn></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mtext>BN</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mtext>DWConv2D</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0002\"><label>(2)</label><mml:math id=\"jats-math-11\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">H</mml:mi><mml:mn mathvariant=\"bold\">2</mml:mn></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mtext>BN</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mtext>Conv2D</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">H</mml:mi><mml:mn mathvariant=\"bold\">1</mml:mn></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0003\"><label>(3)</label><mml:math id=\"jats-math-12\" display=\"block\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo linebreak=\"goodbreak\">+</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">H</mml:mi><mml:mn mathvariant=\"bold\">2</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>where ReLU is the rectified linear unit activation function and BN is the batch&#160;norm.</p></sec><sec id=\"acm270385-sec-0140\"><label>3.3</label><title>Multi&#8208;scale query transformer block</title><p>To effectively segment dermatological lesions, which inherently exhibit significant variations in size and shape, the model must be capable of processing contextual information from multiple resolutions simultaneously.<xref rid=\"acm270385-bib-0015\" ref-type=\"bibr\">\n<sup>15</sup>\n</xref>, <xref rid=\"acm270385-bib-0016\" ref-type=\"bibr\">\n<sup>16</sup>\n</xref> Therefore, the motivation for our multi&#8208;scale key&#8208;value compression strategy is twofold: on one hand, it drastically reduces the computational overhead of the self&#8208;attention<xref rid=\"acm270385-bib-0008\" ref-type=\"bibr\">\n<sup>8</sup>\n</xref> mechanism, making its deployment across all network stages feasible. On the other hand, and more importantly, it empowers a single query vector to simultaneously interact with contextual information aggregated from different spatial scales. This capability is clinically crucial, as it allows the model to be equally adept at capturing the fine details of small lesions and understanding the global structure of large, sprawling&#160;ones.</p><p>Figure&#160;<xref rid=\"acm270385-fig-0005\" ref-type=\"fig\">5</xref> illustrates the computational steps of our proposed MSQFormer module. Specifically, let the input feature map be <mml:math id=\"jats-math-13\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>, where <mml:math id=\"jats-math-14\" display=\"inline\"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math> is the batch size, <mml:math id=\"jats-math-15\" display=\"inline\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math> is the channel dimension, <mml:math id=\"jats-math-16\" display=\"inline\"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math> is the height and <mml:math id=\"jats-math-17\" display=\"inline\"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math> is the width. We reshape the input tensor to <mml:math id=\"jats-math-18\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">q</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> first. Here, <mml:math id=\"jats-math-19\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math> is the number of image tokens. Then, the query matrix is generated via linear projection and reshaping:\n<disp-formula id=\"acm270385-disp-0004\"><label>(4)</label><mml:math id=\"jats-math-20\" display=\"block\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>Reshape</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:msub><mml:mtext>Linear</mml:mtext><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">q</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where, <mml:math id=\"jats-math-21\" display=\"inline\"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math> is the number of attention heads and <mml:math id=\"jats-math-22\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math> is the dimension of each head. It should be noted that the query (Q) maintains the full resolution of the input feature tensor <mml:math id=\"jats-math-23\" display=\"inline\"><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:math>.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0005\" orientation=\"portrait\"><label>FIGURE 5</label><caption><p>The architecture of the proposed multi&#8208;scale query attention block.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-9\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g008.jpg\"/></fig><p>Then, we generate multi&#8208;scale key&#8208;value matrices from the input <mml:math id=\"jats-math-24\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>. Here, we generate spatial downsampling key&#8208;value pairs at three (1/2, 1/4, and 1/8) different resolutions:\n<disp-formula id=\"acm270385-disp-0005\"><label>(5)</label><mml:math id=\"jats-math-25\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mtext>Conv2D</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0006\"><label>(6)</label><mml:math id=\"jats-math-26\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mtext>Conv2D</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0007\"><label>(7)</label><mml:math id=\"jats-math-27\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mtext>Conv2D</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0008\"><label>(8)</label><mml:math id=\"jats-math-28\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak=\"goodbreak\">=</mml:mo><mml:mtext>Split</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:msub><mml:mtext>Linear</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>GELU</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>LN</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0009\"><label>(9)</label><mml:math id=\"jats-math-29\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak=\"goodbreak\">=</mml:mo><mml:mtext>Split</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:msub><mml:mtext>Linear</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>GELU</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>LN</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0010\"><label>(10)</label><mml:math id=\"jats-math-30\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak=\"goodbreak\">=</mml:mo><mml:mtext>Split</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:msub><mml:mtext>Linear</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>GELU</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mtext>LN</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mfenced></mml:mfenced></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>h</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <mml:math id=\"jats-math-31\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:mrow></mml:math>, <mml:math id=\"jats-math-32\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the stride of path <mml:math id=\"jats-math-33\" display=\"inline\"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>. GELU is Gaussian error linear unit activation function and LN is layer&#160;norm.</p><p>To realize local feature enhancement, each value matrix is augmented with depthwise separable convolution:\n<disp-formula id=\"acm270385-disp-0011\"><label>(11)</label><mml:math id=\"jats-math-34\" display=\"block\"><mml:mrow><mml:mrow><mml:msub><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo>&#8764;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak=\"goodbreak\">+</mml:mo><mml:msub><mml:mtext>DWConv</mml:mtext><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width=\"1em\"/><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo stretchy=\"false\">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy=\"false\">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <mml:math id=\"jats-math-35\" display=\"inline\"><mml:mrow><mml:msub><mml:mtext>DWConv</mml:mtext><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math> is the depthwise convolution with kernel size 3<mml:math id=\"jats-math-36\" display=\"inline\"><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:math>3.</p><p>Then, we computed scaled dot&#8208;product attention for each scale path:\n<disp-formula id=\"acm270385-disp-0012\"><label>(12)</label><mml:math id=\"jats-math-37\" display=\"block\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msub><mml:mi mathvariant=\"bold\">SA</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mi>i</mml:mi><mml:mi>&#8868;</mml:mi></mml:msubsup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">SA</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo>&#8764;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where, <mml:math id=\"jats-math-38\" display=\"inline\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the self&#8208;attention weights of the path <mml:math id=\"jats-math-39\" display=\"inline\"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math> and <mml:math id=\"jats-math-40\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the contextual features of path <mml:math id=\"jats-math-41\" display=\"inline\"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>.</p><p>Finally, we concatenate outputs from all paths and project them to realize multi&#8208;scale feature fusion:\n<disp-formula id=\"acm270385-disp-0013\"><label>(13)</label><mml:math id=\"jats-math-42\" display=\"block\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>Concat</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0014\"><label>(14)</label><mml:math id=\"jats-math-43\" display=\"block\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">O</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>Dropout</mml:mtext><mml:mfenced close=\")\" open=\"(\" separators=\"\"><mml:msub><mml:mtext>Linear</mml:mtext><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold\">Z</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n</p><p>Previous Transformer models such as Swin Transformer<xref rid=\"acm270385-bib-0009\" ref-type=\"bibr\">\n<sup>9</sup>\n</xref> and PVT<xref rid=\"acm270385-bib-0022\" ref-type=\"bibr\">\n<sup>22</sup>\n</xref> capture multi&#8208;scale information implicitly through downsampling in a hierarchical feature pyramid, where fusion heavily relies on the encoder's resolution hierarchy. This approach overlooks the multi&#8208;scale nature of objects within a single attention layer, making these models sensitive to scale variations in real&#8208;world scenarios. The root cause lies in their attention design: each layer uses tokens with fixed receptive fields and uniform granularity, preventing simultaneous perception of multiple&#160;scales.</p><p>In contrast, the proposed MSQFormer introduces explicit cross&#8208;scale attention within each ViT block by means of learnable semantic queries. This mechanism decouples multi&#8208;scale fusion from the encoder hierarchy, enabling content&#8208;adaptive, sparse, and efficient aggregation of full&#8208;scale information rather than fixed&#8208;topology fusion across adjacent layers. Consequently, MSQFormer achieves stronger global&#8211;local coupling and greater robustness to scale variation, effectively preserving the structural integrity of large lesions while maintaining fine boundary details in small ones. As shown in Figure&#160;<xref rid=\"acm270385-fig-0006\" ref-type=\"fig\">6</xref>, the difference in modeling behavior across single self&#8208;attention layers is evident: (a) ViT employs global attention with uniform receptive fields, lacking explicit scale awareness. (b) Swin restricts attention to fixed local windows, limiting cross&#8208;scale interactions within one layer. (c) PVT encodes multi&#8208;scale features via hierarchical downsampling, with fusion occurring only across stages. (d) Ours (MSQFormer) performs explicit, content&#8208;adaptive cross&#8208;scale attention using learnable semantic queries, allowing joint modeling of global context and local detail within the same&#160;layer.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0006\" orientation=\"portrait\"><label>FIGURE 6</label><caption><p>Comparison of feature modeling within a single self&#8208;attention layer among ViT, Swin, PVT, and the proposed MSQFormer. (a)ViT): employs global self&#8208;attention with uniform receptive fields, lacking explicit scale awareness. (b)Swin: utilizes window&#8208;based local attention confined to fixed regions, thus limiting cross&#8208;scale interactions within one layer. (c)PVT: encodes multi&#8208;scale features through a hierarchical pyramid, where fusion occurs only across stages rather than inside individual attention layers. (d) Ours (MSQFormer): introduces learnable semantic queries to explicitly and adaptively aggregate information across multiple scales within a single attention layer, achieving enhanced global&#8211;local coupling.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-11\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g012.jpg\"/></fig></sec><sec id=\"acm270385-sec-0150\"><label>3.4</label><title>Wavelet&#8208;attention refinement module</title><p>Standard skip connections in U&#8208;Net<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref> architectures directly concatenate high&#8208;resolution, low&#8208;level features from the encoder with semantically rich, high&#8208;level features from the decoder. This creates a &#8220;semantic gap,&#8221; forcing the model to implicitly learn to suppress irrelevant textures and noise from the detailed encoder features. To address this challenge more explicitly, we introduce the wavelet&#8208;attention refinement module (WARM).</p><p>The core insight behind WARM is to first structurally disentangle the low&#8208;level features before fusing them. For this purpose, the discrete wavelet transform (DWT) is uniquely suited due to its ability to decompose a feature map into multiple sub&#8208;bands that represent different frequency components while preserving their spatial localization. This decomposition separates coarse, low&#8208;frequency structural information from fine&#8208;grained, high&#8208;frequency details (e.g., edges and textures). With features now organized into distinct sub&#8208;bands, we can leverage the high&#8208;level semantic context from the decoder as a precise guide. The subsequent attention mechanism can then selectively focus on the most task&#8208;relevant frequency bands at specific spatial locations, rather than contending with a single, convoluted feature map. This guided refinement process effectively bridges the semantic gap, leading to more accurate feature fusion and superior&#160;segmentation.</p><p>As shown in Figure&#160;<xref rid=\"acm270385-fig-0007\" ref-type=\"fig\">7</xref>, the WARM leverages high&#8208;level semantic information from the decoder to guide the frequency&#8208;domain refinement of low&#8208;level features, rather than performing a direct&#160;concatenation.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0007\" orientation=\"portrait\"><label>FIGURE 7</label><caption><p>The architecture of the proposed WARM. WARM, wavelength&#8208;attention refinement model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-13\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g011.jpg\"/></fig><p>Assuming both the input low&#8208;level feature map from the encoder, <mml:math id=\"jats-math-44\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">enc</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>, and the corresponding upsampled high&#8208;level feature map from the decoder, <mml:math id=\"jats-math-45\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">dec</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>. The encoder feature map <mml:math id=\"jats-math-46\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">enc</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> is passed through a DWT. This operation decomposes it into four sub&#8208;bands, each with half the spatial resolution: one low&#8208;frequency approximation sub&#8208;band, <mml:math id=\"jats-math-47\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math>, and three high&#8208;frequency detail sub&#8208;bands, <mml:math id=\"jats-math-48\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math>, <mml:math id=\"jats-math-49\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math>, and <mml:math id=\"jats-math-50\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math>. The resulting shape for each of these four sub&#8208;bands is <mml:math id=\"jats-math-51\" display=\"inline\"><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math>. The above steps can be represented by formulas as:\n<disp-formula id=\"acm270385-disp-0015\"><label>(15)</label><mml:math id=\"jats-math-52\" display=\"block\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>DWT</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">enc</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <mml:math id=\"jats-math-53\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math> represent the low&#8208;frequency, horizontal high&#8208;frequency, vertical high&#8208;frequency, and diagonal high&#8208;frequency sub&#8208;bands,&#160;respectively.</p><p>Concurrently, the decoder feature map <mml:math id=\"jats-math-54\" display=\"inline\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">dec</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi mathvariant=\"bold\">B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant=\"bold\">C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant=\"bold\">H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi mathvariant=\"bold\">W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> is downsampled via average pooling. This process matches its spatial dimensions to those of the sub&#8208;bands, resulting in a guidance signal <mml:math id=\"jats-math-55\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>. The above step can be represented by formulas as:\n<disp-formula id=\"acm270385-disp-0016\"><label>(16)</label><mml:math id=\"jats-math-56\" display=\"block\"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>AvgPool</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mi mathvariant=\"bold\">dec</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\n</p><p>We define the attention gate function as <mml:math id=\"jats-math-57\" display=\"inline\"><mml:mrow><mml:mi mathvariant=\"script\">A</mml:mi></mml:mrow></mml:math>. The guidance signal <mml:math id=\"jats-math-58\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> is then used to modulate each of the four sub&#8208;bands via separate attention gates. For an arbitrary input sub&#8208;band <mml:math id=\"jats-math-59\" display=\"inline\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mi mathvariant=\"bold\">sub</mml:mi></mml:msub></mml:mrow></mml:math> and the guidance signal <mml:math id=\"jats-math-60\" display=\"inline\"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:math>, the attention gate function <mml:math id=\"jats-math-61\" display=\"inline\"><mml:mrow><mml:mi mathvariant=\"script\">A</mml:mi></mml:mrow></mml:math> can be represented by formulas as:\n<disp-formula id=\"acm270385-disp-0017\"><label>(17)</label><mml:math id=\"jats-math-62\" display=\"block\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"script\">A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mi mathvariant=\"bold\">sub</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mi mathvariant=\"bold\">sub</mml:mi></mml:msub><mml:mo>&#8857;</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>&#968;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi mathvariant=\"bold\">x</mml:mi><mml:mi mathvariant=\"bold\">sub</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo linebreak=\"goodbreak\">+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <mml:math id=\"jats-math-63\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math>, <mml:math id=\"jats-math-64\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math>, and <mml:math id=\"jats-math-65\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>&#968;</mml:mi></mml:msub></mml:mrow></mml:math> are the learnable weights of <mml:math id=\"jats-math-66\" display=\"inline\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math> convolutional layers, ReLU is the rectified linear unit activation function, <mml:math id=\"jats-math-67\" display=\"inline\"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math> is the sigmoid function, and <mml:math id=\"jats-math-68\" display=\"inline\"><mml:mrow><mml:mo>&#8857;</mml:mo></mml:mrow></mml:math> denotes element&#8208;wise multiplication. For instance, the <mml:math id=\"jats-math-69\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> sub&#8208;band and the guidance signal <mml:math id=\"jats-math-70\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math> are fed into the <mml:math id=\"jats-math-71\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math> gate. The gate produces an attention map of shape <mml:math id=\"jats-math-72\" display=\"inline\"><mml:mrow><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math>, which is then applied element&#8208;wise to the ll sub&#8208;band. This process does not change the sub&#8208;band's shape. The output, <mml:math id=\"jats-math-73\" display=\"inline\"><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math>, and similarly <mml:math id=\"jats-math-74\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>, <mml:math id=\"jats-math-75\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>, and <mml:math id=\"jats-math-76\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>, all retain the shape <mml:math id=\"jats-math-77\" display=\"inline\"><mml:mrow><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math>.</p><p>Finally, these four refined sub&#8208;bands are recombined using an inverse discrete wavelet transform (IDWT). This transform merges the frequency components, restoring the original spatial resolution. The final output is a refined feature map, <mml:math id=\"jats-math-78\" display=\"inline\"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math> refined, with a shape of <mml:math id=\"jats-math-79\" display=\"inline\"><mml:mrow><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math>, identical to the original input. The above step can be represented by formulas as:\n<disp-formula id=\"acm270385-disp-0018\"><label>(18)</label><mml:math id=\"jats-math-80\" display=\"block\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mtext>IDWT</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd columnalign=\"left\"><mml:mrow><mml:mspace width=\"1em\"/><mml:mo linebreak=\"badbreak\">&#215;</mml:mo><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>\n</p><p>Prior boundary refinement methods<xref rid=\"acm270385-bib-0023\" ref-type=\"bibr\">\n<sup>23</sup>\n</xref>, <xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> often use spatial gradients or edge maps to impose guidance losses on model outputs, which tends to confuse true boundaries with high&#8208;frequency artifacts (e.g., hair, skin texture, illumination seams). Our WARM differs from existing methods: we decompose features in the wavelet domain into (LL/LH/HL/HH) subbands and gate them with decoding semantics, thereby enhancing only the high&#8208;frequency subbands related to true boundaries while suppressing structured noise (such as hair and skin texture), before transforming back to the spatial domain. This subband&#8208;specific, semantics&#8208;guided refinement mechanism can significantly sharpen edges without amplifying pseudo&#8208;contours and complements MSQFormer's global modeling. Through this sequence, the WARM effectively purifies the low&#8208;level features, creating a representation that is both spatially precise and semantically informed, thereby bridging the semantic gap and significantly improve the model's segmentation accuracy at lesion&#160;boundaries.</p></sec></sec><sec id=\"acm270385-sec-0160\"><label>4</label><title>EXPERIMENT SETTINGS</title><sec id=\"acm270385-sec-0170\"><label>4.1</label><title>Dataset</title><p>To evaluate the effectiveness of our method, we conducted experiments on two publicly available benchmarks from the International skin imaging collaboration challenge: ISIC 2017, which comprises 2&#160;150 dermoscopic images with corresponding segmentation masks, and ISIC 2018, which comprises 2&#160;694 labeled&#160;images.</p><p>To ensure a fair comparison, we adopted the same data initialization protocol as prior works MALUNet<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref> and BDFormer.<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> We split each dataset into 70% for training and 30% for testing. Concretely, in ISIC 2017 we used 1500 images for model training and 650 for evaluation, while in ISIC 2018 we allocated 1&#160;886 images to the training set and 808 to the test&#160;set.</p></sec><sec id=\"acm270385-sec-0180\"><label>4.2</label><title>Evaluation metrics</title><p>Following the prior works MALUNet<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref> and BDFormer,<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> we adopt mean intersection over union (mIoU), dice similarity score (DSC), accuracy (Acc), sensitivity (Sen), and specificity (Spe) as indicators to measure segmentation performances. The mIoU, DSC, Acc, Sen, and Spe are defined as:\n<disp-formula id=\"acm270385-disp-0019\"><label>(19)</label><mml:math id=\"jats-math-81\" display=\"block\"><mml:mrow><mml:mrow><mml:mtext>mIoU</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0020\"><label>(20)</label><mml:math id=\"jats-math-82\" display=\"block\"><mml:mrow><mml:mrow><mml:mtext>DSC</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0021\"><label>(21)</label><mml:math id=\"jats-math-83\" display=\"block\"><mml:mrow><mml:mrow><mml:mtext>Acc</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0022\"><label>(22)</label><mml:math id=\"jats-math-84\" display=\"block\"><mml:mrow><mml:mrow><mml:mtext>Sen</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>\n<disp-formula id=\"acm270385-disp-0023\"><label>(23)</label><mml:math id=\"jats-math-85\" display=\"block\"><mml:mrow><mml:mrow><mml:mtext>Spe</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic toggle=\"yes\">k</italic> is the number of target segmentation categories, TP is the true positive, TN is the true negative, FP is the false negative, and FN is the false negative pixel numbers of the&#160;result.</p><p>To further evaluate the model's computational efficiency, we employ Params, which denotes the number of model parameters, measured in millions (M), and floating point operations (GFLOPs) of the model as metrics. Both Params and GFLOPs are evaluated using an input size of 256 <mml:math id=\"jats-math-86\" display=\"inline\"><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:math>&#160;256.</p></sec><sec id=\"acm270385-sec-0190\"><label>4.3</label><title>Implement details</title><p>All experiments were conducted on a single NVIDIA GeForce RTX 2080Ti GPU. To ensure fair comparison, we adopt the data augmentation strategy from MALUNet,<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref> including vertical flipping, horizontal flipping, and random rotation. The cross&#8208;entropy (CE) loss and dice loss are employed as the loss function. Following MALUNet,<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref> we utilize the AdamW optimizer with an initial learning rate of 0.001, coupled with a cosine annealing scheduler configured with a 300&#8208;period cycle and minimum learning rate of 1e&#8208;5. Models were trained for 300 epochs with a batch size of&#160;2.</p></sec></sec><sec id=\"acm270385-sec-0200\"><label>5</label><title>RESULTS</title><sec id=\"acm270385-sec-0210\"><label>5.1</label><title>Comparison with competitive models</title><p>To validate the effectiveness of our proposed method, Tables&#160;<xref rid=\"acm270385-tbl-0001\" ref-type=\"table\">1</xref>, <xref rid=\"acm270385-tbl-0002\" ref-type=\"table\">2</xref>, <xref rid=\"acm270385-tbl-0003\" ref-type=\"table\">3</xref> present comparative results between our approach and current competitive (SOTA) methods on ISIC 2017<xref rid=\"acm270385-bib-0026\" ref-type=\"bibr\">\n<sup>26</sup>\n</xref> and ISIC 2018<xref rid=\"acm270385-bib-0027\" ref-type=\"bibr\">\n<sup>27</sup>\n</xref> datasets, including both accuracy metrics and model&#160;complexity.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0001\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 1</label><caption><p>Comparative experimental results on ISIC 2017 dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Year</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-87\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-88\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-89\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-90\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Spe</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-91\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Sen</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2015</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">80.07&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">88.38&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.67&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">98.04&#160;&#177;&#160;0.11</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">73.51&#160;&#177;&#160;0.19</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet++<xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">81.14&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">89.11&#160;&#177;&#160;0.13</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.97&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.81&#160;&#177;&#160;0.10</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">76.26&#160;&#177;&#160;0.17</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.57&#160;&#177;&#160;0.25</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.74&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.44&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.93&#160;&#177;&#160;0.11</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.56</bold>&#160;&#177;&#160;<bold>0.18</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.84&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.91&#160;&#177;&#160;0.15</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.61&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.38&#160;&#177;&#160;0.10</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.44&#160;&#177;&#160;0.16</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Trans&#8208;UNet<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.94&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.20&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.50&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.66&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.52&#160;&#177;&#160;0.17</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">MAL&#8208;UNet<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2022</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.67&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.41&#160;&#177;&#160;0.13</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.08&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.58&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.52&#160;&#177;&#160;0.18</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2022</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">81.79&#160;&#177;&#160;0.25</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">89.54&#160;&#177;&#160;0.15</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.01&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.90&#160;&#177;&#160;0.09</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">80.67&#160;&#177;&#160;0.16</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">DCSA&#8208;UNet<xref rid=\"acm270385-bib-0029\" ref-type=\"bibr\">\n<sup>29</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.83&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.51&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.15&#160;&#177;&#160;0.06</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.73&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.24&#160;&#177;&#160;0.17</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Xbound&#8208;Former<xref rid=\"acm270385-bib-0023\" ref-type=\"bibr\">\n<sup>23</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.55&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.34&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.01&#160;&#177;&#160;0.06</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.33&#160;&#177;&#160;0.09</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.23&#160;&#177;&#160;0.16</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Focal&#8208;UNETR<xref rid=\"acm270385-bib-0030\" ref-type=\"bibr\">\n<sup>30</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.97&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.6&#160;&#177;&#160;0.13</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.17&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.58&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.05&#160;&#177;&#160;0.18</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">MSCA&#8208;Net<xref rid=\"acm270385-bib-0031\" ref-type=\"bibr\">\n<sup>31</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.38&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.86&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.22&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.03&#160;&#177;&#160;0.09</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">86.88&#160;&#177;&#160;0.16</underline>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2024</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.68&#160;&#177;&#160;0.25</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.04&#160;&#177;&#160;0.15</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.47&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">98.02&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.70&#160;&#177;&#160;0.15</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">CSCA&#8208;UNet<xref rid=\"acm270385-bib-0033\" ref-type=\"bibr\">\n<sup>33</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2024</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.57&#160;&#177;&#160;0.26</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.98&#160;&#177;&#160;0.16</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.32&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.27&#160;&#177;&#160;0.11</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.34&#160;&#177;&#160;0.18</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2025</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">85.94&#160;&#177;&#160;0.22</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">92.21&#160;&#177;&#160;0.14</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">95.46&#160;&#177;&#160;0.07</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.40&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">86.51&#160;&#177;&#160;0.18</underline>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">HCViT&#8208;Net(Ours)</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.76</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.30</bold>&#160;&#177;&#160; <bold>0.12</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>96.20</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>98.60</bold>&#160;&#177;&#160;<bold>0.04</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.13&#160;&#177;&#160;0.13</td></tr></tbody></table><table-wrap-foot><fn id=\"acm270385-tbl1-note-0001\"><p>\n<italic toggle=\"yes\">Note</italic>: Bold is the best and underline is the second&#160;best.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap><table-wrap position=\"float\" id=\"acm270385-tbl-0002\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 2</label><caption><p>Comparative experimental results on ISIC 2018 dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Year</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-92\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-93\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-94\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-95\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Spe</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-96\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Sen</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2015</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">80.72&#160;&#177;&#160;0.25</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">88.85&#160;&#177;&#160;0.28</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.58&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.56&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">79.82&#160;&#177;&#160;0.21</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet++<xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">82.11&#160;&#177;&#160;0.26</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">89.76&#160;&#177;&#160;0.31</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.27&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>97.73</bold>&#160;&#177;&#160;<bold>0.13</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">78.31&#160;&#177;&#160;0.23</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.52&#160;&#177;&#160;0.27</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.73&#160;&#177;&#160;0.29</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.21&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.05&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">82.69&#160;&#177;&#160;0.27</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.20&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.47&#160;&#177;&#160;0.27</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.61&#160;&#177;&#160;0.08</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">97.66&#160;&#177;&#160;0.16</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">80.53&#160;&#177;&#160;0.26</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">TransUNet<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.19&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.84&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.94&#160;&#177;&#160;0.09</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.52&#160;&#177;&#160;0.15</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.23&#160;&#177;&#160;0.27</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">MALUNet<xref rid=\"acm270385-bib-0025\" ref-type=\"bibr\">\n<sup>25</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2022</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.97&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.71&#160;&#177;&#160;0.26</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.85&#160;&#177;&#160;0.05</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.51&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.89&#160;&#177;&#160;0.27</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2022</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.11&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.42&#160;&#177;&#160;0.27</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">94.55&#160;&#177;&#160;0.09</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.44&#160;&#177;&#160;0.16</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">81.17&#160;&#177;&#160;0.25</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">DCSAU&#8208;Net<xref rid=\"acm270385-bib-0029\" ref-type=\"bibr\">\n<sup>29</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.57&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.46&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.65&#160;&#177;&#160;0.06</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.28&#160;&#177;&#160;0.11</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.79&#160;&#177;&#160;0.23</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Xbound&#8208;Former<xref rid=\"acm270385-bib-0023\" ref-type=\"bibr\">\n<sup>23</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.12&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.80&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.91&#160;&#177;&#160;0.06</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.45&#160;&#177;&#160;0.10</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.29&#160;&#177;&#160;0.21</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">FocalUNETR<xref rid=\"acm270385-bib-0030\" ref-type=\"bibr\">\n<sup>30</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">84.97&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.71&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.82&#160;&#177;&#160;0.06</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.28&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.50&#160;&#177;&#160;0.27</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">MSCA&#8208;Net<xref rid=\"acm270385-bib-0031\" ref-type=\"bibr\">\n<sup>31</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2023</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.24&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.87&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.98&#160;&#177;&#160;0.05</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.65&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.99&#160;&#177;&#160;0.21</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2024</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.66&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.13&#160;&#177;&#160;0.26</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.14&#160;&#177;&#160;0.05</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.49&#160;&#177;&#160;0.12</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">87.08&#160;&#177;&#160;0.27</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2025</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">86.28&#160;&#177;&#160;0.24</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">92.51&#160;&#177;&#160;0.22</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.35&#160;&#177;&#160;0.07</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">96.10&#160;&#177;&#160;0.13</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>89.14</bold>&#160;&#177;&#160;<bold>0.22</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">HCViT&#8208;Net(Ours)</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.45</bold>&#160;&#177;&#160;<bold>0.18</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.19</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>94.95</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">97.26&#160;&#177;&#160;0.09</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">88.05&#160;&#177;&#160;0.21</underline>\n</td></tr></tbody></table><table-wrap-foot><fn id=\"acm270385-tbl2-note-0001\"><p>\n<italic toggle=\"yes\">Note</italic>: Bold is the best and underline is the second&#160;best.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap><table-wrap position=\"float\" id=\"acm270385-tbl-0003\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 3</label><caption><p>Comparison of Params, FLOPs, and Inference Time (IT) at Pi5.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Year</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-97\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Params</mml:mi><mml:mo>&#8595;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-98\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>FLOPs</mml:mi><mml:mo>&#8595;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">IT@Pi5<mml:math id=\"jats-math-99\" display=\"inline\"><mml:mrow><mml:mo>&#8595;</mml:mo></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2015</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">31.04M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">48.33G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">5.0s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">UNet++<xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">36.62M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">138.37G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">14.3s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">23.90M</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">11.99G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">1.2s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">31.87M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">12.65G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2.3s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">TransUNet<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2021</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">105.32M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">38.55G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">4.8s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2022</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">27.17M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">9.42G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">1.3s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2024</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">29.65M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<underline underline-style=\"single\">8.42G</underline>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">0.9s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">CSCA&#8208;UNet<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2024</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">35.28M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">11.94G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">1.3s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">2025</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">103.83M</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">54.31G</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">5.6s</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">HCViT&#8208;Net(Ours)</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">&#8212;</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>5.76M</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>7.51G</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>0.8s</bold>\n</td></tr></tbody></table><table-wrap-foot><fn id=\"acm270385-tbl3-note-0001\"><p>\n<italic toggle=\"yes\">Note</italic>: Bold is the best and underline is the second&#160;best.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap><sec id=\"acm270385-sec-0220\"><label>5.1.1</label><title>Results on ISIC 2017 dataset</title><p>As evidenced by the comprehensive experimental results on the ISIC 2017 dataset presented in Table&#160;<xref rid=\"acm270385-tbl-0001\" ref-type=\"table\">1</xref>, our proposed HCViT&#8208;Net establishes new SOTA performance across critical evaluation metrics such as mIoU, DSC, Acc, and Spe when compared to 14 prominent medical image segmentation models spanning a decade of research (2015&#8211;2025). HCViT&#8208;Net achieves a breakthrough mIoU of 87.76%, surpassing the previous best performer BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> (85.94%) by a margin of 1.82%. In terms of DSC, our model attains 93.30%, exceeding BDFormer's<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> best result of 92.21% by 1.09% improvement. It is noteworthy that Table&#160;<xref rid=\"acm270385-tbl-0003\" ref-type=\"table\">3</xref> presents a comparison of complexity between our proposed model and other models. Compared with the previous SOTA method BDFormer,<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> our model uses only 5.5% of its parameters (5.76M vs. 103.83M) and 13.8% of its computational cost (7.51G vs. 54.31G). The exceptional results highlight HCViT&#8208;Net's significant advancements over diverse architectural paradigms. Our approach outperforms ViT&#8208;based models such as Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref> (+5.97% mIoU) and TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref> (+3.92% mIoU), while also exceeding attention&#8208;enhanced CNN variants like SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref> (+4.19% mIoU). Furthermore, HCViT&#8208;Net demonstrates clear superiority over the latest innovations in the field, outperforming I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref> by 2.08%. Particularly noteworthy is our model's exceptional boundary segmentation capability, evidenced by the 3.21% mIoU advantage over the recent boundary&#8208;focused method Xbound&#8208;Former.<xref rid=\"acm270385-bib-0023\" ref-type=\"bibr\">\n<sup>23</sup>\n</xref>\n</p></sec><sec id=\"acm270385-sec-0230\"><label>5.1.2</label><title>Results on ISIC 2018 dataset</title><p>As demonstrated by the extensive comparative analysis in Table&#160;<xref rid=\"acm270385-tbl-0002\" ref-type=\"table\">2</xref>, our proposed HCViT&#8208;Net establishes new SOTA performance on the ISIC 2018 dataset, surpassing 16 leading medical image segmentation models spanning 2015&#8211;2025. The model achieves a breakthrough mIoU of 87.45%, improving upon the previous best performer BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> (86.28%) by a significant margin of 1.17%. In terms of DSC, HCViT&#8208;Net reaches 93.19%, exceeding BDFormer's<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> 92.51% by 0.68%. While BDFormer<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref> achieves the highest sensitivity (89.14%), HCViT&#8208;Net delivers a competitive second&#8208;best sensitivity of 88.05% while maintaining strong specificity (97.26%)&#8212;demonstrating superior clinical balance compared to models like UNet++<xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref> which achieved 97.73% specificity but only 78.31% sensitivity. The results highlight HCViT&#8208;Net's dominance across diverse architectural paradigms. Our approach shows remarkable gains over transformer&#8208;based models, exceeding Swin&#8208;UNet<xref rid=\"acm270385-bib-0010\" ref-type=\"bibr\">\n<sup>10</sup>\n</xref> by 4.34% in mIoU and TransFuse<xref rid=\"acm270385-bib-0013\" ref-type=\"bibr\">\n<sup>13</sup>\n</xref> by 4.25%. It also outperforms CNN innovations SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref> (+3.93%). Against recent competitive methods, HCViT&#8208;Net surpasses 2024's I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref> by 1.79% in mIoU, while achieving a 2.33% advantage over boundary&#8208;focused Xbound&#8208;Former.<xref rid=\"acm270385-bib-0023\" ref-type=\"bibr\">\n<sup>23</sup>\n</xref> These results establish HCViT&#8208;Net as the new reference architecture for skin lesion segmentation, achieving unprecedented harmony between localization precision, diagnostic reliability, and clinical safety through its advanced feature learning&#160;capabilities.</p></sec><sec id=\"acm270385-sec-0240\"><label>5.1.3</label><title>Comparison of Params, FLOPs, and Inference Time</title><p>As shown in Table&#160;<xref rid=\"acm270385-tbl-0003\" ref-type=\"table\">3</xref>, compared to other SOTA methods, our HCViT&#8208;Net demonstrates significant advantages in terms of both model parameters, computational complexity and inference tTime (Deployed on Raspberry Pi5 with an input size of 256 <mml:math id=\"jats-math-100\" display=\"inline\"><mml:mrow><mml:mo>&#215;</mml:mo></mml:mrow></mml:math> 256). HCViT&#8208;Net achieves a remarkably low parameter count of 5.76M (bold, best), which is the lowest among all listed methods. This represents a 75.9% reduction compared to the previous most parameter&#8208;efficient model SANet<xref rid=\"acm270385-bib-0028\" ref-type=\"bibr\">\n<sup>28</sup>\n</xref> (underlined second best at 15.90M parameters). In terms of computational complexity, HCViT&#8208;Net also sets a new benchmark with FLOPs of only 7.51G (bold, best). Compared to the previous most efficient model I2U&#8208;Net<xref rid=\"acm270385-bib-0032\" ref-type=\"bibr\">\n<sup>32</sup>\n</xref> (underlined second best at 8.42G FLOPs), our model reduces computational complexity by 10.8%. These results confirm that HCViT&#8208;Net achieves unprecedented efficiency in lightweight design, delivering SOTA performance while requiring substantially fewer computational resources &#8212;making it exceptionally suitable for resource&#8208;constrained devices and real&#8208;time applications. In Table&#160;<xref rid=\"acm270385-tbl-0004\" ref-type=\"table\">4</xref>, we present a statistical analysis of the parameter counts and computational complexity (FLOPs) of each component in the model. As shown, the Encoder stage accounts for 79.89% of the total parameters (4.63M), with the MSFormer component contributing 73.43% (4.26M). This is primarily attributed to the high&#8208;dimensional feature representations in the 128&#8208;channel layers of the Encoder. In contrast, the Decoder stage comprises only 1.12M parameters (19.34%), reflecting the asymmetry between feature extraction and reconstruction in the encoder&#8211;decoder architecture. Although the WARM module has the fewest parameters (0.04M, 0.76%), it plays a crucial role in feature refinement and edge enhancement. In terms of FLOPs distribution, the computational load is relatively balanced between the Encoder and Decoder, accounting for 50.83% (3.82G) and 48.97% (3.68G), respectively. Notably, the MSFormer component contributes 49.21% and 47.58% of the computation within the Encoder and Decoder, respectively, confirming the central role of the multi&#8208;head self&#8208;attention mechanism in the model's computational core. The WARM module incurs minimal computational cost (0.03G, 0.34%), indicating that the wavelet&#8208;based attention refinement mechanism effectively enhances feature quality while maintaining computational efficiency. This distribution of parameters and computation aligns with the characteristics of medical image segmentation tasks&#8212;rich semantic feature extraction through the Encoder, precise pixel&#8208;level reconstruction in the Decoder, and improved feature fusion quality from the WARM module without significantly increasing computational&#160;overhead.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0004\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 4</label><caption><p>Parameter count and FLOPs analysis of HCViT&#8208;Net components.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Stage</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Parameters(M)</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Param ratio(%)</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">FLOPs(G)</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">FLOP ratio(%)</th></tr></thead><tbody><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Encoder</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">4.63</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">79.89</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">3.82</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">50.83</td></tr><tr><td style=\"padding-left:10%\" align=\"left\" rowspan=\"1\" colspan=\"1\">CNN</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.11</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">1.95</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.05</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.67</td></tr><tr><td style=\"padding-left:10%\" align=\"left\" rowspan=\"1\" colspan=\"1\">MSFormer</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">4.26</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">73.43</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">3.70</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">49.21</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Decoder</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">1.12</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">19.34</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">3.68</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">48.97</td></tr><tr><td style=\"padding-left:10%\" align=\"left\" rowspan=\"1\" colspan=\"1\">CNN</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.07</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">1.14</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.10</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">1.39</td></tr><tr><td style=\"padding-left:10%\" align=\"left\" rowspan=\"1\" colspan=\"1\">MSFormer</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">1.06</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">18.20</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">3.58</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">47.58</td></tr><tr><td style=\"padding-left:10%\" align=\"left\" rowspan=\"1\" colspan=\"1\">WARM</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.04</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.76</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.03</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">0.34</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Complete model</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">5.76</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">100.00</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">7.51</td><td align=\"char\" rowspan=\"1\" colspan=\"1\">100.00</td></tr></tbody></table><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap></sec></sec><sec id=\"acm270385-sec-0250\"><label>5.2</label><title>Ablation studies</title><sec id=\"acm270385-sec-0260\"><label>5.2.1</label><title>The effect of each component in HCViT&#8208;Net</title><p>Table&#160;<xref rid=\"acm270385-tbl-0005\" ref-type=\"table\">5</xref> presents a systematic ablation study evaluating the contribution of each component in HCViT&#8208;Net. On the ISIC 2017 dataset, the complete model achieves competitive performance (87.76% mIoU, 93.30% DSC, 96.20% Acc). Removal of the MSQFormer module causes the most significant degradation, reducing mIoU by 4.08%&#8211;83.68% and DSC by 2.54%, underscoring its fundamental role in feature extraction. Eliminating the CNN block results in a 1.86% mIoU reduction, indicating the importance of local feature modeling. The absence of the WARM module preserves relatively strong performance (86.63% mIoU). For ISIC 2018, similar trends emerge: The intact HCViT&#8208;Net maintains optimal metrics (87.45% mIoU, 93.19% DSC, 94.95% Acc), while MSQFormer removal again causes the most severe performance drop (4.18% mIoU reduction). Notably, eliminating the CNN block disproportionately affects accuracy (0.68% decrease to 94.27%), whereas WARM ablation primarily impacts DSC (0.57% reduction). These findings demonstrate that while MSQFormer blocks are essential for high performance, the WARM module's comprehensive stage&#8208;wise integration is crucial for maximizing segmentation&#160;accuracy.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0005\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 5</label><caption><p>Ablation study of the effect of each component in HCViT&#8208;Net.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-101\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-102\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-103\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td rowspan=\"4\" align=\"left\" colspan=\"1\">ISIC 2017</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">HCViT&#8208;Net(Ours)</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.76</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.30</bold>&#160;&#177;&#160;<bold>0.22</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>96.20</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o MSQFormer</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.68&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.76&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.92&#160;&#177;&#160;0.06</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o CNN</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.90&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.70&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.52&#160;&#177;&#160;0.05</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.63&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.92&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.79&#160;&#177;&#160;0.05</td></tr><tr><td rowspan=\"4\" align=\"left\" colspan=\"1\">ISIC 2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">HCViT&#8208;Net(Ours)</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.45</bold>&#160;&#177;&#160;<bold>0.18</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.19</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>94.95</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o MSQFormer</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.27&#160;&#177;&#160;0.19</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.55&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.27&#160;&#177;&#160;0.06</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o CNN</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.26&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.51&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.27&#160;&#177;&#160;0.06</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.47&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.62&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.44&#160;&#177;&#160;0.07</td></tr></tbody></table><table-wrap-foot><fn id=\"acm270385-tbl5-note-0001\"><p>Abbreviation: w/o,&#160;without.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap><p>In Figure&#160;<xref rid=\"acm270385-fig-0008\" ref-type=\"fig\">8</xref>, we verified the contribution of MSQFormer to modeling the global context through interpretable visualizations. Concretely, we removed MSQFormer from the model (w/o MSQFormer), compared it with the full model that includes this module (with MSQFormer) under identical training and evaluation protocols, and applied Grad&#8208;CAM to the final segmentation predictions to generate heatmaps. The accompanying figure&#160;presents several representative examples. From the visualizations, adding MSQFormer yields continuous and well&#8208;covered high responses across the entire lesion region in the Grad&#8208;CAM maps, attending to both the center and the periphery. In contrast, removing the module leads to fragmented and locally biased activations that are often attracted to high&#8208;contrast textures while neglecting low&#8208;contrast or boundary areas. Regarding boundaries, the model with MSQFormer is more stable at fuzzy or irregular edges, and its predicted contours align more closely with the annotations; conversely, without the module, the predictions tend to bleed into background or miss concave parts, and the boundary responses in the heatmaps become more discontinuous. This difference is especially pronounced for lesions with strong appearance heterogeneity: with MSQFormer, the model treats the lesion as a coherent whole, whereas without it, attention concentrates on the most salient subregions, resulting in incomplete masks. Mechanistically, MSQFormer introduces long&#8208;range dependencies explicitly via multi&#8208;scale query aggregation, aligning coarse&#8208;scale semantics with fine&#8208;scale boundary cues and re&#8208;weighting encoder features in a query&#8208;guided manner to suppress background clutter and unify lesion responses. This matches the Grad&#8208;CAM evidence of more uniform, coherent lesion activations and clearer, context&#8208;consistent boundaries. Consistent quantitative results further corroborate these findings: under the same settings, incorporating MSQFormer yields improvements in mIoU of approximately 4.08% on ISIC 2017 and 4.18% on ISIC 2018. In summary, the Grad&#8208;CAM comparisons clearly demonstrate that MSQFormer enables the model to effectively capture and exploit global context, thereby improving segmentation&#160;quality.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0008\" orientation=\"portrait\"><label>FIGURE 8</label><caption><p>Qualitative comparison of our full model (with MSQFormer) and its ablation variant (w/o MSQFormer) on four challenging cases. From left to right, the columns display the original dermoscopy image, the Grad&#8208;CAM attention heatmap, and the final segmentation results. In the &#8221;Results&#8221; column, the green contour represents the ground truth, while the blue contour indicates the model's prediction. MSQFormer, multi&#8208;scale query transformer.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-15\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g003.jpg\"/></fig><p>In Figure&#160;<xref rid=\"acm270385-fig-0009\" ref-type=\"fig\">9</xref>, we illustrate how the proposed WARM enhances lesion boundary precision by comparing the full model (with WARM) against an ablated variant (w/o WARM) under identical training and evaluation conditions, using Grad&#8208;CAM heatmaps for visualization. As shown, the model with WARM produces heatmaps with tighter, more continuous, and higher&#8208;intensity response bands that closely follow the true lesion contours, accurately capturing concave and irregular boundary structures. In contrast, removing WARM results in diffuse and unstable activations that often spill beyond the lesion edge or become over&#8208;smoothed, leading to boundary dilation, fragmentation, or omission, especially along low&#8208;contrast regions. Moreover, WARM effectively suppresses high&#8208;frequency background interference&#8212;such as hair, vascular textures, and illumination variations&#8212;keeping attention concentrated around genuine lesion boundaries. The w/o WARM model, however, tends to misfocus on these pseudo&#8208;edges, producing boundary expansion or spurious&#160;protrusions.</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0009\" orientation=\"portrait\"><label>FIGURE 9</label><caption><p>Qualitative comparison of our full model (with WARM) and its ablation variant (w/o WARM) on four challenging cases. From left to right, the columns display the original dermoscopy image, the Grad&#8208;CAM attention heatmap, and the final segmentation results. In the &#8221;Results&#8221; column, the green contour represents the ground truth, while the blue contour indicates the model's prediction. WARM, wavelength&#8208;guided attention refinement module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-17\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g005.jpg\"/></fig><p>The benefit is particularly evident in challenging cases with low boundary contrast or strong internal heterogeneity: WARM yields more coherent and stable edge responses across the lesion, resulting in segmentation masks that align more precisely with ground truth, whereas the ablated variant exhibits fragmented activations and weakened edge energy. Mechanistically, WARM decomposes encoder features into frequency subbands&#8212;low&#8208;frequency structural (LL) and high&#8208;frequency detail (LH/HL/HH) components&#8212;via wavelet transform, and selectively enhances high&#8208;frequency boundary cues using decoder&#8208;guided gating. It then reconstructs spatial features through an inverse wavelet transform, recovering sharper and more continuous lesion boundaries. This behavior aligns with the Grad&#8208;CAM observations of &#8220;ring&#8208;like activations following true contours, reduced background misactivations, and tighter predicted edges.&#8221; We further employed the HD95 metric to quantitatively evaluate the improvement in boundary refinement brought by the WARM module. As shown in Table&#160;<xref rid=\"acm270385-tbl-0006\" ref-type=\"table\">6</xref>, adding WARM yields an HD95 improvement of 1.22 on ISIC 2017 and 1.01 on ISIC 2018, confirming that WARM substantially enhances boundary focus and robustness, leading to more accurate and stable segmentation performance. WARM, wavelet&#8208;attention refinement module; ISIC, International skin imaging&#160;collaboration.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0006\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 6</label><caption><p>Quantitative comparison of lesion boundary segmentation accuracy (HD95) with WARM.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">HD95<mml:math id=\"jats-math-104\" display=\"inline\"><mml:mrow><mml:mo>&#8595;</mml:mo></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td rowspan=\"2\" align=\"left\" colspan=\"1\">ISIC 2017</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">with WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>16.01</bold>&#160;&#177;&#160;<bold>1.73</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">17.23&#160;&#177;&#160;1.69</td></tr><tr><td rowspan=\"2\" align=\"left\" colspan=\"1\">ISIC 2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">with WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>16.27</bold>&#160;&#177;&#160;<bold>1.67</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">w/o WARM</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">17.28&#160;&#177;&#160;1.77</td></tr></tbody></table><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap></sec><sec id=\"acm270385-sec-0270\"><label>5.2.2</label><title>The effect of appending position of MSQFormer block</title><p>As demonstrated in Table&#160;<xref rid=\"acm270385-tbl-0007\" ref-type=\"table\">7</xref>, appending MSQFormer across all stages consistently outperforms encoder&#8208;only and decoder&#8208;only strategies. On ISIC 2017, the full&#8208;stage approach (87.76% mIoU) substantially exceeds encoder&#8208;only (85.58%) and decoder&#8208;only (86.06%) configurations by 2.16% and 1.70% mIoU, respectively. The same superiority holds for ISIC 2018: the full&#8208;stage model (87.45% mIoU) stably outperforms both encoder&#8208;only (85.29%) and decoder&#8208;only (86.33%) variants with margins of 1.96% and 1.12%. As shown in Figure&#160;<xref rid=\"acm270385-fig-0011\" ref-type=\"fig\">11</xref>, the GradCAM<xref rid=\"acm270385-bib-0034\" ref-type=\"bibr\">\n<sup>34</sup>\n</xref> heatmaps of the CNN block and MSQFormer block in the first block of HCViT&#8208;Net reveal that the CNN, constrained by its local receptive field, struggles to focus on the entire target object during the early model stage. In contrast, the MSQFormer, leveraging its global receptive field, can precisely focus on the entire lesion area. This evidence confirms that the full&#8208;stage integration of global information into CNNs yields optimal performance improvements, validating the efficacy of our HCViT&#8208;Net architectural design.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0007\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 7</label><caption><p>Ablation study of the effect of appending position of MSQFormer block.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-105\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-106\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-107\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td rowspan=\"4\" align=\"left\" colspan=\"1\">ISIC 2017</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">All Stages</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.76</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.30</bold>&#160;&#177;&#160;<bold>0.22</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>96.20</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">No appending</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.68&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.76&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.92&#160;&#177;&#160;0.06</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Only Encoder</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.58&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.98&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.31&#160;&#177;&#160;0.05</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Only Decoder</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.06&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.27&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.59&#160;&#177;&#160;0.06</td></tr><tr><td rowspan=\"4\" align=\"left\" colspan=\"1\">ISIC 2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">All Stages</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.45</bold>&#160;&#177;&#160;<bold>0.18</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.19</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>94.95</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">No appending</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">83.27&#160;&#177;&#160;0.19</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">90.55&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.27&#160;&#177;&#160;0.06</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Only Encoder</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">85.29&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">91.91&#160;&#177;&#160;0.23</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">93.97&#160;&#177;&#160;0.07</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Only Decoder</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.33&#160;&#177;&#160;0.22</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.53&#160;&#177;&#160;0.24</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.44&#160;&#177;&#160;0.07</td></tr></tbody></table><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap></sec><sec id=\"acm270385-sec-0280\"><label>5.2.3</label><title>The efficacy of multi&#8208;scale query mechanisms in MSQFormer block</title><p>As shown in Table&#160;<xref rid=\"acm270385-tbl-0008\" ref-type=\"table\">8</xref>, on ISIC 2017 dataset, models with multi&#8208;scale queries achieved 87.47% mIoU, 93.13% DSC, and 96.01% Acc &#8211; outperforming single&#8208;scale baselines by 0.90%, 0.48%, and 0.08%, respectively. This advantage intensified on ISIC 2018, where multi&#8208;scale configuration delivered gains of 0.81% mIoU, 0.47% DSC, and 0.52% Acc. Building upon Figure&#160;<xref rid=\"acm270385-fig-0010\" ref-type=\"fig\">10</xref> and <xref rid=\"acm270385-fig-0011\" ref-type=\"fig\">11</xref>, we further visualize the GradCAM<xref rid=\"acm270385-bib-0034\" ref-type=\"bibr\">\n<sup>34</sup>\n</xref> heatmaps of multi&#8208;scale queries within the MSQFormer block, revealing differences in the model's attention granularity across scales. This demonstrates the superior efficacy of multi&#8208;scale queries in skin lesion&#160;segmentation.</p><table-wrap position=\"float\" id=\"acm270385-tbl-0008\" content-type=\"TABLE\" orientation=\"portrait\"><label>TABLE 8</label><caption><p>Ablation study of the efficacy of multi&#8208;scale query mechanisms in MSQFormer block.</p></caption><table frame=\"hsides\" rules=\"groups\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><thead><tr style=\"border-bottom:solid 1px #000000\"><th align=\"left\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-108\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>mIoU</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-109\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">\n<mml:math id=\"jats-math-110\" display=\"inline\"><mml:mrow><mml:mrow><mml:mi>Acc</mml:mi><mml:mo>&#8593;</mml:mo></mml:mrow></mml:mrow></mml:math>\n</th></tr></thead><tbody><tr><td rowspan=\"2\" align=\"left\" colspan=\"1\">ISIC 2017</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">Multi&#8208;scale Query</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.47</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.13</bold>&#160;&#177;&#160;<bold>0.12</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>96.01</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Single&#8208;scale Query</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.57&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.65&#160;&#177;&#160;0.14</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">95.93&#160;&#177;&#160;0.06</td></tr><tr><td rowspan=\"2\" align=\"left\" colspan=\"1\">ISIC 2018</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">Multi&#8208;scale Query</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>87.45</bold>&#160;&#177;&#160;<bold>0.18</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>93.19</bold>&#160;&#177;&#160;<bold>0.19</bold>\n</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>94.95</bold>&#160;&#177;&#160;<bold>0.05</bold>\n</td></tr><tr><td align=\"left\" rowspan=\"1\" colspan=\"1\">Single&#8208;scale Query</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">86.64&#160;&#177;&#160;0.21</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">92.72&#160;&#177;&#160;0.20</td><td align=\"left\" rowspan=\"1\" colspan=\"1\">94.43&#160;&#177;&#160;0.06</td></tr></tbody></table><permissions><copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder></permissions></table-wrap><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0010\" orientation=\"portrait\"><label>FIGURE 10</label><caption><p>The heatmaps of the CNN block and MSQFormer block in the first stage of HCViT&#8208;Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-19\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g009.jpg\"/></fig><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0011\" orientation=\"portrait\"><label>FIGURE 11</label><caption><p>The heatmaps of multi&#8208;scale queries within the MSQFormer block.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-21\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g007.jpg\"/></fig></sec></sec><sec id=\"acm270385-sec-0290\"><label>5.3</label><title>Visualization of prediction results</title><p>To further validate the effectiveness of our proposed approach, Figures&#160;<xref rid=\"acm270385-fig-0012\" ref-type=\"fig\">12</xref> and <xref rid=\"acm270385-fig-0013\" ref-type=\"fig\">13</xref> present a qualitative comparison of prediction results between our HCViT&#8208;Net and current SOTA models UNet,<xref rid=\"acm270385-bib-0007\" ref-type=\"bibr\">\n<sup>7</sup>\n</xref> UNet++,<xref rid=\"acm270385-bib-0017\" ref-type=\"bibr\">\n<sup>17</sup>\n</xref> TransUNet,<xref rid=\"acm270385-bib-0012\" ref-type=\"bibr\">\n<sup>12</sup>\n</xref> CSCAUNet,<xref rid=\"acm270385-bib-0033\" ref-type=\"bibr\">\n<sup>33</sup>\n</xref> and BDFormer.<xref rid=\"acm270385-bib-0024\" ref-type=\"bibr\">\n<sup>24</sup>\n</xref>\n</p><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0012\" orientation=\"portrait\"><label>FIGURE 12</label><caption><p>Qualitative comparisons between ours and other models on the ISIC 2017 dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-23\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g010.jpg\"/></fig><fig position=\"float\" fig-type=\"FIGURE\" id=\"acm270385-fig-0013\" orientation=\"portrait\"><label>FIGURE 13</label><caption><p>Qualitative comparisons between ours and other models on the ISIC 2018 dataset. ISIC, International skin imaging collaboration.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"anchor\" id=\"jats-graphic-25\" orientation=\"portrait\" xlink:href=\"ACM2-26-e70385-g013.jpg\"/></fig><p>The visual evidence in these two figures&#160;elaborates on these strengths: the first is our model's superior grasp of global dependencies, which significantly reduces misjudgments in large&#8208;scale areas. Concurrently, the second is that, by synergistically combining the strengths of ViT and CNN architectures, our HCViT&#8208;Net achieves enhanced detail processing capabilities compared to prior&#160;methods.</p></sec></sec><sec id=\"acm270385-sec-0300\"><label>6</label><title>DISCUSSION</title><p>In this study, we introduced HCViT&#8208;Net, a novel deep learning framework for skin lesion segmentation, and demonstrated its competitive performance on the challenging ISIC 2017 and ISIC 2018 datasets. The results confirm that our hybrid approach, which marries the local feature strengths of CNNs with the global context capabilities of Transformers, is highly effective for this complex medical imaging&#160;task.</p><p>The superior performance of HCViT&#8208;Net stems from two key architectural innovations. The first is MSQFormer, which enables the synergistic fusion of local CNN features and global ViT context at all stages of the network. By efficiently compressing the key/value tensors, MSQFormer significantly reduces the complexity of the self&#8208;attention mechanism, allowing its integration throughout the entire network backbone and ensuring robust representation learning from shallow to deep layers. In addition, our proposed WARM effectively bridges the semantic gap between encoder and decoder features, resulting in more precise boundary delineation and improved segmentation accuracy. This two&#8208;stage process&#8212;robust feature fusion followed by targeted refinement&#8212;is the primary reason for the model's high accuracy, which achieved an mIoU of 87.76% on the ISIC 2017 benchmark and 87.45% on the ISIC 2018 benchmark, establishing it as a highly competitive tool for dermatological image&#160;analysis.</p><p>From a clinical perspective, the implications of an accurate and efficient segmentation tool like HCViT&#8208;Net are significant. Automated, reliable lesion measurement provides a robust foundation for the quantitative analysis of the &#8220;ABCD&#8221; rule (asymmetry, border, color, diameter), bolstering the objectivity and confidence of clinical decision&#8208;making. Furthermore, the model's computational efficiency is not merely a technical advantage but a critical enabler for practical adoption. Its lightweight nature makes HCViT&#8208;Net a prime candidate for integration into standard dermatological software or even mobile health applications, where it could serve as a real&#8208;time &#8220;second opinion&#8221; for clinicians without requiring specialized, high&#8208;cost hardware. This could help accelerate diagnostic workflows and facilitate longitudinal monitoring of suspicious&#160;lesions.</p><p>Beyond dermoscopy, we believe the full approach (MSQFormer + WARM) is, in principle, broadly transferable, while being particularly advantageous for dermoscopic segmentation due to its close alignment with the characteristic properties of dermoscopic images. From a general applicability perspective, MSQFormer's multi&#8208;scale queries and global modeling reliably capture long&#8208;range dependencies and multi&#8208;scale structures, making it suitable for medical scenarios with pronounced object size variability (e.g., polyps, glands, retinal vessels, and certain organ/tumor segmentations). Its query&#8208;based fusion mechanism is also relatively backbone&#8208;agnostic, facilitating integration into UNet&#8208;like or hybrid CNN&#8211;Transformer encoders. Meanwhile, WARM employs wavelet&#8208;domain decomposition and semantic gating to selectively enhance high&#8208;frequency boundary cues and suppress structured noise in the frequency domain; this boundary refinement strategy is modality&#8211;agnostic in principle. Notably, in dermoscopic tasks the combination exhibits a stronger synergy: lesions often present rich, irregular, and low&#8208;contrast boundary textures, alongside high&#8208;frequency artifacts such as hair, skin texture, and uneven illumination. WARM strengthens true edges while suppressing spurious activations, whereas MSQFormer preserves global shape and contextual consistency amid pronounced multi&#8208;scale variability&#8212;together forming a complementary &#8220;global&#8211;local&#8221; pairing. For modalities with smoother, plateau&#8208;like boundary transitions (e.g., some MRI organs), the gains from WARM may be more modest; in such cases, performance can be tuned by adjusting the wavelet basis, the decomposition level <mml:math id=\"jats-math-111\" display=\"inline\"><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:math>, and the gating strength. More generally, we anticipate positive yet task&#8208;dependent gains across other medical segmentation problems, which can be further improved through lightweight adaptations, including wavelet&#8208;level selection, gating calibration, and 3D&#160;extensions.</p><p>Nevertheless, this study has several limitations that must be acknowledged. First, the training and validation were performed exclusively on dermoscopic images, which are acquired under controlled lighting and magnification. The model's performance on standard clinical photographs, which exhibit greater variability, has not yet been assessed. To enhance generalizability for real&#8208;world primary care settings, future work should incorporate these more diverse image types. These limitations define clear avenues for our future research, which will also include extending the framework to perform simultaneous lesion segmentation and&#160;classification.</p></sec><sec id=\"acm270385-sec-0310\"><label>7</label><title>CONCLUSIONS</title><p>In this paper, we presented HCViT&#8208;Net, a novel hybrid architecture designed to address the core challenges of skin lesion segmentation. Its primary innovation, the MSQFormer, enables the synergistic fusion of local CNN features and global ViT context at all network stages. By efficiently compressing key/value tensors, MSQFormer facilitates powerful self&#8208;attention with significantly reduced complexity, allowing its integration throughout the entire network backbone. This ensures robust representation learning from shallow to deep layers. Furthermore, our WARM effectively mitigates the semantic gap between encoder and decoder features, leading to more refined boundary delineation and enhanced segmentation accuracy. Experimental results on the public ISIC 2017 and ISIC 2018 benchmark datasets confirm that HCViT&#8208;Net achieves competitive segmentation accuracy, providing a more reliable foundation for subsequent automated analysis of diagnostic features like asymmetry and border irregularity. Crucially, this high performance is achieved with notable computational efficiency, making HCViT&#8208;Net a prime candidate for integration into clinical workflows. It holds the potential to serve as a real&#8208;time, reliable &#8220;second opinion&#8221; for dermatologists, helping to accelerate diagnostic procedures without the need for specialized, high&#8208;cost&#160;hardware.</p></sec><sec id=\"acm270385-sec-0320\"><title>AUTHOR CONTRIBUTIONS</title><p>\n<bold>Wei Jiao</bold>: Conceptualization; methodology; software; formal analysis; investigation; data curation; writing original draft preparation; writing review and editing; visualization. <bold>Jianghui Xu</bold>: Conceptualization; methodology; formal analysis; writing review and editing. <bold>Yijiao Fang</bold>: Methodology; Formal analysis; resources; writing review and editing. <bold>Jiaojiao Huang</bold>: Software; data curation. <bold>Yujie Zhu</bold>: Writing&#8212;review and editing. <bold>Dandan Ling</bold>: Conceptualization; supervision; writing&#8212;review and editing. All authors have read and agreed to the published version of the&#160;manuscript.</p></sec><sec sec-type=\"COI-statement\" id=\"acm270385-sec-0340\"><title>CONFLICT OF INTEREST STATEMENT</title><p>The authors declare no conflicts of&#160;interest.</p></sec></body><back><ack id=\"acm270385-sec-0330\"><title>ACKNOWLEDGMENTS</title><p>The authors have nothing to report.</p></ack><sec sec-type=\"data-availability\" id=\"acm270385-sec-0360\"><title>DATA AVAILABILITY STATEMENT</title><p>The code of our paper is available at: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/lddFDU/HCViT-Net\" ext-link-type=\"uri\" specific-use=\"software is-supplemented-by\">https://github.com/lddFDU/HCViT&#8208;Net</ext-link>.</p></sec><ref-list id=\"acm270385-bibl-0001\"><title>REFERENCES</title><ref id=\"acm270385-bib-0001\"><label>1</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0001\"><string-name name-style=\"western\"><surname>Bray</surname><given-names>F</given-names></string-name>, <string-name name-style=\"western\"><surname>Laversanne</surname><given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Sung</surname><given-names>H</given-names></string-name>, et&#160;al. <article-title>Global cancer statistics 2022: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>. <source>CA Cancer J Clin</source>. <year>2024</year>;<volume>74</volume>:<fpage>229</fpage>&#8208;<lpage>263</lpage>.<pub-id pub-id-type=\"pmid\">38572751</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3322/caac.21834</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0002\"><label>2</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0002\"><string-name name-style=\"western\"><surname>Tschandl</surname><given-names>P</given-names></string-name>, <string-name name-style=\"western\"><surname>Codella</surname><given-names>N</given-names></string-name>, <string-name name-style=\"western\"><surname>Cabo</surname><given-names>H</given-names></string-name>, et&#160;al. <article-title>Comparison of the accuracy of human readers versus machine&#8208;learning algorithms for pigmented skin lesion classification: an open, web&#8208;based, international, diagnostic study</article-title>. <source>Lancet Oncol</source>. <year>2019</year>;<volume>20</volume>:<fpage>938</fpage>&#8208;<lpage>947</lpage>.<pub-id pub-id-type=\"pmid\">31201137</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/S1470-2045(19)30333-X</pub-id><pub-id pub-id-type=\"pmcid\">PMC8237239</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0003\"><label>3</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0003\"><string-name name-style=\"western\"><surname>Brinker</surname><given-names>TJ</given-names></string-name>, <string-name name-style=\"western\"><surname>Hekler</surname><given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Enk</surname><given-names>AH</given-names></string-name>, et&#160;al. <article-title>Deep learning outperformed 136 of 157 dermatologists in a head&#8208;to&#8208;head dermoscopic melanoma image classification task</article-title>. <source>Eur J Cancer</source>. <year>2019</year>;<volume>113</volume>:<fpage>47</fpage>&#8208;<lpage>54</lpage>.<pub-id pub-id-type=\"pmid\">30981091</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.ejca.2019.04.001</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0004\"><label>4</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0004\"><string-name name-style=\"western\"><surname>LeCun</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Hinton</surname><given-names>G</given-names></string-name>. <publisher-loc>Deep learning</publisher-loc>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>:<fpage>436</fpage>&#8208;<lpage>444</lpage>.<pub-id pub-id-type=\"pmid\">26017442</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/nature14539</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0005\"><label>5</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0005\"><string-name name-style=\"western\"><surname>Long</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Shelhamer</surname><given-names>E</given-names></string-name>, <string-name name-style=\"western\"><surname>Darrell</surname><given-names>T</given-names></string-name>. <article-title>Fully convolutional networks for semantic segmentation</article-title>. In: <article-title>\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n</article-title>. <year>2015</year>;<volume>39</volume>:<fpage>3431</fpage>&#8208;<lpage>3440</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2016.2572683</pub-id><pub-id pub-id-type=\"pmid\">27244717</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0006\"><label>6</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0006\"><string-name name-style=\"western\"><surname>Chen</surname><given-names>L&#8208;C</given-names></string-name>, <string-name name-style=\"western\"><surname>Papandreou</surname><given-names>G</given-names></string-name>, <string-name name-style=\"western\"><surname>Kokkinos</surname><given-names>I</given-names></string-name>, <string-name name-style=\"western\"><surname>Murphy</surname><given-names>K</given-names></string-name>, <string-name name-style=\"western\"><surname>Yuille</surname><given-names>AL</given-names></string-name>. <article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>40</volume>:<fpage>834</fpage>&#8208;<lpage>848</lpage>.<pub-id pub-id-type=\"pmid\">28463186</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2017.2699184</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0007\"><label>7</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0007\"><string-name name-style=\"western\"><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name name-style=\"western\"><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name name-style=\"western\"><surname>Brox</surname><given-names>T</given-names></string-name>. <article-title>U&#8208;net: Convolutional networks for biomedical image segmentation</article-title>. In: <article-title>\nMedical Image Computing and Computer&#8208;Assisted Intervention&#8211;MICCAI 2015\n</article-title>. Lecture Notes in Computer Science, vol 9351. <publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc>; <year>2015</year>:<fpage>234</fpage>&#8208;<lpage>241</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0008\"><label>8</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0008\"><string-name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Beyer</surname><given-names>L</given-names></string-name>, <string-name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A</given-names></string-name>, et&#160;al. <article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title>. In: <article-title>\nInternational Conference on Learning Representations (ICLR)\n</article-title><year>2021</year>.</mixed-citation></ref><ref id=\"acm270385-bib-0009\"><label>9</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0009\"><string-name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Lin</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Cao</surname><given-names>Y</given-names></string-name>, et&#160;al. <article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title>. In: <article-title>\nProceedings of the IEEE/CVF International Conference on Computer Vision\n</article-title><year>2021</year>:<fpage>10012</fpage>&#8208;<lpage>10022</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0010\"><label>10</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0010\"><string-name name-style=\"western\"><surname>Cao</surname><given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></string-name>, et&#160;al. <article-title>Swin&#8208;unet: Unet&#8208;like pure transformer for medical image segmentation</article-title>. In: <article-title>\nEuropean Conference on Computer Vision\n</article-title><publisher-name>Springer</publisher-name>; <year>2022</year>:<fpage>205</fpage>&#8208;<lpage>218</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0011\"><label>11</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0011\"><string-name name-style=\"western\"><surname>Rekha</surname><given-names>R</given-names></string-name>, <string-name name-style=\"western\"><surname>Shruti</surname><given-names>P</given-names></string-name>, <string-name name-style=\"western\"><surname>Deekshitha</surname><given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Akash</surname><given-names>J</given-names></string-name>. <article-title>DCSwin&#8208;UNet: Dual Encoder U&#8208;Net based on CNN and Swin Transformer with trainable multiplication layer for brain tumor segmentation from MRI images</article-title>. <source>Biomed Signal Process Control</source>. <year>2025</year>;<fpage>110</fpage>:<elocation-id>108325</elocation-id>.</mixed-citation></ref><ref id=\"acm270385-bib-0012\"><label>12</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0012\"><string-name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Mei</surname><given-names>J.</given-names></string-name>, <string-name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></string-name>, et&#160;al. <article-title>TransUNet: Rethinking the U&#8208;Net architecture design for medical image segmentation through the lens of transformers</article-title>. <source>Med Image Anal</source>. <year>2024</year>;<volume>97</volume>:<elocation-id>103280</elocation-id>.<pub-id pub-id-type=\"pmid\">39096845</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2024.103280</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0013\"><label>13</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0013\"><string-name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name name-style=\"western\"><surname>Hu</surname><given-names>Q</given-names></string-name>. <article-title>Transfuse: Fusing transformers and cnns for medical image segmentation</article-title>. In: <article-title>\nMedical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part I 24\n</article-title><publisher-name>Springer</publisher-name>; <year>2021</year>:<fpage>14</fpage>&#8208;<lpage>24</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0014\"><label>14</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0014\"><string-name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Girshick</surname><given-names>R</given-names></string-name>, <string-name name-style=\"western\"><surname>Gupta</surname><given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>He</surname><given-names>K</given-names></string-name>. <article-title>Non&#8208;local neural networks</article-title>. In: <article-title>\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n</article-title><year>2018</year>:<fpage>7794</fpage>&#8208;<lpage>7803</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0015\"><label>15</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0015\"><string-name name-style=\"western\"><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname><given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Yu</surname><given-names>Y</given-names></string-name>, et&#160;al. <article-title>Multi&#8208;scale information residual network: Deep residual network of prostate cancer segmentation based on multi scale information guidance</article-title>. <source>Biomed Signal Process Control</source>. <year>2025</year>;<volume>110</volume>:<elocation-id>108132</elocation-id>.</mixed-citation></ref><ref id=\"acm270385-bib-0016\"><label>16</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0016\"><string-name name-style=\"western\"><surname>Chang</surname><given-names>H&#8208;H</given-names></string-name>, <string-name name-style=\"western\"><surname>Chou</surname><given-names>Y&#8208;X</given-names></string-name>, <string-name name-style=\"western\"><surname>Chao</surname><given-names>C&#8208;C</given-names></string-name>, <string-name name-style=\"western\"><surname>Hsieh</surname><given-names>S&#8208;T</given-names></string-name>. <article-title>Multiscale convolution block U&#8208;Net for automatic epidermis segmentation in immunofluorescence images</article-title>. <source>Biomed Signal Process Control</source>. <year>2025</year>;<volume>109</volume>:<elocation-id>108027</elocation-id>.</mixed-citation></ref><ref id=\"acm270385-bib-0017\"><label>17</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0017\"><string-name name-style=\"western\"><surname>Zhou</surname><given-names>Z</given-names></string-name>, Rahman <string-name name-style=\"western\"><surname>Siddiquee</surname><given-names>MM</given-names></string-name>, <string-name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N</given-names></string-name>, <string-name name-style=\"western\"><surname>Liang</surname><given-names>J</given-names></string-name>. <article-title>Unet++: A nested u&#8208;net architecture for medical image segmentation</article-title>. In: <article-title>\nDeep learning in Med. Image Anal. and multimodal learning for clinical decision support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML&#8208;CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4\n</article-title><publisher-name>Springer</publisher-name>; <year>2018</year>:<fpage>3</fpage>&#8208;<lpage>11</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-030-00889-5_1</pub-id><pub-id pub-id-type=\"pmcid\">PMC7329239</pub-id><pub-id pub-id-type=\"pmid\">32613207</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0018\"><label>18</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0018\"><string-name name-style=\"western\"><surname>Zheng</surname><given-names>S</given-names></string-name>, <string-name name-style=\"western\"><surname>Lu</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhao</surname><given-names>H</given-names></string-name>, et&#160;al. <article-title>Rethinking semantic segmentation from a sequence&#8208;to&#8208;sequence perspective with transformers</article-title>. In: <article-title>\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n</article-title>. <year>2021</year>:<fpage>6881</fpage>&#8208;<lpage>6890</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0019\"><label>19</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0019\"><string-name name-style=\"western\"><surname>Peng</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Huang</surname><given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Gu</surname><given-names>S</given-names></string-name>, et&#160;al. <article-title>Conformer: Local features coupling global representations for visual recognition</article-title>. In: <article-title>\nProceedings of the IEEE/CVF International Conference on Computer Vision\n</article-title><year>2021</year>:<fpage>367</fpage>&#8208;<lpage>376</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0020\"><label>20</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0020\"><string-name name-style=\"western\"><surname>Xie</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Shen</surname><given-names>C</given-names></string-name>, <string-name name-style=\"western\"><surname>Xia</surname><given-names>Y</given-names></string-name>. <article-title>Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation</article-title>. In: <article-title>\nMedical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part III 24\n</article-title><publisher-name>Springer</publisher-name>; <year>2021</year>:<fpage>171</fpage>&#8208;<lpage>180</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0021\"><label>21</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0021\"><string-name name-style=\"western\"><surname>Howard</surname><given-names>AG</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhu</surname><given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname><given-names>B</given-names></string-name>, et&#160;al. <article-title>MobileNets: Efficient convolutional neural networks for mobile vision applications</article-title>. In: <article-title>\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n</article-title>. <year>2017</year>:<fpage>4686</fpage>&#8208;<lpage>4697</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0022\"><label>22</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0022\"><string-name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></string-name>, <string-name name-style=\"western\"><surname>Xie</surname><given-names>E</given-names></string-name>, <string-name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></string-name>, et&#160;al. <article-title>Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</article-title>. In: <article-title>\nProceedings of the IEEE/CVF International Conference on Computer Vision\n</article-title><year>2021</year>:<fpage>568</fpage>&#8208;<lpage>578</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0023\"><label>23</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0023\"><string-name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Chen</surname><given-names>F</given-names></string-name>, <string-name name-style=\"western\"><surname>Ma</surname><given-names>Y</given-names></string-name>, et&#160;al. <article-title>Xbound&#8208;former: Toward cross&#8208;scale boundary modeling in transformers</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2023</year>;<volume>42</volume>:<fpage>1735</fpage>&#8208;<lpage>1745</lpage>.<pub-id pub-id-type=\"pmid\">37018671</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2023.3236037</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0024\"><label>24</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0024\"><string-name name-style=\"western\"><surname>Ji</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Ye</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Ma</surname><given-names>X</given-names></string-name>. <article-title>BDFormer: Boundary&#8208;aware dual&#8208;decoder transformer for skin lesion segmentation</article-title>. <source>Artif Intell Med</source>. <year>2025</year>;<elocation-id>103079</elocation-id>.<pub-id pub-id-type=\"pmid\">39983372</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.artmed.2025.103079</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0025\"><label>25</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0025\"><string-name name-style=\"western\"><surname>Ruan</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Xiang</surname><given-names>S</given-names></string-name>, <string-name name-style=\"western\"><surname>Xie</surname><given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Liu</surname><given-names>T</given-names></string-name>, <string-name name-style=\"western\"><surname>Fu</surname><given-names>Y</given-names></string-name>. <article-title>Malunet: A multi&#8208;attention and light&#8208;weight unet for skin lesion segmentation</article-title>. In: <article-title>\n2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)\n</article-title><publisher-name>IEEE</publisher-name>; <year>2022</year>:<fpage>1150</fpage>&#8208;<lpage>1156</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0026\"><label>26</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0026\"><string-name name-style=\"western\"><surname>Codella</surname><given-names>NC</given-names></string-name>, <string-name name-style=\"western\"><surname>Gutman</surname><given-names>D</given-names></string-name>, <string-name name-style=\"western\"><surname>Helba</surname><given-names>B</given-names></string-name>, et&#160;al. <article-title>Skin lesion analysis toward melanoma detection: A challenge at the 2017 International Symposium on biomedical imaging (ISBI), hosted by the International skin imaging collaboration (ISIC)</article-title>. In: <article-title>\n2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)\n</article-title><publisher-name>IEEE</publisher-name>; <year>2018</year>:<fpage>168</fpage>&#8208;<lpage>172</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0027\"><label>27</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0027\"><string-name name-style=\"western\"><surname>Codella</surname><given-names>N</given-names></string-name>, <string-name name-style=\"western\"><surname>Rotemberg</surname><given-names>V</given-names></string-name>, <string-name name-style=\"western\"><surname>Tschandl</surname><given-names>P</given-names></string-name>, et&#160;al. <article-title>Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International skin imaging collaboration (ISIC)</article-title>. <article-title>\narXiv preprint arXiv:1902.03368\n</article-title><year>2019</year>.</mixed-citation></ref><ref id=\"acm270385-bib-0028\"><label>28</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0028\"><string-name name-style=\"western\"><surname>Wei</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Hu</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname><given-names>R</given-names></string-name>, <string-name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhou</surname><given-names>SK</given-names></string-name>, <string-name name-style=\"western\"><surname>Cui</surname><given-names>S</given-names></string-name>. <article-title>Shallow attention network for polyp segmentation</article-title>. In: <article-title>\nMedical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part I 24\n</article-title><publisher-name>Springer</publisher-name>; <year>2021</year>:<fpage>699</fpage>&#8208;<lpage>708</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0029\"><label>29</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0029\"><string-name name-style=\"western\"><surname>Xu</surname><given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Ma</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Duan</surname><given-names>W</given-names></string-name>. <article-title>DCSAU&#8208;Net: A deeper and more compact split&#8208;attention U&#8208;Net for medical image segmentation</article-title>. <source>Comput Biol Med</source>. <year>2023</year>;<volume>154</volume>:<elocation-id>106626</elocation-id>.<pub-id pub-id-type=\"pmid\">36736096</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2023.106626</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0030\"><label>30</label><mixed-citation publication-type=\"book\" id=\"acm270385-cit-0030\"><string-name name-style=\"western\"><surname>Li</surname><given-names>C</given-names></string-name>, <string-name name-style=\"western\"><surname>Qiang</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Sultan</surname><given-names>RI</given-names></string-name>, et&#160;al. <article-title>Focalunetr: A focal transformer for boundary&#8208;aware prostate segmentation using CT images</article-title>. In: <article-title>\nInternational Conference on medical image computing and computer&#8208;assisted intervention\n</article-title><publisher-name>Springer</publisher-name>; <year>2023</year>:<fpage>592</fpage>&#8208;<lpage>602</lpage>.</mixed-citation></ref><ref id=\"acm270385-bib-0031\"><label>31</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0031\"><string-name name-style=\"western\"><surname>Sun</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Dai</surname><given-names>D</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname><given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Xu</surname><given-names>S</given-names></string-name>, <string-name name-style=\"western\"><surname>Lian</surname><given-names>C</given-names></string-name>. <article-title>MSCA&#8208;Net: Multi&#8208;scale contextual attention network for skin lesion segmentation</article-title>. <source>Pattern Recognit</source>. <year>2023</year>;<volume>139</volume>:<elocation-id>109524</elocation-id>.</mixed-citation></ref><ref id=\"acm270385-bib-0032\"><label>32</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0032\"><string-name name-style=\"western\"><surname>Dai</surname><given-names>D</given-names></string-name>, <string-name name-style=\"western\"><surname>Dong</surname><given-names>C</given-names></string-name>, <string-name name-style=\"western\"><surname>Yan</surname><given-names>Q</given-names></string-name>, <string-name name-style=\"western\"><surname>Sun</surname><given-names>Y</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname><given-names>C</given-names></string-name>, <string-name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name name-style=\"western\"><surname>Xu</surname><given-names>S</given-names></string-name>. <article-title>I2u&#8208;net: a dual&#8208;path u&#8208;net with rich information interaction for medical image segmentation</article-title>. <source>Med Image Anal</source>. <year>2024</year>;<volume>97</volume>:<elocation-id>103241</elocation-id>.<pub-id pub-id-type=\"pmid\">38897032</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2024.103241</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0033\"><label>33</label><mixed-citation publication-type=\"journal\" id=\"acm270385-cit-0033\"><string-name name-style=\"western\"><surname>Shu</surname><given-names>X</given-names></string-name>, <string-name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Zhang</surname><given-names>A</given-names></string-name>, <string-name name-style=\"western\"><surname>Shi</surname><given-names>J</given-names></string-name>, <string-name name-style=\"western\"><surname>Wu</surname><given-names>X&#8208;J</given-names></string-name>. <article-title>CSCA U&#8208;Net: A channel and space compound attention CNN for medical image segmentation</article-title>. <source>Artif Intell Med</source>. <year>2024</year>;<volume>150</volume>:<elocation-id>102800</elocation-id>.<pub-id pub-id-type=\"pmid\">38553146</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.artmed.2024.102800</pub-id></mixed-citation></ref><ref id=\"acm270385-bib-0034\"><label>34</label><mixed-citation publication-type=\"miscellaneous\" id=\"acm270385-cit-0034\"><string-name name-style=\"western\"><surname>Selvaraju</surname><given-names>RR</given-names></string-name>, <string-name name-style=\"western\"><surname>Cogswell</surname><given-names>M</given-names></string-name>, <string-name name-style=\"western\"><surname>Das</surname><given-names>A</given-names></string-name>, et&#160;al. <article-title>Grad&#8208;CAM: Visual explanations from deep networks via gradient&#8208;based localization</article-title>. <article-title>\nProceedings of the IEEE International Conference on Computer Vision (ICCV)\n</article-title>. <year>2017</year>:<fpage>618</fpage>&#8208;<lpage>626</lpage>.</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc J Appl Clin Med Phys J Appl Clin Med Phys 3200 jacmp ACM2 Journal of Applied Clinical Medical Physics 1526-9914 Wiley PMC12658347 PMC12658347.1 12658347 12658347 41306077 10.1002/acm2.70385 ACM270385 1 Research Article IMAGING PHYSICS Research Article HCViT&#8208;Net: Hybrid CNN and multi scale query transformer network for dermatological image segmentation JIAO et&#160;al. Jiao Wei 1 Xu Jianghui 1 Fang Yijiao 1 Huang Jiaojiao 1 Zhu Yujie 2 3 Ling Dandan 1 ldd202506@163.com 1 Department of Anesthesiology Fudan University Shanghai Cancer Center Shanghai China 2 Department of Dermatology Shanghai Ninth People's Hospital Shanghai China 3 School of Medicine Shanghai Jiaotong University Shanghai China * Correspondence Dandan Ling, Department of Anesthesiology, Fudan University Shanghai Cancer Center, Shanghai, China. Email: ldd202506@163.com 27 11 2025 12 2025 26 12 500804 10.1002/acm2.v26.12 e70385 14 10 2025 15 8 2025 21 10 2025 27 11 2025 28 11 2025 28 11 2025 &#169; 2025 The Author(s). Journal of Applied Clinical Medical Physics published by Wiley Periodicals, LLC on behalf of The American Association of Physicists in Medicine. https://creativecommons.org/licenses/by/4.0/ This is an open access article under the terms of the http://creativecommons.org/licenses/by/4.0/ License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. Abstract Background Dermoscopic lesion segmentation is crucial for dermatology, yet existing methods struggle to integrate global context with local details under the efficiency constraints required for clinical&#160;use. Purpose We aim to develop a lightweight model that simultaneously captures long&#8208;range spatial dependencies and preserves fine&#8208;grained boundary details for dermoscopic lesions. The method is designed to achieve a favorable accuracy&#8211;efficiency trade&#8208;off, thereby improving segmentation performance and ensuring potential for practical clinical&#160;deployment. Methods Proposing a lightweight hybrid model, HCViT&#8208;Net, featuring an encoder&#8211;decoder architecture. It incorporates a multi&#8208;scale query transformer (MSQFormer) into each stage of its convolutional encoder to efficiently capture global, multi&#8208;scale context. Furthermore, a wavelet&#8208;guided attention refinement module (WARM) is introduced on the highest&#8208;resolution skip connection to selectively enhance high&#8208;frequency boundary details and bridge the semantic gap between the encoder and decoder, thus improving model&#160;performance. Results Evaluated on ISIC 2017 and 2018, our model achieved mean intersection&#8208;over&#8208;union (mIoU) of 87.76% and 87.45%, respectively. With only 5.76M parameters and 7.51 GFLOPs, it demonstrates performance competitive with existing methods at a significantly lower computational cost. Conclusions HCViT&#8208;Net achieves an excellent accuracy&#8211;efficiency trade&#8208;off. It improves segmentation accuracy with a low computational footprint, showing strong potential for practical deployment in dermatology workflows. boundary refinement medical image segmentation multi&#8208;scale features skin lesion pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes source-schema-version-number 2.0 cover-date December 2025 details-of-publishers-convertor Converter:WILEY_ML3GV2_TO_JATSPMC version:6.6.6 mode:remove_FC converted:27.11.2025 Jiao W , Xu J , Fang Y , Huang J , Zhu Y , Ling D . HCViT&#8208;Net: Hybrid CNN and multi scale query transformer network for dermatological image segmentation . J Appl Clin Med Phys . 2025 ; 26 : e70385 . 10.1002/acm2.70385 41306077 Wei Jiao, Jianhui Xu and Yijiao Fang are contributed equally to this work. 1 INTRODUCTION Melanoma, characterized by aggressive invasiveness, high metastatic potential, and ele vated mortality, has become one of the fastest&#8208;growing malignancies worldwide. 1 In clinical settings, dermatologists must manually identify and delineate lesions via dermatoscopic imaging&#8212;a diagnostic procedure critically dependent on clinician experience and technical proficiency. Substantial evidence demonstrates that automated lesion segmentation in skin imaging enhances the accuracy of abnormality detection by both clinicians and AI diagnostic systems, thereby providing an objective foundation for early screening and differential diagnosis. 2 , 3 The rapid evolution of deep learning and computer vision 4 has revolutionized medical image segmentation, yielding unprecedented gains in accuracy. The fully convolutional network (FCN) 5 pioneered pixel&#8208;level CNN segmentation, and subsequent models&#8212;for example, DeepLab 6 with atrous spatial pyramid pooling (ASPP) for larger receptive fields, and U&#8208;Net 7 with encoder&#8211;decoder skip connections&#8212;have refined multi&#8208;scale context capture and fine&#8208;detail&#160;recovery. While these CNNs&#8208;based segmentation models 5 , 6 , 7 have achieved remarkable success, the inherently local receptive fields of convolutional layers limit global dependency modeling, a key drawback in skin lesion segmentation. As illustrated in the first two columns of Figure&#160; 1 , skin lesions in dermoscopic images often exhibit significant spatial occupancy that may exceed the local receptive fields of CNNs. The intrinsic locality of convolution operations fundamentally limits the model's capacity to capture global morphological characteristics of lesions. Furthermore, the latter two columns of Figure&#160; 1 demonstrate that skin lesions can present textural similarities to benign skin features. Establishing global feature dependencies across image regions proves crucial for enhancing the model's discriminative power against lesion&#8208;mimicking artifacts, thereby improving segmentation&#160;precision. FIGURE 1 Illustration of the global (blue curves) and local (green curves) contextual information in dermatological images. By establishing global feature dependencies across image regions, the model can capture the global morphological characteristics of lesions and enhance its discriminative power against lesion&#8208;mimicking artifacts, thereby improving segmentation precision. Therefore, due to the superior performance of ViTs 8 , 9 architectures in establishing long&#8208;range dependencies, some works 10 , 11 , 12 have introduced them into segmentation models to fully extract global information from images. For example, SwinUNet 10 employs Swin Transformer 9 as the backbone network to construct a U&#8208;shaped medical image segmentation network composed entirely of&#160;ViT. These pure&#8208;ViT&#8208;based segmentation methods can fully extract global information from images. However, limited by the computational mechanism of self&#8208;attention, ViTs typically incur a quadratic computational cost in terms of token quantity, while also lacking the ability to preserve local details during feature extraction. Thus, some works combining CNN and ViT within a single model to overcome this issue. For instance, TransUNet 12 and TransFuse 13 integrates ViT&#8208;R50 8 as the backbone network for feature extraction, enabling the network to simultaneously capture global and local features. Nevertheless, constrained by the high computational cost of ViTs, these methods can only incorporate ViT modules in regions with smaller feature maps. Studies in non&#8208;local 14 have shown that integrating the global information module into the model's shallow layers&#8212;where feature maps typically have larger spatial dimensions&#8212;yields greater performance gains. Moreover, inserting global feature modeling modules at multiple stages can progressively yield better results. In addition, due to the fixed window size and token dimension, existing advanced vision transformer modules 8 , 9 often lack internal multi&#8208;scale information, which is critical for the accuracy of segmentation results. 15 , 16 To address above problems of existing ViT modules, 8 , 9 , 12 we propose a novel architecture named HCViT&#8208;Net that systematically integrates both CNN and ViT components across all stages of the model. Figure&#160; 2 illustrates the architectural distinctions between our proposed method and existing pure CNN&#8208;based, pure ViT&#8208;based, and hybrid CNN&#8208;ViT models. Our method uniquely enables comprehensive learning of both local and global features at every stage of the model, addressing limitations of prior works that often prioritize one type of information over the other. We design a lightweight self&#8208;attention approach featuring multi&#8208;scale key&#8208;value (K&#8208;V) reduction. This solution not only significantly reduces the computational overhead of standard self&#8208;attention (maintaining our HCViT&#8208;Net's advantage in model complexity), but also constructs intrinsic multi&#8208;scale representations within ViT, thereby compensating for the absence of internal multi&#8208;scale information in competitive ViT&#160;architectures. FIGURE 2 Illustration of the differences in model architecture between previous methods and ours. We combine CNN and ViT at all stages of the model to fully exploit the advantages of these two computing paradigms. ViT, vision transformer. Additionally, in the skip connections of UNet 7 architecture, the shallow features generated by the first encoder layer preserve rich spatial details but lack sufficient semantic representation, while the corresponding deep decoder features contain abundant semantic information at the cost of spatial detail loss due to repeated downsampling. This inherent semantic gap leads to significant performance degradation when using conventional fusion methods direct summation or channel concatenation. To address this issue, we propose a feature refinement module named wavelet&#8208;attention refinement module (WARM), which can adaptively establishes cross&#8208;level feature correlations, effectively bridging the semantic gap and consequently improving segmentation&#160;accuracy. Consequently, the contributions in this work can be summarized as follows: 1. To combine the local detail extraction capabilities of CNNs with the global context modeling strengths of ViTs, we introduce HCViT&#8208;Net, a novel hybrid CNN&#8211;ViT architecture that integrates both CNNs and ViTs at all stages of the model&#8212;in contrast to existing approaches which typically insert ViT modules only in a few stages&#8212;thereby enabling the full extraction of local and global contextual information. 2. To address the high computational cost and the lack of internal multi&#8208;scale information in existing ViT modules, we propose the multi&#8208;scale query transformer (MSQFormer), which compresses key and value tensors at multiple resolutions, thereby enabling global and multi&#8208;scale self&#8208;attention with significantly reduced computational complexity. This characteristic is pivotal in maintaining the low computational overhead of our proposed HCViT&#8208;Net. 3. To reduce the semantic gap between early encoder features and late decoder features, we propose the wavelet&#8208;attention refinement module (WARM), which based on wavelet transform attention. WARM decomposes the encoder's feature maps using wavelet transform and then leverages high&#8208;level semantic information from the decoder to guide the extraction of effective low&#8208;level features, thereby improving the final segmentation accuracy. 4. We conduct extensive evaluations on ISIC 2017 and ISIC 2018 benchmarks, demonstrating that HCViT&#8208;Net consistently outperforms pure CNNs, pure ViTs, and existing hybrid methods in segmentation accuracy while maintaining competitive model size and computational cost. This dual achievement underscores the model's significant potential for seamless integration into clinical workflows, offering a powerful tool to assist dermatologists in performing more timely and precise diagnostic assessments. 2 RELATED WORKS 2.1 Pure CNN&#8208;based segmentation methods Since the advent of fully convolutional networks (FCN), 5 end&#8208;to&#8208;end convolutional models have become the cornerstone of modern semantic segmentation. The DeepLab 6 family of models enhances contextual modeling by introducing atrous (dilated) convolutions, which enlarge receptive fields without downsampling, and by employing atrous spatial pyramid pooling (ASPP) for multi&#8208;scale context aggregation. In the medical imaging domain, the U&#8208;Net 7 , 17 architecture addresses challenges of limited data and the need for precise localization through a symmetric encoder&#8211;decoder structure with skip connections. This design directly shuttles encoder features to the decoder, preserving spatial details that might be lost during downsampling while enabling hierarchical feature&#160;extraction. These CNN&#8208;based segmentation models 5 , 6 , 7 , 17 have achieved remarkable success. However, limited by their local receptive fields, standard convolutional operations struggle to model long&#8208;range&#160;dependencies. 2.2 Pure ViT based segmentation methods The advent of the vision transformer (ViT) 8 has spurred researchers to explore extending its powerful global modeling capabilities to dense prediction tasks. ViT's core concept involves dividing an image into a sequence of non&#8208;overlapping patches, successfully adapting the standard Transformer 8 architecture for image classification. In the realm of semantic segmentation, early applications of ViT often treated it as a pixel&#8208;wise encoder. For instance, the SETR 18 model utilizes a ViT encoder followed by upsampling and refinement stages to recover high&#8208;resolution masks. Similarly, in medical image segmentation, Swin&#8208;UNet 10 adeptly employs the Swin Transformer 9 to construct a U&#8208;shaped segmentation network, achieving leading segmentation&#160;results. These pure ViT&#8208;based methods can effectively excavate global dependencies in images, but are constrained by the limitations of existing ViT modules. They fail to preserve adequate local details during feature extraction, 19 and additionally lack multi&#8208;scale query capability within their internal&#160;modules. 2.3 Hybrid CNN and ViT segmentation methods In medical image segmentation, hybrid CNN&#8211;ViT architectures have emerged to leverage the local feature extraction of convolutions alongside the global context modeling of self&#8208;attention. TransUNet 12 pioneered this line by embedding a ViT module in the bottleneck of a U&#8208;shaped CNN, allowing long&#8208;range dependencies to guide upsampling and yielding strong performance on abdominal and cardiac CT tasks. Building on this idea, CoTr 20 introduced a deformable self&#8208;attention mechanism that sparsely attends to salient medical features, reducing both memory footprint and computational overhead while preserving fine&#8208;grained&#160;structures. These methods integrate CNNs and ViTs within one model, but due to the high computational cost of self&#8208;attention mechanisms, these ViT modules can typically only be inserted at the deeper stages of the model. This inherently limits the model's ability to perform global information modeling at other stages and consequently restricts further improvements in segmentation&#160;accuracy. 3 METHODS 3.1 Overview Figure&#160; 3 illustrates the detailed architecture of our proposed model. The input image is progressively downsampled by the encoder and then restored to its original resolution by the decoder. Our baseline is a U&#8208;shaped network composed solely of CNN blocks. To enrich each stage with global context, we insert the proposed multi&#8208;scale query transformer (MSQFormer) block immediately after every CNN block. Multiple skip connections are utilized to pass low&#8208;level details from the encoder to the decoder. For the two smaller&#8208;sized skip connections, features are fused with the upsampled decoder output via element&#8208;wise addition. Notably, the largest feature map from the earliest encoder stage contains minimal semantic content and significant noise. We first refine these features with our proposed wavelet&#8208;attention refinement module (WARM), and then fuse them with the upsampled decoder output using the same additive&#160;method. FIGURE 3 The architecture of the proposed HCViT&#8208;Net. 3.2 CNN block Driven by considerations for computational complexity and parameter efficiency, we built our custom CNN block drawing inspiration from the design philosophy of the classic lightweight network, MobileNet. 21 As shown in Figure&#160; 4 , this module consists of a 7 &#215; 7 depthwise convolution (DWConv) layer followed by a 1 &#215; 1 standard convolution layer. The 7 &#215; 7 depthwise convolution captures spatial contextual information across a wide receptive field at a very low computational cost, while the 1 &#215; 1 standard convolution performs information interaction and recombination across the channel dimension to generate new, more expressive feature representations. Furthermore, each convolution layer is succeeded by a batch normalization (BN) layer and a rectified linear unit (ReLU) activation&#160;function. FIGURE 4 The architecture of the CNN Block in HCViT&#8208;Net. Let the input feature map be X &#8712; R B &#215; C &#215; H &#215; W , where B is the batch size, C is the channel dimension, H is the height and W is the width, the above steps can be represented by formulas as: (1) H 1 = ReLU ( BN ( DWConv2D k = 7 , s = 1 ( X ) ) ) &#8712; R B &#215; C &#215; H &#215; W . (2) H 2 = ReLU ( BN ( Conv2D k = 1 , s = 1 ( H 1 ) ) ) &#8712; R B &#215; C &#215; H &#215; W . (3) Z = X + H 2 &#8712; R B &#215; C &#215; H &#215; W where ReLU is the rectified linear unit activation function and BN is the batch&#160;norm. 3.3 Multi&#8208;scale query transformer block To effectively segment dermatological lesions, which inherently exhibit significant variations in size and shape, the model must be capable of processing contextual information from multiple resolutions simultaneously. 15 , 16 Therefore, the motivation for our multi&#8208;scale key&#8208;value compression strategy is twofold: on one hand, it drastically reduces the computational overhead of the self&#8208;attention 8 mechanism, making its deployment across all network stages feasible. On the other hand, and more importantly, it empowers a single query vector to simultaneously interact with contextual information aggregated from different spatial scales. This capability is clinically crucial, as it allows the model to be equally adept at capturing the fine details of small lesions and understanding the global structure of large, sprawling&#160;ones. Figure&#160; 5 illustrates the computational steps of our proposed MSQFormer module. Specifically, let the input feature map be X &#8712; R B &#215; C &#215; H &#215; W , where B is the batch size, C is the channel dimension, H is the height and W is the width. We reshape the input tensor to X q &#8712; R B &#215; N &#215; C first. Here, N = H &#215; W is the number of image tokens. Then, the query matrix is generated via linear projection and reshaping: (4) Q = Reshape Linear q ( X q ) &#8712; R B &#215; h &#215; N &#215; d . where, h is the number of attention heads and d = C / h is the dimension of each head. It should be noted that the query (Q) maintains the full resolution of the input feature tensor X . FIGURE 5 The architecture of the proposed multi&#8208;scale query attention block. Then, we generate multi&#8208;scale key&#8208;value matrices from the input X &#8712; R B &#215; C &#215; H &#215; W . Here, we generate spatial downsampling key&#8208;value pairs at three (1/2, 1/4, and 1/8) different resolutions: (5) F 1 = Conv2D k = 2 , s = 2 ( X ) &#8712; R B &#215; C &#215; H / 2 &#215; W / 2 . (6) F 2 = Conv2D k = 4 , s = 4 ( X ) &#8712; R B &#215; C &#215; H / 4 &#215; W / 4 . (7) F 3 = Conv2D k = 8 , s = 8 ( X ) &#8712; R B &#215; C &#215; H / 8 &#215; W / 8 . (8) K 1 , V 1 = Split Linear k v 1 GELU LN ( F 1 ) &#8712; R B &#215; h / 2 &#215; M 1 &#215; d . (9) K 2 , V 2 = Split Linear k v 2 GELU LN ( F 2 ) &#8712; R B &#215; h / 2 &#215; M 2 &#215; d . (10) K 3 , V 3 = Split Linear k v 3 GELU LN ( F 3 ) &#8712; R B &#215; h / 2 &#215; M 3 &#215; d . where M i = H W s i 2 , s i is the stride of path i . GELU is Gaussian error linear unit activation function and LN is layer&#160;norm. To realize local feature enhancement, each value matrix is augmented with depthwise separable convolution: (11) V &#8764; i = V i + DWConv 3 &#215; 3 ( V i ) , i &#8712; { 1 , 2 , 3 } where DWConv 3 &#215; 3 is the depthwise convolution with kernel size 3 &#215; 3. Then, we computed scaled dot&#8208;product attention for each scale path: (12) SA i = softmax Q i K i &#8868; d &#8712; R B &#215; h i &#215; N &#215; M i , Z i = SA i V &#8764; i &#8712; R B &#215; h i &#215; N &#215; d . where, S A i is the self&#8208;attention weights of the path i and Z i is the contextual features of path i . Finally, we concatenate outputs from all paths and project them to realize multi&#8208;scale feature fusion: (13) Z = Concat ( Z 1 , Z 2 , Z 3 ) &#8712; R B &#215; N &#215; 3 C / 2 , (14) O = Dropout Linear p r o j ( Z ) &#8712; R B &#215; N &#215; C . Previous Transformer models such as Swin Transformer 9 and PVT 22 capture multi&#8208;scale information implicitly through downsampling in a hierarchical feature pyramid, where fusion heavily relies on the encoder's resolution hierarchy. This approach overlooks the multi&#8208;scale nature of objects within a single attention layer, making these models sensitive to scale variations in real&#8208;world scenarios. The root cause lies in their attention design: each layer uses tokens with fixed receptive fields and uniform granularity, preventing simultaneous perception of multiple&#160;scales. In contrast, the proposed MSQFormer introduces explicit cross&#8208;scale attention within each ViT block by means of learnable semantic queries. This mechanism decouples multi&#8208;scale fusion from the encoder hierarchy, enabling content&#8208;adaptive, sparse, and efficient aggregation of full&#8208;scale information rather than fixed&#8208;topology fusion across adjacent layers. Consequently, MSQFormer achieves stronger global&#8211;local coupling and greater robustness to scale variation, effectively preserving the structural integrity of large lesions while maintaining fine boundary details in small ones. As shown in Figure&#160; 6 , the difference in modeling behavior across single self&#8208;attention layers is evident: (a) ViT employs global attention with uniform receptive fields, lacking explicit scale awareness. (b) Swin restricts attention to fixed local windows, limiting cross&#8208;scale interactions within one layer. (c) PVT encodes multi&#8208;scale features via hierarchical downsampling, with fusion occurring only across stages. (d) Ours (MSQFormer) performs explicit, content&#8208;adaptive cross&#8208;scale attention using learnable semantic queries, allowing joint modeling of global context and local detail within the same&#160;layer. FIGURE 6 Comparison of feature modeling within a single self&#8208;attention layer among ViT, Swin, PVT, and the proposed MSQFormer. (a)ViT): employs global self&#8208;attention with uniform receptive fields, lacking explicit scale awareness. (b)Swin: utilizes window&#8208;based local attention confined to fixed regions, thus limiting cross&#8208;scale interactions within one layer. (c)PVT: encodes multi&#8208;scale features through a hierarchical pyramid, where fusion occurs only across stages rather than inside individual attention layers. (d) Ours (MSQFormer): introduces learnable semantic queries to explicitly and adaptively aggregate information across multiple scales within a single attention layer, achieving enhanced global&#8211;local coupling. 3.4 Wavelet&#8208;attention refinement module Standard skip connections in U&#8208;Net 7 architectures directly concatenate high&#8208;resolution, low&#8208;level features from the encoder with semantically rich, high&#8208;level features from the decoder. This creates a &#8220;semantic gap,&#8221; forcing the model to implicitly learn to suppress irrelevant textures and noise from the detailed encoder features. To address this challenge more explicitly, we introduce the wavelet&#8208;attention refinement module (WARM). The core insight behind WARM is to first structurally disentangle the low&#8208;level features before fusing them. For this purpose, the discrete wavelet transform (DWT) is uniquely suited due to its ability to decompose a feature map into multiple sub&#8208;bands that represent different frequency components while preserving their spatial localization. This decomposition separates coarse, low&#8208;frequency structural information from fine&#8208;grained, high&#8208;frequency details (e.g., edges and textures). With features now organized into distinct sub&#8208;bands, we can leverage the high&#8208;level semantic context from the decoder as a precise guide. The subsequent attention mechanism can then selectively focus on the most task&#8208;relevant frequency bands at specific spatial locations, rather than contending with a single, convoluted feature map. This guided refinement process effectively bridges the semantic gap, leading to more accurate feature fusion and superior&#160;segmentation. As shown in Figure&#160; 7 , the WARM leverages high&#8208;level semantic information from the decoder to guide the frequency&#8208;domain refinement of low&#8208;level features, rather than performing a direct&#160;concatenation. FIGURE 7 The architecture of the proposed WARM. WARM, wavelength&#8208;attention refinement model. Assuming both the input low&#8208;level feature map from the encoder, X enc &#8712; R B &#215; C &#215; H &#215; W , and the corresponding upsampled high&#8208;level feature map from the decoder, X dec &#8712; R B &#215; C &#215; H &#215; W . The encoder feature map X enc &#8712; R B &#215; C &#215; H &#215; W is passed through a DWT. This operation decomposes it into four sub&#8208;bands, each with half the spatial resolution: one low&#8208;frequency approximation sub&#8208;band, l l , and three high&#8208;frequency detail sub&#8208;bands, l h , h l , and h h . The resulting shape for each of these four sub&#8208;bands is ( B , C , H 2 , W 2 ) . The above steps can be represented by formulas as: (15) l l , l h , h l , h h = DWT ( X enc ) &#8712; R B &#215; C &#215; H 2 &#215; W 2 . where l l , l h , h l , h h represent the low&#8208;frequency, horizontal high&#8208;frequency, vertical high&#8208;frequency, and diagonal high&#8208;frequency sub&#8208;bands,&#160;respectively. Concurrently, the decoder feature map X dec &#8712; R B &#215; C &#215; H &#215; W is downsampled via average pooling. This process matches its spatial dimensions to those of the sub&#8208;bands, resulting in a guidance signal g &#8712; R B &#215; C &#215; H 2 &#215; W 2 . The above step can be represented by formulas as: (16) g = AvgPool ( X dec ) &#8712; R B &#215; C &#215; H 2 &#215; W 2 . We define the attention gate function as A . The guidance signal g &#8712; R B &#215; C &#215; H 2 &#215; W 2 is then used to modulate each of the four sub&#8208;bands via separate attention gates. For an arbitrary input sub&#8208;band x sub and the guidance signal g , the attention gate function A can be represented by formulas as: (17) A ( x sub , g ) = x sub &#8857; &#963; ( W &#968; ( ReLU ( W x ( x sub ) + W g ( g ) ) ) ) . where W x , W g , and W &#968; are the learnable weights of 1 &#215; 1 convolutional layers, ReLU is the rectified linear unit activation function, &#963; is the sigmoid function, and &#8857; denotes element&#8208;wise multiplication. For instance, the l l &#8712; R B &#215; C &#215; H 2 &#215; W 2 sub&#8208;band and the guidance signal g &#8712; R B &#215; C &#215; H 2 &#215; W 2 are fed into the a t t n l l gate. The gate produces an attention map of shape R B &#215; 1 &#215; H 2 &#215; W 2 , which is then applied element&#8208;wise to the ll sub&#8208;band. This process does not change the sub&#8208;band's shape. The output, l l r e f i n e d , and similarly l h r e f i n e d , h l r e f i n e d , and h h r e f i n e d , all retain the shape R B &#215; C &#215; H 2 &#215; W 2 . Finally, these four refined sub&#8208;bands are recombined using an inverse discrete wavelet transform (IDWT). This transform merges the frequency components, restoring the original spatial resolution. The final output is a refined feature map, X r e f i n e d refined, with a shape of R B &#215; C &#215; H &#215; W , identical to the original input. The above step can be represented by formulas as: (18) x r e f i n e d = IDWT ( l l r e f i n e d , l h r e f i n e d , h l r e f i n e d , h h r e f i n e d ) &#215; &#8712; R B &#215; C &#215; H &#215; W . Prior boundary refinement methods 23 , 24 often use spatial gradients or edge maps to impose guidance losses on model outputs, which tends to confuse true boundaries with high&#8208;frequency artifacts (e.g., hair, skin texture, illumination seams). Our WARM differs from existing methods: we decompose features in the wavelet domain into (LL/LH/HL/HH) subbands and gate them with decoding semantics, thereby enhancing only the high&#8208;frequency subbands related to true boundaries while suppressing structured noise (such as hair and skin texture), before transforming back to the spatial domain. This subband&#8208;specific, semantics&#8208;guided refinement mechanism can significantly sharpen edges without amplifying pseudo&#8208;contours and complements MSQFormer's global modeling. Through this sequence, the WARM effectively purifies the low&#8208;level features, creating a representation that is both spatially precise and semantically informed, thereby bridging the semantic gap and significantly improve the model's segmentation accuracy at lesion&#160;boundaries. 4 EXPERIMENT SETTINGS 4.1 Dataset To evaluate the effectiveness of our method, we conducted experiments on two publicly available benchmarks from the International skin imaging collaboration challenge: ISIC 2017, which comprises 2&#160;150 dermoscopic images with corresponding segmentation masks, and ISIC 2018, which comprises 2&#160;694 labeled&#160;images. To ensure a fair comparison, we adopted the same data initialization protocol as prior works MALUNet 25 and BDFormer. 24 We split each dataset into 70% for training and 30% for testing. Concretely, in ISIC 2017 we used 1500 images for model training and 650 for evaluation, while in ISIC 2018 we allocated 1&#160;886 images to the training set and 808 to the test&#160;set. 4.2 Evaluation metrics Following the prior works MALUNet 25 and BDFormer, 24 we adopt mean intersection over union (mIoU), dice similarity score (DSC), accuracy (Acc), sensitivity (Sen), and specificity (Spe) as indicators to measure segmentation performances. The mIoU, DSC, Acc, Sen, and Spe are defined as: (19) mIoU = 1 k + 1 &#8721; i = 0 k TP TP + FP + FN (20) DSC = 2 TP 2 TP + FP + FN (21) Acc = TP + TN TP + TN + FP + FN (22) Sen = TP TP + FN (23) Spe = TN TN + FP where k is the number of target segmentation categories, TP is the true positive, TN is the true negative, FP is the false negative, and FN is the false negative pixel numbers of the&#160;result. To further evaluate the model's computational efficiency, we employ Params, which denotes the number of model parameters, measured in millions (M), and floating point operations (GFLOPs) of the model as metrics. Both Params and GFLOPs are evaluated using an input size of 256 &#215; &#160;256. 4.3 Implement details All experiments were conducted on a single NVIDIA GeForce RTX 2080Ti GPU. To ensure fair comparison, we adopt the data augmentation strategy from MALUNet, 25 including vertical flipping, horizontal flipping, and random rotation. The cross&#8208;entropy (CE) loss and dice loss are employed as the loss function. Following MALUNet, 25 we utilize the AdamW optimizer with an initial learning rate of 0.001, coupled with a cosine annealing scheduler configured with a 300&#8208;period cycle and minimum learning rate of 1e&#8208;5. Models were trained for 300 epochs with a batch size of&#160;2. 5 RESULTS 5.1 Comparison with competitive models To validate the effectiveness of our proposed method, Tables&#160; 1 , 2 , 3 present comparative results between our approach and current competitive (SOTA) methods on ISIC 2017 26 and ISIC 2018 27 datasets, including both accuracy metrics and model&#160;complexity. TABLE 1 Comparative experimental results on ISIC 2017 dataset. Model Year mIoU &#8593; DSC &#8593; Acc &#8593; Spe &#8593; Sen &#8593; UNet 7 2015 80.07&#160;&#177;&#160;0.24 88.38&#160;&#177;&#160;0.14 93.67&#160;&#177;&#160;0.08 98.04&#160;&#177;&#160;0.11 73.51&#160;&#177;&#160;0.19 UNet++ 17 2018 81.14&#160;&#177;&#160;0.23 89.11&#160;&#177;&#160;0.13 93.97&#160;&#177;&#160;0.08 97.81&#160;&#177;&#160;0.10 76.26&#160;&#177;&#160;0.17 SANet 28 2021 83.57&#160;&#177;&#160;0.25 90.74&#160;&#177;&#160;0.14 94.44&#160;&#177;&#160;0.08 95.93&#160;&#177;&#160;0.11 87.56 &#160;&#177;&#160; 0.18 TransFuse 13 2021 83.84&#160;&#177;&#160;0.24 90.91&#160;&#177;&#160;0.15 94.61&#160;&#177;&#160;0.07 96.38&#160;&#177;&#160;0.10 86.44&#160;&#177;&#160;0.16 Trans&#8208;UNet 12 2021 85.94&#160;&#177;&#160;0.23 92.20&#160;&#177;&#160;0.14 95.50&#160;&#177;&#160;0.08 97.66&#160;&#177;&#160;0.08 85.52&#160;&#177;&#160;0.17 MAL&#8208;UNet 25 2022 84.67&#160;&#177;&#160;0.24 91.41&#160;&#177;&#160;0.13 95.08&#160;&#177;&#160;0.08 97.58&#160;&#177;&#160;0.07 83.52&#160;&#177;&#160;0.18 Swin&#8208;UNet 10 2022 81.79&#160;&#177;&#160;0.25 89.54&#160;&#177;&#160;0.15 94.01&#160;&#177;&#160;0.08 96.90&#160;&#177;&#160;0.09 80.67&#160;&#177;&#160;0.16 DCSA&#8208;UNet 29 2023 84.83&#160;&#177;&#160;0.23 91.51&#160;&#177;&#160;0.12 95.15&#160;&#177;&#160;0.06 97.73&#160;&#177;&#160;0.08 83.24&#160;&#177;&#160;0.17 Xbound&#8208;Former 23 2023 84.55&#160;&#177;&#160;0.23 91.34&#160;&#177;&#160;0.12 95.01&#160;&#177;&#160;0.06 97.33&#160;&#177;&#160;0.09 84.23&#160;&#177;&#160;0.16 Focal&#8208;UNETR 30 2023 84.97&#160;&#177;&#160;0.24 91.6&#160;&#177;&#160;0.13 95.17&#160;&#177;&#160;0.07 97.58&#160;&#177;&#160;0.07 84.05&#160;&#177;&#160;0.18 MSCA&#8208;Net 31 2023 85.38&#160;&#177;&#160;0.22 91.86&#160;&#177;&#160;0.14 95.22&#160;&#177;&#160;0.07 97.03&#160;&#177;&#160;0.09 86.88&#160;&#177;&#160;0.16 I2U&#8208;Net 32 2024 85.68&#160;&#177;&#160;0.25 92.04&#160;&#177;&#160;0.15 95.47&#160;&#177;&#160;0.08 98.02&#160;&#177;&#160;0.08 83.70&#160;&#177;&#160;0.15 CSCA&#8208;UNet 33 2024 85.57&#160;&#177;&#160;0.26 91.98&#160;&#177;&#160;0.16 95.32&#160;&#177;&#160;0.08 97.27&#160;&#177;&#160;0.11 86.34&#160;&#177;&#160;0.18 BDFormer 24 2025 85.94&#160;&#177;&#160;0.22 92.21&#160;&#177;&#160;0.14 95.46&#160;&#177;&#160;0.07 97.40&#160;&#177;&#160;0.12 86.51&#160;&#177;&#160;0.18 HCViT&#8208;Net(Ours) &#8212; 87.76 &#160;&#177;&#160; 0.19 93.30 &#160;&#177;&#160; 0.12 96.20 &#160;&#177;&#160; 0.05 98.60 &#160;&#177;&#160; 0.04 85.13&#160;&#177;&#160;0.13 Note : Bold is the best and underline is the second&#160;best. John Wiley &amp; Sons, Ltd. TABLE 2 Comparative experimental results on ISIC 2018 dataset. Model Year mIoU &#8593; DSC &#8593; Acc &#8593; Spe &#8593; Sen &#8593; UNet 7 2015 80.72&#160;&#177;&#160;0.25 88.85&#160;&#177;&#160;0.28 93.58&#160;&#177;&#160;0.07 96.56&#160;&#177;&#160;0.12 79.82&#160;&#177;&#160;0.21 UNet++ 17 2018 82.11&#160;&#177;&#160;0.26 89.76&#160;&#177;&#160;0.31 94.27&#160;&#177;&#160;0.08 97.73 &#160;&#177;&#160; 0.13 78.31&#160;&#177;&#160;0.23 SANet 28 2021 83.52&#160;&#177;&#160;0.27 90.73&#160;&#177;&#160;0.29 94.21&#160;&#177;&#160;0.07 97.05&#160;&#177;&#160;0.14 82.69&#160;&#177;&#160;0.27 TransFuse 13 2021 83.20&#160;&#177;&#160;0.20 90.47&#160;&#177;&#160;0.27 94.61&#160;&#177;&#160;0.08 97.66&#160;&#177;&#160;0.16 80.53&#160;&#177;&#160;0.26 TransUNet 12 2021 85.19&#160;&#177;&#160;0.24 91.84&#160;&#177;&#160;0.23 93.94&#160;&#177;&#160;0.09 96.52&#160;&#177;&#160;0.15 86.23&#160;&#177;&#160;0.27 MALUNet 25 2022 84.97&#160;&#177;&#160;0.22 91.71&#160;&#177;&#160;0.26 93.85&#160;&#177;&#160;0.05 96.51&#160;&#177;&#160;0.12 85.89&#160;&#177;&#160;0.27 Swin&#8208;UNet 10 2022 83.11&#160;&#177;&#160;0.20 90.42&#160;&#177;&#160;0.27 94.55&#160;&#177;&#160;0.09 97.44&#160;&#177;&#160;0.16 81.17&#160;&#177;&#160;0.25 DCSAU&#8208;Net 29 2023 84.57&#160;&#177;&#160;0.20 91.46&#160;&#177;&#160;0.20 93.65&#160;&#177;&#160;0.06 96.28&#160;&#177;&#160;0.11 85.79&#160;&#177;&#160;0.23 Xbound&#8208;Former 23 2023 85.12&#160;&#177;&#160;0.23 91.80&#160;&#177;&#160;0.22 93.91&#160;&#177;&#160;0.06 96.45&#160;&#177;&#160;0.10 86.29&#160;&#177;&#160;0.21 FocalUNETR 30 2023 84.97&#160;&#177;&#160;0.21 91.71&#160;&#177;&#160;0.20 93.82&#160;&#177;&#160;0.06 96.28&#160;&#177;&#160;0.14 86.50&#160;&#177;&#160;0.27 MSCA&#8208;Net 31 2023 85.24&#160;&#177;&#160;0.23 91.87&#160;&#177;&#160;0.21 93.98&#160;&#177;&#160;0.05 96.65&#160;&#177;&#160;0.12 85.99&#160;&#177;&#160;0.21 I2U&#8208;Net 32 2024 85.66&#160;&#177;&#160;0.22 92.13&#160;&#177;&#160;0.26 94.14&#160;&#177;&#160;0.05 96.49&#160;&#177;&#160;0.12 87.08&#160;&#177;&#160;0.27 BDFormer 24 2025 86.28&#160;&#177;&#160;0.24 92.51&#160;&#177;&#160;0.22 94.35&#160;&#177;&#160;0.07 96.10&#160;&#177;&#160;0.13 89.14 &#160;&#177;&#160; 0.22 HCViT&#8208;Net(Ours) &#8212; 87.45 &#160;&#177;&#160; 0.18 93.19 &#160;&#177;&#160; 0.19 94.95 &#160;&#177;&#160; 0.05 97.26&#160;&#177;&#160;0.09 88.05&#160;&#177;&#160;0.21 Note : Bold is the best and underline is the second&#160;best. John Wiley &amp; Sons, Ltd. TABLE 3 Comparison of Params, FLOPs, and Inference Time (IT) at Pi5. Model Year Params &#8595; FLOPs &#8595; IT@Pi5 &#8595; UNet 7 2015 31.04M 48.33G 5.0s UNet++ 17 2018 36.62M 138.37G 14.3s SANet 28 2021 23.90M 11.99G 1.2s TransFuse 13 2021 31.87M 12.65G 2.3s TransUNet 12 2021 105.32M 38.55G 4.8s Swin&#8208;UNet 10 2022 27.17M 9.42G 1.3s I2U&#8208;Net 32 2024 29.65M 8.42G 0.9s CSCA&#8208;UNet 32 2024 35.28M 11.94G 1.3s BDFormer 24 2025 103.83M 54.31G 5.6s HCViT&#8208;Net(Ours) &#8212; 5.76M 7.51G 0.8s Note : Bold is the best and underline is the second&#160;best. John Wiley &amp; Sons, Ltd. 5.1.1 Results on ISIC 2017 dataset As evidenced by the comprehensive experimental results on the ISIC 2017 dataset presented in Table&#160; 1 , our proposed HCViT&#8208;Net establishes new SOTA performance across critical evaluation metrics such as mIoU, DSC, Acc, and Spe when compared to 14 prominent medical image segmentation models spanning a decade of research (2015&#8211;2025). HCViT&#8208;Net achieves a breakthrough mIoU of 87.76%, surpassing the previous best performer BDFormer 24 (85.94%) by a margin of 1.82%. In terms of DSC, our model attains 93.30%, exceeding BDFormer's 24 best result of 92.21% by 1.09% improvement. It is noteworthy that Table&#160; 3 presents a comparison of complexity between our proposed model and other models. Compared with the previous SOTA method BDFormer, 24 our model uses only 5.5% of its parameters (5.76M vs. 103.83M) and 13.8% of its computational cost (7.51G vs. 54.31G). The exceptional results highlight HCViT&#8208;Net's significant advancements over diverse architectural paradigms. Our approach outperforms ViT&#8208;based models such as Swin&#8208;UNet 10 (+5.97% mIoU) and TransFuse 13 (+3.92% mIoU), while also exceeding attention&#8208;enhanced CNN variants like SANet 28 (+4.19% mIoU). Furthermore, HCViT&#8208;Net demonstrates clear superiority over the latest innovations in the field, outperforming I2U&#8208;Net 32 by 2.08%. Particularly noteworthy is our model's exceptional boundary segmentation capability, evidenced by the 3.21% mIoU advantage over the recent boundary&#8208;focused method Xbound&#8208;Former. 23 5.1.2 Results on ISIC 2018 dataset As demonstrated by the extensive comparative analysis in Table&#160; 2 , our proposed HCViT&#8208;Net establishes new SOTA performance on the ISIC 2018 dataset, surpassing 16 leading medical image segmentation models spanning 2015&#8211;2025. The model achieves a breakthrough mIoU of 87.45%, improving upon the previous best performer BDFormer 24 (86.28%) by a significant margin of 1.17%. In terms of DSC, HCViT&#8208;Net reaches 93.19%, exceeding BDFormer's 24 92.51% by 0.68%. While BDFormer 24 achieves the highest sensitivity (89.14%), HCViT&#8208;Net delivers a competitive second&#8208;best sensitivity of 88.05% while maintaining strong specificity (97.26%)&#8212;demonstrating superior clinical balance compared to models like UNet++ 17 which achieved 97.73% specificity but only 78.31% sensitivity. The results highlight HCViT&#8208;Net's dominance across diverse architectural paradigms. Our approach shows remarkable gains over transformer&#8208;based models, exceeding Swin&#8208;UNet 10 by 4.34% in mIoU and TransFuse 13 by 4.25%. It also outperforms CNN innovations SANet 28 (+3.93%). Against recent competitive methods, HCViT&#8208;Net surpasses 2024's I2U&#8208;Net 32 by 1.79% in mIoU, while achieving a 2.33% advantage over boundary&#8208;focused Xbound&#8208;Former. 23 These results establish HCViT&#8208;Net as the new reference architecture for skin lesion segmentation, achieving unprecedented harmony between localization precision, diagnostic reliability, and clinical safety through its advanced feature learning&#160;capabilities. 5.1.3 Comparison of Params, FLOPs, and Inference Time As shown in Table&#160; 3 , compared to other SOTA methods, our HCViT&#8208;Net demonstrates significant advantages in terms of both model parameters, computational complexity and inference tTime (Deployed on Raspberry Pi5 with an input size of 256 &#215; 256). HCViT&#8208;Net achieves a remarkably low parameter count of 5.76M (bold, best), which is the lowest among all listed methods. This represents a 75.9% reduction compared to the previous most parameter&#8208;efficient model SANet 28 (underlined second best at 15.90M parameters). In terms of computational complexity, HCViT&#8208;Net also sets a new benchmark with FLOPs of only 7.51G (bold, best). Compared to the previous most efficient model I2U&#8208;Net 32 (underlined second best at 8.42G FLOPs), our model reduces computational complexity by 10.8%. These results confirm that HCViT&#8208;Net achieves unprecedented efficiency in lightweight design, delivering SOTA performance while requiring substantially fewer computational resources &#8212;making it exceptionally suitable for resource&#8208;constrained devices and real&#8208;time applications. In Table&#160; 4 , we present a statistical analysis of the parameter counts and computational complexity (FLOPs) of each component in the model. As shown, the Encoder stage accounts for 79.89% of the total parameters (4.63M), with the MSFormer component contributing 73.43% (4.26M). This is primarily attributed to the high&#8208;dimensional feature representations in the 128&#8208;channel layers of the Encoder. In contrast, the Decoder stage comprises only 1.12M parameters (19.34%), reflecting the asymmetry between feature extraction and reconstruction in the encoder&#8211;decoder architecture. Although the WARM module has the fewest parameters (0.04M, 0.76%), it plays a crucial role in feature refinement and edge enhancement. In terms of FLOPs distribution, the computational load is relatively balanced between the Encoder and Decoder, accounting for 50.83% (3.82G) and 48.97% (3.68G), respectively. Notably, the MSFormer component contributes 49.21% and 47.58% of the computation within the Encoder and Decoder, respectively, confirming the central role of the multi&#8208;head self&#8208;attention mechanism in the model's computational core. The WARM module incurs minimal computational cost (0.03G, 0.34%), indicating that the wavelet&#8208;based attention refinement mechanism effectively enhances feature quality while maintaining computational efficiency. This distribution of parameters and computation aligns with the characteristics of medical image segmentation tasks&#8212;rich semantic feature extraction through the Encoder, precise pixel&#8208;level reconstruction in the Decoder, and improved feature fusion quality from the WARM module without significantly increasing computational&#160;overhead. TABLE 4 Parameter count and FLOPs analysis of HCViT&#8208;Net components. Stage Parameters(M) Param ratio(%) FLOPs(G) FLOP ratio(%) Encoder 4.63 79.89 3.82 50.83 CNN 0.11 1.95 0.05 0.67 MSFormer 4.26 73.43 3.70 49.21 Decoder 1.12 19.34 3.68 48.97 CNN 0.07 1.14 0.10 1.39 MSFormer 1.06 18.20 3.58 47.58 WARM 0.04 0.76 0.03 0.34 Complete model 5.76 100.00 7.51 100.00 John Wiley &amp; Sons, Ltd. 5.2 Ablation studies 5.2.1 The effect of each component in HCViT&#8208;Net Table&#160; 5 presents a systematic ablation study evaluating the contribution of each component in HCViT&#8208;Net. On the ISIC 2017 dataset, the complete model achieves competitive performance (87.76% mIoU, 93.30% DSC, 96.20% Acc). Removal of the MSQFormer module causes the most significant degradation, reducing mIoU by 4.08%&#8211;83.68% and DSC by 2.54%, underscoring its fundamental role in feature extraction. Eliminating the CNN block results in a 1.86% mIoU reduction, indicating the importance of local feature modeling. The absence of the WARM module preserves relatively strong performance (86.63% mIoU). For ISIC 2018, similar trends emerge: The intact HCViT&#8208;Net maintains optimal metrics (87.45% mIoU, 93.19% DSC, 94.95% Acc), while MSQFormer removal again causes the most severe performance drop (4.18% mIoU reduction). Notably, eliminating the CNN block disproportionately affects accuracy (0.68% decrease to 94.27%), whereas WARM ablation primarily impacts DSC (0.57% reduction). These findings demonstrate that while MSQFormer blocks are essential for high performance, the WARM module's comprehensive stage&#8208;wise integration is crucial for maximizing segmentation&#160;accuracy. TABLE 5 Ablation study of the effect of each component in HCViT&#8208;Net. Dataset Method mIoU &#8593; DSC &#8593; Acc &#8593; ISIC 2017 HCViT&#8208;Net(Ours) 87.76 &#160;&#177;&#160; 0.19 93.30 &#160;&#177;&#160; 0.22 96.20 &#160;&#177;&#160; 0.05 w/o MSQFormer 83.68&#160;&#177;&#160;0.21 90.76&#160;&#177;&#160;0.23 94.92&#160;&#177;&#160;0.06 w/o CNN 85.90&#160;&#177;&#160;0.20 92.70&#160;&#177;&#160;0.22 95.52&#160;&#177;&#160;0.05 w/o WARM 86.63&#160;&#177;&#160;0.21 92.92&#160;&#177;&#160;0.22 95.79&#160;&#177;&#160;0.05 ISIC 2018 HCViT&#8208;Net(Ours) 87.45 &#160;&#177;&#160; 0.18 93.19 &#160;&#177;&#160; 0.19 94.95 &#160;&#177;&#160; 0.05 w/o MSQFormer 83.27&#160;&#177;&#160;0.19 90.55&#160;&#177;&#160;0.23 94.27&#160;&#177;&#160;0.06 w/o CNN 86.26&#160;&#177;&#160;0.20 92.51&#160;&#177;&#160;0.22 94.27&#160;&#177;&#160;0.06 w/o WARM 86.47&#160;&#177;&#160;0.22 92.62&#160;&#177;&#160;0.23 94.44&#160;&#177;&#160;0.07 Abbreviation: w/o,&#160;without. John Wiley &amp; Sons, Ltd. In Figure&#160; 8 , we verified the contribution of MSQFormer to modeling the global context through interpretable visualizations. Concretely, we removed MSQFormer from the model (w/o MSQFormer), compared it with the full model that includes this module (with MSQFormer) under identical training and evaluation protocols, and applied Grad&#8208;CAM to the final segmentation predictions to generate heatmaps. The accompanying figure&#160;presents several representative examples. From the visualizations, adding MSQFormer yields continuous and well&#8208;covered high responses across the entire lesion region in the Grad&#8208;CAM maps, attending to both the center and the periphery. In contrast, removing the module leads to fragmented and locally biased activations that are often attracted to high&#8208;contrast textures while neglecting low&#8208;contrast or boundary areas. Regarding boundaries, the model with MSQFormer is more stable at fuzzy or irregular edges, and its predicted contours align more closely with the annotations; conversely, without the module, the predictions tend to bleed into background or miss concave parts, and the boundary responses in the heatmaps become more discontinuous. This difference is especially pronounced for lesions with strong appearance heterogeneity: with MSQFormer, the model treats the lesion as a coherent whole, whereas without it, attention concentrates on the most salient subregions, resulting in incomplete masks. Mechanistically, MSQFormer introduces long&#8208;range dependencies explicitly via multi&#8208;scale query aggregation, aligning coarse&#8208;scale semantics with fine&#8208;scale boundary cues and re&#8208;weighting encoder features in a query&#8208;guided manner to suppress background clutter and unify lesion responses. This matches the Grad&#8208;CAM evidence of more uniform, coherent lesion activations and clearer, context&#8208;consistent boundaries. Consistent quantitative results further corroborate these findings: under the same settings, incorporating MSQFormer yields improvements in mIoU of approximately 4.08% on ISIC 2017 and 4.18% on ISIC 2018. In summary, the Grad&#8208;CAM comparisons clearly demonstrate that MSQFormer enables the model to effectively capture and exploit global context, thereby improving segmentation&#160;quality. FIGURE 8 Qualitative comparison of our full model (with MSQFormer) and its ablation variant (w/o MSQFormer) on four challenging cases. From left to right, the columns display the original dermoscopy image, the Grad&#8208;CAM attention heatmap, and the final segmentation results. In the &#8221;Results&#8221; column, the green contour represents the ground truth, while the blue contour indicates the model's prediction. MSQFormer, multi&#8208;scale query transformer. In Figure&#160; 9 , we illustrate how the proposed WARM enhances lesion boundary precision by comparing the full model (with WARM) against an ablated variant (w/o WARM) under identical training and evaluation conditions, using Grad&#8208;CAM heatmaps for visualization. As shown, the model with WARM produces heatmaps with tighter, more continuous, and higher&#8208;intensity response bands that closely follow the true lesion contours, accurately capturing concave and irregular boundary structures. In contrast, removing WARM results in diffuse and unstable activations that often spill beyond the lesion edge or become over&#8208;smoothed, leading to boundary dilation, fragmentation, or omission, especially along low&#8208;contrast regions. Moreover, WARM effectively suppresses high&#8208;frequency background interference&#8212;such as hair, vascular textures, and illumination variations&#8212;keeping attention concentrated around genuine lesion boundaries. The w/o WARM model, however, tends to misfocus on these pseudo&#8208;edges, producing boundary expansion or spurious&#160;protrusions. FIGURE 9 Qualitative comparison of our full model (with WARM) and its ablation variant (w/o WARM) on four challenging cases. From left to right, the columns display the original dermoscopy image, the Grad&#8208;CAM attention heatmap, and the final segmentation results. In the &#8221;Results&#8221; column, the green contour represents the ground truth, while the blue contour indicates the model's prediction. WARM, wavelength&#8208;guided attention refinement module. The benefit is particularly evident in challenging cases with low boundary contrast or strong internal heterogeneity: WARM yields more coherent and stable edge responses across the lesion, resulting in segmentation masks that align more precisely with ground truth, whereas the ablated variant exhibits fragmented activations and weakened edge energy. Mechanistically, WARM decomposes encoder features into frequency subbands&#8212;low&#8208;frequency structural (LL) and high&#8208;frequency detail (LH/HL/HH) components&#8212;via wavelet transform, and selectively enhances high&#8208;frequency boundary cues using decoder&#8208;guided gating. It then reconstructs spatial features through an inverse wavelet transform, recovering sharper and more continuous lesion boundaries. This behavior aligns with the Grad&#8208;CAM observations of &#8220;ring&#8208;like activations following true contours, reduced background misactivations, and tighter predicted edges.&#8221; We further employed the HD95 metric to quantitatively evaluate the improvement in boundary refinement brought by the WARM module. As shown in Table&#160; 6 , adding WARM yields an HD95 improvement of 1.22 on ISIC 2017 and 1.01 on ISIC 2018, confirming that WARM substantially enhances boundary focus and robustness, leading to more accurate and stable segmentation performance. WARM, wavelet&#8208;attention refinement module; ISIC, International skin imaging&#160;collaboration. TABLE 6 Quantitative comparison of lesion boundary segmentation accuracy (HD95) with WARM. Dataset Method HD95 &#8595; ISIC 2017 with WARM 16.01 &#160;&#177;&#160; 1.73 w/o WARM 17.23&#160;&#177;&#160;1.69 ISIC 2018 with WARM 16.27 &#160;&#177;&#160; 1.67 w/o WARM 17.28&#160;&#177;&#160;1.77 John Wiley &amp; Sons, Ltd. 5.2.2 The effect of appending position of MSQFormer block As demonstrated in Table&#160; 7 , appending MSQFormer across all stages consistently outperforms encoder&#8208;only and decoder&#8208;only strategies. On ISIC 2017, the full&#8208;stage approach (87.76% mIoU) substantially exceeds encoder&#8208;only (85.58%) and decoder&#8208;only (86.06%) configurations by 2.16% and 1.70% mIoU, respectively. The same superiority holds for ISIC 2018: the full&#8208;stage model (87.45% mIoU) stably outperforms both encoder&#8208;only (85.29%) and decoder&#8208;only (86.33%) variants with margins of 1.96% and 1.12%. As shown in Figure&#160; 11 , the GradCAM 34 heatmaps of the CNN block and MSQFormer block in the first block of HCViT&#8208;Net reveal that the CNN, constrained by its local receptive field, struggles to focus on the entire target object during the early model stage. In contrast, the MSQFormer, leveraging its global receptive field, can precisely focus on the entire lesion area. This evidence confirms that the full&#8208;stage integration of global information into CNNs yields optimal performance improvements, validating the efficacy of our HCViT&#8208;Net architectural design. TABLE 7 Ablation study of the effect of appending position of MSQFormer block. Dataset Method mIoU &#8593; DSC &#8593; Acc &#8593; ISIC 2017 All Stages 87.76 &#160;&#177;&#160; 0.19 93.30 &#160;&#177;&#160; 0.22 96.20 &#160;&#177;&#160; 0.05 No appending 83.68&#160;&#177;&#160;0.21 90.76&#160;&#177;&#160;0.23 94.92&#160;&#177;&#160;0.06 Only Encoder 85.58&#160;&#177;&#160;0.21 91.98&#160;&#177;&#160;0.22 95.31&#160;&#177;&#160;0.05 Only Decoder 86.06&#160;&#177;&#160;0.23 92.27&#160;&#177;&#160;0.23 95.59&#160;&#177;&#160;0.06 ISIC 2018 All Stages 87.45 &#160;&#177;&#160; 0.18 93.19 &#160;&#177;&#160; 0.19 94.95 &#160;&#177;&#160; 0.05 No appending 83.27&#160;&#177;&#160;0.19 90.55&#160;&#177;&#160;0.23 94.27&#160;&#177;&#160;0.06 Only Encoder 85.29&#160;&#177;&#160;0.21 91.91&#160;&#177;&#160;0.23 93.97&#160;&#177;&#160;0.07 Only Decoder 86.33&#160;&#177;&#160;0.22 92.53&#160;&#177;&#160;0.24 94.44&#160;&#177;&#160;0.07 John Wiley &amp; Sons, Ltd. 5.2.3 The efficacy of multi&#8208;scale query mechanisms in MSQFormer block As shown in Table&#160; 8 , on ISIC 2017 dataset, models with multi&#8208;scale queries achieved 87.47% mIoU, 93.13% DSC, and 96.01% Acc &#8211; outperforming single&#8208;scale baselines by 0.90%, 0.48%, and 0.08%, respectively. This advantage intensified on ISIC 2018, where multi&#8208;scale configuration delivered gains of 0.81% mIoU, 0.47% DSC, and 0.52% Acc. Building upon Figure&#160; 10 and 11 , we further visualize the GradCAM 34 heatmaps of multi&#8208;scale queries within the MSQFormer block, revealing differences in the model's attention granularity across scales. This demonstrates the superior efficacy of multi&#8208;scale queries in skin lesion&#160;segmentation. TABLE 8 Ablation study of the efficacy of multi&#8208;scale query mechanisms in MSQFormer block. Dataset Method mIoU &#8593; DSC &#8593; Acc &#8593; ISIC 2017 Multi&#8208;scale Query 87.47 &#160;&#177;&#160; 0.19 93.13 &#160;&#177;&#160; 0.12 96.01 &#160;&#177;&#160; 0.05 Single&#8208;scale Query 86.57&#160;&#177;&#160;0.21 92.65&#160;&#177;&#160;0.14 95.93&#160;&#177;&#160;0.06 ISIC 2018 Multi&#8208;scale Query 87.45 &#160;&#177;&#160; 0.18 93.19 &#160;&#177;&#160; 0.19 94.95 &#160;&#177;&#160; 0.05 Single&#8208;scale Query 86.64&#160;&#177;&#160;0.21 92.72&#160;&#177;&#160;0.20 94.43&#160;&#177;&#160;0.06 John Wiley &amp; Sons, Ltd. FIGURE 10 The heatmaps of the CNN block and MSQFormer block in the first stage of HCViT&#8208;Net. FIGURE 11 The heatmaps of multi&#8208;scale queries within the MSQFormer block. 5.3 Visualization of prediction results To further validate the effectiveness of our proposed approach, Figures&#160; 12 and 13 present a qualitative comparison of prediction results between our HCViT&#8208;Net and current SOTA models UNet, 7 UNet++, 17 TransUNet, 12 CSCAUNet, 33 and BDFormer. 24 FIGURE 12 Qualitative comparisons between ours and other models on the ISIC 2017 dataset. FIGURE 13 Qualitative comparisons between ours and other models on the ISIC 2018 dataset. ISIC, International skin imaging collaboration. The visual evidence in these two figures&#160;elaborates on these strengths: the first is our model's superior grasp of global dependencies, which significantly reduces misjudgments in large&#8208;scale areas. Concurrently, the second is that, by synergistically combining the strengths of ViT and CNN architectures, our HCViT&#8208;Net achieves enhanced detail processing capabilities compared to prior&#160;methods. 6 DISCUSSION In this study, we introduced HCViT&#8208;Net, a novel deep learning framework for skin lesion segmentation, and demonstrated its competitive performance on the challenging ISIC 2017 and ISIC 2018 datasets. The results confirm that our hybrid approach, which marries the local feature strengths of CNNs with the global context capabilities of Transformers, is highly effective for this complex medical imaging&#160;task. The superior performance of HCViT&#8208;Net stems from two key architectural innovations. The first is MSQFormer, which enables the synergistic fusion of local CNN features and global ViT context at all stages of the network. By efficiently compressing the key/value tensors, MSQFormer significantly reduces the complexity of the self&#8208;attention mechanism, allowing its integration throughout the entire network backbone and ensuring robust representation learning from shallow to deep layers. In addition, our proposed WARM effectively bridges the semantic gap between encoder and decoder features, resulting in more precise boundary delineation and improved segmentation accuracy. This two&#8208;stage process&#8212;robust feature fusion followed by targeted refinement&#8212;is the primary reason for the model's high accuracy, which achieved an mIoU of 87.76% on the ISIC 2017 benchmark and 87.45% on the ISIC 2018 benchmark, establishing it as a highly competitive tool for dermatological image&#160;analysis. From a clinical perspective, the implications of an accurate and efficient segmentation tool like HCViT&#8208;Net are significant. Automated, reliable lesion measurement provides a robust foundation for the quantitative analysis of the &#8220;ABCD&#8221; rule (asymmetry, border, color, diameter), bolstering the objectivity and confidence of clinical decision&#8208;making. Furthermore, the model's computational efficiency is not merely a technical advantage but a critical enabler for practical adoption. Its lightweight nature makes HCViT&#8208;Net a prime candidate for integration into standard dermatological software or even mobile health applications, where it could serve as a real&#8208;time &#8220;second opinion&#8221; for clinicians without requiring specialized, high&#8208;cost hardware. This could help accelerate diagnostic workflows and facilitate longitudinal monitoring of suspicious&#160;lesions. Beyond dermoscopy, we believe the full approach (MSQFormer + WARM) is, in principle, broadly transferable, while being particularly advantageous for dermoscopic segmentation due to its close alignment with the characteristic properties of dermoscopic images. From a general applicability perspective, MSQFormer's multi&#8208;scale queries and global modeling reliably capture long&#8208;range dependencies and multi&#8208;scale structures, making it suitable for medical scenarios with pronounced object size variability (e.g., polyps, glands, retinal vessels, and certain organ/tumor segmentations). Its query&#8208;based fusion mechanism is also relatively backbone&#8208;agnostic, facilitating integration into UNet&#8208;like or hybrid CNN&#8211;Transformer encoders. Meanwhile, WARM employs wavelet&#8208;domain decomposition and semantic gating to selectively enhance high&#8208;frequency boundary cues and suppress structured noise in the frequency domain; this boundary refinement strategy is modality&#8211;agnostic in principle. Notably, in dermoscopic tasks the combination exhibits a stronger synergy: lesions often present rich, irregular, and low&#8208;contrast boundary textures, alongside high&#8208;frequency artifacts such as hair, skin texture, and uneven illumination. WARM strengthens true edges while suppressing spurious activations, whereas MSQFormer preserves global shape and contextual consistency amid pronounced multi&#8208;scale variability&#8212;together forming a complementary &#8220;global&#8211;local&#8221; pairing. For modalities with smoother, plateau&#8208;like boundary transitions (e.g., some MRI organs), the gains from WARM may be more modest; in such cases, performance can be tuned by adjusting the wavelet basis, the decomposition level J , and the gating strength. More generally, we anticipate positive yet task&#8208;dependent gains across other medical segmentation problems, which can be further improved through lightweight adaptations, including wavelet&#8208;level selection, gating calibration, and 3D&#160;extensions. Nevertheless, this study has several limitations that must be acknowledged. First, the training and validation were performed exclusively on dermoscopic images, which are acquired under controlled lighting and magnification. The model's performance on standard clinical photographs, which exhibit greater variability, has not yet been assessed. To enhance generalizability for real&#8208;world primary care settings, future work should incorporate these more diverse image types. These limitations define clear avenues for our future research, which will also include extending the framework to perform simultaneous lesion segmentation and&#160;classification. 7 CONCLUSIONS In this paper, we presented HCViT&#8208;Net, a novel hybrid architecture designed to address the core challenges of skin lesion segmentation. Its primary innovation, the MSQFormer, enables the synergistic fusion of local CNN features and global ViT context at all network stages. By efficiently compressing key/value tensors, MSQFormer facilitates powerful self&#8208;attention with significantly reduced complexity, allowing its integration throughout the entire network backbone. This ensures robust representation learning from shallow to deep layers. Furthermore, our WARM effectively mitigates the semantic gap between encoder and decoder features, leading to more refined boundary delineation and enhanced segmentation accuracy. Experimental results on the public ISIC 2017 and ISIC 2018 benchmark datasets confirm that HCViT&#8208;Net achieves competitive segmentation accuracy, providing a more reliable foundation for subsequent automated analysis of diagnostic features like asymmetry and border irregularity. Crucially, this high performance is achieved with notable computational efficiency, making HCViT&#8208;Net a prime candidate for integration into clinical workflows. It holds the potential to serve as a real&#8208;time, reliable &#8220;second opinion&#8221; for dermatologists, helping to accelerate diagnostic procedures without the need for specialized, high&#8208;cost&#160;hardware. AUTHOR CONTRIBUTIONS Wei Jiao : Conceptualization; methodology; software; formal analysis; investigation; data curation; writing original draft preparation; writing review and editing; visualization. Jianghui Xu : Conceptualization; methodology; formal analysis; writing review and editing. Yijiao Fang : Methodology; Formal analysis; resources; writing review and editing. Jiaojiao Huang : Software; data curation. Yujie Zhu : Writing&#8212;review and editing. Dandan Ling : Conceptualization; supervision; writing&#8212;review and editing. All authors have read and agreed to the published version of the&#160;manuscript. CONFLICT OF INTEREST STATEMENT The authors declare no conflicts of&#160;interest. ACKNOWLEDGMENTS The authors have nothing to report. DATA AVAILABILITY STATEMENT The code of our paper is available at: https://github.com/lddFDU/HCViT&#8208;Net . REFERENCES 1 Bray F , Laversanne M , Sung H , et&#160;al. Global cancer statistics 2022: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries . CA Cancer J Clin . 2024 ; 74 : 229 &#8208; 263 . 38572751 10.3322/caac.21834 2 Tschandl P , Codella N , Cabo H , et&#160;al. Comparison of the accuracy of human readers versus machine&#8208;learning algorithms for pigmented skin lesion classification: an open, web&#8208;based, international, diagnostic study . Lancet Oncol . 2019 ; 20 : 938 &#8208; 947 . 31201137 10.1016/S1470-2045(19)30333-X PMC8237239 3 Brinker TJ , Hekler A , Enk AH , et&#160;al. Deep learning outperformed 136 of 157 dermatologists in a head&#8208;to&#8208;head dermoscopic melanoma image classification task . Eur J Cancer . 2019 ; 113 : 47 &#8208; 54 . 30981091 10.1016/j.ejca.2019.04.001 4 LeCun Y , Bengio Y , Hinton G . Deep learning . Nature . 2015 ; 521 : 436 &#8208; 444 . 26017442 10.1038/nature14539 5 Long J , Shelhamer E , Darrell T . Fully convolutional networks for semantic segmentation . In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2015 ; 39 : 3431 &#8208; 3440 . 10.1109/TPAMI.2016.2572683 27244717 6 Chen L&#8208;C , Papandreou G , Kokkinos I , Murphy K , Yuille AL . Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs . IEEE Trans Pattern Anal Mach Intell . 2017 ; 40 : 834 &#8208; 848 . 28463186 10.1109/TPAMI.2017.2699184 7 Ronneberger O , Fischer P , Brox T . U&#8208;net: Convolutional networks for biomedical image segmentation . In: Medical Image Computing and Computer&#8208;Assisted Intervention&#8211;MICCAI 2015 . Lecture Notes in Computer Science, vol 9351. Springer , Cham ; 2015 : 234 &#8208; 241 . 8 Dosovitskiy A , Beyer L , Kolesnikov A , et&#160;al. An image is worth 16x16 words: Transformers for image recognition at scale . In: International Conference on Learning Representations (ICLR) 2021 . 9 Liu Z , Lin Y , Cao Y , et&#160;al. Swin transformer: Hierarchical vision transformer using shifted windows . In: Proceedings of the IEEE/CVF International Conference on Computer Vision 2021 : 10012 &#8208; 10022 . 10 Cao H , Wang Y , Chen J , et&#160;al. Swin&#8208;unet: Unet&#8208;like pure transformer for medical image segmentation . In: European Conference on Computer Vision Springer ; 2022 : 205 &#8208; 218 . 11 Rekha R , Shruti P , Deekshitha M , Akash J . DCSwin&#8208;UNet: Dual Encoder U&#8208;Net based on CNN and Swin Transformer with trainable multiplication layer for brain tumor segmentation from MRI images . Biomed Signal Process Control . 2025 ; 110 : 108325 . 12 Chen J , Mei J. , Li X. , et&#160;al. TransUNet: Rethinking the U&#8208;Net architecture design for medical image segmentation through the lens of transformers . Med Image Anal . 2024 ; 97 : 103280 . 39096845 10.1016/j.media.2024.103280 13 Zhang Y , Liu H , Hu Q . Transfuse: Fusing transformers and cnns for medical image segmentation . In: Medical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part I 24 Springer ; 2021 : 14 &#8208; 24 . 14 Wang X , Girshick R , Gupta A , He K . Non&#8208;local neural networks . In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018 : 7794 &#8208; 7803 . 15 Chen X , Liu X , Yu Y , et&#160;al. Multi&#8208;scale information residual network: Deep residual network of prostate cancer segmentation based on multi scale information guidance . Biomed Signal Process Control . 2025 ; 110 : 108132 . 16 Chang H&#8208;H , Chou Y&#8208;X , Chao C&#8208;C , Hsieh S&#8208;T . Multiscale convolution block U&#8208;Net for automatic epidermis segmentation in immunofluorescence images . Biomed Signal Process Control . 2025 ; 109 : 108027 . 17 Zhou Z , Rahman Siddiquee MM , Tajbakhsh N , Liang J . Unet++: A nested u&#8208;net architecture for medical image segmentation . In: Deep learning in Med. Image Anal. and multimodal learning for clinical decision support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML&#8208;CDS 2018, held in conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4 Springer ; 2018 : 3 &#8208; 11 . 10.1007/978-3-030-00889-5_1 PMC7329239 32613207 18 Zheng S , Lu J , Zhao H , et&#160;al. Rethinking semantic segmentation from a sequence&#8208;to&#8208;sequence perspective with transformers . In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2021 : 6881 &#8208; 6890 . 19 Peng Z , Huang W , Gu S , et&#160;al. Conformer: Local features coupling global representations for visual recognition . In: Proceedings of the IEEE/CVF International Conference on Computer Vision 2021 : 367 &#8208; 376 . 20 Xie Y , Zhang J , Shen C , Xia Y . Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation . In: Medical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part III 24 Springer ; 2021 : 171 &#8208; 180 . 21 Howard AG , Zhu M , Chen B , et&#160;al. MobileNets: Efficient convolutional neural networks for mobile vision applications . In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 2017 : 4686 &#8208; 4697 . 22 Wang W , Xie E , Li X , et&#160;al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions . In: Proceedings of the IEEE/CVF International Conference on Computer Vision 2021 : 568 &#8208; 578 . 23 Wang J , Chen F , Ma Y , et&#160;al. Xbound&#8208;former: Toward cross&#8208;scale boundary modeling in transformers . IEEE Trans Med Imaging . 2023 ; 42 : 1735 &#8208; 1745 . 37018671 10.1109/TMI.2023.3236037 24 Ji Z , Ye Y , Ma X . BDFormer: Boundary&#8208;aware dual&#8208;decoder transformer for skin lesion segmentation . Artif Intell Med . 2025 ; 103079 . 39983372 10.1016/j.artmed.2025.103079 25 Ruan J , Xiang S , Xie M , Liu T , Fu Y . Malunet: A multi&#8208;attention and light&#8208;weight unet for skin lesion segmentation . In: 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) IEEE ; 2022 : 1150 &#8208; 1156 . 26 Codella NC , Gutman D , Helba B , et&#160;al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 International Symposium on biomedical imaging (ISBI), hosted by the International skin imaging collaboration (ISIC) . In: 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) IEEE ; 2018 : 168 &#8208; 172 . 27 Codella N , Rotemberg V , Tschandl P , et&#160;al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International skin imaging collaboration (ISIC) . arXiv preprint arXiv:1902.03368 2019 . 28 Wei J , Hu Y , Zhang R , Li Z , Zhou SK , Cui S . Shallow attention network for polyp segmentation . In: Medical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, September 27&#8211;October 1, 2021, Proceedings, Part I 24 Springer ; 2021 : 699 &#8208; 708 . 29 Xu Q , Ma Z , Duan W . DCSAU&#8208;Net: A deeper and more compact split&#8208;attention U&#8208;Net for medical image segmentation . Comput Biol Med . 2023 ; 154 : 106626 . 36736096 10.1016/j.compbiomed.2023.106626 30 Li C , Qiang Y , Sultan RI , et&#160;al. Focalunetr: A focal transformer for boundary&#8208;aware prostate segmentation using CT images . In: International Conference on medical image computing and computer&#8208;assisted intervention Springer ; 2023 : 592 &#8208; 602 . 31 Sun Y , Dai D , Zhang Q , Wang Y , Xu S , Lian C . MSCA&#8208;Net: Multi&#8208;scale contextual attention network for skin lesion segmentation . Pattern Recognit . 2023 ; 139 : 109524 . 32 Dai D , Dong C , Yan Q , Sun Y , Zhang C , Li Z , Xu S . I2u&#8208;net: a dual&#8208;path u&#8208;net with rich information interaction for medical image segmentation . Med Image Anal . 2024 ; 97 : 103241 . 38897032 10.1016/j.media.2024.103241 33 Shu X , Wang J , Zhang A , Shi J , Wu X&#8208;J . CSCA U&#8208;Net: A channel and space compound attention CNN for medical image segmentation . Artif Intell Med . 2024 ; 150 : 102800 . 38553146 10.1016/j.artmed.2024.102800 34 Selvaraju RR , Cogswell M , Das A , et&#160;al. Grad&#8208;CAM: Visual explanations from deep networks via gradient&#8208;based localization . Proceedings of the IEEE International Conference on Computer Vision (ICCV) . 2017 : 618 &#8208; 626 ."
}