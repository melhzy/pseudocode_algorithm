{
  "pmcid": "PMC12656168",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:31.588866",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656168",
    "pmid": "41305218",
    "doi": "10.3390/s25227013",
    "title": "Ghost-Free HDR Imaging in Dynamic Scenes via High–Low-Frequency Decomposition †",
    "year": "2025",
    "month": "11",
    "day": "17",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "17"
    },
    "authors": [
      "Zhang Xiang",
      "Chen Genggeng",
      "Zhang Fan",
      "Zhang Yongzhong"
    ],
    "abstract": "Generating high-quality high-dynamic-range (HDR) images in dynamic scenes remains a challenging task. Recently, Transformers have been introduced into HDR imaging and have demonstrated superior performance over traditional convolutional neural networks (CNNs) in handling large-scale motion. However, due to the low-pass filtering nature of self-attention, Transformers tend to weaken the capture of high-frequency information, which impairs the recovery of structural details. In addition, their high computational complexity limits practical applications. To address these issues, we propose HL-HDR, a high–low-frequency-aware ghost-free HDR reconstruction network for dynamic scenes. By decomposing features into high- and low-frequency components, HL-HDR effectively overcomes the limitations of existing Transformer and CNN-based methods. The Frequency Alignment Module (FAM) captures large-scale motion in the low-frequency branch while refining local details in the high-frequency branch. The Frequency Decomposition Processing Block (FDPB) fuses local high-frequency details and global low-frequency context, enabling precise HDR reconstruction. Extensive experiments on five public HDR datasets demonstrate that HL-HDR consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluation.",
    "keywords": [
      "high dynamic range imaging",
      "ghost-free HDR",
      "High-Low Frequency Decomposition"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656168</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656168.1</article-id><article-id pub-id-type=\"pmcaid\">12656168</article-id><article-id pub-id-type=\"pmcaiid\">12656168</article-id><article-id pub-id-type=\"pmid\">41305218</article-id><article-id pub-id-type=\"doi\">10.3390/s25227013</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-07013</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Ghost-Free HDR Imaging in Dynamic Scenes via High&#8211;Low-Frequency Decomposition <xref rid=\"fn1-sensors-25-07013\" ref-type=\"author-notes\">&#8224;</xref></article-title></title-group><contrib-group><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0003-3270-0020</contrib-id><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"X\">Xiang</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><xref rid=\"af1-sensors-25-07013\" ref-type=\"aff\">1</xref><xref rid=\"c1-sensors-25-07013\" ref-type=\"corresp\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"G\">Genggeng</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-sensors-25-07013\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"F\">Fan</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><xref rid=\"af1-sensors-25-07013\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0009-0000-2829-2293</contrib-id><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"Y\">Yongzhong</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><xref rid=\"af2-sensors-25-07013\" ref-type=\"aff\">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Huang</surname><given-names initials=\"SC\">Shih-Chia</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-07013\"><label>1</label>College of Information and Control Engineering, Xi&#8217;an University of Architecture and Technology, Xi&#8217;an 710055, China; <email>chengeng0613@xauat.edu.cn</email> (G.C.); </aff><aff id=\"af2-sensors-25-07013\"><label>2</label>China United Network Communications Group Co., Ltd., Shaanxi Branch, Xi&#8217;an 710000, China</aff><author-notes><corresp id=\"c1-sensors-25-07013\"><label>*</label>Correspondence: <email>zhangxiang@xauat.edu.cn</email></corresp><fn id=\"fn1-sensors-25-07013\"><label>&#8224;</label><p>This paper is an extended version of our paper published in the Zhang, X.; Chen, G.; Hu, T.; Yang, K.; Zhang, F.; Yan, Q. HL-HDR: Multi-Exposure High Dynamic Range Reconstruction with High-Low Frequency Decomposition. In Proceedings of the 2024 International Joint Conference on Neural Networks (IJCNN), Yokohama, Japan, 30 June&#8211;5 July 2024; IEEE: Piscataway, NJ, USA, 2024; pp. 1&#8211;9.</p></fn></author-notes><pub-date pub-type=\"epub\"><day>17</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>7013</elocation-id><history><date date-type=\"received\"><day>12</day><month>10</month><year>2025</year></date><date date-type=\"rev-recd\"><day>04</day><month>11</month><year>2025</year></date><date date-type=\"accepted\"><day>14</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>17</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-07013.pdf\"/><abstract><p>Generating high-quality high-dynamic-range (HDR) images in dynamic scenes remains a challenging task. Recently, Transformers have been introduced into HDR imaging and have demonstrated superior performance over traditional convolutional neural networks (CNNs) in handling large-scale motion. However, due to the low-pass filtering nature of self-attention, Transformers tend to weaken the capture of high-frequency information, which impairs the recovery of structural details. In addition, their high computational complexity limits practical applications. To address these issues, we propose HL-HDR, a high&#8211;low-frequency-aware ghost-free HDR reconstruction network for dynamic scenes. By decomposing features into high- and low-frequency components, HL-HDR effectively overcomes the limitations of existing Transformer and CNN-based methods. The Frequency Alignment Module (FAM) captures large-scale motion in the low-frequency branch while refining local details in the high-frequency branch. The Frequency Decomposition Processing Block (FDPB) fuses local high-frequency details and global low-frequency context, enabling precise HDR reconstruction. Extensive experiments on five public HDR datasets demonstrate that HL-HDR consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluation.</p></abstract><kwd-group><kwd>high dynamic range imaging</kwd><kwd>ghost-free HDR</kwd><kwd>High-Low Frequency Decomposition</kwd></kwd-group><funding-group><award-group><funding-source>Science and Technology Development Program Projects of the Housing and Urban-Rural Development Department of Shaanxi Province</funding-source><award-id>2023-k48</award-id></award-group><funding-statement>This research was funded by the Science and Technology Development Program Projects of the Housing and Urban-Rural Development Department of Shaanxi Province (Grant No. 2023-k48). The APC was funded by the same agency.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-07013\"><title>1. Introduction</title><p>Contemporary digital cameras are limited by the capabilities of their sensors, making it difficult to capture the full dynamic range of real-world scenes. In contrast, High-Dynamic-Range (HDR) imaging can encompass a much wider range of light intensities, providing a more accurate representation of real-world luminance distributions. HDR imaging has become a key technique in modern visual applications, capable of faithfully reproducing lighting in complex scenes that contain both extremely bright and dark regions. Typical applications include photography and cinematography, where HDR enhances tonal depth and detail representation; virtual reality and game rendering, where it improves lighting realism and immersive experience; and autonomous driving and intelligent surveillance, where it helps maintain clear visibility of critical targets under challenging illumination conditions.</p><p>There are various approaches to generating HDR images, among which one of the most common methods is to reconstruct an HDR image by fusing multiple Low-Dynamic-Range (LDR) images captured under different exposure settings. However, during multi-exposure HDR reconstruction, object motion or camera shake may cause temporal inconsistencies or information loss due to overexposure, resulting in ghosting artifacts. This phenomenon remains one of the major challenges in multi-exposure HDR imaging.</p><p>To tackle the challenges associated with ghosting in HDR imaging, various methodologies have been developed. Traditional techniques commonly employ methods such as alignment-based methods [<xref rid=\"B1-sensors-25-07013\" ref-type=\"bibr\">1</xref>,<xref rid=\"B2-sensors-25-07013\" ref-type=\"bibr\">2</xref>], rejection-based methods [<xref rid=\"B3-sensors-25-07013\" ref-type=\"bibr\">3</xref>,<xref rid=\"B4-sensors-25-07013\" ref-type=\"bibr\">4</xref>,<xref rid=\"B5-sensors-25-07013\" ref-type=\"bibr\">5</xref>], and patch-based methods [<xref rid=\"B6-sensors-25-07013\" ref-type=\"bibr\">6</xref>,<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>] to eliminate or align motion regions in images. However, the efficacy of these methods is largely contingent upon the performance of preprocessing techniques, such as optical flow and motion detection. And when dealing with significant scene motion, the results of these methods typically turn out to be rather unsatisfactory. With the advancement of Deep Neural Networks (DNN), several CNN-based methods [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>,<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>,<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>,<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>,<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>] have been applied in ghost-free HDR imaging. Among them, the &#8220;alignment-fusion&#8221; paradigm has shown remarkable success, especially in scenarios involving large-scale motion. Moreover, Transformer-based approaches [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>,<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], which can capture long-distance dependencies, are introduced as an alternative to CNNs. These methods further enhance HDR imaging performance and are adopted by the current mainstream state-of-the-art methods. However, Transformers still face two major challenges in ghost-free HDR imaging. On one hand, local details and global information are crucial for restoring multi-frame HDR content, while the self-attention mechanism of pure Transformers often exhibits a low-pass filtering effect, reducing the variance of input features and overly smoothing patch tokens. This occurs because self-attention essentially averages features across different patches, suppressing high-frequency information that is vital for distinguishing fine structural details, thereby limiting the Transformer&#8217;s ability to capture high-frequency local details [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]. On the other hand, HDR images are typically high-resolution, and the computational complexity of self-attention grows quadratically with the spatial dimensions of the input feature map. This results in significant computational and memory overhead in high-resolution scenarios, restricting the practical application and scalability of Transformers in high-resolution HDR imaging tasks.</p><p>Considering that the high- and low-frequency components of an image correspond to local details and global structures, respectively, we propose a frequency-decomposition-based ghost-free HDR image reconstruction network. In both the cross-frame alignment and feature fusion stages, features are decomposed into high- and low-frequency components and processed according to their respective characteristics. Since high-frequency components represent local structures while low-frequency components characterize global information, we leverage the low-pass filtering property of average pooling (AvgPool) to decouple features into high-resolution high-frequency components and low-resolution low-frequency components.</p><p>Specifically, global motion or long-range dependencies can be effectively represented by low-frequency features without requiring high-resolution feature maps, while high-frequency features focus on fine-grained local structures that need high-resolution maps and are better modeled by local operators. Based on this, we adopt a dual-branch architecture in both stages to balance global information and local details.</p><p>In the cross-frame alignment stage, we propose the Frequency Alignment Module (FAM). The low-frequency branch employs a lightweight UNet to learn optical flow and align non-reference frames to the reference frame, efficiently capturing large-scale motion while reducing computational cost. Meanwhile, the high-frequency branch combines convolution and attention to adaptively refine edges and textures, suppressing ghosting and preserving structural consistency.</p><p>In the feature fusion stage, we design the Frequency Decomposition Processing Block (FDPB). The high-frequency branch uses a Local Feature Extractor (LFE) to capture details and enhance cross-frame high-frequency information, while the low-frequency branch adopts a Global Feature Extractor (GFE) to model long-range dependencies. To alleviate information loss caused by downsampling, we further introduce a Cross-Scale Fusion Module (CSFM) for effective cross-resolution integration.</p><p>By integrating FAM and FDPB, we propose the High&#8211;Low-Frequency-Aware HDR Network (HL-HDR), which consists of two stages: cross-frame alignment and feature fusion. FAM enables accurate motion modeling and detail preservation, while FDPB hierarchically captures both global and local contexts, leading to high-quality, ghost-free HDR reconstruction.</p><p>The main contributions are summarized as follows:<list list-type=\"bullet\"><list-item><p>We propose a novel alignment method, FAM, in which the low-frequency branch captures large-scale motion through optical flow alignment, while the high-frequency branch refines local edges and textures, effectively suppressing ghosting.</p></list-item><list-item><p>The FDPB module, introduced in our work, addresses low-frequency components by employing a multi-scale feature extraction approach in conjunction with Transformer mechanisms to collectively capture global information. For high-frequency components, we employ small convolutional kernels and densely connected residual links to effectively extract local feature information. This strategic design in our model achieves a harmonious balance between speed and precision.</p></list-item><list-item><p>A plethora of experiments have substantiated that the proposed methodology, denoted as method HL-HDR, attains state-of-the-art (SOTA) performance in HDR imaging tasks. Furthermore, it yields visually appealing outcomes that align with human perceptual aesthetics.</p></list-item></list></p><p>This work is an extended version of our conference paper [<xref rid=\"B15-sensors-25-07013\" ref-type=\"bibr\">15</xref>] presented at the International Joint Conference on Neural Networks (IJCNN 2024). Compared with the conference version, it incorporates a substantial amount of new material. (1) To address the issue of ghosting in moving regions, we optimized the cross-frame alignment stage by designing the FAM module: the low-frequency branch aligns features using optical flow, while the high-frequency branch adaptively refines local edges and textures through convolution and attention mechanisms, effectively suppressing ghost artifacts and maintaining structural consistency. (2) We conducted comparative experiments on additional datasets and against the latest methods, fully demonstrating the advantages of our improved approach. (3) The ablation studies are more detailed and clear, thoroughly verifying and analyzing the contributions of each module.</p></sec><sec id=\"sec2-sensors-25-07013\"><title>2. Related Work</title><p>Presently, HDR deghosting techniques can be primarily classified into alignment-based methods, rejection-based methods, patch-based methods, and CNN-based methods.</p><sec id=\"sec2dot1-sensors-25-07013\"><title>2.1. HDR Deghosting Methods</title><p><bold>Alignment-based Method.</bold> These methods aim to register all LDR images to a reference image using either rigid or non-rigid algorithms. Bogoni [<xref rid=\"B1-sensors-25-07013\" ref-type=\"bibr\">1</xref>] utilized optical flow to estimate motion vectors, while Pece and Kautz [<xref rid=\"B5-sensors-25-07013\" ref-type=\"bibr\">5</xref>] computed the Median Threshold Bitmap (MTB) for input images to detect regions of motion. Kang et al. [<xref rid=\"B16-sensors-25-07013\" ref-type=\"bibr\">16</xref>] transformed the intensities of LDR images into the luminance domain by leveraging exposure time information and computed optical flow to identify corresponding pixels among the LDR images. However, both rigid and non-rigid alignment methods exhibit susceptibility to significant motions, occlusions, and variations in brightness, rendering them prone to errors in complex regions.</p><p><bold>Rejection-based Methods.</bold> Rejection methods, post the global registration procedure, discern and eliminate motion regions within the input data, followed by the fusion of static regions to reconstruct HDR images. Grosch et al. [<xref rid=\"B3-sensors-25-07013\" ref-type=\"bibr\">3</xref>] devised an error map by assessing color disparities post alignment, aiming to exclude pixels with mismatches. Pece et al. [<xref rid=\"B5-sensors-25-07013\" ref-type=\"bibr\">5</xref>] identified regions of motion through the utilization of a median threshold bitmap on input LDR images. Jacobs et al. [<xref rid=\"B17-sensors-25-07013\" ref-type=\"bibr\">17</xref>] pinpointed areas of misalignment via an analytical approach involving weighted intensity variance analysis. These approaches often yield unsatisfactory HDR outcomes as they incur the loss of valuable information while eliminating pixels.</p><p><bold>Patch-based Methods.</bold> The patch-based methods, involving patch-wise alignment among exposure images for deghosting, have been explored in the literature. Sen et al. [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>] introduced a patch-based energy minimization method that simultaneously optimizes alignment and reconstruction. In the work by Hu et al. [<xref rid=\"B6-sensors-25-07013\" ref-type=\"bibr\">6</xref>], an iterative propagation of intensity and gradient information was conducted using a coarse-to-fine schedule. Ma et al. [<xref rid=\"B18-sensors-25-07013\" ref-type=\"bibr\">18</xref>] proposed an approach based on structural patch decomposition, which dissects an image patch into signal strength, signal structure, and mean intensity components for the reconstruction of ghost-free images. However, it is noteworthy that these methods lack compensation for saturation and are burdened by elevated computational costs.</p><p><bold>CNN-based Methods.</bold> Kalantari et al. [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>] initiated the alignment of images using optical flow and subsequently employed a CNN network for their fusion. Yan et al. [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>] introduced a spatial attention mechanism based on CNN to mitigate issues related to motion and oversaturated regions. Yan et al. [<xref rid=\"B19-sensors-25-07013\" ref-type=\"bibr\">19</xref>] formulated a non-local module aimed at expanding the receptive field for comprehensive global merging. In their work, Song et al. [<xref rid=\"B20-sensors-25-07013\" ref-type=\"bibr\">20</xref>] harnessed the benefits of the Transformer&#8217;s extensive receptive field to globally recover areas affected by motion. Additionally, HyHDR [<xref rid=\"B21-sensors-25-07013\" ref-type=\"bibr\">21</xref>] proposed an innovative patch aggregation module grounded in deep learning, strategically fusing valuable information from non-reference frames. Despite the significant performance breakthroughs achieved by these methodologies, their outcomes in both dynamic and static areas remain somewhat unsatisfactory.</p></sec><sec id=\"sec2dot2-sensors-25-07013\"><title>2.2. Vision Transformer</title><p>Transformers have demonstrated remarkable success in natural language processing. The multi-head self-attention mechanism utilized in this context effectively captures long-range correlations among word token embeddings. A recent development, Vision Transformer (ViT) [<xref rid=\"B22-sensors-25-07013\" ref-type=\"bibr\">22</xref>], has illustrated that a pure Transformer architecture can be directly applied to sequences of non-overlapping image patches, exhibiting excellent performance in image classification tasks. This showcases the versatility of Transformer models beyond natural language applications, extending their efficacy to the domain of computer vision. CA-ViT, proposed by Liu et al. [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>], leverages the Transformer&#8217;s capability for capturing long-range dependencies and extracting global feature information, complementing it with the ability of CNN to extract local information. The collaboration between these features has proven to be highly effective. Building upon the Transformer architecture, Zamir et al. [<xref rid=\"B23-sensors-25-07013\" ref-type=\"bibr\">23</xref>] introduced improvements by incorporating a locally aware Transformer design. This design enhances the model&#8217;s perception of local image details by introducing local convolution operations within the Transformer. Our approach is inspired by [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>,<xref rid=\"B23-sensors-25-07013\" ref-type=\"bibr\">23</xref>], strategically handling local and global information differently based on their inherent characteristics.</p></sec></sec><sec id=\"sec3-sensors-25-07013\"><title>3. Method</title><p>In a series of LDR images with varying exposure levels, images of the same scene are grouped together. Each group comprises three images: underexposed, normally exposed, and overexposed. Our goal is to fuse the information from these three LDR images to reconstruct an HDR image without ghosting artifacts. In previous research [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>,<xref rid=\"B24-sensors-25-07013\" ref-type=\"bibr\">24</xref>], the authors utilized a set of three LDR images as input, designating the normally exposed image as the reference frame. Using the input images <inline-formula><mml:math id=\"mm1\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, our model derives the HDR image <inline-formula><mml:math id=\"mm2\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id=\"FD1-sensors-25-07013\"><label>(1)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mover accent=\"true\"><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:mi>&#952;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm4\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the HDR imaging function, and <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> refers to the network&#8217;s parameters.</p><sec id=\"sec3dot1-sensors-25-07013\"><title>3.1. Overview of the HL-HDR Architecture</title><p>As shown in <xref rid=\"sensors-25-07013-f001\" ref-type=\"fig\">Figure 1</xref>, the proposed HL-HDR framework consists of two main stages:</p><p>The first stage is the cross-frame alignment stage, corresponding to the Frequency Alignment Module in the figure. This module takes three images with different exposures as input and extracts shallow features through a shared-weight convolution, producing 64-channel feature maps <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The feature map of the normally exposed image, <inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, is used as the reference frame, while the underexposed feature map <inline-formula><mml:math id=\"mm8\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and overexposed feature map <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are aligned to it.</p><p>The second stage is the feature fusion stage, in which multiple Frequency Decomposition Processing Blocks are stacked to extract and integrate the aligned features. Specifically, the features are decomposed into high-frequency and low-frequency components and processed according to their characteristics: the high-frequency branch employs a Local Feature Extractor to capture local details through stacked convolutional blocks and enhances cross-frame high-frequency information via dense connections; the low-frequency branch uses a Global Feature Extractor to model long-range dependencies through multi-layer channel attention. The extracted high- and low-frequency features are then fused.</p><p>In the final reconstruction stage, the network reduces the number of channels while introducing long-range residual connections, ultimately reconstructing the output as a 3-channel HDR image.</p></sec><sec id=\"sec3dot2-sensors-25-07013\"><title>3.2. Frequency Alignment Module</title><p>To balance large-scale motion modeling and detail preservation during the alignment stage, we first decompose both the reference frame and the non-reference frames into high-frequency and low-frequency components, which are then independently aligned in separate branches. Specifically, the low-frequency components are obtained by applying average pooling to the original feature maps, while the high-frequency components are derived by subtracting the low-frequency part from the original features. This process can be formally expressed as follows:<disp-formula id=\"FD2-sensors-25-07013\"><label>(2)</label><mml:math id=\"mm10\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>il</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Avgpool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD3-sensors-25-07013\"><label>(3)</label><mml:math id=\"mm11\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>Up</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>il</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ih</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-07013\"><label>(4)</label><mml:math id=\"mm12\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant=\"normal\">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Avgpool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD5-sensors-25-07013\"><label>(5)</label><mml:math id=\"mm13\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8243;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>Up</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant=\"normal\">l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant=\"normal\">h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8722;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>&#8243;</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Avgpool</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to average pooling, <inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Up</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for bilinear interpolation upsampling. <inline-formula><mml:math id=\"mm16\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> refers to the non-reference frame, while <inline-formula><mml:math id=\"mm17\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> refers to the reference frame.</p><p>For the alignment of high-frequency components, the high-frequency features of the reference frame and non-reference frames are concatenated and fed into a convolution-based attention module to generate an attention map, which is subsequently applied to the non-reference frame features. This module is similar to the implicit alignment mechanism in AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>], guiding the network to focus on critical information across different exposures. Under the guidance of attention, the model can adaptively reweight the importance of different regions, thereby refining edges and textures, enhancing local detail restoration, and effectively suppressing ghosting while ensuring structural consistency. Formally, the process can be written as:<disp-formula id=\"FD6-sensors-25-07013\"><label>(6)</label><mml:math id=\"mm18\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>am</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>AM</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ih</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant=\"normal\">h</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD7-sensors-25-07013\"><label>(7)</label><mml:math id=\"mm19\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ih</mml:mi></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>am</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>AM</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the attention module, and <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication.</p><p>For the alignment of low-frequency components, we modify the Encoder-Decoder structure of SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>] to predict the optical flow field, which is then used to warp the low-frequency components of the non-reference frames, enabling more accurate modeling of large-scale motion. The above operations can be formulated as:<disp-formula id=\"FD8-sensors-25-07013\"><label>(8)</label><mml:math id=\"mm22\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>am</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>FM</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>il</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant=\"normal\">l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD9-sensors-25-07013\"><label>(9)</label><mml:math id=\"mm23\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Warp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>il</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>am</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm24\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>FM</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to the Encoder-Decoder structure shown in <xref rid=\"sensors-25-07013-f002\" ref-type=\"fig\">Figure 2</xref>, and <inline-formula><mml:math id=\"mm25\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Warp</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the warping operation.</p><p>Finally, the aligned low-frequency and high-frequency components are fused, followed by convolution and a channel attention module to restore channel information and further extract features. This process can be formally expressed as follows:<disp-formula id=\"FD10-sensors-25-07013\"><label>(10)</label><mml:math id=\"mm26\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mi>CA</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>fusion</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">h</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>fusion</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the operation for integrating features at different scales, <inline-formula><mml:math id=\"mm28\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>CA</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents channel attention, <inline-formula><mml:math id=\"mm29\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for a <inline-formula><mml:math id=\"mm30\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution, and <inline-formula><mml:math id=\"mm31\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a <inline-formula><mml:math id=\"mm32\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution.</p></sec><sec id=\"sec3dot3-sensors-25-07013\"><title>3.3. Frequency Decomposition Processing Block</title><p>To more clearly illustrate the high-frequency information, we present three representative examples in <xref rid=\"sensors-25-07013-f003\" ref-type=\"fig\">Figure 3</xref>. The visualizations demonstrate that structural edges and fine-grained details are effectively captured, confirming the reliability of the &#8220;average pooling + subtraction&#8221; operation for separating high- and low-frequency information. Motivated by this observation, we adopt a similar frequency decomposition strategy during the feature fusion stage to further enhance the network&#8217;s representational capacity. As shown in <xref rid=\"sensors-25-07013-f001\" ref-type=\"fig\">Figure 1</xref>, similar to the alignment stage, we also decompose the feature maps into high-frequency and low-frequency components during the feature fusion stage. By supplementing high-frequency details while extracting the global and background information of the image, this approach enhances local textures and edges, resulting in improved visual quality. This process can be formally expressed as follows:<disp-formula id=\"FD11-sensors-25-07013\"><label>(11)</label><mml:math id=\"mm33\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>low</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Avgpool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD12-sensors-25-07013\"><label>(12)</label><mml:math id=\"mm34\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>Up</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>low</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>high</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>&#8722;</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD13-sensors-25-07013\"><label>(13)</label><mml:math id=\"mm35\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>out</mml:mi></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mi>CA</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>LFE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>high</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>GFE</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>low</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Avgpool</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers to average pooling, <inline-formula><mml:math id=\"mm37\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Up</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for bilinear interpolation upsampling, <inline-formula><mml:math id=\"mm38\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>LFE</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for Local Feature Extractor, <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>GFE</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for Global Feature Extractor, <inline-formula><mml:math id=\"mm40\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>CA</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes channel attention, <inline-formula><mml:math id=\"mm41\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for 3 &#215; 3 convolution, and <inline-formula><mml:math id=\"mm42\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> stands for 1 &#215; 1 convolution that restores the channel count from 128 to 64.</p><sec id=\"sec3dot3dot1-sensors-25-07013\"><title>3.3.1. Local Feature Extractor</title><p>To better recover the detailed information in an image, we process the high-frequency information, which inherently contains a wealth of detail. High-frequency information requires local details; thus, the use of convolutions with small kernels allows for a more focused extraction of these details. Moreover, given the superior capability of standard residual learning in maintaining stable feature propagation and enhancing high-frequency detail representation [<xref rid=\"B25-sensors-25-07013\" ref-type=\"bibr\">25</xref>,<xref rid=\"B26-sensors-25-07013\" ref-type=\"bibr\">26</xref>,<xref rid=\"B27-sensors-25-07013\" ref-type=\"bibr\">27</xref>], we incorporate dense residual connections into the high-frequency information extraction process to fully leverage multi-level features and strengthen high-frequency detail modeling. Overall, we utilize six 3 &#215; 3 convolutions. As depicted in <xref rid=\"sensors-25-07013-f004\" ref-type=\"fig\">Figure 4</xref>, we only display a portion of the residual connections, but in reality, these are dense residual connections. They are not merely connections between adjacent layers, but rather, the output of each layer is merged with the outputs of all preceding layers, enabling each layer to directly access the feature information of all previous layers.</p></sec><sec id=\"sec3dot3dot2-sensors-25-07013\"><title>3.3.2. Global Feature Extractor</title><p>For low-frequency information, it is necessary to leverage global context to restore the overall structure and background of the image. As shown in <xref rid=\"sensors-25-07013-f005\" ref-type=\"fig\">Figure 5</xref>, although multi-scale feature extraction enables long-range information interactions, some information may be lost during the downsampling process. To address this, we perform feature extraction at each scale to compensate for the information loss caused by downsampling. In each feature extraction layer, we introduce a Channel Transformer Block, which can establish global contextual information and possesses a global receptive field, making it highly suitable for extracting low-frequency features that depend on global information. Furthermore, to compensate for potential information loss when directly upsampling feature maps of different sizes and concatenating them, we employ the CSFM to effectively merge feature maps of varying resolutions.</p><p><bold>Channel Transformer Block.</bold> Given that the channel-wise self-attention mechanism proposed in [<xref rid=\"B23-sensors-25-07013\" ref-type=\"bibr\">23</xref>] can effectively model cross-channel dependencies while reducing computational complexity, our Transformer architecture abandons spatial self-attention and instead adopts channel-wise self-attention to achieve more efficient feature modeling. The input <inline-formula><mml:math id=\"mm43\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is first layer-normalized to obtain a tensor <inline-formula><mml:math id=\"mm44\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, 1 &#215; 1 convolutions are applied to aggregate pixel-wise cross-channel context, followed by 3 &#215; 3 depth-wise convolutions to encode channel-wise spatial context. This process generates the <inline-formula><mml:math id=\"mm45\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id=\"mm46\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">Q</mml:mi></mml:mrow></mml:math></inline-formula>), <inline-formula><mml:math id=\"mm47\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id=\"mm48\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">K</mml:mi></mml:mrow></mml:math></inline-formula>), and <inline-formula><mml:math id=\"mm49\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id=\"mm50\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">V</mml:mi></mml:mrow></mml:math></inline-formula>), which can be expressed mathematically as:<disp-formula id=\"FD14-sensors-25-07013\"><label>(14)</label><mml:math id=\"mm51\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>d</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>d</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mi>Y</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm52\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the 1 &#215; 1 point-wise convolution and <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the 3 &#215; 3 depth-wise convolution.</p><p>Then reshape Q into <inline-formula><mml:math id=\"mm54\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, reshape K into <inline-formula><mml:math id=\"mm55\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. After this transformation, matrix multiplication can be performed, followed by a softmax operation to obtain an attention map <inline-formula><mml:math id=\"mm56\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"bold\">A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Reshape V into <inline-formula><mml:math id=\"mm57\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, allowing for matrix multiplication with A. The resulting output is reshaped into <inline-formula><mml:math id=\"mm58\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and finally, a residual connection is added by summing the initial feature map with the obtained feature map. The specific process is illustrated as follows:<disp-formula id=\"FD15-sensors-25-07013\"><label>(15)</label><mml:math id=\"mm59\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo>&#183;</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mo>/</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD16-sensors-25-07013\"><label>(16)</label><mml:math id=\"mm60\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">Q</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">K</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">V</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm61\" overflow=\"scroll\"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is a learnable scaling parameter, <inline-formula><mml:math id=\"mm62\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold\">X</mml:mi></mml:mrow></mml:math></inline-formula> refers to the initial input feature map, and <inline-formula><mml:math id=\"mm63\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi mathvariant=\"bold\">X</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> refers to the final result.</p><p>Next, we utilize 1 &#215; 1 Convolution to aggregate information from different channels and employ 3 &#215; 3 depth-wise Convolution to aggregate information from spatially neighboring pixels. Additionally, we incorporate a gating mechanism to enhance information encoding. Finally, a long-range residual connection is added, summing the initial feature map with the feature map obtained at this stage.</p><p><bold>Cross-Scale Fusion Module.</bold> Due to the differing spatial resolutions of features at various scales, they are typically adjusted to a unified resolution via downsampling or upsampling for feature fusion. However, such operations may lead to the loss of important structural details, thereby affecting the final image restoration. To alleviate this problem, we introduce a wavelet-based cross-scale feature fusion strategy that fully leverages the capability of wavelet transforms in representing multi-scale image structures [<xref rid=\"B28-sensors-25-07013\" ref-type=\"bibr\">28</xref>]. As shown in <xref rid=\"sensors-25-07013-f006\" ref-type=\"fig\">Figure 6</xref>, we employ the Haar Discrete Wavelet Transform (DWT) to decompose a feature map <inline-formula><mml:math id=\"mm64\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> into four sub-bands: LL (Low&#8211;Low), LH (Low&#8211;High), HL (High&#8211;Low), and HH (High&#8211;High). Here, &#8220;L&#8221; and &#8220;H&#8221; denote low-pass and high-pass filtering along the horizontal and vertical dimensions, respectively. Each sub-band has half the spatial resolution of the original feature map, while the number of channels remains unchanged. Among these, the LL sub-band retains the low-frequency components, representing the global structure and smooth regions of the feature. It is concatenated with the small-scale feature map <inline-formula><mml:math id=\"mm65\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mfrac><mml:mi>H</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, followed by a <inline-formula><mml:math id=\"mm66\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution for channel reduction (from 128 to 64) and a residual block for feature refinement. The HH, HL, and LH sub-bands preserve the high-frequency components, containing texture and edge details. After concatenation, a <inline-formula><mml:math id=\"mm67\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution reduces the channels to 64, followed by a residual block for detailed feature extraction, and another <inline-formula><mml:math id=\"mm68\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution restores the channel number to 192. The use of two <inline-formula><mml:math id=\"mm69\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions effectively reduces the number of parameters and computational complexity, since directly processing a 192-channel feature map would be computationally expensive. Finally, the refined high- and low-frequency features are recombined through the Inverse Discrete Wavelet Transform (IDWT) to produce the fused representation. This process can be formally expressed as follows:<disp-formula id=\"FD17-sensors-25-07013\"><label>(17)</label><mml:math id=\"mm70\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>DWT</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">b</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD18-sensors-25-07013\"><label>(18)</label><mml:math id=\"mm71\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"normal\">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Res</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD19-sensors-25-07013\"><label>(19)</label><mml:math id=\"mm72\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Res</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD20-sensors-25-07013\"><label>(20)</label><mml:math id=\"mm73\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">F</mml:mi><mml:mo>=</mml:mo><mml:mi>IDWT</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"normal\">b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm74\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the residual block.</p></sec></sec><sec id=\"sec3dot4-sensors-25-07013\"><title>3.4. Training Loss</title><p>Due to the typical display of HDR images after tonemapping, training the network on tonemapped images is more effective than training directly in the HDR domain. When provided with an HDR image H in the HDR domain, we compress the image&#8217;s range using the <inline-formula><mml:math id=\"mm75\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>-law transformation.<disp-formula id=\"FD21-sensors-25-07013\"><label>(21)</label><mml:math id=\"mm76\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mo form=\"prefix\">log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#956;</mml:mi><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo form=\"prefix\">log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm77\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> represents a parameter that defines the degree of compression, and <inline-formula><mml:math id=\"mm78\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the tonemapped image. Throughout this work, we maintain <italic toggle=\"yes\">H</italic> within the range [0, 1] and set <inline-formula><mml:math id=\"mm79\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> to 5000.</p><p><inline-formula><mml:math id=\"mm80\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the predicted result obtained from our HL-HDR model, and H is the Ground Truth. Here, we employ <inline-formula><mml:math id=\"mm81\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> loss to compute the loss. Additionally, we use an auxiliary perceptual loss <inline-formula><mml:math id=\"mm82\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for supervision [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]. The perceptual loss measures the difference between the output image and the Ground Truth image in the feature representations of multiple layers in a pre-trained CNN, achieved by computing the mean squared error between the feature maps of each layer. We can express this as follows:<disp-formula id=\"FD22-sensors-25-07013\"><label>(22)</label><mml:math id=\"mm83\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy=\"false\">&#8741;</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent=\"true\"><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD23-sensors-25-07013\"><label>(23)</label><mml:math id=\"mm84\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy=\"false\">&#8741;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent=\"true\"><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">&#8741;</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm85\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#952;</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the <inline-formula><mml:math id=\"mm86\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> convolutional feature extracted from the pre-trained VGG-16 network, with <italic toggle=\"yes\">j</italic> denoting the <inline-formula><mml:math id=\"mm87\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> layer.</p><p>Therefore, our final loss function is the result of adding <inline-formula><mml:math id=\"mm88\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm89\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, with different weights assigned to each, for which we introduce a parameter <inline-formula><mml:math id=\"mm90\" overflow=\"scroll\"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula>. The final loss function can be expressed by the following formula:<disp-formula id=\"FD24-sensors-25-07013\"><label>(24)</label><mml:math id=\"mm91\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>&#955;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm92\" overflow=\"scroll\"><mml:mrow><mml:mi>&#955;</mml:mi></mml:mrow></mml:math></inline-formula> is set to 0.01.</p></sec></sec><sec id=\"sec4-sensors-25-07013\"><title>4. Experiments</title><sec id=\"sec4dot1-sensors-25-07013\"><title>4.1. Experiments Settings</title><p><bold>Datasets.</bold> The proposed method has been trained on three distinct datasets: Kalantari&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>], Tel&#8217;s dataset [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], and Hu&#8217;s dataset [<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>]. Kalantari&#8217;s dataset consists of 74 training samples and 15 testing samples captured from real-world scenes, with exposure values set at {&#8722;2, 0, +2} and {&#8722;3, 0, +3}. Tel&#8217;s dataset comprises 108 training samples and 36 testing samples. For Hu&#8217;s dataset, the first 85 samples were used for training, while the remaining 15 were reserved for testing. This dataset employs an exposure bias of {&#8722;2, 0, +2} and is synthetically generated using a game engine sensor. To evaluate the effectiveness and generalization capability of the proposed model, we conducted tests on Sen&#8217;s dataset [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>] and Tursun&#8217;s dataset [<xref rid=\"B30-sensors-25-07013\" ref-type=\"bibr\">30</xref>] using weights pre-trained on Kalantari&#8217;s dataset. Since these two datasets contain only LDR images at different exposure levels and lack ground truth, the performance comparison across methods is limited to subjective assessment.</p><p><bold>Evaluation Metrics.</bold> We use five objective measures for quantitative comparison: PSNR-<inline-formula><mml:math id=\"mm93\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, SSIM-<inline-formula><mml:math id=\"mm94\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, PSNR-<italic toggle=\"yes\">l</italic>, SSIM-<italic toggle=\"yes\">l</italic>, and HDR-VDP-2 [<xref rid=\"B31-sensors-25-07013\" ref-type=\"bibr\">31</xref>], where <inline-formula><mml:math id=\"mm95\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and <italic toggle=\"yes\">l</italic> indicate that the metrics are computed in the tonemapped domain and the linear domain, respectively. Among these, HDR-VDP-2 is a perceptual quality metric specifically designed for HDR images. It models the human visual system&#8217;s sensitivity to luminance, contrast, and local structural variations, providing a more accurate assessment of perceived image distortions compared to traditional pixel-wise metrics.</p><p><bold>Implementation Details.</bold> Our implementation is based on PyTorch 3.9. Before training, we sample <inline-formula><mml:math id=\"mm96\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> patches from the dataset with a stride of 64. To enhance the diversity of the training data, we apply data augmentation techniques including rotation and flipping, as well as their combinations, resulting in six different augmentation strategies. We employ the Adam optimizer with a batch size of 8 and an initial learning rate of <inline-formula><mml:math id=\"mm97\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which is reduced every 70 epochs. The model is trained for a total of 250 epochs on a single NVIDIA GeForce RTX 4090 GPU (NVIDIA, Santa Clara, CA, USA).</p></sec><sec id=\"sec4dot2-sensors-25-07013\"><title>4.2. Comparison with the State-of-the-Art Methods</title><p>To comprehensively evaluate the performance of our model, we compared it against representative state-of-the-art deep learning-based approaches spanning different architectural paradigms. Specifically, we considered six CNN-based methods, including DHDR [<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>], AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>], NHDRR [<xref rid=\"B19-sensors-25-07013\" ref-type=\"bibr\">19</xref>], APNT [<xref rid=\"B32-sensors-25-07013\" ref-type=\"bibr\">32</xref>], PGN [<xref rid=\"B33-sensors-25-07013\" ref-type=\"bibr\">33</xref>], and SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>]; one GAN-based method, HDR-GAN [<xref rid=\"B34-sensors-25-07013\" ref-type=\"bibr\">34</xref>]; three Transformer-based models, namely CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>], SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], and HyHDR [<xref rid=\"B21-sensors-25-07013\" ref-type=\"bibr\">21</xref>]; as well as two diffusion-based methods, DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>] and LFDiff [<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>].</p><p><bold>Datasets with Ground Truth.</bold><xref rid=\"sensors-25-07013-t001\" ref-type=\"table\">Table 1</xref>, <xref rid=\"sensors-25-07013-t002\" ref-type=\"table\">Table 2</xref> and <xref rid=\"sensors-25-07013-t003\" ref-type=\"table\">Table 3</xref> presents the quantitative results of HL-HDR on three datasets. Our method is compared against several state-of-the-art approaches using the testing data from [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>,<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>,<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>], which consist of challenging samples characterized by saturated backgrounds and foreground motions. The average of all quantitative results is computed across the testing images.</p><p>Notably, our method performs remarkably well on Kalantari&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>], achieving state-of-the-art performance in PSNR-<inline-formula><mml:math id=\"mm98\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, along with competitive results in other metrics. On Hu&#8217;s dataset [<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>], our method achieves strong performance in both PSNR-<inline-formula><mml:math id=\"mm99\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and PSNR-<italic toggle=\"yes\">l</italic>, with PSNR-<italic toggle=\"yes\">l</italic> outperforming the second-best approach by 0.82 dB. On Tel&#8217;s dataset [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], our method demonstrates overall superiority, where both PSNR-<inline-formula><mml:math id=\"mm100\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and PSNR-<italic toggle=\"yes\">l</italic> substantially surpass the second-best approach, with gains of 0.62 dB and 0.32 dB, respectively.</p><p>In <xref rid=\"sensors-25-07013-f007\" ref-type=\"fig\">Figure 7</xref>, <xref rid=\"sensors-25-07013-f008\" ref-type=\"fig\">Figure 8</xref> and <xref rid=\"sensors-25-07013-f009\" ref-type=\"fig\">Figure 9</xref>, the datasets present significant challenges due to large-scale foreground motion and severe over/under-exposed regions. We qualitatively compare our method with several state-of-the-art approaches. Most competing methods suffer from ghosting artifacts in regions with motion and saturation. On Kalantari&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>], DHDR [<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>] exhibits severe ghosting, while AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>], HDR-GAN [<xref rid=\"B34-sensors-25-07013\" ref-type=\"bibr\">34</xref>], and SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>] not only fail to recover complete structural information but also perform poorly in detail restoration. For example, in the patch comparison shown in <xref rid=\"sensors-25-07013-f007\" ref-type=\"fig\">Figure 7</xref>, these three methods fail to reconstruct the balcony, with sky elements incorrectly blended in, and the wall textures appear blurry. CA-VIT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>] and SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>] further suffer from blocky ghosting artifacts due to patch-based sampling. In contrast, SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>] exhibits only slight wall deformation. Our proposed method not only restores the overall structural content accurately but also excels in preserving fine details. In particular, the wall lines remain sharp and clear, demonstrating the strong capability of our model in capturing and restoring fine-grained information.</p><p>In <xref rid=\"sensors-25-07013-f008\" ref-type=\"fig\">Figure 8</xref>, we show a comparison scene from Tel&#8217;s dataset [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], where only the heads of two people exhibit slight motion. All other methods, however, produced noticeable ghosting artifacts in these motion regions. In contrast, our method accurately detects the motion areas and achieves superior image reconstruction. In <xref rid=\"sensors-25-07013-f009\" ref-type=\"fig\">Figure 9</xref>, we show a comparison scene from Hu&#8217;s dataset [<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>], where the motion is much more substantial. Except for our method, all other approaches generated large ghosting regions in the motion areas, significantly degrading the visual quality.</p><p><bold>Evaluation on Datasets without Ground Truth.</bold> To evaluate the generalization capability of the proposed HDR imaging method, we tested the model trained on Kalantari&#8217;s Dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>] on Sen&#8217;s Dataset [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>] and Tursun&#8217;s Dataset [<xref rid=\"B30-sensors-25-07013\" ref-type=\"bibr\">30</xref>], both of which lack ground truth. Consequently, the quality of the generated HDR images can only be assessed through subjective visual inspection. Notably, in <xref rid=\"sensors-25-07013-f010\" ref-type=\"fig\">Figure 10</xref>, most methods fail to recover overexposed regions, whereas our method performs exceptionally well, not only avoiding overexposure but also successfully restoring rich detail. In <xref rid=\"sensors-25-07013-f011\" ref-type=\"fig\">Figure 11</xref>, both our method and SCTNet are visually the most appealing, with no noticeable ghosting caused by human motion. This is because the scene contains abundant background information, and Transformer-based methods can fully exploit long-range dependencies to extract information from similar regions, thereby restoring details in motion areas and generating ghost-free images.</p></sec><sec id=\"sec4dot3-sensors-25-07013\"><title>4.3. Computational Budgets</title><p>We further compared model parameters and inference times, as summarized in <xref rid=\"sensors-25-07013-t004\" ref-type=\"table\">Table 4</xref>. Traditional patch match&#8211;based methods, such as Sen [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>] and Hu [<xref rid=\"B6-sensors-25-07013\" ref-type=\"bibr\">6</xref>], exhibit very long inference times due to their reliance on CPU computation. CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>] employs standard Transformer blocks, which results in relatively high computational cost despite a moderate number of parameters. DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>], reconstructing HDR images from pure noise, incurs both high inference time and a large parameter count. SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>] achieves fast inference with a small model size, but its reconstruction performance still leaves room for improvement. LFDiff [<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>], although faster than earlier diffusion-based models, still relies on the diffusion process, and our method achieves roughly three times its inference speed. In comparison, our method attains competitive inference speed (0.21 s) on a single A100 GPU with a moderate parameter count (4.08 M), striking a good balance between computational efficiency and model capacity, while also demonstrating superior reconstruction performance.</p></sec><sec id=\"sec4dot4-sensors-25-07013\"><title>4.4. Ablation Studies</title><p>We conducted ablation experiments on the Kalantari dataset to evaluate the effectiveness of each module in our model. The following sections present the ablation analysis from three perspectives, corresponding to the main components of the model.</p><sec id=\"sec4dot4dot1-sensors-25-07013\"><title>4.4.1. Effect of Different Alignment Modules</title><p>To validate the effectiveness of the proposed Frequency Alignment Module, we compared it with two alternative models: one without any alignment module, and the other using the AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>] alignment module. All other model components and parameters were kept identical.</p><p>As shown in <xref rid=\"sensors-25-07013-t005\" ref-type=\"table\">Table 5</xref>, it is evident that the model without any alignment module achieves the lowest metrics. The model using AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>] for alignment shows a slight improvement, but the gain is limited. In contrast, when aligned using our proposed FAM, both PSNR-<inline-formula><mml:math id=\"mm101\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> and PSNR-<italic toggle=\"yes\">l</italic> increase significantly, with PSNR-<italic toggle=\"yes\">l</italic> rising by 0.63 dB. Although the model without alignment achieves relatively lower scores, it still outperforms several existing methods such as CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>] and SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>], indirectly highlighting the effectiveness of our proposed FDPB. In addition, we provide visual comparisons to further demonstrate the effectiveness of our alignment strategy. As shown in <xref rid=\"sensors-25-07013-f012\" ref-type=\"fig\">Figure 12</xref>, we select three representative examples from the test set that involve large-scale motion and overexposed regions. The model without alignment produces noticeable ghosting artifacts, while the AHDR-aligned model achieves partial improvement but still suffers from residual artifacts. In contrast, our FAM-aligned results almost completely eliminate ghosting. This superior performance can be attributed to the incorporation of optical flow within FAM, which effectively captures large-scale motion and further enhances the quality of HDR image reconstruction.</p></sec><sec id=\"sec4dot4dot2-sensors-25-07013\"><title>4.4.2. Ablation Analysis of Components in the Frequency Alignment Module</title><p>To validate the rationale behind our proposed FAM, which decomposes high- and low-frequency features and processes them using different methods, we designed four experimental settings: (1) No high&#8211;low-frequency separation, aligning the two frames using only optical flow; (2) No high&#8211;low-frequency separation, aligning the two frames using convolution and attention(AHDR-aligned); (3) High&#8211;low-frequency separation, aligning both high- and low-frequency features using optical flow; (4) High&#8211;low-frequency separation, aligning both high- and low-frequency features using convolution and attention.</p><p><xref rid=\"sensors-25-07013-t006\" ref-type=\"table\">Table 6</xref> presents the comparison results of different alignment strategies for high- and low-frequency features. As shown in the table, the non-separation strategies (1) and (2) exhibit notable differences: the optical-flow-based approach achieves significantly better PSNR-<italic toggle=\"yes\">l</italic> and SSIM-<italic toggle=\"yes\">l</italic> compared to the convolution + attention approach, demonstrating the superiority of optical flow in handling large-scale motion. When high- and low-frequency features are separated and both aligned using optical flow (3), the performance is comparable to (1) but with only marginal improvement, indicating that separation alone does not yield substantial benefits. In contrast, fully relying on convolution and attention after separation (4) performs even worse than the non-separated cases, highlighting its limitations in capturing large-scale motion. In comparison, our proposed FAM achieves the best results in PSNR-<inline-formula><mml:math id=\"mm102\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, SSIM-<inline-formula><mml:math id=\"mm103\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula>, and SSIM-<italic toggle=\"yes\">l</italic>, while maintaining overall stable performance. These results validate the effectiveness of combining optical flow with frequency separation in our design.</p></sec><sec id=\"sec4dot4dot3-sensors-25-07013\"><title>4.4.3. Ablation Analysis of Components in the Frequency Decomposition Processing Block</title><p>To validate the effectiveness of the proposed FDPB, we design a more fine-grained ablation study consisting of six comparative schemes: (1) Without frequency decomposition, the aligned features are simultaneously fed into both the Global Feature Extractor (GFE) and the Local Feature Extractor (LFE); (2) With frequency decomposition, but with swapped branch functions, where the high-frequency features are processed by the Global Feature Extractor (GFE) and the low-frequency features are processed by the Local Feature Extractor (LFE); (3) With frequency decomposition, but extracting both high- and low-frequency features using the LFE; (4) With frequency decomposition, but extracting both high- and low-frequency features using the GFE; (5) Removing the dense residual connections in the low-frequency branch; (6) Removing the wavelet-based Cross-Scale Fusion Module at each layer of the high-frequency branch.</p><p><xref rid=\"sensors-25-07013-t007\" ref-type=\"table\">Table 7</xref> presents a comprehensive ablation study of the proposed Frequency Decomposition Processing Block (FDPB). It can be observed that performing frequency decomposition significantly enhances HDR reconstruction performance: comparing the scheme without decomposition (1) to the schemes with decomposition (3) and (4), PSNR-<italic toggle=\"yes\">l</italic> is generally improved, indicating that separating high- and low-frequency features facilitates more effective extraction of global structures and local details. Notably, although scheme (4) achieves the highest PSNR-<italic toggle=\"yes\">l</italic>, its PSNR-<inline-formula><mml:math id=\"mm104\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> is relatively low due to insufficient processing of high-frequency information, demonstrating that relying solely on the Global Feature Extractor (GFE) is inadequate for restoring image details. The importance of matching feature types to the appropriate extractor is highlighted in scheme (2), where swapping the high-frequency and low-frequency branches leads to a noticeable performance drop, showing that high-frequency features are better handled by the Local Feature Extractor (LFE) and low-frequency features by the GFE. Furthermore, removing dense residual connections in the low-frequency branch (5) or the Cross-Scale Fusion Module in the high-frequency branch (6) results in decreased performance, emphasizing the critical role of these components in preserving structural information and enhancing details. Overall, the complete FDPB design, integrating frequency decomposition, proper branch assignment, dense residual connections, and wavelet-based cross-scale fusion, achieves the best results across all metrics, confirming its effectiveness in restoring high-quality, ghost-free HDR images.</p></sec></sec></sec><sec sec-type=\"conclusions\" id=\"sec5-sensors-25-07013\"><title>5. Conclusions</title><p>This paper presents HL-HDR, a high&#8211;low-frequency-aware HDR reconstruction network for dynamic scenes. It explicitly decomposes features into high- and low-frequency components, effectively combining global context modeling with fine detail restoration. The Frequency Alignment Module (FAM) enables precise motion estimation and structure preservation, while the Frequency Decomposition Processing Block (FDPB) supports hierarchical cross-scale feature fusion. Compared with state-of-the-art models such as LFDiff and SAFNet, HL-HDR offers clear advantages. Unlike diffusion-based models like LFDiff, which rely on iterative sampling and are computationally expensive, HL-HDR has a compact architecture, stable training, and faster convergence. Unlike lightweight CNNs such as SAFNet, which often sacrifice detail for speed, HL-HDR jointly optimizes high- and low-frequency features to balance visual quality and efficiency. Experiments on multiple public HDR benchmarks show that HL-HDR consistently improves performance. On three datasets with ground-truth HDR images, PSNR-<inline-formula><mml:math id=\"mm105\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> increases by 0.05 dB, 0.62 dB, and 0.28 dB, with visual results showing better ghost removal and detail preservation. For future work, we plan to explore lightweight designs and real-time deployment, leverage diffusion models to improve generalization and robustness in complex dynamic scenes, and build larger multi-scene HDR datasets to promote practical applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.Z. and G.C.; methodology, G.C.; software, X.Z.; validation, X.Z. and G.C.; resources, F.Z. and Y.Z.; data curation, X.Z.; writing&#8212;original draft preparation, X.Z. and G.C.; writing&#8212;review and editing, X.Z. and G.C.; supervision, F.Z. and Y.Z.; funding acquisition, X.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The code is publicly available at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/chengeng0613/HL-HDR_Plus\">https://github.com/chengeng0613/HL-HDR_Plus</uri>.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>Author Yongzhong Zhang was employed by the company China United Network Communications Group Co. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-07013\"><label>1.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bogoni</surname><given-names>L.</given-names></name></person-group><article-title>Extending dynamic range of monochrome and color images through fusion</article-title><source>Proceedings of the Proceedings 15th International Conference on Pattern Recognition, ICPR-2000</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>3&#8211;7 September 2000</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2000</year><volume>Volume 3</volume><fpage>7</fpage><lpage>12</lpage></element-citation></ref><ref id=\"B2-sensors-25-07013\"><label>2.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tomaszewska</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mantiuk</surname><given-names>R.</given-names></name></person-group><article-title>Image registration for multi-exposure high dynamic range image acquisition</article-title><source>Proceedings of the International Conference on Computer Graphics, Visualization and Computer Vision</source><conf-loc>Plzen, Czech Republic</conf-loc><conf-date>29 January&#8211;1 February 2007</conf-date><volume>Volume 2</volume></element-citation></ref><ref id=\"B3-sensors-25-07013\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Grosch</surname><given-names>T.</given-names></name></person-group><article-title>Fast and robust high dynamic range image generation with camera and object movement</article-title><source>Vision, Model. Vis. Rwth Aachen</source><year>2006</year><volume>2</volume><fpage>277</fpage><lpage>284</lpage></element-citation></ref><ref id=\"B4-sensors-25-07013\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lee</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Monga</surname><given-names>V.</given-names></name></person-group><article-title>Ghost-free high dynamic range imaging via rank minimization</article-title><source>IEEE Signal Process. Lett.</source><year>2014</year><volume>21</volume><fpage>1045</fpage><lpage>1049</lpage><pub-id pub-id-type=\"doi\">10.1109/LSP.2014.2323404</pub-id></element-citation></ref><ref id=\"B5-sensors-25-07013\"><label>5.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pece</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Kautz</surname><given-names>J.</given-names></name></person-group><article-title>Bitmap movement detection: HDR for dynamic scenes</article-title><source>Proceedings of the 2010 Conference on Visual Media Production</source><conf-loc>London, UK</conf-loc><conf-date>17&#8211;18 November 2010</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2010</year><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id=\"B6-sensors-25-07013\"><label>6.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Gallo</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Pulli</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>X.</given-names></name></person-group><article-title>HDR deghosting: How to deal with saturation?</article-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><conf-loc>Portland, OR, USA</conf-loc><conf-date>23&#8211;28 June 2013</conf-date><fpage>1163</fpage><lpage>1170</lpage></element-citation></ref><ref id=\"B7-sensors-25-07013\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sen</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Kalantari</surname><given-names>N.K.</given-names></name><name name-style=\"western\"><surname>Yaesoubi</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Darabi</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Goldman</surname><given-names>D.B.</given-names></name><name name-style=\"western\"><surname>Shechtman</surname><given-names>E.</given-names></name></person-group><article-title>Robust patch-based hdr reconstruction of dynamic scenes</article-title><source>ACM Trans. Graph.</source><year>2012</year><volume>31</volume><fpage>203</fpage><pub-id pub-id-type=\"doi\">10.1145/2366145.2366222</pub-id></element-citation></ref><ref id=\"B8-sensors-25-07013\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kalantari</surname><given-names>N.K.</given-names></name><name name-style=\"western\"><surname>Ramamoorthi</surname><given-names>R.</given-names></name></person-group><article-title>Deep high dynamic range imaging of dynamic scenes</article-title><source>ACM Trans. Graph.</source><year>2017</year><volume>36</volume><fpage>144</fpage><pub-id pub-id-type=\"doi\">10.1145/3072959.3073609</pub-id></element-citation></ref><ref id=\"B9-sensors-25-07013\"><label>9.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tai</surname><given-names>Y.W.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>C.K.</given-names></name></person-group><article-title>Deep high dynamic range imaging with large foreground motions</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date><fpage>117</fpage><lpage>132</lpage></element-citation></ref><ref id=\"B10-sensors-25-07013\"><label>10.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Gong</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Hengel</surname><given-names>A.V.D.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Reid</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Attention-guided network for ghost-free high dynamic range imaging</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>1751</fpage><lpage>1760</lpage></element-citation></ref><ref id=\"B11-sensors-25-07013\"><label>11.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kong</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name></person-group><article-title>Safnet: Selective alignment fusion network for efficient hdr imaging</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>256</fpage><lpage>273</lpage></element-citation></ref><ref id=\"B12-sensors-25-07013\"><label>12.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Generating Content for HDR Deghosting from Frequency View</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2404.00849</pub-id><pub-id pub-id-type=\"arxiv\">2404.00849</pub-id></element-citation></ref><ref id=\"B13-sensors-25-07013\"><label>13.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S.</given-names></name></person-group><article-title>Ghost-free high dynamic range imaging with context-aware transformer</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>344</fpage><lpage>360</lpage></element-citation></ref><ref id=\"B14-sensors-25-07013\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tel</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Heyrman</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Demonceaux</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Timofte</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Ginhac</surname><given-names>D.</given-names></name></person-group><article-title>Alignment-free HDR Deghosting with Semantics Consistent Transformer</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2305.18135</pub-id><pub-id pub-id-type=\"arxiv\">2305.18135</pub-id></element-citation></ref><ref id=\"B15-sensors-25-07013\"><label>15.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name></person-group><article-title>HL-HDR: Multi-Exposure High Dynamic Range Reconstruction with High-Low Frequency Decomposition</article-title><source>Proceedings of the 2024 International Joint Conference on Neural Networks (IJCNN)</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>30 June&#8211;5 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id=\"B16-sensors-25-07013\"><label>16.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kang</surname><given-names>S.B.</given-names></name><name name-style=\"western\"><surname>Uyttendaele</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Winder</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Szeliski</surname><given-names>R.</given-names></name></person-group><article-title>High dynamic range video</article-title><source>ACM Trans. Graph. (TOG)</source><year>2003</year><volume>22</volume><fpage>319</fpage><lpage>325</lpage><pub-id pub-id-type=\"doi\">10.1145/882262.882270</pub-id></element-citation></ref><ref id=\"B17-sensors-25-07013\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jacobs</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Loscos</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Ward</surname><given-names>G.</given-names></name></person-group><article-title>Automatic high-dynamic range image generation for dynamic scenes</article-title><source>IEEE Comput. Graph. Appl.</source><year>2008</year><volume>28</volume><fpage>84</fpage><lpage>93</lpage><pub-id pub-id-type=\"doi\">10.1109/MCG.2008.23</pub-id><pub-id pub-id-type=\"pmid\">18350936</pub-id></element-citation></ref><ref id=\"B18-sensors-25-07013\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yong</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Meng</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Robust multi-exposure image fusion: A structural patch decomposition approach</article-title><source>IEEE Trans. Image Process.</source><year>2017</year><volume>26</volume><fpage>2519</fpage><lpage>2532</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2017.2671921</pub-id><pub-id pub-id-type=\"pmid\">28237928</pub-id></element-citation></ref><ref id=\"B19-sensors-25-07013\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Deep HDR imaging via a non-local network</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>4308</fpage><lpage>4322</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2020.2971346</pub-id><pub-id pub-id-type=\"pmid\">32054579</pub-id></element-citation></ref><ref id=\"B20-sensors-25-07013\"><label>20.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names>J.W.</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>Y.I.</given-names></name><name name-style=\"western\"><surname>Kong</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Kwak</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Kang</surname><given-names>S.J.</given-names></name></person-group><article-title>Selective TransHDR: Transformer-Based Selective HDR Imaging Using Ghost Region Mask</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#8211;27 October 2022</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>288</fpage><lpage>304</lpage></element-citation></ref><ref id=\"B21-sensors-25-07013\"><label>21.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>A Unified HDR Imaging Method with Pixel and Patch Level</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>22211</fpage><lpage>22220</lpage></element-citation></ref><ref id=\"B22-sensors-25-07013\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"arxiv\">2010.11929</pub-id></element-citation></ref><ref id=\"B23-sensors-25-07013\"><label>23.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zamir</surname><given-names>S.W.</given-names></name><name name-style=\"western\"><surname>Arora</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Hayat</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>F.S.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>M.H.</given-names></name></person-group><article-title>Restormer: Efficient transformer for high-resolution image restoration</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#8211;24 June 2022</conf-date><fpage>5728</fpage><lpage>5739</lpage></element-citation></ref><ref id=\"B24-sensors-25-07013\"><label>24.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>J.Q.</given-names></name><name name-style=\"western\"><surname>Gong</surname><given-names>D.</given-names></name></person-group><article-title>A lightweight network for high dynamic range imaging</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#8211;20 June 2022</conf-date><fpage>824</fpage><lpage>832</lpage></element-citation></ref><ref id=\"B25-sensors-25-07013\"><label>25.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dong</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Pan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Multi-Scale Residual Low-Pass Filter Network for Image Deblurring</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>12345</fpage><lpage>12354</lpage></element-citation></ref><ref id=\"B26-sensors-25-07013\"><label>26.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J.K.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>K.M.</given-names></name></person-group><article-title>Accurate image super-resolution using very deep convolutional networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>1646</fpage><lpage>1654</lpage></element-citation></ref><ref id=\"B27-sensors-25-07013\"><label>27.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Tai</surname><given-names>Y.W.</given-names></name><etal/></person-group><article-title>Learning dual convolutional neural networks for low-level vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>3070</fpage><lpage>3079</lpage></element-citation></ref><ref id=\"B28-sensors-25-07013\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>J.J.</given-names></name><name name-style=\"western\"><surname>Dragotti</surname><given-names>P.L.</given-names></name></person-group><article-title>WINNet: Wavelet-inspired invertible network for image denoising</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>4377</fpage><lpage>4392</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2022.3184845</pub-id><pub-id pub-id-type=\"pmid\">35759598</pub-id></element-citation></ref><ref id=\"B29-sensors-25-07013\"><label>29.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Choe</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Nadir</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Nabil</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>S.J.</given-names></name><name name-style=\"western\"><surname>Sheikh</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yoo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Polley</surname><given-names>M.</given-names></name></person-group><article-title>Sensor-realistic synthetic data engine for multi-frame high dynamic range photography</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>14&#8211;19 June 2020</conf-date><fpage>516</fpage><lpage>517</lpage></element-citation></ref><ref id=\"B30-sensors-25-07013\"><label>30.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tursun</surname><given-names>O.T.</given-names></name><name name-style=\"western\"><surname>Aky&#252;z</surname><given-names>A.O.</given-names></name><name name-style=\"western\"><surname>Erdem</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Erdem</surname><given-names>E.</given-names></name></person-group><article-title>An objective deghosting quality metric for HDR images</article-title><source>Computer Graphics Forum</source><publisher-name>Wiley Online Library</publisher-name><publisher-loc>Hoboken, NJ, USA</publisher-loc><year>2016</year><volume>Volume 35</volume><fpage>139</fpage><lpage>152</lpage></element-citation></ref><ref id=\"B31-sensors-25-07013\"><label>31.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mantiuk</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>K.J.</given-names></name><name name-style=\"western\"><surname>Rempel</surname><given-names>A.G.</given-names></name><name name-style=\"western\"><surname>Heidrich</surname><given-names>W.</given-names></name></person-group><article-title>HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions</article-title><source>ACM Trans. Graph. (TOG)</source><year>2011</year><volume>30</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type=\"doi\">10.1145/2010324.1964935</pub-id></element-citation></ref><ref id=\"B32-sensors-25-07013\"><label>32.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Chan</surname><given-names>T.N.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Hou</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Chau</surname><given-names>L.P.</given-names></name></person-group><article-title>Attention-guided progressive neural texture fusion for high dynamic range image restoration</article-title><source>IEEE Trans. Image Process.</source><year>2022</year><volume>31</volume><fpage>2661</fpage><lpage>2672</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2022.3160070</pub-id><pub-id pub-id-type=\"pmid\">35316184</pub-id></element-citation></ref><ref id=\"B33-sensors-25-07013\"><label>33.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>From dynamic to static: Stepwisely generate HDR image for ghost removal</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>35</volume><fpage>1409</fpage><lpage>1421</lpage><pub-id pub-id-type=\"doi\">10.1109/TCSVT.2024.3467259</pub-id></element-citation></ref><ref id=\"B34-sensors-25-07013\"><label>34.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Niu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Lau</surname><given-names>R.W.</given-names></name></person-group><article-title>HDR-GAN: HDR image reconstruction from multi-exposed LDR images with large motions</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>3885</fpage><lpage>3896</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2021.3064433</pub-id><pub-id pub-id-type=\"pmid\">33764875</pub-id></element-citation></ref><ref id=\"B35-sensors-25-07013\"><label>35.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Van Gool</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Toward high-quality HDR deghosting with conditional diffusion models</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2023</year><volume>34</volume><fpage>4011</fpage><lpage>4026</lpage><pub-id pub-id-type=\"doi\">10.1109/TCSVT.2023.3326293</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-07013-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>The HL-HDR framework consists of two main stages. First, the cross-frame feature alignment stage uses the Frequency Alignment Module to align overexposed and underexposed images to the reference frame. Second, the feature fusion stage stacks multiple Frequency Decomposition Processing Blocks to extract and integrate features, reconstructing high-quality, ghost-free HDR images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>The architecture of the proposed FAM decomposes both the non-reference frames and the reference frame into high-frequency and low-frequency components, which are then aligned separately.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Visualization of the high-frequency components from three examples. Distinct structural edges and fine detail lines can be clearly observed, indicating that the use of the AvgPool operation effectively and accurately separates the high-frequency information.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>The architecture of the proposed LFE is comprised of a series of standard convolutions and dense residual connections.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>The architecture of the proposed GFE is designed for extracting low-frequency features.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>The architecture of the proposed CSFM. CSFM is a cross-scale fusion module that leverages wavelet transforms to effectively merge feature maps of different spatial resolutions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Examples of Kalantari et al.&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>].</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Examples of Tel et al.&#8217;s dataset [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>].</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g008.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f009\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Examples of Hu et al.&#8217;s dataset [<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>].</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g009.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f010\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Example from Sen et al.&#8217;s dataset [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>].</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g010.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f011\" orientation=\"portrait\"><label>Figure 11</label><caption><p>Example from Tursen et al.&#8217;s dataset [<xref rid=\"B30-sensors-25-07013\" ref-type=\"bibr\">30</xref>].</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g011.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07013-f012\" orientation=\"portrait\"><label>Figure 12</label><caption><p>Comparison of different alignment strategies.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07013-g012.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-07013-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t001_Table 1</object-id><label>Table 1</label><caption><p>Quantitative comparisons on Kalantari&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>]. The best results are highlighted in red, and the second-best results are highlighted in blue.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm106\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm107\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DHDR [<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.64</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">40.91</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9869</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9858</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">60.50</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">43.62</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9900</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9862</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.30</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">NHDRR [<xref rid=\"B19-sensors-25-07013\" ref-type=\"bibr\">19</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.41</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9887</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9861</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">61.21</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HDR-GAN [<xref rid=\"B34-sensors-25-07013\" ref-type=\"bibr\">34</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">43.92</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9905</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9865</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.45</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">APNT [<xref rid=\"B32-sensors-25-07013\" ref-type=\"bibr\">32</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">43.94</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9898</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9879</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">64.05</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.32</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.18</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9916</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9884</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.03</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HyHDR [<xref rid=\"B21-sensors-25-07013\" ref-type=\"bibr\">21</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.64</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.47</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9915</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9894</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.05</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.11</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.73</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9911</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9885</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.52</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.43</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.21</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9918</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9891</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">66.64</named-content>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PGN [<xref rid=\"B33-sensors-25-07013\" ref-type=\"bibr\">33</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.73</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.27</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9918</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9890</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.08</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.66</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">43.18</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9919</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9901</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.11</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">LFDiff [<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">44.76</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.59</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9919</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9906</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.54</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">44.81</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">42.69</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9921</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9901</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">66.71</named-content>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative comparisons on Tel&#8217;s dataset [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>]. The best results are highlighted in red, and the second-best results are highlighted in blue.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm108\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm109\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DHDR [<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">40.05</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">43.37</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9794</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9924</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">67.09</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">45.30</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9837</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9943</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.80</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">NHDRR [<xref rid=\"B19-sensors-25-07013\" ref-type=\"bibr\">19</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">36.68</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">39.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9590</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9853</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.41</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HDR-GAN [<xref rid=\"B34-sensors-25-07013\" ref-type=\"bibr\">34</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.71</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.87</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9832</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9949</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.57</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">46.35</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9844</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9948</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.23</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.55</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">47.51</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9850</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9952</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">70.66</named-content>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.18</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">45.63</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9841</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9946</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.88</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">42.68</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">47.46</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9792</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9955</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.16</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">43.30</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">47.83</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9878</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9957</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">70.73</named-content>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t003_Table 3</object-id><label>Table 3</label><caption><p>Quantitative comparisons on Hu&#8217;s dataset [<xref rid=\"B29-sensors-25-07013\" ref-type=\"bibr\">29</xref>]. The best results are highlighted in red, and the second-best results are highlighted in blue.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm110\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm111\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DHDR [<xref rid=\"B9-sensors-25-07013\" ref-type=\"bibr\">9</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.20</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9870</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9941</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.82</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">45.76</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">49.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9956</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9980</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.04</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">NHDRR [<xref rid=\"B19-sensors-25-07013\" ref-type=\"bibr\">19</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">45.15</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.75</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9956</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9981</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">74.86</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HDR-GAN [<xref rid=\"B34-sensors-25-07013\" ref-type=\"bibr\">34</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">45.86</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">49.14</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9945</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9989</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.19</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">APNT [<xref rid=\"B32-sensors-25-07013\" ref-type=\"bibr\">32</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">46.41</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">47.97</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9953</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9986</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.06</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.10</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">51.17</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9947</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9989</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.12</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HyHDR [<xref rid=\"B21-sensors-25-07013\" ref-type=\"bibr\">21</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.46</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">51.91</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9959</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9991</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.24</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">50.23</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9954</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9989</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.22</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SCTNet [<xref rid=\"B14-sensors-25-07013\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.10</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">51.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9963</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9991</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.14</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PGN [<xref rid=\"B33-sensors-25-07013\" ref-type=\"bibr\">33</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">48.66</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">52.49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9965</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9992</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.33</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">47.18</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">49.35</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9951</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9990</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.83</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">LFDiff [<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">48.74</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">52.10</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9968</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9993</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">77.35</named-content>\n</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">49.02</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">52.92</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9970</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #0000FF\">0.9992</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">77.55</named-content>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t004_Table 4</object-id><label>Table 4</label><caption><p>Average runtime performance of various methods on the testing set of Kalantari&#8217;s dataset [<xref rid=\"B8-sensors-25-07013\" ref-type=\"bibr\">8</xref>]. The inference times are measured on a single A100 GPU under a resolution of <inline-formula><mml:math id=\"mm112\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1500</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><table frame=\"hsides\" rules=\"groups\"><tbody><tr><td align=\"left\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Methods</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Sen [<xref rid=\"B7-sensors-25-07013\" ref-type=\"bibr\">7</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Hu [<xref rid=\"B6-sensors-25-07013\" ref-type=\"bibr\">6</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">AHDR [<xref rid=\"B10-sensors-25-07013\" ref-type=\"bibr\">10</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">CA-ViT [<xref rid=\"B13-sensors-25-07013\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DiffHDR [<xref rid=\"B35-sensors-25-07013\" ref-type=\"bibr\">35</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SAFNet [<xref rid=\"B11-sensors-25-07013\" ref-type=\"bibr\">11</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">LFDiff [<xref rid=\"B12-sensors-25-07013\" ref-type=\"bibr\">12</xref>]</td><td align=\"center\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Ours</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Environment</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(CPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(CPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(GPU)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>Times (s)</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">61.81 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">79.77 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.30 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.34 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">7.53 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.13 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.72 s</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.21 s</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Para. (M)</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#8211;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#8211;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">1.24 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">1.22 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">74.99 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">1.12 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">7.48 M</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">4.08 M</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of different alignment modules. The best results are highlighted in red.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Alignment Module</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm113\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm114\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">None</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.53</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.06</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9917</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9890</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.24</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">AHDR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.62</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9919</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9892</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.37</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>FAM (Ours)</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">44.81</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">42.69</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9921</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9906</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">66.71</named-content>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t006\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of different alignment strategies for high- and low-frequency features. The best results are highlighted in red.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm115\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm116\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(1) No separation, optical flow</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.67</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">42.81</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9920</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9904</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.51</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(2) No separation, conv + attention</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.62</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9919</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9892</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.43</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(3) Separation, optical flow</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.65</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.67</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9920</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9896</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.49</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(4) Separation, conv + attention</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.51</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.14</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9920</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9898</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.55</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>FAM (Ours)</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">44.81</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">42.69</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9921</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9906</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">66.71</named-content>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07013-t007\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07013-t007_Table 7</object-id><label>Table 7</label><caption><p>Ablation study of the proposed FDPB. The best results are highlighted in red.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<inline-formula><mml:math id=\"mm117\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">PSNR-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<inline-formula><mml:math id=\"mm118\" overflow=\"scroll\"><mml:mrow><mml:mstyle mathvariant=\"bold\"><mml:mi mathvariant=\"bold-italic\">&#956;</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">SSIM-<italic toggle=\"yes\">l</italic></th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HDR-VDP-2</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(1) No decomposition, GFE+LFE</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9919</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9896</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.37</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(2) Frequency decomposition, swapped</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.45</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9918</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9895</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.14</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(3) Frequency decomposition, LFE only</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9918</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9892</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.06</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(4) Frequency decomposition, GFE only</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.64</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">42.88</named-content>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9920</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9900</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.64</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(5) Low-freq w/o dense res.</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.68</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.59</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9920</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9902</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.61</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(6) High-freq w/o CSFM</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">44.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.56</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9919</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9901</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.48</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>FDPB (Ours)</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">44.81</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">42.69</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9921</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">0.9906</named-content>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<named-content content-type=\"color: #FF0000\">66.71</named-content>\n</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656168 PMC12656168.1 12656168 12656168 41305218 10.3390/s25227013 sensors-25-07013 1 Article Ghost-Free HDR Imaging in Dynamic Scenes via High&#8211;Low-Frequency Decomposition &#8224; https://orcid.org/0000-0003-3270-0020 Zhang Xiang Conceptualization Resources Funding acquisition Methodology 1 * Chen Genggeng Conceptualization Methodology Writing &#8211; original draft Writing &#8211; review &amp; editing 1 Zhang Fan Resources 1 https://orcid.org/0009-0000-2829-2293 Zhang Yongzhong Resources 2 Huang Shih-Chia Academic Editor 1 College of Information and Control Engineering, Xi&#8217;an University of Architecture and Technology, Xi&#8217;an 710055, China; chengeng0613@xauat.edu.cn (G.C.); 2 China United Network Communications Group Co., Ltd., Shaanxi Branch, Xi&#8217;an 710000, China * Correspondence: zhangxiang@xauat.edu.cn &#8224; This paper is an extended version of our paper published in the Zhang, X.; Chen, G.; Hu, T.; Yang, K.; Zhang, F.; Yan, Q. HL-HDR: Multi-Exposure High Dynamic Range Reconstruction with High-Low Frequency Decomposition. In Proceedings of the 2024 International Joint Conference on Neural Networks (IJCNN), Yokohama, Japan, 30 June&#8211;5 July 2024; IEEE: Piscataway, NJ, USA, 2024; pp. 1&#8211;9. 17 11 2025 11 2025 25 22 501335 7013 12 10 2025 04 11 2025 14 11 2025 17 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Generating high-quality high-dynamic-range (HDR) images in dynamic scenes remains a challenging task. Recently, Transformers have been introduced into HDR imaging and have demonstrated superior performance over traditional convolutional neural networks (CNNs) in handling large-scale motion. However, due to the low-pass filtering nature of self-attention, Transformers tend to weaken the capture of high-frequency information, which impairs the recovery of structural details. In addition, their high computational complexity limits practical applications. To address these issues, we propose HL-HDR, a high&#8211;low-frequency-aware ghost-free HDR reconstruction network for dynamic scenes. By decomposing features into high- and low-frequency components, HL-HDR effectively overcomes the limitations of existing Transformer and CNN-based methods. The Frequency Alignment Module (FAM) captures large-scale motion in the low-frequency branch while refining local details in the high-frequency branch. The Frequency Decomposition Processing Block (FDPB) fuses local high-frequency details and global low-frequency context, enabling precise HDR reconstruction. Extensive experiments on five public HDR datasets demonstrate that HL-HDR consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluation. high dynamic range imaging ghost-free HDR High-Low Frequency Decomposition Science and Technology Development Program Projects of the Housing and Urban-Rural Development Department of Shaanxi Province 2023-k48 This research was funded by the Science and Technology Development Program Projects of the Housing and Urban-Rural Development Department of Shaanxi Province (Grant No. 2023-k48). The APC was funded by the same agency. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Contemporary digital cameras are limited by the capabilities of their sensors, making it difficult to capture the full dynamic range of real-world scenes. In contrast, High-Dynamic-Range (HDR) imaging can encompass a much wider range of light intensities, providing a more accurate representation of real-world luminance distributions. HDR imaging has become a key technique in modern visual applications, capable of faithfully reproducing lighting in complex scenes that contain both extremely bright and dark regions. Typical applications include photography and cinematography, where HDR enhances tonal depth and detail representation; virtual reality and game rendering, where it improves lighting realism and immersive experience; and autonomous driving and intelligent surveillance, where it helps maintain clear visibility of critical targets under challenging illumination conditions. There are various approaches to generating HDR images, among which one of the most common methods is to reconstruct an HDR image by fusing multiple Low-Dynamic-Range (LDR) images captured under different exposure settings. However, during multi-exposure HDR reconstruction, object motion or camera shake may cause temporal inconsistencies or information loss due to overexposure, resulting in ghosting artifacts. This phenomenon remains one of the major challenges in multi-exposure HDR imaging. To tackle the challenges associated with ghosting in HDR imaging, various methodologies have been developed. Traditional techniques commonly employ methods such as alignment-based methods [ 1 , 2 ], rejection-based methods [ 3 , 4 , 5 ], and patch-based methods [ 6 , 7 ] to eliminate or align motion regions in images. However, the efficacy of these methods is largely contingent upon the performance of preprocessing techniques, such as optical flow and motion detection. And when dealing with significant scene motion, the results of these methods typically turn out to be rather unsatisfactory. With the advancement of Deep Neural Networks (DNN), several CNN-based methods [ 8 , 9 , 10 , 11 , 12 ] have been applied in ghost-free HDR imaging. Among them, the &#8220;alignment-fusion&#8221; paradigm has shown remarkable success, especially in scenarios involving large-scale motion. Moreover, Transformer-based approaches [ 13 , 14 ], which can capture long-distance dependencies, are introduced as an alternative to CNNs. These methods further enhance HDR imaging performance and are adopted by the current mainstream state-of-the-art methods. However, Transformers still face two major challenges in ghost-free HDR imaging. On one hand, local details and global information are crucial for restoring multi-frame HDR content, while the self-attention mechanism of pure Transformers often exhibits a low-pass filtering effect, reducing the variance of input features and overly smoothing patch tokens. This occurs because self-attention essentially averages features across different patches, suppressing high-frequency information that is vital for distinguishing fine structural details, thereby limiting the Transformer&#8217;s ability to capture high-frequency local details [ 13 ]. On the other hand, HDR images are typically high-resolution, and the computational complexity of self-attention grows quadratically with the spatial dimensions of the input feature map. This results in significant computational and memory overhead in high-resolution scenarios, restricting the practical application and scalability of Transformers in high-resolution HDR imaging tasks. Considering that the high- and low-frequency components of an image correspond to local details and global structures, respectively, we propose a frequency-decomposition-based ghost-free HDR image reconstruction network. In both the cross-frame alignment and feature fusion stages, features are decomposed into high- and low-frequency components and processed according to their respective characteristics. Since high-frequency components represent local structures while low-frequency components characterize global information, we leverage the low-pass filtering property of average pooling (AvgPool) to decouple features into high-resolution high-frequency components and low-resolution low-frequency components. Specifically, global motion or long-range dependencies can be effectively represented by low-frequency features without requiring high-resolution feature maps, while high-frequency features focus on fine-grained local structures that need high-resolution maps and are better modeled by local operators. Based on this, we adopt a dual-branch architecture in both stages to balance global information and local details. In the cross-frame alignment stage, we propose the Frequency Alignment Module (FAM). The low-frequency branch employs a lightweight UNet to learn optical flow and align non-reference frames to the reference frame, efficiently capturing large-scale motion while reducing computational cost. Meanwhile, the high-frequency branch combines convolution and attention to adaptively refine edges and textures, suppressing ghosting and preserving structural consistency. In the feature fusion stage, we design the Frequency Decomposition Processing Block (FDPB). The high-frequency branch uses a Local Feature Extractor (LFE) to capture details and enhance cross-frame high-frequency information, while the low-frequency branch adopts a Global Feature Extractor (GFE) to model long-range dependencies. To alleviate information loss caused by downsampling, we further introduce a Cross-Scale Fusion Module (CSFM) for effective cross-resolution integration. By integrating FAM and FDPB, we propose the High&#8211;Low-Frequency-Aware HDR Network (HL-HDR), which consists of two stages: cross-frame alignment and feature fusion. FAM enables accurate motion modeling and detail preservation, while FDPB hierarchically captures both global and local contexts, leading to high-quality, ghost-free HDR reconstruction. The main contributions are summarized as follows: We propose a novel alignment method, FAM, in which the low-frequency branch captures large-scale motion through optical flow alignment, while the high-frequency branch refines local edges and textures, effectively suppressing ghosting. The FDPB module, introduced in our work, addresses low-frequency components by employing a multi-scale feature extraction approach in conjunction with Transformer mechanisms to collectively capture global information. For high-frequency components, we employ small convolutional kernels and densely connected residual links to effectively extract local feature information. This strategic design in our model achieves a harmonious balance between speed and precision. A plethora of experiments have substantiated that the proposed methodology, denoted as method HL-HDR, attains state-of-the-art (SOTA) performance in HDR imaging tasks. Furthermore, it yields visually appealing outcomes that align with human perceptual aesthetics. This work is an extended version of our conference paper [ 15 ] presented at the International Joint Conference on Neural Networks (IJCNN 2024). Compared with the conference version, it incorporates a substantial amount of new material. (1) To address the issue of ghosting in moving regions, we optimized the cross-frame alignment stage by designing the FAM module: the low-frequency branch aligns features using optical flow, while the high-frequency branch adaptively refines local edges and textures through convolution and attention mechanisms, effectively suppressing ghost artifacts and maintaining structural consistency. (2) We conducted comparative experiments on additional datasets and against the latest methods, fully demonstrating the advantages of our improved approach. (3) The ablation studies are more detailed and clear, thoroughly verifying and analyzing the contributions of each module. 2. Related Work Presently, HDR deghosting techniques can be primarily classified into alignment-based methods, rejection-based methods, patch-based methods, and CNN-based methods. 2.1. HDR Deghosting Methods Alignment-based Method. These methods aim to register all LDR images to a reference image using either rigid or non-rigid algorithms. Bogoni [ 1 ] utilized optical flow to estimate motion vectors, while Pece and Kautz [ 5 ] computed the Median Threshold Bitmap (MTB) for input images to detect regions of motion. Kang et al. [ 16 ] transformed the intensities of LDR images into the luminance domain by leveraging exposure time information and computed optical flow to identify corresponding pixels among the LDR images. However, both rigid and non-rigid alignment methods exhibit susceptibility to significant motions, occlusions, and variations in brightness, rendering them prone to errors in complex regions. Rejection-based Methods. Rejection methods, post the global registration procedure, discern and eliminate motion regions within the input data, followed by the fusion of static regions to reconstruct HDR images. Grosch et al. [ 3 ] devised an error map by assessing color disparities post alignment, aiming to exclude pixels with mismatches. Pece et al. [ 5 ] identified regions of motion through the utilization of a median threshold bitmap on input LDR images. Jacobs et al. [ 17 ] pinpointed areas of misalignment via an analytical approach involving weighted intensity variance analysis. These approaches often yield unsatisfactory HDR outcomes as they incur the loss of valuable information while eliminating pixels. Patch-based Methods. The patch-based methods, involving patch-wise alignment among exposure images for deghosting, have been explored in the literature. Sen et al. [ 7 ] introduced a patch-based energy minimization method that simultaneously optimizes alignment and reconstruction. In the work by Hu et al. [ 6 ], an iterative propagation of intensity and gradient information was conducted using a coarse-to-fine schedule. Ma et al. [ 18 ] proposed an approach based on structural patch decomposition, which dissects an image patch into signal strength, signal structure, and mean intensity components for the reconstruction of ghost-free images. However, it is noteworthy that these methods lack compensation for saturation and are burdened by elevated computational costs. CNN-based Methods. Kalantari et al. [ 8 ] initiated the alignment of images using optical flow and subsequently employed a CNN network for their fusion. Yan et al. [ 10 ] introduced a spatial attention mechanism based on CNN to mitigate issues related to motion and oversaturated regions. Yan et al. [ 19 ] formulated a non-local module aimed at expanding the receptive field for comprehensive global merging. In their work, Song et al. [ 20 ] harnessed the benefits of the Transformer&#8217;s extensive receptive field to globally recover areas affected by motion. Additionally, HyHDR [ 21 ] proposed an innovative patch aggregation module grounded in deep learning, strategically fusing valuable information from non-reference frames. Despite the significant performance breakthroughs achieved by these methodologies, their outcomes in both dynamic and static areas remain somewhat unsatisfactory. 2.2. Vision Transformer Transformers have demonstrated remarkable success in natural language processing. The multi-head self-attention mechanism utilized in this context effectively captures long-range correlations among word token embeddings. A recent development, Vision Transformer (ViT) [ 22 ], has illustrated that a pure Transformer architecture can be directly applied to sequences of non-overlapping image patches, exhibiting excellent performance in image classification tasks. This showcases the versatility of Transformer models beyond natural language applications, extending their efficacy to the domain of computer vision. CA-ViT, proposed by Liu et al. [ 13 ], leverages the Transformer&#8217;s capability for capturing long-range dependencies and extracting global feature information, complementing it with the ability of CNN to extract local information. The collaboration between these features has proven to be highly effective. Building upon the Transformer architecture, Zamir et al. [ 23 ] introduced improvements by incorporating a locally aware Transformer design. This design enhances the model&#8217;s perception of local image details by introducing local convolution operations within the Transformer. Our approach is inspired by [ 13 , 23 ], strategically handling local and global information differently based on their inherent characteristics. 3. Method In a series of LDR images with varying exposure levels, images of the same scene are grouped together. Each group comprises three images: underexposed, normally exposed, and overexposed. Our goal is to fuse the information from these three LDR images to reconstruct an HDR image without ghosting artifacts. In previous research [ 10 , 24 ], the authors utilized a set of three LDR images as input, designating the normally exposed image as the reference frame. Using the input images { L 1 , L 2 , L 3 } , our model derives the HDR image H ^ as follows: (1) H ^ = f ( L 1 , L 2 , L 3 ; &#952; ) , where f ( &#183; ) represents the HDR imaging function, and &#952; refers to the network&#8217;s parameters. 3.1. Overview of the HL-HDR Architecture As shown in Figure 1 , the proposed HL-HDR framework consists of two main stages: The first stage is the cross-frame alignment stage, corresponding to the Frequency Alignment Module in the figure. This module takes three images with different exposures as input and extracts shallow features through a shared-weight convolution, producing 64-channel feature maps { F 1 , F 2 , F 3 } . The feature map of the normally exposed image, F 2 , is used as the reference frame, while the underexposed feature map F 1 and overexposed feature map F 3 are aligned to it. The second stage is the feature fusion stage, in which multiple Frequency Decomposition Processing Blocks are stacked to extract and integrate the aligned features. Specifically, the features are decomposed into high-frequency and low-frequency components and processed according to their characteristics: the high-frequency branch employs a Local Feature Extractor to capture local details through stacked convolutional blocks and enhances cross-frame high-frequency information via dense connections; the low-frequency branch uses a Global Feature Extractor to model long-range dependencies through multi-layer channel attention. The extracted high- and low-frequency features are then fused. In the final reconstruction stage, the network reduces the number of channels while introducing long-range residual connections, ultimately reconstructing the output as a 3-channel HDR image. 3.2. Frequency Alignment Module To balance large-scale motion modeling and detail preservation during the alignment stage, we first decompose both the reference frame and the non-reference frames into high-frequency and low-frequency components, which are then independently aligned in separate branches. Specifically, the low-frequency components are obtained by applying average pooling to the original feature maps, while the high-frequency components are derived by subtracting the low-frequency part from the original features. This process can be formally expressed as follows: (2) f il = Avgpool ( F i ) , (3) f &#8242; = Up ( f il ) , f ih = F i &#8722; f &#8242; , (4) f 2 l = Avgpool ( F 2 ) , (5) f &#8243; = Up ( f 2 l ) , f 2 h = F 2 &#8722; f &#8243; , where Avgpool ( &#183; ) refers to average pooling, Up ( &#183; ) stands for bilinear interpolation upsampling. F i refers to the non-reference frame, while F 2 refers to the reference frame. For the alignment of high-frequency components, the high-frequency features of the reference frame and non-reference frames are concatenated and fed into a convolution-based attention module to generate an attention map, which is subsequently applied to the non-reference frame features. This module is similar to the implicit alignment mechanism in AHDR [ 10 ], guiding the network to focus on critical information across different exposures. Under the guidance of attention, the model can adaptively reweight the importance of different regions, thereby refining edges and textures, enhancing local detail restoration, and effectively suppressing ghosting while ensuring structural consistency. Formally, the process can be written as: (6) F am = AM ( Concat ( f ih , f 2 h ) ) , (7) F h = f ih &#183; F am , where AM ( &#183; ) refers to the attention module, and ( &#183; ) denotes element-wise multiplication. For the alignment of low-frequency components, we modify the Encoder-Decoder structure of SAFNet [ 11 ] to predict the optical flow field, which is then used to warp the low-frequency components of the non-reference frames, enabling more accurate modeling of large-scale motion. The above operations can be formulated as: (8) F am = FM ( f il , f 2 l ) , (9) F l = Warp ( f il , F am ) , where FM ( &#183; ) refers to the Encoder-Decoder structure shown in Figure 2 , and Warp ( &#183; ) denotes the warping operation. Finally, the aligned low-frequency and high-frequency components are fused, followed by convolution and a channel attention module to restore channel information and further extract features. This process can be formally expressed as follows: (10) F out = Conv 3 ( CA ( Conv 1 ( fusion ( Concat ( F l , F h ) ) ) ) , where fusion ( &#183; ) denotes the operation for integrating features at different scales, CA ( &#183; ) represents channel attention, Conv 1 ( &#183; ) stands for a 1 &#215; 1 convolution, and Conv 3 ( &#183; ) denotes a 3 &#215; 3 convolution. 3.3. Frequency Decomposition Processing Block To more clearly illustrate the high-frequency information, we present three representative examples in Figure 3 . The visualizations demonstrate that structural edges and fine-grained details are effectively captured, confirming the reliability of the &#8220;average pooling + subtraction&#8221; operation for separating high- and low-frequency information. Motivated by this observation, we adopt a similar frequency decomposition strategy during the feature fusion stage to further enhance the network&#8217;s representational capacity. As shown in Figure 1 , similar to the alignment stage, we also decompose the feature maps into high-frequency and low-frequency components during the feature fusion stage. By supplementing high-frequency details while extracting the global and background information of the image, this approach enhances local textures and edges, resulting in improved visual quality. This process can be formally expressed as follows: (11) F low = Avgpool ( F ) , (12) F &#8242; = Up ( F low ) , F high = F &#8722; F &#8242; , (13) F out = F + Conv 3 ( CA ( Conv 1 ( Concat ( LFE ( F high ) , GFE ( F low ) ) ) ) , where Avgpool ( &#183; ) refers to average pooling, Up ( &#183; ) stands for bilinear interpolation upsampling, LFE ( &#183; ) stands for Local Feature Extractor, GFE ( &#183; ) stands for Global Feature Extractor, CA ( &#183; ) denotes channel attention, Conv 3 ( &#183; ) stands for 3 &#215; 3 convolution, and Conv 1 ( &#183; ) stands for 1 &#215; 1 convolution that restores the channel count from 128 to 64. 3.3.1. Local Feature Extractor To better recover the detailed information in an image, we process the high-frequency information, which inherently contains a wealth of detail. High-frequency information requires local details; thus, the use of convolutions with small kernels allows for a more focused extraction of these details. Moreover, given the superior capability of standard residual learning in maintaining stable feature propagation and enhancing high-frequency detail representation [ 25 , 26 , 27 ], we incorporate dense residual connections into the high-frequency information extraction process to fully leverage multi-level features and strengthen high-frequency detail modeling. Overall, we utilize six 3 &#215; 3 convolutions. As depicted in Figure 4 , we only display a portion of the residual connections, but in reality, these are dense residual connections. They are not merely connections between adjacent layers, but rather, the output of each layer is merged with the outputs of all preceding layers, enabling each layer to directly access the feature information of all previous layers. 3.3.2. Global Feature Extractor For low-frequency information, it is necessary to leverage global context to restore the overall structure and background of the image. As shown in Figure 5 , although multi-scale feature extraction enables long-range information interactions, some information may be lost during the downsampling process. To address this, we perform feature extraction at each scale to compensate for the information loss caused by downsampling. In each feature extraction layer, we introduce a Channel Transformer Block, which can establish global contextual information and possesses a global receptive field, making it highly suitable for extracting low-frequency features that depend on global information. Furthermore, to compensate for potential information loss when directly upsampling feature maps of different sizes and concatenating them, we employ the CSFM to effectively merge feature maps of varying resolutions. Channel Transformer Block. Given that the channel-wise self-attention mechanism proposed in [ 23 ] can effectively model cross-channel dependencies while reducing computational complexity, our Transformer architecture abandons spatial self-attention and instead adopts channel-wise self-attention to achieve more efficient feature modeling. The input X &#8712; R H &#215; W &#215; C is first layer-normalized to obtain a tensor Y &#8712; R H &#215; W &#215; C . Then, 1 &#215; 1 convolutions are applied to aggregate pixel-wise cross-channel context, followed by 3 &#215; 3 depth-wise convolutions to encode channel-wise spatial context. This process generates the q u e r y ( Q ), k e y ( K ), and v a l u e ( V ), which can be expressed mathematically as: (14) Q = W p Q W d Q Y , &#160; K = W p K W d K Y , &#160; V = W p V W d V Y , where W p ( &#183; ) represents the 1 &#215; 1 point-wise convolution and W d ( &#183; ) represents the 3 &#215; 3 depth-wise convolution. Then reshape Q into R H W &#215; C , reshape K into R C &#215; H W . After this transformation, matrix multiplication can be performed, followed by a softmax operation to obtain an attention map A &#8712; R C &#215; C . Reshape V into R H W &#215; C , allowing for matrix multiplication with A. The resulting output is reshaped into R H &#215; W &#215; C , and finally, a residual connection is added by summing the initial feature map with the obtained feature map. The specific process is illustrated as follows: (15) A t t e n t i o n ( Q , K , V ) = V &#183; S o f t m a x ( Q &#183; K / &#945; ) , (16) X ^ = A t t e n t i o n ( Q ^ , K ^ , V ^ ) + X , where &#945; is a learnable scaling parameter, X refers to the initial input feature map, and X ^ refers to the final result. Next, we utilize 1 &#215; 1 Convolution to aggregate information from different channels and employ 3 &#215; 3 depth-wise Convolution to aggregate information from spatially neighboring pixels. Additionally, we incorporate a gating mechanism to enhance information encoding. Finally, a long-range residual connection is added, summing the initial feature map with the feature map obtained at this stage. Cross-Scale Fusion Module. Due to the differing spatial resolutions of features at various scales, they are typically adjusted to a unified resolution via downsampling or upsampling for feature fusion. However, such operations may lead to the loss of important structural details, thereby affecting the final image restoration. To alleviate this problem, we introduce a wavelet-based cross-scale feature fusion strategy that fully leverages the capability of wavelet transforms in representing multi-scale image structures [ 28 ]. As shown in Figure 6 , we employ the Haar Discrete Wavelet Transform (DWT) to decompose a feature map F b &#8712; R H &#215; W &#215; C into four sub-bands: LL (Low&#8211;Low), LH (Low&#8211;High), HL (High&#8211;Low), and HH (High&#8211;High). Here, &#8220;L&#8221; and &#8220;H&#8221; denote low-pass and high-pass filtering along the horizontal and vertical dimensions, respectively. Each sub-band has half the spatial resolution of the original feature map, while the number of channels remains unchanged. Among these, the LL sub-band retains the low-frequency components, representing the global structure and smooth regions of the feature. It is concatenated with the small-scale feature map F s &#8712; R H 2 &#215; W 2 &#215; C , followed by a 1 &#215; 1 convolution for channel reduction (from 128 to 64) and a residual block for feature refinement. The HH, HL, and LH sub-bands preserve the high-frequency components, containing texture and edge details. After concatenation, a 1 &#215; 1 convolution reduces the channels to 64, followed by a residual block for detailed feature extraction, and another 1 &#215; 1 convolution restores the channel number to 192. The use of two 1 &#215; 1 convolutions effectively reduces the number of parameters and computational complexity, since directly processing a 192-channel feature map would be computationally expensive. Finally, the refined high- and low-frequency features are recombined through the Inverse Discrete Wavelet Transform (IDWT) to produce the fused representation. This process can be formally expressed as follows: (17) H H , H L , L H , L L = DWT ( F b ) , (18) f b = Conv 1 ( Res ( Conv 1 ( Concat ( H L , L H , L L ) ) ) ) , (19) f s = Conv 1 ( Res ( Concat ( F s , L L ) ) ) , (20) F = IDWT ( f b , f s ) , where R e s ( &#183; ) represents the residual block. 3.4. Training Loss Due to the typical display of HDR images after tonemapping, training the network on tonemapped images is more effective than training directly in the HDR domain. When provided with an HDR image H in the HDR domain, we compress the image&#8217;s range using the &#956; -law transformation. (21) T ( H ) = log ( 1 + &#956; H ) log ( 1 + &#956; ) , where &#956; represents a parameter that defines the degree of compression, and T ( H ) represents the tonemapped image. Throughout this work, we maintain H within the range [0, 1] and set &#956; to 5000. H ^ is the predicted result obtained from our HL-HDR model, and H is the Ground Truth. Here, we employ L 1 loss to compute the loss. Additionally, we use an auxiliary perceptual loss L p for supervision [ 13 ]. The perceptual loss measures the difference between the output image and the Ground Truth image in the feature representations of multiple layers in a pre-trained CNN, achieved by computing the mean squared error between the feature maps of each layer. We can express this as follows: (22) L 1 = &#8741; T ( H ) &#8722; T ( H ^ ) &#8741; 1 , (23) L p = &#8741; &#952; j ( T ( H ) ) &#8722; &#952; j ( T ( H ^ ) ) &#8741; 1 , where &#952; j represents the j t h convolutional feature extracted from the pre-trained VGG-16 network, with j denoting the j t h layer. Therefore, our final loss function is the result of adding L 1 and L p , with different weights assigned to each, for which we introduce a parameter &#955; . The final loss function can be expressed by the following formula: (24) L t o t a l = L 1 + &#955; L p , where &#955; is set to 0.01. 4. Experiments 4.1. Experiments Settings Datasets. The proposed method has been trained on three distinct datasets: Kalantari&#8217;s dataset [ 8 ], Tel&#8217;s dataset [ 14 ], and Hu&#8217;s dataset [ 29 ]. Kalantari&#8217;s dataset consists of 74 training samples and 15 testing samples captured from real-world scenes, with exposure values set at {&#8722;2, 0, +2} and {&#8722;3, 0, +3}. Tel&#8217;s dataset comprises 108 training samples and 36 testing samples. For Hu&#8217;s dataset, the first 85 samples were used for training, while the remaining 15 were reserved for testing. This dataset employs an exposure bias of {&#8722;2, 0, +2} and is synthetically generated using a game engine sensor. To evaluate the effectiveness and generalization capability of the proposed model, we conducted tests on Sen&#8217;s dataset [ 7 ] and Tursun&#8217;s dataset [ 30 ] using weights pre-trained on Kalantari&#8217;s dataset. Since these two datasets contain only LDR images at different exposure levels and lack ground truth, the performance comparison across methods is limited to subjective assessment. Evaluation Metrics. We use five objective measures for quantitative comparison: PSNR- &#956; , SSIM- &#956; , PSNR- l , SSIM- l , and HDR-VDP-2 [ 31 ], where &#956; and l indicate that the metrics are computed in the tonemapped domain and the linear domain, respectively. Among these, HDR-VDP-2 is a perceptual quality metric specifically designed for HDR images. It models the human visual system&#8217;s sensitivity to luminance, contrast, and local structural variations, providing a more accurate assessment of perceived image distortions compared to traditional pixel-wise metrics. Implementation Details. Our implementation is based on PyTorch 3.9. Before training, we sample 256 &#215; 256 patches from the dataset with a stride of 64. To enhance the diversity of the training data, we apply data augmentation techniques including rotation and flipping, as well as their combinations, resulting in six different augmentation strategies. We employ the Adam optimizer with a batch size of 8 and an initial learning rate of 2 &#215; 10 &#8722; 4 , which is reduced every 70 epochs. The model is trained for a total of 250 epochs on a single NVIDIA GeForce RTX 4090 GPU (NVIDIA, Santa Clara, CA, USA). 4.2. Comparison with the State-of-the-Art Methods To comprehensively evaluate the performance of our model, we compared it against representative state-of-the-art deep learning-based approaches spanning different architectural paradigms. Specifically, we considered six CNN-based methods, including DHDR [ 9 ], AHDR [ 10 ], NHDRR [ 19 ], APNT [ 32 ], PGN [ 33 ], and SAFNet [ 11 ]; one GAN-based method, HDR-GAN [ 34 ]; three Transformer-based models, namely CA-ViT [ 13 ], SCTNet [ 14 ], and HyHDR [ 21 ]; as well as two diffusion-based methods, DiffHDR [ 35 ] and LFDiff [ 12 ]. Datasets with Ground Truth. Table 1 , Table 2 and Table 3 presents the quantitative results of HL-HDR on three datasets. Our method is compared against several state-of-the-art approaches using the testing data from [ 8 , 14 , 29 ], which consist of challenging samples characterized by saturated backgrounds and foreground motions. The average of all quantitative results is computed across the testing images. Notably, our method performs remarkably well on Kalantari&#8217;s dataset [ 8 ], achieving state-of-the-art performance in PSNR- &#956; , along with competitive results in other metrics. On Hu&#8217;s dataset [ 29 ], our method achieves strong performance in both PSNR- &#956; and PSNR- l , with PSNR- l outperforming the second-best approach by 0.82 dB. On Tel&#8217;s dataset [ 14 ], our method demonstrates overall superiority, where both PSNR- &#956; and PSNR- l substantially surpass the second-best approach, with gains of 0.62 dB and 0.32 dB, respectively. In Figure 7 , Figure 8 and Figure 9 , the datasets present significant challenges due to large-scale foreground motion and severe over/under-exposed regions. We qualitatively compare our method with several state-of-the-art approaches. Most competing methods suffer from ghosting artifacts in regions with motion and saturation. On Kalantari&#8217;s dataset [ 8 ], DHDR [ 9 ] exhibits severe ghosting, while AHDR [ 10 ], HDR-GAN [ 34 ], and SCTNet [ 14 ] not only fail to recover complete structural information but also perform poorly in detail restoration. For example, in the patch comparison shown in Figure 7 , these three methods fail to reconstruct the balcony, with sky elements incorrectly blended in, and the wall textures appear blurry. CA-VIT [ 13 ] and SCTNet [ 14 ] further suffer from blocky ghosting artifacts due to patch-based sampling. In contrast, SAFNet [ 11 ] exhibits only slight wall deformation. Our proposed method not only restores the overall structural content accurately but also excels in preserving fine details. In particular, the wall lines remain sharp and clear, demonstrating the strong capability of our model in capturing and restoring fine-grained information. In Figure 8 , we show a comparison scene from Tel&#8217;s dataset [ 14 ], where only the heads of two people exhibit slight motion. All other methods, however, produced noticeable ghosting artifacts in these motion regions. In contrast, our method accurately detects the motion areas and achieves superior image reconstruction. In Figure 9 , we show a comparison scene from Hu&#8217;s dataset [ 29 ], where the motion is much more substantial. Except for our method, all other approaches generated large ghosting regions in the motion areas, significantly degrading the visual quality. Evaluation on Datasets without Ground Truth. To evaluate the generalization capability of the proposed HDR imaging method, we tested the model trained on Kalantari&#8217;s Dataset [ 8 ] on Sen&#8217;s Dataset [ 7 ] and Tursun&#8217;s Dataset [ 30 ], both of which lack ground truth. Consequently, the quality of the generated HDR images can only be assessed through subjective visual inspection. Notably, in Figure 10 , most methods fail to recover overexposed regions, whereas our method performs exceptionally well, not only avoiding overexposure but also successfully restoring rich detail. In Figure 11 , both our method and SCTNet are visually the most appealing, with no noticeable ghosting caused by human motion. This is because the scene contains abundant background information, and Transformer-based methods can fully exploit long-range dependencies to extract information from similar regions, thereby restoring details in motion areas and generating ghost-free images. 4.3. Computational Budgets We further compared model parameters and inference times, as summarized in Table 4 . Traditional patch match&#8211;based methods, such as Sen [ 7 ] and Hu [ 6 ], exhibit very long inference times due to their reliance on CPU computation. CA-ViT [ 13 ] employs standard Transformer blocks, which results in relatively high computational cost despite a moderate number of parameters. DiffHDR [ 35 ], reconstructing HDR images from pure noise, incurs both high inference time and a large parameter count. SAFNet [ 11 ] achieves fast inference with a small model size, but its reconstruction performance still leaves room for improvement. LFDiff [ 12 ], although faster than earlier diffusion-based models, still relies on the diffusion process, and our method achieves roughly three times its inference speed. In comparison, our method attains competitive inference speed (0.21 s) on a single A100 GPU with a moderate parameter count (4.08 M), striking a good balance between computational efficiency and model capacity, while also demonstrating superior reconstruction performance. 4.4. Ablation Studies We conducted ablation experiments on the Kalantari dataset to evaluate the effectiveness of each module in our model. The following sections present the ablation analysis from three perspectives, corresponding to the main components of the model. 4.4.1. Effect of Different Alignment Modules To validate the effectiveness of the proposed Frequency Alignment Module, we compared it with two alternative models: one without any alignment module, and the other using the AHDR [ 10 ] alignment module. All other model components and parameters were kept identical. As shown in Table 5 , it is evident that the model without any alignment module achieves the lowest metrics. The model using AHDR [ 10 ] for alignment shows a slight improvement, but the gain is limited. In contrast, when aligned using our proposed FAM, both PSNR- &#956; and PSNR- l increase significantly, with PSNR- l rising by 0.63 dB. Although the model without alignment achieves relatively lower scores, it still outperforms several existing methods such as CA-ViT [ 13 ] and SCTNet [ 14 ], indirectly highlighting the effectiveness of our proposed FDPB. In addition, we provide visual comparisons to further demonstrate the effectiveness of our alignment strategy. As shown in Figure 12 , we select three representative examples from the test set that involve large-scale motion and overexposed regions. The model without alignment produces noticeable ghosting artifacts, while the AHDR-aligned model achieves partial improvement but still suffers from residual artifacts. In contrast, our FAM-aligned results almost completely eliminate ghosting. This superior performance can be attributed to the incorporation of optical flow within FAM, which effectively captures large-scale motion and further enhances the quality of HDR image reconstruction. 4.4.2. Ablation Analysis of Components in the Frequency Alignment Module To validate the rationale behind our proposed FAM, which decomposes high- and low-frequency features and processes them using different methods, we designed four experimental settings: (1) No high&#8211;low-frequency separation, aligning the two frames using only optical flow; (2) No high&#8211;low-frequency separation, aligning the two frames using convolution and attention(AHDR-aligned); (3) High&#8211;low-frequency separation, aligning both high- and low-frequency features using optical flow; (4) High&#8211;low-frequency separation, aligning both high- and low-frequency features using convolution and attention. Table 6 presents the comparison results of different alignment strategies for high- and low-frequency features. As shown in the table, the non-separation strategies (1) and (2) exhibit notable differences: the optical-flow-based approach achieves significantly better PSNR- l and SSIM- l compared to the convolution + attention approach, demonstrating the superiority of optical flow in handling large-scale motion. When high- and low-frequency features are separated and both aligned using optical flow (3), the performance is comparable to (1) but with only marginal improvement, indicating that separation alone does not yield substantial benefits. In contrast, fully relying on convolution and attention after separation (4) performs even worse than the non-separated cases, highlighting its limitations in capturing large-scale motion. In comparison, our proposed FAM achieves the best results in PSNR- &#956; , SSIM- &#956; , and SSIM- l , while maintaining overall stable performance. These results validate the effectiveness of combining optical flow with frequency separation in our design. 4.4.3. Ablation Analysis of Components in the Frequency Decomposition Processing Block To validate the effectiveness of the proposed FDPB, we design a more fine-grained ablation study consisting of six comparative schemes: (1) Without frequency decomposition, the aligned features are simultaneously fed into both the Global Feature Extractor (GFE) and the Local Feature Extractor (LFE); (2) With frequency decomposition, but with swapped branch functions, where the high-frequency features are processed by the Global Feature Extractor (GFE) and the low-frequency features are processed by the Local Feature Extractor (LFE); (3) With frequency decomposition, but extracting both high- and low-frequency features using the LFE; (4) With frequency decomposition, but extracting both high- and low-frequency features using the GFE; (5) Removing the dense residual connections in the low-frequency branch; (6) Removing the wavelet-based Cross-Scale Fusion Module at each layer of the high-frequency branch. Table 7 presents a comprehensive ablation study of the proposed Frequency Decomposition Processing Block (FDPB). It can be observed that performing frequency decomposition significantly enhances HDR reconstruction performance: comparing the scheme without decomposition (1) to the schemes with decomposition (3) and (4), PSNR- l is generally improved, indicating that separating high- and low-frequency features facilitates more effective extraction of global structures and local details. Notably, although scheme (4) achieves the highest PSNR- l , its PSNR- &#956; is relatively low due to insufficient processing of high-frequency information, demonstrating that relying solely on the Global Feature Extractor (GFE) is inadequate for restoring image details. The importance of matching feature types to the appropriate extractor is highlighted in scheme (2), where swapping the high-frequency and low-frequency branches leads to a noticeable performance drop, showing that high-frequency features are better handled by the Local Feature Extractor (LFE) and low-frequency features by the GFE. Furthermore, removing dense residual connections in the low-frequency branch (5) or the Cross-Scale Fusion Module in the high-frequency branch (6) results in decreased performance, emphasizing the critical role of these components in preserving structural information and enhancing details. Overall, the complete FDPB design, integrating frequency decomposition, proper branch assignment, dense residual connections, and wavelet-based cross-scale fusion, achieves the best results across all metrics, confirming its effectiveness in restoring high-quality, ghost-free HDR images. 5. Conclusions This paper presents HL-HDR, a high&#8211;low-frequency-aware HDR reconstruction network for dynamic scenes. It explicitly decomposes features into high- and low-frequency components, effectively combining global context modeling with fine detail restoration. The Frequency Alignment Module (FAM) enables precise motion estimation and structure preservation, while the Frequency Decomposition Processing Block (FDPB) supports hierarchical cross-scale feature fusion. Compared with state-of-the-art models such as LFDiff and SAFNet, HL-HDR offers clear advantages. Unlike diffusion-based models like LFDiff, which rely on iterative sampling and are computationally expensive, HL-HDR has a compact architecture, stable training, and faster convergence. Unlike lightweight CNNs such as SAFNet, which often sacrifice detail for speed, HL-HDR jointly optimizes high- and low-frequency features to balance visual quality and efficiency. Experiments on multiple public HDR benchmarks show that HL-HDR consistently improves performance. On three datasets with ground-truth HDR images, PSNR- &#956; increases by 0.05 dB, 0.62 dB, and 0.28 dB, with visual results showing better ghost removal and detail preservation. For future work, we plan to explore lightweight designs and real-time deployment, leverage diffusion models to improve generalization and robustness in complex dynamic scenes, and build larger multi-scene HDR datasets to promote practical applications. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, X.Z. and G.C.; methodology, G.C.; software, X.Z.; validation, X.Z. and G.C.; resources, F.Z. and Y.Z.; data curation, X.Z.; writing&#8212;original draft preparation, X.Z. and G.C.; writing&#8212;review and editing, X.Z. and G.C.; supervision, F.Z. and Y.Z.; funding acquisition, X.Z. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement The code is publicly available at https://github.com/chengeng0613/HL-HDR_Plus . Conflicts of Interest Author Yongzhong Zhang was employed by the company China United Network Communications Group Co. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. References 1. Bogoni L. Extending dynamic range of monochrome and color images through fusion Proceedings of the Proceedings 15th International Conference on Pattern Recognition, ICPR-2000 Barcelona, Spain 3&#8211;7 September 2000 IEEE Piscataway, NJ, USA 2000 Volume 3 7 12 2. Tomaszewska A. Mantiuk R. Image registration for multi-exposure high dynamic range image acquisition Proceedings of the International Conference on Computer Graphics, Visualization and Computer Vision Plzen, Czech Republic 29 January&#8211;1 February 2007 Volume 2 3. Grosch T. Fast and robust high dynamic range image generation with camera and object movement Vision, Model. Vis. Rwth Aachen 2006 2 277 284 4. Lee C. Li Y. Monga V. Ghost-free high dynamic range imaging via rank minimization IEEE Signal Process. Lett. 2014 21 1045 1049 10.1109/LSP.2014.2323404 5. Pece F. Kautz J. Bitmap movement detection: HDR for dynamic scenes Proceedings of the 2010 Conference on Visual Media Production London, UK 17&#8211;18 November 2010 IEEE Piscataway, NJ, USA 2010 1 8 6. Hu J. Gallo O. Pulli K. Sun X. HDR deghosting: How to deal with saturation? Proceedings of the IEEE conference on computer vision and pattern recognition Portland, OR, USA 23&#8211;28 June 2013 1163 1170 7. Sen P. Kalantari N.K. Yaesoubi M. Darabi S. Goldman D.B. Shechtman E. Robust patch-based hdr reconstruction of dynamic scenes ACM Trans. Graph. 2012 31 203 10.1145/2366145.2366222 8. Kalantari N.K. Ramamoorthi R. Deep high dynamic range imaging of dynamic scenes ACM Trans. Graph. 2017 36 144 10.1145/3072959.3073609 9. Wu S. Xu J. Tai Y.W. Tang C.K. Deep high dynamic range imaging with large foreground motions Proceedings of the European Conference on Computer Vision (ECCV) Munich, Germany 8&#8211;14 September 2018 117 132 10. Yan Q. Gong D. Shi Q. Hengel A.V.D. Shen C. Reid I. Zhang Y. Attention-guided network for ghost-free high dynamic range imaging Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Long Beach, CA, USA 15&#8211;20 June 2019 1751 1760 11. Kong L. Li B. Xiong Y. Zhang H. Gu H. Chen J. Safnet: Selective alignment fusion network for efficient hdr imaging Proceedings of the European Conference on Computer Vision Milan, Italy 29 September&#8211;4 October 2024 Springer Cham, Switzerland 2024 256 273 12. Hu T. Yan Q. Qi Y. Zhang Y. Generating Content for HDR Deghosting from Frequency View arXiv 2024 10.48550/arXiv.2404.00849 2404.00849 13. Liu Z. Wang Y. Zeng B. Liu S. Ghost-free high dynamic range imaging with context-aware transformer Proceedings of the European Conference on Computer Vision Tel Aviv, Israel 23&#8211;27 October 2022 Springer Cham, Switzerland 2022 344 360 14. Tel S. Wu Z. Zhang Y. Heyrman B. Demonceaux C. Timofte R. Ginhac D. Alignment-free HDR Deghosting with Semantics Consistent Transformer arXiv 2023 10.48550/arXiv.2305.18135 2305.18135 15. Zhang X. Chen G. Hu T. Yang K. Zhang F. Yan Q. HL-HDR: Multi-Exposure High Dynamic Range Reconstruction with High-Low Frequency Decomposition Proceedings of the 2024 International Joint Conference on Neural Networks (IJCNN) Yokohama, Japan 30 June&#8211;5 July 2024 IEEE Piscataway, NJ, USA 2024 1 9 16. Kang S.B. Uyttendaele M. Winder S. Szeliski R. High dynamic range video ACM Trans. Graph. (TOG) 2003 22 319 325 10.1145/882262.882270 17. Jacobs K. Loscos C. Ward G. Automatic high-dynamic range image generation for dynamic scenes IEEE Comput. Graph. Appl. 2008 28 84 93 10.1109/MCG.2008.23 18350936 18. Ma K. Li H. Yong H. Wang Z. Meng D. Zhang L. Robust multi-exposure image fusion: A structural patch decomposition approach IEEE Trans. Image Process. 2017 26 2519 2532 10.1109/TIP.2017.2671921 28237928 19. Yan Q. Zhang L. Liu Y. Zhu Y. Sun J. Shi Q. Zhang Y. Deep HDR imaging via a non-local network IEEE Trans. Image Process. 2020 29 4308 4322 10.1109/TIP.2020.2971346 32054579 20. Song J.W. Park Y.I. Kong K. Kwak J. Kang S.J. Selective TransHDR: Transformer-Based Selective HDR Imaging Using Ghost Region Mask Proceedings of the European Conference on Computer Vision Tel Aviv, Israel 23&#8211;27 October 2022 Springer Cham, Switzerland 2022 288 304 21. Yan Q. Chen W. Zhang S. Zhu Y. Sun J. Zhang Y. A Unified HDR Imaging Method with Pixel and Patch Level Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Vancouver, BC, Canada 17&#8211;24 June 2023 22211 22220 22. Dosovitskiy A. Beyer L. Kolesnikov A. Weissenborn D. Zhai X. Unterthiner T. Dehghani M. Minderer M. Heigold G. Gelly S. An image is worth 16x16 words: Transformers for image recognition at scale arXiv 2020 2010.11929 23. Zamir S.W. Arora A. Khan S. Hayat M. Khan F.S. Yang M.H. Restormer: Efficient transformer for high-resolution image restoration Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition New Orleans, LA, USA 18&#8211;24 June 2022 5728 5739 24. Yan Q. Zhang S. Chen W. Liu Y. Zhang Z. Zhang Y. Shi J.Q. Gong D. A lightweight network for high dynamic range imaging Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition New Orleans, LA, USA 19&#8211;20 June 2022 824 832 25. Dong J. Pan J. Yang Z. Tang J. Multi-Scale Residual Low-Pass Filter Network for Image Deblurring Proceedings of the IEEE/CVF International Conference on Computer Vision Paris, France 1&#8211;6 October 2023 12345 12354 26. Kim J. Lee J.K. Lee K.M. Accurate image super-resolution using very deep convolutional networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Las Vegas, NV, USA 27&#8211;30 June 2016 1646 1654 27. Pan J. Liu S. Sun D. Zhang J. Liu Y. Ren J. Li Z. Tang J. Lu H. Tai Y.W. Learning dual convolutional neural networks for low-level vision Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;23 June 2018 3070 3079 28. Huang J.J. Dragotti P.L. WINNet: Wavelet-inspired invertible network for image denoising IEEE Trans. Image Process. 2022 31 4377 4392 10.1109/TIP.2022.3184845 35759598 29. Hu J. Choe G. Nadir Z. Nabil O. Lee S.J. Sheikh H. Yoo Y. Polley M. Sensor-realistic synthetic data engine for multi-frame high dynamic range photography Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops Seattle, WA, USA 14&#8211;19 June 2020 516 517 30. Tursun O.T. Aky&#252;z A.O. Erdem A. Erdem E. An objective deghosting quality metric for HDR images Computer Graphics Forum Wiley Online Library Hoboken, NJ, USA 2016 Volume 35 139 152 31. Mantiuk R. Kim K.J. Rempel A.G. Heidrich W. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions ACM Trans. Graph. (TOG) 2011 30 1 14 10.1145/2010324.1964935 32. Chen J. Yang Z. Chan T.N. Li H. Hou J. Chau L.P. Attention-guided progressive neural texture fusion for high dynamic range image restoration IEEE Trans. Image Process. 2022 31 2661 2672 10.1109/TIP.2022.3160070 35316184 33. Yan Q. Yang K. Hu T. Chen G. Dai K. Wu P. Ren W. Zhang Y. From dynamic to static: Stepwisely generate HDR image for ghost removal IEEE Trans. Circuits Syst. Video Technol. 2024 35 1409 1421 10.1109/TCSVT.2024.3467259 34. Niu Y. Wu J. Liu W. Guo W. Lau R.W. HDR-GAN: HDR image reconstruction from multi-exposed LDR images with large motions IEEE Trans. Image Process. 2021 30 3885 3896 10.1109/TIP.2021.3064433 33764875 35. Yan Q. Hu T. Sun Y. Tang H. Zhu Y. Dong W. Van Gool L. Zhang Y. Toward high-quality HDR deghosting with conditional diffusion models IEEE Trans. Circuits Syst. Video Technol. 2023 34 4011 4026 10.1109/TCSVT.2023.3326293 Figure 1 The HL-HDR framework consists of two main stages. First, the cross-frame feature alignment stage uses the Frequency Alignment Module to align overexposed and underexposed images to the reference frame. Second, the feature fusion stage stacks multiple Frequency Decomposition Processing Blocks to extract and integrate features, reconstructing high-quality, ghost-free HDR images. Figure 2 The architecture of the proposed FAM decomposes both the non-reference frames and the reference frame into high-frequency and low-frequency components, which are then aligned separately. Figure 3 Visualization of the high-frequency components from three examples. Distinct structural edges and fine detail lines can be clearly observed, indicating that the use of the AvgPool operation effectively and accurately separates the high-frequency information. Figure 4 The architecture of the proposed LFE is comprised of a series of standard convolutions and dense residual connections. Figure 5 The architecture of the proposed GFE is designed for extracting low-frequency features. Figure 6 The architecture of the proposed CSFM. CSFM is a cross-scale fusion module that leverages wavelet transforms to effectively merge feature maps of different spatial resolutions. Figure 7 Examples of Kalantari et al.&#8217;s dataset [ 8 ]. Figure 8 Examples of Tel et al.&#8217;s dataset [ 14 ]. Figure 9 Examples of Hu et al.&#8217;s dataset [ 29 ]. Figure 10 Example from Sen et al.&#8217;s dataset [ 7 ]. Figure 11 Example from Tursen et al.&#8217;s dataset [ 30 ]. Figure 12 Comparison of different alignment strategies. sensors-25-07013-t001_Table 1 Table 1 Quantitative comparisons on Kalantari&#8217;s dataset [ 8 ]. The best results are highlighted in red, and the second-best results are highlighted in blue. Methods PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 DHDR [ 9 ] 41.64 40.91 0.9869 0.9858 60.50 AHDR [ 10 ] 43.62 41.03 0.9900 0.9862 62.30 NHDRR [ 19 ] 42.41 41.08 0.9887 0.9861 61.21 HDR-GAN [ 34 ] 43.92 41.57 0.9905 0.9865 65.45 APNT [ 32 ] 43.94 41.61 0.9898 0.9879 64.05 CA-ViT [ 13 ] 44.32 42.18 0.9916 0.9884 66.03 HyHDR [ 21 ] 44.64 42.47 0.9915 0.9894 66.05 DiffHDR [ 35 ] 44.11 41.73 0.9911 0.9885 65.52 SCTNet [ 14 ] 44.43 42.21 0.9918 0.9891 66.64 PGN [ 33 ] 44.73 42.27 0.9918 0.9890 66.08 SAFNet [ 11 ] 44.66 43.18 0.9919 0.9901 66.11 LFDiff [ 12 ] 44.76 42.59 0.9919 0.9906 66.54 Ours 44.81 42.69 0.9921 0.9901 66.71 sensors-25-07013-t002_Table 2 Table 2 Quantitative comparisons on Tel&#8217;s dataset [ 14 ]. The best results are highlighted in red, and the second-best results are highlighted in blue. Methods PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 DHDR [ 9 ] 40.05 43.37 0.9794 0.9924 67.09 AHDR [ 10 ] 42.08 45.30 0.9837 0.9943 68.80 NHDRR [ 19 ] 36.68 39.61 0.9590 0.9853 65.41 HDR-GAN [ 34 ] 41.71 44.87 0.9832 0.9949 69.57 CA-ViT [ 13 ] 42.39 46.35 0.9844 0.9948 69.23 SCTNet [ 14 ] 42.55 47.51 0.9850 0.9952 70.66 DiffHDR [ 35 ] 42.18 45.63 0.9841 0.9946 69.88 SAFNet [ 11 ] 42.68 47.46 0.9792 0.9955 68.16 Ours 43.30 47.83 0.9878 0.9957 70.73 sensors-25-07013-t003_Table 3 Table 3 Quantitative comparisons on Hu&#8217;s dataset [ 29 ]. The best results are highlighted in red, and the second-best results are highlighted in blue. Methods PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 DHDR [ 9 ] 41.13 41.20 0.9870 0.9941 70.82 AHDR [ 10 ] 45.76 49.22 0.9956 0.9980 75.04 NHDRR [ 19 ] 45.15 48.75 0.9956 0.9981 74.86 HDR-GAN [ 34 ] 45.86 49.14 0.9945 0.9989 75.19 APNT [ 32 ] 46.41 47.97 0.9953 0.9986 73.06 CA-ViT [ 13 ] 48.10 51.17 0.9947 0.9989 77.12 HyHDR [ 21 ] 48.46 51.91 0.9959 0.9991 77.24 DiffHDR [ 35 ] 48.03 50.23 0.9954 0.9989 76.22 SCTNet [ 14 ] 48.10 51.03 0.9963 0.9991 77.14 PGN [ 33 ] 48.66 52.49 0.9965 0.9992 77.33 SAFNet [ 11 ] 47.18 49.35 0.9951 0.9990 76.83 LFDiff [ 12 ] 48.74 52.10 0.9968 0.9993 77.35 Ours 49.02 52.92 0.9970 0.9992 77.55 sensors-25-07013-t004_Table 4 Table 4 Average runtime performance of various methods on the testing set of Kalantari&#8217;s dataset [ 8 ]. The inference times are measured on a single A100 GPU under a resolution of 1500 &#215; 1000 . Methods Sen [ 7 ] Hu [ 6 ] AHDR [ 10 ] CA-ViT [ 13 ] DiffHDR [ 35 ] SAFNet [ 11 ] LFDiff [ 12 ] Ours Environment (CPU) (CPU) (GPU) (GPU) (GPU) (GPU) (GPU) (GPU) Times (s) 61.81 s 79.77 s 0.30 s 5.34 s 7.53 s 0.13 s 0.72 s 0.21 s Para. (M) &#8211; &#8211; 1.24 M 1.22 M 74.99 M 1.12 M 7.48 M 4.08 M sensors-25-07013-t005_Table 5 Table 5 Comparison of different alignment modules. The best results are highlighted in red. Alignment Module PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 None 44.53 42.06 0.9917 0.9890 66.24 AHDR 44.62 42.22 0.9919 0.9892 66.37 FAM (Ours) 44.81 42.69 0.9921 0.9906 66.71 sensors-25-07013-t006_Table 6 Table 6 Comparison of different alignment strategies for high- and low-frequency features. The best results are highlighted in red. Method PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 (1) No separation, optical flow 44.67 42.81 0.9920 0.9904 66.51 (2) No separation, conv + attention 44.62 42.22 0.9919 0.9892 66.43 (3) Separation, optical flow 44.65 42.67 0.9920 0.9896 66.49 (4) Separation, conv + attention 44.51 42.14 0.9920 0.9898 66.55 FAM (Ours) 44.81 42.69 0.9921 0.9906 66.71 sensors-25-07013-t007_Table 7 Table 7 Ablation study of the proposed FDPB. The best results are highlighted in red. Method PSNR- &#956; PSNR- l SSIM- &#956; SSIM- l HDR-VDP-2 (1) No decomposition, GFE+LFE 44.49 42.61 0.9919 0.9896 66.37 (2) Frequency decomposition, swapped 44.45 42.33 0.9918 0.9895 66.14 (3) Frequency decomposition, LFE only 44.36 42.22 0.9918 0.9892 66.06 (4) Frequency decomposition, GFE only 44.64 42.88 0.9920 0.9900 66.64 (5) Low-freq w/o dense res. 44.68 42.59 0.9920 0.9902 66.61 (6) High-freq w/o CSFM 44.61 42.56 0.9919 0.9901 66.48 FDPB (Ours) 44.81 42.69 0.9921 0.9906 66.71"
}