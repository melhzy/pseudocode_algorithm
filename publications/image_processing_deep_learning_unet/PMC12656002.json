{
  "pmcid": "PMC12656002",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:31.717912",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656002",
    "pmid": "41305260",
    "doi": "10.3390/s25227053",
    "title": "DPM-UNet: A Mamba-Based Network with Dynamic Perception Feature Enhancement for Medical Image Segmentation",
    "year": "2025",
    "month": "11",
    "day": "19",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "19"
    },
    "authors": [
      "Xu Shangyu",
      "Liu Xiaohang",
      "Lei Hongsheng",
      "Hui Bin"
    ],
    "abstract": "In medical image segmentation, effective integration of global and local features is crucial. Current methods struggle to simultaneously model long-range dependencies and fine local details. Convolutional Neural Networks (CNNs) excel at extracting local features but are limited by their local receptive fields for capturing long-range dependencies. While global self-attention mechanisms (e.g., in Transformers) can capture long-range spatial relationships, their quadratic computational complexity incurs high costs for high-resolution medical images. To address these limitations, State Space Models (SSMs), which maintain linear complexity while effectively establishing long-range dependencies, have been introduced to visual tasks. Leveraging the advantages of SSMs, this paper proposes DPM-UNet. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract local detailed features and a DPMamba Module at deep layers to model global semantic information, achieving effective local global feature fusion. A Multi-scale Aggregation Attention Network (MAAN) is further incorporated to enhance multi-scale representations. The proposed method collaboratively captures local details, long-range dependencies, and multi-scale information in medical images. Experiments on three public datasets demonstrate that DPM-UNet outperforms existing methods across multiple evaluation metrics.",
    "keywords": [
      "medical image segmentation",
      "Mamba",
      "local global feature fusion",
      "multi-scale feature"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656002</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656002.1</article-id><article-id pub-id-type=\"pmcaid\">12656002</article-id><article-id pub-id-type=\"pmcaiid\">12656002</article-id><article-id pub-id-type=\"pmid\">41305260</article-id><article-id pub-id-type=\"doi\">10.3390/s25227053</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-07053</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>DPM-UNet: A Mamba-Based Network with Dynamic Perception Feature Enhancement for Medical Image Segmentation</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"S\">Shangyu</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-sensors-25-07053\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07053\" ref-type=\"aff\">2</xref><xref rid=\"af3-sensors-25-07053\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"X\">Xiaohang</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-sensors-25-07053\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07053\" ref-type=\"aff\">2</xref><xref rid=\"af3-sensors-25-07053\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Lei</surname><given-names initials=\"H\">Hongsheng</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af2-sensors-25-07053\" ref-type=\"aff\">2</xref><xref rid=\"af3-sensors-25-07053\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0002-8394-1644</contrib-id><name name-style=\"western\"><surname>Hui</surname><given-names initials=\"B\">Bin</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-sensors-25-07053\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07053\" ref-type=\"aff\">2</xref><xref rid=\"c1-sensors-25-07053\" ref-type=\"corresp\">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Corsi</surname><given-names initials=\"C\">Cristiana</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-07053\"><label>1</label>Key Laboratory of Opto-Electronic Information Processing, Chinese Academy of Sciences, Shenyang 110016, China</aff><aff id=\"af2-sensors-25-07053\"><label>2</label>Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China</aff><aff id=\"af3-sensors-25-07053\"><label>3</label>University of Chinese Academy of Sciences, Beijing 100049, China</aff><author-notes><corresp id=\"c1-sensors-25-07053\"><label>*</label>Correspondence: <email>huibin@sia.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>19</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>7053</elocation-id><history><date date-type=\"received\"><day>24</day><month>10</month><year>2025</year></date><date date-type=\"rev-recd\"><day>11</day><month>11</month><year>2025</year></date><date date-type=\"accepted\"><day>14</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>19</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-07053.pdf\"/><abstract><p>In medical image segmentation, effective integration of global and local features is crucial. Current methods struggle to simultaneously model long-range dependencies and fine local details. Convolutional Neural Networks (CNNs) excel at extracting local features but are limited by their local receptive fields for capturing long-range dependencies. While global self-attention mechanisms (e.g., in Transformers) can capture long-range spatial relationships, their quadratic computational complexity incurs high costs for high-resolution medical images. To address these limitations, State Space Models (SSMs), which maintain linear complexity while effectively establishing long-range dependencies, have been introduced to visual tasks. Leveraging the advantages of SSMs, this paper proposes DPM-UNet. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract local detailed features and a DPMamba Module at deep layers to model global semantic information, achieving effective local global feature fusion. A Multi-scale Aggregation Attention Network (MAAN) is further incorporated to enhance multi-scale representations. The proposed method collaboratively captures local details, long-range dependencies, and multi-scale information in medical images. Experiments on three public datasets demonstrate that DPM-UNet outperforms existing methods across multiple evaluation metrics.</p></abstract><kwd-group><kwd>medical image segmentation</kwd><kwd>Mamba</kwd><kwd>local global feature fusion</kwd><kwd>multi-scale feature</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-07053\"><title>1. Introduction</title><p>Medical image segmentation plays a pivotal role in modern clinical practice and scientific research, with its value extending across the entire workflow from auxiliary diagnosis and treatment planning to therapeutic outcome evaluation [<xref rid=\"B1-sensors-25-07053\" ref-type=\"bibr\">1</xref>,<xref rid=\"B2-sensors-25-07053\" ref-type=\"bibr\">2</xref>,<xref rid=\"B3-sensors-25-07053\" ref-type=\"bibr\">3</xref>,<xref rid=\"B4-sensors-25-07053\" ref-type=\"bibr\">4</xref>]. Traditional segmentation methods heavily rely on manual annotation by physicians, which are time-consuming, labor-intensive, inefficient, and suffer from significant inter-observer variability, directly impacting diagnostic consistency and result reproducibility [<xref rid=\"B5-sensors-25-07053\" ref-type=\"bibr\">5</xref>,<xref rid=\"B6-sensors-25-07053\" ref-type=\"bibr\">6</xref>]. Against this backdrop, deep learning-based automated segmentation techniques have emerged and have been successfully applied to various imaging segmentation tasks, including magnetic resonance imaging (MRI), nuclei segmentation in microscopic images [<xref rid=\"B7-sensors-25-07053\" ref-type=\"bibr\">7</xref>,<xref rid=\"B8-sensors-25-07053\" ref-type=\"bibr\">8</xref>], and multi-organ segmentation in computed tomography (CT) [<xref rid=\"B9-sensors-25-07053\" ref-type=\"bibr\">9</xref>,<xref rid=\"B10-sensors-25-07053\" ref-type=\"bibr\">10</xref>]. These techniques, by virtue of their high efficiency, high accuracy, and robust stability [<xref rid=\"B11-sensors-25-07053\" ref-type=\"bibr\">11</xref>,<xref rid=\"B12-sensors-25-07053\" ref-type=\"bibr\">12</xref>], lay the groundwork for translating precision medicine from concept to clinical application.</p><p>Despite significant progress in deep learning for medical image segmentation, effectively integrating local features with global long-range dependencies to further enhance segmentation accuracy remains a key research challenge. Convolutional neural networks (CNNs) [<xref rid=\"B13-sensors-25-07053\" ref-type=\"bibr\">13</xref>,<xref rid=\"B14-sensors-25-07053\" ref-type=\"bibr\">14</xref>], exemplified by U-Net [<xref rid=\"B15-sensors-25-07053\" ref-type=\"bibr\">15</xref>], SegResNet [<xref rid=\"B16-sensors-25-07053\" ref-type=\"bibr\">16</xref>], and nnU-Net [<xref rid=\"B17-sensors-25-07053\" ref-type=\"bibr\">17</xref>], excel at extracting local detailed features. However, their inherent local receptive fields limit the model&#8217;s capacity to capture information from distant regions within an image, thereby constraining further improvements in segmentation accuracy. In contrast, Vision Transformers (ViTs) [<xref rid=\"B18-sensors-25-07053\" ref-type=\"bibr\">18</xref>,<xref rid=\"B19-sensors-25-07053\" ref-type=\"bibr\">19</xref>], through their global self-attention mechanism, empower each image patch (token) to attend to all other patches, demonstrating superior performance in modeling long-range feature interactions and capturing global context [<xref rid=\"B20-sensors-25-07053\" ref-type=\"bibr\">20</xref>,<xref rid=\"B21-sensors-25-07053\" ref-type=\"bibr\">21</xref>]. Nevertheless, the computational complexity of the attention mechanism in ViTs grows quadratically with the number of image patches [<xref rid=\"B22-sensors-25-07053\" ref-type=\"bibr\">22</xref>], imposing a substantial computational burden when processing high-resolution medical images.</p><p>In recent years, State Space Sequence Models (SSMs) [<xref rid=\"B23-sensors-25-07053\" ref-type=\"bibr\">23</xref>,<xref rid=\"B24-sensors-25-07053\" ref-type=\"bibr\">24</xref>] have garnered significant attention in the field of computer vision due to their efficiency in processing long sequences. Among them, the Mamba model [<xref rid=\"B22-sensors-25-07053\" ref-type=\"bibr\">22</xref>], with its linear computational complexity and advantages in global modeling [<xref rid=\"B25-sensors-25-07053\" ref-type=\"bibr\">25</xref>,<xref rid=\"B26-sensors-25-07053\" ref-type=\"bibr\">26</xref>], has emerged as an effective solution for capturing long-range dependencies in visual tasks. Compared to Transformer architectures, which exhibit quadratic computational complexity, Mamba-based vision models can process long sequences with near-linear computational overhead, significantly enhancing their scalability and practicality in high-resolution image tasks. Vmamba [<xref rid=\"B25-sensors-25-07053\" ref-type=\"bibr\">25</xref>] effectively addressed the mismatch between the 1D sequential processing of traditional SSMs and the 2D spatial structure of images by introducing strategies such as image patching, sequence flattening, and cross-scanning, thereby achieving efficient modeling of the global image context. This progress has spurred the development of a series of Mamba-based medical image segmentation models, including U-Mamba [<xref rid=\"B27-sensors-25-07053\" ref-type=\"bibr\">27</xref>], SegMamba [<xref rid=\"B28-sensors-25-07053\" ref-type=\"bibr\">28</xref>], and SwinUMamba [<xref rid=\"B29-sensors-25-07053\" ref-type=\"bibr\">29</xref>]. However, existing models, while achieving global perception, fail to adequately integrate local detailed features and spatial contextual information, which limits their expressive power in complex medical image segmentation tasks.</p><p>To fully integrate local and global features, this paper proposes a Mamba-based DPM-UNet network. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract low-level visual features, while a DPMamba Module is introduced at deep layers. The DPMamba Module utilizes Mamba to flatten the image into a sequence, capturing long-range dependencies and global contextual information. To further enhance the model&#8217;s ability to extract and fuse image features at a low computational cost, a Dynamic Perception Feature Enhancement Block (DPFE) is applied to the globally aware feature maps generated by Mamba. Additionally, we design a Multi-scale Aggregation Attention Network (MAAN) to extract multi-scale information from the outputs of the DRFM and optimize feature transmission along the encoder-to-decoder path via skip connections. The main contributions of this paper are summarized as follows:<list list-type=\"simple\"><list-item><label>(1)</label><p>We propose a novel segmentation network named DPM-UNet, which integrates the local feature extraction capability of CNNs with the global information aggregation ability of Mamba, aiming to achieve precise medical image segmentation.</p></list-item><list-item><label>(2)</label><p>We design three key components: the Dual-path Residual Fusion Module (DRFM), the DPMamba Module, and the Multi-scale Aggregation Attention Network (MAAN). The DRFM enhances local feature extraction by fusing features from standard and dilated convolutions. The DPMamba Module leverages Mamba to generate global features and further enhances feature representation in critical channels through a Dynamic Perception Feature Enhancement Block (DPFE). Additionally, the MAAN is embedded in skip connection paths to optimize the transmission and fusion of multi-scale information.</p></list-item><list-item><label>(3)</label><p>Experimental results on three public medical image segmentation datasets demonstrate that DPM-UNet achieves state-of-the-art segmentation performance compared to existing methods, fully validating the effectiveness and strong generalization capability of our approach for medical image segmentation tasks.</p></list-item></list></p></sec><sec sec-type=\"methods\" id=\"sec2-sensors-25-07053\"><title>2. Methods</title><p>The overall architecture of DPM-UNet is illustrated in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>a. In contrast to the nearly pure VSS block design of Swin-UMamba, DPM-UNet adopts a hybrid structure: its shallow stages (1&#8211;3) use convolution-based DRFMs to focus on extracting local detailed features, while the deeper stages (4&#8211;5) incorporate DPMamba modules (VSS-based) to capture global semantic dependencies. The network follows a U-shaped symmetric layout, with max pooling for downsampling and bilinear interpolation for upsampling. Skip connections are incorporated between each corresponding encoder and decoder stage to fuse features across different hierarchies. Additionally, a Multi-scale Aggregation Attention Network (MAAN) is embedded in the skip connections from Stages 1 to 3 to extract multi-scale features and optimize feature transmission during the skip-connection process. This hybrid design balances local and global feature learning, improving performance while reducing complexity compared to pure VSS-based approaches.</p><sec id=\"sec2dot1-sensors-25-07053\"><title>2.1. DPMamba Module</title><p>As shown in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>b, the DPMamba Module first utilizes a VSS Block at the input stage to capture global feature information, and then employs a DPFE Block to further enhance its feature representation capability. Assuming the input feature <inline-formula><mml:math id=\"mm1\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> has a shape of <inline-formula><mml:math id=\"mm2\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we have:<disp-formula id=\"FD1-sensors-25-07053\"><label>(1)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-sensors-25-07053\"><label>(2)</label><mml:math id=\"mm4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>DPMamba can be decomposed into two independent functional components, VSS(&#183;) and DPFE (&#183;), dedicated to global spatial information extraction and feature refinement, respectively.</p><sec id=\"sec2dot1dot1-sensors-25-07053\"><title>2.1.1. VSS Block</title><p>Traditional attention mechanisms struggle with efficient long-sequence modeling due to their quadratic computational complexity, posing significant challenges for processing large-scale medical images. The Mamba architecture, based on State Space Sequence Models (SSMs), reduces computational complexity to linear while demonstrating remarkable performance in natural language processing. Leveraging this advantage of Mamba, we introduce its vision variant&#8212;the Visual State Space (VSS) model&#8212;into the field of medical image segmentation. As illustrated in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>c, the VSS Block employed in this work is designed following the methodology presented in reference [<xref rid=\"B25-sensors-25-07053\" ref-type=\"bibr\">25</xref>]. The processing pipeline is as follows: the input features first pass through a linear layer, whose output is split evenly along the channel dimension into two tensors. One branch undergoes processing through depthwise separable convolution, a SiLU activation function, 2D Selective Scanning (SS2D), and layer normalization. The other branch is processed only by a SiLU activation function. Finally, the outputs of the two branches are multiplied element-wise, and the result is fed into another linear layer to produce the final output.</p></sec><sec id=\"sec2dot1dot2-sensors-25-07053\"><title>2.1.2. Dynamic Perception Feature Enhancement Block (DPFE)</title><p>Recent studies have demonstrated that the gated multilayer perceptron (gated MLP) architecture delivers remarkable performance in natural language processing tasks [<xref rid=\"B30-sensors-25-07053\" ref-type=\"bibr\">30</xref>]. We posit that the gating mechanism introduced in this architecture holds significant potential for application in visual tasks as well. Building upon this rationale, we propose the Dynamic Perception Feature Enhancement Block (DPFE), designed to further enhance the model&#8217;s capability for image feature extraction and fusion at a low computational cost. The structure of the DPFE module is illustrated in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>d. The output features <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> from the VSS Block are first processed by an SE Block [<xref rid=\"B31-sensors-25-07053\" ref-type=\"bibr\">31</xref>] for channel-wise attention calibration, yielding features <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to emphasize important feature channels while suppressing less significant ones. Subsequently, the features <inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are split into <inline-formula><mml:math id=\"mm8\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, which are fed into two parallel branches. <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> passes through a 1 &#215; 1 convolutional layer, followed by a 3 &#215; 3 depthwise separable convolution with residual connections to extract local spatial features, and undergoes non-linear transformation via a GELU activation function, resulting in features <inline-formula><mml:math id=\"mm11\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Meanwhile, <inline-formula><mml:math id=\"mm12\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is processed by a 1 &#215; 1 convolutional layer to produce features <inline-formula><mml:math id=\"mm13\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. The features <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are then multiplied element-wise to achieve dynamic weighting based on feature importance. Finally, the weighted result is passed through a 1 &#215; 1 convolutional layer to produce the output <inline-formula><mml:math id=\"mm16\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby completing an adaptive feature enhancement process that progresses from channel attention calibration to gated dynamic modulation. The mathematical formulation of the DPFE is as follows:<disp-formula id=\"FD3-sensors-25-07053\"><label>(3)</label><mml:math id=\"mm17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-07053\"><label>(4)</label><mml:math id=\"mm18\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD5-sensors-25-07053\"><label>(5)</label><mml:math id=\"mm19\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD6-sensors-25-07053\"><label>(6)</label><mml:math id=\"mm20\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#8855;</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among these, SE denotes the Squeeze-and-Excitation module, <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents a convolutional layer with a kernel size of 1 &#215; 1, <inline-formula><mml:math id=\"mm22\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a depthwise separable convolutional layer with a kernel size of 3 &#215; 3, and <italic toggle=\"yes\">&#963;</italic>(&#183;) refers to the GELU activation function.</p></sec></sec><sec id=\"sec2dot2-sensors-25-07053\"><title>2.2. Dual-Path Residual Fusion Module (DRFM)</title><p>In medical image segmentation tasks, effectively capturing local features while expanding the receptive field to improve segmentation accuracy remains a core challenge. Existing methods often employ multi-scale convolutional kernels to address this issue. While capable of capturing broader contextual information, these approaches typically incur high computational costs and fail to adequately model the correlations between features under different receptive fields. To overcome these limitations, we propose the Dual-path Residual Fusion Module (DRFM), whose structure is illustrated in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>e. This module adopts a dual-path parallel residual design, synergistically integrating features extracted by standard convolutions and dilated convolutions to generate more informative and robust feature representations. Specifically, the input feature <inline-formula><mml:math id=\"mm23\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> first undergoes a 1 &#215; 1 convolution for channel dimension adjustment, yielding feature <inline-formula><mml:math id=\"mm24\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. It is then fed into two parallel paths: one path uses a 3 &#215; 3 standard convolution to capture local detailed textures, while the other employs a 3 &#215; 3 dilated convolution with a dilation rate of 2 to acquire wide-range contextual information at a lower computational cost. The outputs from both paths are fused to obtain <inline-formula><mml:math id=\"mm25\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This feature <inline-formula><mml:math id=\"mm26\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> subsequently undergoes deeper processing via a parallel set of standard and dilated convolutions, producing <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, a 1 &#215; 1 convolution integrates all learned features, and the result is added to the residual connection-provided <inline-formula><mml:math id=\"mm28\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to produce the final output feature <inline-formula><mml:math id=\"mm29\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The mathematical formulation of the DRFM is as follows:<disp-formula id=\"FD7-sensors-25-07053\"><label>(7)</label><mml:math id=\"mm30\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD8-sensors-25-07053\"><label>(8)</label><mml:math id=\"mm31\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD9-sensors-25-07053\"><label>(9)</label><mml:math id=\"mm32\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD10-sensors-25-07053\"><label>(10)</label><mml:math id=\"mm33\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among these, <inline-formula><mml:math id=\"mm34\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm35\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote convolutional layers with kernel sizes of 1 &#215; 1 and 3 &#215; 3, respectively, <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents a dilated convolutional layer with a dilation rate of 2 and a kernel size of 3 &#215; 3, and <inline-formula><mml:math id=\"mm37\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> refers to concatenation along the channel dimension.</p></sec><sec id=\"sec2dot3-sensors-25-07053\"><title>2.3. Multi-Scale Aggregation Attention Network (MAAN)</title><p>In medical image segmentation tasks, significant size variations among target structures impose high demands on multi-scale feature extraction capabilities. To integrate multi-scale semantic information and enhance feature representation capacity for targets of different sizes, we design the Multi-scale Aggregation Attention Network (MAAN), whose structure is illustrated in <xref rid=\"sensors-25-07053-f001\" ref-type=\"fig\">Figure 1</xref>f. This module adopts a parallel dual-branch architecture to enhance input features from both channel and spatial dimensions, respectively. The feature map <inline-formula><mml:math id=\"mm38\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> output by the DRFM encoder is fed into the MAAN module for refinement. In the channel branch, the spatial dimensions are compressed into channel descriptors of size C &#215; 1 &#215; 1 through average pooling and max pooling. These descriptors then sequentially pass through a 1 &#215; 1 convolution, a ReLU activation function, another 1 &#215; 1 convolution, and a Sigmoid activation function to generate channel attention weights. These weights are multiplied element-wise with the input feature <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, yielding the channel-enhanced feature <inline-formula><mml:math id=\"mm40\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. In the spatial branch, the feature <inline-formula><mml:math id=\"mm41\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> undergoes a serial cascaded structure composed of 3 &#215; 3, 5 &#215; 5, and 7 &#215; 7 convolutions, with 1 &#215; 1 convolutions used for feature aggregation between layers, producing the multi-scale fused feature <inline-formula><mml:math id=\"mm42\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Subsequently, both average pooling and max pooling are applied, and the resulting features are concatenated along the channel dimension. This is followed by a 7 &#215; 7 convolution and a Sigmoid activation function to generate a spatial attention map. Finally, this map is weighted and fused with <inline-formula><mml:math id=\"mm43\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to produce the spatially enhanced feature <inline-formula><mml:math id=\"mm44\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The channel feature <inline-formula><mml:math id=\"mm45\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the spatial feature <inline-formula><mml:math id=\"mm46\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are combined with the original input feature <inline-formula><mml:math id=\"mm47\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> via residual connection to produce the output feature <inline-formula><mml:math id=\"mm48\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby enhancing the feature representation capacity in both spatial and channel dimensions. This cross-scale information interaction mechanism effectively improves the model&#8217;s ability to represent multi-target structures with significant scale variations. The mathematical formulation of the MAAN is as follows:<disp-formula id=\"FD11-sensors-25-07053\"><label>(11)</label><mml:math id=\"mm49\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD12-sensors-25-07053\"><label>(12)</label><mml:math id=\"mm50\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD13-sensors-25-07053\"><label>(13)</label><mml:math id=\"mm51\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>&#8855;</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD14-sensors-25-07053\"><label>(14)</label><mml:math id=\"mm52\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among these, <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm54\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm55\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm56\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denote convolutional layers with kernel sizes of 1 &#215; 1, 3 &#215; 3, 5 &#215; 5 and 7 &#215; 7; <inline-formula><mml:math id=\"mm57\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm58\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> indicate max pooling and average pooling operations along the channel dimension; <inline-formula><mml:math id=\"mm59\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm60\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent max pooling and average pooling operations along the spatial dimensions; <italic toggle=\"yes\">&#963;</italic>(&#183;) denotes the Sigmoid activation function; and <inline-formula><mml:math id=\"mm61\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> refers to concatenation along the channel dimension.</p></sec></sec><sec id=\"sec3-sensors-25-07053\"><title>3. Experiments</title><sec id=\"sec3dot1-sensors-25-07053\"><title>3.1. Datasets</title><p>(1) The Abdomen MRI dataset: This dataset used in this study is sourced from the publicly available MICCAI 2022 AMOS Challenge resources [<xref rid=\"B29-sensors-25-07053\" ref-type=\"bibr\">29</xref>]. It is designed for segmenting 13 abdominal organs, such as the liver, spleen, pancreas, kidneys, stomach, gallbladder, esophagus, aorta, inferior vena cava, adrenal glands, and duodenum. To ensure statistical reliability, the data partition (60 scans for training, 50 for testing) follows the established benchmark protocol in reference [<xref rid=\"B27-sensors-25-07053\" ref-type=\"bibr\">27</xref>], which introduced additional scans to overcome limitations of a smaller validation set. These test scans are fully independent, with no patient overlap against the training set, and were annotated by professional radiologists to ensure quality. Throughout the experiments, all images were preprocessed to a resolution of 320 &#215; 320 pixels.</p><p>(2) The Microscopy dataset: This dataset focuses on cell instance segmentation tasks, with image data originating from the publicly available NeurIPS 2022 Cell Segmentation Challenge dataset [<xref rid=\"B32-sensors-25-07053\" ref-type=\"bibr\">32</xref>]. Its specific composition includes 1000 images for training and 101 images for testing. To standardize the input size, all images were cropped to 512 &#215; 512 pixels before training and testing. The data processing methodology follows the scheme proposed in reference [<xref rid=\"B27-sensors-25-07053\" ref-type=\"bibr\">27</xref>].</p><p>(3) The ACDC dataset: This dataset comprises cardiac MRI scans from 150 patients, with each patient containing scans from different physiological phases, such as systole and diastole. Sourced from the Automatic Cardiac Diagnosis Challenge [<xref rid=\"B33-sensors-25-07053\" ref-type=\"bibr\">33</xref>], its core task is to segment the left ventricle, right ventricle, and myocardium. Our experiment utilizes data from 100 patients in this dataset. We divided these 100 cases into training and test sets in an 8:2 ratio. The training set contains 160 scans from 80 patients, and the test set contains 40 scans from 20 patients. All images used for training and testing were preprocessed and uniformly resized to 256 &#215; 256 pixels.</p></sec><sec id=\"sec3dot2-sensors-25-07053\"><title>3.2. Evaluation Metrics and Baselines</title><p>For MRI-based datasets, Abdomen MRI and ACDC, the Dice Similarity Coefficient (DSC) was used to measure volumetric overlap between segmentations and ground truth, while the Normalized Surface Distance (NSD) evaluated boundary accuracy. For the Microscopy dataset, which involves instance-level cell identification, the F1-Score served as the primary metric to assess detection performance at the object level. A prediction was counted as a True Positive only if the IoU with the ground truth exceeded 0.5. Additionally, the DSC metric was applied to complement this by evaluating pixel-level segmentation accuracy within each correctly detected instance. The calculation formulas for all metrics are as follows:<disp-formula id=\"FD15-sensors-25-07053\"><label>(15)</label><mml:math id=\"mm62\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD16-sensors-25-07053\"><label>(16)</label><mml:math id=\"mm63\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#160;</mml:mo><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#8745;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#8745;</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open=\"|\" close=\"|\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD17-sensors-25-07053\"><label>(17)</label><mml:math id=\"mm64\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD18-sensors-25-07053\"><label>(18)</label><mml:math id=\"mm65\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#160;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD19-sensors-25-07053\"><label>(19)</label><mml:math id=\"mm66\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD20-sensors-25-07053\"><label>(20)</label><mml:math id=\"mm67\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, TP (True Positive) denotes the number of samples correctly predicted as positive; FP (False Positive) denotes the number of samples incorrectly predicted as positive; and FN (False Negative) denotes the number of samples incorrectly predicted as negative. <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">p</italic><italic toggle=\"yes\">r</italic><italic toggle=\"yes\">e</italic><italic toggle=\"yes\">d</italic></sub> and <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">g</italic><italic toggle=\"yes\">t</italic></sub> represent the sets of surface points corresponding to the predicted segmentation and the ground truth segmentation, respectively. <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">p</italic><italic toggle=\"yes\">r</italic><italic toggle=\"yes\">e</italic><italic toggle=\"yes\">d</italic>,<italic toggle=\"yes\">&#964;</italic></sub> is defined as the set of all points in the predicted surface whose distance to the ground truth surface is less than the threshold <italic toggle=\"yes\">&#964;</italic>; similarly, <italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">g</italic><italic toggle=\"yes\">t</italic>,<italic toggle=\"yes\">&#964;</italic></sub> refers to the set of all points in the ground truth surface whose distance to the predicted surface is within <italic toggle=\"yes\">&#964;</italic>.</p><p>We compared five medical image segmentation models under uniform experimental conditions, covering three major architecture types: The traditional CNN architectures are represented by nnU-Net [<xref rid=\"B17-sensors-25-07053\" ref-type=\"bibr\">17</xref>] and SegResNet [<xref rid=\"B16-sensors-25-07053\" ref-type=\"bibr\">16</xref>]. The Transformer-based architectures are represented by UNETR [<xref rid=\"B34-sensors-25-07053\" ref-type=\"bibr\">34</xref>] and SwinUNETR [<xref rid=\"B35-sensors-25-07053\" ref-type=\"bibr\">35</xref>]. Swin-UMamba [<xref rid=\"B29-sensors-25-07053\" ref-type=\"bibr\">29</xref>] was selected as the representative Mamba-based architecture. All models were rigorously reproduced within the nnU-Net framework, strictly adhering to the hyperparameter configurations reported in their respective original publications.</p></sec><sec id=\"sec3dot3-sensors-25-07053\"><title>3.3. Implementation Details</title><p>In the architecture of the DPM-UNet network, each of stages 4 to 5 is configured with four consecutive DPMamba modules, forming a [4, 4] structure. All models are implemented within the nnU-Net framework without using any pre-trained weights, and all parameters are optimized through training from scratch. This design enables a focused investigation into network architecture innovation while maintaining consistent experimental conditions&#8212;such as image preprocessing and data augmentation&#8212;across all compared methods. Consequently, DPM-UNet is evaluated under uniform settings, ensuring that the network architecture remains the sole differentiating factor. During training, patch size, batch size, and network configuration adhere to the standard nnU-Net settings. The training process employs the AdamW optimizer with a weight decay coefficient of 0.05. The initial learning rate is set to 2 &#215; 10<sup>&#8722;4</sup> and decays to a minimum of 1 &#215; 10<sup>&#8722;6</sup> using a cosine annealing scheduling strategy. The loss function is defined as the unweighted sum of Dice loss and cross-entropy loss. A five-fold cross-validation strategy is applied on the training set for all three datasets. Since the Dice Similarity Coefficient (DSC) is the most critical evaluation metric in our task, the model checkpoint achieving the highest DSC on the validation set is selected as the optimal model for each training run. The models are trained for 1000 epochs in total without adopting an early stopping strategy, leveraging the cosine annealing learning rate scheduler for full-cycle optimization. The batch size is set to 4 for the microscopy dataset and 8 for the other two datasets. The training curves of three datasets are shown in <xref rid=\"sensors-25-07053-f002\" ref-type=\"fig\">Figure 2</xref>. All experiments are implemented using the PyTorch (torch = 2.1.1) framework on hardware equipped with an NVIDIA RTX 4070 GPU.</p></sec><sec id=\"sec3dot4-sensors-25-07053\"><title>3.4. Experimental Results</title><p><xref rid=\"sensors-25-07053-t001\" ref-type=\"table\">Table 1</xref> presents the quantitative results of multi-organ segmentation on the Abdomen MRI dataset. The DPM-UNet model achieved a DSC of 78.15%, representing a 1.52 percentage point improvement over the second-best performing nnU-Net, while its NSD reached 84.67%, with an improvement of 1.16 percentage points. As shown in <xref rid=\"sensors-25-07053-f003\" ref-type=\"fig\">Figure 3</xref>, regarding organ-level segmentation accuracy, DPM-UNet ranked first in 10 organs including the liver, left and right kidneys, spleen, pancreas, inferior vena cava, left adrenal gland, esophagus, stomach, and duodenum. The overall segmentation visualization in <xref rid=\"sensors-25-07053-f004\" ref-type=\"fig\">Figure 4</xref> and the enlarged local details in <xref rid=\"sensors-25-07053-f005\" ref-type=\"fig\">Figure 5</xref> consistently demonstrate that DPM-UNet not only accurately captures the main structures of organs but also produces more continuous and smoother segmentation contours along complex anatomical boundaries. These results fully validate the effectiveness of the proposed global&#8211;local feature collaboration mechanism in representing multi-scale anatomical structures.</p><p>The experimental results on the Microscopy dataset are presented in <xref rid=\"sensors-25-07053-t002\" ref-type=\"table\">Table 2</xref>. The DPM-UNet model achieved a DSC of 73.25%, representing a 1.56 percentage point improvement over the second-best performing UNETR, while its F1-score reached 60.23%, with an improvement of 5.58 percentage points. As shown in the visual comparison in <xref rid=\"sensors-25-07053-f006\" ref-type=\"fig\">Figure 6</xref>, DPM-UNet accurately captures the main structures of cells. The enlarged local details in <xref rid=\"sensors-25-07053-f007\" ref-type=\"fig\">Figure 7</xref> further demonstrate that DPM-UNet effectively identifies and separates small-sized or adherent nuclei with more precise edge delineation. These results underscore the advantages of the model&#8217;s local feature extraction mechanism, which enables it to capture subtle differential features and fine boundary information between nuclei, thereby significantly improving segmentation accuracy.</p><p>The experimental results on the ACDC dataset are presented in <xref rid=\"sensors-25-07053-t003\" ref-type=\"table\">Table 3</xref>. While the DPM-UNet model achieved optimal performance, the overall performance gap among different models was relatively small. Specifically, DPM-UNet improved the DSC metric by 0.14% compared to nnU-Net and enhanced the NSD metric by 0.05% compared to Swin-UMamba. In the substructure segmentation tasks, DPM-UNet delivered optimal performance for myocardial (Myo) and left ventricular (LV) segmentation, while ranking third in right ventricular (RV) segmentation. The overall comparison in <xref rid=\"sensors-25-07053-f008\" ref-type=\"fig\">Figure 8</xref> and the enlarged local details in <xref rid=\"sensors-25-07053-f009\" ref-type=\"fig\">Figure 9</xref> demonstrate that DPM-UNet generates more accurate and sharper segmentation contours, further validating its local&#8211;global feature interaction capability in effectively capturing critical subtle structural features along the edges.</p></sec><sec id=\"sec3dot5-sensors-25-07053\"><title>3.5. Further Analysis</title><sec id=\"sec3dot5dot1-sensors-25-07053\"><title>3.5.1. Ablation Study</title><p>The ablation experiments on the abdominal MRI dataset evaluated the effectiveness of the proposed modules, with results summarized in <xref rid=\"sensors-25-07053-t004\" ref-type=\"table\">Table 4</xref>. The introduction of the DRFM alone improved performance, increasing the Dice Similarity Coefficient (DSC) by 0.45% and the Normalized Surface Distance (NSD) by 0.88%. After integrating both the DRFM and DPMamba modules, the model performance was further enhanced, achieving additional gains of 0.40% in DSC and 0.41% in NSD. This demonstrates the capability of the DPMamba module to effectively improve segmentation accuracy. Subsequently, incorporating the CBAM module led to further improvements, with DSC and NSD increasing by an additional 0.82% and 0.73%, respectively. When the CBAM module was replaced with the MAAN module, the model achieved its optimal performance, reaching a DSC of 78.15% and an NSD of 84.67%. MAAN outperforms CBAM due to its structural advantage; a dual-path design that concurrently models&#8217; channel and spatial relationships, combined with multi-scale feature aggregation, which enables more effective context capture than CBAM&#8217;s sequential design. These results fully validate the effectiveness of the designed modules and their synergistic contribution to enhancing the model&#8217;s overall segmentation capability. (<bold>Note</bold>: In the following table, a check mark (&#10004;) or cross mark (&#10008;) indicates the presence or absence of the corresponding module. An upward (&#8595;) or downward (&#8593;) arrow indicates that a higher or lower value denotes better performance, respectively.)</p><p>To further investigate the impact of the DRFM structure on the model&#8217;s feature extraction capability, we conducted additional ablation experiments, with the results summarized in <xref rid=\"sensors-25-07053-t005\" ref-type=\"table\">Table 5</xref>. The corresponding schematic diagrams of each experimental setup can be found in <xref rid=\"sensors-25-07053-f010\" ref-type=\"fig\">Figure 10</xref>a&#8211;d. The experimental results demonstrate that introducing dilated convolutions in the second path effectively enhances model performance. Furthermore, fusing features extracted by standard convolutions and dilated convolutions yielded more substantial performance improvements. Additionally, the incorporation of residual connections also contributed significantly to the overall performance enhancement. These results indicate that the DRFM can effectively strengthen the model&#8217;s feature representation capability.</p></sec><sec id=\"sec3dot5dot2-sensors-25-07053\"><title>3.5.2. Model Complexity</title><p>As shown in <xref rid=\"sensors-25-07053-t006\" ref-type=\"table\">Table 6</xref>, this study employs floating-point operations (FLOPs), number of parameters (Params), and the total training time over 1000 epochs as key metrics to evaluate model complexity and practical training efficiency. In terms of computational and storage complexity, DPM-UNet demonstrates a moderate level among the compared models. However, its training time is the second longest, indicating a higher computational cost during the training phase. Critically, the superior performance of DPM-UNet justifies this increased training cost. It achieves the highest Dice Similarity Coefficient (DSC) and Normalized Surface Dice (NSD), demonstrating that the method achieves a favorable balance between computational efficiency and segmentation accuracy.</p></sec></sec></sec><sec sec-type=\"conclusions\" id=\"sec4-sensors-25-07053\"><title>4. Conclusions and Future Work</title><p>This study proposes DPM-UNet, a medical image segmentation network based on the Mamba architecture. The method adopts a U-shaped structure, where a Dual-path Residual Fusion Module (DRFM) is introduced in the shallow layers to enhance local detail feature extraction, while a DPMamba module capable of modeling long-range dependencies is employed in the deep layers to capture global contextual information. Furthermore, the Multi-scale Aggregation Attention Network (MAAN) is incorporated to strengthen the model&#8217;s ability to perceive and fuse multi-scale features. Experimental results on three public medical image segmentation datasets demonstrate the superior segmentation performance of the proposed method. While the current study has established a solid foundation, certain aspects, such as multi-run performance validation, statistical significance testing, and detailed efficiency profiling, were not fully explored within the scope of this work. These areas present meaningful opportunities for further investigation. Future efforts will focus on developing more lightweight architectures and efficient training strategies to reduce computational costs, alongside incorporating rigorous benchmarking and efficiency analysis to enhance the robustness and practicality of the model under resource constraints.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.X., X.L. and B.H.; formal analysis, S.X.; investigation, S.X.; methodology, S.X.; resources, S.X.; software, S.X.; supervision, B.H.; validation, S.X., X.L. and H.L.; visualization, S.X., X.L. and H.L.; writing&#8212;original draft, S.X.; writing&#8212;review and editing, S.X., X.L., H.L. and B.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>You can download the AbdomenMRI/Microscopy/ACDC dataset at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://drive.google.com/drive/folders/1CH2OWQpd4Sa-BES6oFLRC469gTxf6QUO\">https://drive.google.com/drive/folders/1CH2OWQpd4Sa-BES6oFLRC469gTxf6QUO</uri> (accessed on 11 November 2025).</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-07053\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dai</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sheng</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Cai</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Hamzah</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A deep learning system for predicting time to progression of diabetic retinopathy</article-title><source>Nat. Med.</source><year>2024</year><volume>30</volume><fpage>584</fpage><lpage>594</lpage><pub-id pub-id-type=\"doi\">10.1038/s41591-023-02702-z</pub-id><pub-id pub-id-type=\"pmid\">38177850</pub-id><pub-id pub-id-type=\"pmcid\">PMC10878973</pub-id></element-citation></ref><ref id=\"B2-sensors-25-07053\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Fung</surname><given-names>K.M.</given-names></name><name name-style=\"western\"><surname>Thai</surname><given-names>T.C.</given-names></name><name name-style=\"western\"><surname>Moore</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Mannel</surname><given-names>R.S.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>Y.</given-names></name></person-group><article-title>Recent advances and clinical applications of deep learning in medical image analysis</article-title><source>Med. Image Anal.</source><year>2022</year><volume>79</volume><fpage>102444</fpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2022.102444</pub-id><pub-id pub-id-type=\"pmid\">35472844</pub-id><pub-id pub-id-type=\"pmcid\">PMC9156578</pub-id></element-citation></ref><ref id=\"B3-sensors-25-07053\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bai</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Suzuki</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Francis</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Tarroni</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Guitton</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Aung</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Fung</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Petersen</surname><given-names>S.E.</given-names></name><etal/></person-group><article-title>A population-based phenome-wide association study of cardiac and aortic structure and function</article-title><source>Nat. Med.</source><year>2020</year><volume>26</volume><fpage>1654</fpage><lpage>1662</lpage><pub-id pub-id-type=\"doi\">10.1038/s41591-020-1009-y</pub-id><pub-id pub-id-type=\"pmid\">32839619</pub-id><pub-id pub-id-type=\"pmcid\">PMC7613250</pub-id></element-citation></ref><ref id=\"B4-sensors-25-07053\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mei</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>H.-C.</given-names></name><name name-style=\"western\"><surname>Diao</surname><given-names>K.-y.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Robson</surname><given-names>P.M.</given-names></name><name name-style=\"western\"><surname>Chung</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Artificial intelligence&#8211;enabled rapid diagnosis of patients with COVID-19</article-title><source>Nat. Med.</source><year>2020</year><volume>26</volume><fpage>1224</fpage><lpage>1228</lpage><pub-id pub-id-type=\"doi\">10.1038/s41591-020-0931-3</pub-id><pub-id pub-id-type=\"pmid\">32427924</pub-id><pub-id pub-id-type=\"pmcid\">PMC7446729</pub-id></element-citation></ref><ref id=\"B5-sensors-25-07053\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lecun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Bottou</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Bengio</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Haffner</surname><given-names>P.</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type=\"doi\">10.1109/5.726791</pub-id></element-citation></ref><ref id=\"B6-sensors-25-07053\"><label>6.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jungo</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Meier</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Ermis</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Blatti-Moreno</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Herrmann</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Wiest</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Reyes</surname><given-names>M.</given-names></name></person-group><source>On the Effect of Inter-Observer Variability for a Reliable Estimation of Uncertainty of Medical Image Segmentation</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2018</year><fpage>682</fpage><lpage>690</lpage></element-citation></ref><ref id=\"B7-sensors-25-07053\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Graham</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Vu</surname><given-names>Q.D.</given-names></name><name name-style=\"western\"><surname>Raza</surname><given-names>S.E.A.</given-names></name><name name-style=\"western\"><surname>Azam</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tsang</surname><given-names>Y.W.</given-names></name><name name-style=\"western\"><surname>Kwak</surname><given-names>J.T.</given-names></name><name name-style=\"western\"><surname>Rajpoot</surname><given-names>N.</given-names></name></person-group><article-title>Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title><source>Med. Image Anal.</source><year>2019</year><volume>58</volume><fpage>101563</fpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2019.101563</pub-id><pub-id pub-id-type=\"pmid\">31561183</pub-id></element-citation></ref><ref id=\"B8-sensors-25-07053\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zou</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Z.</given-names></name></person-group><article-title>An Automated Framework for Histopathological Nucleus Segmentation with Deep Attention Integrated Networks</article-title><source>IEEE ACM Trans. Comput. Biol. Bioinform.</source><year>2024</year><volume>21</volume><fpage>995</fpage><lpage>1006</lpage><pub-id pub-id-type=\"doi\">10.1109/TCBB.2022.3233400</pub-id><pub-id pub-id-type=\"pmid\">37018302</pub-id></element-citation></ref><ref id=\"B9-sensors-25-07053\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gibson</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Giganti</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Bonmati</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Bandula</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gurusamy</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Davidson</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Pereira</surname><given-names>S.P.</given-names></name><name name-style=\"western\"><surname>Clarkson</surname><given-names>M.J.</given-names></name><name name-style=\"western\"><surname>Barratt</surname><given-names>D.C.</given-names></name></person-group><article-title>Automatic Multi-Organ Segmentation on Abdominal CT with Dense V-Networks</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>1822</fpage><lpage>1834</lpage><pub-id pub-id-type=\"doi\">10.1109/TMI.2018.2806309</pub-id><pub-id pub-id-type=\"pmid\">29994628</pub-id><pub-id pub-id-type=\"pmcid\">PMC6076994</pub-id></element-citation></ref><ref id=\"B10-sensors-25-07053\"><label>10.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qi</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zou</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Shan</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Z.</given-names></name></person-group><article-title>Exploring Generalizable Distillation for Efficient Medical Image Segmentation</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2024</year><volume>28</volume><fpage>4170</fpage><lpage>4183</lpage><pub-id pub-id-type=\"doi\">10.1109/JBHI.2024.3385098</pub-id><pub-id pub-id-type=\"pmid\">38954557</pub-id></element-citation></ref><ref id=\"B11-sensors-25-07053\"><label>11.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Khened</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kollerathu</surname><given-names>V.A.</given-names></name><name name-style=\"western\"><surname>Krishnamurthi</surname><given-names>G.</given-names></name></person-group><article-title>Fully convolutional multi-scale residual DenseNets for cardiac segmentation and automated cardiac diagnosis using ensemble of classifiers</article-title><source>Med. Image Anal.</source><year>2019</year><volume>51</volume><fpage>21</fpage><lpage>45</lpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2018.10.004</pub-id><pub-id pub-id-type=\"pmid\">30390512</pub-id></element-citation></ref><ref id=\"B12-sensors-25-07053\"><label>12.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Hoffmeister</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Brenner</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Muti</surname><given-names>H.S.</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Foersch</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>West</surname><given-names>N.P.</given-names></name><name name-style=\"western\"><surname>Brobeil</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Jonnagaddala</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Hawkins</surname><given-names>N.</given-names></name><etal/></person-group><article-title>End-to-end prognostication in colorectal cancer by deep learning: A retrospective, multicentre study</article-title><source>Lancet Digit. Health</source><year>2024</year><volume>6</volume><fpage>e33</fpage><lpage>e43</lpage><pub-id pub-id-type=\"doi\">10.1016/S2589-7500(23)00208-X</pub-id><pub-id pub-id-type=\"pmid\">38123254</pub-id></element-citation></ref><ref id=\"B13-sensors-25-07053\"><label>13.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id=\"B14-sensors-25-07053\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><etal/></person-group><article-title>InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"arxiv\">2211.05778</pub-id></element-citation></ref><ref id=\"B15-sensors-25-07053\"><label>15.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group><source>U-Net: Convolutional Networks for Biomedical Image Segmentation</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id=\"B16-sensors-25-07053\"><label>16.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name></person-group><source>3D MRI Brain Tumor Segmentation Using Autoencoder Regularization</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>311</fpage><lpage>320</lpage></element-citation></ref><ref id=\"B17-sensors-25-07053\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Jaeger</surname><given-names>P.F.</given-names></name><name name-style=\"western\"><surname>Kohl</surname><given-names>S.A.A.</given-names></name><name name-style=\"western\"><surname>Petersen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Maier-Hein</surname><given-names>K.H.</given-names></name></person-group><article-title>nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nat. Methods</source><year>2021</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type=\"doi\">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type=\"pmid\">33288961</pub-id></element-citation></ref><ref id=\"B18-sensors-25-07053\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Adeli</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yuille</surname><given-names>A.L.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2102.04306</pub-id><pub-id pub-id-type=\"arxiv\">2102.04306</pub-id></element-citation></ref><ref id=\"B19-sensors-25-07053\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>J.</given-names></name></person-group><article-title>Augmented FCN: Rethinking context modeling for semantic segmentation</article-title><source>Sci. China Inf. Sci.</source><year>2023</year><volume>66</volume><fpage>142105</fpage><pub-id pub-id-type=\"doi\">10.1007/s11432-021-3590-1</pub-id></element-citation></ref><ref id=\"B20-sensors-25-07053\"><label>20.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raghu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Kornblith</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name></person-group><article-title>Do Vision Transformers See Like Convolutional Neural Networks?</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Virtual</conf-loc><conf-date>6&#8211;10 December 2021</conf-date></element-citation></ref><ref id=\"B21-sensors-25-07053\"><label>21.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Yin</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Kautz</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Molchanov</surname><given-names>P.</given-names></name></person-group><article-title>Global Context Vision Transformers</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Baltimore, MD, USA</conf-loc><conf-date>17&#8211;23 July 2022</conf-date></element-citation></ref><ref id=\"B22-sensors-25-07053\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name></person-group><article-title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2312.00752</pub-id><pub-id pub-id-type=\"arxiv\">2312.00752</pub-id></element-citation></ref><ref id=\"B23-sensors-25-07053\"><label>23.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Goel</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>R&#233;</surname><given-names>C.</given-names></name></person-group><article-title>Efficiently Modeling Long Sequences with Structured State Spaces</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"arxiv\">2111.00396</pub-id></element-citation></ref><ref id=\"B24-sensors-25-07053\"><label>24.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Johnson</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Goel</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Saab</surname><given-names>K.K.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Rudra</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>R&#233;</surname><given-names>C.</given-names></name></person-group><article-title>Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Virtual</conf-loc><conf-date>6&#8211;10 December 2021</conf-date></element-citation></ref><ref id=\"B25-sensors-25-07053\"><label>25.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>VMamba: Visual State Space Model</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2401.10166</pub-id></element-citation></ref><ref id=\"B26-sensors-25-07053\"><label>26.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Pei</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>You</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Qian</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>LocalMamba: Visual State Space Model with Windowed Selective Scan</article-title><source>Proceedings of the ECCV Workshops</source><conf-loc>Milan, Italy</conf-loc><conf-date>29 September&#8211;4 October 2024</conf-date></element-citation></ref><ref id=\"B27-sensors-25-07053\"><label>27.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.J.A.</given-names></name></person-group><article-title>U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2401.04722</pub-id></element-citation></ref><ref id=\"B28-sensors-25-07053\"><label>28.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xing</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name></person-group><article-title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Marrakesh, Morocco</conf-loc><conf-date>6&#8211;10 October 2024</conf-date></element-citation></ref><ref id=\"B29-sensors-25-07053\"><label>29.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>H.-Y.</given-names></name><name name-style=\"western\"><surname>Xi</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Marrakesh, Morocco</conf-loc><conf-date>6&#8211;10 October 2024</conf-date></element-citation></ref><ref id=\"B30-sensors-25-07053\"><label>30.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rajagopal</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Nirmala</surname><given-names>V.</given-names></name></person-group><article-title>Convolutional Gated MLP: Combining Convolutions &amp; gMLP</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2111.03940</pub-id><pub-id pub-id-type=\"arxiv\">2111.03940</pub-id></element-citation></ref><ref id=\"B31-sensors-25-07053\"><label>31.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-Excitation Networks</article-title><source>Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id=\"B32-sensors-25-07053\"><label>32.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Ayyadhury</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Ge</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>J.</given-names></name><etal/></person-group><article-title>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"arxiv\">2308.05864</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41592-024-02233-6</pub-id><pub-id pub-id-type=\"pmcid\">PMC11210294</pub-id><pub-id pub-id-type=\"pmid\">38532015</pub-id></element-citation></ref><ref id=\"B33-sensors-25-07053\"><label>33.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bernard</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Lalande</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Zotti</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Cervenansky</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Heng</surname><given-names>P.A.</given-names></name><name name-style=\"western\"><surname>Cetin</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Lekadir</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Camara</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Ballester</surname><given-names>M.A.G.</given-names></name><etal/></person-group><article-title>Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>2514</fpage><lpage>2525</lpage><pub-id pub-id-type=\"doi\">10.1109/TMI.2018.2837502</pub-id><pub-id pub-id-type=\"pmid\">29994302</pub-id></element-citation></ref><ref id=\"B34-sensors-25-07053\"><label>34.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group><article-title>UNETR: Transformers for 3D Medical Image Segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2103.10504</pub-id><pub-id pub-id-type=\"arxiv\">2103.10504</pub-id></element-citation></ref><ref id=\"B35-sensors-25-07053\"><label>35.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group><article-title>Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2201.01266</pub-id><pub-id pub-id-type=\"arxiv\">2201.01266</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-07053-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>The overall architecture of the proposed Mamba-based U-Net with Dynamic Perception Feature Enhancement (DPM-UNet) for medical image segmentation (DPM-Unet). (<bold>a</bold>) The main framework of DPM-Unet. (<bold>b</bold>) The detailed design of the DPMamba Module, which internally contains a (<bold>c</bold>) VSS Block and a (<bold>d</bold>) Dynamic Perception Feature Enhancement (DPFE) Block. (<bold>e</bold>) The Dual-path Residual Fusion Module (DRFM). (<bold>f</bold>) The Multi-scale Aggregation Attention Network (MAAN).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Training loss curves for the three datasets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Segmentation Results (DSC) for Various Organs on the Abdomen MRI dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Visualizations on the Abdomen MRI dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Enlarged view of the region indicated by the red box in the Abdomen MRI visualization.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Visualizations on the Microscopy dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Enlarged view of the region indicated by the red box in the Microscopy visualization.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Visualizations on the ACDC dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g008.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f009\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Enlarged view of the region indicated by the red box in the ACDC visualization.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g009.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07053-f010\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Comparison of convolutional module structures. (<bold>a</bold>) Standard convolution block, (<bold>b</bold>) Dual-path convolution block, (<bold>c</bold>) Dual-path convolution block with feature mixing, (<bold>d</bold>) Our proposed DRFM block.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07053-g010.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-07053-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t001_Table 1</object-id><label>Table 1</label><caption><p>Segmentation accuracy of different models on the Abdomen MRI dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NSD</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>76.63</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>83.51</underline>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegResNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">80.13</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">57.46</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.82</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SwinUNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">74.73</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Swin-UMamba</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">74.56</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">81.15</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DPM-UNet</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>78.15</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>84.67</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07053-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t002_Table 2</object-id><label>Table 2</label><caption><p>Segmentation accuracy of different models on the Microscopy dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">F1</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.55</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>54.65</underline>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegResNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.51</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">54.02</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>71.69</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">40.57</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SwinUNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.69</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">37.08</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Swin-UMamba</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">67.60</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">49.00</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DPM-UNet</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>73.25</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>60.23</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07053-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t003_Table 3</object-id><label>Table 3</label><caption><p>Segmentation accuracy of different models on the ACDC dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NSD</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">RV</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Myo</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">LV</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>91.81</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.88</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.44</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>90.60</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>95.38</underline>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegResNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.71</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.99</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>89.64</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">90.27</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">95.23</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.34</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">95.49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.76</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.46</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.80</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SwinUNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.50</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.41</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">90.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">95.06</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Swin-UMamba</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">91.69</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<underline>98.04</underline>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>90.00</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">89.78</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">95.30</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DPM-UNet</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>91.95</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>98.09</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">89.46</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>90.76</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>95.64</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07053-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation study of the designed blocks on the Abdomen MRI dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DRFM</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DPMamba</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CBAM</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">MAAN</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NSD &#8593;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.17</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">81.52</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.62</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.40</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.02</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.81</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.54</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">78.15</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">84.67</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07053-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation study of DRFM structure on the Abdomen MRI dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Dilated Convolution</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Fusion</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Residual</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NSD &#8593;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.04</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.80</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.35</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.06</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10008;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.53</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.32</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10004;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">78.15</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">84.67</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07053-t006\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07053-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of FLOPs and Params on the Abdomen MRI dataset using our method with other models.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs (G) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Param. (M) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Training Time (H)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DSC &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NSD &#8593;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">23</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">6</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.63</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.51</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegResNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">24</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">6</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">80.13</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">17</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">57.46</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.82</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SwinUNETR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">29</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">25</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">20</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">69.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">74.73</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Swin-UMamba</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">63</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">59</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">30</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">74.56</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">81.15</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">DPM-UNet</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">31</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">38</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">24</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">78.15</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">84.67</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656002 PMC12656002.1 12656002 12656002 41305260 10.3390/s25227053 sensors-25-07053 1 Article DPM-UNet: A Mamba-Based Network with Dynamic Perception Feature Enhancement for Medical Image Segmentation Xu Shangyu Conceptualization Formal analysis Investigation Methodology Software Resources Validation Visualization Writing &#8211; original draft Writing &#8211; review &amp; editing 1 2 3 Liu Xiaohang Conceptualization Validation Visualization Writing &#8211; review &amp; editing 1 2 3 Lei Hongsheng Validation Writing &#8211; review &amp; editing 2 3 https://orcid.org/0000-0002-8394-1644 Hui Bin Conceptualization Supervision Writing &#8211; review &amp; editing 1 2 * Corsi Cristiana Academic Editor 1 Key Laboratory of Opto-Electronic Information Processing, Chinese Academy of Sciences, Shenyang 110016, China 2 Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China 3 University of Chinese Academy of Sciences, Beijing 100049, China * Correspondence: huibin@sia.cn 19 11 2025 11 2025 25 22 501335 7053 24 10 2025 11 11 2025 14 11 2025 19 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). In medical image segmentation, effective integration of global and local features is crucial. Current methods struggle to simultaneously model long-range dependencies and fine local details. Convolutional Neural Networks (CNNs) excel at extracting local features but are limited by their local receptive fields for capturing long-range dependencies. While global self-attention mechanisms (e.g., in Transformers) can capture long-range spatial relationships, their quadratic computational complexity incurs high costs for high-resolution medical images. To address these limitations, State Space Models (SSMs), which maintain linear complexity while effectively establishing long-range dependencies, have been introduced to visual tasks. Leveraging the advantages of SSMs, this paper proposes DPM-UNet. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract local detailed features and a DPMamba Module at deep layers to model global semantic information, achieving effective local global feature fusion. A Multi-scale Aggregation Attention Network (MAAN) is further incorporated to enhance multi-scale representations. The proposed method collaboratively captures local details, long-range dependencies, and multi-scale information in medical images. Experiments on three public datasets demonstrate that DPM-UNet outperforms existing methods across multiple evaluation metrics. medical image segmentation Mamba local global feature fusion multi-scale feature This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Medical image segmentation plays a pivotal role in modern clinical practice and scientific research, with its value extending across the entire workflow from auxiliary diagnosis and treatment planning to therapeutic outcome evaluation [ 1 , 2 , 3 , 4 ]. Traditional segmentation methods heavily rely on manual annotation by physicians, which are time-consuming, labor-intensive, inefficient, and suffer from significant inter-observer variability, directly impacting diagnostic consistency and result reproducibility [ 5 , 6 ]. Against this backdrop, deep learning-based automated segmentation techniques have emerged and have been successfully applied to various imaging segmentation tasks, including magnetic resonance imaging (MRI), nuclei segmentation in microscopic images [ 7 , 8 ], and multi-organ segmentation in computed tomography (CT) [ 9 , 10 ]. These techniques, by virtue of their high efficiency, high accuracy, and robust stability [ 11 , 12 ], lay the groundwork for translating precision medicine from concept to clinical application. Despite significant progress in deep learning for medical image segmentation, effectively integrating local features with global long-range dependencies to further enhance segmentation accuracy remains a key research challenge. Convolutional neural networks (CNNs) [ 13 , 14 ], exemplified by U-Net [ 15 ], SegResNet [ 16 ], and nnU-Net [ 17 ], excel at extracting local detailed features. However, their inherent local receptive fields limit the model&#8217;s capacity to capture information from distant regions within an image, thereby constraining further improvements in segmentation accuracy. In contrast, Vision Transformers (ViTs) [ 18 , 19 ], through their global self-attention mechanism, empower each image patch (token) to attend to all other patches, demonstrating superior performance in modeling long-range feature interactions and capturing global context [ 20 , 21 ]. Nevertheless, the computational complexity of the attention mechanism in ViTs grows quadratically with the number of image patches [ 22 ], imposing a substantial computational burden when processing high-resolution medical images. In recent years, State Space Sequence Models (SSMs) [ 23 , 24 ] have garnered significant attention in the field of computer vision due to their efficiency in processing long sequences. Among them, the Mamba model [ 22 ], with its linear computational complexity and advantages in global modeling [ 25 , 26 ], has emerged as an effective solution for capturing long-range dependencies in visual tasks. Compared to Transformer architectures, which exhibit quadratic computational complexity, Mamba-based vision models can process long sequences with near-linear computational overhead, significantly enhancing their scalability and practicality in high-resolution image tasks. Vmamba [ 25 ] effectively addressed the mismatch between the 1D sequential processing of traditional SSMs and the 2D spatial structure of images by introducing strategies such as image patching, sequence flattening, and cross-scanning, thereby achieving efficient modeling of the global image context. This progress has spurred the development of a series of Mamba-based medical image segmentation models, including U-Mamba [ 27 ], SegMamba [ 28 ], and SwinUMamba [ 29 ]. However, existing models, while achieving global perception, fail to adequately integrate local detailed features and spatial contextual information, which limits their expressive power in complex medical image segmentation tasks. To fully integrate local and global features, this paper proposes a Mamba-based DPM-UNet network. The network employs a Dual-path Residual Fusion Module (DRFM) at shallow layers to extract low-level visual features, while a DPMamba Module is introduced at deep layers. The DPMamba Module utilizes Mamba to flatten the image into a sequence, capturing long-range dependencies and global contextual information. To further enhance the model&#8217;s ability to extract and fuse image features at a low computational cost, a Dynamic Perception Feature Enhancement Block (DPFE) is applied to the globally aware feature maps generated by Mamba. Additionally, we design a Multi-scale Aggregation Attention Network (MAAN) to extract multi-scale information from the outputs of the DRFM and optimize feature transmission along the encoder-to-decoder path via skip connections. The main contributions of this paper are summarized as follows: (1) We propose a novel segmentation network named DPM-UNet, which integrates the local feature extraction capability of CNNs with the global information aggregation ability of Mamba, aiming to achieve precise medical image segmentation. (2) We design three key components: the Dual-path Residual Fusion Module (DRFM), the DPMamba Module, and the Multi-scale Aggregation Attention Network (MAAN). The DRFM enhances local feature extraction by fusing features from standard and dilated convolutions. The DPMamba Module leverages Mamba to generate global features and further enhances feature representation in critical channels through a Dynamic Perception Feature Enhancement Block (DPFE). Additionally, the MAAN is embedded in skip connection paths to optimize the transmission and fusion of multi-scale information. (3) Experimental results on three public medical image segmentation datasets demonstrate that DPM-UNet achieves state-of-the-art segmentation performance compared to existing methods, fully validating the effectiveness and strong generalization capability of our approach for medical image segmentation tasks. 2. Methods The overall architecture of DPM-UNet is illustrated in Figure 1 a. In contrast to the nearly pure VSS block design of Swin-UMamba, DPM-UNet adopts a hybrid structure: its shallow stages (1&#8211;3) use convolution-based DRFMs to focus on extracting local detailed features, while the deeper stages (4&#8211;5) incorporate DPMamba modules (VSS-based) to capture global semantic dependencies. The network follows a U-shaped symmetric layout, with max pooling for downsampling and bilinear interpolation for upsampling. Skip connections are incorporated between each corresponding encoder and decoder stage to fuse features across different hierarchies. Additionally, a Multi-scale Aggregation Attention Network (MAAN) is embedded in the skip connections from Stages 1 to 3 to extract multi-scale features and optimize feature transmission during the skip-connection process. This hybrid design balances local and global feature learning, improving performance while reducing complexity compared to pure VSS-based approaches. 2.1. DPMamba Module As shown in Figure 1 b, the DPMamba Module first utilizes a VSS Block at the input stage to capture global feature information, and then employs a DPFE Block to further enhance its feature representation capability. Assuming the input feature X l has a shape of R C &#215; H &#215; W , we have: (1) &#160; X l + 1 = V S S B a t c h N o r m X l + X l (2) X o u t = D P F E B a t c h N o r m X l + 1 + X l + 1 DPMamba can be decomposed into two independent functional components, VSS(&#183;) and DPFE (&#183;), dedicated to global spatial information extraction and feature refinement, respectively. 2.1.1. VSS Block Traditional attention mechanisms struggle with efficient long-sequence modeling due to their quadratic computational complexity, posing significant challenges for processing large-scale medical images. The Mamba architecture, based on State Space Sequence Models (SSMs), reduces computational complexity to linear while demonstrating remarkable performance in natural language processing. Leveraging this advantage of Mamba, we introduce its vision variant&#8212;the Visual State Space (VSS) model&#8212;into the field of medical image segmentation. As illustrated in Figure 1 c, the VSS Block employed in this work is designed following the methodology presented in reference [ 25 ]. The processing pipeline is as follows: the input features first pass through a linear layer, whose output is split evenly along the channel dimension into two tensors. One branch undergoes processing through depthwise separable convolution, a SiLU activation function, 2D Selective Scanning (SS2D), and layer normalization. The other branch is processed only by a SiLU activation function. Finally, the outputs of the two branches are multiplied element-wise, and the result is fed into another linear layer to produce the final output. 2.1.2. Dynamic Perception Feature Enhancement Block (DPFE) Recent studies have demonstrated that the gated multilayer perceptron (gated MLP) architecture delivers remarkable performance in natural language processing tasks [ 30 ]. We posit that the gating mechanism introduced in this architecture holds significant potential for application in visual tasks as well. Building upon this rationale, we propose the Dynamic Perception Feature Enhancement Block (DPFE), designed to further enhance the model&#8217;s capability for image feature extraction and fusion at a low computational cost. The structure of the DPFE module is illustrated in Figure 1 d. The output features X l from the VSS Block are first processed by an SE Block [ 31 ] for channel-wise attention calibration, yielding features X l + 1 to emphasize important feature channels while suppressing less significant ones. Subsequently, the features X l + 1 are split into X 1 l + 1 and X 2 l + 1 , which are fed into two parallel branches. X 1 l + 1 passes through a 1 &#215; 1 convolutional layer, followed by a 3 &#215; 3 depthwise separable convolution with residual connections to extract local spatial features, and undergoes non-linear transformation via a GELU activation function, resulting in features X 1 l + 2 . Meanwhile, X 2 l + 1 is processed by a 1 &#215; 1 convolutional layer to produce features X 2 l + 2 . The features X 1 l + 2 and X 2 l + 2 are then multiplied element-wise to achieve dynamic weighting based on feature importance. Finally, the weighted result is passed through a 1 &#215; 1 convolutional layer to produce the output X o u t , thereby completing an adaptive feature enhancement process that progresses from channel attention calibration to gated dynamic modulation. The mathematical formulation of the DPFE is as follows: (3) &#160; X l + 1 = S E X l (4) &#160; X 1 l + 2 = &#963; f 3 &#215; 3 d e c f 1 &#215; 1 X 1 l + 1 + f 1 &#215; 1 X 1 l + 1 (5) &#160; X 2 l + 2 = f 1 &#215; 1 (6) &#160; X o u t = f 1 &#215; 1 X 1 l + 2 &#8855; X 2 l + 2 Among these, SE denotes the Squeeze-and-Excitation module, f 1 &#215; 1 represents a convolutional layer with a kernel size of 1 &#215; 1, f 3 &#215; 3 d e c denotes a depthwise separable convolutional layer with a kernel size of 3 &#215; 3, and &#963; (&#183;) refers to the GELU activation function. 2.2. Dual-Path Residual Fusion Module (DRFM) In medical image segmentation tasks, effectively capturing local features while expanding the receptive field to improve segmentation accuracy remains a core challenge. Existing methods often employ multi-scale convolutional kernels to address this issue. While capable of capturing broader contextual information, these approaches typically incur high computational costs and fail to adequately model the correlations between features under different receptive fields. To overcome these limitations, we propose the Dual-path Residual Fusion Module (DRFM), whose structure is illustrated in Figure 1 e. This module adopts a dual-path parallel residual design, synergistically integrating features extracted by standard convolutions and dilated convolutions to generate more informative and robust feature representations. Specifically, the input feature X l first undergoes a 1 &#215; 1 convolution for channel dimension adjustment, yielding feature X l + 1 . It is then fed into two parallel paths: one path uses a 3 &#215; 3 standard convolution to capture local detailed textures, while the other employs a 3 &#215; 3 dilated convolution with a dilation rate of 2 to acquire wide-range contextual information at a lower computational cost. The outputs from both paths are fused to obtain X l + 2 . This feature X l + 2 subsequently undergoes deeper processing via a parallel set of standard and dilated convolutions, producing X l + 3 . Finally, a 1 &#215; 1 convolution integrates all learned features, and the result is added to the residual connection-provided X l + 1 to produce the final output feature X o u t . The mathematical formulation of the DRFM is as follows: (7) X l + 1 = f 1 &#215; 1 X l (8) &#160; X l + 2 = C o n c a t B a t c h N o r m f 3 &#215; 3 X l + 1 , &#160; B a t c h N o r m f 3 &#215; 3 2 X l + 1 (9) &#160; X l + 3 = C o n c a t B a t c h N o r m f 3 &#215; 3 X l + 2 , &#160; B a t c h N o r m f 3 &#215; 3 2 X l + 2 (10) &#160; X o u t = X l + 1 + f 1 &#215; 1 X l + 3 Among these, f 1 &#215; 1 and f 3 &#215; 3 denote convolutional layers with kernel sizes of 1 &#215; 1 and 3 &#215; 3, respectively, f 3 &#215; 3 2 represents a dilated convolutional layer with a dilation rate of 2 and a kernel size of 3 &#215; 3, and C o n c a t refers to concatenation along the channel dimension. 2.3. Multi-Scale Aggregation Attention Network (MAAN) In medical image segmentation tasks, significant size variations among target structures impose high demands on multi-scale feature extraction capabilities. To integrate multi-scale semantic information and enhance feature representation capacity for targets of different sizes, we design the Multi-scale Aggregation Attention Network (MAAN), whose structure is illustrated in Figure 1 f. This module adopts a parallel dual-branch architecture to enhance input features from both channel and spatial dimensions, respectively. The feature map X output by the DRFM encoder is fed into the MAAN module for refinement. In the channel branch, the spatial dimensions are compressed into channel descriptors of size C &#215; 1 &#215; 1 through average pooling and max pooling. These descriptors then sequentially pass through a 1 &#215; 1 convolution, a ReLU activation function, another 1 &#215; 1 convolution, and a Sigmoid activation function to generate channel attention weights. These weights are multiplied element-wise with the input feature X , yielding the channel-enhanced feature X c h a n n e l . In the spatial branch, the feature X undergoes a serial cascaded structure composed of 3 &#215; 3, 5 &#215; 5, and 7 &#215; 7 convolutions, with 1 &#215; 1 convolutions used for feature aggregation between layers, producing the multi-scale fused feature X M S . Subsequently, both average pooling and max pooling are applied, and the resulting features are concatenated along the channel dimension. This is followed by a 7 &#215; 7 convolution and a Sigmoid activation function to generate a spatial attention map. Finally, this map is weighted and fused with X M S to produce the spatially enhanced feature X s p a t i a l . The channel feature X c h a n n e l and the spatial feature X s p a t i a l are combined with the original input feature X via residual connection to produce the output feature X o u t , thereby enhancing the feature representation capacity in both spatial and channel dimensions. This cross-scale information interaction mechanism effectively improves the model&#8217;s ability to represent multi-target structures with significant scale variations. The mathematical formulation of the MAAN is as follows: (11) X c h a n n e l = &#963; f 1 &#215; 1 R e L U f 1 &#215; 1 M P s X + A P s X &#8855; X (12) &#160; X M S = f 7 &#215; 7 f 1 &#215; 1 f 5 &#215; 5 f 1 &#215; 1 f 3 &#215; 3 X (13) &#160; X s p a t i a l = &#963; f 7 &#215; 7 C o n c a t M P c X M S , A P c X M S &#8855; X M S (14) &#160; X o u t = X + X c h a n n e l + X s p a t i a l Among these, f 1 &#215; 1 , f 3 &#215; 3 , f 5 &#215; 5 and f 7 &#215; 7 denote convolutional layers with kernel sizes of 1 &#215; 1, 3 &#215; 3, 5 &#215; 5 and 7 &#215; 7; M P c and A P c indicate max pooling and average pooling operations along the channel dimension; M P s and A P s represent max pooling and average pooling operations along the spatial dimensions; &#963; (&#183;) denotes the Sigmoid activation function; and C o n c a t refers to concatenation along the channel dimension. 3. Experiments 3.1. Datasets (1) The Abdomen MRI dataset: This dataset used in this study is sourced from the publicly available MICCAI 2022 AMOS Challenge resources [ 29 ]. It is designed for segmenting 13 abdominal organs, such as the liver, spleen, pancreas, kidneys, stomach, gallbladder, esophagus, aorta, inferior vena cava, adrenal glands, and duodenum. To ensure statistical reliability, the data partition (60 scans for training, 50 for testing) follows the established benchmark protocol in reference [ 27 ], which introduced additional scans to overcome limitations of a smaller validation set. These test scans are fully independent, with no patient overlap against the training set, and were annotated by professional radiologists to ensure quality. Throughout the experiments, all images were preprocessed to a resolution of 320 &#215; 320 pixels. (2) The Microscopy dataset: This dataset focuses on cell instance segmentation tasks, with image data originating from the publicly available NeurIPS 2022 Cell Segmentation Challenge dataset [ 32 ]. Its specific composition includes 1000 images for training and 101 images for testing. To standardize the input size, all images were cropped to 512 &#215; 512 pixels before training and testing. The data processing methodology follows the scheme proposed in reference [ 27 ]. (3) The ACDC dataset: This dataset comprises cardiac MRI scans from 150 patients, with each patient containing scans from different physiological phases, such as systole and diastole. Sourced from the Automatic Cardiac Diagnosis Challenge [ 33 ], its core task is to segment the left ventricle, right ventricle, and myocardium. Our experiment utilizes data from 100 patients in this dataset. We divided these 100 cases into training and test sets in an 8:2 ratio. The training set contains 160 scans from 80 patients, and the test set contains 40 scans from 20 patients. All images used for training and testing were preprocessed and uniformly resized to 256 &#215; 256 pixels. 3.2. Evaluation Metrics and Baselines For MRI-based datasets, Abdomen MRI and ACDC, the Dice Similarity Coefficient (DSC) was used to measure volumetric overlap between segmentations and ground truth, while the Normalized Surface Distance (NSD) evaluated boundary accuracy. For the Microscopy dataset, which involves instance-level cell identification, the F1-Score served as the primary metric to assess detection performance at the object level. A prediction was counted as a True Positive only if the IoU with the ground truth exceeded 0.5. Additionally, the DSC metric was applied to complement this by evaluating pixel-level segmentation accuracy within each correctly detected instance. The calculation formulas for all metrics are as follows: (15) D S C = 2 T P T P + F N + T P + F P (16) &#160; N S D = S p r e d &#8745; S g t , &#964; + S g t &#8745; S p r e d , &#964; S p r e d + S g t (17) I o U = T P T P + F P + F N (18) F 1 = 2 &#215; P r e c i s i o n &#215; R e c a l l P r e c i s i o n + R e c a l l , &#160; i f &#160; I o U &gt; 0.5 (19) P r e c i s i o n = T P T P + F P (20) R e c a l l = T P T P + F N Here, TP (True Positive) denotes the number of samples correctly predicted as positive; FP (False Positive) denotes the number of samples incorrectly predicted as positive; and FN (False Negative) denotes the number of samples incorrectly predicted as negative. S p r e d and S g t represent the sets of surface points corresponding to the predicted segmentation and the ground truth segmentation, respectively. S p r e d , &#964; is defined as the set of all points in the predicted surface whose distance to the ground truth surface is less than the threshold &#964; ; similarly, S g t , &#964; refers to the set of all points in the ground truth surface whose distance to the predicted surface is within &#964; . We compared five medical image segmentation models under uniform experimental conditions, covering three major architecture types: The traditional CNN architectures are represented by nnU-Net [ 17 ] and SegResNet [ 16 ]. The Transformer-based architectures are represented by UNETR [ 34 ] and SwinUNETR [ 35 ]. Swin-UMamba [ 29 ] was selected as the representative Mamba-based architecture. All models were rigorously reproduced within the nnU-Net framework, strictly adhering to the hyperparameter configurations reported in their respective original publications. 3.3. Implementation Details In the architecture of the DPM-UNet network, each of stages 4 to 5 is configured with four consecutive DPMamba modules, forming a [4, 4] structure. All models are implemented within the nnU-Net framework without using any pre-trained weights, and all parameters are optimized through training from scratch. This design enables a focused investigation into network architecture innovation while maintaining consistent experimental conditions&#8212;such as image preprocessing and data augmentation&#8212;across all compared methods. Consequently, DPM-UNet is evaluated under uniform settings, ensuring that the network architecture remains the sole differentiating factor. During training, patch size, batch size, and network configuration adhere to the standard nnU-Net settings. The training process employs the AdamW optimizer with a weight decay coefficient of 0.05. The initial learning rate is set to 2 &#215; 10 &#8722;4 and decays to a minimum of 1 &#215; 10 &#8722;6 using a cosine annealing scheduling strategy. The loss function is defined as the unweighted sum of Dice loss and cross-entropy loss. A five-fold cross-validation strategy is applied on the training set for all three datasets. Since the Dice Similarity Coefficient (DSC) is the most critical evaluation metric in our task, the model checkpoint achieving the highest DSC on the validation set is selected as the optimal model for each training run. The models are trained for 1000 epochs in total without adopting an early stopping strategy, leveraging the cosine annealing learning rate scheduler for full-cycle optimization. The batch size is set to 4 for the microscopy dataset and 8 for the other two datasets. The training curves of three datasets are shown in Figure 2 . All experiments are implemented using the PyTorch (torch = 2.1.1) framework on hardware equipped with an NVIDIA RTX 4070 GPU. 3.4. Experimental Results Table 1 presents the quantitative results of multi-organ segmentation on the Abdomen MRI dataset. The DPM-UNet model achieved a DSC of 78.15%, representing a 1.52 percentage point improvement over the second-best performing nnU-Net, while its NSD reached 84.67%, with an improvement of 1.16 percentage points. As shown in Figure 3 , regarding organ-level segmentation accuracy, DPM-UNet ranked first in 10 organs including the liver, left and right kidneys, spleen, pancreas, inferior vena cava, left adrenal gland, esophagus, stomach, and duodenum. The overall segmentation visualization in Figure 4 and the enlarged local details in Figure 5 consistently demonstrate that DPM-UNet not only accurately captures the main structures of organs but also produces more continuous and smoother segmentation contours along complex anatomical boundaries. These results fully validate the effectiveness of the proposed global&#8211;local feature collaboration mechanism in representing multi-scale anatomical structures. The experimental results on the Microscopy dataset are presented in Table 2 . The DPM-UNet model achieved a DSC of 73.25%, representing a 1.56 percentage point improvement over the second-best performing UNETR, while its F1-score reached 60.23%, with an improvement of 5.58 percentage points. As shown in the visual comparison in Figure 6 , DPM-UNet accurately captures the main structures of cells. The enlarged local details in Figure 7 further demonstrate that DPM-UNet effectively identifies and separates small-sized or adherent nuclei with more precise edge delineation. These results underscore the advantages of the model&#8217;s local feature extraction mechanism, which enables it to capture subtle differential features and fine boundary information between nuclei, thereby significantly improving segmentation accuracy. The experimental results on the ACDC dataset are presented in Table 3 . While the DPM-UNet model achieved optimal performance, the overall performance gap among different models was relatively small. Specifically, DPM-UNet improved the DSC metric by 0.14% compared to nnU-Net and enhanced the NSD metric by 0.05% compared to Swin-UMamba. In the substructure segmentation tasks, DPM-UNet delivered optimal performance for myocardial (Myo) and left ventricular (LV) segmentation, while ranking third in right ventricular (RV) segmentation. The overall comparison in Figure 8 and the enlarged local details in Figure 9 demonstrate that DPM-UNet generates more accurate and sharper segmentation contours, further validating its local&#8211;global feature interaction capability in effectively capturing critical subtle structural features along the edges. 3.5. Further Analysis 3.5.1. Ablation Study The ablation experiments on the abdominal MRI dataset evaluated the effectiveness of the proposed modules, with results summarized in Table 4 . The introduction of the DRFM alone improved performance, increasing the Dice Similarity Coefficient (DSC) by 0.45% and the Normalized Surface Distance (NSD) by 0.88%. After integrating both the DRFM and DPMamba modules, the model performance was further enhanced, achieving additional gains of 0.40% in DSC and 0.41% in NSD. This demonstrates the capability of the DPMamba module to effectively improve segmentation accuracy. Subsequently, incorporating the CBAM module led to further improvements, with DSC and NSD increasing by an additional 0.82% and 0.73%, respectively. When the CBAM module was replaced with the MAAN module, the model achieved its optimal performance, reaching a DSC of 78.15% and an NSD of 84.67%. MAAN outperforms CBAM due to its structural advantage; a dual-path design that concurrently models&#8217; channel and spatial relationships, combined with multi-scale feature aggregation, which enables more effective context capture than CBAM&#8217;s sequential design. These results fully validate the effectiveness of the designed modules and their synergistic contribution to enhancing the model&#8217;s overall segmentation capability. ( Note : In the following table, a check mark (&#10004;) or cross mark (&#10008;) indicates the presence or absence of the corresponding module. An upward (&#8595;) or downward (&#8593;) arrow indicates that a higher or lower value denotes better performance, respectively.) To further investigate the impact of the DRFM structure on the model&#8217;s feature extraction capability, we conducted additional ablation experiments, with the results summarized in Table 5 . The corresponding schematic diagrams of each experimental setup can be found in Figure 10 a&#8211;d. The experimental results demonstrate that introducing dilated convolutions in the second path effectively enhances model performance. Furthermore, fusing features extracted by standard convolutions and dilated convolutions yielded more substantial performance improvements. Additionally, the incorporation of residual connections also contributed significantly to the overall performance enhancement. These results indicate that the DRFM can effectively strengthen the model&#8217;s feature representation capability. 3.5.2. Model Complexity As shown in Table 6 , this study employs floating-point operations (FLOPs), number of parameters (Params), and the total training time over 1000 epochs as key metrics to evaluate model complexity and practical training efficiency. In terms of computational and storage complexity, DPM-UNet demonstrates a moderate level among the compared models. However, its training time is the second longest, indicating a higher computational cost during the training phase. Critically, the superior performance of DPM-UNet justifies this increased training cost. It achieves the highest Dice Similarity Coefficient (DSC) and Normalized Surface Dice (NSD), demonstrating that the method achieves a favorable balance between computational efficiency and segmentation accuracy. 4. Conclusions and Future Work This study proposes DPM-UNet, a medical image segmentation network based on the Mamba architecture. The method adopts a U-shaped structure, where a Dual-path Residual Fusion Module (DRFM) is introduced in the shallow layers to enhance local detail feature extraction, while a DPMamba module capable of modeling long-range dependencies is employed in the deep layers to capture global contextual information. Furthermore, the Multi-scale Aggregation Attention Network (MAAN) is incorporated to strengthen the model&#8217;s ability to perceive and fuse multi-scale features. Experimental results on three public medical image segmentation datasets demonstrate the superior segmentation performance of the proposed method. While the current study has established a solid foundation, certain aspects, such as multi-run performance validation, statistical significance testing, and detailed efficiency profiling, were not fully explored within the scope of this work. These areas present meaningful opportunities for further investigation. Future efforts will focus on developing more lightweight architectures and efficient training strategies to reduce computational costs, alongside incorporating rigorous benchmarking and efficiency analysis to enhance the robustness and practicality of the model under resource constraints. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, S.X., X.L. and B.H.; formal analysis, S.X.; investigation, S.X.; methodology, S.X.; resources, S.X.; software, S.X.; supervision, B.H.; validation, S.X., X.L. and H.L.; visualization, S.X., X.L. and H.L.; writing&#8212;original draft, S.X.; writing&#8212;review and editing, S.X., X.L., H.L. and B.H. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement You can download the AbdomenMRI/Microscopy/ACDC dataset at https://drive.google.com/drive/folders/1CH2OWQpd4Sa-BES6oFLRC469gTxf6QUO (accessed on 11 November 2025). Conflicts of Interest The authors declare no conflicts of interest. References 1. Dai L. Sheng B. Chen T. Wu Q. Liu R. Cai C. Wu L. Yang D. Hamzah H. Liu Y. A deep learning system for predicting time to progression of diabetic retinopathy Nat. Med. 2024 30 584 594 10.1038/s41591-023-02702-z 38177850 PMC10878973 2. Chen X. Wang X. Zhang K. Fung K.M. Thai T.C. Moore K. Mannel R.S. Liu H. Zheng B. Qiu Y. Recent advances and clinical applications of deep learning in medical image analysis Med. Image Anal. 2022 79 102444 10.1016/j.media.2022.102444 35472844 PMC9156578 3. Bai W. Suzuki H. Huang J. Francis C. Wang S. Tarroni G. Guitton F. Aung N. Fung K. Petersen S.E. A population-based phenome-wide association study of cardiac and aortic structure and function Nat. Med. 2020 26 1654 1662 10.1038/s41591-020-1009-y 32839619 PMC7613250 4. Mei X. Lee H.-C. Diao K.-y. Huang M. Lin B. Liu C. Xie Z. Ma Y. Robson P.M. Chung M. Artificial intelligence&#8211;enabled rapid diagnosis of patients with COVID-19 Nat. Med. 2020 26 1224 1228 10.1038/s41591-020-0931-3 32427924 PMC7446729 5. Lecun Y. Bottou L. Bengio Y. Haffner P. Gradient-based learning applied to document recognition Proc. IEEE 1998 86 2278 2324 10.1109/5.726791 6. Jungo A. Meier R. Ermis E. Blatti-Moreno M. Herrmann E. Wiest R. Reyes M. On the Effect of Inter-Observer Variability for a Reliable Estimation of Uncertainty of Medical Image Segmentation Springer Cham, Switzerland 2018 682 690 7. Graham S. Vu Q.D. Raza S.E.A. Azam A. Tsang Y.W. Kwak J.T. Rajpoot N. Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images Med. Image Anal. 2019 58 101563 10.1016/j.media.2019.101563 31561183 8. Sun M. Zou W. Wang Z. Wang S. Sun Z. An Automated Framework for Histopathological Nucleus Segmentation with Deep Attention Integrated Networks IEEE ACM Trans. Comput. Biol. Bioinform. 2024 21 995 1006 10.1109/TCBB.2022.3233400 37018302 9. Gibson E. Giganti F. Hu Y. Bonmati E. Bandula S. Gurusamy K. Davidson B. Pereira S.P. Clarkson M.J. Barratt D.C. Automatic Multi-Organ Segmentation on Abdominal CT with Dense V-Networks IEEE Trans. Med. Imaging 2018 37 1822 1834 10.1109/TMI.2018.2806309 29994628 PMC6076994 10. Qi X. Wu Z. Zou W. Ren M. Gao Y. Sun M. Zhang S. Shan C. Sun Z. Exploring Generalizable Distillation for Efficient Medical Image Segmentation IEEE J. Biomed. Health Inform. 2024 28 4170 4183 10.1109/JBHI.2024.3385098 38954557 11. Khened M. Kollerathu V.A. Krishnamurthi G. Fully convolutional multi-scale residual DenseNets for cardiac segmentation and automated cardiac diagnosis using ensemble of classifiers Med. Image Anal. 2019 51 21 45 10.1016/j.media.2018.10.004 30390512 12. Jiang X. Hoffmeister M. Brenner H. Muti H.S. Yuan T. Foersch S. West N.P. Brobeil A. Jonnagaddala J. Hawkins N. End-to-end prognostication in colorectal cancer by deep learning: A retrospective, multicentre study Lancet Digit. Health 2024 6 e33 e43 10.1016/S2589-7500(23)00208-X 38123254 13. He K. Zhang X. Ren S. Sun J. Deep Residual Learning for Image Recognition Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Las Vegas, NV, USA 27&#8211;30 June 2016 770 778 14. Wang W. Dai J. Chen Z. Huang Z. Li Z. Zhu X. Hu X. Lu T. Lu L. Li H. InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions arXiv 2022 2211.05778 15. Ronneberger O. Fischer P. Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation Springer Cham, Switzerland 2015 234 241 16. Myronenko A. 3D MRI Brain Tumor Segmentation Using Autoencoder Regularization Springer Cham, Switzerland 2019 311 320 17. Isensee F. Jaeger P.F. Kohl S.A.A. Petersen J. Maier-Hein K.H. nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 18. Chen J. Lu Y. Yu Q. Luo X. Adeli E. Wang Y. Lu L. Yuille A.L. Zhou Y. TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation arXiv 2021 10.48550/arXiv.2102.04306 2102.04306 19. Zhang D. Zhang L. Tang J. Augmented FCN: Rethinking context modeling for semantic segmentation Sci. China Inf. Sci. 2023 66 142105 10.1007/s11432-021-3590-1 20. Raghu M. Unterthiner T. Kornblith S. Zhang C. Dosovitskiy A. Do Vision Transformers See Like Convolutional Neural Networks? Proceedings of the Neural Information Processing Systems Virtual 6&#8211;10 December 2021 21. Hatamizadeh A. Yin H. Kautz J. Molchanov P. Global Context Vision Transformers Proceedings of the International Conference on Machine Learning Baltimore, MD, USA 17&#8211;23 July 2022 22. Gu A. Dao T. Mamba: Linear-Time Sequence Modeling with Selective State Spaces arXiv 2023 10.48550/arXiv.2312.00752 2312.00752 23. Gu A. Goel K. R&#233; C. Efficiently Modeling Long Sequences with Structured State Spaces arXiv 2021 2111.00396 24. Gu A. Johnson I. Goel K. Saab K.K. Dao T. Rudra A. R&#233; C. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers Proceedings of the Neural Information Processing Systems Virtual 6&#8211;10 December 2021 25. Liu Y. Tian Y. Zhao Y. Yu H. Xie L. Wang Y. Ye Q. Liu Y. VMamba: Visual State Space Model arXiv 2024 2401.10166 26. Huang T. Pei X. You S. Wang F. Qian C. Xu C. LocalMamba: Visual State Space Model with Windowed Selective Scan Proceedings of the ECCV Workshops Milan, Italy 29 September&#8211;4 October 2024 27. Ma J. Li F. Wang B.J.A. U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation arXiv 2024 2401.04722 28. Xing Z. Ye T. Yang Y. Liu G. Zhu L. SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention Marrakesh, Morocco 6&#8211;10 October 2024 29. Liu J. Yang H. Zhou H.-Y. Xi Y. Yu L. Yu Y. Liang Y. Shi G. Zhang S. Zheng H. Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention Marrakesh, Morocco 6&#8211;10 October 2024 30. Rajagopal A. Nirmala V. Convolutional Gated MLP: Combining Convolutions &amp; gMLP arXiv 2021 10.48550/arXiv.2111.03940 2111.03940 31. Hu J. Shen L. Sun G. Squeeze-and-Excitation Networks Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;23 June 2018 7132 7141 32. Ma J. Xie R. Ayyadhury S. Ge C. Gupta A. Gupta R. Gu S. Zhang Y. Lee G. Kim J. The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions arXiv 2023 2308.05864 10.1038/s41592-024-02233-6 PMC11210294 38532015 33. Bernard O. Lalande A. Zotti C. Cervenansky F. Yang X. Heng P.A. Cetin I. Lekadir K. Camara O. Ballester M.A.G. Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved? IEEE Trans. Med. Imaging 2018 37 2514 2525 10.1109/TMI.2018.2837502 29994302 34. Hatamizadeh A. Tang Y. Nath V. Yang D. Myronenko A. Landman B. Roth H. Xu D. UNETR: Transformers for 3D Medical Image Segmentation arXiv 2021 10.48550/arXiv.2103.10504 2103.10504 35. Hatamizadeh A. Nath V. Tang Y. Yang D. Roth H. Xu D. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images arXiv 2022 10.48550/arXiv.2201.01266 2201.01266 Figure 1 The overall architecture of the proposed Mamba-based U-Net with Dynamic Perception Feature Enhancement (DPM-UNet) for medical image segmentation (DPM-Unet). ( a ) The main framework of DPM-Unet. ( b ) The detailed design of the DPMamba Module, which internally contains a ( c ) VSS Block and a ( d ) Dynamic Perception Feature Enhancement (DPFE) Block. ( e ) The Dual-path Residual Fusion Module (DRFM). ( f ) The Multi-scale Aggregation Attention Network (MAAN). Figure 2 Training loss curves for the three datasets. Figure 3 Segmentation Results (DSC) for Various Organs on the Abdomen MRI dataset. Figure 4 Visualizations on the Abdomen MRI dataset. Figure 5 Enlarged view of the region indicated by the red box in the Abdomen MRI visualization. Figure 6 Visualizations on the Microscopy dataset. Figure 7 Enlarged view of the region indicated by the red box in the Microscopy visualization. Figure 8 Visualizations on the ACDC dataset. Figure 9 Enlarged view of the region indicated by the red box in the ACDC visualization. Figure 10 Comparison of convolutional module structures. ( a ) Standard convolution block, ( b ) Dual-path convolution block, ( c ) Dual-path convolution block with feature mixing, ( d ) Our proposed DRFM block. sensors-25-07053-t001_Table 1 Table 1 Segmentation accuracy of different models on the Abdomen MRI dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore. Methods DSC NSD nnU-Net 76.63 83.51 SegResNet 73.84 80.13 UNETR 57.46 62.82 SwinUNETR 69.08 74.73 Swin-UMamba 74.56 81.15 DPM-UNet 78.15 84.67 sensors-25-07053-t002_Table 2 Table 2 Segmentation accuracy of different models on the Microscopy dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore. Methods DSC F1 nnU-Net 69.55 54.65 SegResNet 68.51 54.02 UNETR 71.69 40.57 SwinUNETR 66.69 37.08 Swin-UMamba 67.60 49.00 DPM-UNet 73.25 60.23 sensors-25-07053-t003_Table 3 Table 3 Segmentation accuracy of different models on the ACDC dataset. The best results are displayed in bold, and the second-best results are indicated with an underscore. Methods DSC NSD RV Myo LV nnU-Net 91.81 97.88 89.44 90.60 95.38 SegResNet 91.71 97.99 89.64 90.27 95.23 UNETR 89.34 95.49 86.76 87.46 93.80 SwinUNETR 91.50 97.61 89.41 90.03 95.06 Swin-UMamba 91.69 98.04 90.00 89.78 95.30 DPM-UNet 91.95 98.09 89.46 90.76 95.64 sensors-25-07053-t004_Table 4 Table 4 Ablation study of the designed blocks on the Abdomen MRI dataset. DRFM DPMamba CBAM MAAN DSC &#8593; NSD &#8593; &#10008; &#10008; &#10008; &#10008; 75.17 81.52 &#10004; &#10008; &#10008; &#10008; 75.62 82.40 &#10004; &#10004; &#10008; &#10008; 76.02 82.81 &#10004; &#10004; &#10004; &#10008; 76.84 83.54 &#10004; &#10004; &#10008; &#10004; 78.15 84.67 sensors-25-07053-t005_Table 5 Table 5 Ablation study of DRFM structure on the Abdomen MRI dataset. Dilated Convolution Fusion Residual DSC &#8593; NSD &#8593; &#10008; &#10008; &#10008; 76.04 82.80 &#10004; &#10008; &#10008; 76.35 83.06 &#10004; &#10004; &#10008; 77.53 84.32 &#10004; &#10004; &#10004; 78.15 84.67 sensors-25-07053-t006_Table 6 Table 6 Comparison of FLOPs and Params on the Abdomen MRI dataset using our method with other models. Methods FLOPs (G) &#8595; Param. (M) &#8595; Training Time (H) DSC &#8593; NSD &#8593; nnU-Net 23 33 6 76.63 83.51 SegResNet 24 6 8 73.84 80.13 UNETR 41 87 17 57.46 62.82 SwinUNETR 29 25 20 69.08 74.73 Swin-UMamba 63 59 30 74.56 81.15 DPM-UNet 31 38 24 78.15 84.67"
}