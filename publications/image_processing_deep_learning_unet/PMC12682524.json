{
  "pmcid": "PMC12682524",
  "source": "PMC",
  "download_date": "2025-12-09T16:06:41.111321",
  "metadata": {
    "journal_title": "Quantitative Imaging in Medicine and Surgery",
    "journal_nlm_ta": "Quant Imaging Med Surg",
    "journal_iso_abbrev": "Quant Imaging Med Surg",
    "journal": "Quantitative Imaging in Medicine and Surgery",
    "pmcid": "PMC12682524",
    "doi": "10.21037/qims-2025-1364",
    "title": "MFA-Net: multi-scale feature aggregation network with background-aware module for ultrasound segmentation of thyroid nodules",
    "year": "2025",
    "month": "11",
    "day": "21",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "21"
    },
    "authors": [
      "Ye Dongfen",
      "Lan Kun",
      "Cheng Jianzhen",
      "Jiang Xiaoliang"
    ],
    "abstract": "Background The size and morphology of thyroid nodules are the essential basis for distinguishing benign and malignant in clinical diagnosis. However, achieving precise segmentation of these nodules in ultrasound images remains a significant task due to the weak and indistinct edges, low contrast, and complex internal structure. To tackle these challenges, our goal is to develop a multi-scale feature aggregation network (MFA-Net) with background-aware module (BAM), which can effectively and robustly segment ultrasound images of thyroid nodules. Methods In MFA-Net framework, through the multi-scale feature aggregation module (MFAM), it effectively captures multi-scale context information and thus improves fine-grained details and global structure representation. Additionally, the BAM inhibits background noise, which allows for effective differentiation between nodules and surrounding tissue. To refine the segmentation performance, we add spatial and channel attentions to the residual decoder module (RDM). Results The quantitative evaluation results showed that on the thyroid nodule 3493 (TN3K) dataset, MFA-Net achieved Dice of 0.8616, intersection over union (IoU) of 0.7586, accuracy of 0.9698 and Matthews correlation coefficient (Mcc) of 0.8457. On the thyroid gland 3583 (TG3K) dataset, the Dice, IoU, accuracy and Mcc reached 0.9857, 0.9718, 0.9977 and 0.9844. On the digital database thyroid image (DDTI) dataset, the Dice, IoU, accuracy and Mcc were 0.7483, 0.5981 0.9283 and 0.7078. On the BrainTumor dataset, MFA-Net obtained Dice of 0.8485, IoU of 0.7421, accuracy of 0.9949 and Mcc of 0.8469. Conclusions These results outperform current leading models and confirm significant performance improvements. In addition, the MFAM, BAM and RDM demonstrate their robustness and adaptability in different segmentation scenarios when conducting independent ablation experiments.",
    "keywords": [
      "Thyroid nodules diagnosis",
      "ultrasound image segmentation",
      "multi-scale feature aggregation network (MFA-Net)",
      "multi-scale feature aggregation module (MFAM)",
      "background-aware mechanism"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"iso-abbrev\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1905</journal-id><journal-id journal-id-type=\"pmc-domain\">qims</journal-id><journal-id journal-id-type=\"publisher-id\">QIMS</journal-id><journal-title-group><journal-title>Quantitative Imaging in Medicine and Surgery</journal-title></journal-title-group><issn pub-type=\"ppub\">2223-4292</issn><issn pub-type=\"epub\">2223-4306</issn><publisher><publisher-name>AME Publications</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682524</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682524.1</article-id><article-id pub-id-type=\"pmcaid\">12682524</article-id><article-id pub-id-type=\"pmcaiid\">12682524</article-id><article-id pub-id-type=\"doi\">10.21037/qims-2025-1364</article-id><article-id pub-id-type=\"publisher-id\">qims-15-12-12167</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>MFA-Net: multi-scale feature aggregation network with background-aware module for ultrasound segmentation of thyroid nodules</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Ye</surname><given-names initials=\"D\">Dongfen</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Lan</surname><given-names initials=\"K\">Kun</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Cheng</surname><given-names initials=\"J\">Jianzhen</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">\n<sup>3</sup>\n</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names initials=\"X\">Xiaoliang</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref></contrib><aff id=\"aff1\"><label>1</label><institution>College of Electrical and Information Engineering, Quzhou University, Quzhou</institution>, <country country=\"cn\">China</country>;</aff><aff id=\"aff2\"><label>2</label><institution>College of Mechanical Engineering, Quzhou University, Quzhou</institution>, <country country=\"cn\">China</country>;</aff><aff id=\"aff3\"><label>3</label><institution>Department of Rehabilitation, Quzhou Third Hospital, Quzhou</institution>, <country country=\"cn\">China</country></aff></contrib-group><author-notes><fn id=\"afn1\"><p><italic toggle=\"yes\">Contributions:</italic> (I) Conception and design: D Ye, X Jiang; (II) Administrative support: J Cheng; (III) Provision of study materials or patients: D Ye, K Lan; (IV) Collection and assembly of data: D Ye, K Lan; (V) Data analysis and interpretation: D Ye, X Jiang; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors.</p></fn><corresp id=\"cor1\"><italic toggle=\"yes\">Correspondence to:</italic> Kun Lan, PhD. College of Mechanical Engineering, Quzhou University, 78 North Jiuhua Road, Quzhou 324000, China. Email: <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"36116@qzc.edu.cn\">36116@qzc.edu.cn</email>; Jianzhen Cheng, Bachelor. Department of Rehabilitation, Quzhou Third Hospital, 226 North Baiyun Road, Quzhou 324000, China. Email: <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qzsycjz@163.com\">qzsycjz@163.com</email>.</corresp></author-notes><pub-date pub-type=\"epub\"><day>21</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"ppub\"><day>01</day><month>12</month><year>2025</year></pub-date><volume>15</volume><issue>12</issue><issue-id pub-id-type=\"pmc-issue-id\">502028</issue-id><fpage>12167</fpage><lpage>12189</lpage><history><date date-type=\"received\"><day>15</day><month>6</month><year>2025</year></date><date date-type=\"accepted\"><day>11</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 00:25:14.317\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 AME Publishing Company.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>AME Publishing Company.</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><italic toggle=\"yes\">Open Access Statement:</italic> This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">https://creativecommons.org/licenses/by-nc-nd/4.0</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"qims-15-12-12167.pdf\"/><abstract><sec><title>Background</title><p>The size and morphology of thyroid nodules are the essential basis for distinguishing benign and malignant in clinical diagnosis. However, achieving precise segmentation of these nodules in ultrasound images remains a significant task due to the weak and indistinct edges, low contrast, and complex internal structure. To tackle these challenges, our goal is to develop a multi-scale feature aggregation network (MFA-Net) with background-aware module (BAM), which can effectively and robustly segment ultrasound images of thyroid nodules.</p></sec><sec><title>Methods</title><p>In MFA-Net framework, through the multi-scale feature aggregation module (MFAM), it effectively captures multi-scale context information and thus improves fine-grained details and global structure representation. Additionally, the BAM inhibits background noise, which allows for effective differentiation between nodules and surrounding tissue. To refine the segmentation performance, we add spatial and channel attentions to the residual decoder module (RDM).</p></sec><sec><title>Results</title><p>The quantitative evaluation results showed that on the thyroid nodule 3493 (TN3K) dataset, MFA-Net achieved Dice of 0.8616, intersection over union (IoU) of 0.7586, accuracy of 0.9698 and Matthews correlation coefficient (Mcc) of 0.8457. On the thyroid gland 3583 (TG3K) dataset, the Dice, IoU, accuracy and Mcc reached 0.9857, 0.9718, 0.9977 and 0.9844. On the digital database thyroid image (DDTI) dataset, the Dice, IoU, accuracy and Mcc were 0.7483, 0.5981 0.9283 and 0.7078. On the BrainTumor dataset, MFA-Net obtained Dice of 0.8485, IoU of 0.7421, accuracy of 0.9949 and Mcc of 0.8469.</p></sec><sec><title>Conclusions</title><p>These results outperform current leading models and confirm significant performance improvements. In addition, the MFAM, BAM and RDM demonstrate their robustness and adaptability in different segmentation scenarios when conducting independent ablation experiments.</p></sec></abstract><kwd-group kwd-group-type=\"author\"><title>Keywords: </title><kwd>Thyroid nodules diagnosis</kwd><kwd>ultrasound image segmentation</kwd><kwd>multi-scale feature aggregation network (MFA-Net)</kwd><kwd>multi-scale feature aggregation module (MFAM)</kwd><kwd>background-aware mechanism</kwd></kwd-group><funding-group><award-group><funding-source id=\"sp1\">National Natural Science Foundation of China</funding-source><award-id rid=\"sp1\">No. 62102227</award-id></award-group></funding-group><funding-group><award-group><funding-source id=\"sp2\">Joint Fund of Zhejiang Provincial Natural Science Foundation of China</funding-source><award-id rid=\"sp2\">No. LZY24E060001</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\"><title>Introduction</title><p>Thyroid nodules are common diseases of the endocrine system and have become an important health problem of global concern. These nodules appear as abnormal growths within the thyroid and are usually caused by a variety of causes, including benign hyperplasia, cyst formation, benign tumors, or malignant transformations (<xref rid=\"r1\" ref-type=\"bibr\">1</xref>). With the continuous advancement of various imaging technologies, doctors can utilize them for early screening and diagnosis of thyroid nodules, which is critical to determining their nature and developing treatment plans. Among the various techniques, ultrasound imaging has become the frontline choice for thyroid nodules due to its low cost, non-radiation and ability to scan continuously. However, due to the blurred boundary of nodules, low contrast, and complex thyroid tissue structure, doctors&#8217; identification and segmentation of nodules are highly dependent on personal experience, which can easily lead to misdiagnosis or missed diagnosis (<xref rid=\"r2\" ref-type=\"bibr\">2</xref>). Therefore, the use of artificial intelligence technology to develop efficient thyroid nodule segmentation algorithms can help doctors provide patients with more reliable treatment plans and improve diagnostic efficiency.</p><p>Thyroid nodules in ultrasound images usually vary in size, shape, and appearance and are similar to the surrounding thyroid tissue. Additionally, ultrasound imaging is inherently susceptible to various types of noise, which stem from both environmental interference and instrumental limitations during tissue propagation. Given these complexities, traditional segmentation techniques that rely on handcrafted features or threshold-based methods frequently struggle to provide accurate and reliable results. However, with the development of deep learning technologies, new opportunities have been presented, such as U-Net (<xref rid=\"r3\" ref-type=\"bibr\">3</xref>), DESENet (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>), BMANet (<xref rid=\"r5\" ref-type=\"bibr\">5</xref>) and NLIE-UNet (<xref rid=\"r6\" ref-type=\"bibr\">6</xref>). These encoder-decoder models utilize hierarchical feature extraction and multi-scale information processing to improve segmentation accuracy. However, despite their effectiveness, they are not without limitations. One key issue is improper feature fusion, which can lead to feature fusion inconsistencies across different network layers. Additionally, during the encoding process, lesion boundary details may be gradually lost due to repeated down-sampling operations. Consequently, further optimization of feature aggregation and boundary refinement strategies is essential to improve the robustness and generalization ability of thyroid nodule segmentation.</p><p>The segmentation algorithms of thyroid nodules generally fall into traditional models and deep learning-based models. The traditional method primarily relies on gray-level intensity, texture features, or geometric information for feature extraction and region-based segmentation, which has the advantages of high computational efficiency and mature theory. However, due to the inherent characteristics of ultrasound images, which often lead to inconsistent and inaccurate segmentation results. In contrast, the deep learning-based algorithms leverage powerful architectures and transformer to automatically extract advanced features, which can effectively solve the challenges posed by ultrasonic noise and different nodule shapes. Among them, Wu<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r7\" ref-type=\"bibr\">7</xref>) incorporated deep convolutional layers within the encoder-decoder framework of Swin Transformer, effectively strengthening the representation of both global and local features. Furthermore, they introduced a multi-scale feature fusion module that enables more effective integration and exchange of features across different hierarchical levels. Li<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r8\" ref-type=\"bibr\">8</xref>) proposed a global structure-enhanced decoder that can effectively enhance feature representation and ensure more accurate boundary depiction. Hu<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r9\" ref-type=\"bibr\">9</xref>) designed a dual-decoder branch architecture by integrating Mamba and ResNet-34 to enhance feature extraction and segmentation performance. Ozcan<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r10\" ref-type=\"bibr\">10</xref>) proposed a hybrid segmentation model that enhances remote dependencies and the ability to capture global context information. Ali<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r11\" ref-type=\"bibr\">11</xref>) presented an encoder architecture with dense connections to enhance feature propagation and reuse across different network layers. Zheng<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r12\" ref-type=\"bibr\">12</xref>) adopted a cascade convolution strategy, which can effectively capture multi-scale context information while maintaining a larger acceptance field.</p><p>The attention mechanism (<xref rid=\"r13\" ref-type=\"bibr\">13</xref>,<xref rid=\"r14\" ref-type=\"bibr\">14</xref>) plays a crucial role in enhancing feature representation by selectively focusing on important regions while minimizing the impact of background noise and irrelevant information. By dynamically adjusting the weights of different spatial locations, channels, or positional elements, the attention mechanism can improve model performance and generalization ability. Due to its effectiveness, attention mechanism has been widely integrated into semantic segmentation, object detection, and image classification. Among them, Gu<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r15\" ref-type=\"bibr\">15</xref>) presented a multi-scale coordinate attention algorithm to enhance feature representation by effectively capturing spatial and contextual dependencies across different scales. Ni<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r16\" ref-type=\"bibr\">16</xref>) combined channel attention and positional attention to enhance feature representation by jointly capturing global dependencies and spatial relationships within an image. Shang<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r17\" ref-type=\"bibr\">17</xref>) introduced a cascaded attention fusion module, which enhanced feature representation by progressively integrating multi-level attention mechanisms. Qi<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r18\" ref-type=\"bibr\">18</xref>) combined graph-based convolution operations with attention mechanisms, which allowed for more focused and efficient encoding of relevant information. Apart from the utilization of attention mechanisms, several emerging technologies have demonstrated promising performance. Among them, Wu<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r19\" ref-type=\"bibr\">19</xref>) combined dynamic condition encoding with feature frequency parser and developed the first general medical image segmentation framework that utilizes the diffusion probabilistic model. Liu<italic toggle=\"yes\"> et al. </italic>(<xref rid=\"r20\" ref-type=\"bibr\">20</xref>) introduced a scale-aware pyramidal feature learning strategy, which explicitly exploits multi-scale contextual information to strengthen feature representation.</p><p>In this paper, we present a novel multi-scale feature aggregation network (MFA-Net) with background-aware module (BAM) for thyroid nodules segmentation. Unlike existing segmentation approaches, our MFA-Net leverages an encoder-decoder architecture combined with multi-scale feature extraction to enhance segmentation accuracy and robustness. By simultaneously modeling global dependencies across both spatial and channel dimensions, the network effectively captures long-range contextual information while preserving fine-grained details. The key contributions of our research are outlined as:</p><list list-type=\"simple\" id=\"L1\"><list-item><p>&#10070; The multi-scale feature aggregation module (MFAM) is designed to effectively capture contextual information at multiple levels, ensuring a comprehensive understanding of both local and global image structures. This design significantly enhances the model&#8217;s adaptability to different shapes, sizes and appearances of thyroid nodules, and improves the overall performance under complex imaging conditions.</p></list-item><list-item><p>&#10070; The background-aware mechanism is integrated to suppress irrelevant or non-informative regions, guiding the focus of the network to more prominent foreground areas. By refining feature selection, this mechanism enhances the distinction between nodules and surrounding tissue and reduces interference from background noise.</p></list-item><list-item><p>&#10070; The residual decoder module (RDM) is augmented with spatial and channel mechanisms, which work synergistically to highlight critical features while diminishing the influence of less relevant information. This dual-attention strategy improves segmentation precision by preserving structural integrity and enhancing boundary clarity, ultimately leading to more accurate and robust segmentation results.</p></list-item></list><p>We present this article in accordance with the CLEAR reporting checklist (available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc</ext-link>).</p></sec><sec sec-type=\"methods\"><title>Methods</title><p>This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments.</p><sec><title>Overview</title><p><xref rid=\"f1\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 1</italic></xref> illustrates the overall architecture of MFA-Net, which is a deep learning framework following the encoder-decoder structure. The encoder adopts Encoder-block while the decoder uses Decoder-block, both of which are designed for the encoding and decoding of thyroid nodule images. In the encoding stage, each Encoder-block employs a double convolutional inspired by U-Net structure, which enhances feature extraction while preserving fine-grained local details. To enhance the segmentation performance, MFA-Net incorporates MFAM and BAM. These modules are responsible for capturing multi-scale contextual information and suppressing interference information. In each layer, the MFAM processes the output features from the corresponding Encoder block and performs multi-scale contextual information acquisition. Then, the up-sampling (UP) module (transpose convolution using ConvTranspose2d function) enhances feature resolution by UP the output of the next-layer Decoder-block, making these refined features available for both the current Decoder-block and the prediction output. Meanwhile, the BAM strengthens model focus by applying attention mechanisms to distinguish foreground from background, leveraging both the Encoder block&#8217;s feature representations and the predictive output from the UP module. Throughout the decoding process, the Decoder-block accepts the output features from the BAM, MFAM, and UP. Finally, the highest-level Decoder-block performs final refinements using a 1&#215;1 convolution followed by a Sigmoid activation, ultimately generating the model&#8217;s segmentation prediction.</p><fig position=\"float\" id=\"f1\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 1</label><caption><p>The overall framework of MFA-Net. BAM, background-aware module; Conv, convolution; MFAM, Multi-scale feature aggregation module; MFA-Net, multi-scale feature aggregation network; UP, up-sampling.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f1.jpg\"/></fig></sec><sec><title>MFAM</title><p>As illustrated in <xref rid=\"f2\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 2</italic></xref>, the MFAM begins by receiving the feature maps from the Encoder-block. To enhance feature representation across different spatial scales, the input features are simultaneously passed through multiple convolutional branches, each utilizing distinct kernel sizes to extract diverse patterns and structural details. Firstly, the 1&#215;5 and 5&#215;1 convolutions are responsible for capturing fine-grained horizontal and vertical edge information. To further extend spatial awareness, the 1&#215;7 and 7&#215;1 convolutions contribute to capturing long-range dependencies. Finally, the 1&#215;11 and 11&#215;1 convolutions provide the largest receptive field, capturing both fine details and broader spatial relationships. Once features are extracted at multiple scales, their outputs are concatenated along the channel dimension to construct a comprehensive multi-scale feature representation. This aggregated feature map is subsequently processed through a 1&#215;1 convolution, which reduces channel dimensions while refining feature interactions. Moreover, the processed features are passed through a Sigmoid function, which normalizes values between 0 and 1, thereby learning attention-based weights for different feature regions. Following this, the Sigmoid-activated feature undergoes element-wise multiplication with the original encoder feature. This operation ensures that salient features are emphasized, while less relevant information is suppressed. The MFAM module plays a vital role in MFA-Net, as it effectively captures multi-scale contextual dependencies, leading to enhanced thyroid nodule segmentation with improved accuracy and robustness.</p><fig position=\"float\" id=\"f2\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Structure of MFAM. MFAM, multi-scale feature aggregation module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f2.jpg\"/></fig></sec><sec><title>BAM</title><p>As depicted in <xref rid=\"f3\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 3</italic></xref>, the BAM plays a crucial role in enhancing the model&#8217;s ability to discriminate between foreground (i.e., thyroid nodules) and background (surrounding non-nodule regions). Specifically, the feature map output from the Encoder block, as well as the prediction map generated by the UP module. To build an attention mechanism, BAM first computes the background-aware map by applying 1-pred to the prediction maps, where pred represents the probability map obtained by applying the Sigmoid activation to the predicted result. This transformation effectively inverts the predictions, ensuring that higher weights are assigned to background areas while foreground features are suppressed. The computed attention map is then applied to the encoder feature map channel-wise through element-wise multiplication. The attention-refined features are subsequently processed through a 3&#215;3 convolution layer and a squeeze-and-excitation block (<xref rid=\"r21\" ref-type=\"bibr\">21</xref>,<xref rid=\"r22\" ref-type=\"bibr\">22</xref>), which enhances channel-wise attention by dynamically adjusting feature importance, as shown in <xref rid=\"f4\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 4</italic></xref>. To preserve rich spatial information and prevent excessive feature loss, skip connection is introduced to add the original encoder features back to the refined attention-enhanced feature. By integrating BAM into the network, MFA-Net effectively suppresses background noise and improves prediction accuracy.</p><fig position=\"float\" id=\"f3\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Structure of BAM. BAM, background-aware module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f3.jpg\"/></fig><fig position=\"float\" id=\"f4\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Structure of squeeze-and-excitation block. FC, fully connected layer; ReLU, rectified linear unit.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f4.jpg\"/></fig></sec><sec><title>RDM</title><p>As illustrated in <xref rid=\"f5\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 5</italic></xref>, the Decoder-block incorporates an RDM, which processes inputs from the BAM, MFAM, and UP. To ensure comprehensive feature representation, these inputs are first concatenated together. Once concatenated, the features are refined through two successive 3&#215;3 convolutional layers. To further optimize learning, a 1&#215;1 convolution-based residual connection is introduced in parallel. Following feature transformation, the output is refined through an attention mechanism comprising channel attention and spatial attention. Ultimately, this structured decoding approach results in a well-defined feature representation at this stage, contributing to a more precise and context-aware final segmentation output.</p><fig position=\"float\" id=\"f5\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Structure of RDM. BAM, background-aware module; Conv, convolution; MFAM, Multi-scale feature aggregation module; RDM, residual decoder module; UP, up-sampling.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f5.jpg\"/></fig><p>As shown in <xref rid=\"f6\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 6</italic></xref>, the channel attention module (<xref rid=\"r23\" ref-type=\"bibr\">23</xref>,<xref rid=\"r24\" ref-type=\"bibr\">24</xref>) enriches feature representation by selectively emphasizing important channels, and its input feature map has dimensions of C &#215; H &#215; W. To extract inter-channel dependencies, we employ two parallel processing branches: one branch reshapes the input tensor into a matrix of size C &#215; (H &#215; W) to facilitate channel-wise interaction, and the second branch applies both reshaping and a transpose operation, transforming the feature map into a (H &#215; W) &#215; C matrix. The two transformed feature representations are then multiplied using a dot product to compute a channel affinity matrix of size C &#215; C. This matrix effectively captures the relationships between different channels. To adjust the attention distribution, the calculated channel association matrix is normalized by Softmax to ensure that the sum of all weights equals one.</p><fig position=\"float\" id=\"f6\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Structure of channel attention module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f6.jpg\"/></fig><p>Subsequently, the resulting attention matrix is applied to the reshaped feature matrix through matrix multiplication. To preserve essential original information while integrating learned attention-based improvements, the refined output is added back to the original input through a residual connection. This approach enhances feature learning stability and ensures that the output retains the same spatial dimensions C&#215;H&#215;W as the initial input, while benefiting from refined channel-wise feature weighting.</p><p>As illustrated in <xref rid=\"f7\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 7</italic></xref>, the spatial attention module (<xref rid=\"r25\" ref-type=\"bibr\">25</xref>,<xref rid=\"r26\" ref-type=\"bibr\">26</xref>) functions similarly to the channel attention module but differs in its primary focus. However, unlike capturing global dependencies between channels, spatial attention ensures higher attention to key areas such as object boundaries and key structural details by analyzing spatial dependencies. By dynamically adjusting attention weights across spatial positions, the module ensures that essential features are more prominent, which is especially advantageous in segmentation tasks where precise object delineation is required.</p><fig position=\"float\" id=\"f7\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Structure of spatial attention module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f7.jpg\"/></fig></sec><sec><title>Loss function</title><p>Unlike conventional loss functions that only operate on a pixel-by-pixel basis, we employ a combined loss function consisting of Dice loss (<xref rid=\"r27\" ref-type=\"bibr\">27</xref>) and binary cross-entropy (BCE) loss (<xref rid=\"r28\" ref-type=\"bibr\">28</xref>) to optimize the segmentation network. Among them, Dice loss is designed to evaluate the similarity between predicted segmentation maps and ground truth labels by directly computing their overlap. Additionally, BCE loss provides stable pixel-level supervision by penalizing misclassified pixels, which is particularly effective in handling class imbalance. By integrating these two components, the joint loss leverages the strengths of both global region-level optimization and local pixel-wise accuracy. For numerical stability, the joint loss is computed on the probability maps obtained after applying the Sigmoid activation to the network output. The mathematical formulation of the combined loss is as follows:</p><disp-formula id=\"e1\"><mml:math id=\"m1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><label>[1]</label></disp-formula><disp-formula id=\"e2\"><mml:math id=\"m2\" display=\"block\" overflow=\"scroll\"><mml:mtable columnalign=\"left\"><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math><label>[2]</label></disp-formula><p>where N is the number of pixels, <inline-formula><mml:math id=\"m3\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <italic toggle=\"yes\">y<sub>i</sub></italic> denote the labeled value and predicted value. <italic toggle=\"yes\">&#945;</italic> and <italic toggle=\"yes\">&#946;</italic> are weighting coefficients that balance the contributions of the two loss terms.</p></sec><sec><title>Dataset description</title><p>To comprehensively assess the generalizability of MFA-Net, we conducted extensive experiments on four benchmark datasets: thyroid nodule 3493 (TN3K), thyroid gland 3583 (TG3K), digital database thyroid image (DDTI) (<xref rid=\"r29\" ref-type=\"bibr\">29</xref>) and BrainTumor (<xref rid=\"r30\" ref-type=\"bibr\">30</xref>). The detailed characteristics of these datasets are summarized in <xref rid=\"t1\" ref-type=\"table\"><italic toggle=\"yes\">Table 1</italic></xref>.</p><table-wrap position=\"float\" id=\"t1\" orientation=\"portrait\"><label>Table 1</label><caption><title>Detailed introduction of three thyroid nodule and one BrainTumor datasets</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"20.03%\" span=\"1\"/><col width=\"20%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dataset</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Number</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Train</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Validate</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Test</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">TN3K</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3,493</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2,160</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">719</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">614</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">TG3K</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3,583</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2,152</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">717</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">716</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DDTI</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">637</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">383</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">127</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">127</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BrainTumor</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3,064</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1,839</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">613</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">612</td></tr></tbody></table><table-wrap-foot><p>DDTI, digital database thyroid image; TG3K, thyroid gland 3583 dataset; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap><p>The TN3K dataset comprises 3,493 ultrasound images, each containing at least one thyroid nodule region. The images in TN3K are carefully curated to include diverse nodule shapes, sizes, and locations, providing a robust foundation for assessing segmentation performance across varying anatomical structures. The TG3K dataset consists of 3,583 ultrasound images, where the thyroid gland region has been precisely segmented from ultrasound video sequences. To ensure data quality and relevance, only images where the thyroid gland occupies at least 6% of the total image area are considered. The DDTI dataset contains 637 ultrasound images of the thyroid nodule, each annotated with pixel-level segmentation masks obtained from a single ultrasound imaging device. The BrainTumor dataset provides a well-structured resource tailored for advancing brain tumor segmentation research. It includes a total of 3,064 magnetic resonance imaging (MRI) brain scans, each precisely aligned with a manually annotated binary mask that delineates the tumor regions.</p></sec><sec><title>Experimental scheme</title><p>All experiments were performed utilizing the PyTorch framework and executed on a high-performance NVIDIA RTX A6000 GPU. To optimize the model, we employed the Adam optimizer, initializing its learning rate at 1&#215;10<sup>&#8722;3</sup> to facilitate stable and efficient convergence. During the training phase, we configured the batch size to 32 and the number of iterations to 200. To ensure consistency across the dataset, all input ultrasound images were resized to 256&#215;256 pixels before being fed into the network.</p><p>To thoroughly evaluate the effectiveness of MFA-Net and ensure a fair comparison with some well-established segmentation methods, we employ Dice (<xref rid=\"r31\" ref-type=\"bibr\">31</xref>,<xref rid=\"r32\" ref-type=\"bibr\">32</xref>), intersection over union (IoU) (<xref rid=\"r33\" ref-type=\"bibr\">33</xref>,<xref rid=\"r34\" ref-type=\"bibr\">34</xref>), accuracy (<xref rid=\"r35\" ref-type=\"bibr\">35</xref>,<xref rid=\"r36\" ref-type=\"bibr\">36</xref>) and Matthews correlation coefficient (Mcc) (<xref rid=\"r37\" ref-type=\"bibr\">37</xref>,<xref rid=\"r38\" ref-type=\"bibr\">38</xref>) as the performance metrics. The mathematical formulations of these metrics are as follows:</p><disp-formula id=\"e3\"><mml:math id=\"m4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mtext>&#8202;</mml:mtext></mml:mrow></mml:math><label>[3]</label></disp-formula><disp-formula id=\"e4\"><mml:math id=\"m5\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mtext>+</mml:mtext><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[4]</label></disp-formula><disp-formula id=\"e5\"><mml:math id=\"m6\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mtext>+</mml:mtext><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[5]</label></disp-formula><disp-formula id=\"e6\"><mml:math id=\"m7\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[6]</label></disp-formula></sec></sec><sec sec-type=\"results\"><title>Results</title><sec><title>Experimental results on TN3K dataset</title><p>To evaluate the performance of MFA-Net, a series of comparative experiments was conducted on the TN3K dataset, with the results summarized in <xref rid=\"t2\" ref-type=\"table\"><italic toggle=\"yes\">Table 2</italic></xref>. These methods include U-Net (<xref rid=\"r3\" ref-type=\"bibr\">3</xref>), BSNet (<xref rid=\"r39\" ref-type=\"bibr\">39</xref>), MDA-Net (<xref rid=\"r40\" ref-type=\"bibr\">40</xref>), HFENet (<xref rid=\"r41\" ref-type=\"bibr\">41</xref>), MSFCN (<xref rid=\"r42\" ref-type=\"bibr\">42</xref>), ERDUnet (<xref rid=\"r43\" ref-type=\"bibr\">43</xref>), LANet (<xref rid=\"r44\" ref-type=\"bibr\">44</xref>), AMSUnet (<xref rid=\"r45\" ref-type=\"bibr\">45</xref>), DESENet (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>), BMANet (<xref rid=\"r5\" ref-type=\"bibr\">5</xref>) and NLIE-UNet (<xref rid=\"r6\" ref-type=\"bibr\">6</xref>). Among the compared methods, MSFCN and U-Net exhibit the lowest performance, with Dice of 0.7113 and 0.7151, IoU of 0.5560 and 0.5649, accuracy of 0.9358 and 0.0399, Mcc of 0.6762 and 0.6823. Despite their foundational role in segmentation tasks, both models struggle with complex nodule structures and often produce coarse predictions. ERDUnet and HFENet show moderately improved results, achieving Dice of 0.8014 and 0.8041, IoU of 0.6719 and 0.6758, accuracy of 0.9558 and 0.9550, Mcc of 0.7765 and 0.7802. However, they still face challenges in accurately delineating nodular boundaries, especially in low-contrast or noisy regions. LANet, AMSUnet, BMANet and NLIE-UNet achieve Dice scores between 0.805 and 0.82, reflecting their better balance between detail preservation and structural consistency, though some fine-grained errors persist. Further enhancements are observed in BSNet, MDA-Net, and DESENet, which obtain Dice scores of 0.8377, 0.8409, and 0.8365, and Mcc values above 0.81, demonstrating stronger feature extraction ability and improved robustness in boundary prediction. However, there are still minor errors in complex regions, which slightly affect the overall consistency. Among all evaluated models, the proposed MFA-Net achieves the highest performance, with Dice of 0.8616, IoU of 0.7586, accuracy of 0.9698 and Mcc of 0.8457. By capturing both global and local dependencies, MFA-Net excels in delineating fine structures and maintaining accurate contours, particularly in challenging cases involving small or irregular nodules. <xref rid=\"f8\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 8</italic></xref> provides qualitative segmentation results produced by various approaches on the TN3K dataset. These qualitative observations strongly support the quantitative findings, confirming that MFA-Net offers superior robustness and precision in clinical ultrasound-based thyroid segmentation.</p><table-wrap position=\"float\" id=\"t2\" orientation=\"portrait\"><label>Table 2</label><caption><title>Comparison experiments of various models on the TN3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"20.03%\" span=\"1\"/><col width=\"20%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7151</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5649</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9399</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6823</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BSNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8377</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7226</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9648</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8196</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MDA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8409</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7275</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9649</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8216</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">HFENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8041</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6758</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9550</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7802</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSFCN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7113</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5560</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9358</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6762</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ERDUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8014</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6719</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9558</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7765</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8163</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6919</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9601</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7953</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">AMSUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8053</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6787</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9589</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7828</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DESENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8365</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7219</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9642</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8175</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BMANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8091</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6833</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9587</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7879</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">NLIE-UNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8054</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6774</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9563</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7817</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8618</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7586</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9698</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8457</td></tr></tbody></table><table-wrap-foot><p>IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f8\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Comparison experiments of various models on the TN3K dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f8.jpg\"/></fig></sec><sec><title>Experimental results on TG3K dataset</title><p><xref rid=\"t3\" ref-type=\"table\"><italic toggle=\"yes\">Table 3</italic></xref> displays the quantitative evaluation of several models on the TG3K dataset, which features thyroid nodules with well-defined lesions and relatively consistent shapes and anatomical positions. These characteristics contribute to the high segmentation accuracy achieved by most models. Specifically, with the exception of U-Net, all methods reported Dice coefficients exceeding 97%, IoU scores above 94%, and Mcc are higher than 94%, reflecting the dataset&#8217;s lower complexity compared to more heterogeneous collections. Despite this overall strong performance, U-Net significantly underperforms, yielding a Dice of 78.63% and IoU of 65.23%, suggesting limited capability in handling even moderately variable nodule presentations. The visual results in <xref rid=\"f9\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 9</italic></xref> further illustrate this point, where U-Net displays a large number of erroneous segmentations; it is unable to capture the complete structure of the nodules or introduce false positives in the surrounding area. In contrast, the differences between other models are very small. However, among all the approaches, the proposed MFA-Net achieves the highest performance, with Dice of 98.57%, IoU of 97.18%, accuracy of 99.77% and Mcc of 98.44%. This superior outcome highlights MFA-Net&#8217;s exceptional ability to capture fine-grained boundaries and preserve spatial consistency, even in datasets where lesions may appear visually subtle or homogeneous.</p><table-wrap position=\"float\" id=\"t3\" orientation=\"portrait\"><label>Table 3</label><caption><title>Comparison experiments of various models on the TG3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"20.03%\" span=\"1\"/><col width=\"20%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7863</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6523</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9602</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7711</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BSNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9820</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9647</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9972</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9805</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MDA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9821</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9650</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9972</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9806</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">HFENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9792</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9593</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9967</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9774</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSFCN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9785</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9581</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9966</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9767</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ERDUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9714</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9445</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9955</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9691</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9823</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9653</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9973</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9808</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">AMSUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9778</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9567</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9966</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9759</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DESENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9845</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9695</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9976</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9832</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BMANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9836</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9677</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9975</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9822</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">NLIE-UNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9736</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9487</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9959</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9713</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9857</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9718</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9977</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9844</td></tr></tbody></table><table-wrap-foot><p>IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; TG3K, thyroid gland 3583 dataset.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f9\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Comparison experiments of various models on the TG3K dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network; TG3K, thyroid gland 3583 dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f9.jpg\"/></fig></sec><sec><title>Experimental results on DDTI dataset</title><p>Drawing upon the quantitative metrics detailed in <xref rid=\"t4\" ref-type=\"table\"><italic toggle=\"yes\">Table 4</italic></xref> and the visual outcomes illustrated in <xref rid=\"f10\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 10</italic></xref>, we can comprehensively evaluate and contrast the performance of various models on the DDTI dataset. Due to the higher variability in image quality and nodule appearance of the DDTI dataset, all models have lower Dice and IoU scores compared to TG3K or TN3K. Specifically, MSFCN demonstrates the lowest performance among all approaches, with Dice of 61.65%, IoU of 44.68%, accuracy of 88.36% and IoU of 55.08%. These results highlight MSFCN&#8217;s limitations in capturing fine boundary details in more complex scenarios. U-Net also underperforms, with Dice of 62.49%, IoU of 45.47%, accuracy of 87.02% and Mcc of 55.70%, suggesting insufficient capacity for modeling the intricate and irregular structures commonly found in DDTI. HFENet, AMSUnet and NLIE-UNet show moderate results, but still struggle to achieve satisfactory segmentation precision, particularly in edge delineation. MDA-Net, BSNet, LANet, DESENet and BMANet perform comparably, with Dice scores in the 70-73% range, and IoU values between 55% and 58%, indicating relatively stable but not optimal performance. ERDUnet stands out slightly from the above group, achieving Dice of 74.10%, IoU of 58.94%, accuracy of 92.56% and Mcc of 69.81%, suggesting better generalization to the heterogeneous features present in the dataset. However, the proposed method achieves the best quantitative performance, with Dice of 74.83%, IoU of 59.81%, accuracy of 92.83% and Mcc of 70.78%. The combined quantitative and qualitative analyses affirm that MFA-Net outperforms other state-of-the-art models in handling the challenging characteristics of the DDTI dataset. This indicates its superior ability to handle low-contrast and irregularly shaped nodules, thanks to the multi-scale and attention-guided mechanisms embedded in the architecture.</p><table-wrap position=\"float\" id=\"t4\" orientation=\"portrait\"><label>Table 4</label><caption><title>Comparison experiments of various models on the DDTI dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"20.03%\" span=\"1\"/><col width=\"20%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6249</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.4547</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8702</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5570</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BSNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7346</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5810</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9169</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6874</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MDA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7255</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5696</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9135</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6765</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">HFENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6586</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.4917</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8929</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5972</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSFCN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6165</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.4468</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8836</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5508</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ERDUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7410</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5894</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9256</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6981</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7097</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5507</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9085</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6576</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">AMSUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6844</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5210</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8971</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6275</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DESENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7113</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5527</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9160</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6624</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BMANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7282</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5736</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9175</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6801</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">NLIE-UNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6756</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5115</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9025</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6193</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7483</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5981</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9283</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7078</td></tr></tbody></table><table-wrap-foot><p>DDTI, digital database thyroid image; IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f10\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Comparison experiments of various models on the DDTI dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. DDTI, digital database thyroid image; MFA-Net, multi-scale feature aggregation network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f10.jpg\"/></fig></sec><sec><title>Experimental results on BrainTumor dataset</title><p>Drawing upon the quantitative metrics reported in <xref rid=\"t5\" ref-type=\"table\"><italic toggle=\"yes\">Table 5</italic></xref> and the qualitative visualizations presented in <xref rid=\"f11\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 11</italic></xref>, the performance of different segmentation models on the BrainTumor dataset can be thoroughly evaluated. Among them, U-Net provides the baseline with relatively modest Dice (77.55%) and IoU (64.85%), highlighting its limited ability to capture complex tumor structures. AMSUnet, MSFCN, and ERDUnet also demonstrate lower accuracy in boundary delineation, with Dice scores below 80%, indicating difficulties in segmenting irregular or small tumor regions. AMSUnet, MSFCN, and ERDUnet also demonstrate lower accuracy in boundary delineation, with Dice scores below 80%, indicating difficulties in segmenting irregular or small tumor regions. HFENet, LANet, and NLIE-UNet achieve slightly better results, with Dice around 80% and Mcc values close to 0.80, reflecting moderate improvements in segmentation consistency but still failing to handle subtle tumor boundaries effectively. In contrast, BSNet, MDA-Net, DESENet, and BMANet deliver more competitive outcomes, with Dice ranging from 81% to 82% and IoU values above 69%. The visual outcomes also suggest that these approaches yield more coherent tumor contours compared to the baseline U-Net. Notably, MFA-Net surpasses all competing methods, achieving the best performance with Dice of 84.85%, IoU of 74.21%, accuracy of 99.49%, and Mcc of 84.69%. Both the quantitative and qualitative evaluations indicate that MFA-Net consistently produces more accurate and complete tumor boundaries, outperforming state-of-the-art methods.</p><table-wrap position=\"float\" id=\"t5\" orientation=\"portrait\"><label>Table 5</label><caption><title>Comparison experiments of various models on the BrainTumor dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"20.03%\" span=\"1\"/><col width=\"20%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><col width=\"19.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7755</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6485</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9934</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7873</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BSNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8243</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7071</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9943</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8233</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MDA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8245</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7078</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9938</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8226</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">HFENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8023</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6756</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9933</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7997</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSFCN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7922</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6633</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9932</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7912</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ERDUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7934</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6662</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9933</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7934</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8009</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6759</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9936</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8001</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">AMSUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7843</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6543</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9928</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7836</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DESENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8154</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6944</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9939</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8141</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BMANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8093</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6847</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9935</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8064</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">NLIE-UNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8062</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6804</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9934</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8038</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8485</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7421</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9949</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8469</td></tr></tbody></table><table-wrap-foot><p>IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f11\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 11</label><caption><p>Comparison experiments of various models on the BrainTumor dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f11.jpg\"/></fig></sec></sec><sec sec-type=\"discussion\"><title>Discussion</title><sec><title>Ablative studies</title><p>To thoroughly evaluate the performance benefits and individual contributions of each core module in the proposed MFA-Net architecture, we carried out detailed ablation studies using the TN3K dataset. In this evaluation, the MFAM, background-aware mechanism and RDM were independently incorporated into the baseline architecture. Each component was evaluated in isolation to quantify its specific effects on segmentation accuracy and robustness. Finally, all modules were combined to form the complete MFA-Net. The corresponding quantitative results and qualitative visualizations are presented in <xref rid=\"t6\" ref-type=\"table\"><italic toggle=\"yes\">Table 6</italic></xref> and <xref rid=\"f12\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 12</italic></xref>, respectively.</p><table-wrap position=\"float\" id=\"t6\" orientation=\"portrait\"><label>Table 6</label><caption><title>Ablative studies results on the TN3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"26.63%\" span=\"1\"/><col width=\"11.57%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><col width=\"10.3%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Param (M)</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FPS</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">GFLOPs</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7151</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.5649</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9399</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6823</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.94</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">227.29</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.48</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + MFAM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8431</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7312</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9651</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8237</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.80</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">138.70</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.84</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + BAM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8410</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7273</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9655</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8225</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">139.47</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">7.64</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + RDM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8476</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7380</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9672</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8301</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.55</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">101.23</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.07</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + MFAM + BAM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8321</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7158</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9646</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8137</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.00</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">103.33</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8.56</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + MFAM + RDM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8584</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7536</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9683</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8413</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.85</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.50</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.99</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Baseline + BAM + RDM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8536</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7459</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9671</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8356</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.77</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.43</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">7.86</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8618</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7586</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9698</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8457</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.07</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">67.46</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8.78</td></tr></tbody></table><table-wrap-foot><p>BAM, background-aware module; IoU, intersection over union; FPS, frame-per-second; GFLOPs, giga floating-point operations per second; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; MFAM, multi-scale feature aggregation module; RDM, residual decoder module; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f12\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 12</label><caption><p>Ablative studies result on the TN3K dataset. (A) Original image. (B) Mask. (C) Baseline. (D) Baseline + MFAM. (E) Baseline + BAM. (F) Baseline + RDM. (G) Baseline + MFAM + BAM. (H) Baseline + MFAM + RDM. (I) Baseline + BAM + RDM. (J) MFA-Net. BAM, background-aware module; MFAM, multi-scale feature aggregation module; RDM, residual decoder module; TN3K, thyroid nodule 3493 dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-12167-f12.jpg\"/></fig><p>To validate the impact of the MFAM within the proposed MFA-Net architecture, we conducted focused ablation experiments by integrating MFAM independently into the baseline. As presented in <xref rid=\"t6\" ref-type=\"table\"><italic toggle=\"yes\">Table 6</italic></xref> and illustrated in <xref rid=\"f12\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 12</italic></xref>, the addition of MFAM resulted in a marked increase in segmentation accuracy when compared to the baseline configuration. Specifically, the Dice increased from 71.51% to 84.31%, the IoU rose from 56.49% to 73.12%, the accuracy improved from 93.99% to 96.51%, and the Mcc increased from 68.23% to 82.37%. These enhancements clearly indicate MFAM&#8217;s effectiveness in enriching the feature representation by leveraging multi-scale contextual cues. By combining detailed local textures with broader structural information, MFAM strengthens the model&#8217;s ability to delineate thyroid nodule boundaries with higher accuracy, especially in cases where lesion contours are subtle or irregular. From a computational perspective, the inclusion of MFAM leads to a moderate increase in model complexity: the number of parameters rises from 1.94 to 2.80 M, and the computational cost grows from 3.48 to 4.84 giga floating-point operations per second (GFLOPs). Meanwhile, the frame-per-second (FPS) rate decreases from 227.29 to 138.70, which remains acceptable for real-time or near-real-time clinical applications. Qualitatively, visualizations in <xref rid=\"f12\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 12</italic></xref> demonstrate that the MFAM produces more complete and accurate nodule masks, especially in complex scenarios with low contrast or fragmented edges. The segmentation predictions produced after incorporating the MFAM exhibit a markedly higher consistency with the ground truth masks than those generated by the baseline, confirming that MFAM enhances the model&#8217;s capacity to identify nodules with varying scales and textures.</p><p>In terms of quantitative performance, the integration of BAM into the baseline network leads to a significant improvement in segmentation accuracy. Specifically, the Dice increases from 71.51% to 84.10%, while the IoU rises from 56.49% to 72.73%. In addition, the accuracy improves from 93.99% to 96.55%, and the Mcc increases from 68.23% to 82.25%. The visualization results show that BAM&#8217;s capacity to guide the model&#8217;s attention more effectively toward meaningful foreground features while diminishing the influence of irrelevant background content. From a computational perspective, the addition of BAM increases the model&#8217;s parameter count from 1.94 to 3.71 M, with the frame rate dropping from 227.29 to 139.47 FPS, while the computational complexity rises from 3.48 to 7.64 GFLOPs. Although its speed is slower than the base version, its performance still falls within the feasible range suitable for practical clinical applications.</p><p>Integrating RDM into the baseline network architecture can significantly enhance the segmentation performance. Quantitatively, the Dice improves from 71.51% to 84.76%, while the IoU increases from 56.49% to 73.80%. Moreover, the accuracy rises from 93.99% to 96.72%, and the Mcc improves from 68.23% to 83.01%. From a qualitative perspective, the generated predictions demonstrate improved alignment with ground truth boundaries, especially in regions characterized by subtle textures or complex structural variations. In terms of computational implications, the introduction of RDM leads to a moderate increase in model complexity, expanding the parameter count from 1.94 to 2.55 M and GFLOPs increasing from 3.48 to 4.07. The additional computational cost is reflected in a decrease in processing speed, with the inference frame rate dropping from 227.29 FPS to 101.23 FPS.</p><p>When multiple modules are integrated, the performance of the network improves further through their complementary strengths. As shown in <xref rid=\"t6\" ref-type=\"table\"><italic toggle=\"yes\">Table 6</italic></xref>, the combination of MFAM and BAM leads to clear improvements over the baseline, with the Dice increasing from 71.51% to 83.21%, IoU from 56.49% to 71.58%, accuracy from 93.99% to 96.46%, and Mcc from 68.23% to 81.37%. Although this dual-module configuration raises the parameter count to 4.00 M and increases GFLOPs to 8.56, the achieved segmentation improvements demonstrate that MFAM and BAM cooperate effectively by capturing multi-scale contextual cues and refining spatial attention. Similarly, the integration of MFAM and RDM produces the most notable gains among the dual-module settings. In this case, the Dice reaches 85.84%, IoU rises to 75.36%, accuracy improves to 96.83%, and Mcc increases to 84.13%, all of which represent significant advances compared to the baseline. The parameter count moderately grows to 2.85 M, GFLOPs to 4.99, and FPS drops to 86.50. The combination of BAM and RDM also yields strong results, with Dice improving to 85.36%, IoU to 74.59%, accuracy to 96.71%, and Mcc to 83.56%. Although this configuration has a higher parameter load of 3.77M and computational cost of 7.86 GFLOPs, the improvements in feature refinement and boundary delineation make it particularly effective for complex cases with heterogeneous nodule appearances. Finally, when all three modules are jointly integrated into the baseline, the proposed MFA-Net exhibits the most outstanding overall performance in both quantitative metrics and qualitative visual outcomes. This comprehensive configuration leverages the individual strengths of each component, leading to a synergistic enhancement in segmentation capability. In terms of model complexity, the parameter count increases to 4.07 M, GFLOPs to 8.78, and FPS decreases from 227.29 to 67.46, reflecting the added architectural complexity. Despite these increases, the performance gains achieved justify the trade-off. The network remains lightweight enough for practical use, especially in scenarios where segmentation accuracy is critical and computational resources are moderately available.</p></sec><sec><title>Selection experiment of loss function</title><p>To identify the most effective loss function for enhancing segmentation performance in the context of thyroid nodule detection, we conducted a comparative evaluation of six widely used loss functions and the results are shown in <xref rid=\"t7\" ref-type=\"table\"><italic toggle=\"yes\">Table 7</italic></xref>. Each loss function was integrated into MFA-Net under identical training settings, ensuring a fair comparison based solely on their individual optimization capabilities. Specifically, BCE loss achieved Dice of 83.39%, IoU of 71.73%, accuracy of 96.31%, and Mcc of 81.37%. While it provides a solid baseline, its performance is limited in capturing fine-grained regions due to the lack of direct overlap optimization. Dice loss improved all metrics, achieving Dice of 85.88%, IoU of 75.36%, accuracy of 96.84%, and Mcc of 84.14%. This demonstrates its strong ability to directly maximize overlap between predicted masks and ground truth, making it particularly effective for handling class imbalance. IoU loss focused on optimizing the intersection-over-union metric, but its overall performance was lower (Dice 77.59%, IoU 63.76%, accuracy 94.07%, Mcc 75.74%). This indicates that although optimizing IoU is useful, during the training process, it may not be as stable as the Dice loss. The Tversky loss function (we set 0.3 and 0.7 for the hyperparameter) aims to balance the situations of false positives and false negatives, and its results are quite satisfactory (with a Dice value of 79.82%, an IoU value of 66.73%, an accuracy rate of 94.79%, and a Mcc value of 78.02%). This indicates that although it can solve the problem of class imbalance, its overall segmentation performance is slightly lower than that of the Dice loss function. Focal (weighted 0.1) + Tversky (weighted 0.9) loss, which emphasizes difficult-to-segment regions, resulted in Dice of 78.22%, IoU of 64.51%, accuracy of 94.12%, and Mcc of 76.48%. Although this combination is theoretically advantageous for hard examples, it underperformed compared to simpler Dice-based losses in our experiments. BCE (weighted 0.5) + Dice (weighted 0.5) loss achieved the highest overall performance (Dice 86.18%, IoU 75.86%, accuracy 96.98%, Mcc 84.57%), slightly outperforming Dice loss alone. This combination benefits from both pixel-wise probability optimization (BCE) and overlap maximization (Dice), resulting in more balanced segmentation performance across all metrics.</p><table-wrap position=\"float\" id=\"t7\" orientation=\"portrait\"><label>Table 7</label><caption><title>Selection experiment of loss function on the TN3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"28.04%\" span=\"1\"/><col width=\"17.99%\" span=\"1\"/><col width=\"17.99%\" span=\"1\"/><col width=\"17.99%\" span=\"1\"/><col width=\"17.99%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Loss function</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BCE</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8339</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7173</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9631</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8137</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Dice</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8588</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7536</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9684</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8414</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">IoU</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7759</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6376</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9407</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7574</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Tversky</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7982</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6673</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9479</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7802</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Focal (0.1) + Tversky (0.9)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7822</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6451</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9412</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7648</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BCE (0.5) + Dice (0.5)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8618</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7586</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9698</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8457</td></tr></tbody></table><table-wrap-foot><p>BCE, binary cross-entropy; IoU, intersection over union; Mcc, Matthews correlation coefficient; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap></sec><sec><title>The effects of kernel sizes on the MFAM</title><p><xref rid=\"t8\" ref-type=\"table\"><italic toggle=\"yes\">Table 8</italic></xref> presents the results of our experiments on the MFAM in the MFA-Net architecture, where we evaluate the performance of various kernel size configurations. Specifically, we compared the use of horizontal and vertical convolutions to standard convolutions across different kernel combinations: 3+5+7, 3+5+9, 3+5+11, 5+7+9, 5+7+11, and 7+9+11. Our analysis shows that the horizontal and vertical convolution configurations outperformed the standard convolution configurations in most cases (except for 7+9+11), and they perform better across all evaluated metrics. Notably, the combination of 5+7+11 kernels yielded the best performance across all configurations, achieving Dice of 0.8618, IoU of 0.7586, accuracy of 96.98%, and Mcc of 0.8457. This configuration showed a marked improvement over the standard convolution configurations, where the best result (from the 7+9+11 kernel size) yielded Dice of 0.8581, IoU of 0.7530, accuracy of 96.78%, and Mcc of 0.8405. In conclusion, the horizontal and vertical convolution kernel configurations, particularly with kernel sizes of 5+7+11, are more effective for improving the segmentation performance of the MFA-Net model, as compared to traditional standard convolution configurations.</p><table-wrap position=\"float\" id=\"t8\" orientation=\"portrait\"><label>Table 8</label><caption><title>The effects of kernel sizes on the TN3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"26.1%\" span=\"1\"/><col width=\"14.78%\" span=\"1\"/><col width=\"14.78%\" span=\"1\"/><col width=\"14.78%\" span=\"1\"/><col width=\"14.78%\" span=\"1\"/><col width=\"14.78%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Convolution type</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Kernel size</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">IoU</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Accuracy</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mcc</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+7</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8511</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7427</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9664</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8327</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8525</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7445</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9672</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8352</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Standard convolution</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8541</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7472</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9676</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8367</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5+7+9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8502</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7415</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9662</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8316</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5+7+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8497</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7409</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9667</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8320</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">7+9+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8581</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7530</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9678</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8405</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+7</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8574</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7521</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9676</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8397</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8553</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7488</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9681</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8382</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Horizontal and vertical</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3+5+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8579</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7528</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9685</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8408</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5+7+9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8552</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7484</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9682</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8387</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5+7+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8618</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7586</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9698</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8457</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">7+9+11</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8543</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7476</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9677</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8367</td></tr></tbody></table><table-wrap-foot><p>IoU, intersection over union; Mcc, Matthews correlation coefficient; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap></sec><sec><title>Computational efficiency</title><p><xref rid=\"t9\" ref-type=\"table\"><italic toggle=\"yes\">Table 9</italic></xref> presents a comprehensive comparison of model complexity (in terms of parameter count), computational efficiency (measured by FPS), and computational load (measured by GFLOPs) across various thyroid nodule segmentation methods on the TN3K dataset. Among the models evaluated, HFENet stands out with the lowest number of parameters (only 0.15 million), the highest FPS of 233.49, and a minimal computational load of 1.47 GFLOPs, highlighting its exceptional efficiency for real-time applications. U-Net is one of the lightest architectures in this comparison, but compared with the newer models, it often lags behind in terms of segmentation accuracy. It has 1.94 million parameters, 227.29 FPS, and a 3.48 GFLOPs computational load. AMSUnet is another efficient model with a modest parameter size of 2.61 million and 84.18 FPS, offering a slightly slower inference speed than U-Net. It has 6.12 GFLOPs, reflecting a moderate computational burden. MSFCN has 14.17 million parameters, an FPS of 129.21, and a high GFLOPs of 55.80, indicating a high computational load for high-accuracy segmentation but a well-balanced design in terms of model complexity and inference speed. LANet features 23.79 million parameters, positioning it on the higher end in terms of model size. Despite this, it maintains a relatively high FPS of 121.72 and a moderate GFLOPs of 8.30, demonstrating efficient architectural optimization that allows for competitive processing speeds even with increased complexity. ERDUnet carries 10.21 million parameters, but its FPS drops to 39.59, and its GFLOPs of 10.29 indicate slower processing efficiency, making it more computationally intensive despite its moderate parameter count. MDA-Net has a significantly larger footprint with 29.84 million parameters, and a moderate FPS of 57.95, accompanied by GFLOPs of 45.80, indicating its high computational load. BSNet possesses the largest parameter count at 43.98 million, making it the heaviest model among those compared. Its FPS is the lowest, at only 25.39, with a GFLOPs of 45.80, highlighting its computational intensity. MFA-Net has 4.07 million parameters and a frame processing rate of 67.46 per second. This indicates that its running speed is moderate, with 8.78 floating-point operations per second, which keeps it in a balanced state in terms of computational load.</p><table-wrap position=\"float\" id=\"t9\" orientation=\"portrait\"><label>Table 9</label><caption><title>Parameters and computational efficiency of various models on the TN3K dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"25.39%\" span=\"1\"/><col width=\"23.66%\" span=\"1\"/><col width=\"25.47%\" span=\"1\"/><col width=\"25.48%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"justify\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Param (M)</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FPS</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">GFLOPs</th></tr></thead><tbody><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">HFENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.15</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">233.49</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.47</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">DESENet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.15 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">45.35</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.17</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.94 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">227.29 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.48</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">AMSUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.61 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.18</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.12</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">NLIE-UNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">15.83</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.02</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MFA-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.07 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">67.46</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8.78</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ERDUnet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">10.21</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">39.59</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">10.29</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSFCN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">14.17</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">129.21</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">55.80</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">23.79 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">121.72</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8.30</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BSNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">43.98</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">25.39 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">45.80</td></tr><tr><td valign=\"top\" align=\"justify\" scope=\"row\" rowspan=\"1\" colspan=\"1\">BMANet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">28.40 </td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">33.43</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">9.19</td></tr></tbody></table><table-wrap-foot><p>FPS, frame-per-second; GFLOPs, giga floating-point operations per second; MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset.</p></table-wrap-foot></table-wrap></sec><sec><title>Limitations</title><p>Although the proposed MFA-Net achieves promising segmentation performance, several limitations remain. Firstly, our experiment was conducted on publicly available datasets, which may not fully encompass the diversity of real-world clinical data. Future work will involve collaborating with hospitals to validate our framework on larger and more diverse sample groups and using LLM-based annotation. Secondly, our current design is based on task-specific deep learning architectures, whereas recent advances in foundation models have demonstrated powerful generalization capabilities across tasks and modalities. Integrating MFA-Net with such pre-trained large-scale models may further enhance segmentation accuracy and reduce dependence on extensive labeled medical data. Thirdly, this study focuses primarily on single-modality imaging. In practice, integrating multimodal information, such as histology slides, genomic profiles, or clinical notes, could provide complementary insights that improve both segmentation precision and downstream clinical decision-making (<xref rid=\"r46\" ref-type=\"bibr\">46</xref>).</p></sec></sec><sec sec-type=\"conclusions\"><title>Conclusions</title><p>In this research, we introduce MFA-Net, an advanced deep learning framework meticulously designed for accurate and efficient segmentation of thyroid nodules. MFA-Net integrates three key components: MFAM, background-aware mechanism, and RDM. The MFAM enhances the network&#8217;s capacity to obtain fine details and contextual information across varying receptive fields, while BAM effectively suppresses background noise, guiding the network&#8217;s attention toward critical nodule regions. The RDM further strengthens decoding by preserving semantic continuity and refining boundary precision. Extensive experiments conducted on three datasets demonstrate that MFA-Net consistently surpasses a wide range of leading methods in terms of Dice coefficient and IoU value. Ablation studies further validate the individual contributions of each component, confirming their synergistic impact on performance enhancement. Additionally, MFA-Net achieves a favorable trade-off between accuracy and computational efficiency, making it a robust and practical solution for clinical applications. These findings affirm that MFA-Net delivers precise, stable, and scalable segmentation results, with strong generalization capability across diverse thyroid nodule patterns and imaging conditions.</p></sec><sec sec-type=\"supplementary-material\"><title>Supplementary</title><supplementary-material position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><p>The article&#8217;s supplementary files as</p></supplementary-material><supplementary-material position=\"anchor\" id=\"su1\" content-type=\"local-data\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qims-15-12-12167-rc.pdf\" id=\"d67e2369\" position=\"anchor\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.21037/qims-2025-1364</object-id></media><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qims-15-12-12167-coif.pdf\" id=\"d67e2372\" position=\"anchor\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.21037/qims-2025-1364</object-id></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>We would like to sincerely thank our research collaborators Simon Fong and Yaoyang Wu from University of Macau for their assistance in data collection and the language editing of this manuscript.</p></ack><fn-group><fn id=\"fn1\"><p><italic toggle=\"yes\">Reporting Checklist:</italic> The authors have completed the CLEAR reporting checklist. Available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc</ext-link></p></fn><fn fn-type=\"financial-disclosure\"><p><italic toggle=\"yes\">Funding: </italic>This work was supported by <funding-source rid=\"sp1\">National Natural Science Foundation of China</funding-source> (<award-id rid=\"sp1\">No. 62102227</award-id>) and <funding-source rid=\"sp2\">Joint Fund of Zhejiang Provincial Natural Science Foundation of China</funding-source> (<award-id rid=\"sp2\">No. LZY24E060001</award-id>).</p></fn><fn fn-type=\"COI-statement\"><p><italic toggle=\"yes\">Conflicts of Interest:</italic> All authors have completed the ICMJE uniform disclosure form (available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/coif\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/coif</ext-link>). The authors have no conflicts of interest to declare.</p></fn></fn-group><notes><p content-type=\"note added in proof\"><italic toggle=\"yes\">Ethical Statement: </italic>The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments.</p></notes><ref-list><title>References</title><ref id=\"r1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>C</given-names></name></person-group>. <article-title>Diagnosis of thyroid nodules using ultrasound images based on deep learning features: online dynamic nomogram and gradient-weighted class activation mapping.</article-title><source>Quant Imaging Med Surg</source><year>2025</year>;<volume>15</volume>:<fpage>5689</fpage>-<lpage>702</lpage>. <pub-id pub-id-type=\"doi\">10.21037/qims-2025-159</pub-id><pub-id pub-id-type=\"pmid\">40606380</pub-id><pub-id pub-id-type=\"pmcid\">PMC12209616</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>QG</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Deng</surname><given-names>GX</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>HQ</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>JJ</given-names></name></person-group>. <article-title>Development and validation of a nomogram based on conventional and contrast-enhanced ultrasound for differentiating malignant from benign thyroid nodules.</article-title><source>Quant Imaging Med Surg</source><year>2025</year>;<volume>15</volume>:<fpage>4641</fpage>-<lpage>54</lpage>. <pub-id pub-id-type=\"doi\">10.21037/qims-24-1796</pub-id><pub-id pub-id-type=\"pmid\">40384666</pub-id><pub-id pub-id-type=\"pmcid\">PMC12082573</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3</label><mixed-citation publication-type=\"book\">Ronneberger O, Fischer P, Brox T, editors. U-Net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells W, Frangi A, editors. Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015. Lecture Notes in Computer Science. Cham: Springer; 2015.</mixed-citation></ref><ref id=\"r4\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Min</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>DESENet: A bilateral network with detail-enhanced semantic encoder for real-time semantic segmentation.</article-title><source>Meas Sci Technol</source><year>2025</year>;<volume>36</volume>:<elocation-id>015425</elocation-id>.</mixed-citation></ref><ref id=\"r5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><article-title>BMANet: Boundary-guided multi-level attention network for polyp segmentation in colonoscopy images.</article-title><source>Biomed Signal Process Control</source><year>2025</year>;<volume>105</volume>:<elocation-id>107524</elocation-id>.</mixed-citation></ref><ref id=\"r6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wan</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Kang</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name></person-group><article-title>Dynamic neighbourhood-enhanced UNet with interwoven fusion for medical image segmentation.</article-title><source>Vis Comput</source><year>2025</year>;<volume>41</volume>:<fpage>7703</fpage>-<lpage>21</lpage>.</mixed-citation></ref><ref id=\"r7\"><label>7</label><mixed-citation publication-type=\"other\">Wu Y, Huang L, Yang T. Thyroid nodule ultrasound image segmentation based on improved Swin Transformer. IEEE Access 2025;13:19788-95.</mixed-citation></ref><ref id=\"r8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>T.</given-names></name></person-group><article-title>GSE-Nets: Global structure enhancement decoder for thyroid nodule segmentation.</article-title><source>Biomed Signal Process Control</source><year>2025</year>;<volume>102</volume>:<elocation-id>107340</elocation-id>.</mixed-citation></ref><ref id=\"r9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Lv</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>Mamba- and ResNet-Based Dual-Branch Network for Ultrasound Thyroid Nodule Segmentation.</article-title><source>Bioengineering (Basel)</source><year>2024</year>;<volume>11</volume>:<fpage>1047</fpage>. <pub-id pub-id-type=\"doi\">10.3390/bioengineering11101047</pub-id><pub-id pub-id-type=\"pmid\">39451422</pub-id><pub-id pub-id-type=\"pmcid\">PMC11504408</pub-id></mixed-citation></ref><ref id=\"r10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ozcan</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Tosun</surname><given-names>&#214;</given-names></name><name name-style=\"western\"><surname>Donmez</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Sanwal</surname><given-names>M.</given-names></name></person-group><article-title>Enhanced-TransUNet for ultrasound segmentation of thyroid nodules.</article-title><source>Biomed Signal Process Control</source><year>2024</year>;<volume>95</volume>:<elocation-id>106472</elocation-id>.</mixed-citation></ref><ref id=\"r11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ali</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>J.</given-names></name></person-group><article-title>Cil-net: Densely connected context information learning network for boosting thyroid nodule segmentation using ultrasound images.</article-title><source>Cogn Comput</source><year>2024</year>;<volume>16</volume>:<fpage>1176</fpage>-<lpage>97</lpage>.</mixed-citation></ref><ref id=\"r12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zheng</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Weng</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Chai</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Bu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>T.</given-names></name></person-group><article-title>A segmentation-based algorithm for classification of benign and malignancy Thyroid nodules with multi-feature information.</article-title><source>Biomed Eng Lett</source><year>2024</year>;<volume>14</volume>:<fpage>785</fpage>-<lpage>800</lpage>. <pub-id pub-id-type=\"doi\">10.1007/s13534-024-00375-2</pub-id><pub-id pub-id-type=\"pmid\">38946824</pub-id><pub-id pub-id-type=\"pmcid\">PMC11208362</pub-id></mixed-citation></ref><ref id=\"r13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>YM</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>KM</given-names></name><name name-style=\"western\"><surname>To</surname><given-names>S</given-names></name></person-group>. <article-title>Atrous residual interconnected encoder to attention decoder framework for vertebrae segmentation via 3D volumetric CT images.</article-title><source>Eng Appl Artif Intell</source><year>2022</year>;<volume>114</volume>:<elocation-id>105102</elocation-id>.</mixed-citation></ref><ref id=\"r14\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ji</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Ge</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Chukwudi</surname><given-names>C</given-names></name></person-group>, <article-title>U K, Zhang SM, Peng Y, Zhu J, Zaki H, Zhang X, Yang S, Wang X, Chen Y, Zhao J. Counterfactual Bidirectional Co-Attention Transformer for Integrative Histology-Genomic Cancer Risk Stratification.</article-title><source>IEEE J Biomed Health Inform</source><year>2025</year>;<volume>29</volume>:<fpage>5862</fpage>-<lpage>74</lpage>. <pub-id pub-id-type=\"doi\">10.1109/JBHI.2025.3548048</pub-id><pub-id pub-id-type=\"pmid\">40042950</pub-id></mixed-citation></ref><ref id=\"r15\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>L</given-names></name></person-group>. <article-title>An agricultural leaf disease segmentation model applying multi-scale coordinate attention mechanism.</article-title><source>Appl Soft Comput</source><year>2025</year>;<volume>172</volume>:<elocation-id>112904</elocation-id>.</mixed-citation></ref><ref id=\"r16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ni</surname><given-names>JC</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>SH</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>YC</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>CS</given-names></name></person-group>. <article-title>Improved U-Net based on ResNet and SE-Net with dual attention mechanism for glottis semantic segmentation.</article-title><source>Med Eng Phys</source><year>2025</year>;<volume>136</volume>:<elocation-id>104298</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.medengphy.2025.104298</pub-id><pub-id pub-id-type=\"pmid\">39979012</pub-id></mixed-citation></ref><ref id=\"r17\"><label>17</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>PVT-MA: Pyramid vision transformers with multi-attention fusion mechanism for polyp segmentation.</article-title><source>Appl Intell</source><year>2025</year>;<volume>55</volume>:<fpage>17</fpage>.</mixed-citation></ref><ref id=\"r18\"><label>18</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qi</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Niu</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Long</surname><given-names>X.</given-names></name></person-group><article-title>MG-Net: A fetal brain tissue segmentation method based on multiscale feature fusion and graph convolution attention mechanisms.</article-title><source>Comput Methods Programs Biomed</source><year>2024</year>;<volume>257</volume>:<elocation-id>108451</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.cmpb.2024.108451</pub-id><pub-id pub-id-type=\"pmid\">39395303</pub-id></mixed-citation></ref><ref id=\"r19\"><label>19</label><mixed-citation publication-type=\"preprint\">Wu J, Fu R, Fang H, Zhang Y, Yang Y, Xiong H, Liu H, Xu Y. MedSegDiff: Medical image segmentation with diffusion probabilistic model. arXiv:2211.00611 [Preprint]. Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2211.00611\" ext-link-type=\"uri\">https://arxiv.org/abs/2211.00611</ext-link></mixed-citation></ref><ref id=\"r20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Qian</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Xing</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Chukwudi</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>J</given-names></name></person-group>. <article-title>Advancing hierarchical neural networks with scale-aware pyramidal feature learning for medical image dense prediction.</article-title><source>Comput Methods Programs Biomed</source><year>2025</year>;<volume>265</volume>:<elocation-id>108705</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.cmpb.2025.108705</pub-id><pub-id pub-id-type=\"pmid\">40184852</pub-id></mixed-citation></ref><ref id=\"r21\"><label>21</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>MCSE-U-Net: multi-convolution blocks and squeeze and excitation blocks for vessel segmentation.</article-title><source>Quant Imaging Med Surg</source><year>2024</year>;<volume>14</volume>:<fpage>2426</fpage>-<lpage>40</lpage>. <pub-id pub-id-type=\"doi\">10.21037/qims-23-1454</pub-id><pub-id pub-id-type=\"pmid\">38545081</pub-id><pub-id pub-id-type=\"pmcid\">PMC10963822</pub-id></mixed-citation></ref><ref id=\"r22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Yi</surname><given-names>C.</given-names></name></person-group><article-title>SSA-UNet: Whole brain segmentation by U-Net with squeeze-and-excitation block and self-attention block from the 2.5D slice image.</article-title><source>IET Image Process</source><year>2024</year>;<volume>18</volume>:<fpage>1598</fpage>-<lpage>612</lpage>.</mixed-citation></ref><ref id=\"r23\"><label>23</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Klomp</surname><given-names>SR</given-names></name><name name-style=\"western\"><surname>Wijnhoven</surname><given-names>RG</given-names></name><name name-style=\"western\"><surname>de With</surname><given-names>PH</given-names></name></person-group>. <article-title>Performance-efficiency comparisons of channel attention modules for resnets.</article-title><source>Neural Process Lett</source><year>2023</year>;<volume>55</volume>:<fpage>6797</fpage>-<lpage>813</lpage>.</mixed-citation></ref><ref id=\"r24\"><label>24</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shan</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Cai</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>Y</given-names></name></person-group>. <article-title>Convolutional neural network optimization via channel reassessment attention module.</article-title><source>Digital Signal Processing</source><year>2022</year>;<volume>123</volume>:<elocation-id>103408</elocation-id>.</mixed-citation></ref><ref id=\"r25\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhan</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Hao</surname><given-names>X.</given-names></name></person-group><article-title>MobileNetV2 with Spatial Attention module for traffic congestion recognition in surveillance images.</article-title><source>Expert Syst Appl</source><year>2024</year>;<volume>255</volume>:<elocation-id>124701</elocation-id>.</mixed-citation></ref><ref id=\"r26\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>F</given-names></name></person-group>. <article-title>An end-to-end vision-based seizure detection with a guided spatial attention module for patient detection.</article-title><source>IEEE Internet Things J</source><year>2024</year>;<volume>11</volume>:<fpage>18869</fpage>-<lpage>79</lpage>.</mixed-citation></ref><ref id=\"r27\"><label>27</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Hui</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Fei</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ji</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>T</given-names></name></person-group>. <article-title>Improving polyp segmentation with boundary-assisted guidance and cross-scale interaction fusion transformer network.</article-title><source>Processes</source><year>2024</year>;<volume>12</volume>:<fpage>1030</fpage>.</mixed-citation></ref><ref id=\"r28\"><label>28</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Luo</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name></person-group><article-title>MPEDA-Net: A lightweight brain tumor segmentation network using multi-perspective extraction and dense attention.</article-title><source>Biomed Signal Process Control</source><year>2024</year>;<volume>91</volume>:<elocation-id>106054</elocation-id>.</mixed-citation></ref><ref id=\"r29\"><label>29</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gong</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>F</given-names></name></person-group>. <article-title>Thyroid region prior guided attention for ultrasound segmentation of thyroid nodules.</article-title><source>Comput Biol Med</source><year>2023</year>;<volume>155</volume>:<elocation-id>106389</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2022.106389</pub-id><pub-id pub-id-type=\"pmid\">36812810</pub-id></mixed-citation></ref><ref id=\"r30\"><label>30</label><mixed-citation publication-type=\"webpage\"><article-title>BrainTumor dataset.</article-title> Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://figshare.com/articles/dataset/brain_tumor_dataset/1512427\" ext-link-type=\"uri\">https://figshare.com/articles/dataset/brain_tumor_dataset/1512427</ext-link></mixed-citation></ref><ref id=\"r31\"><label>31</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Selvaraj</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Nithiyaraj</surname><given-names>E.</given-names></name></person-group><article-title>CEDRNN: A convolutional encoder-decoder residual neural network for liver tumour segmentation.</article-title><source>Neural Process Lett</source><year>2023</year>;<volume>55</volume>:<fpage>1605</fpage>-<lpage>24</lpage>.</mixed-citation></ref><ref id=\"r32\"><label>32</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>JY</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>GS</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>XF</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>G</given-names></name></person-group>. <article-title>Global Transformer and Dual Local Attention Network via Deep-Shallow Hierarchical Feature Fusion for Retinal Vessel Segmentation.</article-title><source>IEEE Trans Cybern</source><year>2023</year>;<volume>53</volume>:<fpage>5826</fpage>-<lpage>39</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TCYB.2022.3194099</pub-id><pub-id pub-id-type=\"pmid\">35984806</pub-id></mixed-citation></ref><ref id=\"r33\"><label>33</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>T.</given-names></name></person-group><article-title>GLFNet: Global-local fusion network for the segmentation in ultrasound images.</article-title><source>Comput Biol Med</source><year>2024</year>;<volume>171</volume>:<elocation-id>108103</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2024.108103</pub-id><pub-id pub-id-type=\"pmid\">38335822</pub-id></mixed-citation></ref><ref id=\"r34\"><label>34</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yuan</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name></person-group><article-title>DSCA-PSPNet: Dynamic spatial-channel attention pyramid scene parsing network for sugarcane field segmentation in satellite imagery.</article-title><source>Front Plant Sci</source><year>2023</year>;<volume>14</volume>:<elocation-id>1324491</elocation-id>. <pub-id pub-id-type=\"doi\">10.3389/fpls.2023.1324491</pub-id><pub-id pub-id-type=\"pmid\">38298601</pub-id><pub-id pub-id-type=\"pmcid\">PMC10829042</pub-id></mixed-citation></ref><ref id=\"r35\"><label>35</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>L&#252;</surname><given-names>X.</given-names></name></person-group><article-title>MUNet: a novel framework for accurate brain tumor segmentation combining UNet and mamba networks.</article-title><source>Front Comput Neurosci</source><year>2025</year>;<volume>19</volume>:<elocation-id>1513059</elocation-id>. <pub-id pub-id-type=\"doi\">10.3389/fncom.2025.1513059</pub-id><pub-id pub-id-type=\"pmid\">39944950</pub-id><pub-id pub-id-type=\"pmcid\">PMC11814164</pub-id></mixed-citation></ref><ref id=\"r36\"><label>36</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Ping</surname><given-names>Y.</given-names></name></person-group><article-title>LAMFFNet: Lightweight adaptive multi-layer feature fusion network for medical image segmentation.</article-title><source>Biomed Signal Process Control</source><year>2025</year>;<volume>103</volume>:<elocation-id>107456</elocation-id>.</mixed-citation></ref><ref id=\"r37\"><label>37</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rainio</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Teuho</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Kl&#233;n</surname><given-names>R</given-names></name></person-group>. <article-title>Evaluation metrics and statistical tests for machine learning.</article-title><source>Sci Rep</source><year>2024</year>;<volume>14</volume>:<fpage>6086</fpage>. <pub-id pub-id-type=\"doi\">10.1038/s41598-024-56706-x</pub-id><pub-id pub-id-type=\"pmid\">38480847</pub-id><pub-id pub-id-type=\"pmcid\">PMC10937649</pub-id></mixed-citation></ref><ref id=\"r38\"><label>38</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>Q.</given-names></name></person-group><article-title>On the performance of matthews correlation coefficient (Mcc) for imbalanced dataset.</article-title><source>Pattern Recognit Lett</source><year>2020</year>;<volume>136</volume>:<fpage>71</fpage>-<lpage>80</lpage>.</mixed-citation></ref><ref id=\"r39\"><label>39</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cong</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Kwong</surname><given-names>S</given-names></name></person-group>. <article-title>Boundary guided semantic learning for real-time COVID-19 lung infection segmentation system.</article-title><source>IEEE Transactions on Consumer Electronics</source><year>2022</year>;<volume>68</volume>:<fpage>376</fpage>-<lpage>86</lpage>.</mixed-citation></ref><ref id=\"r40\"><label>40</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Iqbal</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sharif</surname><given-names>M.</given-names></name></person-group><article-title>MDA-Net: Multiscale dual attention-based network for breast lesion segmentation using ultrasound images.</article-title><source>J King Saud Univ Comput Inf Sci</source><year>2022</year>;<volume>34</volume>:<fpage>7283</fpage>-<lpage>99</lpage>.</mixed-citation></ref><ref id=\"r41\"><label>41</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>X.</given-names></name></person-group><article-title>HFENet: A lightweight hand&#8208;crafted feature enhanced CNN for ceramic tile surface defect detection.</article-title><source>Int J Intell Syst</source><year>2022</year>;<volume>37</volume>:<fpage>10670</fpage>-<lpage>93</lpage>.</mixed-citation></ref><ref id=\"r42\"><label>42</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Duan</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>C</given-names></name></person-group>. <article-title>Land cover classification from remote sensing images based on multi-scale fully convolutional network.</article-title><source>Geo Spat Inf Sci</source><year>2022</year>;<volume>25</volume>:<fpage>278</fpage>-<lpage>94</lpage>.</mixed-citation></ref><ref id=\"r43\"><label>43</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>DH</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>Y</given-names></name></person-group>. <article-title>ERDUnet: An efficient residual double-coding unet for medical image segmentation.</article-title><source>IEEE Trans Circuits Syst Video Technol</source><year>2024</year>;<volume>34</volume>:<fpage>2083</fpage>-<lpage>96</lpage>.</mixed-citation></ref><ref id=\"r44\"><label>44</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Bruzzone</surname><given-names>L.</given-names></name></person-group><article-title>LANet: Local attention embedding to improve the semantic segmentation of remote sensing images.</article-title><source>IEEE Trans Geosci Remote Sens</source><year>2021</year>;<volume>59</volume>:<fpage>426</fpage>-<lpage>35</lpage>.</mixed-citation></ref><ref id=\"r45\"><label>45</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yin</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Jian</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>GG</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>R</given-names></name></person-group>. <article-title>AMSUnet: A neural network using atrous multi-scale convolution for medical image segmentation.</article-title><source>Comput Biol Med</source><year>2023</year>;<volume>162</volume>:<elocation-id>107120</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2023.107120</pub-id><pub-id pub-id-type=\"pmid\">37276753</pub-id></mixed-citation></ref><ref id=\"r46\"><label>46</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ong</surname><given-names>HT</given-names></name><name name-style=\"western\"><surname>Karatas</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Poquillon</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Grenci</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Furlan</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Dilasser</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Mohamad Raffi</surname><given-names>SB</given-names></name><name name-style=\"western\"><surname>Blanc</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Drimaracci</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Mikec</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Galisot</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Johnson</surname><given-names>BA</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>AZ</given-names></name><name name-style=\"western\"><surname>Thiel</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ullrich</surname><given-names>O</given-names></name></person-group>; <article-title>OrgaRES Consortium; Racine V, Beghin A. Digitalized organoids: integrated pipeline for high-speed 3D analysis of organoid structures using multilevel segmentation and cellular topology.</article-title><source>Nat Methods</source><year>2025</year>;<volume>22</volume>:<fpage>1343</fpage>-<lpage>54</lpage>. <pub-id pub-id-type=\"doi\">10.1038/s41592-025-02685-4</pub-id><pub-id pub-id-type=\"pmid\">40369245</pub-id><pub-id pub-id-type=\"pmcid\">PMC12165853</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Quant Imaging Med Surg Quant Imaging Med Surg 1905 qims QIMS Quantitative Imaging in Medicine and Surgery 2223-4292 2223-4306 AME Publications PMC12682524 PMC12682524.1 12682524 12682524 10.21037/qims-2025-1364 qims-15-12-12167 1 Original Article MFA-Net: multi-scale feature aggregation network with background-aware module for ultrasound segmentation of thyroid nodules Ye Dongfen 1 Lan Kun 2 Cheng Jianzhen 3 Jiang Xiaoliang 2 1 College of Electrical and Information Engineering, Quzhou University, Quzhou , China ; 2 College of Mechanical Engineering, Quzhou University, Quzhou , China ; 3 Department of Rehabilitation, Quzhou Third Hospital, Quzhou , China Contributions: (I) Conception and design: D Ye, X Jiang; (II) Administrative support: J Cheng; (III) Provision of study materials or patients: D Ye, K Lan; (IV) Collection and assembly of data: D Ye, K Lan; (V) Data analysis and interpretation: D Ye, X Jiang; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors. Correspondence to: Kun Lan, PhD. College of Mechanical Engineering, Quzhou University, 78 North Jiuhua Road, Quzhou 324000, China. Email: 36116@qzc.edu.cn ; Jianzhen Cheng, Bachelor. Department of Rehabilitation, Quzhou Third Hospital, 226 North Baiyun Road, Quzhou 324000, China. Email: qzsycjz@163.com . 21 11 2025 01 12 2025 15 12 502028 12167 12189 15 6 2025 11 10 2025 01 12 2025 09 12 2025 09 12 2025 &#169; 2025 AME Publishing Company. 2025 AME Publishing Company. https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access Statement: This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: https://creativecommons.org/licenses/by-nc-nd/4.0 . Background The size and morphology of thyroid nodules are the essential basis for distinguishing benign and malignant in clinical diagnosis. However, achieving precise segmentation of these nodules in ultrasound images remains a significant task due to the weak and indistinct edges, low contrast, and complex internal structure. To tackle these challenges, our goal is to develop a multi-scale feature aggregation network (MFA-Net) with background-aware module (BAM), which can effectively and robustly segment ultrasound images of thyroid nodules. Methods In MFA-Net framework, through the multi-scale feature aggregation module (MFAM), it effectively captures multi-scale context information and thus improves fine-grained details and global structure representation. Additionally, the BAM inhibits background noise, which allows for effective differentiation between nodules and surrounding tissue. To refine the segmentation performance, we add spatial and channel attentions to the residual decoder module (RDM). Results The quantitative evaluation results showed that on the thyroid nodule 3493 (TN3K) dataset, MFA-Net achieved Dice of 0.8616, intersection over union (IoU) of 0.7586, accuracy of 0.9698 and Matthews correlation coefficient (Mcc) of 0.8457. On the thyroid gland 3583 (TG3K) dataset, the Dice, IoU, accuracy and Mcc reached 0.9857, 0.9718, 0.9977 and 0.9844. On the digital database thyroid image (DDTI) dataset, the Dice, IoU, accuracy and Mcc were 0.7483, 0.5981 0.9283 and 0.7078. On the BrainTumor dataset, MFA-Net obtained Dice of 0.8485, IoU of 0.7421, accuracy of 0.9949 and Mcc of 0.8469. Conclusions These results outperform current leading models and confirm significant performance improvements. In addition, the MFAM, BAM and RDM demonstrate their robustness and adaptability in different segmentation scenarios when conducting independent ablation experiments. Keywords: Thyroid nodules diagnosis ultrasound image segmentation multi-scale feature aggregation network (MFA-Net) multi-scale feature aggregation module (MFAM) background-aware mechanism National Natural Science Foundation of China No. 62102227 Joint Fund of Zhejiang Provincial Natural Science Foundation of China No. LZY24E060001 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Introduction Thyroid nodules are common diseases of the endocrine system and have become an important health problem of global concern. These nodules appear as abnormal growths within the thyroid and are usually caused by a variety of causes, including benign hyperplasia, cyst formation, benign tumors, or malignant transformations ( 1 ). With the continuous advancement of various imaging technologies, doctors can utilize them for early screening and diagnosis of thyroid nodules, which is critical to determining their nature and developing treatment plans. Among the various techniques, ultrasound imaging has become the frontline choice for thyroid nodules due to its low cost, non-radiation and ability to scan continuously. However, due to the blurred boundary of nodules, low contrast, and complex thyroid tissue structure, doctors&#8217; identification and segmentation of nodules are highly dependent on personal experience, which can easily lead to misdiagnosis or missed diagnosis ( 2 ). Therefore, the use of artificial intelligence technology to develop efficient thyroid nodule segmentation algorithms can help doctors provide patients with more reliable treatment plans and improve diagnostic efficiency. Thyroid nodules in ultrasound images usually vary in size, shape, and appearance and are similar to the surrounding thyroid tissue. Additionally, ultrasound imaging is inherently susceptible to various types of noise, which stem from both environmental interference and instrumental limitations during tissue propagation. Given these complexities, traditional segmentation techniques that rely on handcrafted features or threshold-based methods frequently struggle to provide accurate and reliable results. However, with the development of deep learning technologies, new opportunities have been presented, such as U-Net ( 3 ), DESENet ( 4 ), BMANet ( 5 ) and NLIE-UNet ( 6 ). These encoder-decoder models utilize hierarchical feature extraction and multi-scale information processing to improve segmentation accuracy. However, despite their effectiveness, they are not without limitations. One key issue is improper feature fusion, which can lead to feature fusion inconsistencies across different network layers. Additionally, during the encoding process, lesion boundary details may be gradually lost due to repeated down-sampling operations. Consequently, further optimization of feature aggregation and boundary refinement strategies is essential to improve the robustness and generalization ability of thyroid nodule segmentation. The segmentation algorithms of thyroid nodules generally fall into traditional models and deep learning-based models. The traditional method primarily relies on gray-level intensity, texture features, or geometric information for feature extraction and region-based segmentation, which has the advantages of high computational efficiency and mature theory. However, due to the inherent characteristics of ultrasound images, which often lead to inconsistent and inaccurate segmentation results. In contrast, the deep learning-based algorithms leverage powerful architectures and transformer to automatically extract advanced features, which can effectively solve the challenges posed by ultrasonic noise and different nodule shapes. Among them, Wu et al. ( 7 ) incorporated deep convolutional layers within the encoder-decoder framework of Swin Transformer, effectively strengthening the representation of both global and local features. Furthermore, they introduced a multi-scale feature fusion module that enables more effective integration and exchange of features across different hierarchical levels. Li et al. ( 8 ) proposed a global structure-enhanced decoder that can effectively enhance feature representation and ensure more accurate boundary depiction. Hu et al. ( 9 ) designed a dual-decoder branch architecture by integrating Mamba and ResNet-34 to enhance feature extraction and segmentation performance. Ozcan et al. ( 10 ) proposed a hybrid segmentation model that enhances remote dependencies and the ability to capture global context information. Ali et al. ( 11 ) presented an encoder architecture with dense connections to enhance feature propagation and reuse across different network layers. Zheng et al. ( 12 ) adopted a cascade convolution strategy, which can effectively capture multi-scale context information while maintaining a larger acceptance field. The attention mechanism ( 13 , 14 ) plays a crucial role in enhancing feature representation by selectively focusing on important regions while minimizing the impact of background noise and irrelevant information. By dynamically adjusting the weights of different spatial locations, channels, or positional elements, the attention mechanism can improve model performance and generalization ability. Due to its effectiveness, attention mechanism has been widely integrated into semantic segmentation, object detection, and image classification. Among them, Gu et al. ( 15 ) presented a multi-scale coordinate attention algorithm to enhance feature representation by effectively capturing spatial and contextual dependencies across different scales. Ni et al. ( 16 ) combined channel attention and positional attention to enhance feature representation by jointly capturing global dependencies and spatial relationships within an image. Shang et al. ( 17 ) introduced a cascaded attention fusion module, which enhanced feature representation by progressively integrating multi-level attention mechanisms. Qi et al. ( 18 ) combined graph-based convolution operations with attention mechanisms, which allowed for more focused and efficient encoding of relevant information. Apart from the utilization of attention mechanisms, several emerging technologies have demonstrated promising performance. Among them, Wu et al. ( 19 ) combined dynamic condition encoding with feature frequency parser and developed the first general medical image segmentation framework that utilizes the diffusion probabilistic model. Liu et al. ( 20 ) introduced a scale-aware pyramidal feature learning strategy, which explicitly exploits multi-scale contextual information to strengthen feature representation. In this paper, we present a novel multi-scale feature aggregation network (MFA-Net) with background-aware module (BAM) for thyroid nodules segmentation. Unlike existing segmentation approaches, our MFA-Net leverages an encoder-decoder architecture combined with multi-scale feature extraction to enhance segmentation accuracy and robustness. By simultaneously modeling global dependencies across both spatial and channel dimensions, the network effectively captures long-range contextual information while preserving fine-grained details. The key contributions of our research are outlined as: &#10070; The multi-scale feature aggregation module (MFAM) is designed to effectively capture contextual information at multiple levels, ensuring a comprehensive understanding of both local and global image structures. This design significantly enhances the model&#8217;s adaptability to different shapes, sizes and appearances of thyroid nodules, and improves the overall performance under complex imaging conditions. &#10070; The background-aware mechanism is integrated to suppress irrelevant or non-informative regions, guiding the focus of the network to more prominent foreground areas. By refining feature selection, this mechanism enhances the distinction between nodules and surrounding tissue and reduces interference from background noise. &#10070; The residual decoder module (RDM) is augmented with spatial and channel mechanisms, which work synergistically to highlight critical features while diminishing the influence of less relevant information. This dual-attention strategy improves segmentation precision by preserving structural integrity and enhancing boundary clarity, ultimately leading to more accurate and robust segmentation results. We present this article in accordance with the CLEAR reporting checklist (available at https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc ). Methods This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. Overview Figure 1 illustrates the overall architecture of MFA-Net, which is a deep learning framework following the encoder-decoder structure. The encoder adopts Encoder-block while the decoder uses Decoder-block, both of which are designed for the encoding and decoding of thyroid nodule images. In the encoding stage, each Encoder-block employs a double convolutional inspired by U-Net structure, which enhances feature extraction while preserving fine-grained local details. To enhance the segmentation performance, MFA-Net incorporates MFAM and BAM. These modules are responsible for capturing multi-scale contextual information and suppressing interference information. In each layer, the MFAM processes the output features from the corresponding Encoder block and performs multi-scale contextual information acquisition. Then, the up-sampling (UP) module (transpose convolution using ConvTranspose2d function) enhances feature resolution by UP the output of the next-layer Decoder-block, making these refined features available for both the current Decoder-block and the prediction output. Meanwhile, the BAM strengthens model focus by applying attention mechanisms to distinguish foreground from background, leveraging both the Encoder block&#8217;s feature representations and the predictive output from the UP module. Throughout the decoding process, the Decoder-block accepts the output features from the BAM, MFAM, and UP. Finally, the highest-level Decoder-block performs final refinements using a 1&#215;1 convolution followed by a Sigmoid activation, ultimately generating the model&#8217;s segmentation prediction. Figure 1 The overall framework of MFA-Net. BAM, background-aware module; Conv, convolution; MFAM, Multi-scale feature aggregation module; MFA-Net, multi-scale feature aggregation network; UP, up-sampling. MFAM As illustrated in Figure 2 , the MFAM begins by receiving the feature maps from the Encoder-block. To enhance feature representation across different spatial scales, the input features are simultaneously passed through multiple convolutional branches, each utilizing distinct kernel sizes to extract diverse patterns and structural details. Firstly, the 1&#215;5 and 5&#215;1 convolutions are responsible for capturing fine-grained horizontal and vertical edge information. To further extend spatial awareness, the 1&#215;7 and 7&#215;1 convolutions contribute to capturing long-range dependencies. Finally, the 1&#215;11 and 11&#215;1 convolutions provide the largest receptive field, capturing both fine details and broader spatial relationships. Once features are extracted at multiple scales, their outputs are concatenated along the channel dimension to construct a comprehensive multi-scale feature representation. This aggregated feature map is subsequently processed through a 1&#215;1 convolution, which reduces channel dimensions while refining feature interactions. Moreover, the processed features are passed through a Sigmoid function, which normalizes values between 0 and 1, thereby learning attention-based weights for different feature regions. Following this, the Sigmoid-activated feature undergoes element-wise multiplication with the original encoder feature. This operation ensures that salient features are emphasized, while less relevant information is suppressed. The MFAM module plays a vital role in MFA-Net, as it effectively captures multi-scale contextual dependencies, leading to enhanced thyroid nodule segmentation with improved accuracy and robustness. Figure 2 Structure of MFAM. MFAM, multi-scale feature aggregation module. BAM As depicted in Figure 3 , the BAM plays a crucial role in enhancing the model&#8217;s ability to discriminate between foreground (i.e., thyroid nodules) and background (surrounding non-nodule regions). Specifically, the feature map output from the Encoder block, as well as the prediction map generated by the UP module. To build an attention mechanism, BAM first computes the background-aware map by applying 1-pred to the prediction maps, where pred represents the probability map obtained by applying the Sigmoid activation to the predicted result. This transformation effectively inverts the predictions, ensuring that higher weights are assigned to background areas while foreground features are suppressed. The computed attention map is then applied to the encoder feature map channel-wise through element-wise multiplication. The attention-refined features are subsequently processed through a 3&#215;3 convolution layer and a squeeze-and-excitation block ( 21 , 22 ), which enhances channel-wise attention by dynamically adjusting feature importance, as shown in Figure 4 . To preserve rich spatial information and prevent excessive feature loss, skip connection is introduced to add the original encoder features back to the refined attention-enhanced feature. By integrating BAM into the network, MFA-Net effectively suppresses background noise and improves prediction accuracy. Figure 3 Structure of BAM. BAM, background-aware module. Figure 4 Structure of squeeze-and-excitation block. FC, fully connected layer; ReLU, rectified linear unit. RDM As illustrated in Figure 5 , the Decoder-block incorporates an RDM, which processes inputs from the BAM, MFAM, and UP. To ensure comprehensive feature representation, these inputs are first concatenated together. Once concatenated, the features are refined through two successive 3&#215;3 convolutional layers. To further optimize learning, a 1&#215;1 convolution-based residual connection is introduced in parallel. Following feature transformation, the output is refined through an attention mechanism comprising channel attention and spatial attention. Ultimately, this structured decoding approach results in a well-defined feature representation at this stage, contributing to a more precise and context-aware final segmentation output. Figure 5 Structure of RDM. BAM, background-aware module; Conv, convolution; MFAM, Multi-scale feature aggregation module; RDM, residual decoder module; UP, up-sampling. As shown in Figure 6 , the channel attention module ( 23 , 24 ) enriches feature representation by selectively emphasizing important channels, and its input feature map has dimensions of C &#215; H &#215; W. To extract inter-channel dependencies, we employ two parallel processing branches: one branch reshapes the input tensor into a matrix of size C &#215; (H &#215; W) to facilitate channel-wise interaction, and the second branch applies both reshaping and a transpose operation, transforming the feature map into a (H &#215; W) &#215; C matrix. The two transformed feature representations are then multiplied using a dot product to compute a channel affinity matrix of size C &#215; C. This matrix effectively captures the relationships between different channels. To adjust the attention distribution, the calculated channel association matrix is normalized by Softmax to ensure that the sum of all weights equals one. Figure 6 Structure of channel attention module. Subsequently, the resulting attention matrix is applied to the reshaped feature matrix through matrix multiplication. To preserve essential original information while integrating learned attention-based improvements, the refined output is added back to the original input through a residual connection. This approach enhances feature learning stability and ensures that the output retains the same spatial dimensions C&#215;H&#215;W as the initial input, while benefiting from refined channel-wise feature weighting. As illustrated in Figure 7 , the spatial attention module ( 25 , 26 ) functions similarly to the channel attention module but differs in its primary focus. However, unlike capturing global dependencies between channels, spatial attention ensures higher attention to key areas such as object boundaries and key structural details by analyzing spatial dependencies. By dynamically adjusting attention weights across spatial positions, the module ensures that essential features are more prominent, which is especially advantageous in segmentation tasks where precise object delineation is required. Figure 7 Structure of spatial attention module. Loss function Unlike conventional loss functions that only operate on a pixel-by-pixel basis, we employ a combined loss function consisting of Dice loss ( 27 ) and binary cross-entropy (BCE) loss ( 28 ) to optimize the segmentation network. Among them, Dice loss is designed to evaluate the similarity between predicted segmentation maps and ground truth labels by directly computing their overlap. Additionally, BCE loss provides stable pixel-level supervision by penalizing misclassified pixels, which is particularly effective in handling class imbalance. By integrating these two components, the joint loss leverages the strengths of both global region-level optimization and local pixel-wise accuracy. For numerical stability, the joint loss is computed on the probability maps obtained after applying the Sigmoid activation to the network output. The mathematical formulation of the combined loss is as follows: L t o t a l = &#945; L D i c e + &#946; L B C E [1] L D i c e = 1 &#8722; 2 &#8721; i = 1 N y i y ^ i &#8721; i = 1 N y i + &#8721; i = 1 N y ^ i L B C E = &#8722; 1 N &#8721; i = 1 N [ y i log y ^ i + ( 1 &#8722; y i ) log ( 1 &#8722; y ^ i ) ] [2] where N is the number of pixels, y ^ i and y i denote the labeled value and predicted value. &#945; and &#946; are weighting coefficients that balance the contributions of the two loss terms. Dataset description To comprehensively assess the generalizability of MFA-Net, we conducted extensive experiments on four benchmark datasets: thyroid nodule 3493 (TN3K), thyroid gland 3583 (TG3K), digital database thyroid image (DDTI) ( 29 ) and BrainTumor ( 30 ). The detailed characteristics of these datasets are summarized in Table 1 . Table 1 Detailed introduction of three thyroid nodule and one BrainTumor datasets Dataset Number Train Validate Test TN3K 3,493 2,160 719 614 TG3K 3,583 2,152 717 716 DDTI 637 383 127 127 BrainTumor 3,064 1,839 613 612 DDTI, digital database thyroid image; TG3K, thyroid gland 3583 dataset; TN3K, thyroid nodule 3493 dataset. The TN3K dataset comprises 3,493 ultrasound images, each containing at least one thyroid nodule region. The images in TN3K are carefully curated to include diverse nodule shapes, sizes, and locations, providing a robust foundation for assessing segmentation performance across varying anatomical structures. The TG3K dataset consists of 3,583 ultrasound images, where the thyroid gland region has been precisely segmented from ultrasound video sequences. To ensure data quality and relevance, only images where the thyroid gland occupies at least 6% of the total image area are considered. The DDTI dataset contains 637 ultrasound images of the thyroid nodule, each annotated with pixel-level segmentation masks obtained from a single ultrasound imaging device. The BrainTumor dataset provides a well-structured resource tailored for advancing brain tumor segmentation research. It includes a total of 3,064 magnetic resonance imaging (MRI) brain scans, each precisely aligned with a manually annotated binary mask that delineates the tumor regions. Experimental scheme All experiments were performed utilizing the PyTorch framework and executed on a high-performance NVIDIA RTX A6000 GPU. To optimize the model, we employed the Adam optimizer, initializing its learning rate at 1&#215;10 &#8722;3 to facilitate stable and efficient convergence. During the training phase, we configured the batch size to 32 and the number of iterations to 200. To ensure consistency across the dataset, all input ultrasound images were resized to 256&#215;256 pixels before being fed into the network. To thoroughly evaluate the effectiveness of MFA-Net and ensure a fair comparison with some well-established segmentation methods, we employ Dice ( 31 , 32 ), intersection over union (IoU) ( 33 , 34 ), accuracy ( 35 , 36 ) and Matthews correlation coefficient (Mcc) ( 37 , 38 ) as the performance metrics. The mathematical formulations of these metrics are as follows: D i c e = 2 T P 2 T P + F N + F P &#8202; [3] I o U = T P T P + F N + F P [4] A c c u r a c y = T P + T N T P + T N + F N + F P [5] M c c = T P &#215; T N &#8722; F P &#215; F N ( T P + F N ) ( T P + F P ) ( T N + F N ) ( T N + F P ) [6] Results Experimental results on TN3K dataset To evaluate the performance of MFA-Net, a series of comparative experiments was conducted on the TN3K dataset, with the results summarized in Table 2 . These methods include U-Net ( 3 ), BSNet ( 39 ), MDA-Net ( 40 ), HFENet ( 41 ), MSFCN ( 42 ), ERDUnet ( 43 ), LANet ( 44 ), AMSUnet ( 45 ), DESENet ( 4 ), BMANet ( 5 ) and NLIE-UNet ( 6 ). Among the compared methods, MSFCN and U-Net exhibit the lowest performance, with Dice of 0.7113 and 0.7151, IoU of 0.5560 and 0.5649, accuracy of 0.9358 and 0.0399, Mcc of 0.6762 and 0.6823. Despite their foundational role in segmentation tasks, both models struggle with complex nodule structures and often produce coarse predictions. ERDUnet and HFENet show moderately improved results, achieving Dice of 0.8014 and 0.8041, IoU of 0.6719 and 0.6758, accuracy of 0.9558 and 0.9550, Mcc of 0.7765 and 0.7802. However, they still face challenges in accurately delineating nodular boundaries, especially in low-contrast or noisy regions. LANet, AMSUnet, BMANet and NLIE-UNet achieve Dice scores between 0.805 and 0.82, reflecting their better balance between detail preservation and structural consistency, though some fine-grained errors persist. Further enhancements are observed in BSNet, MDA-Net, and DESENet, which obtain Dice scores of 0.8377, 0.8409, and 0.8365, and Mcc values above 0.81, demonstrating stronger feature extraction ability and improved robustness in boundary prediction. However, there are still minor errors in complex regions, which slightly affect the overall consistency. Among all evaluated models, the proposed MFA-Net achieves the highest performance, with Dice of 0.8616, IoU of 0.7586, accuracy of 0.9698 and Mcc of 0.8457. By capturing both global and local dependencies, MFA-Net excels in delineating fine structures and maintaining accurate contours, particularly in challenging cases involving small or irregular nodules. Figure 8 provides qualitative segmentation results produced by various approaches on the TN3K dataset. These qualitative observations strongly support the quantitative findings, confirming that MFA-Net offers superior robustness and precision in clinical ultrasound-based thyroid segmentation. Table 2 Comparison experiments of various models on the TN3K dataset Model Dice IoU Accuracy Mcc U-Net 0.7151 0.5649 0.9399 0.6823 BSNet 0.8377 0.7226 0.9648 0.8196 MDA-Net 0.8409 0.7275 0.9649 0.8216 HFENet 0.8041 0.6758 0.9550 0.7802 MSFCN 0.7113 0.5560 0.9358 0.6762 ERDUnet 0.8014 0.6719 0.9558 0.7765 LANet 0.8163 0.6919 0.9601 0.7953 AMSUnet 0.8053 0.6787 0.9589 0.7828 DESENet 0.8365 0.7219 0.9642 0.8175 BMANet 0.8091 0.6833 0.9587 0.7879 NLIE-UNet 0.8054 0.6774 0.9563 0.7817 MFA-Net 0.8618 0.7586 0.9698 0.8457 IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset. Figure 8 Comparison experiments of various models on the TN3K dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset. Experimental results on TG3K dataset Table 3 displays the quantitative evaluation of several models on the TG3K dataset, which features thyroid nodules with well-defined lesions and relatively consistent shapes and anatomical positions. These characteristics contribute to the high segmentation accuracy achieved by most models. Specifically, with the exception of U-Net, all methods reported Dice coefficients exceeding 97%, IoU scores above 94%, and Mcc are higher than 94%, reflecting the dataset&#8217;s lower complexity compared to more heterogeneous collections. Despite this overall strong performance, U-Net significantly underperforms, yielding a Dice of 78.63% and IoU of 65.23%, suggesting limited capability in handling even moderately variable nodule presentations. The visual results in Figure 9 further illustrate this point, where U-Net displays a large number of erroneous segmentations; it is unable to capture the complete structure of the nodules or introduce false positives in the surrounding area. In contrast, the differences between other models are very small. However, among all the approaches, the proposed MFA-Net achieves the highest performance, with Dice of 98.57%, IoU of 97.18%, accuracy of 99.77% and Mcc of 98.44%. This superior outcome highlights MFA-Net&#8217;s exceptional ability to capture fine-grained boundaries and preserve spatial consistency, even in datasets where lesions may appear visually subtle or homogeneous. Table 3 Comparison experiments of various models on the TG3K dataset Model Dice IoU Accuracy Mcc U-Net 0.7863 0.6523 0.9602 0.7711 BSNet 0.9820 0.9647 0.9972 0.9805 MDA-Net 0.9821 0.9650 0.9972 0.9806 HFENet 0.9792 0.9593 0.9967 0.9774 MSFCN 0.9785 0.9581 0.9966 0.9767 ERDUnet 0.9714 0.9445 0.9955 0.9691 LANet 0.9823 0.9653 0.9973 0.9808 AMSUnet 0.9778 0.9567 0.9966 0.9759 DESENet 0.9845 0.9695 0.9976 0.9832 BMANet 0.9836 0.9677 0.9975 0.9822 NLIE-UNet 0.9736 0.9487 0.9959 0.9713 MFA-Net 0.9857 0.9718 0.9977 0.9844 IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; TG3K, thyroid gland 3583 dataset. Figure 9 Comparison experiments of various models on the TG3K dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network; TG3K, thyroid gland 3583 dataset. Experimental results on DDTI dataset Drawing upon the quantitative metrics detailed in Table 4 and the visual outcomes illustrated in Figure 10 , we can comprehensively evaluate and contrast the performance of various models on the DDTI dataset. Due to the higher variability in image quality and nodule appearance of the DDTI dataset, all models have lower Dice and IoU scores compared to TG3K or TN3K. Specifically, MSFCN demonstrates the lowest performance among all approaches, with Dice of 61.65%, IoU of 44.68%, accuracy of 88.36% and IoU of 55.08%. These results highlight MSFCN&#8217;s limitations in capturing fine boundary details in more complex scenarios. U-Net also underperforms, with Dice of 62.49%, IoU of 45.47%, accuracy of 87.02% and Mcc of 55.70%, suggesting insufficient capacity for modeling the intricate and irregular structures commonly found in DDTI. HFENet, AMSUnet and NLIE-UNet show moderate results, but still struggle to achieve satisfactory segmentation precision, particularly in edge delineation. MDA-Net, BSNet, LANet, DESENet and BMANet perform comparably, with Dice scores in the 70-73% range, and IoU values between 55% and 58%, indicating relatively stable but not optimal performance. ERDUnet stands out slightly from the above group, achieving Dice of 74.10%, IoU of 58.94%, accuracy of 92.56% and Mcc of 69.81%, suggesting better generalization to the heterogeneous features present in the dataset. However, the proposed method achieves the best quantitative performance, with Dice of 74.83%, IoU of 59.81%, accuracy of 92.83% and Mcc of 70.78%. The combined quantitative and qualitative analyses affirm that MFA-Net outperforms other state-of-the-art models in handling the challenging characteristics of the DDTI dataset. This indicates its superior ability to handle low-contrast and irregularly shaped nodules, thanks to the multi-scale and attention-guided mechanisms embedded in the architecture. Table 4 Comparison experiments of various models on the DDTI dataset Model Dice IoU Accuracy Mcc U-Net 0.6249 0.4547 0.8702 0.5570 BSNet 0.7346 0.5810 0.9169 0.6874 MDA-Net 0.7255 0.5696 0.9135 0.6765 HFENet 0.6586 0.4917 0.8929 0.5972 MSFCN 0.6165 0.4468 0.8836 0.5508 ERDUnet 0.7410 0.5894 0.9256 0.6981 LANet 0.7097 0.5507 0.9085 0.6576 AMSUnet 0.6844 0.5210 0.8971 0.6275 DESENet 0.7113 0.5527 0.9160 0.6624 BMANet 0.7282 0.5736 0.9175 0.6801 NLIE-UNet 0.6756 0.5115 0.9025 0.6193 MFA-Net 0.7483 0.5981 0.9283 0.7078 DDTI, digital database thyroid image; IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network. Figure 10 Comparison experiments of various models on the DDTI dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. DDTI, digital database thyroid image; MFA-Net, multi-scale feature aggregation network. Experimental results on BrainTumor dataset Drawing upon the quantitative metrics reported in Table 5 and the qualitative visualizations presented in Figure 11 , the performance of different segmentation models on the BrainTumor dataset can be thoroughly evaluated. Among them, U-Net provides the baseline with relatively modest Dice (77.55%) and IoU (64.85%), highlighting its limited ability to capture complex tumor structures. AMSUnet, MSFCN, and ERDUnet also demonstrate lower accuracy in boundary delineation, with Dice scores below 80%, indicating difficulties in segmenting irregular or small tumor regions. AMSUnet, MSFCN, and ERDUnet also demonstrate lower accuracy in boundary delineation, with Dice scores below 80%, indicating difficulties in segmenting irregular or small tumor regions. HFENet, LANet, and NLIE-UNet achieve slightly better results, with Dice around 80% and Mcc values close to 0.80, reflecting moderate improvements in segmentation consistency but still failing to handle subtle tumor boundaries effectively. In contrast, BSNet, MDA-Net, DESENet, and BMANet deliver more competitive outcomes, with Dice ranging from 81% to 82% and IoU values above 69%. The visual outcomes also suggest that these approaches yield more coherent tumor contours compared to the baseline U-Net. Notably, MFA-Net surpasses all competing methods, achieving the best performance with Dice of 84.85%, IoU of 74.21%, accuracy of 99.49%, and Mcc of 84.69%. Both the quantitative and qualitative evaluations indicate that MFA-Net consistently produces more accurate and complete tumor boundaries, outperforming state-of-the-art methods. Table 5 Comparison experiments of various models on the BrainTumor dataset Model Dice IoU Accuracy Mcc U-Net 0.7755 0.6485 0.9934 0.7873 BSNet 0.8243 0.7071 0.9943 0.8233 MDA-Net 0.8245 0.7078 0.9938 0.8226 HFENet 0.8023 0.6756 0.9933 0.7997 MSFCN 0.7922 0.6633 0.9932 0.7912 ERDUnet 0.7934 0.6662 0.9933 0.7934 LANet 0.8009 0.6759 0.9936 0.8001 AMSUnet 0.7843 0.6543 0.9928 0.7836 DESENet 0.8154 0.6944 0.9939 0.8141 BMANet 0.8093 0.6847 0.9935 0.8064 NLIE-UNet 0.8062 0.6804 0.9934 0.8038 MFA-Net 0.8485 0.7421 0.9949 0.8469 IoU, intersection over union; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network. Figure 11 Comparison experiments of various models on the BrainTumor dataset. (A) Original image. (B) Mask. (C) U-Net. (D) BSNet. (E) MDA-Net. (F) HFENet. (G) MSFCN. (H) ERDUnet. (I) LANet. (J) AMSUnet. (K) DESENet. (L) BMANet. (M) NLIE-UNet. (N) MFA-Net. MFA-Net, multi-scale feature aggregation network. Discussion Ablative studies To thoroughly evaluate the performance benefits and individual contributions of each core module in the proposed MFA-Net architecture, we carried out detailed ablation studies using the TN3K dataset. In this evaluation, the MFAM, background-aware mechanism and RDM were independently incorporated into the baseline architecture. Each component was evaluated in isolation to quantify its specific effects on segmentation accuracy and robustness. Finally, all modules were combined to form the complete MFA-Net. The corresponding quantitative results and qualitative visualizations are presented in Table 6 and Figure 12 , respectively. Table 6 Ablative studies results on the TN3K dataset Model Dice IoU Accuracy Mcc Param (M) FPS GFLOPs Baseline 0.7151 0.5649 0.9399 0.6823 1.94 227.29 3.48 Baseline + MFAM 0.8431 0.7312 0.9651 0.8237 2.80 138.70 4.84 Baseline + BAM 0.8410 0.7273 0.9655 0.8225 3.71 139.47 7.64 Baseline + RDM 0.8476 0.7380 0.9672 0.8301 2.55 101.23 4.07 Baseline + MFAM + BAM 0.8321 0.7158 0.9646 0.8137 4.00 103.33 8.56 Baseline + MFAM + RDM 0.8584 0.7536 0.9683 0.8413 2.85 86.50 4.99 Baseline + BAM + RDM 0.8536 0.7459 0.9671 0.8356 3.77 84.43 7.86 MFA-Net 0.8618 0.7586 0.9698 0.8457 4.07 67.46 8.78 BAM, background-aware module; IoU, intersection over union; FPS, frame-per-second; GFLOPs, giga floating-point operations per second; Mcc, Matthews correlation coefficient; MFA-Net, multi-scale feature aggregation network; MFAM, multi-scale feature aggregation module; RDM, residual decoder module; TN3K, thyroid nodule 3493 dataset. Figure 12 Ablative studies result on the TN3K dataset. (A) Original image. (B) Mask. (C) Baseline. (D) Baseline + MFAM. (E) Baseline + BAM. (F) Baseline + RDM. (G) Baseline + MFAM + BAM. (H) Baseline + MFAM + RDM. (I) Baseline + BAM + RDM. (J) MFA-Net. BAM, background-aware module; MFAM, multi-scale feature aggregation module; RDM, residual decoder module; TN3K, thyroid nodule 3493 dataset. To validate the impact of the MFAM within the proposed MFA-Net architecture, we conducted focused ablation experiments by integrating MFAM independently into the baseline. As presented in Table 6 and illustrated in Figure 12 , the addition of MFAM resulted in a marked increase in segmentation accuracy when compared to the baseline configuration. Specifically, the Dice increased from 71.51% to 84.31%, the IoU rose from 56.49% to 73.12%, the accuracy improved from 93.99% to 96.51%, and the Mcc increased from 68.23% to 82.37%. These enhancements clearly indicate MFAM&#8217;s effectiveness in enriching the feature representation by leveraging multi-scale contextual cues. By combining detailed local textures with broader structural information, MFAM strengthens the model&#8217;s ability to delineate thyroid nodule boundaries with higher accuracy, especially in cases where lesion contours are subtle or irregular. From a computational perspective, the inclusion of MFAM leads to a moderate increase in model complexity: the number of parameters rises from 1.94 to 2.80 M, and the computational cost grows from 3.48 to 4.84 giga floating-point operations per second (GFLOPs). Meanwhile, the frame-per-second (FPS) rate decreases from 227.29 to 138.70, which remains acceptable for real-time or near-real-time clinical applications. Qualitatively, visualizations in Figure 12 demonstrate that the MFAM produces more complete and accurate nodule masks, especially in complex scenarios with low contrast or fragmented edges. The segmentation predictions produced after incorporating the MFAM exhibit a markedly higher consistency with the ground truth masks than those generated by the baseline, confirming that MFAM enhances the model&#8217;s capacity to identify nodules with varying scales and textures. In terms of quantitative performance, the integration of BAM into the baseline network leads to a significant improvement in segmentation accuracy. Specifically, the Dice increases from 71.51% to 84.10%, while the IoU rises from 56.49% to 72.73%. In addition, the accuracy improves from 93.99% to 96.55%, and the Mcc increases from 68.23% to 82.25%. The visualization results show that BAM&#8217;s capacity to guide the model&#8217;s attention more effectively toward meaningful foreground features while diminishing the influence of irrelevant background content. From a computational perspective, the addition of BAM increases the model&#8217;s parameter count from 1.94 to 3.71 M, with the frame rate dropping from 227.29 to 139.47 FPS, while the computational complexity rises from 3.48 to 7.64 GFLOPs. Although its speed is slower than the base version, its performance still falls within the feasible range suitable for practical clinical applications. Integrating RDM into the baseline network architecture can significantly enhance the segmentation performance. Quantitatively, the Dice improves from 71.51% to 84.76%, while the IoU increases from 56.49% to 73.80%. Moreover, the accuracy rises from 93.99% to 96.72%, and the Mcc improves from 68.23% to 83.01%. From a qualitative perspective, the generated predictions demonstrate improved alignment with ground truth boundaries, especially in regions characterized by subtle textures or complex structural variations. In terms of computational implications, the introduction of RDM leads to a moderate increase in model complexity, expanding the parameter count from 1.94 to 2.55 M and GFLOPs increasing from 3.48 to 4.07. The additional computational cost is reflected in a decrease in processing speed, with the inference frame rate dropping from 227.29 FPS to 101.23 FPS. When multiple modules are integrated, the performance of the network improves further through their complementary strengths. As shown in Table 6 , the combination of MFAM and BAM leads to clear improvements over the baseline, with the Dice increasing from 71.51% to 83.21%, IoU from 56.49% to 71.58%, accuracy from 93.99% to 96.46%, and Mcc from 68.23% to 81.37%. Although this dual-module configuration raises the parameter count to 4.00 M and increases GFLOPs to 8.56, the achieved segmentation improvements demonstrate that MFAM and BAM cooperate effectively by capturing multi-scale contextual cues and refining spatial attention. Similarly, the integration of MFAM and RDM produces the most notable gains among the dual-module settings. In this case, the Dice reaches 85.84%, IoU rises to 75.36%, accuracy improves to 96.83%, and Mcc increases to 84.13%, all of which represent significant advances compared to the baseline. The parameter count moderately grows to 2.85 M, GFLOPs to 4.99, and FPS drops to 86.50. The combination of BAM and RDM also yields strong results, with Dice improving to 85.36%, IoU to 74.59%, accuracy to 96.71%, and Mcc to 83.56%. Although this configuration has a higher parameter load of 3.77M and computational cost of 7.86 GFLOPs, the improvements in feature refinement and boundary delineation make it particularly effective for complex cases with heterogeneous nodule appearances. Finally, when all three modules are jointly integrated into the baseline, the proposed MFA-Net exhibits the most outstanding overall performance in both quantitative metrics and qualitative visual outcomes. This comprehensive configuration leverages the individual strengths of each component, leading to a synergistic enhancement in segmentation capability. In terms of model complexity, the parameter count increases to 4.07 M, GFLOPs to 8.78, and FPS decreases from 227.29 to 67.46, reflecting the added architectural complexity. Despite these increases, the performance gains achieved justify the trade-off. The network remains lightweight enough for practical use, especially in scenarios where segmentation accuracy is critical and computational resources are moderately available. Selection experiment of loss function To identify the most effective loss function for enhancing segmentation performance in the context of thyroid nodule detection, we conducted a comparative evaluation of six widely used loss functions and the results are shown in Table 7 . Each loss function was integrated into MFA-Net under identical training settings, ensuring a fair comparison based solely on their individual optimization capabilities. Specifically, BCE loss achieved Dice of 83.39%, IoU of 71.73%, accuracy of 96.31%, and Mcc of 81.37%. While it provides a solid baseline, its performance is limited in capturing fine-grained regions due to the lack of direct overlap optimization. Dice loss improved all metrics, achieving Dice of 85.88%, IoU of 75.36%, accuracy of 96.84%, and Mcc of 84.14%. This demonstrates its strong ability to directly maximize overlap between predicted masks and ground truth, making it particularly effective for handling class imbalance. IoU loss focused on optimizing the intersection-over-union metric, but its overall performance was lower (Dice 77.59%, IoU 63.76%, accuracy 94.07%, Mcc 75.74%). This indicates that although optimizing IoU is useful, during the training process, it may not be as stable as the Dice loss. The Tversky loss function (we set 0.3 and 0.7 for the hyperparameter) aims to balance the situations of false positives and false negatives, and its results are quite satisfactory (with a Dice value of 79.82%, an IoU value of 66.73%, an accuracy rate of 94.79%, and a Mcc value of 78.02%). This indicates that although it can solve the problem of class imbalance, its overall segmentation performance is slightly lower than that of the Dice loss function. Focal (weighted 0.1) + Tversky (weighted 0.9) loss, which emphasizes difficult-to-segment regions, resulted in Dice of 78.22%, IoU of 64.51%, accuracy of 94.12%, and Mcc of 76.48%. Although this combination is theoretically advantageous for hard examples, it underperformed compared to simpler Dice-based losses in our experiments. BCE (weighted 0.5) + Dice (weighted 0.5) loss achieved the highest overall performance (Dice 86.18%, IoU 75.86%, accuracy 96.98%, Mcc 84.57%), slightly outperforming Dice loss alone. This combination benefits from both pixel-wise probability optimization (BCE) and overlap maximization (Dice), resulting in more balanced segmentation performance across all metrics. Table 7 Selection experiment of loss function on the TN3K dataset Loss function Dice IoU Accuracy Mcc BCE 0.8339 0.7173 0.9631 0.8137 Dice 0.8588 0.7536 0.9684 0.8414 IoU 0.7759 0.6376 0.9407 0.7574 Tversky 0.7982 0.6673 0.9479 0.7802 Focal (0.1) + Tversky (0.9) 0.7822 0.6451 0.9412 0.7648 BCE (0.5) + Dice (0.5) 0.8618 0.7586 0.9698 0.8457 BCE, binary cross-entropy; IoU, intersection over union; Mcc, Matthews correlation coefficient; TN3K, thyroid nodule 3493 dataset. The effects of kernel sizes on the MFAM Table 8 presents the results of our experiments on the MFAM in the MFA-Net architecture, where we evaluate the performance of various kernel size configurations. Specifically, we compared the use of horizontal and vertical convolutions to standard convolutions across different kernel combinations: 3+5+7, 3+5+9, 3+5+11, 5+7+9, 5+7+11, and 7+9+11. Our analysis shows that the horizontal and vertical convolution configurations outperformed the standard convolution configurations in most cases (except for 7+9+11), and they perform better across all evaluated metrics. Notably, the combination of 5+7+11 kernels yielded the best performance across all configurations, achieving Dice of 0.8618, IoU of 0.7586, accuracy of 96.98%, and Mcc of 0.8457. This configuration showed a marked improvement over the standard convolution configurations, where the best result (from the 7+9+11 kernel size) yielded Dice of 0.8581, IoU of 0.7530, accuracy of 96.78%, and Mcc of 0.8405. In conclusion, the horizontal and vertical convolution kernel configurations, particularly with kernel sizes of 5+7+11, are more effective for improving the segmentation performance of the MFA-Net model, as compared to traditional standard convolution configurations. Table 8 The effects of kernel sizes on the TN3K dataset Convolution type Kernel size Dice IoU Accuracy Mcc 3+5+7 0.8511 0.7427 0.9664 0.8327 3+5+9 0.8525 0.7445 0.9672 0.8352 Standard convolution 3+5+11 0.8541 0.7472 0.9676 0.8367 5+7+9 0.8502 0.7415 0.9662 0.8316 5+7+11 0.8497 0.7409 0.9667 0.8320 7+9+11 0.8581 0.7530 0.9678 0.8405 3+5+7 0.8574 0.7521 0.9676 0.8397 3+5+9 0.8553 0.7488 0.9681 0.8382 Horizontal and vertical 3+5+11 0.8579 0.7528 0.9685 0.8408 5+7+9 0.8552 0.7484 0.9682 0.8387 5+7+11 0.8618 0.7586 0.9698 0.8457 7+9+11 0.8543 0.7476 0.9677 0.8367 IoU, intersection over union; Mcc, Matthews correlation coefficient; TN3K, thyroid nodule 3493 dataset. Computational efficiency Table 9 presents a comprehensive comparison of model complexity (in terms of parameter count), computational efficiency (measured by FPS), and computational load (measured by GFLOPs) across various thyroid nodule segmentation methods on the TN3K dataset. Among the models evaluated, HFENet stands out with the lowest number of parameters (only 0.15 million), the highest FPS of 233.49, and a minimal computational load of 1.47 GFLOPs, highlighting its exceptional efficiency for real-time applications. U-Net is one of the lightest architectures in this comparison, but compared with the newer models, it often lags behind in terms of segmentation accuracy. It has 1.94 million parameters, 227.29 FPS, and a 3.48 GFLOPs computational load. AMSUnet is another efficient model with a modest parameter size of 2.61 million and 84.18 FPS, offering a slightly slower inference speed than U-Net. It has 6.12 GFLOPs, reflecting a moderate computational burden. MSFCN has 14.17 million parameters, an FPS of 129.21, and a high GFLOPs of 55.80, indicating a high computational load for high-accuracy segmentation but a well-balanced design in terms of model complexity and inference speed. LANet features 23.79 million parameters, positioning it on the higher end in terms of model size. Despite this, it maintains a relatively high FPS of 121.72 and a moderate GFLOPs of 8.30, demonstrating efficient architectural optimization that allows for competitive processing speeds even with increased complexity. ERDUnet carries 10.21 million parameters, but its FPS drops to 39.59, and its GFLOPs of 10.29 indicate slower processing efficiency, making it more computationally intensive despite its moderate parameter count. MDA-Net has a significantly larger footprint with 29.84 million parameters, and a moderate FPS of 57.95, accompanied by GFLOPs of 45.80, indicating its high computational load. BSNet possesses the largest parameter count at 43.98 million, making it the heaviest model among those compared. Its FPS is the lowest, at only 25.39, with a GFLOPs of 45.80, highlighting its computational intensity. MFA-Net has 4.07 million parameters and a frame processing rate of 67.46 per second. This indicates that its running speed is moderate, with 8.78 floating-point operations per second, which keeps it in a balanced state in terms of computational load. Table 9 Parameters and computational efficiency of various models on the TN3K dataset Model Param (M) FPS GFLOPs HFENet 0.15 233.49 1.47 DESENet 1.15 45.35 3.17 U-Net 1.94 227.29 3.48 AMSUnet 2.61 84.18 6.12 NLIE-UNet 2.71 15.83 6.02 MFA-Net 4.07 67.46 8.78 ERDUnet 10.21 39.59 10.29 MSFCN 14.17 129.21 55.80 LANet 23.79 121.72 8.30 BSNet 43.98 25.39 45.80 BMANet 28.40 33.43 9.19 FPS, frame-per-second; GFLOPs, giga floating-point operations per second; MFA-Net, multi-scale feature aggregation network; TN3K, thyroid nodule 3493 dataset. Limitations Although the proposed MFA-Net achieves promising segmentation performance, several limitations remain. Firstly, our experiment was conducted on publicly available datasets, which may not fully encompass the diversity of real-world clinical data. Future work will involve collaborating with hospitals to validate our framework on larger and more diverse sample groups and using LLM-based annotation. Secondly, our current design is based on task-specific deep learning architectures, whereas recent advances in foundation models have demonstrated powerful generalization capabilities across tasks and modalities. Integrating MFA-Net with such pre-trained large-scale models may further enhance segmentation accuracy and reduce dependence on extensive labeled medical data. Thirdly, this study focuses primarily on single-modality imaging. In practice, integrating multimodal information, such as histology slides, genomic profiles, or clinical notes, could provide complementary insights that improve both segmentation precision and downstream clinical decision-making ( 46 ). Conclusions In this research, we introduce MFA-Net, an advanced deep learning framework meticulously designed for accurate and efficient segmentation of thyroid nodules. MFA-Net integrates three key components: MFAM, background-aware mechanism, and RDM. The MFAM enhances the network&#8217;s capacity to obtain fine details and contextual information across varying receptive fields, while BAM effectively suppresses background noise, guiding the network&#8217;s attention toward critical nodule regions. The RDM further strengthens decoding by preserving semantic continuity and refining boundary precision. Extensive experiments conducted on three datasets demonstrate that MFA-Net consistently surpasses a wide range of leading methods in terms of Dice coefficient and IoU value. Ablation studies further validate the individual contributions of each component, confirming their synergistic impact on performance enhancement. Additionally, MFA-Net achieves a favorable trade-off between accuracy and computational efficiency, making it a robust and practical solution for clinical applications. These findings affirm that MFA-Net delivers precise, stable, and scalable segmentation results, with strong generalization capability across diverse thyroid nodule patterns and imaging conditions. Supplementary The article&#8217;s supplementary files as 10.21037/qims-2025-1364 10.21037/qims-2025-1364 Acknowledgments We would like to sincerely thank our research collaborators Simon Fong and Yaoyang Wu from University of Macau for their assistance in data collection and the language editing of this manuscript. Reporting Checklist: The authors have completed the CLEAR reporting checklist. Available at https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/rc Funding: This work was supported by National Natural Science Foundation of China ( No. 62102227 ) and Joint Fund of Zhejiang Provincial Natural Science Foundation of China ( No. LZY24E060001 ). Conflicts of Interest: All authors have completed the ICMJE uniform disclosure form (available at https://qims.amegroups.com/article/view/10.21037/qims-2025-1364/coif ). The authors have no conflicts of interest to declare. Ethical Statement: The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. References 1 Lu W Zhang D Zhou W Wei W Wu X Ding W Zhang C . Diagnosis of thyroid nodules using ultrasound images based on deep learning features: online dynamic nomogram and gradient-weighted class activation mapping. Quant Imaging Med Surg 2025 ; 15 : 5689 - 702 . 10.21037/qims-2025-159 40606380 PMC12209616 2 Wang QG Li M Deng GX Huang HQ Qiu Q Lin JJ . Development and validation of a nomogram based on conventional and contrast-enhanced ultrasound for differentiating malignant from benign thyroid nodules. Quant Imaging Med Surg 2025 ; 15 : 4641 - 54 . 10.21037/qims-24-1796 40384666 PMC12082573 3 Ronneberger O, Fischer P, Brox T, editors. U-Net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells W, Frangi A, editors. Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015. Lecture Notes in Computer Science. Cham: Springer; 2015. 4 Tang Q Min S Shi X Zhang Q Liu Y. DESENet: A bilateral network with detail-enhanced semantic encoder for real-time semantic segmentation. Meas Sci Technol 2025 ; 36 : 015425 . 5 Wu Z Chen H Xiong X Wu S Li H Zhou X. BMANet: Boundary-guided multi-level attention network for polyp segmentation in colonoscopy images. Biomed Signal Process Control 2025 ; 105 : 107524 . 6 Wan L Song L Zhou Y Kang C Zheng S Chen G. Dynamic neighbourhood-enhanced UNet with interwoven fusion for medical image segmentation. Vis Comput 2025 ; 41 : 7703 - 21 . 7 Wu Y, Huang L, Yang T. Thyroid nodule ultrasound image segmentation based on improved Swin Transformer. IEEE Access 2025;13:19788-95. 8 Li X Fu C Wang Q Zhang W Ye C Ma T. GSE-Nets: Global structure enhancement decoder for thyroid nodule segmentation. Biomed Signal Process Control 2025 ; 102 : 107340 . 9 Hu M Zhang Y Xue H Lv H Han S. Mamba- and ResNet-Based Dual-Branch Network for Ultrasound Thyroid Nodule Segmentation. Bioengineering (Basel) 2024 ; 11 : 1047 . 10.3390/bioengineering11101047 39451422 PMC11504408 10 Ozcan A Tosun &#214; Donmez E Sanwal M. Enhanced-TransUNet for ultrasound segmentation of thyroid nodules. Biomed Signal Process Control 2024 ; 95 : 106472 . 11 Ali H Wang M Xie J. Cil-net: Densely connected context information learning network for boosting thyroid nodule segmentation using ultrasound images. Cogn Comput 2024 ; 16 : 1176 - 97 . 12 Zheng Z Liang E Zhang Y Weng Z Chai J Bu W Xu J Su T. A segmentation-based algorithm for classification of benign and malignancy Thyroid nodules with multi-feature information. Biomed Eng Lett 2024 ; 14 : 785 - 800 . 10.1007/s13534-024-00375-2 38946824 PMC11208362 13 Li W Tang YM Wang Z Yu KM To S . Atrous residual interconnected encoder to attention decoder framework for vertebrae segmentation via 3D volumetric CT images. Eng Appl Artif Intell 2022 ; 114 : 105102 . 14 Ji Z Ge Y Chukwudi C , U K, Zhang SM, Peng Y, Zhu J, Zaki H, Zhang X, Yang S, Wang X, Chen Y, Zhao J. Counterfactual Bidirectional Co-Attention Transformer for Integrative Histology-Genomic Cancer Risk Stratification. IEEE J Biomed Health Inform 2025 ; 29 : 5862 - 74 . 10.1109/JBHI.2025.3548048 40042950 15 Gu R Liu L . An agricultural leaf disease segmentation model applying multi-scale coordinate attention mechanism. Appl Soft Comput 2025 ; 172 : 112904 . 16 Ni JC Lee SH Shen YC Yang CS . Improved U-Net based on ResNet and SE-Net with dual attention mechanism for glottis semantic segmentation. Med Eng Phys 2025 ; 136 : 104298 . 10.1016/j.medengphy.2025.104298 39979012 17 Shang X Wu S Liu Y Zhao Z Wang S. PVT-MA: Pyramid vision transformers with multi-attention fusion mechanism for polyp segmentation. Appl Intell 2025 ; 55 : 17 . 18 Qi K Yan C Niu D Zhang B Liang D Long X. MG-Net: A fetal brain tissue segmentation method based on multiscale feature fusion and graph convolution attention mechanisms. Comput Methods Programs Biomed 2024 ; 257 : 108451 . 10.1016/j.cmpb.2024.108451 39395303 19 Wu J, Fu R, Fang H, Zhang Y, Yang Y, Xiong H, Liu H, Xu Y. MedSegDiff: Medical image segmentation with diffusion probabilistic model. arXiv:2211.00611 [Preprint]. Available online: https://arxiv.org/abs/2211.00611 20 Liu X Liang J Zhang J Qian Z Xing P Chen T Yang S Chukwudi C Qiu L Liu D Zhao J . Advancing hierarchical neural networks with scale-aware pyramidal feature learning for medical image dense prediction. Comput Methods Programs Biomed 2025 ; 265 : 108705 . 10.1016/j.cmpb.2025.108705 40184852 21 Zhang L Xu C Li Y Liu T Sun J. MCSE-U-Net: multi-convolution blocks and squeeze and excitation blocks for vessel segmentation. Quant Imaging Med Surg 2024 ; 14 : 2426 - 40 . 10.21037/qims-23-1454 38545081 PMC10963822 22 Jiang S Chen X Yi C. SSA-UNet: Whole brain segmentation by U-Net with squeeze-and-excitation block and self-attention block from the 2.5D slice image. IET Image Process 2024 ; 18 : 1598 - 612 . 23 Klomp SR Wijnhoven RG de With PH . Performance-efficiency comparisons of channel attention modules for resnets. Neural Process Lett 2023 ; 55 : 6797 - 813 . 24 Shan X Shen Y Cai H Wen Y . Convolutional neural network optimization via channel reassessment attention module. Digital Signal Processing 2022 ; 123 : 103408 . 25 Lin C Hu X Zhan Y Hao X. MobileNetV2 with Spatial Attention module for traffic congestion recognition in surveillance images. Expert Syst Appl 2024 ; 255 : 124701 . 26 Hu D Fang Y Cao J Jiang T Gao F . An end-to-end vision-based seizure detection with a guided spatial attention module for patient detection. IEEE Internet Things J 2024 ; 11 : 18869 - 79 . 27 Jiang L Hui Y Fei Y Ji Y Zeng T . Improving polyp segmentation with boundary-assisted guidance and cross-scale interaction fusion transformer network. Processes 2024 ; 12 : 1030 . 28 Luo H Zhou D Cheng Y Wang S. MPEDA-Net: A lightweight brain tumor segmentation network using multi-perspective extraction and dense attention. Biomed Signal Process Control 2024 ; 91 : 106054 . 29 Gong H Chen J Chen G Li H Li G Chen F . Thyroid region prior guided attention for ultrasound segmentation of thyroid nodules. Comput Biol Med 2023 ; 155 : 106389 . 10.1016/j.compbiomed.2022.106389 36812810 30 BrainTumor dataset. Available online: https://figshare.com/articles/dataset/brain_tumor_dataset/1512427 31 Selvaraj A Nithiyaraj E. CEDRNN: A convolutional encoder-decoder residual neural network for liver tumour segmentation. Neural Process Lett 2023 ; 55 : 1605 - 24 . 32 Li Y Zhang Y Liu JY Wang K Zhang K Zhang GS Liao XF Yang G . Global Transformer and Dual Local Attention Network via Deep-Shallow Hierarchical Feature Fusion for Retinal Vessel Segmentation. IEEE Trans Cybern 2023 ; 53 : 5826 - 39 . 10.1109/TCYB.2022.3194099 35984806 33 Sun S Fu C Xu S Wen Y Ma T. GLFNet: Global-local fusion network for the segmentation in ultrasound images. Comput Biol Med 2024 ; 171 : 108103 . 10.1016/j.compbiomed.2024.108103 38335822 34 Yuan Y Yang L Chang K Huang Y Yang H Wang J. DSCA-PSPNet: Dynamic spatial-channel attention pyramid scene parsing network for sugarcane field segmentation in satellite imagery. Front Plant Sci 2023 ; 14 : 1324491 . 10.3389/fpls.2023.1324491 38298601 PMC10829042 35 Yang L Dong Q Lin D Tian C L&#252; X. MUNet: a novel framework for accurate brain tumor segmentation combining UNet and mamba networks. Front Comput Neurosci 2025 ; 19 : 1513059 . 10.3389/fncom.2025.1513059 39944950 PMC11814164 36 Hu M Dong Y Li J Jiang L Zhang P Ping Y. LAMFFNet: Lightweight adaptive multi-layer feature fusion network for medical image segmentation. Biomed Signal Process Control 2025 ; 103 : 107456 . 37 Rainio O Teuho J Kl&#233;n R . Evaluation metrics and statistical tests for machine learning. Sci Rep 2024 ; 14 : 6086 . 10.1038/s41598-024-56706-x 38480847 PMC10937649 38 Zhu Q. On the performance of matthews correlation coefficient (Mcc) for imbalanced dataset. Pattern Recognit Lett 2020 ; 136 : 71 - 80 . 39 Cong R Zhang Y Yang N Li H Zhang X Li R Chen Z Zhao Y Kwong S . Boundary guided semantic learning for real-time COVID-19 lung infection segmentation system. IEEE Transactions on Consumer Electronics 2022 ; 68 : 376 - 86 . 40 Iqbal A Sharif M. MDA-Net: Multiscale dual attention-based network for breast lesion segmentation using ultrasound images. J King Saud Univ Comput Inf Sci 2022 ; 34 : 7283 - 99 . 41 Lu F Zhang Z Guo L Chen J Zhu Y Yan K Zhou X. HFENet: A lightweight hand&#8208;crafted feature enhanced CNN for ceramic tile surface defect detection. Int J Intell Syst 2022 ; 37 : 10670 - 93 . 42 Li R Zheng S Duan C Wang L Zhang C . Land cover classification from remote sensing images based on multi-scale fully convolutional network. Geo Spat Inf Sci 2022 ; 25 : 278 - 94 . 43 Li H Zhai DH Xia Y . ERDUnet: An efficient residual double-coding unet for medical image segmentation. IEEE Trans Circuits Syst Video Technol 2024 ; 34 : 2083 - 96 . 44 Ding L Tang H Bruzzone L. LANet: Local attention embedding to improve the semantic segmentation of remote sensing images. IEEE Trans Geosci Remote Sens 2021 ; 59 : 426 - 35 . 45 Yin Y Han Z Jian M Wang GG Chen L Wang R . AMSUnet: A neural network using atrous multi-scale convolution for medical image segmentation. Comput Biol Med 2023 ; 162 : 107120 . 10.1016/j.compbiomed.2023.107120 37276753 46 Ong HT Karatas E Poquillon T Grenci G Furlan A Dilasser F Mohamad Raffi SB Blanc D Drimaracci E Mikec D Galisot G Johnson BA Liu AZ Thiel C Ullrich O ; OrgaRES Consortium; Racine V, Beghin A. Digitalized organoids: integrated pipeline for high-speed 3D analysis of organoid structures using multilevel segmentation and cellular topology. Nat Methods 2025 ; 22 : 1343 - 54 . 10.1038/s41592-025-02685-4 40369245 PMC12165853"
}