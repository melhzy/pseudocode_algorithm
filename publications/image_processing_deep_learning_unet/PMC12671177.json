{
  "pmcid": "PMC12671177",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.337065",
  "metadata": {
    "journal_title": "Journal of Biomedical Optics",
    "journal_nlm_ta": "J Biomed Opt",
    "journal_iso_abbrev": "J Biomed Opt",
    "journal": "Journal of Biomedical Optics",
    "pmcid": "PMC12671177",
    "pmid": "41341649",
    "doi": "10.1117/1.JBO.30.12.126001",
    "title": "Rolling shutter-resistant confocal endomicroscopy image stitching via dual-path Gaussian U-Net",
    "year": "2025",
    "month": "12",
    "day": "2",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "2"
    },
    "authors": [
      "Lu Yuhua",
      "Chen Shangbin",
      "Liu Qian"
    ],
    "abstract": "Abstract. Significance Confocal endomicroscopic image stitching can expand the field of view and improve examination efficiency. However, due to interference from the rolling shutter effect, traditional stitching methods may produce misalignments, leading to structural distortion and artifacts. Suppressing the rolling shutter effect in confocal endomicroscopic images can effectively enhance stitching quality. Aim We propose a Dual-Path Gaussian U-Net (DGU-Net)-based framework for confocal endomicroscopic image stitching. The parallel dual-encoder paths of DGU-Net extract Gaussian features and conventional features at different resolutions, respectively, achieving more precise gland segmentation masks. Based on these masks, we filter stable frames and optimize feature matching to effectively suppress rolling shutter interference and improve stitching quality. Approach We annotated a segmentation dataset comprising 80 rat confocal laser endomicroscopy (CLE) images to train the segmentation network and validated the frame selection methodâ€™s effectiveness in suppressing the rolling shutter effect on consecutively acquired rat CLE video sequences. The stitching results generated from the filtered stable image sequences were compared with conventional methods. Results Experimental results demonstrate that DGU-Net achieves superior performance with a Dice score of 85.17 on CLE datasets, significantly outperforming existing segmentation networks. Compared with Auto-Stitching, our method improves regional consistency across the panoramic image by eliminating artifacts caused by mismatches while delivering enhanced stitching accuracy and image quality. Conclusions The proposed method effectively accomplishes confocal image stitching tasks, significantly enhancing endomicroscopic examination efficiency and contributing to improved diagnostic outcomes.",
    "keywords": [
      "confocal endomicroscopy",
      "image stitching",
      "U-Net",
      "rolling shutter effect"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">J Biomed Opt</journal-id><journal-id journal-id-type=\"iso-abbrev\">J Biomed Opt</journal-id><journal-id journal-id-type=\"pmc-domain-id\">953</journal-id><journal-id journal-id-type=\"pmc-domain\">jbiomedopt</journal-id><journal-id journal-id-type=\"publisher-id\">JBO</journal-id><journal-title-group><journal-title>Journal of Biomedical Optics</journal-title></journal-title-group><issn pub-type=\"ppub\">1083-3668</issn><issn pub-type=\"epub\">1560-2281</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12671177</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12671177.1</article-id><article-id pub-id-type=\"pmcaid\">12671177</article-id><article-id pub-id-type=\"pmcaiid\">12671177</article-id><article-id pub-id-type=\"pmid\">41341649</article-id><article-id pub-id-type=\"doi\">10.1117/1.JBO.30.12.126001</article-id><article-id pub-id-type=\"publisher-id\">250104GRRR</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Imaging</subject></subj-group><subj-group subj-group-type=\"SPIE-art-type\"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Rolling shutter-resistant confocal endomicroscopy image stitching via dual-path Gaussian U-Net</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0009-0005-3217-048X</contrib-id><name name-style=\"western\"><surname>Lu</surname><given-names initials=\"Y\">Yuhua</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">a</xref><email>lyh99sz@gmail.com</email></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0001-5773-0360</contrib-id><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"S\">Shangbin</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">a</xref><email>sbchen@hust.edu.cn</email></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0002-8398-1021</contrib-id><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"Q\">Qian</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">b</xref><xref rid=\"cor1\" ref-type=\"corresp\">*</xref><email>qliu@hainanu.edu.cn</email></contrib><aff id=\"aff1\"><label>a</label><institution>Huazhong University of Science and Technology</institution>, Wuhan National Laboratory for Optoelectronics, Wuhan, <country>China</country></aff><aff id=\"aff2\"><label>b</label><institution>Hainan University</institution>, School of Biomedical Engineering, Key Laboratory of Biomedical Engineering of Hainan Province, Hainan, <country>China</country></aff></contrib-group><author-notes><corresp id=\"cor1\"><label>*</label>Address all correspondence to Qian Liu, <email>qliu@hainanu.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>2</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"ppub\"><month>12</month><year>2025</year></pub-date><volume>30</volume><issue>12</issue><issue-id pub-id-type=\"pmc-issue-id\">494472</issue-id><elocation-id>126001</elocation-id><history><date date-type=\"received\"><day>22</day><month>4</month><year>2025</year></date><date date-type=\"rev-recd\"><day>20</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>24</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>02</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-04 09:25:13.720\"><day>04</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Authors</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"JBO-030-126001.pdf\"/><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:title=\"pdf\" xlink:href=\"JBO_30_12_126001.pdf\"/><abstract><title>Abstract.</title><sec><title>Significance</title><p>Confocal endomicroscopic image stitching can expand the field of view and improve examination efficiency. However, due to interference from the rolling shutter effect, traditional stitching methods may produce misalignments, leading to structural distortion and artifacts. Suppressing the rolling shutter effect in confocal endomicroscopic images can effectively enhance stitching quality.</p></sec><sec><title>Aim</title><p>We propose a Dual-Path Gaussian U-Net (DGU-Net)-based framework for confocal endomicroscopic image stitching. The parallel dual-encoder paths of DGU-Net extract Gaussian features and conventional features at different resolutions, respectively, achieving more precise gland segmentation masks. Based on these masks, we filter stable frames and optimize feature matching to effectively suppress rolling shutter interference and improve stitching quality.</p></sec><sec><title>Approach</title><p>We annotated a segmentation dataset comprising 80 rat confocal laser endomicroscopy (CLE) images to train the segmentation network and validated the frame selection method&#8217;s effectiveness in suppressing the rolling shutter effect on consecutively acquired rat CLE video sequences. The stitching results generated from the filtered stable image sequences were compared with conventional methods.</p></sec><sec><title>Results</title><p>Experimental results demonstrate that DGU-Net achieves superior performance with a Dice score of 85.17 on CLE datasets, significantly outperforming existing segmentation networks. Compared with Auto-Stitching, our method improves regional consistency across the panoramic image by eliminating artifacts caused by mismatches while delivering enhanced stitching accuracy and image quality.</p></sec><sec><title>Conclusions</title><p>The proposed method effectively accomplishes confocal image stitching tasks, significantly enhancing endomicroscopic examination efficiency and contributing to improved diagnostic outcomes.</p></sec></abstract><kwd-group><title>Keywords:</title><kwd>confocal endomicroscopy</kwd><kwd>image stitching</kwd><kwd>U-Net</kwd><kwd>rolling shutter effect</kwd></kwd-group><funding-group><award-group id=\"sp1\"><funding-source>National Key Research and Development Program of China</funding-source><award-id>2022YFC2404401</award-id></award-group><award-group id=\"sp2\"><funding-source>National Natural Science Foundation of China</funding-source><award-id>62475063</award-id></award-group><award-group id=\"sp3\"><funding-source>Open Project Program of Wuhan National Laboratory for Optoelectronics</funding-source><award-id>2023WNLOKF013</award-id></award-group><funding-statement>This work was supported by the National Key Research and Development Program of China (Grant No. 2022YFC2404401), National Natural Science Foundation of China (Grant No. 62475063), and Open Project Program of Wuhan National Laboratory for Optoelectronics (Grant No. 2023WNLOKF013).</funding-statement></funding-group><counts><fig-count count=\"9\"/><table-count count=\"10\"/><ref-count count=\"50\"/><page-count count=\"16\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>running-head</meta-name><meta-value>Lu, Chen, and Liu: Rolling shutter-resistant confocal endomicroscopy image stitching&#8230;</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"sec1\"><label>1</label><title>Introduction</title><p>Colorectal cancer, gastric cancer, and esophageal cancer are all major threats to human health. They account for 16.1% of all new cancer cases and 20.7% cancer-related deaths.<xref rid=\"r1\" ref-type=\"bibr\"><sup>1</sup></xref> Early diagnosis is crucial, as 5-year survival rates for early-stage cancers are over 90% compared with less than 20% for advanced carcinomas. Endomicroscopic biopsies remain the gold standard in confirming neoplastic tumors and staging progression. However, their invasive nature and delayed feedback contribute to a non-negligible rate of missed diagnoses and false negatives. Confocal laser endomicroscopy (CLE), which allows real-time visualization at the micron level of mucosal cellular structures and subcellular structures, overcomes these limitations. It facilitates precise lesion location and &#8220;optical biopsy&#8221; capabilities.<xref rid=\"r2\" ref-type=\"bibr\"><sup>2</sup></xref><named-content content-type=\"online\"><xref rid=\"r3\" ref-type=\"bibr\"/><xref rid=\"r4\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r5\" ref-type=\"bibr\"><sup>5</sup></xref></p><p>Confocal endomicroscopic image stitching can significantly expand the examination field of view, enabling clinicians to more comprehensively assess lesion areas, thereby improving diagnostic efficiency and accuracy.<xref rid=\"r6\" ref-type=\"bibr\"><sup>6</sup></xref> However, due to the progressive line-scanning mechanism of CLE,<xref rid=\"r2\" ref-type=\"bibr\"><sup>2</sup></xref> probe movement during imaging introduces rolling shutter effects. As shown in <xref rid=\"f1\" ref-type=\"fig\">Fig.&#160;1</xref>, in adjacent frames, nonsynchronous imaging of different regions causes nonuniform deformation and stretching of glands at different positions. These distortions lead to parallax artifacts and feature matching errors during stitching, significantly compromising final image quality.<xref rid=\"r7\" ref-type=\"bibr\"><sup>7</sup></xref><sup>,</sup><xref rid=\"r8\" ref-type=\"bibr\"><sup>8</sup></xref> Therefore, developing effective rolling shutter suppression algorithms is crucial for enhancing the clinical utility of confocal endomicroscopic image stitching.</p><fig position=\"float\" id=\"f1\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Rolling shutter effects inducing inconsistent distortions between adjacent frames: (a)&#160;nonuniform angular displacement in frames 36 (left) and 37 (middle). Single-headed yellow arrows show angular changes of glandular structures relative to a common reference point. The histogram (right) displays angular differences for glandular structures between the two frames; (b)&#160;differential glandular deformation in frames 43 (left) and 44 (middle). Double-headed arrows indicate length variations of glandular structures. The histogram (right) displays length differences for glandular structures between the two frames.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g001.jpg\"/></fig><p>Traditional image stitching methods typically rely on feature point matching and geometric transformations. Brown and Lowe<xref rid=\"r9\" ref-type=\"bibr\"><sup>9</sup></xref><sup>,</sup><xref rid=\"r10\" ref-type=\"bibr\"><sup>10</sup></xref> pioneered Scale-Invariant Feature Transform (SIFT)&#8211;based global homography alignment for panoramic image generation in computational image stitching, laying the foundation for commercial applications. Subsequent research focused on hardware advancements and feature descriptor optimizations, including Principal Component Analysis-SIFT (PCA-SIFT),<xref rid=\"r11\" ref-type=\"bibr\"><sup>11</sup></xref> Speeded-Up Robust Features (SURF),<xref rid=\"r12\" ref-type=\"bibr\"><sup>12</sup></xref> and Features from Accelerated Segment Test (FAST).<xref rid=\"r13\" ref-type=\"bibr\"><sup>13</sup></xref> However, the global unresponsive matrix alignment method based on feature point matching requires that the optical centers of the images involved in the stitching coincide; otherwise, parallax will occur to fail to align the pictures. To solve this problem, Gao et&#160;al.<xref rid=\"r14\" ref-type=\"bibr\"><sup>14</sup></xref> introduced dual-plane homography by separating scenes into foreground and background layers. Further, Lin et&#160;al.<xref rid=\"r15\" ref-type=\"bibr\"><sup>15</sup></xref> rely on a plurality of affine transformations to align the pictures, giving the image stitching some ability to handle parallax. Zaragoza et&#160;al.<xref rid=\"r16\" ref-type=\"bibr\"><sup>16</sup></xref> proposed dense grid-based homography adaptation to mitigate parallax and distortion artifacts. Zhang et&#160;al.<xref rid=\"r17\" ref-type=\"bibr\"><sup>17</sup></xref> improved the stitching performance of large parallax scenes based on a video de-jittering method and seam line domination. Gao et&#160;al.<xref rid=\"r18\" ref-type=\"bibr\"><sup>18</sup></xref> estimated the optimal geometric transformation using the seam-cut method. Li et&#160;al.<xref rid=\"r19\" ref-type=\"bibr\"><sup>19</sup></xref> obtained the optimal seam line using automatic quaternions.</p><p>The advent of deep learning catalyzed paradigm shifts in image stitching, with convolutional neural networks (CNNs) being progressively adapted for this domain. Yi et&#160;al.<xref rid=\"r20\" ref-type=\"bibr\"><sup>20</sup></xref><named-content content-type=\"online\"><xref rid=\"r21\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r22\" ref-type=\"bibr\"><sup>22</sup></xref> pioneered LIFT-Net, a tripartite neural framework comprising a dedicated subnetwork for feature point detection, orientation estimation, and descriptor generation. Subsequently, Han et&#160;al.<xref rid=\"r23\" ref-type=\"bibr\"><sup>23</sup></xref> developed MatchNet, which employs a dual-tower architecture for feature extraction, followed by similarity computation through three cascaded fully connected layers. DeTone et&#160;al.<xref rid=\"r24\" ref-type=\"bibr\"><sup>24</sup></xref> introduced HomographyNet, leveraging VGG-inspired architectures to regress the eight parameters of homography matrices directly. Simultaneously, Nie et&#160;al.<xref rid=\"r25\" ref-type=\"bibr\"><sup>25</sup></xref> proposed a rectangling network to eliminate irregular boundaries caused by homography transformations. Jiang et&#160;al. integrated adversarial attacks into the stitching pipeline to enhance robustness, whereas Cai et&#160;al.<xref rid=\"r26\" ref-type=\"bibr\"><sup>26</sup></xref> incorporated geometric constraints for natural image stitching applications. However, most of these methods rely on the stability of feature point detection without fully considering the impact of rolling shutter effects on feature detection.</p><p>In endomicroscopic image stitching, Vercauteren et&#160;al.<xref rid=\"r6\" ref-type=\"bibr\"><sup>6</sup></xref> pioneered diffeomorphic modeling for confocal image reconstruction, which was subsequently integrated into the commercial CLE system developed by MKT. Building upon this imaging platform, Gong et&#160;al.<xref rid=\"r27\" ref-type=\"bibr\"><sup>27</sup></xref> proposed a context-aware registration algorithm (CWCR) that combines feature-based matching with template-based rigid alignment to achieve global frame registration. Kose et&#160;al.<xref rid=\"r28\" ref-type=\"bibr\"><sup>28</sup></xref> implemented video stitching through SIFT feature extraction and graph-cut optimization. Rosa et&#160;al.<xref rid=\"r29\" ref-type=\"bibr\"><sup>29</sup></xref> developed a servo motor-controlled probe positioning system, where image registration was guided by precise motor position feedback. Bedard et&#160;al.<xref rid=\"r30\" ref-type=\"bibr\"><sup>30</sup></xref> introduced a cost-effective discrete Fourier transform-based approach for frame alignment in a custom high-resolution endomicroscopic system. In gastroscopic applications, Zhou et&#160;al.<xref rid=\"r31\" ref-type=\"bibr\"><sup>31</sup></xref> achieved real-time stitching through stable frame selection combined with SURF feature matching. Zenteno et&#160;al.<xref rid=\"r32\" ref-type=\"bibr\"><sup>32</sup></xref> enhanced stitching robustness by employing optical flow-based image selection to minimize texture discontinuities. These methodologies demonstrate systematic optimization of stitching algorithms tailored to specific endomicroscopic imaging characteristics.</p><p>In this paper, we propose a novel stitching technique for the gastrointestinal CLE imaging system developed by our research group.<xref rid=\"r2\" ref-type=\"bibr\"><sup>2</sup></xref> We developed a Dual-Path Gaussian U-Net (DGU-Net) that extracts Gaussian and non-Gaussian features at different resolutions through its dual-path mechanism, enabling better perception of textural features in gastrointestinal CLE images for more accurate segmentation mask generation. The segmentation masks are then used to filter stable frames from the image sequence and extract foreground features, thereby suppressing rolling shutter effects. Finally, we utilize the foreground SIFT features of the selected stable frame sequence to generate stitched images. The main contributions to this work can be summarized as follows:</p><list list-type=\"simple\"><list-item><label>1.</label><p>Constructed a meticulously annotated segmentation dataset comprising 80 gastrointestinal CLE images and developed a dedicated gland segmentation network for digestive endomicroscopy</p></list-item><list-item><label>2.</label><p>Developed a segmentation mask-based frame selection method that identifies stable frames in gastrointestinal CLE image sequences, effectively suppressing rolling shutter effects</p></list-item><list-item><label>3.</label><p>Successfully implemented gastrointestinal confocal stitching using our proposed method. Comparative experiments were conducted with conventional methods to demonstrate the superiority of our approach.</p></list-item></list><p>The remainder of this paper is organized as follows: Section&#160;<xref rid=\"sec2\" ref-type=\"sec\">2</xref> details the proposed methodology. Section&#160;<xref rid=\"sec3\" ref-type=\"sec\">3</xref> presents experimental validation and results. Section&#160;<xref rid=\"sec4\" ref-type=\"sec\">4</xref> discusses limitations and future directions. Section&#160;<xref rid=\"sec5\" ref-type=\"sec\">5</xref> concludes by summarizing key contributions and highlighting potential clinical applications of the proposed framework.</p></sec><sec id=\"sec2\"><label>2</label><title>Materials and Methods</title><p>The CLE imaging system employs a galvanometer-based <inline-formula><mml:math id=\"math1\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> scanning mechanism with counter-rotating mirrors for row-wise acquisition.<xref rid=\"r2\" ref-type=\"bibr\"><sup>2</sup></xref> Each frame is sequentially captured by scanning individual rows while progressively advancing columns along the <inline-formula><mml:math id=\"math2\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>-axis. Probe movement during this scanning process generates nonuniform spatial displacements&#8212;particularly evident in glandular structures&#8212;manifesting as rolling shutter effects.</p><p>We developed a novel image stitching framework based on a gastrointestinal gland segmentation network, as shown in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2</xref>, to suppress rolling shutter effects and improve stitching accuracy.</p><fig position=\"float\" id=\"f2\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Confocal endomicroscopic image stitching pipeline, consisting of three key stages: (1)&#160;a gland segmentation network trained to generate binary masks isolating diagnostically relevant regions, (2)&#160;stable frame selection by minimizing inter-frame projection differences using segmentation masks, and (3)&#160;SIFT descriptors extracted exclusively from stabilized foreground regions enable robust feature matching and seamless image alignment.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g002.jpg\"/></fig><p><bold>First</bold>, we trained a gland segmentation network on CLE images to generate binary masks isolating glandular foregrounds from background interference. <bold>Subsequently</bold>, these masks guided frame stabilization through temporal consistency analysis, where sequential frames were aligned using mask-based projective differences to compensate for rolling shutter effects. <bold>Finally</bold>, SIFT descriptors were exclusively extracted from stabilized glandular regions to enable robust feature matching, followed by multi-band blending to generate seamless panoramic images.</p><sec id=\"sec2.1\"><label>2.1</label><title>Segmentation Network</title><p>As shown in <xref rid=\"f3\" ref-type=\"fig\">Fig.&#160;3</xref>, we designed a DGU-Net that integrates Gaussian smoothing and dual-path mechanisms to enhance feature extraction and segmentation performance.<xref rid=\"r33\" ref-type=\"bibr\"><sup>33</sup></xref><named-content content-type=\"online\"><xref rid=\"r34\" ref-type=\"bibr\"/><xref rid=\"r35\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r36\" ref-type=\"bibr\"><sup>36</sup></xref> The dual-path mechanism processes features through two parallel streams, which are later fused to capture both local texture details and global contextual information.</p><fig position=\"float\" id=\"f3\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Architecture of the Dual-Path Gaussian U-Net with <inline-formula><mml:math id=\"math3\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>892</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>892</mml:mn><mml:mrow><mml:mtext>&#8201;&#8201;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pixel</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> input and output dimensions. Blue blocks represent DoubleConv layers with <inline-formula><mml:math id=\"math4\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> convolutional kernels, whereas yellow blocks denote GaussDoubleConv layers incorporating a <inline-formula><mml:math id=\"math5\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>9</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:math></inline-formula> Gaussian convolutional kernel before the DoubleConv layer. Black arrows indicate data flow, green arrows represent downsampling layers, red arrows denote upsampling layers, and blue arrows signify concatenation operations.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g003.jpg\"/></fig><sec id=\"sec2.1.1\"><label>2.1.1</label><title>Gaussian convolution</title><p>The proposed architecture relies on Gaussian convolutional layers. These layers apply a fixed Gaussian kernel to the input feature map, smoothing the data while maintaining structural information. The 2D Gaussian kernel is defined as a size: <disp-formula id=\"e001\"><mml:math id=\"math6\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mtext>&#8201;</mml:mtext><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(1)</label></disp-formula>where <inline-formula><mml:math id=\"math7\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the spatial coordinates, <inline-formula><mml:math id=\"math8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:math></inline-formula> is the mean (centered at the kernel&#8217;s midpoint), and <inline-formula><mml:math id=\"math9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:math></inline-formula> is the standard deviation. The kernel is normalized to ensure that the sum of its weights equals one.</p></sec><sec id=\"sec2.1.2\"><label>2.1.2</label><title>Encoder</title><p>After initial Gaussian convolution preprocessing, feature maps enter the five-stage progressive encoder structure. The first path applies Gaussian convolution before the initial downsampling, whereas the second path maintains original feature processing until the fifth downsampling stage, extracting both Gaussian and non-Gaussian features at different resolutions. Feature concatenation occurs at the fourth and fifth downsampling levels of both paths, enabling fusion of local details with global features. This design preserves U-Net&#8217;s multi-scale extraction advantages while enhancing feature diversity through differentiated dual-path processing.</p></sec><sec id=\"sec2.1.3\"><label>2.1.3</label><title>Decoder</title><p>The decoder mirrors the encoder&#8217;s structural design. The first upsampling path employs Gaussian convolution in its initial upsampling, whereas the second path applies Gaussian processing at the fourth upsampling stage&#8212;balancing detail preservation with feature smoothing to ensure edge continuity in final outputs. Following feature concatenation of both paths, a final upsampling restores feature map dimensions before two convolutional layers generate the segmentation mask.</p></sec></sec><sec id=\"sec2.2\"><label>2.2</label><title>Stable Frame Selection</title><p>Clinical CLE examinations follow a &#8220;slow translation-brief pause&#8221; model where operators intermittently move and stabilize the probe. This generates alternating sequences of roll shutter-distorted frames and stable frames. Stabilizing frame selection is critical for mitigating rolling shutter-induced distortions in image stitching.</p><p>The gland segmentation network was employed to generate binary masks for each frame. Structural variations were quantified through row-wise and column-wise projection analysis of these masks: The row projection <inline-formula><mml:math id=\"math10\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"normal\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> and column projection <inline-formula><mml:math id=\"math11\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> are defined as <disp-formula id=\"e002\"><mml:math id=\"math12\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"normal\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula><disp-formula id=\"e003\"><mml:math id=\"math13\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munderover><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula><inline-formula><mml:math id=\"math14\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math15\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> denote image width and height, respectively, and represent the binary segmentation mask.</p><p>The projection difference metric <inline-formula><mml:math id=\"math16\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> between consecutive frames was computed as <disp-formula id=\"e004\"><mml:math id=\"math17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:munderover><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(4)</label></disp-formula>where <inline-formula><mml:math id=\"math18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math19\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> denote row projections of frames <inline-formula><mml:math id=\"math20\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math21\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, respectively, whereas <inline-formula><mml:math id=\"math22\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math23\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> represent their column projections. Frames are classified as stable if <inline-formula><mml:math id=\"math24\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, all are below an empirically determined threshold and discarded as motion-corrupted frames. This value corresponds to the optimal separation point that maximizes discrimination between lower-value clusters (indicating stable frames) and higher-value clusters (indicating motion-corrupted frames) in the characteristic bimodal distribution. In this work, this threshold was set to 0.8.</p></sec><sec id=\"sec2.3\"><label>2.3</label><title>Image Stitching</title><p>The stabilized frames and their foreground features were processed through the following pipeline.</p><sec id=\"sec2.3.1\"><label>2.3.1</label><title>Feature extraction</title><p>Mask-constrained SIFT features were extracted by filtering keypoints using gland segmentation masks. Specifically, a binary mask is used to constrain the SIFT feature extraction process. Only keypoints located within the foreground region (where the mask value is 1) are retained, whereas keypoints in the background region are discarded. This ensures that the extracted features are relevant to the foreground objects and significantly reduces the impact of background clutter on the stitching process. SIFT descriptors (128-dimensional feature vectors encoding local gradient histograms) are computed at all detected keypoints. Subsequently, the set of filtered keypoints <inline-formula><mml:math id=\"math25\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mtext>filtered</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is defined as <disp-formula id=\"e005\"><mml:math id=\"math26\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mtext>filtered</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">{</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">|</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(5)</label></disp-formula>where <inline-formula><mml:math id=\"math27\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> are the coordinates of the keypoints detected by SIFT, <inline-formula><mml:math id=\"math28\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> are the keypoints that are within the segmentation masks. This filtering step ensures that only keypoints within the foreground region are considered for further processing.</p></sec><sec id=\"sec2.3.2\"><label>2.3.2</label><title>Feature matching</title><p>After extracting all features from the images, we use the FLANN-based matching algorithm and Lowe&#8217;s Ratio Test to find correspondences. The ratio test filters ambiguous matches out by retaining only matches where the distance between the best and second best matches is below a threshold (ratio = 0.75). This step produces reliable feature correspondences for image pairs that are used to estimate geometric transformations between images.</p></sec><sec id=\"sec2.3.3\"><label>2.3.3</label><title>Homography estimation</title><p>We estimate the homography matrix for each image pair using the matched feature points. The homography matrix represents the geometric transformation (e.g., rotation, translation, scaling) that aligns one image with another. Random Sample Consensus (RANSAC) is used to handle feature matches. RANSAC selects a subset, computes the homography, and evaluates its consistency. The homography that has the most inliers will be chosen as the final transform. This robust estimation ensures that alignment is accurate even when features are noisy or mismatched.</p></sec><sec id=\"sec2.3.4\"><label>2.3.4</label><title>Gain compensation</title><p>We apply a gain-compensation algorithm to address brightness inconsistencies between images. The algorithm minimizes differences in intensity between overlapping regions of image pairs. It solves a system of linear equations to calculate a gain factor per image. The gain factors are then applied to the images to ensure seamless blending of the final panorama. This step is crucial for producing visually consistent results.</p></sec><sec id=\"sec2.3.5\"><label>2.3.5</label><title>Multi-band blending</title><p>Finally, the aligned pictures are blended together into a single panoramic image using a multi-band blend. This technique divides each image using Gaussian Pyramids into multiple frequency bands and blends them separately. The low-frequency band is blended over a large spatial range to smooth out color and brightness differences, whereas the high-frequency band is blended over a small range to preserve fine detail. The blended bands are recombined to create the final panorama. This ensures that the final result will be visually seamless and full of detail.</p></sec></sec><sec id=\"sec2.4\"><label>2.4</label><title>Dataset</title><p>A custom fiber-optic CLE probe was used to image excised rat stomach tissues with DSS-induced ulcerative gastritis. System specifications included a 488-nm excitation laser, an intravenous fluorescein sodium contrast agent, <inline-formula><mml:math id=\"math29\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1.4</mml:mn><mml:mtext>-</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:math></inline-formula> lateral resolution, <inline-formula><mml:math id=\"math30\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>512.5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>512.5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:math></inline-formula> FOV, five fps acquisition rate, and <inline-formula><mml:math id=\"math31\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>892</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>892</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>px</mml:mi></mml:mrow></mml:math></inline-formula> image size. The dataset comprises 400 frames acquired from two rats (200 frames per rat). From these, 80 images were randomly selected for segmentation network annotation, with multiple glandular structures annotated in each image. Ground truth segmentation masks were meticulously annotated by two experienced researchers under the supervision of a senior endoscopist. All imaging experiments were approved by the animal experiment guidelines of the Animal Experimentation Ethics Committee of Huazhong University of Science and Technology (HUST, Wuhan, China).</p></sec></sec><sec id=\"sec3\"><label>3</label><title>Result</title><sec id=\"sec3.1\"><label>3.1</label><title>Segmentation Performance</title><p>In this section, experimental results are presented of our <bold>DualPath Gaussian UNet (Ours)</bold>, which we have proposed for gland segmentation on confocal endomicroscopy of the digestive system. To evaluate the effectiveness of our approach, we conducted extensive experiments including ablation studies, comparisons with other segmentation methods, and comparisons to state-of-the-art segmentation methods. Results are reported based on five widely used evaluation metrics: accuracy (AC), Dice coefficient (Dice), Jaccard Index (JA), sensitivity (SE), and specificity (SP).</p><sec id=\"sec3.1.1\"><label>3.1.1</label><title>Ablation study</title><p>To compare the various variants of our proposed network and demonstrate their effectiveness, we conducted a study on ablation.</p><p>U-Net: U-Net is the baseline architecture.</p><p>GU-Net: A U-Net variant incorporating Gaussian convolution (GaussianConv2d) instead of standard convolutions.</p><p>DU-Net: A U-Net variant introducing a dual-path structure but without Gaussian convolution.</p><p>Ours (DualPath Gaussian UNet): We propose a network that combines Gaussian Convolution with a dual-path architecture.</p><p>The results of the ablation study are summarized in <xref rid=\"t001\" ref-type=\"table\">Table&#160;1</xref>. Our proposed DualPath Gaussian U-Net achieves superior performance on key segmentation metrics, with an AC of 88.10, a Dice of 85.17, JA of 74.75, and SE of 87.03, significantly outperforming other variants. Although the specificity (SP, 88.83) is marginally lower than DU-Net (89.79), the overall performance demonstrates the synergistic effect of Gaussian convolution and dual-path structure in enhancing the network&#8217;s capability to segment fine glandular structures. Notably, GU-Net underperformed the baseline U-Net due to excessive smoothing, whereas DU-Net&#8217;s higher Dice and JA scores confirm the effectiveness of the dual-path architecture. As shown in <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4</xref>, the segmentation results for the proposed method and the variant networks are presented.</p><table-wrap position=\"float\" id=\"t001\" orientation=\"portrait\"><label>Table 1</label><caption><p>Ablation study of different network components.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AC</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Dice</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">JA</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SE</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SP</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"r37\" ref-type=\"bibr\"><sup>37</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.25</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">83.03</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">71.66</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.91</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.48</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GU-Net</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.41</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">82.16</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">70.32</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.34</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.47</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DU-Net</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.45</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.89</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">73.04</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.01</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>89.79</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>88.10</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>85.17</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>74.75</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>87.03</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">88.83</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f4\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Ablation study results, presenting contour comparisons between the proposed method, variant networks, and ground truth.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g004.jpg\"/></fig></sec><sec id=\"sec3.1.2\"><label>3.1.2</label><title>Comparison with state-of-the-art methods</title><p>We compared our proposed DualPath Gaussian U-Net with several state-of-the-art segmentation methods, including U-Net,<xref rid=\"r37\" ref-type=\"bibr\"><sup>37</sup></xref> U-Net++,<xref rid=\"r38\" ref-type=\"bibr\"><sup>38</sup></xref> U-Net3+,<xref rid=\"r39\" ref-type=\"bibr\"><sup>39</sup></xref> Attention U-Net (Att U-Net),<xref rid=\"r40\" ref-type=\"bibr\"><sup>40</sup></xref> Adaptive Attention U-Net (AAU-Net),<xref rid=\"r41\" ref-type=\"bibr\"><sup>41</sup></xref> and SegNet.<xref rid=\"r42\" ref-type=\"bibr\"><sup>42</sup></xref> The results are presented in <xref rid=\"t002\" ref-type=\"table\">Table&#160;2</xref>.</p><table-wrap position=\"float\" id=\"t002\" orientation=\"portrait\"><label>Table 2</label><caption><p>Segmentation results of different methods.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AC</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Dice</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">JA</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SE</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SP</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"r37\" ref-type=\"bibr\"><sup>37</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.25</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">83.03</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">71.66</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.91</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.48</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net++<xref rid=\"r38\" ref-type=\"bibr\"><sup>38</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.09</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.23</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">73.26</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.36</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.90</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net3+<xref rid=\"r39\" ref-type=\"bibr\"><sup>39</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.10</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.11</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">73.15</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">86.82</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.29</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Att U-Net<xref rid=\"r40\" ref-type=\"bibr\"><sup>40</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">77.54</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">76.51</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">62.72</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>93.33</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">66.80</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AAU-Net<xref rid=\"r41\" ref-type=\"bibr\"><sup>41</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.99</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">81.64</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">69.74</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">85.47</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.66</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SegNet<xref rid=\"r42\" ref-type=\"bibr\"><sup>42</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">83.43</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">80.05</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">67.42</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">84.47</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">82.57</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>88.10</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>85.17</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>74.75</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.03</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>88.83</bold>\n</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold.</p></table-wrap-foot></table-wrap><p>Our method outperforms the second-best performer across most evaluation metrics. We achieved improvements of +1.0 AC,+0.94 Dice, and +1.49 JA over the second-best performer. As shown in <xref rid=\"f5\" ref-type=\"fig\">Fig.&#160;5</xref>, the proposed method achieves a cleaner segmentation boundary than other approaches. These results demonstrate the superiority of our approach when handling the complex structures and varying intensity present in confocal endomicroscopy images.</p><fig position=\"float\" id=\"f5\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Comparison study results. For clarity, competing methods are divided into two groups, each showing contour comparisons between the proposed method, three competing methods, and ground truth: (a)&#160;UNet, UNet++, and UNet3+; (b)&#160;AttUNet, AUUNet, and SegNet.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g005.jpg\"/></fig></sec><sec id=\"sec3.1.3\"><label>3.1.3</label><title>External test</title><p>To validate the generalization capability of our approach, we evaluated the segmentation network on an external breast ultrasound dataset comprising 42 images, each containing an annotated breast tumor region and uniformly resized to <inline-formula><mml:math id=\"math32\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn><mml:mrow><mml:mtext>&#8201;&#8201;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pixels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> as network input.<xref rid=\"r43\" ref-type=\"bibr\"><sup>43</sup></xref> Compared with gastrointestinal confocal endomicroscopy images, breast ultrasound images differ in texture patterns and lesion morphology, yet share similar segmentation challenges in distinguishing salient structures from noisy backgrounds, making them an appropriate benchmark for evaluating our model&#8217;s universal feature extraction capability. As shown in <xref rid=\"t003\" ref-type=\"table\">Table&#160;3</xref>, the proposed method achieved the highest accuracy (AC) and Jaccard Index (JA) among all evaluated networks. Although not optimal in all metrics, its performance across different medical imaging modalities confirms the generalization of the dual-path Gaussian architecture for segmentation tasks.</p><table-wrap position=\"float\" id=\"t003\" orientation=\"portrait\"><label>Table 3</label><caption><p>Segmentation performance comparison on the external breast ultrasound dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Methods</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AC</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Dice</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">JA</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SE</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SP</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"r37\" ref-type=\"bibr\"><sup>37</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">97.08</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>92.81</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">83.31</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.55</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.47</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net++<xref rid=\"r38\" ref-type=\"bibr\"><sup>38</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">97.73</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">89.28</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">83.39</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">93.74</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.28</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net3+<xref rid=\"r39\" ref-type=\"bibr\"><sup>39</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">97.68</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">89.35</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">82.44</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">89.68</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.79</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Att U-Net<xref rid=\"r40\" ref-type=\"bibr\"><sup>40</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">97.50</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.24</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">81.00</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.82</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>98.83</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AAU-Net<xref rid=\"r41\" ref-type=\"bibr\"><sup>41</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">97.58</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">88.14</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">81.64</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">88.75</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.80</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SegNet<xref rid=\"r42\" ref-type=\"bibr\"><sup>42</sup></xref></td><td align=\"center\" colspan=\"1\" rowspan=\"1\">96.70</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.08</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">77.04</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">87.01</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.11</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>98.13</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">91.41</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>85.92</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>94.07</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">98.69</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold.</p></table-wrap-foot></table-wrap></sec><sec id=\"sec3.1.4\"><label>3.1.4</label><title>Experiment details</title><p>The loss function for network training is binary-cross-entropy. Adam optimizer is used to train the network. The initial learning rate is set at 0.00005. The dataset is randomly split into training (80%) and validation (20%) sets using a fixed random seed to ensure reproducibility. Multiple cross-validation shows that the best segmentation performance is obtained when epoch size and batch size are set to 100 and 2, respectively. The development environment consists of Pytorch with CUDA acceleration 12.1, Python 3.9, and an NVIDIA RTX3090 GPU.</p></sec></sec><sec id=\"sec3.2\"><label>3.2</label><title>Stabilization Performance</title><p>In this section, we present the results from the frame selection method for mitigating rolling shutter effects in confocal image sequences of the digestive system. We employed the video stabilization evaluation framework proposed by M. Grundmann<xref rid=\"r44\" ref-type=\"bibr\"><sup>44</sup></xref> to assess the improvement in stability of the stabilized frame sequence compared with the original sequence by estimating the variations in the camera path.</p><sec id=\"sec3.2.1\"><label>3.2.1</label><title>Experimental protocol</title><p>Because the frame selection method removes shaky frames from the original image sequence, the stabilized image sequence has fewer frames. Before conducting the stability evaluation, it is necessary to interpolate frames in the stabilized sequence to match the frame count of the original sequence. We employ the Farneback optical flow<xref rid=\"r45\" ref-type=\"bibr\"><sup>45</sup></xref> method, with the interpolated frame <inline-formula><mml:math id=\"math33\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>interp</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is computed as <disp-formula id=\"e006\"><mml:math id=\"math34\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>interp</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(6)</label></disp-formula>where <inline-formula><mml:math id=\"math35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> denote forward- and backward-warped frames via the optical flow field <inline-formula><mml:math id=\"math37\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id=\"math38\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#945;</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy=\"false\">[</mml:mo><mml:mn>01</mml:mn><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow></mml:math></inline-formula> controlling temporal weighting.</p><p>The forward optical flow is remapped as <inline-formula><mml:math id=\"math39\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the reverse optical flow is remapped as <inline-formula><mml:math id=\"math40\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, The interpolation factor <inline-formula><mml:math id=\"math41\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow></mml:math></inline-formula> is determined by, where <inline-formula><mml:math id=\"math42\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is the total interpolated frames between two stable frames, and <inline-formula><mml:math id=\"math43\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is the current interpolation index.</p><p>Subsequently, the camera path of the image sequence is estimated, and the affine change matrix <inline-formula><mml:math id=\"math44\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is estimated for each neighboring frame, accumulating the neighboring frame transformation matrix <inline-formula><mml:math id=\"math45\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>accum</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#183;</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the extraction of translation information <inline-formula><mml:math id=\"math46\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math47\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be used to plot camera motion trajectories of the original and stabilized image sequence.</p></sec><sec id=\"sec3.2.2\"><label>3.2.2</label><title>Performance comparison</title><p>As shown in <xref rid=\"f6\" ref-type=\"fig\">Fig.&#160;6</xref>, we present the camera paths of the original image sequence and the stabilized image sequence with frame count alignment, showing a reduction in displacement variance along both the <inline-formula><mml:math id=\"math48\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>-axis and <inline-formula><mml:math id=\"math49\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>-axis. After stable frame selection and subsequent frame interpolation using the optical flow method to align the frame count, the stabilized image sequence demonstrates lower displacement variance in the <inline-formula><mml:math id=\"math50\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math51\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> directions than the original sequence. This indicates a jitter reduction and effectively suppresses the rolling shutter effects in the stabilized sequence.</p><fig position=\"float\" id=\"f6\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Camera motion paths in the <inline-formula><mml:math id=\"math52\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math53\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> directions of the original image sequence, and the frame-interpolated stabilized sequence over 150 frames. Our method reduces the jitter of the image sequence.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g006.jpg\"/></fig></sec></sec><sec id=\"sec3.3\"><label>3.3</label><title>Stitching Performance</title><p>This section compares the stitching results between our method and the traditional AutoStitch technique and shows how our method preserves important diagnostic information during confocal endomicroscope inspections of the digestive system.</p><p>As shown in <xref rid=\"f7\" ref-type=\"fig\">Fig.&#160;7</xref>, for 50 consecutive frames of confocal images of a rat stomach, the AutoStitch method, under the interference of the rolling shutter effects, resulted in incorrect scaling and transformations during image registration, causing some images to be stitched into erroneous positions. By contrast, our method utilizes gland segmentation masks to filter stable frames and foreground feature points, effectively suppressing the rolling shutter effects and background noise. As shown in <xref rid=\"f7\" ref-type=\"fig\">Fig.&#160;7(b)</xref>, the resulting stitched images maintain anatomical continuity and eliminate mismatches.</p><fig position=\"float\" id=\"f7\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>A 50-frame sequence of confocal endomicroscopic images of the gastrointestinal tract: (a)&#160;auto-stitching result, showing misalignments; (b)&#160;result of our proposed method.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g007.jpg\"/></fig><p>A comparison was made between the proposed method and the conventional Auto Stitch approach. <xref rid=\"f7\" ref-type=\"fig\">Figure&#160;7</xref> demonstrates stitching outcomes generated from sequential CLE frames of gastrointestinal examinations.</p><p>For a sequence of confocal endomicroscopic images of the digestive tract continuously captured by the same device, the clarity of each frame should be consistent, and the resulting stitched image should exhibit more uniform sharpness across all regions. As shown in <xref rid=\"f8\" ref-type=\"fig\">Fig.&#160;8</xref>, we divided the stitched images into several equally sized small patches. We tested the variance of sharpness across these patches at different patch sizes for both our method and the auto-stitching method, as presented in <xref rid=\"t004\" ref-type=\"table\">Table&#160;4</xref>. A lower variance indicates better consistency in sharpness and higher quality of the stitched image. According to <xref rid=\"t004\" ref-type=\"table\">Table&#160;4</xref>, under different metrics and patch segmentation sizes, the stitched images generated by our method consistently demonstrate superior uniformity.</p><fig position=\"float\" id=\"f8\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Block-based sharpness consistency evaluation (block size: <inline-formula><mml:math id=\"math54\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn><mml:mrow><mml:mtext>&#8201;&#8201;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pixels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>): (a)&#160;grid partitioning of AutoStitch result&#8212;the stitched image divided into equal-sized blocks, with border regions containing black margins treated as invalid; (b)&#160;sharpness heatmap for AutoStitch&#8212;individual block sharpness quantified using Laplacian variance, normalized and visualized through a thermal color map (purple indicating lower values, blue indicating higher values); (c)&#160;grid partitioning of proposed method&#8217;s result under identical partitioning scheme; and (d)&#160;sharpness heatmap for proposed method&#8217;s result under identical partitioning scheme.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g008.jpg\"/></fig><table-wrap position=\"float\" id=\"t004\" orientation=\"portrait\"><label>Table 4</label><caption><p>Comparison of variance in image sharpness evaluation metrics across different tile sizes (<inline-formula><mml:math id=\"math55\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"math56\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>128</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"math57\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn><mml:mrow><mml:mtext>&#8201;&#8201;</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pixels</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>) for stitched images generated by our method and AutoStitch (AS). Lower variance values indicate superior sharpness uniformity.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"2\" align=\"left\" valign=\"top\" colspan=\"1\">&#160;</th><th colspan=\"2\" align=\"center\" valign=\"top\" rowspan=\"1\">Laplacian<xref rid=\"r46\" ref-type=\"bibr\"><sup>46</sup></xref></th><th colspan=\"2\" align=\"center\" valign=\"top\" rowspan=\"1\">Sobel<xref rid=\"r47\" ref-type=\"bibr\"><sup>47</sup></xref></th><th colspan=\"2\" align=\"center\" valign=\"top\" rowspan=\"1\">Brenner<xref rid=\"r48\" ref-type=\"bibr\"><sup>48</sup></xref></th></tr><tr><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Ours</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AS</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Ours</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AS</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Ours</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">AS</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">64</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>133.789</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">3120.209</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>664,856</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">2675004</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>745.4142</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">908.0533</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">128</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>33.04581</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">1878.634</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>529,234</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">2164628</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>409.6784</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">821.5077</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">256</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>40.9421</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">1035.356</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>316,227</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">1775584</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>476.4396</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">1029.241</td></tr></tbody></table><table-wrap-foot><p>Methods achieving better consistency (lower variance) are highlighted in bold.</p></table-wrap-foot></table-wrap><p>To further evaluate stitching quality on the 100-frame sequence, we employed two complementary metrics: Structural Similarity Index Measure (SSIM) and root mean square error (RMSE), as presented in <xref rid=\"t004\" ref-type=\"table\">Table&#160;4</xref>. Although the SSIM value of the proposed method was marginally lower than that of Auto-Stitch, this is attributed to the frame selection process, which intentionally excludes motion-distorted frames, leading to increased variations in temporal continuity. Conversely, the proposed method achieved a lower RMSE, indicating superior suppression of large misalignments. These results demonstrate enhanced precision in maintaining diagnostic feature alignment while ensuring clinically acceptable structural integrity (<xref rid=\"t005\" ref-type=\"table\">Table&#160;5</xref>).</p><table-wrap position=\"float\" id=\"t005\" orientation=\"portrait\"><label>Table 5</label><caption><p>Comparison of Structural Similarity Index Measure (SSIM) and root mean square error (RMSE) for stitching images generated by our method and Auto-Stitching using a 100-frame sequence. Higher SSIM and lower RMSE indicate superior quality.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"top\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SSIM<xref rid=\"r49\" ref-type=\"bibr\"><sup>49</sup></xref></th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">RMSE<xref rid=\"r49\" ref-type=\"bibr\"><sup>49</sup></xref></th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Auto-stitching</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>0.8741 &#177; 0.0761</bold>\n</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">44.46 &#177; 31.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">0.8461 &#177; 0.1105</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">\n<bold>37.47 &#177; 21.01</bold>\n</td></tr></tbody></table><table-wrap-foot><p>Methods achieving better performance per metric are highlighted in bold.</p></table-wrap-foot></table-wrap><p>As shown in <xref rid=\"f9\" ref-type=\"fig\">Fig.&#160;9</xref>, our stitching results preserve fluorescein leakage points, a key feature for diagnosing atrophic gastritis in the confocal endomicroscope.<xref rid=\"r3\" ref-type=\"bibr\"><sup>3</sup></xref> The proposed method, which utilizes gland segmentation masks for stable frame selection and foreground feature extraction, not only improves the accuracy and quality of stitching but also retains critical diagnostic information.</p><fig position=\"float\" id=\"f9\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>Stitching results of 100 consecutive confocal endomicroscopic images of the gastrointestinal tract. Red arrows indicate observable fluorescein leakage points. (Field of view: <inline-formula><mml:math id=\"math58\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1000</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>700</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:math></inline-formula>).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-126001-g009.jpg\"/></fig></sec></sec><sec id=\"sec4\"><label>4</label><title>Discussion</title><p>In this paper, a method is presented for stitching confocal pictures of the digestive system based on a U-shaped CNN. During confocal endomicroscopy of the digestive system, disease stage and type diagnosis are based on the characteristics of the superficial cells and glands. The limited field of vision under high magnification in confocal endomicroscopy does not allow for the full lesion to be displayed in a single image. Image stitching can increase the field of view and provide a more accurate and comprehensive representation of the superficial state of the digestive system. This improves the efficiency of clinical examinations and the accuracy of diagnosis. Due to the line scan imaging principle of confocal endomicroscopy, multiple image regions are not captured at once. The rolling shutter effect is caused by the operator moving the probe. This results in mismatched images and affects the image quality.</p><p>We use the texture features from confocal images with glandular tissue in the foreground to construct a Dual Path Gaussian UNet for obtaining segmentation masks. The network is able to better perceive texture features in images through Gaussian Convolution and a dual-path structure. As presented in <xref rid=\"t002\" ref-type=\"table\">Table&#160;2</xref>, we achieve superior segmentation results compared to U-Net networks for confocal images of the digestive system.</p><p>As shown in <xref rid=\"t003\" ref-type=\"table\">Table&#160;3</xref>, the proposed model maintains robust segmentation performance on the external dataset. Although the training dataset comprises only 80 annotated images, each contains multiple glandular structures, resulting in almost 1000 annotated gland instances. This rich morphological diversity enables the model to achieve effective generalization capability despite the limited training volume.</p><p>In addition, based on a clinical practice of &#8220;slow translation, brief pause,&#8221; we use a foreground segmentation to filter out interferences from free superficial cells and a frame selection to remove shaky images while retaining stable frames. The image sequence stabilized from stable frames shows reduced jitter and effectively suppresses the rolling shutter effects.</p><p>As shown in <xref rid=\"f7\" ref-type=\"fig\">Fig.&#160;7</xref>, the panoramic image generated through SIFT feature registration of foreground elements in stable frames eliminates mismatches inherent to conventional methods. This approach provides enhanced visualization of glandular morphology and pathological indicators (e.g., fluorescein leakage points) under confocal endomicroscopy, enabling comprehensive assessment of the digestive tract&#8217;s superficial state. Clinicians can thereby accurately stage diseases, localize lesions, and improve diagnostic efficiency during confocal endoscopic examinations.</p><p>This study has some limitations. The algorithm is not real-time because the stitching speed is slower than that of confocal endomicroscopy, resulting in a delay during actual examination. Second, as shown in <xref rid=\"f6\" ref-type=\"fig\">Fig.&#160;6</xref>, the pronounced gap in the <inline-formula><mml:math id=\"math59\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula>-axis trajectory, the algorithm&#8217;s potential application is currently constrained by the lack of tracing technology specifically optimized for confocal endomicroscopy, where existing motion estimation frameworks exhibit incomplete compatibility with probe tracking characteristics, leading to difficulties in determining accurate stitched image contours and posing potential risks of contour deformation.</p><p>Future work will focus primarily on improving the real-time performance of the algorithm and integrating object tracking algorithms to confirm endoscope movement paths, generate accurate stitched image contours, and ultimately produce more informative stitched images. This will improve the efficiency and diagnostic accuracy for confocal endomicroscopy of the digestive tract.</p></sec><sec id=\"sec5\"><label>5</label><title>Conclusion</title><p>In this paper, we propose a gland segmentation network-based stitching algorithm for the gastrointestinal CLE images. Our method leverages the unique textural features of gastrointestinal CLE images to generate precise gland segmentation masks, which are then employed to select stable frames and extract foreground features, thereby effectively suppressing rolling shutter effects and improving image stitching quality. Experimental results show that our proposed network outperforms several advanced deep learning segmentation methods in confocal endomicroscopic images of the gastrointestinal tract. Our image stitching method surpasses the AutoStitch method in accuracy and image quality. The resulting stitched images can display key diagnostic information, demonstrating significant potential for practical applications and clinical diagnosis support.</p></sec></body><back><ack><title>Acknowledgments</title><p><named-content content-type=\"funding-statement\">This work was supported by the National Key Research and Development Program of China (Grant No. 2022YFC2404401), National Natural Science Foundation of China (Grant No. 62475063), and Open Project Program of Wuhan National Laboratory for Optoelectronics (Grant No. 2023WNLOKF013).</named-content> The authors utilized DeepSeek<xref rid=\"r50\" ref-type=\"bibr\"><sup>50</sup></xref> for language refinement and grammatical improvement during manuscript preparation. This AI tool was prompted with specific requests such as &#8220;Polish the academic tone of this paragraph&#8221; and &#8220;Check technical terms consistency in the Methods section.&#8221;</p></ack><bio id=\"d6341e3765\" content-type=\"general\"><p>Biographies of the authors are not available.</p></bio><sec sec-type=\"conflict\"><title>Disclosures</title><p>The authors declare no conflicts of interest.</p></sec><sec sec-type=\"data-availability\"><title>Code and Data Availability</title><p>The confocal endomicroscopy image dataset can be obtained from the authors upon reasonable request. The DGU-Net source code is available on the GitHub via the link <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/PtK929/DGU-Net/tree/main\" ext-link-type=\"uri\">https://github.com/PtK929/DGU-Net/tree/main</ext-link>.</p></sec><ref-list><title>References</title><ref id=\"r1\"><label>1.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Differences in the incidence and mortality of digestive cancer between Global Cancer Observatory 2020 and Global Burden of Disease 2019</article-title>,&#8221; <source>Int. J. Cancer</source><volume>154</volume>(<issue>4</issue>), <fpage>615</fpage>&#8211;<lpage>625</lpage> (<year>2024</year>).<pub-id pub-id-type=\"doi\">10.1002/ijc.34740</pub-id><pub-id pub-id-type=\"pmid\">37750191</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A confocal endoscope for cellular imaging</article-title>,&#8221; <source>Engineering</source><volume>1</volume>(<issue>3</issue>), <fpage>351</fpage>&#8211;<lpage>360</lpage> (<year>2015</year>).<pub-id pub-id-type=\"coden\">ENGNA2</pub-id><issn>0013-7782</issn><pub-id pub-id-type=\"doi\">10.15302/J-ENG-2015081</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>New classification of gastric pit patterns and vessel architecture using probe-based confocal laser endomicroscopy</article-title>,&#8221; <source>J. Clin. Gastroenterol.</source><volume>50</volume>(<issue>1</issue>), <fpage>23</fpage>&#8211;<lpage>32</lpage> (<year>2016</year>).<pub-id pub-id-type=\"doi\">10.1097/MCG.0000000000000298</pub-id><pub-id pub-id-type=\"pmid\">25751373</pub-id></mixed-citation></ref><ref id=\"r4\"><label>4.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vennelaganti</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Validation of probe-based Confocal Laser Endomicroscopy (pCLE) criteria for diagnosing colon polyp histology</article-title>,&#8221; <source>J. Clin. Gastroenterol.</source><volume>52</volume>(<issue>9</issue>), <fpage>812</fpage>&#8211;<lpage>816</lpage> (<year>2018</year>).<pub-id pub-id-type=\"doi\">10.1097/MCG.0000000000000927</pub-id><pub-id pub-id-type=\"pmid\">28885303</pub-id></mixed-citation></ref><ref id=\"r5\"><label>5.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>W.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Confocal laser endomicroscopy for detection of early upper gastrointestinal cancer</article-title>,&#8221; <source>Cancers</source><volume>15</volume>(<issue>3</issue>), <fpage>776</fpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.3390/cancers15030776</pub-id><pub-id pub-id-type=\"pmid\">36765734</pub-id><pub-id pub-id-type=\"pmcid\">PMC9913498</pub-id></mixed-citation></ref><ref id=\"r6\"><label>6.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vercauteren</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>Image registration and mosaicing for dynamic in vivo fibered confocal microscopy</article-title>&#8221;.</mixed-citation></ref><ref id=\"r7\"><label>7.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rengarajan</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Image registration and change detection under rolling shutter motion blur</article-title>,&#8221; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>39</volume>(<issue>10</issue>), <fpage>1959</fpage>&#8211;<lpage>1972</lpage> (<year>2017</year>).<pub-id pub-id-type=\"coden\">ITPIDJ</pub-id><issn>0162-8828</issn><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2016.2630687</pub-id><pub-id pub-id-type=\"pmid\">27875216</pub-id></mixed-citation></ref><ref id=\"r8\"><label>8.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ait-Aider</surname><given-names>O.</given-names></name></person-group>, &#8220;<article-title>Rolling shutter homography and its applications</article-title>,&#8221; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>43</volume>(<issue>8</issue>), <fpage>2780</fpage>&#8211;<lpage>2793</lpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">ITPIDJ</pub-id><issn>0162-8828</issn><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2020.2977644</pub-id><pub-id pub-id-type=\"pmid\">32142425</pub-id></mixed-citation></ref><ref id=\"r9\"><label>9.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brown</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Lowe</surname><given-names>D. G.</given-names></name></person-group>, &#8220;<article-title>Recognising panoramas</article-title>,&#8221; in <conf-name>Proc. Ninth IEEE Int. Conf. Comput. Vision</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Nice</publisher-loc>, Vol.&#160;2, pp.&#160;<fpage>1218</fpage>&#8211;<lpage>1225</lpage> (<year>2003</year>).</mixed-citation></ref><ref id=\"r10\"><label>10.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brown</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Lowe</surname><given-names>D. G.</given-names></name></person-group>, &#8220;<article-title>Automatic panoramic image stitching using invariant features</article-title>,&#8221; <source>Int. J. Comput. Vision</source><volume>74</volume>(<issue>1</issue>), <fpage>59</fpage>&#8211;<lpage>73</lpage> (<year>2007</year>).<pub-id pub-id-type=\"coden\">IJCVEQ</pub-id><issn>0920-5691</issn><pub-id pub-id-type=\"doi\">10.1007/s11263-006-0002-3</pub-id></mixed-citation></ref><ref id=\"r11\"><label>11.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ke</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sukthankar</surname><given-names>R.</given-names></name></person-group>, &#8220;<article-title>PCA-SIFT: a more distinctive representation for local image descriptors</article-title>,&#8221; in <conf-name>Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognit., CVPR</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Washington, DC</publisher-loc>, Vol.&#160;2, pp.&#160;<fpage>506</fpage>&#8211;<lpage>513</lpage> (<year>2004</year>).</mixed-citation></ref><ref id=\"r12\"><label>12.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bay</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Tuytelaars</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Van Gool</surname><given-names>L.</given-names></name></person-group>, &#8220;<article-title>SURF: speeded up robust features</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>3951</volume>, <fpage>404</fpage>&#8211;<lpage>417</lpage> (<year>2006</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/11744023_32</pub-id></mixed-citation></ref><ref id=\"r13\"><label>13.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rosten</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Drummond</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>Machine learning for high-speed corner detection</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>3951</volume>, <fpage>430</fpage>&#8211;<lpage>443</lpage> (<year>2006</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/11744023_34</pub-id></mixed-citation></ref><ref id=\"r14\"><label>14.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>S. J.</given-names></name><name name-style=\"western\"><surname>Brown</surname><given-names>M. S.</given-names></name></person-group>, &#8220;<article-title>Constructing image panoramas using dual-homography warping</article-title>,&#8221; in <conf-name>CVPR</conf-name>, <article-title>IEEE</article-title>, <publisher-loc>Colorado Springs, CO</publisher-loc>, pp.&#160;<fpage>49</fpage>&#8211;<lpage>56</lpage> (<year>2011</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2011.5995433</pub-id></mixed-citation></ref><ref id=\"r15\"><label>15.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names>W.-Y.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Smoothly varying affine stitching</article-title>,&#8221; in <conf-name>CVPR</conf-name>, <article-title>IEEE</article-title>, <publisher-loc>Colorado Springs, CO</publisher-loc>, pp.&#160;<fpage>345</fpage>&#8211;<lpage>352</lpage> (<year>2011</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2011.5995314</pub-id></mixed-citation></ref><ref id=\"r16\"><label>16.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zaragoza</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>As-projective-as-possible image stitching with moving DLT</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Portland, OR</publisher-loc>, pp.&#160;<fpage>2339</fpage>&#8211;<lpage>2346</lpage> (<year>2013</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2013.303</pub-id><pub-id pub-id-type=\"pmid\">26353303</pub-id></mixed-citation></ref><ref id=\"r17\"><label>17.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>F.</given-names></name></person-group>, &#8220;<article-title>Parallax-tolerant image stitching</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Columbus, OH</publisher-loc>, pp.&#160;<fpage>3262</fpage>&#8211;<lpage>3269</lpage> (<year>2014</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2014.423</pub-id></mixed-citation></ref><ref id=\"r18\"><label>18.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Seam-driven image stitching</article-title>,&#8221; in <conf-name>Eurographics 2013 - Short Papers</conf-name>, <publisher-name>The Eurographics Association</publisher-name>, p.&#160;<fpage>4</fpage> (<year>2013</year>).<pub-id pub-id-type=\"doi\">10.2312/CONF/EG2013/SHORT/045-048</pub-id></mixed-citation></ref><ref id=\"r19\"><label>19.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group>, &#8220;<article-title>Automatic quaternion-domain color image stitching</article-title>,&#8221; <source>IEEE Trans. Image Process.</source><volume>33</volume>, <fpage>1299</fpage>&#8211;<lpage>1312</lpage> (<year>2024</year>).<pub-id pub-id-type=\"coden\">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type=\"doi\">10.1109/TIP.2024.3361688</pub-id><pub-id pub-id-type=\"pmid\">38329845</pub-id></mixed-citation></ref><ref id=\"r20\"><label>20.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Verdie</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>TILDE: a temporally invariant learned DEtector</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Boston, MA</publisher-loc>, pp.&#160;<fpage>5279</fpage>&#8211;<lpage>5288</lpage> (<year>2015</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2015.7299165</pub-id></mixed-citation></ref><ref id=\"r21\"><label>21.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Simo-Serra</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Discriminative learning of deep convolutional feature point descriptors</article-title>,&#8221; in <conf-name>IEEE Int. Conf. Comput. Vision (ICCV)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Santiago</publisher-loc>, pp.&#160;<fpage>118</fpage>&#8211;<lpage>126</lpage> (<year>2015</year>).<pub-id pub-id-type=\"doi\">10.1109/ICCV.2015.22</pub-id></mixed-citation></ref><ref id=\"r22\"><label>22.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yi</surname><given-names>K. M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Learning to assign orientations to feature points</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Las Vegas, Nevada</publisher-loc>, pp.&#160;<fpage>107</fpage>&#8211;<lpage>116</lpage> (<year>2016</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2016.19</pub-id></mixed-citation></ref><ref id=\"r23\"><label>23.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>MatchNet: unifying feature and metric learning for patch-based matching</article-title>,&#8221; in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Boston, MA</publisher-loc>, pp.&#160;<fpage>3279</fpage>&#8211;<lpage>3286</lpage> (<year>2015</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2015.7298948</pub-id></mixed-citation></ref><ref id=\"r24\"><label>24.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>DeTone</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Malisiewicz</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group>, &#8220;<article-title>Deep image homography estimation</article-title>,&#8221; arXiv:1606.03798 (<year>2016</year>).</mixed-citation></ref><ref id=\"r25\"><label>25.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nie</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep rectangling for image stitching: a learning baseline</article-title>,&#8221; in <conf-name>IEEE/CVF Conf. Comput. Vision and Pattern Recognit. (CVPR)</conf-name>, pp.&#160;<fpage>5730</fpage>&#8211;<lpage>5738</lpage> (<year>2022</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR52688.2022.00565</pub-id></mixed-citation></ref><ref id=\"r26\"><label>26.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cai</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>W.</given-names></name></person-group>, &#8220;<article-title>Object-level geometric structure preserving for natural image stitching</article-title>,&#8221; arXiv:2402.12677 (<year>2024</year>).</mixed-citation></ref><ref id=\"r27\"><label>27.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gong</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Robust mosaicing of endomicroscopic videos via context-weighted correlation ratio</article-title>,&#8221; <source>IEEE Trans. Biomed. Eng.</source><volume>68</volume>(<issue>2</issue>), <fpage>579</fpage>&#8211;<lpage>591</lpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">IEBEAX</pub-id><issn>0018-9294</issn><pub-id pub-id-type=\"doi\">10.1109/TBME.2020.3007768</pub-id><pub-id pub-id-type=\"pmid\">32746056</pub-id></mixed-citation></ref><ref id=\"r28\"><label>28.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kose</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Automated video-mosaicking approach for confocal microscopic imaging in vivo: an approach to address challenges in imaging living tissue and extend field of view</article-title>,&#8221; <source>Sci. Rep.</source><volume>7</volume>(<issue>1</issue>), <fpage>10759</fpage> (<year>2017</year>).<pub-id pub-id-type=\"coden\">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type=\"doi\">10.1038/s41598-017-11072-9</pub-id><pub-id pub-id-type=\"pmid\">28883434</pub-id><pub-id pub-id-type=\"pmcid\">PMC5589933</pub-id></mixed-citation></ref><ref id=\"r29\"><label>29.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rosa</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Building large mosaics of confocal edomicroscopic images using visual servoing</article-title>,&#8221; <source>IEEE Trans. Biomed. Eng.</source><volume>60</volume>(<issue>4</issue>), <fpage>1041</fpage>&#8211;<lpage>1049</lpage> (<year>2013</year>).<pub-id pub-id-type=\"coden\">IEBEAX</pub-id><issn>0018-9294</issn><pub-id pub-id-type=\"doi\">10.1109/TBME.2012.2228859</pub-id><pub-id pub-id-type=\"pmid\">23192481</pub-id></mixed-citation></ref><ref id=\"r30\"><label>30.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bedard</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Real-time video mosaicing with a high-resolution microendoscope</article-title>,&#8221; <source>Biomed. Opt. Express</source><volume>3</volume>(<issue>10</issue>), <fpage>2428</fpage> (<year>2012</year>).<pub-id pub-id-type=\"coden\">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type=\"doi\">10.1364/BOE.3.002428</pub-id><pub-id pub-id-type=\"pmid\">23082285</pub-id><pub-id pub-id-type=\"pmcid\">PMC3469983</pub-id></mixed-citation></ref><ref id=\"r31\"><label>31.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Jayender</surname><given-names>J.</given-names></name></person-group>, &#8220;<article-title>Real-time nonrigid mosaicking of laparoscopy images</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>40</volume>(<issue>6</issue>), <fpage>1726</fpage>&#8211;<lpage>1736</lpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type=\"doi\">10.1109/TMI.2021.3065030</pub-id><pub-id pub-id-type=\"pmid\">33690113</pub-id><pub-id pub-id-type=\"pmcid\">PMC8169627</pub-id></mixed-citation></ref><ref id=\"r32\"><label>32.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zenteno</surname><given-names>O.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optical biopsy mapping on endoscopic image mosaics with a marker-free probe</article-title>,&#8221; <source>Comput. Biol. Med.</source><volume>143</volume>, <fpage>105234</fpage> (<year>2022</year>).<pub-id pub-id-type=\"coden\">CBMDAW</pub-id><issn>0010-4825</issn><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2022.105234</pub-id><pub-id pub-id-type=\"pmid\">35093845</pub-id></mixed-citation></ref><ref id=\"r33\"><label>33.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>Y.-F.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.-Y.</given-names></name><name name-style=\"western\"><surname>Qin</surname><given-names>P.</given-names></name></person-group>, &#8220;<article-title>GL-UNet: a deep learning model of breast tumor lesion segmentation for MRI images</article-title>,&#8221; in <conf-name>43rd Chin. Control Conf. (CCC)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Kunming</publisher-loc>, pp.&#160;<fpage>7403</fpage>&#8211;<lpage>7407</lpage> (<year>2024</year>).<pub-id pub-id-type=\"doi\">10.23919/CCC63176.2024.10662291</pub-id></mixed-citation></ref><ref id=\"r34\"><label>34.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fu</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Dual attention network for scene segmentation</article-title>,&#8221; arXiv:1809.02983 (<year>2019</year>).</mixed-citation></ref><ref id=\"r35\"><label>35.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Dual parallel net: a novel deep learning model for rectal tumor segmentation via CNN and transformer with Gaussian Mixture prior</article-title>,&#8221; <source>J. Biomed. Inf.</source><volume>139</volume>, <fpage>104304</fpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.1016/j.jbi.2023.104304</pub-id><pub-id pub-id-type=\"pmid\">36736447</pub-id></mixed-citation></ref><ref id=\"r36\"><label>36.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ban</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>L.</given-names></name></person-group>, &#8220;<article-title>Superpixel segmentation using Gaussian mixture model</article-title>,&#8221; <source>IEEE Trans. Image Process.</source><volume>27</volume>(<issue>8</issue>), <fpage>4105</fpage>&#8211;<lpage>4117</lpage> (<year>2018</year>).<pub-id pub-id-type=\"coden\">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type=\"doi\">10.1109/TIP.2018.2836306</pub-id><pub-id pub-id-type=\"pmid\">29994528</pub-id></mixed-citation></ref><ref id=\"r37\"><label>37.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>9351</volume>, <fpage>234</fpage>&#8211;<lpage>241</lpage> (<year>2015</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation></ref><ref id=\"r38\"><label>38.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>UNet++: redesigning skip connections to exploit multiscale features in image segmentation</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>39</volume>(<issue>6</issue>), <fpage>1856</fpage>&#8211;<lpage>1867</lpage> (<year>2020</year>).<pub-id pub-id-type=\"coden\">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type=\"doi\">10.1109/TMI.2019.2959609</pub-id><pub-id pub-id-type=\"pmid\">31841402</pub-id><pub-id pub-id-type=\"pmcid\">PMC7357299</pub-id></mixed-citation></ref><ref id=\"r39\"><label>39.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>UNet 3+: a full-scale connected UNet for medical image segmentation</article-title>,&#8221; in <conf-name>ICASSP 2020-2020 IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Barcelona</publisher-loc>, pp.&#160;<fpage>1055</fpage>&#8211;<lpage>1059</lpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.1109/ICASSP40776.2020.9053405</pub-id></mixed-citation></ref><ref id=\"r40\"><label>40.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oktay</surname><given-names>O.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Attention U-Net: learning where to look for the pancreas</article-title>,&#8221; arXiv:1804.03999 (<year>2018</year>).</mixed-citation></ref><ref id=\"r41\"><label>41.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>AAU-Net: an adaptive attention U-Net for breast lesions segmentation in ultrasound images</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>42</volume>(<issue>5</issue>), <fpage>1289</fpage>&#8211;<lpage>1300</lpage> (<year>2023</year>).<pub-id pub-id-type=\"coden\">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type=\"doi\">10.1109/TMI.2022.3226268</pub-id><pub-id pub-id-type=\"pmid\">36455083</pub-id></mixed-citation></ref><ref id=\"r42\"><label>42.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Badrinarayanan</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Kendall</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Cipolla</surname><given-names>R.</given-names></name></person-group>, &#8220;<article-title>SegNet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>,&#8221; <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><volume>39</volume>(<issue>12</issue>), <fpage>2481</fpage>&#8211;<lpage>2495</lpage> (<year>2017</year>).<pub-id pub-id-type=\"coden\">ITPIDJ</pub-id><issn>0162-8828</issn><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type=\"pmid\">28060704</pub-id></mixed-citation></ref><ref id=\"r43\"><label>43.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhuang</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>An RDAU-NET model for lesion segmentation in breast ultrasound images</article-title>,&#8221; <source>PLoS One</source><volume>14</volume>(<issue>8</issue>), <fpage>e0221535</fpage> (<year>2019</year>).<pub-id pub-id-type=\"coden\">POLNCL</pub-id><issn>1932-6203</issn><pub-id pub-id-type=\"doi\">10.1371/journal.pone.0221535</pub-id><pub-id pub-id-type=\"pmid\">31442268</pub-id><pub-id pub-id-type=\"pmcid\">PMC6707567</pub-id></mixed-citation></ref><ref id=\"r44\"><label>44.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Grundmann</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kwatra</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Essa</surname><given-names>I.</given-names></name></person-group>, &#8220;<article-title>Auto-directed video stabilization with robust L1 optimal camera paths</article-title>,&#8221; in <conf-name>CVPR</conf-name>, <article-title>IEEE</article-title>, <publisher-name>Colorado Springs, CO</publisher-name>, pp.&#160;<fpage>225</fpage>&#8211;<lpage>232</lpage> (<year>2011</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2011.5995525</pub-id></mixed-citation></ref><ref id=\"r45\"><label>45.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Farneb&#228;ck</surname><given-names>G.</given-names></name></person-group>, &#8220;<article-title>Two-frame motion estimation based on polynomial expansion</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>2749</volume>, <fpage>363</fpage>&#8211;<lpage>370</lpage> (<year>2003</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/3-540-45103-X_50</pub-id></mixed-citation></ref><ref id=\"r46\"><label>46.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xue</surname><given-names>W.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features</article-title>,&#8221; <source>IEEE Trans. Image Process.</source><volume>23</volume>(<issue>11</issue>), <fpage>4850</fpage>&#8211;<lpage>4862</lpage> (<year>2014</year>).<pub-id pub-id-type=\"coden\">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type=\"doi\">10.1109/TIP.2014.2355716</pub-id><pub-id pub-id-type=\"pmid\">25216482</pub-id></mixed-citation></ref><ref id=\"r47\"><label>47.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><collab>Tiruppur Kumaran College for Women, India</collab><etal>et al.</etal></person-group>, &#8220;<article-title>Sobel operator and PCA for nearest target of retina images</article-title>,&#8221; <source>IJIVP</source><volume>11</volume>(<issue>4</issue>), <fpage>2483</fpage>&#8211;<lpage>2491</lpage> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.21917/ijivp.2021.0353</pub-id></mixed-citation></ref><ref id=\"r48\"><label>48.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Wan</surname><given-names>H.</given-names></name></person-group>, &#8220;<article-title>The fast iris image clarity evaluation based on Brenner</article-title>,&#8221; in <conf-name>2nd Int. Symp. Instrum. and Meas. Sens. Network and Autom. (IMSNA)</conf-name>, <publisher-name>IEEE</publisher-name>, <publisher-loc>Toronto, ON</publisher-loc>, pp.&#160;<fpage>300</fpage>&#8211;<lpage>302</lpage> (<year>2013</year>).<pub-id pub-id-type=\"doi\">10.1109/IMSNA.2013.6743274</pub-id></mixed-citation></ref><ref id=\"r49\"><label>49.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Multispectral image stitching via global-aware quadrature pyramid regression</article-title>,&#8221; <source>IEEE Trans. Image Process.</source><volume>33</volume>, <fpage>4288</fpage>&#8211;<lpage>4302</lpage> (<year>2024</year>).<pub-id pub-id-type=\"coden\">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type=\"doi\">10.1109/TIP.2024.3430532</pub-id><pub-id pub-id-type=\"pmid\">39046864</pub-id></mixed-citation></ref><ref id=\"r50\"><label>50.</label><mixed-citation publication-type=\"webpage\"><collab>DeepSeek</collab>, &#8220;<article-title>DeepSeek AI Language Model</article-title>,&#8221; Hangzhou DeepSeek Artificial Intelligence, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.deepseek.com\" ext-link-type=\"uri\">https://www.deepseek.com</ext-link> (accessed during manuscript preparation in 2025).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc J Biomed Opt J Biomed Opt 953 jbiomedopt JBO Journal of Biomedical Optics 1083-3668 1560-2281 Society of Photo-Optical Instrumentation Engineers PMC12671177 PMC12671177.1 12671177 12671177 41341649 10.1117/1.JBO.30.12.126001 250104GRRR 1 Imaging Paper Rolling shutter-resistant confocal endomicroscopy image stitching via dual-path Gaussian U-Net https://orcid.org/0009-0005-3217-048X Lu Yuhua a lyh99sz@gmail.com https://orcid.org/0000-0001-5773-0360 Chen Shangbin a sbchen@hust.edu.cn https://orcid.org/0000-0002-8398-1021 Liu Qian b * qliu@hainanu.edu.cn a Huazhong University of Science and Technology , Wuhan National Laboratory for Optoelectronics, Wuhan, China b Hainan University , School of Biomedical Engineering, Key Laboratory of Biomedical Engineering of Hainan Province, Hainan, China * Address all correspondence to Qian Liu, qliu@hainanu.edu.cn 2 12 2025 12 2025 30 12 494472 126001 22 4 2025 20 10 2025 24 10 2025 02 12 2025 03 12 2025 04 12 2025 &#169; 2025 The Authors 2025 The Authors https://creativecommons.org/licenses/by/4.0/ Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. Abstract. Significance Confocal endomicroscopic image stitching can expand the field of view and improve examination efficiency. However, due to interference from the rolling shutter effect, traditional stitching methods may produce misalignments, leading to structural distortion and artifacts. Suppressing the rolling shutter effect in confocal endomicroscopic images can effectively enhance stitching quality. Aim We propose a Dual-Path Gaussian U-Net (DGU-Net)-based framework for confocal endomicroscopic image stitching. The parallel dual-encoder paths of DGU-Net extract Gaussian features and conventional features at different resolutions, respectively, achieving more precise gland segmentation masks. Based on these masks, we filter stable frames and optimize feature matching to effectively suppress rolling shutter interference and improve stitching quality. Approach We annotated a segmentation dataset comprising 80 rat confocal laser endomicroscopy (CLE) images to train the segmentation network and validated the frame selection method&#8217;s effectiveness in suppressing the rolling shutter effect on consecutively acquired rat CLE video sequences. The stitching results generated from the filtered stable image sequences were compared with conventional methods. Results Experimental results demonstrate that DGU-Net achieves superior performance with a Dice score of 85.17 on CLE datasets, significantly outperforming existing segmentation networks. Compared with Auto-Stitching, our method improves regional consistency across the panoramic image by eliminating artifacts caused by mismatches while delivering enhanced stitching accuracy and image quality. Conclusions The proposed method effectively accomplishes confocal image stitching tasks, significantly enhancing endomicroscopic examination efficiency and contributing to improved diagnostic outcomes. Keywords: confocal endomicroscopy image stitching U-Net rolling shutter effect National Key Research and Development Program of China 2022YFC2404401 National Natural Science Foundation of China 62475063 Open Project Program of Wuhan National Laboratory for Optoelectronics 2023WNLOKF013 This work was supported by the National Key Research and Development Program of China (Grant No. 2022YFC2404401), National Natural Science Foundation of China (Grant No. 62475063), and Open Project Program of Wuhan National Laboratory for Optoelectronics (Grant No. 2023WNLOKF013). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes running-head Lu, Chen, and Liu: Rolling shutter-resistant confocal endomicroscopy image stitching&#8230; 1 Introduction Colorectal cancer, gastric cancer, and esophageal cancer are all major threats to human health. They account for 16.1% of all new cancer cases and 20.7% cancer-related deaths. 1 Early diagnosis is crucial, as 5-year survival rates for early-stage cancers are over 90% compared with less than 20% for advanced carcinomas. Endomicroscopic biopsies remain the gold standard in confirming neoplastic tumors and staging progression. However, their invasive nature and delayed feedback contribute to a non-negligible rate of missed diagnoses and false negatives. Confocal laser endomicroscopy (CLE), which allows real-time visualization at the micron level of mucosal cellular structures and subcellular structures, overcomes these limitations. It facilitates precise lesion location and &#8220;optical biopsy&#8221; capabilities. 2 &#8211; 5 Confocal endomicroscopic image stitching can significantly expand the examination field of view, enabling clinicians to more comprehensively assess lesion areas, thereby improving diagnostic efficiency and accuracy. 6 However, due to the progressive line-scanning mechanism of CLE, 2 probe movement during imaging introduces rolling shutter effects. As shown in Fig.&#160;1 , in adjacent frames, nonsynchronous imaging of different regions causes nonuniform deformation and stretching of glands at different positions. These distortions lead to parallax artifacts and feature matching errors during stitching, significantly compromising final image quality. 7 , 8 Therefore, developing effective rolling shutter suppression algorithms is crucial for enhancing the clinical utility of confocal endomicroscopic image stitching. Fig. 1 Rolling shutter effects inducing inconsistent distortions between adjacent frames: (a)&#160;nonuniform angular displacement in frames 36 (left) and 37 (middle). Single-headed yellow arrows show angular changes of glandular structures relative to a common reference point. The histogram (right) displays angular differences for glandular structures between the two frames; (b)&#160;differential glandular deformation in frames 43 (left) and 44 (middle). Double-headed arrows indicate length variations of glandular structures. The histogram (right) displays length differences for glandular structures between the two frames. Traditional image stitching methods typically rely on feature point matching and geometric transformations. Brown and Lowe 9 , 10 pioneered Scale-Invariant Feature Transform (SIFT)&#8211;based global homography alignment for panoramic image generation in computational image stitching, laying the foundation for commercial applications. Subsequent research focused on hardware advancements and feature descriptor optimizations, including Principal Component Analysis-SIFT (PCA-SIFT), 11 Speeded-Up Robust Features (SURF), 12 and Features from Accelerated Segment Test (FAST). 13 However, the global unresponsive matrix alignment method based on feature point matching requires that the optical centers of the images involved in the stitching coincide; otherwise, parallax will occur to fail to align the pictures. To solve this problem, Gao et&#160;al. 14 introduced dual-plane homography by separating scenes into foreground and background layers. Further, Lin et&#160;al. 15 rely on a plurality of affine transformations to align the pictures, giving the image stitching some ability to handle parallax. Zaragoza et&#160;al. 16 proposed dense grid-based homography adaptation to mitigate parallax and distortion artifacts. Zhang et&#160;al. 17 improved the stitching performance of large parallax scenes based on a video de-jittering method and seam line domination. Gao et&#160;al. 18 estimated the optimal geometric transformation using the seam-cut method. Li et&#160;al. 19 obtained the optimal seam line using automatic quaternions. The advent of deep learning catalyzed paradigm shifts in image stitching, with convolutional neural networks (CNNs) being progressively adapted for this domain. Yi et&#160;al. 20 &#8211; 22 pioneered LIFT-Net, a tripartite neural framework comprising a dedicated subnetwork for feature point detection, orientation estimation, and descriptor generation. Subsequently, Han et&#160;al. 23 developed MatchNet, which employs a dual-tower architecture for feature extraction, followed by similarity computation through three cascaded fully connected layers. DeTone et&#160;al. 24 introduced HomographyNet, leveraging VGG-inspired architectures to regress the eight parameters of homography matrices directly. Simultaneously, Nie et&#160;al. 25 proposed a rectangling network to eliminate irregular boundaries caused by homography transformations. Jiang et&#160;al. integrated adversarial attacks into the stitching pipeline to enhance robustness, whereas Cai et&#160;al. 26 incorporated geometric constraints for natural image stitching applications. However, most of these methods rely on the stability of feature point detection without fully considering the impact of rolling shutter effects on feature detection. In endomicroscopic image stitching, Vercauteren et&#160;al. 6 pioneered diffeomorphic modeling for confocal image reconstruction, which was subsequently integrated into the commercial CLE system developed by MKT. Building upon this imaging platform, Gong et&#160;al. 27 proposed a context-aware registration algorithm (CWCR) that combines feature-based matching with template-based rigid alignment to achieve global frame registration. Kose et&#160;al. 28 implemented video stitching through SIFT feature extraction and graph-cut optimization. Rosa et&#160;al. 29 developed a servo motor-controlled probe positioning system, where image registration was guided by precise motor position feedback. Bedard et&#160;al. 30 introduced a cost-effective discrete Fourier transform-based approach for frame alignment in a custom high-resolution endomicroscopic system. In gastroscopic applications, Zhou et&#160;al. 31 achieved real-time stitching through stable frame selection combined with SURF feature matching. Zenteno et&#160;al. 32 enhanced stitching robustness by employing optical flow-based image selection to minimize texture discontinuities. These methodologies demonstrate systematic optimization of stitching algorithms tailored to specific endomicroscopic imaging characteristics. In this paper, we propose a novel stitching technique for the gastrointestinal CLE imaging system developed by our research group. 2 We developed a Dual-Path Gaussian U-Net (DGU-Net) that extracts Gaussian and non-Gaussian features at different resolutions through its dual-path mechanism, enabling better perception of textural features in gastrointestinal CLE images for more accurate segmentation mask generation. The segmentation masks are then used to filter stable frames from the image sequence and extract foreground features, thereby suppressing rolling shutter effects. Finally, we utilize the foreground SIFT features of the selected stable frame sequence to generate stitched images. The main contributions to this work can be summarized as follows: 1. Constructed a meticulously annotated segmentation dataset comprising 80 gastrointestinal CLE images and developed a dedicated gland segmentation network for digestive endomicroscopy 2. Developed a segmentation mask-based frame selection method that identifies stable frames in gastrointestinal CLE image sequences, effectively suppressing rolling shutter effects 3. Successfully implemented gastrointestinal confocal stitching using our proposed method. Comparative experiments were conducted with conventional methods to demonstrate the superiority of our approach. The remainder of this paper is organized as follows: Section&#160; 2 details the proposed methodology. Section&#160; 3 presents experimental validation and results. Section&#160; 4 discusses limitations and future directions. Section&#160; 5 concludes by summarizing key contributions and highlighting potential clinical applications of the proposed framework. 2 Materials and Methods The CLE imaging system employs a galvanometer-based X Y scanning mechanism with counter-rotating mirrors for row-wise acquisition. 2 Each frame is sequentially captured by scanning individual rows while progressively advancing columns along the Y -axis. Probe movement during this scanning process generates nonuniform spatial displacements&#8212;particularly evident in glandular structures&#8212;manifesting as rolling shutter effects. We developed a novel image stitching framework based on a gastrointestinal gland segmentation network, as shown in Fig.&#160;2 , to suppress rolling shutter effects and improve stitching accuracy. Fig. 2 Confocal endomicroscopic image stitching pipeline, consisting of three key stages: (1)&#160;a gland segmentation network trained to generate binary masks isolating diagnostically relevant regions, (2)&#160;stable frame selection by minimizing inter-frame projection differences using segmentation masks, and (3)&#160;SIFT descriptors extracted exclusively from stabilized foreground regions enable robust feature matching and seamless image alignment. First , we trained a gland segmentation network on CLE images to generate binary masks isolating glandular foregrounds from background interference. Subsequently , these masks guided frame stabilization through temporal consistency analysis, where sequential frames were aligned using mask-based projective differences to compensate for rolling shutter effects. Finally , SIFT descriptors were exclusively extracted from stabilized glandular regions to enable robust feature matching, followed by multi-band blending to generate seamless panoramic images. 2.1 Segmentation Network As shown in Fig.&#160;3 , we designed a DGU-Net that integrates Gaussian smoothing and dual-path mechanisms to enhance feature extraction and segmentation performance. 33 &#8211; 36 The dual-path mechanism processes features through two parallel streams, which are later fused to capture both local texture details and global contextual information. Fig. 3 Architecture of the Dual-Path Gaussian U-Net with 892 &#215; 892 &#8201;&#8201; pixel input and output dimensions. Blue blocks represent DoubleConv layers with 3 &#215; 3 convolutional kernels, whereas yellow blocks denote GaussDoubleConv layers incorporating a 9 &#215; 9 Gaussian convolutional kernel before the DoubleConv layer. Black arrows indicate data flow, green arrows represent downsampling layers, red arrows denote upsampling layers, and blue arrows signify concatenation operations. 2.1.1 Gaussian convolution The proposed architecture relies on Gaussian convolutional layers. These layers apply a fixed Gaussian kernel to the input feature map, smoothing the data while maintaining structural information. The 2D Gaussian kernel is defined as a size: G ( x , y ) = 1 2 &#960; &#963; 2 &#8201; exp ( &#8722; ( x &#8722; &#956; ) 2 + ( y &#8722; &#956; ) 2 2 &#963; 2 ) , (1) where ( x , y ) represents the spatial coordinates, &#956; is the mean (centered at the kernel&#8217;s midpoint), and &#963; is the standard deviation. The kernel is normalized to ensure that the sum of its weights equals one. 2.1.2 Encoder After initial Gaussian convolution preprocessing, feature maps enter the five-stage progressive encoder structure. The first path applies Gaussian convolution before the initial downsampling, whereas the second path maintains original feature processing until the fifth downsampling stage, extracting both Gaussian and non-Gaussian features at different resolutions. Feature concatenation occurs at the fourth and fifth downsampling levels of both paths, enabling fusion of local details with global features. This design preserves U-Net&#8217;s multi-scale extraction advantages while enhancing feature diversity through differentiated dual-path processing. 2.1.3 Decoder The decoder mirrors the encoder&#8217;s structural design. The first upsampling path employs Gaussian convolution in its initial upsampling, whereas the second path applies Gaussian processing at the fourth upsampling stage&#8212;balancing detail preservation with feature smoothing to ensure edge continuity in final outputs. Following feature concatenation of both paths, a final upsampling restores feature map dimensions before two convolutional layers generate the segmentation mask. 2.2 Stable Frame Selection Clinical CLE examinations follow a &#8220;slow translation-brief pause&#8221; model where operators intermittently move and stabilize the probe. This generates alternating sequences of roll shutter-distorted frames and stable frames. Stabilizing frame selection is critical for mitigating rolling shutter-induced distortions in image stitching. The gland segmentation network was employed to generate binary masks for each frame. Structural variations were quantified through row-wise and column-wise projection analysis of these masks: The row projection R ( y ) and column projection C ( x ) are defined as R ( y ) = &#8721; x = 1 W M ( x , y ) , (2) C ( x ) = &#8721; y = 1 H M ( x , y ) , (3) W and H denote image width and height, respectively, and represent the binary segmentation mask. The projection difference metric D ( i , i + 1 ) between consecutive frames was computed as D ( i , i + 1 ) = &#8721; y = 1 H ( R i ( y ) &#8722; R i + 1 ( y ) ) 2 + &#8721; x = 1 W ( C i ( x ) &#8722; C i + 1 ( x ) ) 2 , (4) where R i ( y ) and R i + 1 ( y ) denote row projections of frames i and i + 1 , respectively, whereas C i ( x ) and C i + 1 ( x ) represent their column projections. Frames are classified as stable if D ( i , i + 1 ) , all are below an empirically determined threshold and discarded as motion-corrupted frames. This value corresponds to the optimal separation point that maximizes discrimination between lower-value clusters (indicating stable frames) and higher-value clusters (indicating motion-corrupted frames) in the characteristic bimodal distribution. In this work, this threshold was set to 0.8. 2.3 Image Stitching The stabilized frames and their foreground features were processed through the following pipeline. 2.3.1 Feature extraction Mask-constrained SIFT features were extracted by filtering keypoints using gland segmentation masks. Specifically, a binary mask is used to constrain the SIFT feature extraction process. Only keypoints located within the foreground region (where the mask value is 1) are retained, whereas keypoints in the background region are discarded. This ensures that the extracted features are relevant to the foreground objects and significantly reduces the impact of background clutter on the stitching process. SIFT descriptors (128-dimensional feature vectors encoding local gradient histograms) are computed at all detected keypoints. Subsequently, the set of filtered keypoints K filtered is defined as K filtered = { ( x , y ) | M ( x , y ) = 1 } , (5) where ( x , y ) are the coordinates of the keypoints detected by SIFT, M ( x , y ) are the keypoints that are within the segmentation masks. This filtering step ensures that only keypoints within the foreground region are considered for further processing. 2.3.2 Feature matching After extracting all features from the images, we use the FLANN-based matching algorithm and Lowe&#8217;s Ratio Test to find correspondences. The ratio test filters ambiguous matches out by retaining only matches where the distance between the best and second best matches is below a threshold (ratio = 0.75). This step produces reliable feature correspondences for image pairs that are used to estimate geometric transformations between images. 2.3.3 Homography estimation We estimate the homography matrix for each image pair using the matched feature points. The homography matrix represents the geometric transformation (e.g., rotation, translation, scaling) that aligns one image with another. Random Sample Consensus (RANSAC) is used to handle feature matches. RANSAC selects a subset, computes the homography, and evaluates its consistency. The homography that has the most inliers will be chosen as the final transform. This robust estimation ensures that alignment is accurate even when features are noisy or mismatched. 2.3.4 Gain compensation We apply a gain-compensation algorithm to address brightness inconsistencies between images. The algorithm minimizes differences in intensity between overlapping regions of image pairs. It solves a system of linear equations to calculate a gain factor per image. The gain factors are then applied to the images to ensure seamless blending of the final panorama. This step is crucial for producing visually consistent results. 2.3.5 Multi-band blending Finally, the aligned pictures are blended together into a single panoramic image using a multi-band blend. This technique divides each image using Gaussian Pyramids into multiple frequency bands and blends them separately. The low-frequency band is blended over a large spatial range to smooth out color and brightness differences, whereas the high-frequency band is blended over a small range to preserve fine detail. The blended bands are recombined to create the final panorama. This ensures that the final result will be visually seamless and full of detail. 2.4 Dataset A custom fiber-optic CLE probe was used to image excised rat stomach tissues with DSS-induced ulcerative gastritis. System specifications included a 488-nm excitation laser, an intravenous fluorescein sodium contrast agent, 1.4 - &#956; m lateral resolution, 512.5 &#215; 512.5 &#8201;&#8201; &#956; m FOV, five fps acquisition rate, and 892 &#215; 892 &#8201;&#8201; px image size. The dataset comprises 400 frames acquired from two rats (200 frames per rat). From these, 80 images were randomly selected for segmentation network annotation, with multiple glandular structures annotated in each image. Ground truth segmentation masks were meticulously annotated by two experienced researchers under the supervision of a senior endoscopist. All imaging experiments were approved by the animal experiment guidelines of the Animal Experimentation Ethics Committee of Huazhong University of Science and Technology (HUST, Wuhan, China). 3 Result 3.1 Segmentation Performance In this section, experimental results are presented of our DualPath Gaussian UNet (Ours) , which we have proposed for gland segmentation on confocal endomicroscopy of the digestive system. To evaluate the effectiveness of our approach, we conducted extensive experiments including ablation studies, comparisons with other segmentation methods, and comparisons to state-of-the-art segmentation methods. Results are reported based on five widely used evaluation metrics: accuracy (AC), Dice coefficient (Dice), Jaccard Index (JA), sensitivity (SE), and specificity (SP). 3.1.1 Ablation study To compare the various variants of our proposed network and demonstrate their effectiveness, we conducted a study on ablation. U-Net: U-Net is the baseline architecture. GU-Net: A U-Net variant incorporating Gaussian convolution (GaussianConv2d) instead of standard convolutions. DU-Net: A U-Net variant introducing a dual-path structure but without Gaussian convolution. Ours (DualPath Gaussian UNet): We propose a network that combines Gaussian Convolution with a dual-path architecture. The results of the ablation study are summarized in Table&#160;1 . Our proposed DualPath Gaussian U-Net achieves superior performance on key segmentation metrics, with an AC of 88.10, a Dice of 85.17, JA of 74.75, and SE of 87.03, significantly outperforming other variants. Although the specificity (SP, 88.83) is marginally lower than DU-Net (89.79), the overall performance demonstrates the synergistic effect of Gaussian convolution and dual-path structure in enhancing the network&#8217;s capability to segment fine glandular structures. Notably, GU-Net underperformed the baseline U-Net due to excessive smoothing, whereas DU-Net&#8217;s higher Dice and JA scores confirm the effectiveness of the dual-path architecture. As shown in Fig.&#160;4 , the segmentation results for the proposed method and the variant networks are presented. Table 1 Ablation study of different network components. Methods AC Dice JA SE SP U-Net 37 86.25 83.03 71.66 85.91 86.48 GU-Net 85.41 82.16 70.32 85.34 85.47 DU-Net 87.45 84.89 73.04 84.01 89.79 Ours 88.10 85.17 74.75 87.03 88.83 The best results are highlighted in bold. Fig. 4 Ablation study results, presenting contour comparisons between the proposed method, variant networks, and ground truth. 3.1.2 Comparison with state-of-the-art methods We compared our proposed DualPath Gaussian U-Net with several state-of-the-art segmentation methods, including U-Net, 37 U-Net++, 38 U-Net3+, 39 Attention U-Net (Att U-Net), 40 Adaptive Attention U-Net (AAU-Net), 41 and SegNet. 42 The results are presented in Table&#160;2 . Table 2 Segmentation results of different methods. Methods AC Dice JA SE SP U-Net 37 86.25 83.03 71.66 85.91 86.48 U-Net++ 38 87.09 84.23 73.26 87.36 86.90 U-Net3+ 39 87.10 84.11 73.15 86.82 87.29 Att U-Net 40 77.54 76.51 62.72 93.33 66.80 AAU-Net 41 84.99 81.64 69.74 85.47 84.66 SegNet 42 83.43 80.05 67.42 84.47 82.57 Ours 88.10 85.17 74.75 87.03 88.83 The best results are highlighted in bold. Our method outperforms the second-best performer across most evaluation metrics. We achieved improvements of +1.0 AC,+0.94 Dice, and +1.49 JA over the second-best performer. As shown in Fig.&#160;5 , the proposed method achieves a cleaner segmentation boundary than other approaches. These results demonstrate the superiority of our approach when handling the complex structures and varying intensity present in confocal endomicroscopy images. Fig. 5 Comparison study results. For clarity, competing methods are divided into two groups, each showing contour comparisons between the proposed method, three competing methods, and ground truth: (a)&#160;UNet, UNet++, and UNet3+; (b)&#160;AttUNet, AUUNet, and SegNet. 3.1.3 External test To validate the generalization capability of our approach, we evaluated the segmentation network on an external breast ultrasound dataset comprising 42 images, each containing an annotated breast tumor region and uniformly resized to 128 &#215; 128 &#8201;&#8201; pixels as network input. 43 Compared with gastrointestinal confocal endomicroscopy images, breast ultrasound images differ in texture patterns and lesion morphology, yet share similar segmentation challenges in distinguishing salient structures from noisy backgrounds, making them an appropriate benchmark for evaluating our model&#8217;s universal feature extraction capability. As shown in Table&#160;3 , the proposed method achieved the highest accuracy (AC) and Jaccard Index (JA) among all evaluated networks. Although not optimal in all metrics, its performance across different medical imaging modalities confirms the generalization of the dual-path Gaussian architecture for segmentation tasks. Table 3 Segmentation performance comparison on the external breast ultrasound dataset. Methods AC Dice JA SE SP U-Net 37 97.08 92.81 83.31 87.55 98.47 U-Net++ 38 97.73 89.28 83.39 93.74 98.28 U-Net3+ 39 97.68 89.35 82.44 89.68 98.79 Att U-Net 40 97.50 87.24 81.00 87.82 98.83 AAU-Net 41 97.58 88.14 81.64 88.75 98.80 SegNet 42 96.70 87.08 77.04 87.01 98.11 Ours 98.13 91.41 85.92 94.07 98.69 The best results are highlighted in bold. 3.1.4 Experiment details The loss function for network training is binary-cross-entropy. Adam optimizer is used to train the network. The initial learning rate is set at 0.00005. The dataset is randomly split into training (80%) and validation (20%) sets using a fixed random seed to ensure reproducibility. Multiple cross-validation shows that the best segmentation performance is obtained when epoch size and batch size are set to 100 and 2, respectively. The development environment consists of Pytorch with CUDA acceleration 12.1, Python 3.9, and an NVIDIA RTX3090 GPU. 3.2 Stabilization Performance In this section, we present the results from the frame selection method for mitigating rolling shutter effects in confocal image sequences of the digestive system. We employed the video stabilization evaluation framework proposed by M. Grundmann 44 to assess the improvement in stability of the stabilized frame sequence compared with the original sequence by estimating the variations in the camera path. 3.2.1 Experimental protocol Because the frame selection method removes shaky frames from the original image sequence, the stabilized image sequence has fewer frames. Before conducting the stability evaluation, it is necessary to interpolate frames in the stabilized sequence to match the frame count of the original sequence. We employ the Farneback optical flow 45 method, with the interpolated frame I interp is computed as I interp ( x , y ) = ( 1 &#8722; &#945; ) &#183; I 1 ( x &#8242; , y &#8242; ) + &#945; &#183; I 2 ( x &#8242; , y &#8242; ) , (6) where I 1 ( x &#8242; , y &#8242; ) and I 2 ( x &#8242; , y &#8242; ) denote forward- and backward-warped frames via the optical flow field F ( x , y ) = ( F x , F y ) with &#945; &#8712; [ 01 ] controlling temporal weighting. The forward optical flow is remapped as ( x &#8242; , y &#8242; ) = ( x + &#945; &#183; F x ( x , y ) , y + &#945; &#183; F y ( x , y ) ) , and the reverse optical flow is remapped as ( x &#8242; , y &#8242; ) = ( x &#8722; 1 &#8722; &#945; &#183; F x ( x , y ) , y &#8722; 1 &#8722; &#945; &#183; F y ( x , y ) ) , The interpolation factor &#945; is determined by, where T is the total interpolated frames between two stable frames, and t is the current interpolation index. Subsequently, the camera path of the image sequence is estimated, and the affine change matrix T is estimated for each neighboring frame, accumulating the neighboring frame transformation matrix T accum = T 1 &#183; T 2 &#183; &#8230; &#183; T i , and the extraction of translation information T x and T y can be used to plot camera motion trajectories of the original and stabilized image sequence. 3.2.2 Performance comparison As shown in Fig.&#160;6 , we present the camera paths of the original image sequence and the stabilized image sequence with frame count alignment, showing a reduction in displacement variance along both the Y -axis and X -axis. After stable frame selection and subsequent frame interpolation using the optical flow method to align the frame count, the stabilized image sequence demonstrates lower displacement variance in the X and Y directions than the original sequence. This indicates a jitter reduction and effectively suppresses the rolling shutter effects in the stabilized sequence. Fig. 6 Camera motion paths in the X and Y directions of the original image sequence, and the frame-interpolated stabilized sequence over 150 frames. Our method reduces the jitter of the image sequence. 3.3 Stitching Performance This section compares the stitching results between our method and the traditional AutoStitch technique and shows how our method preserves important diagnostic information during confocal endomicroscope inspections of the digestive system. As shown in Fig.&#160;7 , for 50 consecutive frames of confocal images of a rat stomach, the AutoStitch method, under the interference of the rolling shutter effects, resulted in incorrect scaling and transformations during image registration, causing some images to be stitched into erroneous positions. By contrast, our method utilizes gland segmentation masks to filter stable frames and foreground feature points, effectively suppressing the rolling shutter effects and background noise. As shown in Fig.&#160;7(b) , the resulting stitched images maintain anatomical continuity and eliminate mismatches. Fig. 7 A 50-frame sequence of confocal endomicroscopic images of the gastrointestinal tract: (a)&#160;auto-stitching result, showing misalignments; (b)&#160;result of our proposed method. A comparison was made between the proposed method and the conventional Auto Stitch approach. Figure&#160;7 demonstrates stitching outcomes generated from sequential CLE frames of gastrointestinal examinations. For a sequence of confocal endomicroscopic images of the digestive tract continuously captured by the same device, the clarity of each frame should be consistent, and the resulting stitched image should exhibit more uniform sharpness across all regions. As shown in Fig.&#160;8 , we divided the stitched images into several equally sized small patches. We tested the variance of sharpness across these patches at different patch sizes for both our method and the auto-stitching method, as presented in Table&#160;4 . A lower variance indicates better consistency in sharpness and higher quality of the stitched image. According to Table&#160;4 , under different metrics and patch segmentation sizes, the stitched images generated by our method consistently demonstrate superior uniformity. Fig. 8 Block-based sharpness consistency evaluation (block size: 128 &#215; 128 &#8201;&#8201; pixels ): (a)&#160;grid partitioning of AutoStitch result&#8212;the stitched image divided into equal-sized blocks, with border regions containing black margins treated as invalid; (b)&#160;sharpness heatmap for AutoStitch&#8212;individual block sharpness quantified using Laplacian variance, normalized and visualized through a thermal color map (purple indicating lower values, blue indicating higher values); (c)&#160;grid partitioning of proposed method&#8217;s result under identical partitioning scheme; and (d)&#160;sharpness heatmap for proposed method&#8217;s result under identical partitioning scheme. Table 4 Comparison of variance in image sharpness evaluation metrics across different tile sizes ( 64 &#215; 64 , 128 &#215; 128 , 256 &#215; 256 &#8201;&#8201; pixels ) for stitched images generated by our method and AutoStitch (AS). Lower variance values indicate superior sharpness uniformity. &#160; Laplacian 46 Sobel 47 Brenner 48 Ours AS Ours AS Ours AS 64 133.789 3120.209 664,856 2675004 745.4142 908.0533 128 33.04581 1878.634 529,234 2164628 409.6784 821.5077 256 40.9421 1035.356 316,227 1775584 476.4396 1029.241 Methods achieving better consistency (lower variance) are highlighted in bold. To further evaluate stitching quality on the 100-frame sequence, we employed two complementary metrics: Structural Similarity Index Measure (SSIM) and root mean square error (RMSE), as presented in Table&#160;4 . Although the SSIM value of the proposed method was marginally lower than that of Auto-Stitch, this is attributed to the frame selection process, which intentionally excludes motion-distorted frames, leading to increased variations in temporal continuity. Conversely, the proposed method achieved a lower RMSE, indicating superior suppression of large misalignments. These results demonstrate enhanced precision in maintaining diagnostic feature alignment while ensuring clinically acceptable structural integrity ( Table&#160;5 ). Table 5 Comparison of Structural Similarity Index Measure (SSIM) and root mean square error (RMSE) for stitching images generated by our method and Auto-Stitching using a 100-frame sequence. Higher SSIM and lower RMSE indicate superior quality. Method SSIM 49 RMSE 49 Auto-stitching 0.8741 &#177; 0.0761 44.46 &#177; 31.63 Ours 0.8461 &#177; 0.1105 37.47 &#177; 21.01 Methods achieving better performance per metric are highlighted in bold. As shown in Fig.&#160;9 , our stitching results preserve fluorescein leakage points, a key feature for diagnosing atrophic gastritis in the confocal endomicroscope. 3 The proposed method, which utilizes gland segmentation masks for stable frame selection and foreground feature extraction, not only improves the accuracy and quality of stitching but also retains critical diagnostic information. Fig. 9 Stitching results of 100 consecutive confocal endomicroscopic images of the gastrointestinal tract. Red arrows indicate observable fluorescein leakage points. (Field of view: 1000 &#215; 700 &#8201;&#8201; &#956; m ). 4 Discussion In this paper, a method is presented for stitching confocal pictures of the digestive system based on a U-shaped CNN. During confocal endomicroscopy of the digestive system, disease stage and type diagnosis are based on the characteristics of the superficial cells and glands. The limited field of vision under high magnification in confocal endomicroscopy does not allow for the full lesion to be displayed in a single image. Image stitching can increase the field of view and provide a more accurate and comprehensive representation of the superficial state of the digestive system. This improves the efficiency of clinical examinations and the accuracy of diagnosis. Due to the line scan imaging principle of confocal endomicroscopy, multiple image regions are not captured at once. The rolling shutter effect is caused by the operator moving the probe. This results in mismatched images and affects the image quality. We use the texture features from confocal images with glandular tissue in the foreground to construct a Dual Path Gaussian UNet for obtaining segmentation masks. The network is able to better perceive texture features in images through Gaussian Convolution and a dual-path structure. As presented in Table&#160;2 , we achieve superior segmentation results compared to U-Net networks for confocal images of the digestive system. As shown in Table&#160;3 , the proposed model maintains robust segmentation performance on the external dataset. Although the training dataset comprises only 80 annotated images, each contains multiple glandular structures, resulting in almost 1000 annotated gland instances. This rich morphological diversity enables the model to achieve effective generalization capability despite the limited training volume. In addition, based on a clinical practice of &#8220;slow translation, brief pause,&#8221; we use a foreground segmentation to filter out interferences from free superficial cells and a frame selection to remove shaky images while retaining stable frames. The image sequence stabilized from stable frames shows reduced jitter and effectively suppresses the rolling shutter effects. As shown in Fig.&#160;7 , the panoramic image generated through SIFT feature registration of foreground elements in stable frames eliminates mismatches inherent to conventional methods. This approach provides enhanced visualization of glandular morphology and pathological indicators (e.g., fluorescein leakage points) under confocal endomicroscopy, enabling comprehensive assessment of the digestive tract&#8217;s superficial state. Clinicians can thereby accurately stage diseases, localize lesions, and improve diagnostic efficiency during confocal endoscopic examinations. This study has some limitations. The algorithm is not real-time because the stitching speed is slower than that of confocal endomicroscopy, resulting in a delay during actual examination. Second, as shown in Fig.&#160;6 , the pronounced gap in the Y -axis trajectory, the algorithm&#8217;s potential application is currently constrained by the lack of tracing technology specifically optimized for confocal endomicroscopy, where existing motion estimation frameworks exhibit incomplete compatibility with probe tracking characteristics, leading to difficulties in determining accurate stitched image contours and posing potential risks of contour deformation. Future work will focus primarily on improving the real-time performance of the algorithm and integrating object tracking algorithms to confirm endoscope movement paths, generate accurate stitched image contours, and ultimately produce more informative stitched images. This will improve the efficiency and diagnostic accuracy for confocal endomicroscopy of the digestive tract. 5 Conclusion In this paper, we propose a gland segmentation network-based stitching algorithm for the gastrointestinal CLE images. Our method leverages the unique textural features of gastrointestinal CLE images to generate precise gland segmentation masks, which are then employed to select stable frames and extract foreground features, thereby effectively suppressing rolling shutter effects and improving image stitching quality. Experimental results show that our proposed network outperforms several advanced deep learning segmentation methods in confocal endomicroscopic images of the gastrointestinal tract. Our image stitching method surpasses the AutoStitch method in accuracy and image quality. The resulting stitched images can display key diagnostic information, demonstrating significant potential for practical applications and clinical diagnosis support. Acknowledgments This work was supported by the National Key Research and Development Program of China (Grant No. 2022YFC2404401), National Natural Science Foundation of China (Grant No. 62475063), and Open Project Program of Wuhan National Laboratory for Optoelectronics (Grant No. 2023WNLOKF013). The authors utilized DeepSeek 50 for language refinement and grammatical improvement during manuscript preparation. This AI tool was prompted with specific requests such as &#8220;Polish the academic tone of this paragraph&#8221; and &#8220;Check technical terms consistency in the Methods section.&#8221; Biographies of the authors are not available. Disclosures The authors declare no conflicts of interest. Code and Data Availability The confocal endomicroscopy image dataset can be obtained from the authors upon reasonable request. The DGU-Net source code is available on the GitHub via the link https://github.com/PtK929/DGU-Net/tree/main . References 1. Yu Z. et al. , &#8220; Differences in the incidence and mortality of digestive cancer between Global Cancer Observatory 2020 and Global Burden of Disease 2019 ,&#8221; Int. J. Cancer 154 ( 4 ), 615 &#8211; 625 ( 2024 ). 10.1002/ijc.34740 37750191 2. Wang J. et al. , &#8220; A confocal endoscope for cellular imaging ,&#8221; Engineering 1 ( 3 ), 351 &#8211; 360 ( 2015 ). ENGNA2 0013-7782 10.15302/J-ENG-2015081 3. Li Z. et al. , &#8220; New classification of gastric pit patterns and vessel architecture using probe-based confocal laser endomicroscopy ,&#8221; J. Clin. Gastroenterol. 50 ( 1 ), 23 &#8211; 32 ( 2016 ). 10.1097/MCG.0000000000000298 25751373 4. Vennelaganti S. et al. , &#8220; Validation of probe-based Confocal Laser Endomicroscopy (pCLE) criteria for diagnosing colon polyp histology ,&#8221; J. Clin. Gastroenterol. 52 ( 9 ), 812 &#8211; 816 ( 2018 ). 10.1097/MCG.0000000000000927 28885303 5. Han W. et al. , &#8220; Confocal laser endomicroscopy for detection of early upper gastrointestinal cancer ,&#8221; Cancers 15 ( 3 ), 776 ( 2023 ). 10.3390/cancers15030776 36765734 PMC9913498 6. Vercauteren T. , &#8220; Image registration and mosaicing for dynamic in vivo fibered confocal microscopy &#8221;. 7. Rengarajan V. et al. , &#8220; Image registration and change detection under rolling shutter motion blur ,&#8221; IEEE Trans. Pattern Anal. Mach. Intell. 39 ( 10 ), 1959 &#8211; 1972 ( 2017 ). ITPIDJ 0162-8828 10.1109/TPAMI.2016.2630687 27875216 8. Lao Y. Ait-Aider O. , &#8220; Rolling shutter homography and its applications ,&#8221; IEEE Trans. Pattern Anal. Mach. Intell. 43 ( 8 ), 2780 &#8211; 2793 ( 2021 ). ITPIDJ 0162-8828 10.1109/TPAMI.2020.2977644 32142425 9. Brown M. Lowe D. G. , &#8220; Recognising panoramas ,&#8221; in Proc. Ninth IEEE Int. Conf. Comput. Vision , IEEE , Nice , Vol.&#160;2, pp.&#160; 1218 &#8211; 1225 ( 2003 ). 10. Brown M. Lowe D. G. , &#8220; Automatic panoramic image stitching using invariant features ,&#8221; Int. J. Comput. Vision 74 ( 1 ), 59 &#8211; 73 ( 2007 ). IJCVEQ 0920-5691 10.1007/s11263-006-0002-3 11. Ke Y. Sukthankar R. , &#8220; PCA-SIFT: a more distinctive representation for local image descriptors ,&#8221; in Proc. IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognit., CVPR , IEEE , Washington, DC , Vol.&#160;2, pp.&#160; 506 &#8211; 513 ( 2004 ). 12. Bay H. Tuytelaars T. Van Gool L. , &#8220; SURF: speeded up robust features ,&#8221; Lect. Notes Comput. Sci. 3951 , 404 &#8211; 417 ( 2006 ). LNCSD9 0302-9743 10.1007/11744023_32 13. Rosten E. Drummond T. , &#8220; Machine learning for high-speed corner detection ,&#8221; Lect. Notes Comput. Sci. 3951 , 430 &#8211; 443 ( 2006 ). LNCSD9 0302-9743 10.1007/11744023_34 14. Gao J. Kim S. J. Brown M. S. , &#8220; Constructing image panoramas using dual-homography warping ,&#8221; in CVPR , IEEE , Colorado Springs, CO , pp.&#160; 49 &#8211; 56 ( 2011 ). 10.1109/CVPR.2011.5995433 15. Lin W.-Y. et al. , &#8220; Smoothly varying affine stitching ,&#8221; in CVPR , IEEE , Colorado Springs, CO , pp.&#160; 345 &#8211; 352 ( 2011 ). 10.1109/CVPR.2011.5995314 16. Zaragoza J. et al. , &#8220; As-projective-as-possible image stitching with moving DLT ,&#8221; in IEEE Conf. Comput. Vision and Pattern Recognit. , IEEE , Portland, OR , pp.&#160; 2339 &#8211; 2346 ( 2013 ). 10.1109/CVPR.2013.303 26353303 17. Zhang F. Liu F. , &#8220; Parallax-tolerant image stitching ,&#8221; in IEEE Conf. Comput. Vision and Pattern Recognit. , IEEE , Columbus, OH , pp.&#160; 3262 &#8211; 3269 ( 2014 ). 10.1109/CVPR.2014.423 18. Gao J. et al. , &#8220; Seam-driven image stitching ,&#8221; in Eurographics 2013 - Short Papers , The Eurographics Association , p.&#160; 4 ( 2013 ). 10.2312/CONF/EG2013/SHORT/045-048 19. Li J. Zhou Y. , &#8220; Automatic quaternion-domain color image stitching ,&#8221; IEEE Trans. Image Process. 33 , 1299 &#8211; 1312 ( 2024 ). IIPRE4 1057-7149 10.1109/TIP.2024.3361688 38329845 20. Verdie Y. et al. , &#8220; TILDE: a temporally invariant learned DEtector ,&#8221; in IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR) , IEEE , Boston, MA , pp.&#160; 5279 &#8211; 5288 ( 2015 ). 10.1109/CVPR.2015.7299165 21. Simo-Serra E. et al. , &#8220; Discriminative learning of deep convolutional feature point descriptors ,&#8221; in IEEE Int. Conf. Comput. Vision (ICCV) , IEEE , Santiago , pp.&#160; 118 &#8211; 126 ( 2015 ). 10.1109/ICCV.2015.22 22. Yi K. M. et al. , &#8220; Learning to assign orientations to feature points ,&#8221; in IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR) , IEEE , Las Vegas, Nevada , pp.&#160; 107 &#8211; 116 ( 2016 ). 10.1109/CVPR.2016.19 23. Han X. et al. , &#8220; MatchNet: unifying feature and metric learning for patch-based matching ,&#8221; in IEEE Conf. Comput. Vision and Pattern Recognit. (CVPR) , IEEE , Boston, MA , pp.&#160; 3279 &#8211; 3286 ( 2015 ). 10.1109/CVPR.2015.7298948 24. DeTone D. Malisiewicz T. Rabinovich A. , &#8220; Deep image homography estimation ,&#8221; arXiv:1606.03798 ( 2016 ). 25. Nie L. et al. , &#8220; Deep rectangling for image stitching: a learning baseline ,&#8221; in IEEE/CVF Conf. Comput. Vision and Pattern Recognit. (CVPR) , pp.&#160; 5730 &#8211; 5738 ( 2022 ). 10.1109/CVPR52688.2022.00565 26. Cai W. Yang W. , &#8220; Object-level geometric structure preserving for natural image stitching ,&#8221; arXiv:2402.12677 ( 2024 ). 27. Gong L. et al. , &#8220; Robust mosaicing of endomicroscopic videos via context-weighted correlation ratio ,&#8221; IEEE Trans. Biomed. Eng. 68 ( 2 ), 579 &#8211; 591 ( 2021 ). IEBEAX 0018-9294 10.1109/TBME.2020.3007768 32746056 28. Kose K. et al. , &#8220; Automated video-mosaicking approach for confocal microscopic imaging in vivo: an approach to address challenges in imaging living tissue and extend field of view ,&#8221; Sci. Rep. 7 ( 1 ), 10759 ( 2017 ). SRCEC3 2045-2322 10.1038/s41598-017-11072-9 28883434 PMC5589933 29. Rosa B. et al. , &#8220; Building large mosaics of confocal edomicroscopic images using visual servoing ,&#8221; IEEE Trans. Biomed. Eng. 60 ( 4 ), 1041 &#8211; 1049 ( 2013 ). IEBEAX 0018-9294 10.1109/TBME.2012.2228859 23192481 30. Bedard N. et al. , &#8220; Real-time video mosaicing with a high-resolution microendoscope ,&#8221; Biomed. Opt. Express 3 ( 10 ), 2428 ( 2012 ). BOEICL 2156-7085 10.1364/BOE.3.002428 23082285 PMC3469983 31. Zhou H. Jayender J. , &#8220; Real-time nonrigid mosaicking of laparoscopy images ,&#8221; IEEE Trans. Med. Imaging 40 ( 6 ), 1726 &#8211; 1736 ( 2021 ). ITMID4 0278-0062 10.1109/TMI.2021.3065030 33690113 PMC8169627 32. Zenteno O. et al. , &#8220; Optical biopsy mapping on endoscopic image mosaics with a marker-free probe ,&#8221; Comput. Biol. Med. 143 , 105234 ( 2022 ). CBMDAW 0010-4825 10.1016/j.compbiomed.2022.105234 35093845 33. Chen Y.-F. Wang Z.-Y. Qin P. , &#8220; GL-UNet: a deep learning model of breast tumor lesion segmentation for MRI images ,&#8221; in 43rd Chin. Control Conf. (CCC) , IEEE , Kunming , pp.&#160; 7403 &#8211; 7407 ( 2024 ). 10.23919/CCC63176.2024.10662291 34. Fu J. et al. , &#8220; Dual attention network for scene segmentation ,&#8221; arXiv:1809.02983 ( 2019 ). 35. Zhang H. et al. , &#8220; Dual parallel net: a novel deep learning model for rectal tumor segmentation via CNN and transformer with Gaussian Mixture prior ,&#8221; J. Biomed. Inf. 139 , 104304 ( 2023 ). 10.1016/j.jbi.2023.104304 36736447 36. Ban Z. Liu J. Cao L. , &#8220; Superpixel segmentation using Gaussian mixture model ,&#8221; IEEE Trans. Image Process. 27 ( 8 ), 4105 &#8211; 4117 ( 2018 ). IIPRE4 1057-7149 10.1109/TIP.2018.2836306 29994528 37. Ronneberger O. Fischer P. Brox T. , &#8220; U-Net: convolutional networks for biomedical image segmentation ,&#8221; Lect. Notes Comput. Sci. 9351 , 234 &#8211; 241 ( 2015 ). LNCSD9 0302-9743 10.1007/978-3-319-24574-4_28 38. Zhou Z. et al. , &#8220; UNet++: redesigning skip connections to exploit multiscale features in image segmentation ,&#8221; IEEE Trans. Med. Imaging 39 ( 6 ), 1856 &#8211; 1867 ( 2020 ). ITMID4 0278-0062 10.1109/TMI.2019.2959609 31841402 PMC7357299 39. Huang H. et al. , &#8220; UNet 3+: a full-scale connected UNet for medical image segmentation ,&#8221; in ICASSP 2020-2020 IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP) , IEEE , Barcelona , pp.&#160; 1055 &#8211; 1059 ( 2020 ). 10.1109/ICASSP40776.2020.9053405 40. Oktay O. et al. , &#8220; Attention U-Net: learning where to look for the pancreas ,&#8221; arXiv:1804.03999 ( 2018 ). 41. Chen G. et al. , &#8220; AAU-Net: an adaptive attention U-Net for breast lesions segmentation in ultrasound images ,&#8221; IEEE Trans. Med. Imaging 42 ( 5 ), 1289 &#8211; 1300 ( 2023 ). ITMID4 0278-0062 10.1109/TMI.2022.3226268 36455083 42. Badrinarayanan V. Kendall A. Cipolla R. , &#8220; SegNet: a deep convolutional encoder-decoder architecture for image segmentation ,&#8221; IEEE Trans. Pattern Anal. Mach. Intell. 39 ( 12 ), 2481 &#8211; 2495 ( 2017 ). ITPIDJ 0162-8828 10.1109/TPAMI.2016.2644615 28060704 43. Zhuang Z. et al. , &#8220; An RDAU-NET model for lesion segmentation in breast ultrasound images ,&#8221; PLoS One 14 ( 8 ), e0221535 ( 2019 ). POLNCL 1932-6203 10.1371/journal.pone.0221535 31442268 PMC6707567 44. Grundmann M. Kwatra V. Essa I. , &#8220; Auto-directed video stabilization with robust L1 optimal camera paths ,&#8221; in CVPR , IEEE , Colorado Springs, CO , pp.&#160; 225 &#8211; 232 ( 2011 ). 10.1109/CVPR.2011.5995525 45. Farneb&#228;ck G. , &#8220; Two-frame motion estimation based on polynomial expansion ,&#8221; Lect. Notes Comput. Sci. 2749 , 363 &#8211; 370 ( 2003 ). LNCSD9 0302-9743 10.1007/3-540-45103-X_50 46. Xue W. et al. , &#8220; Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features ,&#8221; IEEE Trans. Image Process. 23 ( 11 ), 4850 &#8211; 4862 ( 2014 ). IIPRE4 1057-7149 10.1109/TIP.2014.2355716 25216482 47. Tiruppur Kumaran College for Women, India et al. , &#8220; Sobel operator and PCA for nearest target of retina images ,&#8221; IJIVP 11 ( 4 ), 2483 &#8211; 2491 ( 2021 ). 10.21917/ijivp.2021.0353 48. Chen L. Han M. Wan H. , &#8220; The fast iris image clarity evaluation based on Brenner ,&#8221; in 2nd Int. Symp. Instrum. and Meas. Sens. Network and Autom. (IMSNA) , IEEE , Toronto, ON , pp.&#160; 300 &#8211; 302 ( 2013 ). 10.1109/IMSNA.2013.6743274 49. Jiang Z. et al. , &#8220; Multispectral image stitching via global-aware quadrature pyramid regression ,&#8221; IEEE Trans. Image Process. 33 , 4288 &#8211; 4302 ( 2024 ). IIPRE4 1057-7149 10.1109/TIP.2024.3430532 39046864 50. DeepSeek , &#8220; DeepSeek AI Language Model ,&#8221; Hangzhou DeepSeek Artificial Intelligence, https://www.deepseek.com (accessed during manuscript preparation in 2025)."
}