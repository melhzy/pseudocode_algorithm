{
  "pmcid": "PMC12664954",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.847352",
  "metadata": {
    "journal_title": "Patterns",
    "journal_nlm_ta": "Patterns (N Y)",
    "journal_iso_abbrev": "Patterns (N Y)",
    "journal": "Patterns",
    "pmcid": "PMC12664954",
    "pmid": "41328156",
    "doi": "10.1016/j.patter.2025.101298",
    "title": "UltraLight VM-UNet: Parallel Vision Mamba significantly reduces parameters for skin lesion segmentation",
    "year": "2025",
    "month": "6",
    "day": "26",
    "pub_date": {
      "year": "2025",
      "month": "6",
      "day": "26"
    },
    "authors": [
      "Wu Renkai",
      "Liu Yinghao",
      "Ning Guochen",
      "Liang Pengchen",
      "Chang Qing"
    ],
    "abstract": "Summary Traditionally, to improve the segmentation performance of models, most approaches prefer to use more complex modules. This is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models, represented by Mamba, have become a strong competitor to traditional convolutional neural networks and transformers. In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named the PVM Layer, which achieves competitive performance with the lowest computational complexity while keeping the overall number of processing channels constant. We conducted segmentation experiments on three public datasets of skin lesions and showed that UltraLight VM-UNet exhibits competitive performance with only 0.049M parameters and 0.060 GFLOPs.",
    "keywords": [
      "skin lesion segmentation",
      "lightweight model",
      "Mamba"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Patterns (N Y)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Patterns (N Y)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3955</journal-id><journal-id journal-id-type=\"pmc-domain\">patterns</journal-id><journal-title-group><journal-title>Patterns</journal-title></journal-title-group><issn pub-type=\"epub\">2666-3899</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12664954</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12664954.1</article-id><article-id pub-id-type=\"pmcaid\">12664954</article-id><article-id pub-id-type=\"pmcaiid\">12664954</article-id><article-id pub-id-type=\"pmid\">41328156</article-id><article-id pub-id-type=\"doi\">10.1016/j.patter.2025.101298</article-id><article-id pub-id-type=\"pii\">S2666-3899(25)00146-1</article-id><article-id pub-id-type=\"publisher-id\">101298</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>UltraLight VM-UNet: Parallel Vision Mamba significantly reduces parameters for skin lesion segmentation</article-title></title-group><contrib-group><contrib contrib-type=\"author\" id=\"au1\"><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"R\">Renkai</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">1</xref><xref rid=\"aff2\" ref-type=\"aff\">2</xref><xref rid=\"aff3\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\" id=\"au2\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"Y\">Yinghao</given-names></name><xref rid=\"aff4\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\" id=\"au3\"><name name-style=\"western\"><surname>Ning</surname><given-names initials=\"G\">Guochen</given-names></name><xref rid=\"aff5\" ref-type=\"aff\">5</xref></contrib><contrib contrib-type=\"author\" id=\"au4\"><name name-style=\"western\"><surname>Liang</surname><given-names initials=\"P\">Pengchen</given-names></name><email>liangpengchen@shu.edu.cn</email><xref rid=\"aff2\" ref-type=\"aff\">2</xref><xref rid=\"cor1\" ref-type=\"corresp\">&#8727;</xref></contrib><contrib contrib-type=\"author\" id=\"au5\"><name name-style=\"western\"><surname>Chang</surname><given-names initials=\"Q\">Qing</given-names></name><email>robie0510@hotmail.com</email><xref rid=\"aff1\" ref-type=\"aff\">1</xref><xref rid=\"aff2\" ref-type=\"aff\">2</xref><xref rid=\"aff3\" ref-type=\"aff\">3</xref><xref rid=\"fn1\" ref-type=\"fn\">6</xref><xref rid=\"cor2\" ref-type=\"corresp\">&#8727;&#8727;</xref></contrib><aff id=\"aff1\"><label>1</label>Department of Geriatrics, Medical Center on Aging of Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai 200025, China</aff><aff id=\"aff2\"><label>2</label>School of Microelectronics, Shanghai University, Shanghai 201800, China</aff><aff id=\"aff3\"><label>3</label>The Innovation Center, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai 200025, China</aff><aff id=\"aff4\"><label>4</label>School of Health Science and Engineering, University of Shanghai for Science and Technology, Shanghai 200093, China</aff><aff id=\"aff5\"><label>5</label>School of Clinical Medicine, Tsinghua University, Beijing 100084, China</aff></contrib-group><author-notes><corresp id=\"cor1\"><label>&#8727;</label>Corresponding author <email>liangpengchen@shu.edu.cn</email></corresp><corresp id=\"cor2\"><label>&#8727;&#8727;</label>Corresponding author <email>robie0510@hotmail.com</email></corresp><fn id=\"fn1\"><label>6</label><p id=\"ntpara0010\">Lead contact</p></fn></author-notes><pub-date pub-type=\"collection\"><day>14</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"epub\"><day>26</day><month>6</month><year>2025</year></pub-date><volume>6</volume><issue>11</issue><issue-id pub-id-type=\"pmc-issue-id\">501518</issue-id><elocation-id>101298</elocation-id><history><date date-type=\"received\"><day>24</day><month>2</month><year>2025</year></date><date date-type=\"rev-recd\"><day>8</day><month>4</month><year>2025</year></date><date date-type=\"accepted\"><day>30</day><month>5</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>26</day><month>06</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-02 09:25:13.697\"><day>02</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Author(s)</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"main.pdf\"/><abstract id=\"abs0010\"><title>Summary</title><p>Traditionally, to improve the segmentation performance of models, most approaches prefer to use more complex modules. This is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models, represented by Mamba, have become a strong competitor to traditional convolutional neural networks and transformers. In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named the PVM Layer, which achieves competitive performance with the lowest computational complexity while keeping the overall number of processing channels constant. We conducted segmentation experiments on three public datasets of skin lesions and showed that UltraLight VM-UNet exhibits competitive performance with only 0.049M parameters and 0.060 GFLOPs.</p></abstract><abstract abstract-type=\"graphical\" id=\"abs0015\"><title>Graphical abstract</title><fig id=\"undfig1\" position=\"anchor\" orientation=\"portrait\"><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fx1.jpg\"/></fig></abstract><abstract abstract-type=\"author-highlights\" id=\"abs0020\"><title>Highlights</title><p><list list-type=\"simple\" id=\"ulist0010\"><list-item id=\"u0010\"><label>&#8226;</label><p id=\"p0010\">Mamba was analyzed in depth for its parameter key influences</p></list-item><list-item id=\"u0015\"><label>&#8226;</label><p id=\"p0015\">A new framework and theoretical analysis for lightweight model of medical scenarios</p></list-item><list-item id=\"u0020\"><label>&#8226;</label><p id=\"p0020\">Parallel Vision Mamba (or Mamba) is a winner for lightweight models</p></list-item><list-item id=\"u0025\"><label>&#8226;</label><p id=\"p0025\">Provides an UltraLight Vision Mamba UNet for mobile skin lesion segmentation</p></list-item></list></p></abstract><abstract abstract-type=\"editor-highlights\" id=\"abs0025\"><title>The bigger picture</title><p>Previously, medical image segmentation models were often constrained by computational resources and memory, limiting their application in mobile medical devices or resource-constrained environments. In contrast, the UltraLight VM-UNet model proposed in this study, with its low parameter count and computational complexity, is able to reduce the requirements on device resources without sacrificing performance. This means that it can conveniently assist doctors in diagnosing diseases in more scenarios, such as on medical devices in remote areas or in emergency medical rescue for fast processing of medical image data. In the long run, the ambition of this research is to promote the popularization and efficiency of medical image segmentation technology. It is hoped that accurate medical image diagnosis will no longer be limited to high-end equipment in large hospitals, but can be extended to all kinds of primary healthcare organizations, and even portable medical equipment, so that more patients can be diagnosed in a timely and accurate manner. Moreover, the lightweight module proposed in this study is plug-and-play, which can simply replace the redundant modules in other technology models, reducing the requirements on equipment resources while maintaining excellent performance. This will promote the rapid development of lightweight medical image processing, improve the overall quality and popularity of medical services, and bring positive social impacts.</p></abstract><abstract abstract-type=\"teaser\" id=\"abs0030\"><p>In the field of computer-aided medical diagnosis, medical image segmentation techniques (e.g., skin lesion segmentation) have been a key research hotspot. In this paper, the authors propose the UltraLight VM-UNet model, which operates efficiently in resource-constrained environments while guaranteeing high segmentation performance through an innovative PVM Layer. These results have implications for enhancing the diagnostic capabilities of mobile medical devices and promoting the widespread application of medical image segmentation technology.</p></abstract><kwd-group id=\"kwrds0010\"><title>Keywords</title><kwd>skin lesion segmentation</kwd><kwd>lightweight model</kwd><kwd>Mamba</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta><notes><p id=\"misc9010\">Published: June 26, 2025</p></notes></front><body><sec id=\"sec1\"><title>Introduction</title><p id=\"p0030\">With the rapid advancement of computer technology and hardware capabilities, computer-aided diagnosis has seen widespread adoption in the medical domain, with medical image segmentation serving as a critical component. Modern segmentation methods are predominantly powered by deep learning networks, particularly those based on convolutional neural networks and transformers. Convolutional architectures excel at extracting local features, yet they struggle to capture long-range dependencies effectively.<xref rid=\"bib1\" ref-type=\"bibr\"><sup>1</sup></xref><sup>,</sup><xref rid=\"bib2\" ref-type=\"bibr\"><sup>2</sup></xref><sup>,</sup><xref rid=\"bib3\" ref-type=\"bibr\"><sup>3</sup></xref> To address this limitation, previous works<xref rid=\"bib4\" ref-type=\"bibr\"><sup>4</sup></xref><sup>,</sup><xref rid=\"bib5\" ref-type=\"bibr\"><sup>5</sup></xref> have explored the use of large convolutional kernels, aiming to extend the receptive field and thus improve the modeling of distant spatial relationships. On the other hand, transformer-based architectures have recently garnered significant attention in medical image analysis.<xref rid=\"bib6\" ref-type=\"bibr\"><sup>6</sup></xref><sup>,</sup><xref rid=\"bib7\" ref-type=\"bibr\"><sup>7</sup></xref> Their self-attention mechanism inherently facilitates global context modeling by operating over sequences of image patches. However, this advantage comes at the cost of increased computational complexity, as the self-attention operation scales quadratically with respect to the input image size.</p><p id=\"p0035\">Moreover, to enhance the accuracy of computer-aided diagnosis, many existing approaches tend to increase the number of model parameters to boost predictive performance.<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref><sup>,</sup><xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> However, such strategies are often impractical in real-world clinical settings, where computational power and memory resources are inherently limited. In the context of mobile health applications, models must meet strict requirements for low parameter counts and minimal memory consumption.<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref> Consequently, there is a pressing need for algorithmic models that can deliver strong performance while maintaining low computational complexity&#8212;making them well suited for deployment on future mobile medical devices.</p><p id=\"p0040\">Recently, state-space models (SSMs) have shown linear complexity in terms of input size and memory occupation,<xref rid=\"bib11\" ref-type=\"bibr\"><sup>11</sup></xref><sup>,</sup><xref rid=\"bib12\" ref-type=\"bibr\"><sup>12</sup></xref> which makes them key to lightweight model foundations. In addition, SSMs excel at capturing remote dependencies, which can critically address the problem of convolution for extracting information over long distances. In Gu and Dao,<xref rid=\"bib13\" ref-type=\"bibr\"><sup>13</sup></xref> time-varying parameters were introduced into an SSM to obtain Mamba, and it was demonstrated that Mamba is able to process textual information with lower parameters than transformers. On the vision side, the introduction of Vision Mamba (VM)<xref rid=\"bib12\" ref-type=\"bibr\"><sup>12</sup></xref> has once again furthered people&#8217;s understanding of Mamba, which saves 86.8<inline-formula><mml:math id=\"M1\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of memory when reasoning about images of <inline-formula><mml:math id=\"M2\" altimg=\"si2.gif\"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>248</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>248</mml:mn></mml:mrow></mml:math></inline-formula> size without the need for an attentional mechanism. With the outstanding work of the researchers mentioned above, we are more confident that Mamba will occupy a major position in the future as a basic building block for lightweight models.</p><p id=\"p0045\">In this paper, we propose a lightweight model based on VM. We deeply explore the critical memory footprint of Mamba and the performance trade-offs, and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet). The proposed UltraLight VM-UNet represents an ultra-lightweight VM model with only 0.049M parameters and 0.060 GFLOPs, demonstrating highly competitive performance across three skin lesion segmentation tasks (<xref rid=\"fig1\" ref-type=\"fig\">Figure 1</xref>). Specifically, we delve into the keys affecting the computational complexity in Mamba, and conclude that the number of channels is a key factor in the explosive memory occupation for Mamba computation. We build on this finding of ours by proposing a parallel Vision Mamba (PVM) approach for processing deep features, named the PVM Layer, which simultaneously keeps the overall processing channel count constant. The proposed PVM Layer achieves excellent performance with surprisingly low parameters. In addition, the deep feature extraction of the proposed UltraLight VM-UNet that we implement using only the PVM Layer containing Mamba, as shown in <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>. In the <xref rid=\"sec4\" ref-type=\"sec\">methods</xref> section, we present the details of the proposed UltraLight VM-UNet as well as the key factors of the parameter effects in Mamba and the performance balancing approach.<fig id=\"fig1\" position=\"float\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Visualization of the comparison results for the ISIC2017 dataset</p><p>The <italic toggle=\"yes\">X</italic> axis corresponds to parameters and GFLOPs, the fewer the better. The <italic toggle=\"yes\">Y</italic> axis corresponds to segmentation performance (DSC), the higher the better.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr1.jpg\"/></fig><fig id=\"fig2\" position=\"float\" orientation=\"portrait\"><label>Figure 2</label><caption><p>The proposed UltraLight Vision Mamba UNet (UltraLight VM-UNet) model architecture</p><p>(A) The main part of the UltraLight VM-UNet.</p><p>(B) The skip-connection part of the UltraLight VM-UNet.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr2.jpg\"/></fig></p><p id=\"p0050\">Although similar parallel connection of modules has been mentioned in previous studies,<xref rid=\"bib14\" ref-type=\"bibr\"><sup>14</sup></xref><sup>,</sup><xref rid=\"bib15\" ref-type=\"bibr\"><sup>15</sup></xref><sup>,</sup><xref rid=\"bib16\" ref-type=\"bibr\"><sup>16</sup></xref><sup>,</sup><xref rid=\"bib17\" ref-type=\"bibr\"><sup>17</sup></xref> the impact of using parallel connection in Mamba is still unknown. Does parallel connection lead to a significant performance degradation of Mamba when utilizing the SSM selection mechanism? This is because parallel connections lead to a reduction in the number of feature channels learned per SSM. In this paper, we give the answer in detail. PVM or Mamba not only remain competitive in terms of performance, but achieve a significant reduction in parameters and computational complexity.</p><p id=\"p0055\">Our contributions and findings can be summarized as follows:<list list-type=\"simple\" id=\"olist0010\"><list-item id=\"o0010\"><label>(1)</label><p id=\"p0060\">An UltraLight Vision Mamba UNet (UltraLight VM-UNet) is proposed for skin lesion segmentation with parameters of only 0.049M and GFLOPs of only 0.060.</p></list-item><list-item id=\"o0015\"><label>(2)</label><p id=\"p0065\">A PVM method for processing deep features, named the PVM Layer, is proposed, which achieves excellent performance with the lowest computational complexity while keeping the overall number of processing channels constant. This can be generalized to the parallel connection of any Mamba variant.</p></list-item><list-item id=\"o0020\"><label>(3)</label><p id=\"p0070\">We provide an in-depth analysis of the key factors influencing the parameters of Mamba, and provide a theoretical basis for Mamba to become a mainstream module for lightweight models in the future.</p></list-item><list-item id=\"o0025\"><label>(4)</label><p id=\"p0075\">The proposed UltraLight VM-UNet parameters are 99.82<inline-formula><mml:math id=\"M3\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than the traditional pure Vision Mamba UNet model (VM-UNet) and 87.84<inline-formula><mml:math id=\"M4\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than the parameters of the current lightest Vision Mamba UNet model (LightM-UNet). In addition, the UltraLight VM-UNet maintains strong performance competitiveness in all three publicly available skin lesion segmentation datasets.</p></list-item></list></p></sec><sec id=\"sec2\"><title>Results</title><sec id=\"sec2.1\"><title>Architecture overview</title><p id=\"p0080\">The proposed UltraLight VM-UNet is shown in <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>. UltraLight VM-UNet has a total of 6 layers in a structure consisting of a U-shaped structure (encoder, decoder, and skip-connection path). The number of channels in the 6-layer structure is set to [8,16,24,32,48,64]. The extraction of shallow features in the first 3 layers is composed using a convolution module (Conv Block), where each layer includes a standard convolution with a <inline-formula><mml:math id=\"M5\" altimg=\"si3.gif\"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> kernel and a maximum pooling operation. The deeper features from layer 4 to layer 6 are our core part, where each layer consists of our proposed PVM Layer. The decoder part maintains the same setup as the encoder. The skip-connection path utilize the Channel Attention Bridge (CAB) module and the Spatial Attention Bridge (SAB) module for multilevel and multiscale information fusion.<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></p></sec><sec id=\"sec2.2\"><title>Mamba parameter impact analysis</title><p id=\"p0085\">The VM for PVM Layer is mainly composed using Mamba combined with residual connections and adjustment factors (<xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref>A), which allows traditional Mamba to improve the capture of remote spatial relations without introducing additional parameters and computational complexity.<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref> This has a better improvement in the performance of Mamba in visual tasks while keeping the parameter and computational complexity low.<fig id=\"fig3\" position=\"float\" orientation=\"portrait\"><label>Figure 3</label><caption><p>The proposed PVM Layer architecture</p><p>(A) The main part of the PVM Layer. VM is composed by Mamba combined with residual connection and adjustment factor.</p><p>(B) Mamba composition structure.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr3.jpg\"/></fig></p><p id=\"p0090\">Among SSM-based Mamba, the number of channels, the size of the SSM state dimension, the size of the internal 1D convolutional kernel, the projection dilation multiplier, and the rank of the step size all affect the parameters. And in this, the impact of the channel number is explosive, and its main influence is from the following multiple directions:</p><p id=\"p0095\">First, <inline-formula><mml:math id=\"M6\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> of the Mamba internal extended projection channel is determined by the product of the projection expansion multiplier and the number of input channels. This can be specifically expressed by the following equation:<disp-formula id=\"fd1\"><label>(Equation 1)</label><mml:math id=\"M7\" altimg=\"si5.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id=\"M8\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> is the internal expansion projection channel, <inline-formula><mml:math id=\"M9\" altimg=\"si6.gif\"><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> is the projection expansion multiplier (fixed at 2 by default), and <inline-formula><mml:math id=\"M10\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> is the number of input channels. We can conclude that <inline-formula><mml:math id=\"M11\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> will double up as the number of channels <inline-formula><mml:math id=\"M12\" altimg=\"si8.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> per layer in the model increases.</p><p id=\"p0100\">Second, the parameters of the input projection layer (the same input linear layer is used for both branches) and the output projection layer within Mamba will be directly related to the number of input channels. The input projection layer and output projection layer operate as follows:<disp-formula id=\"fd2\"><label>(Equation 2)</label><mml:math id=\"M13\" altimg=\"si9.gif\"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd3\"><label>(Equation 3)</label><mml:math id=\"M14\" altimg=\"si10.gif\"><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where the input projection <inline-formula><mml:math id=\"M15\" altimg=\"si11.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> layer parameter is <inline-formula><mml:math id=\"M16\" altimg=\"si12.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, and the output projection layer <inline-formula><mml:math id=\"M17\" altimg=\"si13.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> parameter is <inline-formula><mml:math id=\"M18\" altimg=\"si14.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>. We can conclude that the number of input channels, <inline-formula><mml:math id=\"M19\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>, is the key element controlling the parameter, where the internal extended projection channel, <inline-formula><mml:math id=\"M20\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, is also controlled by <inline-formula><mml:math id=\"M21\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0105\">Further, the intermediate linear projection layers of SSM are also key to the influence of the parameters. The details are as follows:<disp-formula id=\"fd4\"><label>(Equation 4)</label><mml:math id=\"M22\" altimg=\"si15.gif\"><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd5\"><label>(Equation 5)</label><mml:math id=\"M23\" altimg=\"si16.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id=\"M24\" altimg=\"si17.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is the rank of the step <inline-formula><mml:math id=\"M25\" altimg=\"si18.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>16</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M26\" altimg=\"si19.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> is the size of the state dimension (fixed to 16), where the parameters can be derived as <inline-formula><mml:math id=\"M27\" altimg=\"si20.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id=\"M28\" altimg=\"si21.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> is a linear projection layer for step size, with parameters <inline-formula><mml:math id=\"M29\" altimg=\"si22.gif\"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, mainly used for linear projection for step size <inline-formula><mml:math id=\"M30\" altimg=\"si23.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. So, we can conclude that all parameters are still mainly controlled by the number of input channels <inline-formula><mml:math id=\"M31\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0110\">In addition, the internal convolution (<inline-formula><mml:math id=\"M32\" altimg=\"si24.gif\"><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) also provides parametric influence with <inline-formula><mml:math id=\"M33\" altimg=\"si25.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>. In this paper, <inline-formula><mml:math id=\"M34\" altimg=\"si26.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> is fixed to 4, so the convolution provides a parameter of <inline-formula><mml:math id=\"M35\" altimg=\"si27.gif\"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, which is also controlled by the <inline-formula><mml:math id=\"M36\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0115\">Also, the logarithmic form parameters <inline-formula><mml:math id=\"M37\" altimg=\"si28.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of the transfer matrix <inline-formula><mml:math id=\"M38\" altimg=\"si29.gif\"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> in the SSM module are an important influencing element. <inline-formula><mml:math id=\"M39\" altimg=\"si30.gif\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> is a parameter matrix of the shape (<inline-formula><mml:math id=\"M40\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M41\" altimg=\"si19.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula>), so its parameters can be derived as <inline-formula><mml:math id=\"M42\" altimg=\"si31.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula>. In addition, the trainable vector parameter <inline-formula><mml:math id=\"M43\" altimg=\"si32.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of the process computed within the SSM contains the <inline-formula><mml:math id=\"M44\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> parameter, which is used to selectively integrate the SSM state outputs with the original input signals, thus enhancing the model&#8217;s expressiveness and training stability.</p><p id=\"p0120\">In summary, assuming that the original channel number is 1,024, keeping other parameters unchanged, and when the channel number is reduced to one-fourth of the original (channel number 256), the original total parameters can be calculated from the above parameter formula to get the original total parameters reduced from 23,435,264 to 1,484,288. The parameter explosion is reduced by 93.7<inline-formula><mml:math id=\"M45\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, which further confirms that the number of channels has a very critical impact on Mamba parameters.</p><p id=\"p0125\">Building upon the in-depth analysis of the key factors influencing parameter efficiency in Mamba, we propose a PVM Layer for feature processing. This design achieves outstanding performance with minimal calculated load, while maintaining a constant total number of processing channels. The architectural details and implementation of the PVM Layer are presented in the following section.</p></sec><sec id=\"sec2.3\"><title>Mamba variant (SS2D) parameter impact analysis</title><p id=\"p0130\">The 2D Selective Scan (SS2D) have been developed based on SSM which are more suitable for visual tasks, and SS2D are usually embedded in a Visual State Space (VSS) Block.<xref rid=\"bib11\" ref-type=\"bibr\"><sup>11</sup></xref> The VSS Block consists of two main branches, the first one mainly consists of a linear layer and SiLU activation function.<xref rid=\"bib20\" ref-type=\"bibr\"><sup>20</sup></xref> The second branch is mainly composed of linear layers, convolution, SiLU activation function, SS2D, and LayerNorm. Finally, the two branches merge the outputs by element-by-element multiplication.</p><p id=\"p0135\">The components of SS2D include scan expansion operation, S6 block feature extraction, and scan merge operation. The sequence is first expanded in four directions from top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right by scan expansion operation. Subsequently, the extracted features are passed through the S6 block<xref rid=\"bib13\" ref-type=\"bibr\"><sup>13</sup></xref> for deep feature refinement. A scan merge operation is then employed to reconstruct the spatial resolution to match that of the original input image.</p><p id=\"p0140\">In the VSS Block, the number of input channels, the size of the state dimension of the S6 block, the size of the internal convolution kernel, the projection dilation multiplier, and the rank of the projection matrix all affect the parameters. Among them, the influence of the number of input channels is explosive, and its influence mainly comes from the following aspects.</p><p id=\"p0145\">First, the <inline-formula><mml:math id=\"M46\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> of the VSS Block internal extended projection channel is determined by the product of the projection expansion multiplier and the number of input channels. This can be specifically expressed by the following equation:<disp-formula id=\"fd6\"><label>(Equation 6)</label><mml:math id=\"M47\" altimg=\"si5.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id=\"M48\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> is the internal expansion projection channel, <inline-formula><mml:math id=\"M49\" altimg=\"si6.gif\"><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> is the projection expansion multiplier (fixed at 2 by default), and <inline-formula><mml:math id=\"M50\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula> is the number of input channels. We can see that <inline-formula><mml:math id=\"M51\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> will rise exponentially as the number of channels per layer in the model increases dramatically.</p><p id=\"p0150\">Second, the parameters of the input projection layer (the same input linear layer is used for both branches) and output projection layer within VSS Block will be directly related to the number of input channels. The input projection layer and output projection layer operate as follows:<disp-formula id=\"fd7\"><label>(Equation 7)</label><mml:math id=\"M52\" altimg=\"si9.gif\"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd8\"><label>(Equation 8)</label><mml:math id=\"M53\" altimg=\"si10.gif\"><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where the input projection <inline-formula><mml:math id=\"M54\" altimg=\"si11.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> layer parameter is <inline-formula><mml:math id=\"M55\" altimg=\"si33.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and the output projection layer <inline-formula><mml:math id=\"M56\" altimg=\"si13.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> parameter is <inline-formula><mml:math id=\"M57\" altimg=\"si34.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. In addition, the output section has a layer normalization operation (LayerNorm) with parameter <inline-formula><mml:math id=\"M58\" altimg=\"si35.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. We can see that the number of input channels, <inline-formula><mml:math id=\"M59\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>, is the key element controlling the parameter, where the internal extended projection channel, <inline-formula><mml:math id=\"M60\" altimg=\"si4.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, is also controlled by <inline-formula><mml:math id=\"M61\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0155\">Further, the linear projection layers in the S6 block of SS2D are also key to the parameter effects. Each linear projection layer is specified as follows:<disp-formula id=\"fd9\"><label>(Equation 9)</label><mml:math id=\"M62\" altimg=\"si15.gif\"><mml:mrow><mml:mi>x</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd10\"><label>(Equation 10)</label><mml:math id=\"M63\" altimg=\"si16.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id=\"M64\" altimg=\"si17.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> is the rank of the projection matrix <inline-formula><mml:math id=\"M65\" altimg=\"si18.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>16</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M66\" altimg=\"si19.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> is the size of the S6 block state dimension (fixed to 16), and the parameters for each linear projection layer are <inline-formula><mml:math id=\"M67\" altimg=\"si36.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. However, there are 4 linear projection layers in total, so the total parameters are <inline-formula><mml:math id=\"M68\" altimg=\"si37.gif\"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#8727;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In addition, <inline-formula><mml:math id=\"M69\" altimg=\"si21.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> is a linear projection layer for step size with parameter <inline-formula><mml:math id=\"M70\" altimg=\"si38.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, which is mainly used for linear projection for step size <inline-formula><mml:math id=\"M71\" altimg=\"si23.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id=\"M72\" altimg=\"si21.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> also has 4 layers, with a total parameter of <inline-formula><mml:math id=\"M73\" altimg=\"si39.gif\"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#8727;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>k</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. So, from the above, we can know that all parameters are still mainly controlled by the number of input channels <inline-formula><mml:math id=\"M74\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0160\">In addition, the internal convolution (<inline-formula><mml:math id=\"M75\" altimg=\"si40.gif\"><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) also provides parametric influence with <inline-formula><mml:math id=\"M76\" altimg=\"si41.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>. In this paper, <inline-formula><mml:math id=\"M77\" altimg=\"si26.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> is fixed to 3, so the convolution provides a parameter of <inline-formula><mml:math id=\"M78\" altimg=\"si42.gif\"><mml:mrow><mml:msup><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">+</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, which is also controlled by the <inline-formula><mml:math id=\"M79\" altimg=\"si7.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id=\"p0165\">Also, the <inline-formula><mml:math id=\"M80\" altimg=\"si30.gif\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> of the parameter matrix controlling the attention weights of the different states of the S6 block in the SS2D module is an important influencing element. <inline-formula><mml:math id=\"M81\" altimg=\"si30.gif\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> is a parameter matrix of the shape <inline-formula><mml:math id=\"M82\" altimg=\"si43.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M83\" altimg=\"si44.gif\"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula> is a hyperparameter that is usually fixed to 4. Therefore, the parameter <inline-formula><mml:math id=\"M84\" altimg=\"si30.gif\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>_</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> can be derived as <inline-formula><mml:math id=\"M85\" altimg=\"si45.gif\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>&#8727;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. In addition, the trainable vector parameter <inline-formula><mml:math id=\"M86\" altimg=\"si46.gif\"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of the process computed within the SS2D contains the <inline-formula><mml:math id=\"M87\" altimg=\"si47.gif\"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#8727;</mml:mo><mml:mi>d</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> parameter, which is used to selectively integrate the SS2D state outputs with the original input signals, thereby enhancing the model&#8217;s expressiveness and training stability.</p><p id=\"p0170\">In summary, assuming that the original number of input channels is 1,024, keeping the other parameters unchanged, and reducing the number of channels to a quarter of the original (the number of input channels becomes 256), the original total parameters can be calculated by the above parameter formulae from 45,504,512 to 2,921,984. The parameter explosion reduces the number of channels by 93.6<inline-formula><mml:math id=\"M88\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, which further confirms that the number of input channels has a very critical impact on the VSS Block parameters.</p></sec><sec id=\"sec2.4\"><title>PVM Layer</title><p id=\"p0175\">As analyzed in the previous subsection, the number of input channels has an explosive effect on the parameters of Mamba. As shown in <xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref>A, we propose the PVM Layer for processing deep features. Specifically, a feature <inline-formula><mml:math id=\"M89\" altimg=\"si48.gif\"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> with channel number <inline-formula><mml:math id=\"M90\" altimg=\"si49.gif\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> first passes through a LayerNorm layer and then is divided into <inline-formula><mml:math id=\"M91\" altimg=\"si50.gif\"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M92\" altimg=\"si51.gif\"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M93\" altimg=\"si52.gif\"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M94\" altimg=\"si53.gif\"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> features each with channel number <inline-formula><mml:math id=\"M95\" altimg=\"si54.gif\"><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. After that, each of the features is then fed into Mamba, and then the output is subjected to residual concatenation and adjustment factor for optimizing the remote spatial information acquisition capability.<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref> Finally, the four features are combined into the feature <inline-formula><mml:math id=\"M96\" altimg=\"si55.gif\"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with channel number <inline-formula><mml:math id=\"M97\" altimg=\"si49.gif\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> by concat operation, and then output by LayerNorm and Projection operation, respectively. The specific operations can be expressed by the following equations:<disp-formula id=\"fd11\"><label>(Equation 11)</label><mml:math id=\"M98\" altimg=\"si56.gif\"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy=\"true\">[</mml:mo><mml:mrow><mml:mi mathvariant=\"italic\">LN</mml:mi><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy=\"true\">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd12\"><label>(Equation 12)</label><mml:math id=\"M99\" altimg=\"si57.gif\"><mml:mrow><mml:mi>V</mml:mi><mml:mi>M</mml:mi><mml:mo>_</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mi>&#952;</mml:mi><mml:mo linebreak=\"badbreak\">&#183;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mspace width=\"1em\"/><mml:mi>i</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd13\"><label>(Equation 13)</label><mml:math id=\"M100\" altimg=\"si58.gif\"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mo linebreak=\"badbreak\">&#8722;</mml:mo></mml:msub><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mo linebreak=\"badbreak\">&#8722;</mml:mo></mml:msub><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mo linebreak=\"badbreak\">&#8722;</mml:mo></mml:msub><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msub><mml:mi>M</mml:mi><mml:mo linebreak=\"badbreak\">&#8722;</mml:mo></mml:msub><mml:msubsup><mml:mi>Y</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd14\"><label>(Equation 14)</label><mml:math id=\"M101\" altimg=\"si59.gif\"><mml:mrow><mml:mi>O</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:mo stretchy=\"true\">[</mml:mo><mml:mrow><mml:mi mathvariant=\"italic\">LN</mml:mi><mml:mrow><mml:mo stretchy=\"true\">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"true\">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy=\"true\">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id=\"M102\" altimg=\"si60.gif\"><mml:mrow><mml:mi mathvariant=\"italic\">LN</mml:mi></mml:mrow></mml:math></inline-formula> is the LayerNorm, Sp is the Split operation, <inline-formula><mml:math id=\"M103\" altimg=\"si61.gif\"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> is the Mamba operation, <inline-formula><mml:math id=\"M104\" altimg=\"si62.gif\"><mml:mrow><mml:mi>&#952;</mml:mi></mml:mrow></mml:math></inline-formula> is the adjustment factor for the residual connection, <inline-formula><mml:math id=\"M105\" altimg=\"si63.gif\"><mml:mrow><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is the concat operation, and <inline-formula><mml:math id=\"M106\" altimg=\"si64.gif\"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:math></inline-formula> is the Projection operation. From Equation 12, we used PVM processing features, while ensuring that the total number of channels processed remains constant, maintaining high accuracy while maximizing parameter reduction. As shown in <xref rid=\"fig3\" ref-type=\"fig\">Figure 3</xref>A for methods A and B, again assuming a channel count size of 1,024, each VM in method A reduces the parameters by 93.7<inline-formula><mml:math id=\"M107\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, and it contains 4 such operations; when summed up, the comparison method B parameters are reduced by 74.8<inline-formula><mml:math id=\"M108\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> overall. Through our proposed PVM operation, the parameter reduction is maximized while maintaining strong performance competitiveness.</p></sec><sec id=\"sec2.5\"><title>Quantitative and qualitative analysis</title><p id=\"p0180\">To validate the performance of the proposed UltraLight VM-UNet under the 0.049M parameter, we conducted comparison experiments with several state-of-the-art lightweight and classical medical image segmentation models. Specifically, they include U-Net,<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref> SCR-Net,<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref> Swin-Unet,<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref> ATTENTION SWIN U-NET,<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref>\n<inline-formula><mml:math id=\"M109\" altimg=\"si65.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> SDG,<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref> VM-UNet,<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref> VM-UNet v2,<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref> MALUNet,<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref> LightM-UNet,<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref> EGE-UNet,<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref> DermoSegDiff,<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref> and LiteMamba-Bound.<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></p><p id=\"p0185\"><xref rid=\"tbl1\" ref-type=\"table\">Table 1</xref> show the experimental results on the ISIC2017, ISIC2018, and <inline-formula><mml:math id=\"M110\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> datasets, respectively. As shown in the table, the parameters of our model are 99.82<inline-formula><mml:math id=\"M111\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than those of the traditional pure VM-UNet and 87.84<inline-formula><mml:math id=\"M112\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than those of the current LightM-UNet. In addition, the GFLOPs of our model are 98.54<inline-formula><mml:math id=\"M113\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than VM-UNet and 84.65<inline-formula><mml:math id=\"M114\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than LightM-UNet. With such a large reduction in parameters and GFLOPs, the performance of our model still maintains excellent and highly competitive performance. In addition, MALUNet is a lightweight model proposed based on convolution, and although it has lower parameters and GFOLPs than VM-UNet and LightM-UNet, the parameters and GFOLPs of our model are still 72.0<inline-formula><mml:math id=\"M115\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and 27.71<inline-formula><mml:math id=\"M116\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than them, respectively. In particular, the performance of MALUNet, the proposed lightweight model based on convolution, is much lower than that of the Mamba-based model, which reflects that it is difficult for the convolution-based lightweight model to balance the relationship between performance and computational complexity. In addition, for a comprehensive analysis of the results, the proposed UltraLight VM-UNet also exhibits slightly lower values for some metrics compared with other models in the ISIC2017 and ISIC2018 datasets. For example, the specificity (SP) of UltraLight VM-UNet is lower than that of other comparison models in the ISIC2017 and ISIC2018 datasets. This is due to the fact that the proposed UltraLight VM-UNet uses multiple PVM focusing on the target region to maintain the ultra-lightweight architecture, but this also affects the model&#8217;s ability in recognizing the background, which leads to a slightly lower SP index. However, in the vast majority of metrics, especially in the ability to recognize lesion targets, the key dice similarity coefficient (DSC)/F1 metrics and intersection over union (IoU) metrics, etc., the proposed UltraLight VM-UNet is leading. Further, <xref rid=\"fig4\" ref-type=\"fig\">Figure 4</xref> shows the comparison between the proposed method and the comparison method in terms of inference speed and memory usage. As can be seen from the figure, the inference speed of the proposed UltraLight VM-UNet maintains an efficient value, which is faster than any other equivalent Mamba-based lightweight model. In terms of memory usage, the proposed UltraLight VM-UNet has the lowest memory usage (batch size uniformly 8) of all the compared models, which again demonstrates the excellent trade-off between lightweight and performance of UltraLight VM-UNet.<table-wrap position=\"float\" id=\"tbl1\" orientation=\"portrait\"><label>Table 1</label><caption><p>Experimental comparisons of the proposed UltraLight VM-UNet with the best lightweight and classical models</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Model</th><th colspan=\"1\" rowspan=\"1\">Parameters (millions)</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"9\" rowspan=\"1\"><bold>Comparison experiments on the ISIC2017 dataset</bold></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.009</td><td colspan=\"1\" rowspan=\"1\">3.224</td><td colspan=\"1\" rowspan=\"1\">0.8989</td><td colspan=\"1\" rowspan=\"1\">0.8793</td><td colspan=\"1\" rowspan=\"1\">0.9812</td><td colspan=\"1\" rowspan=\"1\">0.9613</td><td colspan=\"1\" rowspan=\"1\">0.8165</td><td colspan=\"1\" rowspan=\"1\">0.9196</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.801</td><td colspan=\"1\" rowspan=\"1\">1.567</td><td colspan=\"1\" rowspan=\"1\">0.8898</td><td colspan=\"1\" rowspan=\"1\">0.8497</td><td colspan=\"1\" rowspan=\"1\">0.9853</td><td colspan=\"1\" rowspan=\"1\">0.9588</td><td colspan=\"1\" rowspan=\"1\">0.8015</td><td colspan=\"1\" rowspan=\"1\">0.9340</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-Unet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.176</td><td colspan=\"1\" rowspan=\"1\">7.724</td><td colspan=\"1\" rowspan=\"1\">0.8670</td><td colspan=\"1\" rowspan=\"1\">0.8427</td><td colspan=\"1\" rowspan=\"1\">0.9754</td><td colspan=\"1\" rowspan=\"1\">0.9494</td><td colspan=\"1\" rowspan=\"1\">0.7652</td><td colspan=\"1\" rowspan=\"1\">0.8928</td></tr><tr><td colspan=\"1\" rowspan=\"1\">ATTENTION SWIN U-NET<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref></td><td colspan=\"1\" rowspan=\"1\">46.910</td><td colspan=\"1\" rowspan=\"1\">14.181</td><td colspan=\"1\" rowspan=\"1\">0.8859</td><td colspan=\"1\" rowspan=\"1\">0.8492</td><td colspan=\"1\" rowspan=\"1\">0.9847</td><td colspan=\"1\" rowspan=\"1\">0.9591</td><td colspan=\"1\" rowspan=\"1\">0.7998</td><td colspan=\"1\" rowspan=\"1\">0.9444</td></tr><tr><td colspan=\"1\" rowspan=\"1\">C<sup>2</sup>SDG<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.001</td><td colspan=\"1\" rowspan=\"1\">7.972</td><td colspan=\"1\" rowspan=\"1\">0.8938</td><td colspan=\"1\" rowspan=\"1\">0.8859</td><td colspan=\"1\" rowspan=\"1\">0.9765</td><td colspan=\"1\" rowspan=\"1\">0.9588</td><td colspan=\"1\" rowspan=\"1\">0.8081</td><td colspan=\"1\" rowspan=\"1\">0.9019</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.427</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.9070</td><td colspan=\"1\" rowspan=\"1\">0.8837</td><td colspan=\"1\" rowspan=\"1\">0.9842</td><td colspan=\"1\" rowspan=\"1\">0.9645</td><td colspan=\"1\" rowspan=\"1\">0.8298</td><td colspan=\"1\" rowspan=\"1\">0.9302</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771</td><td colspan=\"1\" rowspan=\"1\">4.400</td><td colspan=\"1\" rowspan=\"1\">0.9045</td><td colspan=\"1\" rowspan=\"1\">0.8768</td><td colspan=\"1\" rowspan=\"1\">0.9849</td><td colspan=\"1\" rowspan=\"1\">0.9637</td><td colspan=\"1\" rowspan=\"1\">0.8256</td><td colspan=\"1\" rowspan=\"1\">0.9168</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.175</td><td colspan=\"1\" rowspan=\"1\">0.083</td><td colspan=\"1\" rowspan=\"1\">0.8896</td><td colspan=\"1\" rowspan=\"1\">0.8824</td><td colspan=\"1\" rowspan=\"1\">0.9762</td><td colspan=\"1\" rowspan=\"1\">0.9583</td><td colspan=\"1\" rowspan=\"1\">0.8008</td><td colspan=\"1\" rowspan=\"1\">0.9295</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.403</td><td colspan=\"1\" rowspan=\"1\">0.391</td><td colspan=\"1\" rowspan=\"1\">0.9080</td><td colspan=\"1\" rowspan=\"1\">0.8839</td><td colspan=\"1\" rowspan=\"1\">0.9846</td><td colspan=\"1\" rowspan=\"1\">0.9649<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8303</td><td colspan=\"1\" rowspan=\"1\">0.9321</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053</td><td colspan=\"1\" rowspan=\"1\">0.072</td><td colspan=\"1\" rowspan=\"1\">0.9073</td><td colspan=\"1\" rowspan=\"1\">0.8931</td><td colspan=\"1\" rowspan=\"1\">0.9816</td><td colspan=\"1\" rowspan=\"1\">0.9642</td><td colspan=\"1\" rowspan=\"1\">0.8302</td><td colspan=\"1\" rowspan=\"1\">0.9219</td></tr><tr><td colspan=\"1\" rowspan=\"1\">DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td><td colspan=\"1\" rowspan=\"1\">45.112</td><td colspan=\"1\" rowspan=\"1\">38.636</td><td colspan=\"1\" rowspan=\"1\">0.8789</td><td colspan=\"1\" rowspan=\"1\">0.8435</td><td colspan=\"1\" rowspan=\"1\">0.9815</td><td colspan=\"1\" rowspan=\"1\">0.9545</td><td colspan=\"1\" rowspan=\"1\">0.7841</td><td colspan=\"1\" rowspan=\"1\">0.9174</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.957</td><td colspan=\"1\" rowspan=\"1\">1.591</td><td colspan=\"1\" rowspan=\"1\">0.9054</td><td colspan=\"1\" rowspan=\"1\">0.8743</td><td colspan=\"1\" rowspan=\"1\">0.9861<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9643</td><td colspan=\"1\" rowspan=\"1\">0.8272</td><td colspan=\"1\" rowspan=\"1\">0.9374</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (Our)</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.060<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9091<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9053<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9790</td><td colspan=\"1\" rowspan=\"1\">0.9646</td><td colspan=\"1\" rowspan=\"1\">0.8334<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9481<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><bold>Comparison experiments on the ISIC2018 dataset</bold></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.009</td><td colspan=\"1\" rowspan=\"1\">3.224</td><td colspan=\"1\" rowspan=\"1\">0.8851</td><td colspan=\"1\" rowspan=\"1\">0.8735</td><td colspan=\"1\" rowspan=\"1\">0.9744</td><td colspan=\"1\" rowspan=\"1\">0.9539</td><td colspan=\"1\" rowspan=\"1\">0.7938</td><td colspan=\"1\" rowspan=\"1\">0.8970</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.801</td><td colspan=\"1\" rowspan=\"1\">1.567</td><td colspan=\"1\" rowspan=\"1\">0.8886</td><td colspan=\"1\" rowspan=\"1\">0.8892</td><td colspan=\"1\" rowspan=\"1\">0.9714</td><td colspan=\"1\" rowspan=\"1\">0.9547</td><td colspan=\"1\" rowspan=\"1\">0.7995</td><td colspan=\"1\" rowspan=\"1\">0.8880</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-Unet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.176</td><td colspan=\"1\" rowspan=\"1\">7.724</td><td colspan=\"1\" rowspan=\"1\">0.8342</td><td colspan=\"1\" rowspan=\"1\">0.8142</td><td colspan=\"1\" rowspan=\"1\">0.9648</td><td colspan=\"1\" rowspan=\"1\">0.9343</td><td colspan=\"1\" rowspan=\"1\">0.7155</td><td colspan=\"1\" rowspan=\"1\">0.8551</td></tr><tr><td colspan=\"1\" rowspan=\"1\">ATTENTION SWIN U-NET<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref></td><td colspan=\"1\" rowspan=\"1\">46.910</td><td colspan=\"1\" rowspan=\"1\">14.181</td><td colspan=\"1\" rowspan=\"1\">0.8540</td><td colspan=\"1\" rowspan=\"1\">0.8057</td><td colspan=\"1\" rowspan=\"1\">0.9826<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9480</td><td colspan=\"1\" rowspan=\"1\">0.7683</td><td colspan=\"1\" rowspan=\"1\">0.9183</td></tr><tr><td colspan=\"1\" rowspan=\"1\">C<sup>2</sup>SDG<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.001</td><td colspan=\"1\" rowspan=\"1\">7.972</td><td colspan=\"1\" rowspan=\"1\">0.8806</td><td colspan=\"1\" rowspan=\"1\">0.8970</td><td colspan=\"1\" rowspan=\"1\">0.9643</td><td colspan=\"1\" rowspan=\"1\">0.9506</td><td colspan=\"1\" rowspan=\"1\">0.7867</td><td colspan=\"1\" rowspan=\"1\">0.8648</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.427</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.8891</td><td colspan=\"1\" rowspan=\"1\">0.8809</td><td colspan=\"1\" rowspan=\"1\">0.9743</td><td colspan=\"1\" rowspan=\"1\">0.9554</td><td colspan=\"1\" rowspan=\"1\">0.8004</td><td colspan=\"1\" rowspan=\"1\">0.8966</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771</td><td colspan=\"1\" rowspan=\"1\">4.400</td><td colspan=\"1\" rowspan=\"1\">0.8902</td><td colspan=\"1\" rowspan=\"1\">0.8959</td><td colspan=\"1\" rowspan=\"1\">0.9702</td><td colspan=\"1\" rowspan=\"1\">0.9551</td><td colspan=\"1\" rowspan=\"1\">0.8020</td><td colspan=\"1\" rowspan=\"1\">0.8672</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.175</td><td colspan=\"1\" rowspan=\"1\">0.083</td><td colspan=\"1\" rowspan=\"1\">0.8931</td><td colspan=\"1\" rowspan=\"1\">0.8890</td><td colspan=\"1\" rowspan=\"1\">0.9725</td><td colspan=\"1\" rowspan=\"1\">0.9548</td><td colspan=\"1\" rowspan=\"1\">0.8028</td><td colspan=\"1\" rowspan=\"1\">0.8827</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.403</td><td colspan=\"1\" rowspan=\"1\">0.391</td><td colspan=\"1\" rowspan=\"1\">0.8898</td><td colspan=\"1\" rowspan=\"1\">0.8829</td><td colspan=\"1\" rowspan=\"1\">0.9765</td><td colspan=\"1\" rowspan=\"1\">0.9555</td><td colspan=\"1\" rowspan=\"1\">0.8013</td><td colspan=\"1\" rowspan=\"1\">0.8902</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053</td><td colspan=\"1\" rowspan=\"1\">0.072</td><td colspan=\"1\" rowspan=\"1\">0.8819</td><td colspan=\"1\" rowspan=\"1\">0.9009<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9638</td><td colspan=\"1\" rowspan=\"1\">0.9510</td><td colspan=\"1\" rowspan=\"1\">0.7887</td><td colspan=\"1\" rowspan=\"1\">0.8637</td></tr><tr><td colspan=\"1\" rowspan=\"1\">DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td><td colspan=\"1\" rowspan=\"1\">45.112</td><td colspan=\"1\" rowspan=\"1\">38.636</td><td colspan=\"1\" rowspan=\"1\">0.8672</td><td colspan=\"1\" rowspan=\"1\">0.8344</td><td colspan=\"1\" rowspan=\"1\">0.9693</td><td colspan=\"1\" rowspan=\"1\">0.9421</td><td colspan=\"1\" rowspan=\"1\">0.7697</td><td colspan=\"1\" rowspan=\"1\">0.8760</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.957</td><td colspan=\"1\" rowspan=\"1\">1.591</td><td colspan=\"1\" rowspan=\"1\">0.8929</td><td colspan=\"1\" rowspan=\"1\">0.8911</td><td colspan=\"1\" rowspan=\"1\">0.9714</td><td colspan=\"1\" rowspan=\"1\">0.9546</td><td colspan=\"1\" rowspan=\"1\">0.8019</td><td colspan=\"1\" rowspan=\"1\">0.8894</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (Our)</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.060<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8940<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8680</td><td colspan=\"1\" rowspan=\"1\">0.9781</td><td colspan=\"1\" rowspan=\"1\">0.9558<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8056<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9197<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"9\" rowspan=\"1\">Comparison experiments on the <inline-formula><mml:math id=\"M117\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> dataset</td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.009</td><td colspan=\"1\" rowspan=\"1\">3.224</td><td colspan=\"1\" rowspan=\"1\">0.9060</td><td colspan=\"1\" rowspan=\"1\">0.9255</td><td colspan=\"1\" rowspan=\"1\">0.9440</td><td colspan=\"1\" rowspan=\"1\">0.9381</td><td colspan=\"1\" rowspan=\"1\">0.8282</td><td colspan=\"1\" rowspan=\"1\">0.8874</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.801</td><td colspan=\"1\" rowspan=\"1\">1.567</td><td colspan=\"1\" rowspan=\"1\">0.8989</td><td colspan=\"1\" rowspan=\"1\">0.9114</td><td colspan=\"1\" rowspan=\"1\">0.9446</td><td colspan=\"1\" rowspan=\"1\">0.9339</td><td colspan=\"1\" rowspan=\"1\">0.8164</td><td colspan=\"1\" rowspan=\"1\">0.8868</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-Unet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.176</td><td colspan=\"1\" rowspan=\"1\">7.724</td><td colspan=\"1\" rowspan=\"1\">0.8631</td><td colspan=\"1\" rowspan=\"1\">0.8613</td><td colspan=\"1\" rowspan=\"1\">0.9359</td><td colspan=\"1\" rowspan=\"1\">0.9119</td><td colspan=\"1\" rowspan=\"1\">0.7591</td><td colspan=\"1\" rowspan=\"1\">0.8649</td></tr><tr><td colspan=\"1\" rowspan=\"1\">ATTENTION SWIN U-NET<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref></td><td colspan=\"1\" rowspan=\"1\">46.910</td><td colspan=\"1\" rowspan=\"1\">14.181</td><td colspan=\"1\" rowspan=\"1\">0.8850</td><td colspan=\"1\" rowspan=\"1\">0.8886</td><td colspan=\"1\" rowspan=\"1\">0.9363</td><td colspan=\"1\" rowspan=\"1\">0.9213</td><td colspan=\"1\" rowspan=\"1\">0.7990</td><td colspan=\"1\" rowspan=\"1\">0.8838</td></tr><tr><td colspan=\"1\" rowspan=\"1\">C<sup>2</sup>SDG<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.001</td><td colspan=\"1\" rowspan=\"1\">7.972</td><td colspan=\"1\" rowspan=\"1\">0.9030</td><td colspan=\"1\" rowspan=\"1\">0.9137</td><td colspan=\"1\" rowspan=\"1\">0.9476</td><td colspan=\"1\" rowspan=\"1\">0.9367</td><td colspan=\"1\" rowspan=\"1\">0.8231</td><td colspan=\"1\" rowspan=\"1\">0.8925</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.427</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.9033</td><td colspan=\"1\" rowspan=\"1\">0.9131</td><td colspan=\"1\" rowspan=\"1\">0.9483</td><td colspan=\"1\" rowspan=\"1\">0.9369</td><td colspan=\"1\" rowspan=\"1\">0.8237</td><td colspan=\"1\" rowspan=\"1\">0.8948</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771</td><td colspan=\"1\" rowspan=\"1\">4.400</td><td colspan=\"1\" rowspan=\"1\">0.9050</td><td colspan=\"1\" rowspan=\"1\">0.9160</td><td colspan=\"1\" rowspan=\"1\">0.9485</td><td colspan=\"1\" rowspan=\"1\">0.9380</td><td colspan=\"1\" rowspan=\"1\">0.8265</td><td colspan=\"1\" rowspan=\"1\">0.8797</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.175</td><td colspan=\"1\" rowspan=\"1\">0.083</td><td colspan=\"1\" rowspan=\"1\">0.8865</td><td colspan=\"1\" rowspan=\"1\">0.8922</td><td colspan=\"1\" rowspan=\"1\">0.9425</td><td colspan=\"1\" rowspan=\"1\">0.9263</td><td colspan=\"1\" rowspan=\"1\">0.8010</td><td colspan=\"1\" rowspan=\"1\">0.8993</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.403</td><td colspan=\"1\" rowspan=\"1\">0.391</td><td colspan=\"1\" rowspan=\"1\">0.9156</td><td colspan=\"1\" rowspan=\"1\">0.9129</td><td colspan=\"1\" rowspan=\"1\">0.9613<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9457</td><td colspan=\"1\" rowspan=\"1\">0.8443</td><td colspan=\"1\" rowspan=\"1\">0.9103</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053</td><td colspan=\"1\" rowspan=\"1\">0.072</td><td colspan=\"1\" rowspan=\"1\">0.9086</td><td colspan=\"1\" rowspan=\"1\">0.9198</td><td colspan=\"1\" rowspan=\"1\">0.9502</td><td colspan=\"1\" rowspan=\"1\">0.9404</td><td colspan=\"1\" rowspan=\"1\">0.8325</td><td colspan=\"1\" rowspan=\"1\">0.8978</td></tr><tr><td colspan=\"1\" rowspan=\"1\">DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td><td colspan=\"1\" rowspan=\"1\">45.112</td><td colspan=\"1\" rowspan=\"1\">38.636</td><td colspan=\"1\" rowspan=\"1\">0.8928</td><td colspan=\"1\" rowspan=\"1\">0.8779</td><td colspan=\"1\" rowspan=\"1\">0.9577</td><td colspan=\"1\" rowspan=\"1\">0.9320</td><td colspan=\"1\" rowspan=\"1\">0.8064</td><td colspan=\"1\" rowspan=\"1\">0.9082</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.957</td><td colspan=\"1\" rowspan=\"1\">1.591</td><td colspan=\"1\" rowspan=\"1\">0.9217</td><td colspan=\"1\" rowspan=\"1\">0.9341</td><td colspan=\"1\" rowspan=\"1\">0.9558</td><td colspan=\"1\" rowspan=\"1\">0.9488</td><td colspan=\"1\" rowspan=\"1\">0.8548</td><td colspan=\"1\" rowspan=\"1\">0.9097</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (Our)</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.060<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9265<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9345<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9606</td><td colspan=\"1\" rowspan=\"1\">0.9521<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8631<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9187<xref rid=\"tblfn1\" ref-type=\"table-fn\">a</xref></td></tr></tbody></table><table-wrap-foot><fn id=\"tblfn1\"><label>a</label><p id=\"ntpara0015\">These values represent the best performance.</p></fn></table-wrap-foot></table-wrap><fig id=\"fig4\" position=\"float\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Comparison of inference time and memory usage between the proposed and compared methods</p><p>(A) Comparison of inference time for different models.</p><p>(B) Comparison of memory usage for different models.</p><p>Numbers 1&#8211;13 refer to U-Net, SCR-Net, Swin-Unet, ATTENTION SWIN U-Net, <inline-formula><mml:math id=\"M118\" altimg=\"si65.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> SDG, VM-UNet, VM-UNet v2, MALUNet, LightM-UNet, EGE-UNet, DermoSegDiff, LiteMamba-Bound, and UltraLight VM-UNet (Our).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr4.jpg\"/></fig></p><p id=\"p0190\">To more directly represent the competitive nature of UltraLight VM-UNet in terms of segmentation performance, we visualized the segmentation results (<xref rid=\"fig5\" ref-type=\"fig\">Figure 5</xref>). In addition, we have also visualized the segmentation results of several state-of-the-art lightweight and classical medical image segmentation models. From the visualizations, it can be concluded that the segmentation results of UltraLight VM-UNet have smooth, clear, and more accurate boundaries. This shows that UltraLight VM-UNet not only outperforms the rest of the current lightweight models in terms of parameters and computational complexity, but also remains competitive in terms of performance.<fig id=\"fig5\" position=\"float\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Visualization of segmentation graphs for comparison experiments of three publicly available skin lesion segmentation datasets</p><p>The red contour line is the true value and the blue contour line is the predicted value.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr5.jpg\"/></fig></p></sec></sec><sec id=\"sec3\"><title>Discussion</title><sec id=\"sec3.1\"><title>Ablation experiments</title><sec id=\"sec3.1.1\"><title>VM with different levels of parallelism</title><p id=\"p0195\">To verify the validity of the proposed method of VM with different parallelism, we performed a series of ablation experiments. As shown in <xref rid=\"fig6\" ref-type=\"fig\">Figure 6</xref>, we performed three different settings. Setting 1 is a conventional connection of VM, setting 2 is a connection using a parallel connection of two VMs with half the number of channels, and setting 3 is a connection using parallel connection of four VMs each with <inline-formula><mml:math id=\"M119\" altimg=\"si54.gif\"><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> the number of channels. By analyzing the parameters of Mamba in \"mamba parameter impact analysis,\" assuming that the parameter of this module is <inline-formula><mml:math id=\"M120\" altimg=\"si67.gif\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> for setting 1 of the traditional VM connection method, setting 2 can be calculated with a parameter of <inline-formula><mml:math id=\"M121\" altimg=\"si68.gif\"><mml:mrow><mml:mn>0.502</mml:mn><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and setting 3 with a parameter of <inline-formula><mml:math id=\"M122\" altimg=\"si69.gif\"><mml:mrow><mml:mn>0.252</mml:mn><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>. <xref rid=\"tbl2\" ref-type=\"table\">Table 2</xref> shows the results of this ablation experiment, and it should be noted that the parameters here refer to the parameters of the overall model (which contains the Conv Block and the skip-connection part). The parameters of setting 2 and setting 3 are 51.47<inline-formula><mml:math id=\"M123\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and 36.03<inline-formula><mml:math id=\"M124\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, respectively, of the parameters of setting 1 for the traditional VM connection method, while the GFLOPs as a whole do not change much. In terms of performance, the lowest parameter of setting 3 still maintains better segmentation performance. This is due to the focus on the target area by multiple PVM, but this also affects the ability of the model to recognize the background, which results in the SP index for setting 3 being the lowest. However, in the vast majority of the indices, especially the contrast lesion target recognition ability, i.e., the key DSC/F1 index and the IoU index are leading. Furthermore, in <xref rid=\"fig7\" ref-type=\"fig\">Figure 7</xref>, which shows the inference speed and memory usage for three different parallel settings, it can be concluded that, due to the design of the ultra-lightweight architecture, although parallel processing increases the number of matrix operations slightly, the difference between four-parallel operations is only slightly manifested at the millisecond level at most. However, as can be seen from <xref rid=\"tbl2\" ref-type=\"table\">Table 2</xref>, quad-parallel processing will reduce the number of parameters by a significant 74.8<inline-formula><mml:math id=\"M125\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>. Therefore, to realize the ultra-lightweight architecture design with millisecond difference and excellent performance, we adopt setting 3 as the key structure of the proposed PVM Layer.<fig id=\"fig6\" position=\"float\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Settings for ablation experiments with Vision Mamba used in different parallel ways (PVM Layer)</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr6.jpg\"/></fig><table-wrap position=\"float\" id=\"tbl2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Ablation experiments on the effect of Vision Mamba in different parallel connections</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Settings</th><th colspan=\"1\" rowspan=\"1\">Parameters (millions)</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">1</td><td colspan=\"1\" rowspan=\"1\">0.136</td><td colspan=\"1\" rowspan=\"1\">0.060</td><td colspan=\"1\" rowspan=\"1\">0.9069</td><td colspan=\"1\" rowspan=\"1\">0.8861</td><td colspan=\"1\" rowspan=\"1\">0.9834</td><td colspan=\"1\" rowspan=\"1\">0.9644</td><td colspan=\"1\" rowspan=\"1\">0.8266</td><td colspan=\"1\" rowspan=\"1\">0.9354</td></tr><tr><td colspan=\"1\" rowspan=\"1\">2</td><td colspan=\"1\" rowspan=\"1\">0.070</td><td colspan=\"1\" rowspan=\"1\">0.060</td><td colspan=\"1\" rowspan=\"1\">0.9073</td><td colspan=\"1\" rowspan=\"1\">0.8866</td><td colspan=\"1\" rowspan=\"1\">0.9835<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9645</td><td colspan=\"1\" rowspan=\"1\">0.8284</td><td colspan=\"1\" rowspan=\"1\">0.9398</td></tr><tr><td colspan=\"1\" rowspan=\"1\">3</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.060<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9091<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9053<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9790</td><td colspan=\"1\" rowspan=\"1\">0.9646<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8334<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9481<xref rid=\"tblfn2\" ref-type=\"table-fn\">a</xref></td></tr></tbody></table><table-wrap-foot><fn id=\"tblfn2\"><label>a</label><p id=\"ntpara0020\">These values represent the best performance.</p></fn></table-wrap-foot></table-wrap><fig id=\"fig7\" position=\"float\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Comparison of inference time and memory usage of Vision Mamba with different parallel connections</p><p>(A) Comparison of inference time of Vision Mamba with different parallel connections.</p><p>(B) Comparison of memory usage of Vision Mamba with different parallel connections.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr7.jpg\"/></fig></p></sec><sec id=\"sec3.1.2\"><title>Parallelization of different SSM variants</title><p id=\"p0200\">In the <xref rid=\"sec4\" ref-type=\"sec\">methods</xref>, we detail the key to the influence of the parameters of Mamba, represented by SSM. However, many current studies propose improvements based on SSM to adapt it to 2D image processing. In Liu et al.,<xref rid=\"bib11\" ref-type=\"bibr\"><sup>11</sup></xref> researchers proposed SS2D for visual image processing. In Ruan andXiang,<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref> researchers proposed VM-UNet for medical image segmentation by combining SS2D with UNet framework. In addition, based on SS2D, Wu et al.<xref rid=\"bib29\" ref-type=\"bibr\"><sup>29</sup></xref> proposed high-order SS2D (H-SS2D) for medical image segmentation. The parameter and performance effects of SS2D and H-SS2D using parallel approach are shown in <xref rid=\"tbl3\" ref-type=\"table\">Table 3</xref>. From the table, it is concluded that the parameters and GFLOPs of (<italic toggle=\"yes\">P</italic>) SS2D are reduced by 81.35<inline-formula><mml:math id=\"M126\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and 61.97<inline-formula><mml:math id=\"M127\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, respectively, while those of (<italic toggle=\"yes\">P</italic>) H-SS2D are reduced by 80.34<inline-formula><mml:math id=\"M128\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and 57.14<inline-formula><mml:math id=\"M129\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. The above results reveal that the parallel approach is effective in reducing parameters and GFLOPs not only for Mamba represented by SSM but also for different variants of SSM at the same time.<table-wrap position=\"float\" id=\"tbl3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Impact of adopting parallelism for different SSM variants</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Methods</th><th colspan=\"1\" rowspan=\"1\">SSM variants</th><th colspan=\"1\" rowspan=\"1\">Params</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC</th><th colspan=\"1\" rowspan=\"1\">SE</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">SS2D</td><td colspan=\"1\" rowspan=\"1\">27.427M</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.9070</td><td colspan=\"1\" rowspan=\"1\">0.8837</td><td colspan=\"1\" rowspan=\"1\">0.9842</td><td colspan=\"1\" rowspan=\"1\">0.9645</td><td colspan=\"1\" rowspan=\"1\">0.8298</td><td colspan=\"1\" rowspan=\"1\">0.9302</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">SS2D</td><td colspan=\"1\" rowspan=\"1\">27.427M</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.8891</td><td colspan=\"1\" rowspan=\"1\">0.8809</td><td colspan=\"1\" rowspan=\"1\">0.9743</td><td colspan=\"1\" rowspan=\"1\">0.9554</td><td colspan=\"1\" rowspan=\"1\">0.8004</td><td colspan=\"1\" rowspan=\"1\">0.8966</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;3&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">SS2D</td><td colspan=\"1\" rowspan=\"1\">27.427M</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.9033</td><td colspan=\"1\" rowspan=\"1\">0.9131</td><td colspan=\"1\" rowspan=\"1\">0.9483</td><td colspan=\"1\" rowspan=\"1\">0.9369</td><td colspan=\"1\" rowspan=\"1\">0.8237</td><td colspan=\"1\" rowspan=\"1\">0.8948</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) SS2D</td><td colspan=\"1\" rowspan=\"1\">5.116M</td><td colspan=\"1\" rowspan=\"1\">1.564</td><td colspan=\"1\" rowspan=\"1\">0.9159</td><td colspan=\"1\" rowspan=\"1\">0.8843</td><td colspan=\"1\" rowspan=\"1\">0.9886</td><td colspan=\"1\" rowspan=\"1\">0.9682</td><td colspan=\"1\" rowspan=\"1\">0.8366</td><td colspan=\"1\" rowspan=\"1\">0.9352</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) SS2D</td><td colspan=\"1\" rowspan=\"1\">5.116M</td><td colspan=\"1\" rowspan=\"1\">1.564</td><td colspan=\"1\" rowspan=\"1\">0.8977</td><td colspan=\"1\" rowspan=\"1\">0.8996</td><td colspan=\"1\" rowspan=\"1\">0.9734</td><td colspan=\"1\" rowspan=\"1\">0.9584</td><td colspan=\"1\" rowspan=\"1\">0.8009</td><td colspan=\"1\" rowspan=\"1\">0.9002</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;3&#8221; VM-UNet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) SS2D</td><td colspan=\"1\" rowspan=\"1\">5.116M</td><td colspan=\"1\" rowspan=\"1\">1.564</td><td colspan=\"1\" rowspan=\"1\">0.9185</td><td colspan=\"1\" rowspan=\"1\">0.9340</td><td colspan=\"1\" rowspan=\"1\">0.9525</td><td colspan=\"1\" rowspan=\"1\">0.9465</td><td colspan=\"1\" rowspan=\"1\">0.8377</td><td colspan=\"1\" rowspan=\"1\">0.9034</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">H-SS2D</td><td colspan=\"1\" rowspan=\"1\">8.973M</td><td colspan=\"1\" rowspan=\"1\">0.742</td><td colspan=\"1\" rowspan=\"1\">0.9172</td><td colspan=\"1\" rowspan=\"1\">0.9056</td><td colspan=\"1\" rowspan=\"1\">0.9831</td><td colspan=\"1\" rowspan=\"1\">0.9680</td><td colspan=\"1\" rowspan=\"1\">0.8471</td><td colspan=\"1\" rowspan=\"1\">0.9291</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">H-SS2D</td><td colspan=\"1\" rowspan=\"1\">8.973M</td><td colspan=\"1\" rowspan=\"1\">0.742</td><td colspan=\"1\" rowspan=\"1\">0.8966</td><td colspan=\"1\" rowspan=\"1\">0.8952</td><td colspan=\"1\" rowspan=\"1\">0.9741</td><td colspan=\"1\" rowspan=\"1\">0.9581</td><td colspan=\"1\" rowspan=\"1\">0.8073</td><td colspan=\"1\" rowspan=\"1\">0.8756</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;3&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">H-SS2D</td><td colspan=\"1\" rowspan=\"1\">8.973M</td><td colspan=\"1\" rowspan=\"1\">0.742</td><td colspan=\"1\" rowspan=\"1\">0.9146</td><td colspan=\"1\" rowspan=\"1\">0.9295</td><td colspan=\"1\" rowspan=\"1\">0.9509</td><td colspan=\"1\" rowspan=\"1\">0.9440</td><td colspan=\"1\" rowspan=\"1\">0.8427</td><td colspan=\"1\" rowspan=\"1\">0.9002</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) H-SS2D</td><td colspan=\"1\" rowspan=\"1\">1.764M</td><td colspan=\"1\" rowspan=\"1\">0.318</td><td colspan=\"1\" rowspan=\"1\">0.9066</td><td colspan=\"1\" rowspan=\"1\">0.8825</td><td colspan=\"1\" rowspan=\"1\">0.9843</td><td colspan=\"1\" rowspan=\"1\">0.9644</td><td colspan=\"1\" rowspan=\"1\">0.8344</td><td colspan=\"1\" rowspan=\"1\">0.9349</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) H-SS2D</td><td colspan=\"1\" rowspan=\"1\">1.764M</td><td colspan=\"1\" rowspan=\"1\">0.318</td><td colspan=\"1\" rowspan=\"1\">0.8948</td><td colspan=\"1\" rowspan=\"1\">0.9067</td><td colspan=\"1\" rowspan=\"1\">0.9695</td><td colspan=\"1\" rowspan=\"1\">0.9567</td><td colspan=\"1\" rowspan=\"1\">0.8079</td><td colspan=\"1\" rowspan=\"1\">0.8744</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;3&#8221; H-vmunet</td><td colspan=\"1\" rowspan=\"1\">(<italic toggle=\"yes\">P</italic>) H-SS2D</td><td colspan=\"1\" rowspan=\"1\">1.764M</td><td colspan=\"1\" rowspan=\"1\">0.318</td><td colspan=\"1\" rowspan=\"1\">0.9181</td><td colspan=\"1\" rowspan=\"1\">0.9254</td><td colspan=\"1\" rowspan=\"1\">0.9569</td><td colspan=\"1\" rowspan=\"1\">0.9467</td><td colspan=\"1\" rowspan=\"1\">0.8473</td><td colspan=\"1\" rowspan=\"1\">0.9020</td></tr></tbody></table><table-wrap-foot><fn><p>(<italic toggle=\"yes\">P</italic>) shows the adoption of quadruple parallelism to replace the original form; &#8220;1&#8221; indicates experiments on the ISIC2017 dataset, &#8220;2&#8221; indicates the ISIC2018 dataset, and &#8220;3&#8221; indicates the <inline-formula><mml:math id=\"M130\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> dataset.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id=\"sec3.1.3\"><title>Plug-and-play PVM Layer</title><p id=\"p0205\">The proposed PVM Layer can simply replace the base building blocks of any model, which include but are not limited to Convolution, Vision Transformers, Mamba, VM, and so on. <xref rid=\"tbl4\" ref-type=\"table\">Table 4</xref> shows the powerful plug-and-play capabilities of the PVM Layer. From the table, it can be concluded that, after replacing the base building blocks or key functional modules of any model with PVM Layer, there is a significant decrease in parameters and GFLOPs, and the performance is still clearly competitive. In addition, MALUNet and EGE-UNet, as the most advanced lightweight models, after replacing the key modules in the PVM Layer, the parameters and GFLOPs can still be effectively reduced, and the performance is improved. With the above results, it is shown that the powerful plug-and-play feature of the PVM Layer is used to significantly reduce the parameters and GFLOPs of arbitrary models.<table-wrap position=\"float\" id=\"tbl4\" orientation=\"portrait\"><label>Table 4</label><caption><p>Comparative experiments where the PVM Layer directly replaces modules from different models, with results including parameters, computational complexity, and performance</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th rowspan=\"2\" colspan=\"1\">Methods</th><th colspan=\"4\" rowspan=\"1\">Parameters and computational complexity<hr/></th><th colspan=\"6\" rowspan=\"1\">Performance evaluation<hr/></th></tr><tr><th colspan=\"1\" rowspan=\"1\">Params</th><th colspan=\"1\" rowspan=\"1\"><inline-formula><mml:math id=\"M131\" altimg=\"si82.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>(%)</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\"><inline-formula><mml:math id=\"M132\" altimg=\"si83.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">G</mml:mi><mml:mspace width=\"0.25em\"/></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>(%)</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">UNet<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.009M</td><td colspan=\"1\" rowspan=\"1\">80.38</td><td colspan=\"1\" rowspan=\"1\">3.224</td><td colspan=\"1\" rowspan=\"1\">78.74</td><td colspan=\"1\" rowspan=\"1\">0.8989</td><td colspan=\"1\" rowspan=\"1\">0.8793</td><td colspan=\"1\" rowspan=\"1\">0.9812</td><td colspan=\"1\" rowspan=\"1\">0.9613</td><td colspan=\"1\" rowspan=\"1\">0.8165</td><td colspan=\"1\" rowspan=\"1\">0.9196</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UNet_<italic toggle=\"yes\">Conv</italic></td><td colspan=\"1\" rowspan=\"1\">0.394M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.686</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.8974</td><td colspan=\"1\" rowspan=\"1\">0.8667</td><td colspan=\"1\" rowspan=\"1\">0.9842</td><td colspan=\"1\" rowspan=\"1\">0.9612</td><td colspan=\"1\" rowspan=\"1\">0.8147</td><td colspan=\"1\" rowspan=\"1\">0.9211</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Att UNet<xref rid=\"bib30\" ref-type=\"bibr\"><sup>30</sup></xref></td><td colspan=\"1\" rowspan=\"1\">3.581M</td><td colspan=\"1\" rowspan=\"1\">45.10</td><td colspan=\"1\" rowspan=\"1\">8.575</td><td colspan=\"1\" rowspan=\"1\">29.61</td><td colspan=\"1\" rowspan=\"1\">0.8821</td><td colspan=\"1\" rowspan=\"1\">0.8423</td><td colspan=\"1\" rowspan=\"1\">0.9836</td><td colspan=\"1\" rowspan=\"1\">0.9560</td><td colspan=\"1\" rowspan=\"1\">0.7891</td><td colspan=\"1\" rowspan=\"1\">0.9259</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Att UNet_<italic toggle=\"yes\">Conv_block</italic></td><td colspan=\"1\" rowspan=\"1\">1.966M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">6.036</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.8857</td><td colspan=\"1\" rowspan=\"1\">0.8414</td><td colspan=\"1\" rowspan=\"1\">0.9857</td><td colspan=\"1\" rowspan=\"1\">0.9575</td><td colspan=\"1\" rowspan=\"1\">0.7921</td><td colspan=\"1\" rowspan=\"1\">0.9241</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.812M</td><td colspan=\"1\" rowspan=\"1\">81.90</td><td colspan=\"1\" rowspan=\"1\">1.567</td><td colspan=\"1\" rowspan=\"1\">72.75</td><td colspan=\"1\" rowspan=\"1\">0.8898</td><td colspan=\"1\" rowspan=\"1\">0.8497</td><td colspan=\"1\" rowspan=\"1\">0.9853</td><td colspan=\"1\" rowspan=\"1\">0.9588</td><td colspan=\"1\" rowspan=\"1\">0.8015</td><td colspan=\"1\" rowspan=\"1\">0.9340</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net_<italic toggle=\"yes\">Conv</italic></td><td colspan=\"1\" rowspan=\"1\">0.147M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.427</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9058</td><td colspan=\"1\" rowspan=\"1\">0.8847</td><td colspan=\"1\" rowspan=\"1\">0.9833</td><td colspan=\"1\" rowspan=\"1\">0.9640</td><td colspan=\"1\" rowspan=\"1\">0.8233</td><td colspan=\"1\" rowspan=\"1\">0.9370</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-UNet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.176M</td><td colspan=\"1\" rowspan=\"1\">76.11</td><td colspan=\"1\" rowspan=\"1\">7.724</td><td colspan=\"1\" rowspan=\"1\">75.48</td><td colspan=\"1\" rowspan=\"1\">0.8670</td><td colspan=\"1\" rowspan=\"1\">0.8427</td><td colspan=\"1\" rowspan=\"1\">0.9754</td><td colspan=\"1\" rowspan=\"1\">0.9494</td><td colspan=\"1\" rowspan=\"1\">0.7652</td><td colspan=\"1\" rowspan=\"1\">0.8928</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-UNet_<italic toggle=\"yes\">Vision Transformer</italic></td><td colspan=\"1\" rowspan=\"1\">6.491M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">1.894</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.8734</td><td colspan=\"1\" rowspan=\"1\">0.8445</td><td colspan=\"1\" rowspan=\"1\">0.9783</td><td colspan=\"1\" rowspan=\"1\">0.9521</td><td colspan=\"1\" rowspan=\"1\">0.7710</td><td colspan=\"1\" rowspan=\"1\">0.8953</td></tr><tr><td colspan=\"1\" rowspan=\"1\">META-Unet<xref rid=\"bib31\" ref-type=\"bibr\"><sup>31</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.209M</td><td colspan=\"1\" rowspan=\"1\">94.77</td><td colspan=\"1\" rowspan=\"1\">5.140</td><td colspan=\"1\" rowspan=\"1\">91.91</td><td colspan=\"1\" rowspan=\"1\">0.9068</td><td colspan=\"1\" rowspan=\"1\">0.8801</td><td colspan=\"1\" rowspan=\"1\">0.9836</td><td colspan=\"1\" rowspan=\"1\">0.9639</td><td colspan=\"1\" rowspan=\"1\">0.8301</td><td colspan=\"1\" rowspan=\"1\">0.9327</td></tr><tr><td colspan=\"1\" rowspan=\"1\">META-Unet_<italic toggle=\"yes\">ResNet Layer</italic></td><td colspan=\"1\" rowspan=\"1\">1.161M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.416</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9047</td><td colspan=\"1\" rowspan=\"1\">0.8915</td><td colspan=\"1\" rowspan=\"1\">0.9807</td><td colspan=\"1\" rowspan=\"1\">0.9633</td><td colspan=\"1\" rowspan=\"1\">0.8264</td><td colspan=\"1\" rowspan=\"1\">0.9285</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MHorUNet<xref rid=\"bib3\" ref-type=\"bibr\"><sup>3</sup></xref></td><td colspan=\"1\" rowspan=\"1\">9.585M</td><td colspan=\"1\" rowspan=\"1\">90.35</td><td colspan=\"1\" rowspan=\"1\">0.864</td><td colspan=\"1\" rowspan=\"1\">81.94</td><td colspan=\"1\" rowspan=\"1\">0.9132</td><td colspan=\"1\" rowspan=\"1\">0.8974</td><td colspan=\"1\" rowspan=\"1\">0.9834</td><td colspan=\"1\" rowspan=\"1\">0.9666</td><td colspan=\"1\" rowspan=\"1\">0.8348</td><td colspan=\"1\" rowspan=\"1\">0.9289</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MHorUNet_<italic toggle=\"yes\">Horblock</italic></td><td colspan=\"1\" rowspan=\"1\">0.925M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.156</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9107</td><td colspan=\"1\" rowspan=\"1\">0.8806</td><td colspan=\"1\" rowspan=\"1\">0.9870</td><td colspan=\"1\" rowspan=\"1\">0.9662</td><td colspan=\"1\" rowspan=\"1\">0.8318</td><td colspan=\"1\" rowspan=\"1\">0.9300</td></tr><tr><td colspan=\"1\" rowspan=\"1\">HSH-UNet<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref></td><td colspan=\"1\" rowspan=\"1\">18.803M</td><td colspan=\"1\" rowspan=\"1\">84.64</td><td colspan=\"1\" rowspan=\"1\">9.362</td><td colspan=\"1\" rowspan=\"1\">90.58</td><td colspan=\"1\" rowspan=\"1\">0.9108</td><td colspan=\"1\" rowspan=\"1\">0.8907</td><td colspan=\"1\" rowspan=\"1\">0.9864</td><td colspan=\"1\" rowspan=\"1\">0.9654</td><td colspan=\"1\" rowspan=\"1\">0.8214</td><td colspan=\"1\" rowspan=\"1\">0.9356</td></tr><tr><td colspan=\"1\" rowspan=\"1\">HSH-UNet_<italic toggle=\"yes\">HSHB</italic></td><td colspan=\"1\" rowspan=\"1\">2.888M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.882</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9001</td><td colspan=\"1\" rowspan=\"1\">0.8938</td><td colspan=\"1\" rowspan=\"1\">0.9776</td><td colspan=\"1\" rowspan=\"1\">0.9612</td><td colspan=\"1\" rowspan=\"1\">0.8161</td><td colspan=\"1\" rowspan=\"1\">0.9286</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.175M</td><td colspan=\"1\" rowspan=\"1\">26.86</td><td colspan=\"1\" rowspan=\"1\">0.083</td><td colspan=\"1\" rowspan=\"1\">10.84</td><td colspan=\"1\" rowspan=\"1\">0.8896</td><td colspan=\"1\" rowspan=\"1\">0.8824</td><td colspan=\"1\" rowspan=\"1\">0.9762</td><td colspan=\"1\" rowspan=\"1\">0.9583</td><td colspan=\"1\" rowspan=\"1\">0.8008</td><td colspan=\"1\" rowspan=\"1\">0.9295</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet_<italic toggle=\"yes\">DGA</italic></td><td colspan=\"1\" rowspan=\"1\">0.128M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.074</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9025</td><td colspan=\"1\" rowspan=\"1\">0.8623</td><td colspan=\"1\" rowspan=\"1\">0.9882</td><td colspan=\"1\" rowspan=\"1\">0.9635</td><td colspan=\"1\" rowspan=\"1\">0.8234</td><td colspan=\"1\" rowspan=\"1\">0.9327</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053M</td><td colspan=\"1\" rowspan=\"1\">1.89</td><td colspan=\"1\" rowspan=\"1\">0.072</td><td colspan=\"1\" rowspan=\"1\">1.39</td><td colspan=\"1\" rowspan=\"1\">0.9073</td><td colspan=\"1\" rowspan=\"1\">0.8931</td><td colspan=\"1\" rowspan=\"1\">0.9816</td><td colspan=\"1\" rowspan=\"1\">0.9642</td><td colspan=\"1\" rowspan=\"1\">0.8302</td><td colspan=\"1\" rowspan=\"1\">0.9219</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet_<italic toggle=\"yes\">GHPA</italic></td><td colspan=\"1\" rowspan=\"1\">0.052M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.071</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9092</td><td colspan=\"1\" rowspan=\"1\">0.8941</td><td colspan=\"1\" rowspan=\"1\">0.9823</td><td colspan=\"1\" rowspan=\"1\">0.9650</td><td colspan=\"1\" rowspan=\"1\">0.8332</td><td colspan=\"1\" rowspan=\"1\">0.9241</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.427M</td><td colspan=\"1\" rowspan=\"1\">60.94</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">56.10</td><td colspan=\"1\" rowspan=\"1\">0.9070</td><td colspan=\"1\" rowspan=\"1\">0.8837</td><td colspan=\"1\" rowspan=\"1\">0.9842</td><td colspan=\"1\" rowspan=\"1\">0.9645</td><td colspan=\"1\" rowspan=\"1\">0.8298</td><td colspan=\"1\" rowspan=\"1\">0.9302</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet_<italic toggle=\"yes\">Vision Mamba</italic></td><td colspan=\"1\" rowspan=\"1\">10.713M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">1.805</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.8885</td><td colspan=\"1\" rowspan=\"1\">0.8517</td><td colspan=\"1\" rowspan=\"1\">0.9841</td><td colspan=\"1\" rowspan=\"1\">0.9582</td><td colspan=\"1\" rowspan=\"1\">0.8120</td><td colspan=\"1\" rowspan=\"1\">0.9177</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771M</td><td colspan=\"1\" rowspan=\"1\">73.69</td><td colspan=\"1\" rowspan=\"1\">4.400</td><td colspan=\"1\" rowspan=\"1\">65.43</td><td colspan=\"1\" rowspan=\"1\">0.9045</td><td colspan=\"1\" rowspan=\"1\">0.8768</td><td colspan=\"1\" rowspan=\"1\">0.9849</td><td colspan=\"1\" rowspan=\"1\">0.9637</td><td colspan=\"1\" rowspan=\"1\">0.8256</td><td colspan=\"1\" rowspan=\"1\">0.9168</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2_<italic toggle=\"yes\">Vision Mamba</italic></td><td colspan=\"1\" rowspan=\"1\">5.991M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">1.521</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.8916</td><td colspan=\"1\" rowspan=\"1\">0.8692</td><td colspan=\"1\" rowspan=\"1\">0.9804</td><td colspan=\"1\" rowspan=\"1\">0.9586</td><td colspan=\"1\" rowspan=\"1\">0.8205</td><td colspan=\"1\" rowspan=\"1\">0.9119</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.403M</td><td colspan=\"1\" rowspan=\"1\">72.95</td><td colspan=\"1\" rowspan=\"1\">0.391</td><td colspan=\"1\" rowspan=\"1\">0.26</td><td colspan=\"1\" rowspan=\"1\">0.9080</td><td colspan=\"1\" rowspan=\"1\">0.8839</td><td colspan=\"1\" rowspan=\"1\">0.9846</td><td colspan=\"1\" rowspan=\"1\">0.9649</td><td colspan=\"1\" rowspan=\"1\">0.8303</td><td colspan=\"1\" rowspan=\"1\">0.9321</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet_<italic toggle=\"yes\">Mamba Layer</italic></td><td colspan=\"1\" rowspan=\"1\">0.109M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.390</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.9045</td><td colspan=\"1\" rowspan=\"1\">0.8668</td><td colspan=\"1\" rowspan=\"1\">0.9878</td><td colspan=\"1\" rowspan=\"1\">0.9642</td><td colspan=\"1\" rowspan=\"1\">0.8292</td><td colspan=\"1\" rowspan=\"1\">0.9297</td></tr></tbody></table><table-wrap-foot><fn><p>Italics in the method column (e.g., <italic toggle=\"yes\">_Conv</italic>) indicate that the PVM Layer replaces the convolution module. In particular, we replace modules containing Convolution, Vision Transformer, Mamba, and Vision Mamba, covering the four most commonly used base modules. <inline-formula><mml:math id=\"M133\" altimg=\"si82.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"M134\" altimg=\"si83.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">G</mml:mi><mml:mspace width=\"0.25em\"/></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denote the percentage decrease in parameters and GFLOPs, respectively, after replacing with PVM Layer.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id=\"sec3.1.4\"><title>Impact of components in the UltraLight VM-UNet</title><p id=\"p0210\">A series of ablation experiments were performed to verify the impact of each module in the UltraLight VM-UNet. As shown in <xref rid=\"tbl5\" ref-type=\"table\">Table 5</xref>, we replace the PVM Layer of the encoder, decoder, respectively, with a standard convolution with convolution kernel 3. In addition, we also replace the PVM Layer of the encoder and decoder at the same time with a standard convolution. Also, <inline-formula><mml:math id=\"M135\" altimg=\"si70.gif\"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mspace width=\"1em\"/><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width=\"1em\"/><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> indicates that the skip-connection part of UltraLight VM-UNet does not use the SAB and CAB modules. From the table, we can conclude that, by replacing the PVM Layer of the encoder and decoder separately, the parameters increased by 63.26<inline-formula><mml:math id=\"M136\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and the GFLOPs increased in both, while the performance decreased in both. In particular, after replacing the PVM Layer of the encoder and decoder simultaneously, the parameters increase by 151<inline-formula><mml:math id=\"M137\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and the GFOLPs increase by 25<inline-formula><mml:math id=\"M138\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>. In summary, it is shown that, after replacing the PVM Layer, there is a decrease in all performance aspects and an increase in both parameters and GFLOPs. This again proves the crucial role of PVM Layer. What is more, although the parameters and GFOLPs were further reduced with the removal of the SAB and CAB modules. However, the performance also exhibits some degradation, which is due to the ability of the SAB and CAB modules to combine multi-scale feature information for learning, which improves the sensitivity to lesions and accelerates the model convergence.<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref> Therefore, to balance the performance and computational complexity relationship, UltraLight VM-UNet employs the SAB and CAB modules as a bridge for skip connections to further improve segmentation performance. Even so, the performance, parameters, and GFLOPs of UltraLight VM-UNet are still in the forefront compared with the rest of the state-of-the-art lightweight models available currently.<table-wrap position=\"float\" id=\"tbl5\" orientation=\"portrait\"><label>Table 5</label><caption><p>Ablation experiments on the effect of each module in the UltraLight VM-UNet</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Methods</th><th colspan=\"1\" rowspan=\"1\">Params</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (baseline)</td><td colspan=\"1\" rowspan=\"1\">0.049M</td><td colspan=\"1\" rowspan=\"1\">0.060</td><td colspan=\"1\" rowspan=\"1\">0.9091<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9053<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9790</td><td colspan=\"1\" rowspan=\"1\">0.9646<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8334<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9481<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Baseline_Encoder_Conv</td><td colspan=\"1\" rowspan=\"1\">0.080M</td><td colspan=\"1\" rowspan=\"1\">0.071</td><td colspan=\"1\" rowspan=\"1\">0.9033</td><td colspan=\"1\" rowspan=\"1\">0.8643</td><td colspan=\"1\" rowspan=\"1\">0.9880</td><td colspan=\"1\" rowspan=\"1\">0.9638</td><td colspan=\"1\" rowspan=\"1\">0.8237</td><td colspan=\"1\" rowspan=\"1\">0.9461</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Baseline_Decoder_Conv</td><td colspan=\"1\" rowspan=\"1\">0.080M</td><td colspan=\"1\" rowspan=\"1\">0.064</td><td colspan=\"1\" rowspan=\"1\">0.8958</td><td colspan=\"1\" rowspan=\"1\">0.8512</td><td colspan=\"1\" rowspan=\"1\">0.9881<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9612</td><td colspan=\"1\" rowspan=\"1\">0.8113</td><td colspan=\"1\" rowspan=\"1\">0.9454</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Baseline_(En+De)_Conv</td><td colspan=\"1\" rowspan=\"1\">0.123M</td><td colspan=\"1\" rowspan=\"1\">0.075</td><td colspan=\"1\" rowspan=\"1\">0.9065</td><td colspan=\"1\" rowspan=\"1\">0.8784</td><td colspan=\"1\" rowspan=\"1\">0.9855</td><td colspan=\"1\" rowspan=\"1\">0.9645</td><td colspan=\"1\" rowspan=\"1\">0.8291</td><td colspan=\"1\" rowspan=\"1\">0.9365</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Baseline_SCAB not applicable</td><td colspan=\"1\" rowspan=\"1\">0.033M<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.058<xref rid=\"tblfn3\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9029</td><td colspan=\"1\" rowspan=\"1\">0.8767</td><td colspan=\"1\" rowspan=\"1\">0.9841</td><td colspan=\"1\" rowspan=\"1\">0.9631</td><td colspan=\"1\" rowspan=\"1\">0.8221</td><td colspan=\"1\" rowspan=\"1\">0.9436</td></tr></tbody></table><table-wrap-foot><fn id=\"tblfn3\"><label>a</label><p id=\"ntpara0025\">These <inline-formula><mml:math id=\"M139\" altimg=\"si84.gif\"><mml:mrow><mml:mtext>values</mml:mtext></mml:mrow></mml:math></inline-formula> represent the best performance.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id=\"sec3.1.5\"><title>Impact of different channel numbers in UltraLight VM-UNet</title><p id=\"p0215\">In addition, to verify the impact of different channel numbers in the UltraLight VM-UNet, we conducted a series of ablation experiments. As shown in <xref rid=\"tbl6\" ref-type=\"table\">Table 6</xref>, we set up four different combinations of channel numbers, including [8,16,24,32,48,64], [8,16,32,64,128,256], [16,32,64,128,256,512], and [32,64,128,256,512,1024], respectively. In addition, we conducted experiments both in the parallel-free manner (labeled by &#8220;1&#8221;) and in the parallel manner (labeled by &#8220;2&#8221;). From the table, it can be concluded that the parameters and GFLOPs at [8,16,24,32,48,64] are the lowest, while the overall difference in performance is not very large. However, as the number of channels increases, both parameters and GFLOPs show a significant rise. The parameter increases from 0.136M to 13.607M for the group without parallelism and from 0.049M to 3.305M for the group with parallelism. In particular, the average decrease in parameters for the same number of channel combinations using the parallel approach over the non-parallel approach is 72.70<inline-formula><mml:math id=\"M140\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, which is extremely close to the percentage of decrease in the four-parallel approach over the non-parallel approach analyzed in the <xref rid=\"sec4\" ref-type=\"sec\">methods</xref> section (74.80<inline-formula><mml:math id=\"M141\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>). In addition, at [8,16,24,32,48,64], the parameters are more susceptible to the rest of the modules, so the decrease is not as large as for the rest of the channel number combinations. However, its overall parameters and GFLOPs reach an impressive 0.049M and 0.060, and show strong competitive segmentation performance.<table-wrap position=\"float\" id=\"tbl6\" orientation=\"portrait\"><label>Table 6</label><caption><p>Ablation experiments on the effect of combining different channel numbers in the UltraLight VM-UNet</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Methods</th><th colspan=\"1\" rowspan=\"1\">Params</th><th colspan=\"1\" rowspan=\"1\"><inline-formula><mml:math id=\"M142\" altimg=\"si85.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>74.80</mml:mn><mml:mo>%</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; [8,16,24,32,48,64]</td><td colspan=\"1\" rowspan=\"1\">0.136M</td><td colspan=\"1\" rowspan=\"1\">63.98<inline-formula><mml:math id=\"M143\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula><sup>(&#8722;10.82%)</sup></td><td colspan=\"1\" rowspan=\"1\">0.060</td><td colspan=\"1\" rowspan=\"1\">0.9069</td><td colspan=\"1\" rowspan=\"1\">0.8861</td><td colspan=\"1\" rowspan=\"1\">0.9834</td><td colspan=\"1\" rowspan=\"1\">0.9644</td><td colspan=\"1\" rowspan=\"1\">0.8266</td><td colspan=\"1\" rowspan=\"1\">0.9354</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; [8,16,24,32,48,64]</td><td colspan=\"1\" rowspan=\"1\">0.049M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.060</td><td colspan=\"1\" rowspan=\"1\">0.9091</td><td colspan=\"1\" rowspan=\"1\">0.9053</td><td colspan=\"1\" rowspan=\"1\">0.9790</td><td colspan=\"1\" rowspan=\"1\">0.9646</td><td colspan=\"1\" rowspan=\"1\">0.8334</td><td colspan=\"1\" rowspan=\"1\">0.9481</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; [8,16,32,64,128,256]</td><td colspan=\"1\" rowspan=\"1\">0.909M</td><td colspan=\"1\" rowspan=\"1\">75.47<inline-formula><mml:math id=\"M144\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula><sup>(+0.67%)</sup></td><td colspan=\"1\" rowspan=\"1\">0.074</td><td colspan=\"1\" rowspan=\"1\">0.9018</td><td colspan=\"1\" rowspan=\"1\">0.8749</td><td colspan=\"1\" rowspan=\"1\">0.9840</td><td colspan=\"1\" rowspan=\"1\">0.9627</td><td colspan=\"1\" rowspan=\"1\">0.8279</td><td colspan=\"1\" rowspan=\"1\">0.9417</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; [8,16,32,64,128,256]</td><td colspan=\"1\" rowspan=\"1\">0.223M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.074</td><td colspan=\"1\" rowspan=\"1\">0.9079</td><td colspan=\"1\" rowspan=\"1\">0.8965</td><td colspan=\"1\" rowspan=\"1\">0.9809</td><td colspan=\"1\" rowspan=\"1\">0.9644</td><td colspan=\"1\" rowspan=\"1\">0.8282</td><td colspan=\"1\" rowspan=\"1\">0.9326</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; [16,32,64,128,256,512]</td><td colspan=\"1\" rowspan=\"1\">3.479M</td><td colspan=\"1\" rowspan=\"1\">75.63<inline-formula><mml:math id=\"M145\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula><sup>(+0.83%)</sup></td><td colspan=\"1\" rowspan=\"1\">0.259</td><td colspan=\"1\" rowspan=\"1\">0.9049</td><td colspan=\"1\" rowspan=\"1\">0.8981</td><td colspan=\"1\" rowspan=\"1\">0.9789</td><td colspan=\"1\" rowspan=\"1\">0.9630</td><td colspan=\"1\" rowspan=\"1\">0.8311</td><td colspan=\"1\" rowspan=\"1\">0.9402</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; [16,32,64,128,256,512]</td><td colspan=\"1\" rowspan=\"1\">0.848M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.259</td><td colspan=\"1\" rowspan=\"1\">0.9056</td><td colspan=\"1\" rowspan=\"1\">0.8906</td><td colspan=\"1\" rowspan=\"1\">0.9815</td><td colspan=\"1\" rowspan=\"1\">0.9637</td><td colspan=\"1\" rowspan=\"1\">0.8224</td><td colspan=\"1\" rowspan=\"1\">0.9450</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;1&#8221; [32,64,128,256,512,1024]</td><td colspan=\"1\" rowspan=\"1\">13.607M</td><td colspan=\"1\" rowspan=\"1\">75.71<inline-formula><mml:math id=\"M146\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula><sup>(+0.91%)</sup></td><td colspan=\"1\" rowspan=\"1\">0.970</td><td colspan=\"1\" rowspan=\"1\">0.9053</td><td colspan=\"1\" rowspan=\"1\">0.8878</td><td colspan=\"1\" rowspan=\"1\">0.9821</td><td colspan=\"1\" rowspan=\"1\">0.9637</td><td colspan=\"1\" rowspan=\"1\">0.8214</td><td colspan=\"1\" rowspan=\"1\">0.9459</td></tr><tr><td colspan=\"1\" rowspan=\"1\">&#8220;2&#8221; [32,64,128,256,512,1024]</td><td colspan=\"1\" rowspan=\"1\">3.305M</td><td colspan=\"1\" rowspan=\"1\"/><td colspan=\"1\" rowspan=\"1\">0.970</td><td colspan=\"1\" rowspan=\"1\">0.9012</td><td colspan=\"1\" rowspan=\"1\">0.8812</td><td colspan=\"1\" rowspan=\"1\">0.9819</td><td colspan=\"1\" rowspan=\"1\">0.9622</td><td colspan=\"1\" rowspan=\"1\">0.8297</td><td colspan=\"1\" rowspan=\"1\">0.9334</td></tr></tbody></table><table-wrap-foot><fn><p><inline-formula><mml:math id=\"M147\" altimg=\"si82.gif\"><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">D</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denotes the percentage of parameter reduction with the parallel approach; 74.80<inline-formula><mml:math id=\"M148\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> denotes the theoretical percentage reduction of localized module parameters derived from the Mamba analysis in the <xref rid=\"sec4\" ref-type=\"sec\">methods</xref>; &#8220;1&#8221; indicates non-parallel connection method and &#8220;2&#8221; indicates a quadruple parallel connection method.</p></fn></table-wrap-foot></table-wrap></p></sec></sec><sec id=\"sec3.2\"><title>Discussion in practical engineering</title><p id=\"p0220\">In practical engineering applications, it is common for different devices to be used for training and inference in skin lesion segmentation tasks. To further discuss the superiority of the proposed method, we provide the throughput of our model versus the baseline model on different devices in <xref rid=\"fig8\" ref-type=\"fig\">Figure 8</xref>. Specifically, the throughput of our proposed model on 3090, 4090, and V100 is about two times higher than that of the baseline model (VM-UNet). In particular, compared with LightM-UNet, our model achieves a 20<inline-formula><mml:math id=\"M149\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>-40<inline-formula><mml:math id=\"M150\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> improvement in throughput across a variety of hardware platforms.<fig id=\"fig8\" position=\"float\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Discussion of the results in practical engineering applications</p><p>(A) Comparison of training speed and inference throughput of the proposed model with baseline model and the lightest Mamba-based medical image segmentation model at different resolutions and different devices.</p><p>(B) Comparison of the memory required for training the proposed model with the baseline model and the lightest Mamba-based medical image segmentation model at different resolutions.</p><p>(C) Comparison of the memory required for inference of the proposed model with the baseline model and the lightest Mamba-based medical image segmentation model at different resolutions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"gr8.jpg\"/></fig></p><p id=\"p0225\">In addition, the need for segmentation of skin lesion images at different resolutions is also encountered through practical engineering applications. Based on this problem, we tested the training speed and inference speed (throughput) of the proposed model versus the baseline model in <xref rid=\"fig8\" ref-type=\"fig\">Figure 8</xref> at different resolutions. However, a batch size of 1 is used for inference, so the inference speed (throughput) does not change much at different resolutions. In particular, the advantage of our model is shown more clearly on the high resolution <inline-formula><mml:math id=\"M151\" altimg=\"si71.gif\"><mml:mrow><mml:mn>512</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>512</mml:mn><mml:mtext>.</mml:mtext></mml:mrow></mml:math></inline-formula> Nonetheless, researchers prefer to use <inline-formula><mml:math id=\"M152\" altimg=\"si72.gif\"><mml:mrow><mml:mn>256</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> resolution in processing images of skin lesion.<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref><sup>,</sup><xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref><sup>,</sup><xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref><sup>,</sup><xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref><sup>,</sup><xref rid=\"bib29\" ref-type=\"bibr\"><sup>29</sup></xref></p></sec><sec id=\"sec3.3\"><title>Discussion under the task of non-skin lesion segmentation</title><p id=\"p0230\">To further validate the potential of the proposed UltraLight VM-UNet, we conducted comprehensive experiments on two additional datasets: the Autooral dataset<xref rid=\"bib32\" ref-type=\"bibr\"><sup>32</sup></xref> and the Spleen dataset.<xref rid=\"bib33\" ref-type=\"bibr\"><sup>33</sup></xref> These datasets comprise oral ulcer (cancer) images acquired through endoscopy and spleen images in CT mode, respectively. The Autooral dataset, developed by Jiang et al.,<xref rid=\"bib32\" ref-type=\"bibr\"><sup>32</sup></xref> contains high-quality oral ulcer (cancer) images obtained by oral endoscopy. We maintained experimental consistency with the original study&#8217;s protocols. The Spleen dataset, established by Memorial Sloan Kettering Cancer Center,<xref rid=\"bib33\" ref-type=\"bibr\"><sup>33</sup></xref> provides CT modality data for spleen segmentation, and the experimental configuration adhered to the methodology outlined by Wu et al.<xref rid=\"bib29\" ref-type=\"bibr\"><sup>29</sup></xref> to ensure a fair comparison. The results of the experiments on the Autooral dataset and the Spleen dataset are presented in <xref rid=\"tbl7\" ref-type=\"table\">Table 7</xref>. Since our focus is on the exploration of lightweight, we performed experiments with the current state-of-the-art lightweight and classical medical image segmentation models. From the table, it can be concluded that although the UltraLight VM-UNet is slightly inferior to other models in terms of DSC and sensitivity (SE) metrics on endoscopic mode and CT mode; however, it still shows strong competitiveness in terms of SP and accuracy (ACC) metrics. In particular, compared with other current state-of-the-art lightweight models (MALUNet, LightM-UNet, and EGE-UNet), we lead in several metrics. The UltraLight VM-UNet demonstrates competitiveness through experiments in both endoscopic and CT modalities, as well as the dermoscopic segmentation task that is the focus of this paper. In particular, the UltraLight VM-UNet is more competitive for dermatoscope segmentation tasks.<table-wrap position=\"float\" id=\"tbl7\" orientation=\"portrait\"><label>Table 7</label><caption><p>Experimental results with state-of-the-art lightweight and classical medical image segmentation models on Autooral dataset and Spleen dataset</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th colspan=\"1\" rowspan=\"1\">Model</th><th colspan=\"1\" rowspan=\"1\">Parameters (millions)</th><th colspan=\"1\" rowspan=\"1\">GFLOPs</th><th colspan=\"1\" rowspan=\"1\">DSC/F1</th><th colspan=\"1\" rowspan=\"1\">SE/Recall</th><th colspan=\"1\" rowspan=\"1\">SP</th><th colspan=\"1\" rowspan=\"1\">ACC</th><th colspan=\"1\" rowspan=\"1\">IoU</th><th colspan=\"1\" rowspan=\"1\">Prec</th></tr></thead><tbody><tr><td colspan=\"9\" rowspan=\"1\"><bold>Comparison experiments on the Autooral dataset (non-skin lesion segmentation task)</bold></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.009</td><td colspan=\"1\" rowspan=\"1\">3.224</td><td colspan=\"1\" rowspan=\"1\">0.7480<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.7282</td><td colspan=\"1\" rowspan=\"1\">0.9815</td><td colspan=\"1\" rowspan=\"1\">0.9617</td><td colspan=\"1\" rowspan=\"1\">0.5575</td><td colspan=\"1\" rowspan=\"1\">0.7290</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.801</td><td colspan=\"1\" rowspan=\"1\">1.567</td><td colspan=\"1\" rowspan=\"1\">0.7069</td><td colspan=\"1\" rowspan=\"1\">0.6148</td><td colspan=\"1\" rowspan=\"1\">0.9896</td><td colspan=\"1\" rowspan=\"1\">0.9602</td><td colspan=\"1\" rowspan=\"1\">0.5467</td><td colspan=\"1\" rowspan=\"1\">0.8315<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-Unet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.176</td><td colspan=\"1\" rowspan=\"1\">7.724</td><td colspan=\"1\" rowspan=\"1\">0.4484</td><td colspan=\"1\" rowspan=\"1\">0.3675</td><td colspan=\"1\" rowspan=\"1\">0.9770</td><td colspan=\"1\" rowspan=\"1\">0.9295</td><td colspan=\"1\" rowspan=\"1\">0.2890</td><td colspan=\"1\" rowspan=\"1\">0.5750</td></tr><tr><td colspan=\"1\" rowspan=\"1\">ATTENTION SWIN U-NET<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref></td><td colspan=\"1\" rowspan=\"1\">46.910</td><td colspan=\"1\" rowspan=\"1\">14.181</td><td colspan=\"1\" rowspan=\"1\">0.6463</td><td colspan=\"1\" rowspan=\"1\">0.5032</td><td colspan=\"1\" rowspan=\"1\">0.9954<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9571</td><td colspan=\"1\" rowspan=\"1\">0.4774</td><td colspan=\"1\" rowspan=\"1\">0.7732</td></tr><tr><td colspan=\"1\" rowspan=\"1\">C<sup>2</sup>SDG<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.001</td><td colspan=\"1\" rowspan=\"1\">7.972</td><td colspan=\"1\" rowspan=\"1\">0.7210</td><td colspan=\"1\" rowspan=\"1\">0.6554</td><td colspan=\"1\" rowspan=\"1\">0.9862</td><td colspan=\"1\" rowspan=\"1\">0.9604</td><td colspan=\"1\" rowspan=\"1\">0.5638</td><td colspan=\"1\" rowspan=\"1\">0.8012<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.427</td><td colspan=\"1\" rowspan=\"1\">4.112</td><td colspan=\"1\" rowspan=\"1\">0.7639<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.7555<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9811</td><td colspan=\"1\" rowspan=\"1\">0.9636<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.5970<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.6879</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771</td><td colspan=\"1\" rowspan=\"1\">4.400</td><td colspan=\"1\" rowspan=\"1\">0.7461</td><td colspan=\"1\" rowspan=\"1\">0.7285<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9813</td><td colspan=\"1\" rowspan=\"1\">0.9602</td><td colspan=\"1\" rowspan=\"1\">0.5580</td><td colspan=\"1\" rowspan=\"1\">0.7060</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.175</td><td colspan=\"1\" rowspan=\"1\">0.083</td><td colspan=\"1\" rowspan=\"1\">0.6318</td><td colspan=\"1\" rowspan=\"1\">0.6500</td><td colspan=\"1\" rowspan=\"1\">0.9655</td><td colspan=\"1\" rowspan=\"1\">0.9409</td><td colspan=\"1\" rowspan=\"1\">0.4832</td><td colspan=\"1\" rowspan=\"1\">0.7201</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.403</td><td colspan=\"1\" rowspan=\"1\">0.391</td><td colspan=\"1\" rowspan=\"1\">0.6551</td><td colspan=\"1\" rowspan=\"1\">0.6130</td><td colspan=\"1\" rowspan=\"1\">0.9781</td><td colspan=\"1\" rowspan=\"1\">0.9497</td><td colspan=\"1\" rowspan=\"1\">0.4483</td><td colspan=\"1\" rowspan=\"1\">0.5831</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.072<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.5893</td><td colspan=\"1\" rowspan=\"1\">0.7263</td><td colspan=\"1\" rowspan=\"1\">0.9375</td><td colspan=\"1\" rowspan=\"1\">0.9211</td><td colspan=\"1\" rowspan=\"1\">0.4178</td><td colspan=\"1\" rowspan=\"1\">0.4958</td></tr><tr><td colspan=\"1\" rowspan=\"1\">DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td><td colspan=\"1\" rowspan=\"1\">45.112</td><td colspan=\"1\" rowspan=\"1\">38.636</td><td colspan=\"1\" rowspan=\"1\">0.3569</td><td colspan=\"1\" rowspan=\"1\">0.2565</td><td colspan=\"1\" rowspan=\"1\">0.9845</td><td colspan=\"1\" rowspan=\"1\">0.9338</td><td colspan=\"1\" rowspan=\"1\">0.2134</td><td colspan=\"1\" rowspan=\"1\">0.7599</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.957</td><td colspan=\"1\" rowspan=\"1\">1.591</td><td colspan=\"1\" rowspan=\"1\">0.7425</td><td colspan=\"1\" rowspan=\"1\">0.7166</td><td colspan=\"1\" rowspan=\"1\">0.9833</td><td colspan=\"1\" rowspan=\"1\">0.9609</td><td colspan=\"1\" rowspan=\"1\">0.5981<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.7533</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (Our)</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.060<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.7255</td><td colspan=\"1\" rowspan=\"1\">0.6637</td><td colspan=\"1\" rowspan=\"1\">0.9903<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9621<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.5742</td><td colspan=\"1\" rowspan=\"1\">0.7673</td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><bold>Comparison experiments on the Spleen dataset (non-skin lesion segmentation task)</bold></td></tr><tr><td colspan=\"9\" rowspan=\"1\"><hr/></td></tr><tr><td colspan=\"1\" rowspan=\"1\">U-Net<xref rid=\"bib21\" ref-type=\"bibr\"><sup>21</sup></xref></td><td colspan=\"1\" rowspan=\"1\">2.010</td><td colspan=\"1\" rowspan=\"1\">5.037</td><td colspan=\"1\" rowspan=\"1\">0.9441<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9604<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9989</td><td colspan=\"1\" rowspan=\"1\">0.9983<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.8679</td><td colspan=\"1\" rowspan=\"1\">0.9283</td></tr><tr><td colspan=\"1\" rowspan=\"1\">SCR-Net<xref rid=\"bib22\" ref-type=\"bibr\"><sup>22</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.812</td><td colspan=\"1\" rowspan=\"1\">2.449</td><td colspan=\"1\" rowspan=\"1\">0.9181</td><td colspan=\"1\" rowspan=\"1\">0.9122</td><td colspan=\"1\" rowspan=\"1\">0.9988</td><td colspan=\"1\" rowspan=\"1\">0.9976</td><td colspan=\"1\" rowspan=\"1\">0.8486</td><td colspan=\"1\" rowspan=\"1\">0.9220</td></tr><tr><td colspan=\"1\" rowspan=\"1\">Swin-Unet<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.193</td><td colspan=\"1\" rowspan=\"1\">12.084</td><td colspan=\"1\" rowspan=\"1\">0.6758</td><td colspan=\"1\" rowspan=\"1\">0.5526</td><td colspan=\"1\" rowspan=\"1\">0.9923</td><td colspan=\"1\" rowspan=\"1\">0.9818</td><td colspan=\"1\" rowspan=\"1\">0.4832</td><td colspan=\"1\" rowspan=\"1\">0.5541</td></tr><tr><td colspan=\"1\" rowspan=\"1\">ATTENTION SWIN U-NET<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref></td><td colspan=\"1\" rowspan=\"1\">46.945</td><td colspan=\"1\" rowspan=\"1\">22.175</td><td colspan=\"1\" rowspan=\"1\">0.7829</td><td colspan=\"1\" rowspan=\"1\">0.6662</td><td colspan=\"1\" rowspan=\"1\">0.9994<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9945</td><td colspan=\"1\" rowspan=\"1\">0.6433</td><td colspan=\"1\" rowspan=\"1\">0.9341</td></tr><tr><td colspan=\"1\" rowspan=\"1\">C<sup>2</sup>SDG<xref rid=\"bib24\" ref-type=\"bibr\"><sup>24</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.010</td><td colspan=\"1\" rowspan=\"1\">12.457</td><td colspan=\"1\" rowspan=\"1\">0.9354</td><td colspan=\"1\" rowspan=\"1\">0.9263</td><td colspan=\"1\" rowspan=\"1\">0.9991<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9981</td><td colspan=\"1\" rowspan=\"1\">0.8787</td><td colspan=\"1\" rowspan=\"1\">0.9448<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet<xref rid=\"bib25\" ref-type=\"bibr\"><sup>25</sup></xref></td><td colspan=\"1\" rowspan=\"1\">27.428</td><td colspan=\"1\" rowspan=\"1\">6.425</td><td colspan=\"1\" rowspan=\"1\">0.9418</td><td colspan=\"1\" rowspan=\"1\">0.9429</td><td colspan=\"1\" rowspan=\"1\">0.9991<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9982</td><td colspan=\"1\" rowspan=\"1\">0.8859</td><td colspan=\"1\" rowspan=\"1\">0.9248</td></tr><tr><td colspan=\"1\" rowspan=\"1\">VM-UNet v2<xref rid=\"bib26\" ref-type=\"bibr\"><sup>26</sup></xref></td><td colspan=\"1\" rowspan=\"1\">22.771</td><td colspan=\"1\" rowspan=\"1\">6.874</td><td colspan=\"1\" rowspan=\"1\">0.9445<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9315</td><td colspan=\"1\" rowspan=\"1\">0.9990</td><td colspan=\"1\" rowspan=\"1\">0.9982</td><td colspan=\"1\" rowspan=\"1\">0.8902<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9419</td></tr><tr><td colspan=\"1\" rowspan=\"1\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.178</td><td colspan=\"1\" rowspan=\"1\">0.130</td><td colspan=\"1\" rowspan=\"1\">0.9310</td><td colspan=\"1\" rowspan=\"1\">0.9305</td><td colspan=\"1\" rowspan=\"1\">0.9989</td><td colspan=\"1\" rowspan=\"1\">0.9979</td><td colspan=\"1\" rowspan=\"1\">0.8709</td><td colspan=\"1\" rowspan=\"1\">0.9315</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.405</td><td colspan=\"1\" rowspan=\"1\">0.587</td><td colspan=\"1\" rowspan=\"1\">0.9329</td><td colspan=\"1\" rowspan=\"1\">0.9459</td><td colspan=\"1\" rowspan=\"1\">0.9988</td><td colspan=\"1\" rowspan=\"1\">0.9982</td><td colspan=\"1\" rowspan=\"1\">0.8646</td><td colspan=\"1\" rowspan=\"1\">0.9317</td></tr><tr><td colspan=\"1\" rowspan=\"1\">EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.053<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.113<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.8992</td><td colspan=\"1\" rowspan=\"1\">0.8866</td><td colspan=\"1\" rowspan=\"1\">0.9987</td><td colspan=\"1\" rowspan=\"1\">0.9975</td><td colspan=\"1\" rowspan=\"1\">0.8251</td><td colspan=\"1\" rowspan=\"1\">0.9347</td></tr><tr><td colspan=\"1\" rowspan=\"1\">DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref></td><td colspan=\"1\" rowspan=\"1\">45.112</td><td colspan=\"1\" rowspan=\"1\">60.368</td><td colspan=\"1\" rowspan=\"1\">0.9266</td><td colspan=\"1\" rowspan=\"1\">0.9507<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9987</td><td colspan=\"1\" rowspan=\"1\">0.9979</td><td colspan=\"1\" rowspan=\"1\">0.8777</td><td colspan=\"1\" rowspan=\"1\">0.9196</td></tr><tr><td colspan=\"1\" rowspan=\"1\">LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref></td><td colspan=\"1\" rowspan=\"1\">0.957</td><td colspan=\"1\" rowspan=\"1\">2.484</td><td colspan=\"1\" rowspan=\"1\">0.9214</td><td colspan=\"1\" rowspan=\"1\">0.9455</td><td colspan=\"1\" rowspan=\"1\">0.9990</td><td colspan=\"1\" rowspan=\"1\">0.9981</td><td colspan=\"1\" rowspan=\"1\">0.8897<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9314</td></tr><tr><td colspan=\"1\" rowspan=\"1\">UltraLight VM-UNet (Our)</td><td colspan=\"1\" rowspan=\"1\">0.049<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.094<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.9168</td><td colspan=\"1\" rowspan=\"1\">0.8991</td><td colspan=\"1\" rowspan=\"1\">0.9991<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td><td colspan=\"1\" rowspan=\"1\">0.9984<xref rid=\"tblfn4\" ref-type=\"table-fn\">a</xref></td><td colspan=\"1\" rowspan=\"1\">0.8566</td><td colspan=\"1\" rowspan=\"1\">0.9430<xref rid=\"tblfn5\" ref-type=\"table-fn\">b</xref></td></tr></tbody></table><table-wrap-foot><fn id=\"tblfn4\"><label>a</label><p id=\"ntpara0030\">These <inline-formula><mml:math id=\"M153\" altimg=\"si84.gif\"><mml:mrow><mml:mtext>values</mml:mtext></mml:mrow></mml:math></inline-formula> represent the best performance.</p></fn></table-wrap-foot><table-wrap-foot><fn id=\"tblfn5\"><label>b</label><p id=\"ntpara0035\">These <inline-formula><mml:math id=\"M154\" altimg=\"si84.gif\"><mml:mrow><mml:mtext>values</mml:mtext></mml:mrow></mml:math></inline-formula> represent the second-best performance.</p></fn></table-wrap-foot></table-wrap></p></sec><sec id=\"sec3.4\"><title>Conclusion</title><p id=\"p0235\">In this study, we conduct a detailed analysis of the key factors influencing parameter efficiency in Mamba. Building upon these insights, we propose a PVM Layer for efficient deep feature processing. The PVM Layer consists of four VM modules operating in parallel, with each module responsible for processing one-quarter of the original input channels. This is due to the fact that the number of input channels to the SSM in Mamba has an explosive effect on the number of parameters, and the VM parameters for processing a quarter of the number of channels are 6.3<inline-formula><mml:math id=\"M155\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of the original VM parameters, which is an explosive reduction of 93.7<inline-formula><mml:math id=\"M156\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>. In addition, the PVM Layer can be generalized to any Mamba variant (P <inline-formula><mml:math id=\"M157\" altimg=\"si67.gif\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> M Layer). Based on the PVM Layer, we propose the UltraLight VM-UNet with a parameter of only 0.049M and GFLOPs of only 0.060. The UltraLight VM-UNet parameters are 99.82<inline-formula><mml:math id=\"M158\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than those of the traditionally pure VM-UNet model and 87.84<inline-formula><mml:math id=\"M159\" altimg=\"si1.gif\"><mml:mrow><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> lower than those of the lightest LightM-UNet model. In addition, we experimentally demonstrated on three publicly available skin lesion datasets that the UltraLight VM-UNet has equally strong performance competitiveness with such low parameters. We also discuss the performance of the UltraLight VM-UNet for non-skin lesion segmentation tasks and demonstrate its potential competitiveness.</p><p id=\"p0240\">In particular, it is important to note that in this work we have not just proposed a lightweight model but an in-depth exploration of Mamba for lightweight research in healthcare settings. Based on this work, Mamba may become a new mainstream module for lightweight model.</p></sec></sec><sec id=\"sec4\"><title>Methods</title><sec id=\"sec4.1\"><title>Related work</title><p id=\"p0245\">Medical image segmentation, as one of the important branches in image segmentation, is also one of the research directions to which many researchers have devoted their efforts. Among them, multi-scale variation problem and feature refinement learning are the key problems in medical image segmentation.<xref rid=\"bib34\" ref-type=\"bibr\"><sup>34</sup></xref> Also, skin lesion segmentation has rich and varied feature information with regard to the high lethality caused by malignant melanoma,<xref rid=\"bib35\" ref-type=\"bibr\"><sup>35</sup></xref> which has led many researchers to carry out a series of studies around skin lesion segmentation.<xref rid=\"bib3\" ref-type=\"bibr\"><sup>3</sup></xref><sup>,</sup><xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref><sup>,</sup><xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref></p><p id=\"p0250\">Medical image segmentation algorithms represented by skin lesions have been rapidly developed after the advent of U-Net. In Aghdam et al.,<xref rid=\"bib8\" ref-type=\"bibr\"><sup>8</sup></xref> an inhibition operation of the attention mechanism in cascade operation has been proposed for skin lesion segmentation based on Swin-Unet.<xref rid=\"bib23\" ref-type=\"bibr\"><sup>23</sup></xref> The MHorUNet<xref rid=\"bib3\" ref-type=\"bibr\"><sup>3</sup></xref> model proposes a high-order spatial interaction UNet model for skin lesion segmentation. In Wu et al.,<xref rid=\"bib9\" ref-type=\"bibr\"><sup>9</sup></xref> an adaptive selection of a higher-order UNet model for order interaction has been proposed for skin lesion segmentation. DSU-Net<xref rid=\"bib36\" ref-type=\"bibr\"><sup>36</sup></xref> proposes to utilize convolutional neural networks and transformers to construct a bipolar UNet for skin lesion segmentation, which proceeds from coarse to fine through two stages. However, the joint use of convolutional neural networks and transformers further makes the architecture more bloated. BDFormer<xref rid=\"bib37\" ref-type=\"bibr\"><sup>37</sup></xref> proposes a new boundary-aware bi-decoder transformer, which employs a single-encoder and bi-decoder framework for skin lesion segmentation and expanding boundary segmentation. DermoSegDiff<xref rid=\"bib27\" ref-type=\"bibr\"><sup>27</sup></xref> proposes to introduce denoised diffusion probabilistic models into the skin lesion segmentation task which effectively integrates noise and semantic information, and proposes a loss function adapted to the boundary region to improve lesion boundary recognition. EGE-UNet<xref rid=\"bib10\" ref-type=\"bibr\"><sup>10</sup></xref> proposes a lightweight model for skin lesion segmentation that employs Group multi-axis Hadamard Product Attention module and Group Aggregation Bridge module, which significantly reduces the number of parameters and computation while maintaining high performance.</p><p id=\"p0255\">MALUNet<xref rid=\"bib18\" ref-type=\"bibr\"><sup>18</sup></xref> is another lightweight model for skin lesion segmentation proposed by researchers, which introduces multiple attention mechanism modules to significantly reduce the number of parameters and computational complexity while maintaining high performance in skin lesion segmentation tasks. In addition, there are many algorithms based on U-Net improved for skin lesion segmentation. However, it is common for researchers to add richer modules to the model to improve the accuracy of recognition, but this also significantly increases the parameters and computational complexity of the model. After the emergence of VM, LightM-UNet<xref rid=\"bib19\" ref-type=\"bibr\"><sup>19</sup></xref> was proposed to reduce the number of parameters in the model based on Mamba. LightM-UNet further extracts the deep semantics and tele-relationships by using the residual VM, and achieves better performance with a smaller number of parameters. In addition, U-Mamba<xref rid=\"bib38\" ref-type=\"bibr\"><sup>38</sup></xref> introduced VM into the U-framework, but its having a large number of parameters (173.53M) limits its use in real clinical settings. LiteMamba-Bound<xref rid=\"bib28\" ref-type=\"bibr\"><sup>28</sup></xref> proposed a lightweight model designed for skin lesion segmentation task, which improves skin lesion recognition by introducing a channel-attentive bi-Mamba module and an inverse-attentive boundary module.</p><p id=\"p0260\">In this paper, to solve the current problem of large model parameters and to reveal the key factors affecting Mamba parameters. We propose UltraLight VM-UNet based on Mamba with a parameter of only 0.049M. In addition, we propose a PVM method for processing deep features, named the PVM Layer, based on a detailed theoretical analysis. It is a plug-and-play module that can simply replace the convolutional layers and transformers to significantly reduce the number of model parameters and maintain excellent segmentation performance.</p></sec><sec id=\"sec4.3\"><title>Datasets</title><p id=\"p0265\">To validate that the proposed UltraLight VM-UNet maintains competitive performance with only 0.049M parameters, we conducted experiments on three publicly available dermatologic lesion datasets. The ISIC2017<xref rid=\"bib39\" ref-type=\"bibr\"><sup>39</sup></xref> and ISIC2018<xref rid=\"bib40\" ref-type=\"bibr\"><sup>40</sup></xref> datasets are two large datasets published by the International Skin Imaging Collaboration (ISIC), respectively. The <inline-formula><mml:math id=\"M160\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula><xref rid=\"bib41\" ref-type=\"bibr\"><sup>41</sup></xref> dataset is a small skin lesion dataset, so we used <inline-formula><mml:math id=\"M161\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> as an external validation using training weights from ISIC2017.</p><p id=\"p0270\">For the ISIC2017 dataset we acquired 2,000 images as well as dermatoscope images with segmentation mask labels. Among them, the dataset was randomly divided, 1,250 images were used for model training, 150 images were used for model validation, and 600 images were used for model testing. The initial size of the images is <inline-formula><mml:math id=\"M162\" altimg=\"si73.gif\"><mml:mrow><mml:mn>576</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>767</mml:mn></mml:mrow></mml:math></inline-formula> pixels, and we standardize the size to <inline-formula><mml:math id=\"M163\" altimg=\"si72.gif\"><mml:mrow><mml:mn>256</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> pixels when inputting the model.</p><p id=\"p0275\">For the ISIC2018 dataset we acquired 2,594 images as well as dermatoscope images with segmentation mask labels. Among them, the dataset was randomly divided, 1,815 images were used for model training, 259 images were used for model validation, and 520 images were used for model testing. The initial size of the images is <inline-formula><mml:math id=\"M164\" altimg=\"si74.gif\"><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>016</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>024</mml:mn></mml:mrow></mml:math></inline-formula> pixels, and we standardize the size to <inline-formula><mml:math id=\"M165\" altimg=\"si72.gif\"><mml:mrow><mml:mn>256</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> pixels when inputting the model.</p><p id=\"p0280\">For the <inline-formula><mml:math id=\"M166\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> dataset we acquired 200 images as well as dermatoscope images with segmentation mask labels. All 200 images were used for external validation. The initial size of the images was <inline-formula><mml:math id=\"M167\" altimg=\"si75.gif\"><mml:mrow><mml:mn>768</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>560</mml:mn></mml:mrow></mml:math></inline-formula> pixels and we standardized the size to <inline-formula><mml:math id=\"M168\" altimg=\"si72.gif\"><mml:mrow><mml:mn>256</mml:mn><mml:mo linebreak=\"goodbreak\" linebreakstyle=\"after\">&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> pixels.</p></sec><sec id=\"sec4.4\"><title>Implementation details</title><p id=\"p0285\">The proposed UltraLight VM-UNet is shown in <xref rid=\"fig2\" ref-type=\"fig\">Figure 2</xref>. The experiments were all implemented based on Python 3.8 and Pytorch 1.13.0. A single NVIDIA V100 GPU with 32 GB of memory was used for all experiments. All experiments used the same data augmentation operations to more fairly determine the performance of the model, including horizontal and vertical flips, and random rotation operations. BceDice loss function was used, with AdamW<xref rid=\"bib42\" ref-type=\"bibr\"><sup>42</sup></xref> as the optimizer, a training epoch of 250, a batch size of 8, and a cosine annealing learning rate scheduler with an initial learning rate of 0.001 and a minimum learning rate set to 0.00001.</p></sec><sec id=\"sec4.5\"><title>Evaluation metrics</title><p id=\"p0290\">The DSC, SE, SP, ACC, IoU, and precision (Prec) are the most commonly used evaluation metrics in medical image segmentation. The DSC is used to measure the similarity between the ground truth and the predicted segmentation maps, and it is equivalent to the F1 score in image segmentation tasks. SE is primarily used to measure the percentage of true positives among true positives and false negatives, it is equivalent to Recall. SP is mainly used to measure the percentage of true negatives among true negatives and false positives. ACC is mainly used to measure the proportion of correctly classified samples. IoU is primarily used to measure the degree of overlap between the predicted segmentation region and the true segmentation region. Prec is mainly used to measure the proportion of correctly segmented pixels in the predicted segmentation region.<disp-formula id=\"fd15\"><label>(Equation 15)</label><mml:math id=\"M169\" altimg=\"si76.gif\"><mml:mrow><mml:mtext>DSC</mml:mtext><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mi mathvariant=\"normal\">F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak=\"goodbreak\">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd16\"><label>(Equation 16)</label><mml:math id=\"M170\" altimg=\"si77.gif\"><mml:mrow><mml:mtext>ACC</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd17\"><label>(Equation 17)</label><mml:math id=\"M171\" altimg=\"si78.gif\"><mml:mrow><mml:mtext>SE</mml:mtext><mml:mo linebreak=\"badbreak\">/</mml:mo><mml:mtext>Recall</mml:mtext><mml:mo linebreak=\"goodbreak\">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd18\"><label>(Equation 18)</label><mml:math id=\"M172\" altimg=\"si79.gif\"><mml:mrow><mml:mtext>SP</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd19\"><label>(Equation 19)</label><mml:math id=\"M173\" altimg=\"si80.gif\"><mml:mrow><mml:mtext>IoU</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id=\"fd20\"><label>(Equation 20)</label><mml:math id=\"M174\" altimg=\"si81.gif\"><mml:mrow><mml:mtext>Prec</mml:mtext><mml:mo linebreak=\"badbreak\">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak=\"badbreak\">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where TP denotes true positive, TN denotes true negative, FP denotes false positive, and FN denotes false negative.</p></sec></sec><sec id=\"sec5\"><title>Resource availability</title><sec id=\"sec5.1\"><title>Lead contact</title><p id=\"p0295\">Requests for further information and resources should be directed to and will be fulfilled by the lead contact, Qing Chang (<email>robie0510@hotmail.com</email>).</p></sec><sec id=\"sec5.2\"><title>Materials availability</title><p id=\"p0300\">This study did not generate new materials.</p></sec><sec sec-type=\"data-availability\" id=\"sec5.3\"><title>Data and code availability</title><p id=\"p0305\">Our source code is available at GitHub (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/wurenkai/UltraLight-VM-UNet\" id=\"intref0015\">https://github.com/wurenkai/UltraLight-VM-UNet</ext-link>) and has been archived at Zenodo.<xref rid=\"bib43\" ref-type=\"bibr\"><sup>43</sup></xref> The ISIC2017 dataset is from Codella et al.<xref rid=\"bib39\" ref-type=\"bibr\"><sup>39</sup></xref> and is available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://challenge.isic-archive.com/data/#2017\" id=\"intref0020\">https://challenge.isic-archive.com/data/#2017</ext-link>. The ISIC2018 dataset is from Codella et al.<xref rid=\"bib40\" ref-type=\"bibr\"><sup>40</sup></xref> and is available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://challenge.isic-archive.com/data/#2018\" id=\"intref0025\">https://challenge.isic-archive.com/data/#2018</ext-link>. The <inline-formula><mml:math id=\"M175\" altimg=\"si66.gif\"><mml:mrow><mml:msup><mml:mtext>PH</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> dataset is from Mendon&#231;a et al.<xref rid=\"bib41\" ref-type=\"bibr\"><sup>41</sup></xref> and is available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://drive.google.com/file/d/1AEMJKAiORlrwdDi37dRqbqXi6zLmnU3Q/view?usp=sharing\" id=\"intref0030\">https://drive.google.com/file/d/1AEMJKAiORlrwdDi37dRqbqXi6zLmnU3Q/view?usp=sharing</ext-link>.</p></sec></sec><sec id=\"sec6\"><title>Acknowledgments</title><p id=\"p0310\">This work was supported partly by Medicine-engineering Interdisciplinary Project set up by <funding-source id=\"gs1\"><institution-wrap><institution-id institution-id-type=\"doi\">10.13039/501100008050</institution-id><institution>University of Shanghai for Science and Technology</institution></institution-wrap></funding-source> (<award-id award-type=\"grant\" rid=\"gs1\">2023-LXY-RUIJIN01Z</award-id>).</p></sec><sec id=\"sec7\"><title>Author contributions</title><p id=\"p0315\">Conceptualization, R.W. and Y.L.; methodology, R.W. and Y.L.; writing &#8211; original draft, R.W.; writing &#8211; review &amp; editing, R.W., Y.L., G.N., and P.L.; formal analysis, R.W. and G.M.; validation, Y.L. and P.L.; project administration, Q.C.; funding acquisition, Q.C.; <bold>s</bold>upervision, Q.C.</p></sec><sec sec-type=\"COI-statement\" id=\"sec8\"><title>Declaration of interests</title><p id=\"p0320\">The authors declare no competing interests.</p></sec></body><back><ref-list id=\"cebib0010\"><title>References</title><ref id=\"bib1\"><label>1</label><element-citation publication-type=\"journal\" id=\"sref1\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lim</surname><given-names>S.N.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>J.</given-names></name></person-group><article-title>Hornet: Efficient high-order spatial interactions with recursive gated convolutions</article-title><source>Adv. Neural Inf. Process. Syst.</source><volume>35</volume><year>2022</year><fpage>10353</fpage><lpage>10366</lpage></element-citation></ref><ref id=\"bib2\"><label>2</label><element-citation publication-type=\"journal\" id=\"sref2\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J.</given-names></name></person-group><article-title>Global filter networks for image classification</article-title><source>Adv. Neural Inf. Process. Syst.</source><volume>34</volume><year>2021</year><fpage>980</fpage><lpage>993</lpage></element-citation></ref><ref id=\"bib3\"><label>3</label><element-citation publication-type=\"journal\" id=\"sref3\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name></person-group><article-title>Mhorunet: High-order spatial interaction unet for skin lesion segmentation</article-title><source>Biomed. Signal Process Control</source><volume>88</volume><year>2024</year><object-id pub-id-type=\"publisher-id\">105517</object-id></element-citation></ref><ref id=\"bib4\"><label>4</label><element-citation publication-type=\"book\" id=\"sref1a\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Mao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>C.Y.</given-names></name><name name-style=\"western\"><surname>Feichtenhofer</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Darrell</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>S.</given-names></name></person-group><part-title>A convnet for the 2020s</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Chellappa</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Matas</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Quan</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Shah</surname><given-names>M.</given-names></name></person-group><source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source><year>2022</year><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><fpage>11976</fpage><lpage>11986</lpage></element-citation></ref><ref id=\"bib5\"><label>5</label><element-citation publication-type=\"book\" id=\"sref5\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>G.</given-names></name></person-group><part-title>Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Chellappa</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Matas</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Quan</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Shah</surname><given-names>M.</given-names></name></person-group><source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source><year>2022</year><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><fpage>11963</fpage><lpage>11975</lpage></element-citation></ref><ref id=\"bib6\"><label>6</label><element-citation publication-type=\"book\" id=\"sref6\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H.R.</given-names></name><name name-style=\"western\"><surname>Unetr</surname><given-names>Xu D.</given-names></name></person-group><part-title>Transformers for 3d medical image segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Farrell</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Bowyer</surname><given-names>K.</given-names></name></person-group><source>Proceedings of the IEEE/CVF winter conference on applications of computer vision</source><year>2022</year><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><fpage>574</fpage><lpage>584</lpage></element-citation></ref><ref id=\"bib7\"><label>7</label><element-citation publication-type=\"book\" id=\"sref7\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H.R.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group><part-title>Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Crimi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Bakas</surname><given-names>S.</given-names></name></person-group><source>International MICCAI Brainlesion Workshop</source><year>2021</year><publisher-name>Springer</publisher-name><fpage>272</fpage><lpage>284</lpage></element-citation></ref><ref id=\"bib8\"><label>8</label><element-citation publication-type=\"book\" id=\"sref8\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Aghdam</surname><given-names>E.K.</given-names></name><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Zarvani</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><part-title>Attention swin u-net: Cross-contextual attention mechanism for skin lesion segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Lepore</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Acosta</surname><given-names>O.</given-names></name></person-group><source>2023 IEEE 20th International Symposium on Biomedical Imaging. (ISBI)</source><year>2023</year><publisher-name>IEEE</publisher-name><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id=\"bib9\"><label>9</label><element-citation publication-type=\"journal\" id=\"sref9\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Lv</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>X.</given-names></name></person-group><article-title>Hsh-unet: Hybrid selective high order interactive u-shaped model for automated skin lesion segmentation</article-title><source>Comput. Biol. Med.</source><volume>168</volume><year>2024</year><object-id pub-id-type=\"publisher-id\">107798</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2023.107798</pub-id><pub-id pub-id-type=\"pmid\">38043470</pub-id></element-citation></ref><ref id=\"bib10\"><label>10</label><element-citation publication-type=\"book\" id=\"sref10\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ruan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>Y.</given-names></name></person-group><part-title>Ege-unet: an efficient group enhanced unet for skin lesion segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Greenspan</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Madabhushi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mousavi</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Salcudean</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Duncan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Syeda-Mahmood</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Taylor</surname><given-names>R.</given-names></name></person-group><source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source><year>2023</year><publisher-name>Springer</publisher-name><fpage>481</fpage><lpage>490</lpage></element-citation></ref><ref id=\"bib11\"><label>11</label><element-citation publication-type=\"journal\" id=\"sref11\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Jiao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Vmamba: Visual state space model</article-title><source>Adv. Neural Inf. Process. Syst.</source><volume>37</volume><year>2024</year><fpage>103031</fpage><lpage>103063</lpage></element-citation></ref><ref id=\"bib12\"><label>12</label><element-citation publication-type=\"journal\" id=\"sref12\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Vision mamba: Efficient visual representation learning with bidirectional state space model</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.09417</pub-id></element-citation></ref><ref id=\"bib13\"><label>13</label><element-citation publication-type=\"journal\" id=\"sref13\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name></person-group><article-title>Mamba: Linear-time sequence modeling with selective state spaces</article-title><comment>Preprint at</comment><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2312.00752</pub-id></element-citation></ref><ref id=\"bib14\"><label>14</label><element-citation publication-type=\"book\" id=\"sref14\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sermanet</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Reed</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><part-title>Going deeper with convolutions</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Bischof</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Forsyth</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Schmid</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Sclaroff</surname><given-names>S.</given-names></name></person-group><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2015</year><publisher-name>Institute of Electrical and Electronics Engineers</publisher-name><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id=\"bib15\"><label>15</label><element-citation publication-type=\"book\" id=\"sref15\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ioffe</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><part-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Bach</surname><given-names>F.R.</given-names></name><name name-style=\"western\"><surname>Blei</surname><given-names>D.M.</given-names></name></person-group><source>International conference on machine learning</source><year>2015</year><fpage>448</fpage><lpage>456</lpage><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://JMLR.org\" id=\"intref00810\">JMLR.org</ext-link></element-citation></ref><ref id=\"bib16\"><label>16</label><element-citation publication-type=\"book\" id=\"sref16\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Ioffe</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Shlens</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wojna</surname><given-names>Z.</given-names></name></person-group><part-title>Rethinking the inception architecture for computer vision</part-title><source>Proceedings of the IEEE conference on computer vision and pattern recognition</source><year>2016</year><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id=\"bib17\"><label>17</label><mixed-citation publication-type=\"other\" id=\"sref17\">Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference on Artificial Intelligence vol. 31.</mixed-citation></ref><ref id=\"bib18\"><label>18</label><element-citation publication-type=\"book\" id=\"sref18\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ruan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Malunet</surname><given-names>Fu Y.</given-names></name></person-group><part-title>A multi-attention and light-weight unet for skin lesion segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Aluru</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Narasimhan</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name></person-group><source>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source><year>2022</year><publisher-name>IEEE</publisher-name><fpage>1150</fpage><lpage>1156</lpage></element-citation></ref><ref id=\"bib19\"><label>19</label><element-citation publication-type=\"journal\" id=\"sref19\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liao</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Pan</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>L.</given-names></name></person-group><article-title>Lightm-unet: Mamba assists in lightweight unet for medical image segmentation</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2403.05246</pub-id></element-citation></ref><ref id=\"bib20\"><label>20</label><element-citation publication-type=\"journal\" id=\"sref20\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Elfwing</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Uchibe</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Doya</surname><given-names>K.</given-names></name></person-group><article-title>Sigmoid-weighted linear units for neural network function approximation in reinforcement learning</article-title><source>Neural Netw.</source><volume>107</volume><year>2018</year><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type=\"pmid\">29395652</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.neunet.2017.12.012</pub-id></element-citation></ref><ref id=\"bib21\"><label>21</label><element-citation publication-type=\"book\" id=\"sref21\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group><part-title>U-net: Convolutional networks for biomedical image segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Navab</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Hornegger</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wells</surname><given-names>W.M.</given-names></name><name name-style=\"western\"><surname>Frangi</surname><given-names>A.</given-names></name></person-group><source>Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</source><year>2015</year><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id=\"bib22\"><label>22</label><mixed-citation publication-type=\"other\" id=\"sref22\">Wu, H., Zhong, J., Wang, W., Wen, Z., and Qin, J. (2021). Precise yet efficient semantic calibration and refinement in convnets for real-time polyp segmentation from colonoscopy videos. In Proceedings of the AAAI Conference on Artificial Intelligence vol. 35. pp. 2916&#8211;2924.</mixed-citation></ref><ref id=\"bib23\"><label>23</label><element-citation publication-type=\"book\" id=\"sref23\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M.</given-names></name></person-group><part-title>Swin-unet: Unet-like pure transformer for medical image segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Avidan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Brostow</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Ciss&#233;</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Farinella</surname><given-names>G.M.</given-names></name><name name-style=\"western\"><surname>Hassner</surname><given-names>T.</given-names></name></person-group><source>European conference on computer vision</source><year>2022</year><publisher-name>Springer</publisher-name><fpage>205</fpage><lpage>218</lpage></element-citation></ref><ref id=\"bib24\"><label>24</label><element-citation publication-type=\"book\" id=\"sref24\"><part-title>Devil is in channels: Contrastive single domain generalization for medical image segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Hu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Greenspan</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Madabhushi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mousavi</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Salcudean</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Duncan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Syeda-Mahmood</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Taylor</surname><given-names>R.</given-names></name></person-group><source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source><year>2023</year><publisher-name>Springer</publisher-name><fpage>14</fpage><lpage>23</lpage></element-citation></ref><ref id=\"bib25\"><label>25</label><element-citation publication-type=\"journal\" id=\"sref25\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ruan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>S.</given-names></name></person-group><article-title>Vm-unet: Vision mamba unet for medical image segmentation</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2402.02491</pub-id></element-citation></ref><ref id=\"bib26\"><label>26</label><element-citation publication-type=\"book\" id=\"sref26\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Jin</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Ling</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Tao</surname><given-names>X.</given-names></name></person-group><part-title>Vm-unet-v2: rethinking vision mamba unet for medical image segmentation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Peng</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Cai</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Skums</surname><given-names>P.</given-names></name></person-group><source>International Symposium on Bioinformatics Research and Applications</source><year>2024</year><publisher-name>Springer</publisher-name><fpage>335</fpage><lpage>346</lpage></element-citation></ref><ref id=\"bib27\"><label>27</label><element-citation publication-type=\"book\" id=\"sref27\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bozorgpour</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Sadegheih</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Kazerouni</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><part-title>Dermosegdiff: A boundary-aware segmentation diffusion model for skin lesion delineation</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Rekik</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Adeli</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>S.H.</given-names></name><name name-style=\"western\"><surname>Schnabel</surname><given-names>J.</given-names></name></person-group><source>International workshop on predictive intelligence in medicine</source><year>2023</year><publisher-name>Springer</publisher-name><fpage>146</fpage><lpage>158</lpage></element-citation></ref><ref id=\"bib28\"><label>28</label><element-citation publication-type=\"journal\" id=\"sref28\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ho</surname><given-names>Q.H.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>T.N.</given-names></name><name name-style=\"western\"><surname>Tran</surname><given-names>T.T.</given-names></name><name name-style=\"western\"><surname>Pham</surname><given-names>V.T.</given-names></name></person-group><article-title>Litemamba-bound: A lightweight mamba-based model with boundary-aware and normalized active contour loss for skin lesion segmentation</article-title><source>Methods</source><volume>235</volume><year>2025</year><fpage>10</fpage><lpage>25</lpage><pub-id pub-id-type=\"pmid\">39864606</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.ymeth.2025.01.008</pub-id></element-citation></ref><ref id=\"bib29\"><label>29</label><element-citation publication-type=\"journal\" id=\"sref29\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name></person-group><article-title>H-vmunet: High-order vision mamba unet for medical image segmentation</article-title><source>Neurocomputing</source><volume>624</volume><year>2025</year><object-id pub-id-type=\"publisher-id\">129447</object-id></element-citation></ref><ref id=\"bib30\"><label>30</label><element-citation publication-type=\"journal\" id=\"sref30\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oktay</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Schlemper</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Folgoc</surname><given-names>L.L.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Heinrich</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Misawa</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Mori</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>McDonagh</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Hammerla</surname><given-names>N.Y.</given-names></name><name name-style=\"western\"><surname>Kainz</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Attention u-net: Learning where to look for the pancreas</article-title><comment>Preprint at</comment><source>arXiv</source><year>2018</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1804.03999</pub-id></element-citation></ref><ref id=\"bib31\"><label>31</label><element-citation publication-type=\"journal\" id=\"sref31\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Meta-unet: Multi-scale efficient transformer attention unet for fast and high-accuracy polyp segmentation</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><volume>21</volume><year>2024</year><fpage>4117</fpage><lpage>4128</lpage></element-citation></ref><ref id=\"bib32\"><label>32</label><element-citation publication-type=\"journal\" id=\"sref32\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>Y.</given-names></name></person-group><article-title>A high-order focus interaction model and oral ulcer dataset for oral ulcer segmentation</article-title><source>Sci. Rep.</source><volume>14</volume><year>2024</year><object-id pub-id-type=\"publisher-id\">20085</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-69125-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC11362486</pub-id><pub-id pub-id-type=\"pmid\">39209880</pub-id></element-citation></ref><ref id=\"bib33\"><label>33</label><element-citation publication-type=\"journal\" id=\"sref33\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Antonelli</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Reinke</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Bakas</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Farahani</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Kopp-Schneider</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B.A.</given-names></name><name name-style=\"western\"><surname>Litjens</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Menze</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Summers</surname><given-names>R.M.</given-names></name><etal/></person-group><article-title>The medical segmentation decathlon</article-title><source>Nat. Commun.</source><volume>13</volume><year>2022</year><fpage>4128</fpage><pub-id pub-id-type=\"pmid\">35840566</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41467-022-30695-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC9287542</pub-id></element-citation></ref><ref id=\"bib34\"><label>34</label><element-citation publication-type=\"journal\" id=\"sref34\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xiao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><article-title>Transformers in medical image segmentation: A review</article-title><source>Biomed. Signal Process Control</source><volume>84</volume><year>2023</year><object-id pub-id-type=\"publisher-id\">104791</object-id></element-citation></ref><ref id=\"bib35\"><label>35</label><element-citation publication-type=\"journal\" id=\"sref35\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Siegel</surname><given-names>R.L.</given-names></name><name name-style=\"western\"><surname>Miller</surname><given-names>K.D.</given-names></name><name name-style=\"western\"><surname>Fuchs</surname><given-names>H.E.</given-names></name><name name-style=\"western\"><surname>Jemal</surname><given-names>A.</given-names></name></person-group><article-title>Cancer statistics, 2022</article-title><source>CA Cancer J. Clin.</source><volume>72</volume><year>2022</year><fpage>7</fpage><lpage>33</lpage><pub-id pub-id-type=\"pmid\">35020204</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3322/caac.21708</pub-id></element-citation></ref><ref id=\"bib36\"><label>36</label><element-citation publication-type=\"journal\" id=\"sref36\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhong</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>L.</given-names></name></person-group><article-title>Dsu-net: Dual-stage u-net based on cnn and transformer for skin lesion segmentation</article-title><source>Biomed. Signal Process Control</source><volume>100</volume><year>2025</year><object-id pub-id-type=\"publisher-id\">107090</object-id></element-citation></ref><ref id=\"bib37\"><label>37</label><element-citation publication-type=\"journal\" id=\"sref37\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ji</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>X.</given-names></name></person-group><article-title>Bdformer: Boundary-aware dual-decoder transformer for skin lesion segmentation</article-title><source>Artif. Intell. Med.</source><volume>162</volume><year>2025</year><object-id pub-id-type=\"publisher-id\">103079</object-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.artmed.2025.103079</pub-id><pub-id pub-id-type=\"pmid\">39983372</pub-id></element-citation></ref><ref id=\"bib38\"><label>38</label><element-citation publication-type=\"journal\" id=\"sref38\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>U-mamba: Enhancing long-range dependency for biomedical image segmentation</article-title><comment>Preprint at</comment><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.04722</pub-id></element-citation></ref><ref id=\"bib39\"><label>39</label><element-citation publication-type=\"book\" id=\"sref39\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Codella</surname><given-names>N.C.</given-names></name><name name-style=\"western\"><surname>Gutman</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Celebi</surname><given-names>M.E.</given-names></name><name name-style=\"western\"><surname>Helba</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Marchetti</surname><given-names>M.A.</given-names></name><name name-style=\"western\"><surname>Dusza</surname><given-names>S.W.</given-names></name><name name-style=\"western\"><surname>Kalloo</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Liopyris</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Mishra</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Kittler</surname><given-names>H.</given-names></name><etal/></person-group><part-title>Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Amini</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Acton</surname><given-names>S.</given-names></name></person-group><source>2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)</source><year>2018</year><publisher-name>IEEE</publisher-name><fpage>168</fpage><lpage>172</lpage></element-citation></ref><ref id=\"bib40\"><label>40</label><element-citation publication-type=\"journal\" id=\"sref40\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Codella</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Rotemberg</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Tschandl</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Celebi</surname><given-names>M.E.</given-names></name><name name-style=\"western\"><surname>Dusza</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gutman</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Helba</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Kalloo</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Liopyris</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Marchetti</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</article-title><comment>Preprint at</comment><source>arXiv</source><year>2019</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1902.03368</pub-id></element-citation></ref><ref id=\"bib41\"><label>41</label><element-citation publication-type=\"book\" id=\"sref41\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mendon&#231;a</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ferreira</surname><given-names>P.M.</given-names></name><name name-style=\"western\"><surname>Marques</surname><given-names>J.S.</given-names></name><name name-style=\"western\"><surname>Marcal</surname><given-names>A.R.</given-names></name><name name-style=\"western\"><surname>Rozeira</surname><given-names>J.</given-names></name></person-group><part-title>Ph 2-a dermoscopic image database for research and benchmarking</part-title><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Sunagawa</surname><given-names>K.</given-names></name></person-group><source>2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC)</source><year>2013</year><publisher-name>IEEE</publisher-name><fpage>5437</fpage><lpage>5440</lpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/EMBC.2013.6610779</pub-id><pub-id pub-id-type=\"pmid\">24110966</pub-id></element-citation></ref><ref id=\"bib42\"><label>42</label><element-citation publication-type=\"journal\" id=\"sref42\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Loshchilov</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><comment>Preprint at</comment><source>arXiv</source><year>2017</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1711.05101</pub-id></element-citation></ref><ref id=\"bib43\"><label>43</label><mixed-citation publication-type=\"other\" id=\"sref43\">Wu, R., Liu, Y., Ning, G., Liang, P., and Chang, Q. (2025). Source code for the paper &#8220;UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation&#8221;. Zenodo. <pub-id pub-id-type=\"doi\">10.5281/zenodo.15354520</pub-id>.</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Patterns (N Y) Patterns (N Y) 3955 patterns Patterns 2666-3899 Elsevier PMC12664954 PMC12664954.1 12664954 12664954 41328156 10.1016/j.patter.2025.101298 S2666-3899(25)00146-1 101298 1 Article UltraLight VM-UNet: Parallel Vision Mamba significantly reduces parameters for skin lesion segmentation Wu Renkai 1 2 3 Liu Yinghao 4 Ning Guochen 5 Liang Pengchen liangpengchen@shu.edu.cn 2 &#8727; Chang Qing robie0510@hotmail.com 1 2 3 6 &#8727;&#8727; 1 Department of Geriatrics, Medical Center on Aging of Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai 200025, China 2 School of Microelectronics, Shanghai University, Shanghai 201800, China 3 The Innovation Center, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai 200025, China 4 School of Health Science and Engineering, University of Shanghai for Science and Technology, Shanghai 200093, China 5 School of Clinical Medicine, Tsinghua University, Beijing 100084, China &#8727; Corresponding author liangpengchen@shu.edu.cn &#8727;&#8727; Corresponding author robie0510@hotmail.com 6 Lead contact 14 11 2025 26 6 2025 6 11 501518 101298 24 2 2025 8 4 2025 30 5 2025 26 06 2025 01 12 2025 02 12 2025 &#169; 2025 The Author(s) 2025 https://creativecommons.org/licenses/by/4.0/ This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). Summary Traditionally, to improve the segmentation performance of models, most approaches prefer to use more complex modules. This is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models, represented by Mamba, have become a strong competitor to traditional convolutional neural networks and transformers. In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named the PVM Layer, which achieves competitive performance with the lowest computational complexity while keeping the overall number of processing channels constant. We conducted segmentation experiments on three public datasets of skin lesions and showed that UltraLight VM-UNet exhibits competitive performance with only 0.049M parameters and 0.060 GFLOPs. Graphical abstract Highlights &#8226; Mamba was analyzed in depth for its parameter key influences &#8226; A new framework and theoretical analysis for lightweight model of medical scenarios &#8226; Parallel Vision Mamba (or Mamba) is a winner for lightweight models &#8226; Provides an UltraLight Vision Mamba UNet for mobile skin lesion segmentation The bigger picture Previously, medical image segmentation models were often constrained by computational resources and memory, limiting their application in mobile medical devices or resource-constrained environments. In contrast, the UltraLight VM-UNet model proposed in this study, with its low parameter count and computational complexity, is able to reduce the requirements on device resources without sacrificing performance. This means that it can conveniently assist doctors in diagnosing diseases in more scenarios, such as on medical devices in remote areas or in emergency medical rescue for fast processing of medical image data. In the long run, the ambition of this research is to promote the popularization and efficiency of medical image segmentation technology. It is hoped that accurate medical image diagnosis will no longer be limited to high-end equipment in large hospitals, but can be extended to all kinds of primary healthcare organizations, and even portable medical equipment, so that more patients can be diagnosed in a timely and accurate manner. Moreover, the lightweight module proposed in this study is plug-and-play, which can simply replace the redundant modules in other technology models, reducing the requirements on equipment resources while maintaining excellent performance. This will promote the rapid development of lightweight medical image processing, improve the overall quality and popularity of medical services, and bring positive social impacts. In the field of computer-aided medical diagnosis, medical image segmentation techniques (e.g., skin lesion segmentation) have been a key research hotspot. In this paper, the authors propose the UltraLight VM-UNet model, which operates efficiently in resource-constrained environments while guaranteeing high segmentation performance through an innovative PVM Layer. These results have implications for enhancing the diagnostic capabilities of mobile medical devices and promoting the widespread application of medical image segmentation technology. Keywords skin lesion segmentation lightweight model Mamba pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Published: June 26, 2025 Introduction With the rapid advancement of computer technology and hardware capabilities, computer-aided diagnosis has seen widespread adoption in the medical domain, with medical image segmentation serving as a critical component. Modern segmentation methods are predominantly powered by deep learning networks, particularly those based on convolutional neural networks and transformers. Convolutional architectures excel at extracting local features, yet they struggle to capture long-range dependencies effectively. 1 , 2 , 3 To address this limitation, previous works 4 , 5 have explored the use of large convolutional kernels, aiming to extend the receptive field and thus improve the modeling of distant spatial relationships. On the other hand, transformer-based architectures have recently garnered significant attention in medical image analysis. 6 , 7 Their self-attention mechanism inherently facilitates global context modeling by operating over sequences of image patches. However, this advantage comes at the cost of increased computational complexity, as the self-attention operation scales quadratically with respect to the input image size. Moreover, to enhance the accuracy of computer-aided diagnosis, many existing approaches tend to increase the number of model parameters to boost predictive performance. 8 , 9 However, such strategies are often impractical in real-world clinical settings, where computational power and memory resources are inherently limited. In the context of mobile health applications, models must meet strict requirements for low parameter counts and minimal memory consumption. 10 Consequently, there is a pressing need for algorithmic models that can deliver strong performance while maintaining low computational complexity&#8212;making them well suited for deployment on future mobile medical devices. Recently, state-space models (SSMs) have shown linear complexity in terms of input size and memory occupation, 11 , 12 which makes them key to lightweight model foundations. In addition, SSMs excel at capturing remote dependencies, which can critically address the problem of convolution for extracting information over long distances. In Gu and Dao, 13 time-varying parameters were introduced into an SSM to obtain Mamba, and it was demonstrated that Mamba is able to process textual information with lower parameters than transformers. On the vision side, the introduction of Vision Mamba (VM) 12 has once again furthered people&#8217;s understanding of Mamba, which saves 86.8 % of memory when reasoning about images of 1 , 248 &#215; 1 , 248 size without the need for an attentional mechanism. With the outstanding work of the researchers mentioned above, we are more confident that Mamba will occupy a major position in the future as a basic building block for lightweight models. In this paper, we propose a lightweight model based on VM. We deeply explore the critical memory footprint of Mamba and the performance trade-offs, and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet). The proposed UltraLight VM-UNet represents an ultra-lightweight VM model with only 0.049M parameters and 0.060 GFLOPs, demonstrating highly competitive performance across three skin lesion segmentation tasks ( Figure 1 ). Specifically, we delve into the keys affecting the computational complexity in Mamba, and conclude that the number of channels is a key factor in the explosive memory occupation for Mamba computation. We build on this finding of ours by proposing a parallel Vision Mamba (PVM) approach for processing deep features, named the PVM Layer, which simultaneously keeps the overall processing channel count constant. The proposed PVM Layer achieves excellent performance with surprisingly low parameters. In addition, the deep feature extraction of the proposed UltraLight VM-UNet that we implement using only the PVM Layer containing Mamba, as shown in Figure 2 . In the methods section, we present the details of the proposed UltraLight VM-UNet as well as the key factors of the parameter effects in Mamba and the performance balancing approach. Figure 1 Visualization of the comparison results for the ISIC2017 dataset The X axis corresponds to parameters and GFLOPs, the fewer the better. The Y axis corresponds to segmentation performance (DSC), the higher the better. Figure 2 The proposed UltraLight Vision Mamba UNet (UltraLight VM-UNet) model architecture (A) The main part of the UltraLight VM-UNet. (B) The skip-connection part of the UltraLight VM-UNet. Although similar parallel connection of modules has been mentioned in previous studies, 14 , 15 , 16 , 17 the impact of using parallel connection in Mamba is still unknown. Does parallel connection lead to a significant performance degradation of Mamba when utilizing the SSM selection mechanism? This is because parallel connections lead to a reduction in the number of feature channels learned per SSM. In this paper, we give the answer in detail. PVM or Mamba not only remain competitive in terms of performance, but achieve a significant reduction in parameters and computational complexity. Our contributions and findings can be summarized as follows: (1) An UltraLight Vision Mamba UNet (UltraLight VM-UNet) is proposed for skin lesion segmentation with parameters of only 0.049M and GFLOPs of only 0.060. (2) A PVM method for processing deep features, named the PVM Layer, is proposed, which achieves excellent performance with the lowest computational complexity while keeping the overall number of processing channels constant. This can be generalized to the parallel connection of any Mamba variant. (3) We provide an in-depth analysis of the key factors influencing the parameters of Mamba, and provide a theoretical basis for Mamba to become a mainstream module for lightweight models in the future. (4) The proposed UltraLight VM-UNet parameters are 99.82 % lower than the traditional pure Vision Mamba UNet model (VM-UNet) and 87.84 % lower than the parameters of the current lightest Vision Mamba UNet model (LightM-UNet). In addition, the UltraLight VM-UNet maintains strong performance competitiveness in all three publicly available skin lesion segmentation datasets. Results Architecture overview The proposed UltraLight VM-UNet is shown in Figure 2 . UltraLight VM-UNet has a total of 6 layers in a structure consisting of a U-shaped structure (encoder, decoder, and skip-connection path). The number of channels in the 6-layer structure is set to [8,16,24,32,48,64]. The extraction of shallow features in the first 3 layers is composed using a convolution module (Conv Block), where each layer includes a standard convolution with a 3 &#215; 3 kernel and a maximum pooling operation. The deeper features from layer 4 to layer 6 are our core part, where each layer consists of our proposed PVM Layer. The decoder part maintains the same setup as the encoder. The skip-connection path utilize the Channel Attention Bridge (CAB) module and the Spatial Attention Bridge (SAB) module for multilevel and multiscale information fusion. 18 Mamba parameter impact analysis The VM for PVM Layer is mainly composed using Mamba combined with residual connections and adjustment factors ( Figure 3 A), which allows traditional Mamba to improve the capture of remote spatial relations without introducing additional parameters and computational complexity. 19 This has a better improvement in the performance of Mamba in visual tasks while keeping the parameter and computational complexity low. Figure 3 The proposed PVM Layer architecture (A) The main part of the PVM Layer. VM is composed by Mamba combined with residual connection and adjustment factor. (B) Mamba composition structure. Among SSM-based Mamba, the number of channels, the size of the SSM state dimension, the size of the internal 1D convolutional kernel, the projection dilation multiplier, and the rank of the step size all affect the parameters. And in this, the impact of the channel number is explosive, and its main influence is from the following multiple directions: First, d _ i n n e r of the Mamba internal extended projection channel is determined by the product of the projection expansion multiplier and the number of input channels. This can be specifically expressed by the following equation: (Equation 1) d _ i n n e r = e x p a n d &#8727; d _ m o d e l where d _ i n n e r is the internal expansion projection channel, e x p a n d is the projection expansion multiplier (fixed at 2 by default), and d _ m o d e l is the number of input channels. We can conclude that d _ i n n e r will double up as the number of channels ( d _ m o d e l ) per layer in the model increases. Second, the parameters of the input projection layer (the same input linear layer is used for both branches) and the output projection layer within Mamba will be directly related to the number of input channels. The input projection layer and output projection layer operate as follows: (Equation 2) i n _ p r o j : n n . L i n e a r ( d _ m o d e l , d _ i n n e r &#8727; 2 , b i a s = F a l s e ) (Equation 3) o u t _ p r o j : n n . L i n e a r ( d _ i n n e r , d _ m o d e l , b i a s = F a l s e ) where the input projection ( i n _ p r o j ) layer parameter is d _ m o d e l &#8727; d _ i n n e r &#8727; 2 , and the output projection layer ( o u t _ p r o j ) parameter is d _ i n n e r &#8727; d _ m o d e l . We can conclude that the number of input channels, d _ m o d e l , is the key element controlling the parameter, where the internal extended projection channel, d _ i n n e r , is also controlled by d _ m o d e l . Further, the intermediate linear projection layers of SSM are also key to the influence of the parameters. The details are as follows: (Equation 4) x _ p r o j = n n . L i n e a r ( d _ i n n e r , ( d t _ r a n k + d _ s t a t e &#8727; 2 ) , b i a s = F a l s e ) (Equation 5) d t _ p r o j = n n . L i n e a r ( d t _ r a n k , d _ i n n e r , b i a s = T r u e ) where d t _ r a n k is the rank of the step ( d t _ r a n k = d _ m o d e l / 16 ) , d _ s t a t e is the size of the state dimension (fixed to 16), where the parameters can be derived as d _ i n n e r &#8727; ( d t _ r a n k + d _ s t a t e &#8727; 2 ) . d t _ p r o j is a linear projection layer for step size, with parameters ( d t _ r a n k &#8727; d _ i n n e r ) + d _ i n n e r , mainly used for linear projection for step size ( d t ) . So, we can conclude that all parameters are still mainly controlled by the number of input channels d _ m o d e l . In addition, the internal convolution ( n n . C o n v 1 d ( d _ i n n e r , d _ i n n e r , d _ c o n v , b i a s = T r u e ) ) also provides parametric influence with d _ i n n e r &#8727; d _ i n n e r &#8727; d _ c o n v + d _ i n n e r . In this paper, d _ c o n v is fixed to 4, so the convolution provides a parameter of 4 &#8727; d _ i n n e r 2 + d _ i n n e r , which is also controlled by the d _ m o d e l . Also, the logarithmic form parameters ( A _ l o g s ) of the transfer matrix A in the SSM module are an important influencing element. A _ l o g s is a parameter matrix of the shape ( d _ i n n e r , d _ s t a t e ), so its parameters can be derived as d _ i n n e r &#8727; d _ s t a t e . In addition, the trainable vector parameter ( D ) of the process computed within the SSM contains the d _ i n n e r parameter, which is used to selectively integrate the SSM state outputs with the original input signals, thus enhancing the model&#8217;s expressiveness and training stability. In summary, assuming that the original channel number is 1,024, keeping other parameters unchanged, and when the channel number is reduced to one-fourth of the original (channel number 256), the original total parameters can be calculated from the above parameter formula to get the original total parameters reduced from 23,435,264 to 1,484,288. The parameter explosion is reduced by 93.7 % , which further confirms that the number of channels has a very critical impact on Mamba parameters. Building upon the in-depth analysis of the key factors influencing parameter efficiency in Mamba, we propose a PVM Layer for feature processing. This design achieves outstanding performance with minimal calculated load, while maintaining a constant total number of processing channels. The architectural details and implementation of the PVM Layer are presented in the following section. Mamba variant (SS2D) parameter impact analysis The 2D Selective Scan (SS2D) have been developed based on SSM which are more suitable for visual tasks, and SS2D are usually embedded in a Visual State Space (VSS) Block. 11 The VSS Block consists of two main branches, the first one mainly consists of a linear layer and SiLU activation function. 20 The second branch is mainly composed of linear layers, convolution, SiLU activation function, SS2D, and LayerNorm. Finally, the two branches merge the outputs by element-by-element multiplication. The components of SS2D include scan expansion operation, S6 block feature extraction, and scan merge operation. The sequence is first expanded in four directions from top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right by scan expansion operation. Subsequently, the extracted features are passed through the S6 block 13 for deep feature refinement. A scan merge operation is then employed to reconstruct the spatial resolution to match that of the original input image. In the VSS Block, the number of input channels, the size of the state dimension of the S6 block, the size of the internal convolution kernel, the projection dilation multiplier, and the rank of the projection matrix all affect the parameters. Among them, the influence of the number of input channels is explosive, and its influence mainly comes from the following aspects. First, the d _ i n n e r of the VSS Block internal extended projection channel is determined by the product of the projection expansion multiplier and the number of input channels. This can be specifically expressed by the following equation: (Equation 6) d _ i n n e r = e x p a n d &#8727; d _ m o d e l where d _ i n n e r is the internal expansion projection channel, e x p a n d is the projection expansion multiplier (fixed at 2 by default), and d _ m o d e l is the number of input channels. We can see that d _ i n n e r will rise exponentially as the number of channels per layer in the model increases dramatically. Second, the parameters of the input projection layer (the same input linear layer is used for both branches) and output projection layer within VSS Block will be directly related to the number of input channels. The input projection layer and output projection layer operate as follows: (Equation 7) i n _ p r o j : n n . L i n e a r ( d _ m o d e l , d _ i n n e r &#8727; 2 , b i a s = F a l s e ) (Equation 8) o u t _ p r o j : n n . L i n e a r ( d _ i n n e r , d _ m o d e l , b i a s = F a l s e ) where the input projection ( i n _ p r o j ) layer parameter is ( d _ m o d e l &#8727; d _ i n n e r &#8727; 2 ) , and the output projection layer ( o u t _ p r o j ) parameter is ( d _ i n n e r &#8727; d _ m o d e l ) . In addition, the output section has a layer normalization operation (LayerNorm) with parameter ( d _ i n n e r + d _ i n n e r ) . We can see that the number of input channels, d _ m o d e l , is the key element controlling the parameter, where the internal extended projection channel, d _ i n n e r , is also controlled by d _ m o d e l . Further, the linear projection layers in the S6 block of SS2D are also key to the parameter effects. Each linear projection layer is specified as follows: (Equation 9) x _ p r o j = n n . L i n e a r ( d _ i n n e r , ( d t _ r a n k + d _ s t a t e &#8727; 2 ) , b i a s = F a l s e ) (Equation 10) d t _ p r o j = n n . L i n e a r ( d t _ r a n k , d _ i n n e r , b i a s = T r u e ) where d t _ r a n k is the rank of the projection matrix ( d t _ r a n k = d _ m o d e l / 16 ) , d _ s t a t e is the size of the S6 block state dimension (fixed to 16), and the parameters for each linear projection layer are ( d _ i n n e r &#8727; ( d t _ r a n k + d _ s t a t e &#8727; 2 ) ) . However, there are 4 linear projection layers in total, so the total parameters are 4 &#8727; ( d _ i n n e r &#8727; ( d t _ r a n k + d _ s t a t e &#8727; 2 ) ) . In addition, d t _ p r o j is a linear projection layer for step size with parameter d t _ r a n k &#8727; d _ i n n e r + d _ i n n e r , which is mainly used for linear projection for step size ( d t ) . d t _ p r o j also has 4 layers, with a total parameter of 4 &#8727; ( d t _ r a n k &#8727; d _ i n n e r + d _ i n n e r ) . So, from the above, we can know that all parameters are still mainly controlled by the number of input channels d _ m o d e l . In addition, the internal convolution ( n n . C o n v 2 d ( d _ i n n e r , d _ i n n e r , d _ c o n v , b i a s = T r u e ) ) also provides parametric influence with d _ i n n e r &#8727; d _ i n n e r &#8727; d _ c o n v &#8727; d _ c o n v + d _ i n n e r . In this paper, d _ c o n v is fixed to 3, so the convolution provides a parameter of 3 2 &#8727; d _ i n n e r 2 + d _ i n n e r , which is also controlled by the d _ m o d e l . Also, the A _ l o g s of the parameter matrix controlling the attention weights of the different states of the S6 block in the SS2D module is an important influencing element. A _ l o g s is a parameter matrix of the shape ( K &#8727; d _ i n n e r , d _ s t a t e ) , and K is a hyperparameter that is usually fixed to 4. Therefore, the parameter A _ l o g s can be derived as d _ i n n e r &#8727; d _ s t a t e &#8727; 4 . In addition, the trainable vector parameter ( D s ) of the process computed within the SS2D contains the 4 &#8727; d _ i n n e r parameter, which is used to selectively integrate the SS2D state outputs with the original input signals, thereby enhancing the model&#8217;s expressiveness and training stability. In summary, assuming that the original number of input channels is 1,024, keeping the other parameters unchanged, and reducing the number of channels to a quarter of the original (the number of input channels becomes 256), the original total parameters can be calculated by the above parameter formulae from 45,504,512 to 2,921,984. The parameter explosion reduces the number of channels by 93.6 % , which further confirms that the number of input channels has a very critical impact on the VSS Block parameters. PVM Layer As analyzed in the previous subsection, the number of input channels has an explosive effect on the parameters of Mamba. As shown in Figure 3 A, we propose the PVM Layer for processing deep features. Specifically, a feature X with channel number C first passes through a LayerNorm layer and then is divided into Y 1 C / 4 , Y 2 C / 4 , Y 3 C / 4 , and Y 4 C / 4 features each with channel number C / 4 . After that, each of the features is then fed into Mamba, and then the output is subjected to residual concatenation and adjustment factor for optimizing the remote spatial information acquisition capability. 19 Finally, the four features are combined into the feature X o u t with channel number C by concat operation, and then output by LayerNorm and Projection operation, respectively. The specific operations can be expressed by the following equations: (Equation 11) Y 1 C / 4 , Y 2 C / 4 , Y 3 C / 4 , Y 4 C / 4 = S p [ LN ( X i n C ) ] (Equation 12) V M _ Y i C / 4 = M a m b a ( Y i C / 4 ) + &#952; &#183; Y i C / 4 i = 1 , 2 , 3 , 4 (Equation 13) X o u t = C a t ( V M &#8722; Y 1 C / 4 , V M &#8722; Y 2 C / 4 , V M &#8722; Y 3 C / 4 , V M &#8722; Y 4 C / 4 ) (Equation 14) O u t = P r o [ LN ( X o u t ) ] where LN is the LayerNorm, Sp is the Split operation, M a m b a is the Mamba operation, &#952; is the adjustment factor for the residual connection, C a t is the concat operation, and P r o is the Projection operation. From Equation 12, we used PVM processing features, while ensuring that the total number of channels processed remains constant, maintaining high accuracy while maximizing parameter reduction. As shown in Figure 3 A for methods A and B, again assuming a channel count size of 1,024, each VM in method A reduces the parameters by 93.7 % , and it contains 4 such operations; when summed up, the comparison method B parameters are reduced by 74.8 % overall. Through our proposed PVM operation, the parameter reduction is maximized while maintaining strong performance competitiveness. Quantitative and qualitative analysis To validate the performance of the proposed UltraLight VM-UNet under the 0.049M parameter, we conducted comparison experiments with several state-of-the-art lightweight and classical medical image segmentation models. Specifically, they include U-Net, 21 SCR-Net, 22 Swin-Unet, 23 ATTENTION SWIN U-NET, 8 C 2 SDG, 24 VM-UNet, 25 VM-UNet v2, 26 MALUNet, 18 LightM-UNet, 19 EGE-UNet, 10 DermoSegDiff, 27 and LiteMamba-Bound. 28 Table 1 show the experimental results on the ISIC2017, ISIC2018, and PH 2 datasets, respectively. As shown in the table, the parameters of our model are 99.82 % lower than those of the traditional pure VM-UNet and 87.84 % lower than those of the current LightM-UNet. In addition, the GFLOPs of our model are 98.54 % lower than VM-UNet and 84.65 % lower than LightM-UNet. With such a large reduction in parameters and GFLOPs, the performance of our model still maintains excellent and highly competitive performance. In addition, MALUNet is a lightweight model proposed based on convolution, and although it has lower parameters and GFOLPs than VM-UNet and LightM-UNet, the parameters and GFOLPs of our model are still 72.0 % and 27.71 % lower than them, respectively. In particular, the performance of MALUNet, the proposed lightweight model based on convolution, is much lower than that of the Mamba-based model, which reflects that it is difficult for the convolution-based lightweight model to balance the relationship between performance and computational complexity. In addition, for a comprehensive analysis of the results, the proposed UltraLight VM-UNet also exhibits slightly lower values for some metrics compared with other models in the ISIC2017 and ISIC2018 datasets. For example, the specificity (SP) of UltraLight VM-UNet is lower than that of other comparison models in the ISIC2017 and ISIC2018 datasets. This is due to the fact that the proposed UltraLight VM-UNet uses multiple PVM focusing on the target region to maintain the ultra-lightweight architecture, but this also affects the model&#8217;s ability in recognizing the background, which leads to a slightly lower SP index. However, in the vast majority of metrics, especially in the ability to recognize lesion targets, the key dice similarity coefficient (DSC)/F1 metrics and intersection over union (IoU) metrics, etc., the proposed UltraLight VM-UNet is leading. Further, Figure 4 shows the comparison between the proposed method and the comparison method in terms of inference speed and memory usage. As can be seen from the figure, the inference speed of the proposed UltraLight VM-UNet maintains an efficient value, which is faster than any other equivalent Mamba-based lightweight model. In terms of memory usage, the proposed UltraLight VM-UNet has the lowest memory usage (batch size uniformly 8) of all the compared models, which again demonstrates the excellent trade-off between lightweight and performance of UltraLight VM-UNet. Table 1 Experimental comparisons of the proposed UltraLight VM-UNet with the best lightweight and classical models Model Parameters (millions) GFLOPs DSC/F1 SE/Recall SP ACC IoU Prec Comparison experiments on the ISIC2017 dataset U-Net 21 2.009 3.224 0.8989 0.8793 0.9812 0.9613 0.8165 0.9196 SCR-Net 22 0.801 1.567 0.8898 0.8497 0.9853 0.9588 0.8015 0.9340 Swin-Unet 23 27.176 7.724 0.8670 0.8427 0.9754 0.9494 0.7652 0.8928 ATTENTION SWIN U-NET 8 46.910 14.181 0.8859 0.8492 0.9847 0.9591 0.7998 0.9444 C 2 SDG 24 22.001 7.972 0.8938 0.8859 0.9765 0.9588 0.8081 0.9019 VM-UNet 25 27.427 4.112 0.9070 0.8837 0.9842 0.9645 0.8298 0.9302 VM-UNet v2 26 22.771 4.400 0.9045 0.8768 0.9849 0.9637 0.8256 0.9168 MALUNet 18 0.175 0.083 0.8896 0.8824 0.9762 0.9583 0.8008 0.9295 LightM-UNet 19 0.403 0.391 0.9080 0.8839 0.9846 0.9649 a 0.8303 0.9321 EGE-UNet 10 0.053 0.072 0.9073 0.8931 0.9816 0.9642 0.8302 0.9219 DermoSegDiff 27 45.112 38.636 0.8789 0.8435 0.9815 0.9545 0.7841 0.9174 LiteMamba-Bound 28 0.957 1.591 0.9054 0.8743 0.9861 a 0.9643 0.8272 0.9374 UltraLight VM-UNet (Our) 0.049 a 0.060 a 0.9091 a 0.9053 a 0.9790 0.9646 0.8334 a 0.9481 a Comparison experiments on the ISIC2018 dataset U-Net 21 2.009 3.224 0.8851 0.8735 0.9744 0.9539 0.7938 0.8970 SCR-Net 22 0.801 1.567 0.8886 0.8892 0.9714 0.9547 0.7995 0.8880 Swin-Unet 23 27.176 7.724 0.8342 0.8142 0.9648 0.9343 0.7155 0.8551 ATTENTION SWIN U-NET 8 46.910 14.181 0.8540 0.8057 0.9826 a 0.9480 0.7683 0.9183 C 2 SDG 24 22.001 7.972 0.8806 0.8970 0.9643 0.9506 0.7867 0.8648 VM-UNet 25 27.427 4.112 0.8891 0.8809 0.9743 0.9554 0.8004 0.8966 VM-UNet v2 26 22.771 4.400 0.8902 0.8959 0.9702 0.9551 0.8020 0.8672 MALUNet 18 0.175 0.083 0.8931 0.8890 0.9725 0.9548 0.8028 0.8827 LightM-UNet 19 0.403 0.391 0.8898 0.8829 0.9765 0.9555 0.8013 0.8902 EGE-UNet 10 0.053 0.072 0.8819 0.9009 a 0.9638 0.9510 0.7887 0.8637 DermoSegDiff 27 45.112 38.636 0.8672 0.8344 0.9693 0.9421 0.7697 0.8760 LiteMamba-Bound 28 0.957 1.591 0.8929 0.8911 0.9714 0.9546 0.8019 0.8894 UltraLight VM-UNet (Our) 0.049 a 0.060 a 0.8940 a 0.8680 0.9781 0.9558 a 0.8056 a 0.9197 a Comparison experiments on the PH 2 dataset U-Net 21 2.009 3.224 0.9060 0.9255 0.9440 0.9381 0.8282 0.8874 SCR-Net 22 0.801 1.567 0.8989 0.9114 0.9446 0.9339 0.8164 0.8868 Swin-Unet 23 27.176 7.724 0.8631 0.8613 0.9359 0.9119 0.7591 0.8649 ATTENTION SWIN U-NET 8 46.910 14.181 0.8850 0.8886 0.9363 0.9213 0.7990 0.8838 C 2 SDG 24 22.001 7.972 0.9030 0.9137 0.9476 0.9367 0.8231 0.8925 VM-UNet 25 27.427 4.112 0.9033 0.9131 0.9483 0.9369 0.8237 0.8948 VM-UNet v2 26 22.771 4.400 0.9050 0.9160 0.9485 0.9380 0.8265 0.8797 MALUNet 18 0.175 0.083 0.8865 0.8922 0.9425 0.9263 0.8010 0.8993 LightM-UNet 19 0.403 0.391 0.9156 0.9129 0.9613 a 0.9457 0.8443 0.9103 EGE-UNet 10 0.053 0.072 0.9086 0.9198 0.9502 0.9404 0.8325 0.8978 DermoSegDiff 27 45.112 38.636 0.8928 0.8779 0.9577 0.9320 0.8064 0.9082 LiteMamba-Bound 28 0.957 1.591 0.9217 0.9341 0.9558 0.9488 0.8548 0.9097 UltraLight VM-UNet (Our) 0.049 a 0.060 a 0.9265 a 0.9345 a 0.9606 0.9521 a 0.8631 a 0.9187 a a These values represent the best performance. Figure 4 Comparison of inference time and memory usage between the proposed and compared methods (A) Comparison of inference time for different models. (B) Comparison of memory usage for different models. Numbers 1&#8211;13 refer to U-Net, SCR-Net, Swin-Unet, ATTENTION SWIN U-Net, C 2 SDG, VM-UNet, VM-UNet v2, MALUNet, LightM-UNet, EGE-UNet, DermoSegDiff, LiteMamba-Bound, and UltraLight VM-UNet (Our). To more directly represent the competitive nature of UltraLight VM-UNet in terms of segmentation performance, we visualized the segmentation results ( Figure 5 ). In addition, we have also visualized the segmentation results of several state-of-the-art lightweight and classical medical image segmentation models. From the visualizations, it can be concluded that the segmentation results of UltraLight VM-UNet have smooth, clear, and more accurate boundaries. This shows that UltraLight VM-UNet not only outperforms the rest of the current lightweight models in terms of parameters and computational complexity, but also remains competitive in terms of performance. Figure 5 Visualization of segmentation graphs for comparison experiments of three publicly available skin lesion segmentation datasets The red contour line is the true value and the blue contour line is the predicted value. Discussion Ablation experiments VM with different levels of parallelism To verify the validity of the proposed method of VM with different parallelism, we performed a series of ablation experiments. As shown in Figure 6 , we performed three different settings. Setting 1 is a conventional connection of VM, setting 2 is a connection using a parallel connection of two VMs with half the number of channels, and setting 3 is a connection using parallel connection of four VMs each with C / 4 the number of channels. By analyzing the parameters of Mamba in \"mamba parameter impact analysis,\" assuming that the parameter of this module is x for setting 1 of the traditional VM connection method, setting 2 can be calculated with a parameter of 0.502 x and setting 3 with a parameter of 0.252 x . Table 2 shows the results of this ablation experiment, and it should be noted that the parameters here refer to the parameters of the overall model (which contains the Conv Block and the skip-connection part). The parameters of setting 2 and setting 3 are 51.47 % and 36.03 % , respectively, of the parameters of setting 1 for the traditional VM connection method, while the GFLOPs as a whole do not change much. In terms of performance, the lowest parameter of setting 3 still maintains better segmentation performance. This is due to the focus on the target area by multiple PVM, but this also affects the ability of the model to recognize the background, which results in the SP index for setting 3 being the lowest. However, in the vast majority of the indices, especially the contrast lesion target recognition ability, i.e., the key DSC/F1 index and the IoU index are leading. Furthermore, in Figure 7 , which shows the inference speed and memory usage for three different parallel settings, it can be concluded that, due to the design of the ultra-lightweight architecture, although parallel processing increases the number of matrix operations slightly, the difference between four-parallel operations is only slightly manifested at the millisecond level at most. However, as can be seen from Table 2 , quad-parallel processing will reduce the number of parameters by a significant 74.8 % . Therefore, to realize the ultra-lightweight architecture design with millisecond difference and excellent performance, we adopt setting 3 as the key structure of the proposed PVM Layer. Figure 6 Settings for ablation experiments with Vision Mamba used in different parallel ways (PVM Layer) Table 2 Ablation experiments on the effect of Vision Mamba in different parallel connections Settings Parameters (millions) GFLOPs DSC/F1 SE/Recall SP ACC IoU Prec 1 0.136 0.060 0.9069 0.8861 0.9834 0.9644 0.8266 0.9354 2 0.070 0.060 0.9073 0.8866 0.9835 a 0.9645 0.8284 0.9398 3 0.049 a 0.060 a 0.9091 a 0.9053 a 0.9790 0.9646 a 0.8334 a 0.9481 a a These values represent the best performance. Figure 7 Comparison of inference time and memory usage of Vision Mamba with different parallel connections (A) Comparison of inference time of Vision Mamba with different parallel connections. (B) Comparison of memory usage of Vision Mamba with different parallel connections. Parallelization of different SSM variants In the methods , we detail the key to the influence of the parameters of Mamba, represented by SSM. However, many current studies propose improvements based on SSM to adapt it to 2D image processing. In Liu et al., 11 researchers proposed SS2D for visual image processing. In Ruan andXiang, 25 researchers proposed VM-UNet for medical image segmentation by combining SS2D with UNet framework. In addition, based on SS2D, Wu et al. 29 proposed high-order SS2D (H-SS2D) for medical image segmentation. The parameter and performance effects of SS2D and H-SS2D using parallel approach are shown in Table 3 . From the table, it is concluded that the parameters and GFLOPs of ( P ) SS2D are reduced by 81.35 % and 61.97 % , respectively, while those of ( P ) H-SS2D are reduced by 80.34 % and 57.14 % , respectively. The above results reveal that the parallel approach is effective in reducing parameters and GFLOPs not only for Mamba represented by SSM but also for different variants of SSM at the same time. Table 3 Impact of adopting parallelism for different SSM variants Methods SSM variants Params GFLOPs DSC SE SP ACC IoU Prec &#8220;1&#8221; VM-UNet SS2D 27.427M 4.112 0.9070 0.8837 0.9842 0.9645 0.8298 0.9302 &#8220;2&#8221; VM-UNet SS2D 27.427M 4.112 0.8891 0.8809 0.9743 0.9554 0.8004 0.8966 &#8220;3&#8221; VM-UNet SS2D 27.427M 4.112 0.9033 0.9131 0.9483 0.9369 0.8237 0.8948 &#8220;1&#8221; VM-UNet ( P ) SS2D 5.116M 1.564 0.9159 0.8843 0.9886 0.9682 0.8366 0.9352 &#8220;2&#8221; VM-UNet ( P ) SS2D 5.116M 1.564 0.8977 0.8996 0.9734 0.9584 0.8009 0.9002 &#8220;3&#8221; VM-UNet ( P ) SS2D 5.116M 1.564 0.9185 0.9340 0.9525 0.9465 0.8377 0.9034 &#8220;1&#8221; H-vmunet H-SS2D 8.973M 0.742 0.9172 0.9056 0.9831 0.9680 0.8471 0.9291 &#8220;2&#8221; H-vmunet H-SS2D 8.973M 0.742 0.8966 0.8952 0.9741 0.9581 0.8073 0.8756 &#8220;3&#8221; H-vmunet H-SS2D 8.973M 0.742 0.9146 0.9295 0.9509 0.9440 0.8427 0.9002 &#8220;1&#8221; H-vmunet ( P ) H-SS2D 1.764M 0.318 0.9066 0.8825 0.9843 0.9644 0.8344 0.9349 &#8220;2&#8221; H-vmunet ( P ) H-SS2D 1.764M 0.318 0.8948 0.9067 0.9695 0.9567 0.8079 0.8744 &#8220;3&#8221; H-vmunet ( P ) H-SS2D 1.764M 0.318 0.9181 0.9254 0.9569 0.9467 0.8473 0.9020 ( P ) shows the adoption of quadruple parallelism to replace the original form; &#8220;1&#8221; indicates experiments on the ISIC2017 dataset, &#8220;2&#8221; indicates the ISIC2018 dataset, and &#8220;3&#8221; indicates the PH 2 dataset. Plug-and-play PVM Layer The proposed PVM Layer can simply replace the base building blocks of any model, which include but are not limited to Convolution, Vision Transformers, Mamba, VM, and so on. Table 4 shows the powerful plug-and-play capabilities of the PVM Layer. From the table, it can be concluded that, after replacing the base building blocks or key functional modules of any model with PVM Layer, there is a significant decrease in parameters and GFLOPs, and the performance is still clearly competitive. In addition, MALUNet and EGE-UNet, as the most advanced lightweight models, after replacing the key modules in the PVM Layer, the parameters and GFLOPs can still be effectively reduced, and the performance is improved. With the above results, it is shown that the powerful plug-and-play feature of the PVM Layer is used to significantly reduce the parameters and GFLOPs of arbitrary models. Table 4 Comparative experiments where the PVM Layer directly replaces modules from different models, with results including parameters, computational complexity, and performance Methods Parameters and computational complexity Performance evaluation Params D P (%) GFLOPs D G (%) DSC/F1 SE/Recall SP ACC IoU Prec UNet 21 2.009M 80.38 3.224 78.74 0.8989 0.8793 0.9812 0.9613 0.8165 0.9196 UNet_ Conv 0.394M 0.686 0.8974 0.8667 0.9842 0.9612 0.8147 0.9211 Att UNet 30 3.581M 45.10 8.575 29.61 0.8821 0.8423 0.9836 0.9560 0.7891 0.9259 Att UNet_ Conv_block 1.966M 6.036 0.8857 0.8414 0.9857 0.9575 0.7921 0.9241 SCR-Net 22 0.812M 81.90 1.567 72.75 0.8898 0.8497 0.9853 0.9588 0.8015 0.9340 SCR-Net_ Conv 0.147M 0.427 0.9058 0.8847 0.9833 0.9640 0.8233 0.9370 Swin-UNet 23 27.176M 76.11 7.724 75.48 0.8670 0.8427 0.9754 0.9494 0.7652 0.8928 Swin-UNet_ Vision Transformer 6.491M 1.894 0.8734 0.8445 0.9783 0.9521 0.7710 0.8953 META-Unet 31 22.209M 94.77 5.140 91.91 0.9068 0.8801 0.9836 0.9639 0.8301 0.9327 META-Unet_ ResNet Layer 1.161M 0.416 0.9047 0.8915 0.9807 0.9633 0.8264 0.9285 MHorUNet 3 9.585M 90.35 0.864 81.94 0.9132 0.8974 0.9834 0.9666 0.8348 0.9289 MHorUNet_ Horblock 0.925M 0.156 0.9107 0.8806 0.9870 0.9662 0.8318 0.9300 HSH-UNet 9 18.803M 84.64 9.362 90.58 0.9108 0.8907 0.9864 0.9654 0.8214 0.9356 HSH-UNet_ HSHB 2.888M 0.882 0.9001 0.8938 0.9776 0.9612 0.8161 0.9286 MALUNet 18 0.175M 26.86 0.083 10.84 0.8896 0.8824 0.9762 0.9583 0.8008 0.9295 MALUNet_ DGA 0.128M 0.074 0.9025 0.8623 0.9882 0.9635 0.8234 0.9327 EGE-UNet 10 0.053M 1.89 0.072 1.39 0.9073 0.8931 0.9816 0.9642 0.8302 0.9219 EGE-UNet_ GHPA 0.052M 0.071 0.9092 0.8941 0.9823 0.9650 0.8332 0.9241 VM-UNet 25 27.427M 60.94 4.112 56.10 0.9070 0.8837 0.9842 0.9645 0.8298 0.9302 VM-UNet_ Vision Mamba 10.713M 1.805 0.8885 0.8517 0.9841 0.9582 0.8120 0.9177 VM-UNet v2 26 22.771M 73.69 4.400 65.43 0.9045 0.8768 0.9849 0.9637 0.8256 0.9168 VM-UNet v2_ Vision Mamba 5.991M 1.521 0.8916 0.8692 0.9804 0.9586 0.8205 0.9119 LightM-UNet 19 0.403M 72.95 0.391 0.26 0.9080 0.8839 0.9846 0.9649 0.8303 0.9321 LightM-UNet_ Mamba Layer 0.109M 0.390 0.9045 0.8668 0.9878 0.9642 0.8292 0.9297 Italics in the method column (e.g., _Conv ) indicate that the PVM Layer replaces the convolution module. In particular, we replace modules containing Convolution, Vision Transformer, Mamba, and Vision Mamba, covering the four most commonly used base modules. D P and D G denote the percentage decrease in parameters and GFLOPs, respectively, after replacing with PVM Layer. Impact of components in the UltraLight VM-UNet A series of ablation experiments were performed to verify the impact of each module in the UltraLight VM-UNet. As shown in Table 5 , we replace the PVM Layer of the encoder, decoder, respectively, with a standard convolution with convolution kernel 3. In addition, we also replace the PVM Layer of the encoder and decoder at the same time with a standard convolution. Also, B a s e l i n e _ S C A B n o t a p p l i c a b l e indicates that the skip-connection part of UltraLight VM-UNet does not use the SAB and CAB modules. From the table, we can conclude that, by replacing the PVM Layer of the encoder and decoder separately, the parameters increased by 63.26 % and the GFLOPs increased in both, while the performance decreased in both. In particular, after replacing the PVM Layer of the encoder and decoder simultaneously, the parameters increase by 151 % and the GFOLPs increase by 25 % . In summary, it is shown that, after replacing the PVM Layer, there is a decrease in all performance aspects and an increase in both parameters and GFLOPs. This again proves the crucial role of PVM Layer. What is more, although the parameters and GFOLPs were further reduced with the removal of the SAB and CAB modules. However, the performance also exhibits some degradation, which is due to the ability of the SAB and CAB modules to combine multi-scale feature information for learning, which improves the sensitivity to lesions and accelerates the model convergence. 18 Therefore, to balance the performance and computational complexity relationship, UltraLight VM-UNet employs the SAB and CAB modules as a bridge for skip connections to further improve segmentation performance. Even so, the performance, parameters, and GFLOPs of UltraLight VM-UNet are still in the forefront compared with the rest of the state-of-the-art lightweight models available currently. Table 5 Ablation experiments on the effect of each module in the UltraLight VM-UNet Methods Params GFLOPs DSC/F1 SE/Recall SP ACC IoU Prec UltraLight VM-UNet (baseline) 0.049M 0.060 0.9091 a 0.9053 a 0.9790 0.9646 a 0.8334 a 0.9481 a Baseline_Encoder_Conv 0.080M 0.071 0.9033 0.8643 0.9880 0.9638 0.8237 0.9461 Baseline_Decoder_Conv 0.080M 0.064 0.8958 0.8512 0.9881 a 0.9612 0.8113 0.9454 Baseline_(En+De)_Conv 0.123M 0.075 0.9065 0.8784 0.9855 0.9645 0.8291 0.9365 Baseline_SCAB not applicable 0.033M a 0.058 a 0.9029 0.8767 0.9841 0.9631 0.8221 0.9436 a These values represent the best performance. Impact of different channel numbers in UltraLight VM-UNet In addition, to verify the impact of different channel numbers in the UltraLight VM-UNet, we conducted a series of ablation experiments. As shown in Table 6 , we set up four different combinations of channel numbers, including [8,16,24,32,48,64], [8,16,32,64,128,256], [16,32,64,128,256,512], and [32,64,128,256,512,1024], respectively. In addition, we conducted experiments both in the parallel-free manner (labeled by &#8220;1&#8221;) and in the parallel manner (labeled by &#8220;2&#8221;). From the table, it can be concluded that the parameters and GFLOPs at [8,16,24,32,48,64] are the lowest, while the overall difference in performance is not very large. However, as the number of channels increases, both parameters and GFLOPs show a significant rise. The parameter increases from 0.136M to 13.607M for the group without parallelism and from 0.049M to 3.305M for the group with parallelism. In particular, the average decrease in parameters for the same number of channel combinations using the parallel approach over the non-parallel approach is 72.70 % , which is extremely close to the percentage of decrease in the four-parallel approach over the non-parallel approach analyzed in the methods section (74.80 % ). In addition, at [8,16,24,32,48,64], the parameters are more susceptible to the rest of the modules, so the decrease is not as large as for the rest of the channel number combinations. However, its overall parameters and GFLOPs reach an impressive 0.049M and 0.060, and show strong competitive segmentation performance. Table 6 Ablation experiments on the effect of combining different channel numbers in the UltraLight VM-UNet Methods Params D P ( 74.80 % ) GFLOPs DSC/F1 SE/Recall SP ACC IoU Prec &#8220;1&#8221; [8,16,24,32,48,64] 0.136M 63.98 % (&#8722;10.82%) 0.060 0.9069 0.8861 0.9834 0.9644 0.8266 0.9354 &#8220;2&#8221; [8,16,24,32,48,64] 0.049M 0.060 0.9091 0.9053 0.9790 0.9646 0.8334 0.9481 &#8220;1&#8221; [8,16,32,64,128,256] 0.909M 75.47 % (+0.67%) 0.074 0.9018 0.8749 0.9840 0.9627 0.8279 0.9417 &#8220;2&#8221; [8,16,32,64,128,256] 0.223M 0.074 0.9079 0.8965 0.9809 0.9644 0.8282 0.9326 &#8220;1&#8221; [16,32,64,128,256,512] 3.479M 75.63 % (+0.83%) 0.259 0.9049 0.8981 0.9789 0.9630 0.8311 0.9402 &#8220;2&#8221; [16,32,64,128,256,512] 0.848M 0.259 0.9056 0.8906 0.9815 0.9637 0.8224 0.9450 &#8220;1&#8221; [32,64,128,256,512,1024] 13.607M 75.71 % (+0.91%) 0.970 0.9053 0.8878 0.9821 0.9637 0.8214 0.9459 &#8220;2&#8221; [32,64,128,256,512,1024] 3.305M 0.970 0.9012 0.8812 0.9819 0.9622 0.8297 0.9334 D P denotes the percentage of parameter reduction with the parallel approach; 74.80 % denotes the theoretical percentage reduction of localized module parameters derived from the Mamba analysis in the methods ; &#8220;1&#8221; indicates non-parallel connection method and &#8220;2&#8221; indicates a quadruple parallel connection method. Discussion in practical engineering In practical engineering applications, it is common for different devices to be used for training and inference in skin lesion segmentation tasks. To further discuss the superiority of the proposed method, we provide the throughput of our model versus the baseline model on different devices in Figure 8 . Specifically, the throughput of our proposed model on 3090, 4090, and V100 is about two times higher than that of the baseline model (VM-UNet). In particular, compared with LightM-UNet, our model achieves a 20 % -40 % improvement in throughput across a variety of hardware platforms. Figure 8 Discussion of the results in practical engineering applications (A) Comparison of training speed and inference throughput of the proposed model with baseline model and the lightest Mamba-based medical image segmentation model at different resolutions and different devices. (B) Comparison of the memory required for training the proposed model with the baseline model and the lightest Mamba-based medical image segmentation model at different resolutions. (C) Comparison of the memory required for inference of the proposed model with the baseline model and the lightest Mamba-based medical image segmentation model at different resolutions. In addition, the need for segmentation of skin lesion images at different resolutions is also encountered through practical engineering applications. Based on this problem, we tested the training speed and inference speed (throughput) of the proposed model versus the baseline model in Figure 8 at different resolutions. However, a batch size of 1 is used for inference, so the inference speed (throughput) does not change much at different resolutions. In particular, the advantage of our model is shown more clearly on the high resolution 512 &#215; 512 . Nonetheless, researchers prefer to use 256 &#215; 256 resolution in processing images of skin lesion. 8 , 9 , 10 , 18 , 29 Discussion under the task of non-skin lesion segmentation To further validate the potential of the proposed UltraLight VM-UNet, we conducted comprehensive experiments on two additional datasets: the Autooral dataset 32 and the Spleen dataset. 33 These datasets comprise oral ulcer (cancer) images acquired through endoscopy and spleen images in CT mode, respectively. The Autooral dataset, developed by Jiang et al., 32 contains high-quality oral ulcer (cancer) images obtained by oral endoscopy. We maintained experimental consistency with the original study&#8217;s protocols. The Spleen dataset, established by Memorial Sloan Kettering Cancer Center, 33 provides CT modality data for spleen segmentation, and the experimental configuration adhered to the methodology outlined by Wu et al. 29 to ensure a fair comparison. The results of the experiments on the Autooral dataset and the Spleen dataset are presented in Table 7 . Since our focus is on the exploration of lightweight, we performed experiments with the current state-of-the-art lightweight and classical medical image segmentation models. From the table, it can be concluded that although the UltraLight VM-UNet is slightly inferior to other models in terms of DSC and sensitivity (SE) metrics on endoscopic mode and CT mode; however, it still shows strong competitiveness in terms of SP and accuracy (ACC) metrics. In particular, compared with other current state-of-the-art lightweight models (MALUNet, LightM-UNet, and EGE-UNet), we lead in several metrics. The UltraLight VM-UNet demonstrates competitiveness through experiments in both endoscopic and CT modalities, as well as the dermoscopic segmentation task that is the focus of this paper. In particular, the UltraLight VM-UNet is more competitive for dermatoscope segmentation tasks. Table 7 Experimental results with state-of-the-art lightweight and classical medical image segmentation models on Autooral dataset and Spleen dataset Model Parameters (millions) GFLOPs DSC/F1 SE/Recall SP ACC IoU Prec Comparison experiments on the Autooral dataset (non-skin lesion segmentation task) U-Net 21 2.009 3.224 0.7480 b 0.7282 0.9815 0.9617 0.5575 0.7290 SCR-Net 22 0.801 1.567 0.7069 0.6148 0.9896 0.9602 0.5467 0.8315 a Swin-Unet 23 27.176 7.724 0.4484 0.3675 0.9770 0.9295 0.2890 0.5750 ATTENTION SWIN U-NET 8 46.910 14.181 0.6463 0.5032 0.9954 a 0.9571 0.4774 0.7732 C 2 SDG 24 22.001 7.972 0.7210 0.6554 0.9862 0.9604 0.5638 0.8012 b VM-UNet 25 27.427 4.112 0.7639 a 0.7555 a 0.9811 0.9636 a 0.5970 b 0.6879 VM-UNet v2 26 22.771 4.400 0.7461 0.7285 b 0.9813 0.9602 0.5580 0.7060 MALUNet 18 0.175 0.083 0.6318 0.6500 0.9655 0.9409 0.4832 0.7201 LightM-UNet 19 0.403 0.391 0.6551 0.6130 0.9781 0.9497 0.4483 0.5831 EGE-UNet 10 0.053 b 0.072 b 0.5893 0.7263 0.9375 0.9211 0.4178 0.4958 DermoSegDiff 27 45.112 38.636 0.3569 0.2565 0.9845 0.9338 0.2134 0.7599 LiteMamba-Bound 28 0.957 1.591 0.7425 0.7166 0.9833 0.9609 0.5981 a 0.7533 UltraLight VM-UNet (Our) 0.049 a 0.060 a 0.7255 0.6637 0.9903 b 0.9621 b 0.5742 0.7673 Comparison experiments on the Spleen dataset (non-skin lesion segmentation task) U-Net 21 2.010 5.037 0.9441 b 0.9604 a 0.9989 0.9983 b 0.8679 0.9283 SCR-Net 22 0.812 2.449 0.9181 0.9122 0.9988 0.9976 0.8486 0.9220 Swin-Unet 23 27.193 12.084 0.6758 0.5526 0.9923 0.9818 0.4832 0.5541 ATTENTION SWIN U-NET 8 46.945 22.175 0.7829 0.6662 0.9994 a 0.9945 0.6433 0.9341 C 2 SDG 24 22.010 12.457 0.9354 0.9263 0.9991 b 0.9981 0.8787 0.9448 a VM-UNet 25 27.428 6.425 0.9418 0.9429 0.9991 b 0.9982 0.8859 0.9248 VM-UNet v2 26 22.771 6.874 0.9445 a 0.9315 0.9990 0.9982 0.8902 a 0.9419 MALUNet 18 0.178 0.130 0.9310 0.9305 0.9989 0.9979 0.8709 0.9315 LightM-UNet 19 0.405 0.587 0.9329 0.9459 0.9988 0.9982 0.8646 0.9317 EGE-UNet 10 0.053 b 0.113 b 0.8992 0.8866 0.9987 0.9975 0.8251 0.9347 DermoSegDiff 27 45.112 60.368 0.9266 0.9507 b 0.9987 0.9979 0.8777 0.9196 LiteMamba-Bound 28 0.957 2.484 0.9214 0.9455 0.9990 0.9981 0.8897 b 0.9314 UltraLight VM-UNet (Our) 0.049 a 0.094 a 0.9168 0.8991 0.9991 b 0.9984 a 0.8566 0.9430 b a These values represent the best performance. b These values represent the second-best performance. Conclusion In this study, we conduct a detailed analysis of the key factors influencing parameter efficiency in Mamba. Building upon these insights, we propose a PVM Layer for efficient deep feature processing. The PVM Layer consists of four VM modules operating in parallel, with each module responsible for processing one-quarter of the original input channels. This is due to the fact that the number of input channels to the SSM in Mamba has an explosive effect on the number of parameters, and the VM parameters for processing a quarter of the number of channels are 6.3 % of the original VM parameters, which is an explosive reduction of 93.7 % . In addition, the PVM Layer can be generalized to any Mamba variant (P x M Layer). Based on the PVM Layer, we propose the UltraLight VM-UNet with a parameter of only 0.049M and GFLOPs of only 0.060. The UltraLight VM-UNet parameters are 99.82 % lower than those of the traditionally pure VM-UNet model and 87.84 % lower than those of the lightest LightM-UNet model. In addition, we experimentally demonstrated on three publicly available skin lesion datasets that the UltraLight VM-UNet has equally strong performance competitiveness with such low parameters. We also discuss the performance of the UltraLight VM-UNet for non-skin lesion segmentation tasks and demonstrate its potential competitiveness. In particular, it is important to note that in this work we have not just proposed a lightweight model but an in-depth exploration of Mamba for lightweight research in healthcare settings. Based on this work, Mamba may become a new mainstream module for lightweight model. Methods Related work Medical image segmentation, as one of the important branches in image segmentation, is also one of the research directions to which many researchers have devoted their efforts. Among them, multi-scale variation problem and feature refinement learning are the key problems in medical image segmentation. 34 Also, skin lesion segmentation has rich and varied feature information with regard to the high lethality caused by malignant melanoma, 35 which has led many researchers to carry out a series of studies around skin lesion segmentation. 3 , 8 , 9 Medical image segmentation algorithms represented by skin lesions have been rapidly developed after the advent of U-Net. In Aghdam et al., 8 an inhibition operation of the attention mechanism in cascade operation has been proposed for skin lesion segmentation based on Swin-Unet. 23 The MHorUNet 3 model proposes a high-order spatial interaction UNet model for skin lesion segmentation. In Wu et al., 9 an adaptive selection of a higher-order UNet model for order interaction has been proposed for skin lesion segmentation. DSU-Net 36 proposes to utilize convolutional neural networks and transformers to construct a bipolar UNet for skin lesion segmentation, which proceeds from coarse to fine through two stages. However, the joint use of convolutional neural networks and transformers further makes the architecture more bloated. BDFormer 37 proposes a new boundary-aware bi-decoder transformer, which employs a single-encoder and bi-decoder framework for skin lesion segmentation and expanding boundary segmentation. DermoSegDiff 27 proposes to introduce denoised diffusion probabilistic models into the skin lesion segmentation task which effectively integrates noise and semantic information, and proposes a loss function adapted to the boundary region to improve lesion boundary recognition. EGE-UNet 10 proposes a lightweight model for skin lesion segmentation that employs Group multi-axis Hadamard Product Attention module and Group Aggregation Bridge module, which significantly reduces the number of parameters and computation while maintaining high performance. MALUNet 18 is another lightweight model for skin lesion segmentation proposed by researchers, which introduces multiple attention mechanism modules to significantly reduce the number of parameters and computational complexity while maintaining high performance in skin lesion segmentation tasks. In addition, there are many algorithms based on U-Net improved for skin lesion segmentation. However, it is common for researchers to add richer modules to the model to improve the accuracy of recognition, but this also significantly increases the parameters and computational complexity of the model. After the emergence of VM, LightM-UNet 19 was proposed to reduce the number of parameters in the model based on Mamba. LightM-UNet further extracts the deep semantics and tele-relationships by using the residual VM, and achieves better performance with a smaller number of parameters. In addition, U-Mamba 38 introduced VM into the U-framework, but its having a large number of parameters (173.53M) limits its use in real clinical settings. LiteMamba-Bound 28 proposed a lightweight model designed for skin lesion segmentation task, which improves skin lesion recognition by introducing a channel-attentive bi-Mamba module and an inverse-attentive boundary module. In this paper, to solve the current problem of large model parameters and to reveal the key factors affecting Mamba parameters. We propose UltraLight VM-UNet based on Mamba with a parameter of only 0.049M. In addition, we propose a PVM method for processing deep features, named the PVM Layer, based on a detailed theoretical analysis. It is a plug-and-play module that can simply replace the convolutional layers and transformers to significantly reduce the number of model parameters and maintain excellent segmentation performance. Datasets To validate that the proposed UltraLight VM-UNet maintains competitive performance with only 0.049M parameters, we conducted experiments on three publicly available dermatologic lesion datasets. The ISIC2017 39 and ISIC2018 40 datasets are two large datasets published by the International Skin Imaging Collaboration (ISIC), respectively. The PH 2 41 dataset is a small skin lesion dataset, so we used PH 2 as an external validation using training weights from ISIC2017. For the ISIC2017 dataset we acquired 2,000 images as well as dermatoscope images with segmentation mask labels. Among them, the dataset was randomly divided, 1,250 images were used for model training, 150 images were used for model validation, and 600 images were used for model testing. The initial size of the images is 576 &#215; 767 pixels, and we standardize the size to 256 &#215; 256 pixels when inputting the model. For the ISIC2018 dataset we acquired 2,594 images as well as dermatoscope images with segmentation mask labels. Among them, the dataset was randomly divided, 1,815 images were used for model training, 259 images were used for model validation, and 520 images were used for model testing. The initial size of the images is 2 , 016 &#215; 3 , 024 pixels, and we standardize the size to 256 &#215; 256 pixels when inputting the model. For the PH 2 dataset we acquired 200 images as well as dermatoscope images with segmentation mask labels. All 200 images were used for external validation. The initial size of the images was 768 &#215; 560 pixels and we standardized the size to 256 &#215; 256 pixels. Implementation details The proposed UltraLight VM-UNet is shown in Figure 2 . The experiments were all implemented based on Python 3.8 and Pytorch 1.13.0. A single NVIDIA V100 GPU with 32 GB of memory was used for all experiments. All experiments used the same data augmentation operations to more fairly determine the performance of the model, including horizontal and vertical flips, and random rotation operations. BceDice loss function was used, with AdamW 42 as the optimizer, a training epoch of 250, a batch size of 8, and a cosine annealing learning rate scheduler with an initial learning rate of 0.001 and a minimum learning rate set to 0.00001. Evaluation metrics The DSC, SE, SP, ACC, IoU, and precision (Prec) are the most commonly used evaluation metrics in medical image segmentation. The DSC is used to measure the similarity between the ground truth and the predicted segmentation maps, and it is equivalent to the F1 score in image segmentation tasks. SE is primarily used to measure the percentage of true positives among true positives and false negatives, it is equivalent to Recall. SP is mainly used to measure the percentage of true negatives among true negatives and false positives. ACC is mainly used to measure the proportion of correctly classified samples. IoU is primarily used to measure the degree of overlap between the predicted segmentation region and the true segmentation region. Prec is mainly used to measure the proportion of correctly segmented pixels in the predicted segmentation region. (Equation 15) DSC / F 1 = 2 TP 2 TP + FP + FN (Equation 16) ACC = TP + TN TP + TN + FP + FN (Equation 17) SE / Recall = TP TP + FN (Equation 18) SP = TN TN + FP (Equation 19) IoU = TP TP + FP + FN (Equation 20) Prec = TP TP + FP where TP denotes true positive, TN denotes true negative, FP denotes false positive, and FN denotes false negative. Resource availability Lead contact Requests for further information and resources should be directed to and will be fulfilled by the lead contact, Qing Chang ( robie0510@hotmail.com ). Materials availability This study did not generate new materials. Data and code availability Our source code is available at GitHub ( https://github.com/wurenkai/UltraLight-VM-UNet ) and has been archived at Zenodo. 43 The ISIC2017 dataset is from Codella et al. 39 and is available at https://challenge.isic-archive.com/data/#2017 . The ISIC2018 dataset is from Codella et al. 40 and is available at https://challenge.isic-archive.com/data/#2018 . The PH 2 dataset is from Mendon&#231;a et al. 41 and is available at https://drive.google.com/file/d/1AEMJKAiORlrwdDi37dRqbqXi6zLmnU3Q/view?usp=sharing . Acknowledgments This work was supported partly by Medicine-engineering Interdisciplinary Project set up by 10.13039/501100008050 University of Shanghai for Science and Technology ( 2023-LXY-RUIJIN01Z ). Author contributions Conceptualization, R.W. and Y.L.; methodology, R.W. and Y.L.; writing &#8211; original draft, R.W.; writing &#8211; review &amp; editing, R.W., Y.L., G.N., and P.L.; formal analysis, R.W. and G.M.; validation, Y.L. and P.L.; project administration, Q.C.; funding acquisition, Q.C.; s upervision, Q.C. Declaration of interests The authors declare no competing interests. References 1 Rao Y. Zhao W. Tang Y. Zhou J. Lim S.N. Lu J. Hornet: Efficient high-order spatial interactions with recursive gated convolutions Adv. Neural Inf. Process. Syst. 35 2022 10353 10366 2 Rao Y. Zhao W. Zhu Z. Lu J. Zhou J. Global filter networks for image classification Adv. Neural Inf. Process. Syst. 34 2021 980 993 3 Wu R. Liang P. Huang X. Shi L. Gu Y. Zhu H. Chang Q. Mhorunet: High-order spatial interaction unet for skin lesion segmentation Biomed. Signal Process Control 88 2024 105517 4 Liu Z. Mao H. Wu C.Y. Feichtenhofer C. Darrell T. Xie S. A convnet for the 2020s Chellappa R. Matas J. Quan L. Shah M. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 2022 Institute of Electrical and Electronics Engineers 11976 11986 5 Ding X. Zhang X. Han J. Ding G. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns Chellappa R. Matas J. Quan L. Shah M. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 2022 Institute of Electrical and Electronics Engineers 11963 11975 6 Hatamizadeh A. Tang Y. Nath V. Yang D. Myronenko A. Landman B. Roth H.R. Unetr Xu D. Transformers for 3d medical image segmentation Farrell R. Bowyer K. Proceedings of the IEEE/CVF winter conference on applications of computer vision 2022 Institute of Electrical and Electronics Engineers 574 584 7 Hatamizadeh A. Nath V. Tang Y. Yang D. Roth H.R. Xu D. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images Crimi A. Bakas S. International MICCAI Brainlesion Workshop 2021 Springer 272 284 8 Aghdam E.K. Azad R. Zarvani M. Merhof D. Attention swin u-net: Cross-contextual attention mechanism for skin lesion segmentation Lepore N. Acosta O. 2023 IEEE 20th International Symposium on Biomedical Imaging. (ISBI) 2023 IEEE 1 5 9 Wu R. Lv H. Liang P. Cui X. Chang Q. Huang X. Hsh-unet: Hybrid selective high order interactive u-shaped model for automated skin lesion segmentation Comput. Biol. Med. 168 2024 107798 10.1016/j.compbiomed.2023.107798 38043470 10 Ruan J. Xie M. Gao J. Liu T. Fu Y. Ege-unet: an efficient group enhanced unet for skin lesion segmentation Greenspan H. Madabhushi A. Mousavi P. Salcudean S. Duncan J. Syeda-Mahmood T. Taylor R. International Conference on Medical Image Computing and Computer-Assisted Intervention 2023 Springer 481 490 11 Liu Y. Tian Y. Zhao Y. Yu H. Xie L. Wang Y. Ye Q. Jiao J. Liu Y. Vmamba: Visual state space model Adv. Neural Inf. Process. Syst. 37 2024 103031 103063 12 Zhu L. Liao B. Zhang Q. Wang X. Liu W. Wang X. Vision mamba: Efficient visual representation learning with bidirectional state space model Preprint at arXiv 2024 10.48550/arXiv.2401.09417 13 Gu A. Dao T. Mamba: Linear-time sequence modeling with selective state spaces Preprint at arXiv 2023 10.48550/arXiv.2312.00752 14 Szegedy C. Liu W. Jia Y. Sermanet P. Reed S. Anguelov D. Erhan D. Vanhoucke V. Rabinovich A. Going deeper with convolutions Bischof H. Forsyth D. Schmid C. Sclaroff S. Proceedings of the IEEE conference on computer vision and pattern recognition 2015 Institute of Electrical and Electronics Engineers 1 9 15 Ioffe S. Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift Bach F.R. Blei D.M. International conference on machine learning 2015 448 456 JMLR.org 16 Szegedy C. Vanhoucke V. Ioffe S. Shlens J. Wojna Z. Rethinking the inception architecture for computer vision Proceedings of the IEEE conference on computer vision and pattern recognition 2016 2818 2826 17 Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference on Artificial Intelligence vol. 31. 18 Ruan J. Xiang S. Xie M. Liu T. Malunet Fu Y. A multi-attention and light-weight unet for skin lesion segmentation Aluru S. Narasimhan G. Wang J. 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2022 IEEE 1150 1156 19 Liao W. Zhu Y. Wang X. Pan C. Wang Y. Ma L. Lightm-unet: Mamba assists in lightweight unet for medical image segmentation Preprint at arXiv 2024 10.48550/arXiv.2403.05246 20 Elfwing S. Uchibe E. Doya K. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning Neural Netw. 107 2018 3 11 29395652 10.1016/j.neunet.2017.12.012 21 Ronneberger O. Fischer P. Brox T. U-net: Convolutional networks for biomedical image segmentation Navab N. Hornegger J. Wells W.M. Frangi A. Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 2015 Springer 234 241 22 Wu, H., Zhong, J., Wang, W., Wen, Z., and Qin, J. (2021). Precise yet efficient semantic calibration and refinement in convnets for real-time polyp segmentation from colonoscopy videos. In Proceedings of the AAAI Conference on Artificial Intelligence vol. 35. pp. 2916&#8211;2924. 23 Cao H. Wang Y. Chen J. Jiang D. Zhang X. Tian Q. Wang M. Swin-unet: Unet-like pure transformer for medical image segmentation Avidan S. Brostow G. Ciss&#233; M. Farinella G.M. Hassner T. European conference on computer vision 2022 Springer 205 218 24 Devil is in channels: Contrastive single domain generalization for medical image segmentation Hu S. Liao Z. Xia Y. Greenspan H. Madabhushi A. Mousavi P. Salcudean S. Duncan J. Syeda-Mahmood T. Taylor R. International Conference on Medical Image Computing and Computer-Assisted Intervention 2023 Springer 14 23 25 Ruan J. Xiang S. Vm-unet: Vision mamba unet for medical image segmentation Preprint at arXiv 2024 10.48550/arXiv.2402.02491 26 Zhang M. Yu Y. Jin S. Gu L. Ling T. Tao X. Vm-unet-v2: rethinking vision mamba unet for medical image segmentation Peng W. Cai Z. Skums P. International Symposium on Bioinformatics Research and Applications 2024 Springer 335 346 27 Bozorgpour A. Sadegheih Y. Kazerouni A. Azad R. Merhof D. Dermosegdiff: A boundary-aware segmentation diffusion model for skin lesion delineation Rekik I. Adeli E. Park S.H. Schnabel J. International workshop on predictive intelligence in medicine 2023 Springer 146 158 28 Ho Q.H. Nguyen T.N. Tran T.T. Pham V.T. Litemamba-bound: A lightweight mamba-based model with boundary-aware and normalized active contour loss for skin lesion segmentation Methods 235 2025 10 25 39864606 10.1016/j.ymeth.2025.01.008 29 Wu R. Liu Y. Liang P. Chang Q. H-vmunet: High-order vision mamba unet for medical image segmentation Neurocomputing 624 2025 129447 30 Oktay O. Schlemper J. Folgoc L.L. Lee M. Heinrich M. Misawa K. Mori K. McDonagh S. Hammerla N.Y. Kainz B. Attention u-net: Learning where to look for the pancreas Preprint at arXiv 2018 10.48550/arXiv.1804.03999 31 Wu H. Zhao Z. Wang Z. Meta-unet: Multi-scale efficient transformer attention unet for fast and high-accuracy polyp segmentation IEEE Trans. Autom. Sci. Eng. 21 2024 4117 4128 32 Jiang C. Wu R. Liu Y. Wang Y. Chang Q. Liang P. Fan Y. A high-order focus interaction model and oral ulcer dataset for oral ulcer segmentation Sci. Rep. 14 2024 20085 10.1038/s41598-024-69125-9 PMC11362486 39209880 33 Antonelli M. Reinke A. Bakas S. Farahani K. Kopp-Schneider A. Landman B.A. Litjens G. Menze B. Ronneberger O. Summers R.M. The medical segmentation decathlon Nat. Commun. 13 2022 4128 35840566 10.1038/s41467-022-30695-9 PMC9287542 34 Xiao H. Li L. Liu Q. Zhu X. Zhang Q. Transformers in medical image segmentation: A review Biomed. Signal Process Control 84 2023 104791 35 Siegel R.L. Miller K.D. Fuchs H.E. Jemal A. Cancer statistics, 2022 CA Cancer J. Clin. 72 2022 7 33 35020204 10.3322/caac.21708 36 Zhong L. Li T. Cui M. Cui S. Wang H. Yu L. Dsu-net: Dual-stage u-net based on cnn and transformer for skin lesion segmentation Biomed. Signal Process Control 100 2025 107090 37 Ji Z. Ye Y. Ma X. Bdformer: Boundary-aware dual-decoder transformer for skin lesion segmentation Artif. Intell. Med. 162 2025 103079 10.1016/j.artmed.2025.103079 39983372 38 Ma J. Li F. Wang B. U-mamba: Enhancing long-range dependency for biomedical image segmentation Preprint at arXiv 2024 10.48550/arXiv.2401.04722 39 Codella N.C. Gutman D. Celebi M.E. Helba B. Marchetti M.A. Dusza S.W. Kalloo A. Liopyris K. Mishra N. Kittler H. Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic) Amini A. Acton S. 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018) 2018 IEEE 168 172 40 Codella N. Rotemberg V. Tschandl P. Celebi M.E. Dusza S. Gutman D. Helba B. Kalloo A. Liopyris K. Marchetti M. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic) Preprint at arXiv 2019 10.48550/arXiv.1902.03368 41 Mendon&#231;a T. Ferreira P.M. Marques J.S. Marcal A.R. Rozeira J. Ph 2-a dermoscopic image database for research and benchmarking Sunagawa K. 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC) 2013 IEEE 5437 5440 10.1109/EMBC.2013.6610779 24110966 42 Loshchilov I. Hutter F. Decoupled weight decay regularization Preprint at arXiv 2017 10.48550/arXiv.1711.05101 43 Wu, R., Liu, Y., Ning, G., Liang, P., and Chang, Q. (2025). Source code for the paper &#8220;UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation&#8221;. Zenodo. 10.5281/zenodo.15354520 ."
}