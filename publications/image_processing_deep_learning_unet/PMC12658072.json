{
  "pmcid": "PMC12658072",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.156713",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12658072",
    "pmid": "41298516",
    "doi": "10.1038/s41598-025-25993-3",
    "title": "Enhancing smart city mobility through real time explainable AI in autonomous vehicles",
    "year": "2025",
    "month": "11",
    "day": "26",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "26"
    },
    "authors": [
      "Malik Ali Zaman",
      "Naz Naila Samar",
      "Ahmed Fahad",
      "Saleem Muhammad",
      "Farooq Muhammad Sajid",
      "Rehman Ateeq Ur",
      "Ismael Waleed M.",
      "Khan Muhammad Adnan"
    ],
    "abstract": "The quick advancement of Autonomous Vehicular Networks (AVNs) shows remarkable potential to transform urban transportation systems in smart cities. This transformational process faces several crucial issues, primarily due to the lack of clear decision-making, concerns about public confidence, and the need for prompt protective measures in various contexts. The implementation of AVNs depends on resolving current adversities, as these difficulties affect both system safety, user trust, and performance reliability. Traditional AVN development focused on enhancing technical capabilities, such as reliability, but failed to adequately address issues with decision transparency and interpretability. The current systems fall short because they lack an understanding of how Autonomous Vehicles (AVs) generate decisions in real-time urban conditions, which impedes public confidence and broader adoption. To address these limitations, this study integrates You Only Look Once, V5 (YOLOv5), a fast and lightweight object detection model well-suited for AVs, alongside Explainable AI (XAI) techniques to ensure interpretability and transparency. In this research, an XAI-based YOLOv5 model is proposed to enable real-time, explainable decision-making. Its objectives are to increase transparency, increase safety, and gain public acceptance for connecting the AVNs to smart city systems. The proposed model achieves an accuracy of 99% with a miss rate of 1%, thereby enhancing classification accuracy and public confidence. The proposed work also aims to foster public trust in AVNs within smart city ecosystems by making AI decisions more transparent and interpretable.",
    "keywords": [
      "Autonomous vehicular networks (AVNs)",
      "Explainable AI (XAI)",
      "Vehicle-to-Vehicle (V2V)",
      "Vehicle-to-Infrastructure (V2I)",
      "Infrastructure-to-Infrastructure (I2I)",
      "Engineering",
      "Mathematics and computing"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12658072</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12658072.1</article-id><article-id pub-id-type=\"pmcaid\">12658072</article-id><article-id pub-id-type=\"pmcaiid\">12658072</article-id><article-id pub-id-type=\"pmid\">41298516</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-25993-3</article-id><article-id pub-id-type=\"publisher-id\">25993</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhancing smart city mobility through real time explainable AI in autonomous vehicles</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Malik</surname><given-names initials=\"AZ\">Ali Zaman</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Naz</surname><given-names initials=\"NS\">Naila Samar</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Ahmed</surname><given-names initials=\"F\">Fahad</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Saleem</surname><given-names initials=\"M\">Muhammad</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Farooq</surname><given-names initials=\"MS\">Muhammad Sajid</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Rehman</surname><given-names initials=\"AU\">Ateeq Ur</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Ismael</surname><given-names initials=\"WM\">Waleed M.</given-names></name><address><email>waleed.m@auhd.edu.ye</email></address><xref ref-type=\"aff\" rid=\"Aff6\">6</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Khan</surname><given-names initials=\"MA\">Muhammad Adnan</given-names></name><address><email>adnan@gachon.ac.kr</email></address><xref ref-type=\"aff\" rid=\"Aff7\">7</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02my4wj17</institution-id><institution-id institution-id-type=\"GRID\">grid.444933.d</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0608 8111</institution-id><institution>Department of Computer Science, </institution><institution>National College of Business Administration and Economics, </institution></institution-wrap>Lahore, 54000 Pakistan </aff><aff id=\"Aff2\"><label>2</label>Department of Cyber Security, NASTP Institute of Information Technology, Lahore, 58810 Pakistan </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/0034me914</institution-id><institution-id institution-id-type=\"GRID\">grid.412431.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0444 045X</institution-id><institution>Computer Science and Engineering, Saveetha School of Engineering, </institution><institution>Saveetha Institute of Medical and Technical Sciences, </institution></institution-wrap>Chennai, Tamilnadu India </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01ah6nb52</institution-id><institution-id institution-id-type=\"GRID\">grid.411423.1</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0622 534X</institution-id><institution>Applied Science Research Center, </institution><institution>Applied Science Private University, </institution></institution-wrap>Amman, Jordan </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/05t4pvx35</institution-id><institution-id institution-id-type=\"GRID\">grid.448792.4</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 4678 9721</institution-id><institution>University Center for Research and Development, </institution><institution>Chandigarh University, </institution></institution-wrap>Mohali, 140413 Punjab India </aff><aff id=\"Aff6\"><label>6</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02zv8ns48</institution-id><institution>Department of Information Technology, Faculty of Engineering, </institution><institution>Azal University for Human Development, </institution></institution-wrap>Sanaa, Yemen </aff><aff id=\"Aff7\"><label>7</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03ryywt80</institution-id><institution-id institution-id-type=\"GRID\">grid.256155.0</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0647 2973</institution-id><institution>Department of Software, Faculty of Artificial Intelligence and Software, </institution><institution>Gachon University, </institution></institution-wrap>Seongnam-si, 13557 Republic of Korea </aff></contrib-group><pub-date pub-type=\"epub\"><day>26</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>42118</elocation-id><history><date date-type=\"received\"><day>7</day><month>7</month><year>2025</year></date><date date-type=\"accepted\"><day>27</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>26</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 14:25:12.873\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_25993.pdf\"/><abstract id=\"Abs1\"><p id=\"Par10\">The quick advancement of Autonomous Vehicular Networks (AVNs) shows remarkable potential to transform urban transportation systems in smart cities. This transformational process faces several crucial issues, primarily due to the lack of clear decision-making, concerns about public confidence, and the need for prompt protective measures in various contexts. The implementation of AVNs depends on resolving current adversities, as these difficulties affect both system safety, user trust, and performance reliability. Traditional AVN development focused on enhancing technical capabilities, such as reliability, but failed to adequately address issues with decision transparency and interpretability. The current systems fall short because they lack an understanding of how Autonomous Vehicles (AVs) generate decisions in real-time urban conditions, which impedes public confidence and broader adoption. To address these limitations, this study integrates You Only Look Once, V5 (YOLOv5), a fast and lightweight object detection model well-suited for AVs, alongside Explainable AI (XAI) techniques to ensure interpretability and transparency. In this research, an XAI-based YOLOv5 model is proposed to enable real-time, explainable decision-making. Its objectives are to increase transparency, increase safety, and gain public acceptance for connecting the AVNs to smart city systems. The proposed model achieves an accuracy of 99% with a miss rate of 1%, thereby enhancing classification accuracy and public confidence. The proposed work also aims to foster public trust in AVNs within smart city ecosystems by making AI decisions more transparent and interpretable.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Autonomous vehicular networks (AVNs)</kwd><kwd>Explainable AI (XAI)</kwd><kwd>Vehicle-to-Vehicle (V2V)</kwd><kwd>Vehicle-to-Infrastructure (V2I)</kwd><kwd>Infrastructure-to-Infrastructure (I2I)</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Engineering</kwd><kwd>Mathematics and computing</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par11\">The advent of AVNs represents a groundbreaking innovation in the urban transport system, incorporating the application of Artificial Intelligence (AI) and modern communication technology to provide smart transport systems<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. These networks have the potential to provide the best possible traffic patterns, less congested roads, improved traffic signals, and enhanced traffic safety. However, these advancements raise the following critical concern in AVNs: the lack of transparency in the decisions made by the AI systems used in AVs. In the case of traditional AVN models, they are black box models where the decision-making process is not amenable to the users or regulators<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref>,<xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>. Such a lack of explainability leads to public skepticism, risks of legal responsibility, and legislative issues in cases of car crashes or reckless driving. XAI appears to be a currently sought-after solution aimed at enhancing the vague vehicular decisions concerning interpretability, public acceptance, and regulatory approval. Thus, XAI helps to improve the transparency and accountability of these models and can be regarded as a key factor for users&#8217; trust and compliance with the regulation<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>.</p><p id=\"Par12\">Another significant hurdle hindering the use of AVN is that it is challenging to show precisely how and why AVs make decisions instantaneously<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>. For instance, when an AV decides to change lanes or apply the brakes suddenly, the passengers, pedestrians, and traffic authorities must be able to understand why the car made such a decision. For this reason, when there is no interpretability, the population loses trust in the AVNs, or there may be a fear of how safe these vehicles are. Similarly, road safety continues to be an area of concern, and according to the WHO, global road traffic deaths stand at 1.35 million per year, with the majority resulting from human error<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref>,<xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>. AVNs can prevent such fatal accidents by removing distractions and impaired decision-making, but since they lack an XAI system, their full potential cannot be harnessed. This can be solved by implementing the YOLOv5 real-time object detection model and incorporating AI explainability approaches to improve the transparency of AVN&#8217;s decision-making.</p><p id=\"Par13\">YOLOv5 operates as a lightweight and real-time object detection model that achieves a balance between speed and accuracy for applications like AVNs that require prompt decision-making<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref>,<xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>. However, YOLOv5 performs well in overall perception tasks in real-time applications, but it does not come with a method for explaining the decision-making process it undertakes.</p><p id=\"Par14\">To tackle this, XAI methods such as gradient-based saliency, perturbation-based heatmaps, and occlusion sensitivity are implemented into YOLOv5, allowing the AVNs to provide human-understandable reasons for actions like changing lanes, halting at signal-controlled intersections, or rerouting caused by an obstacle. The integration of explainability into the decision-making process within AVN also enhances the safety factor of the system, as the actions executed by AI can be verified and justified. This is illustrated in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>, which presents an AVN communication system involving Vehicle-to-Infrastructure (V2I) interaction essential for informed decision-making.</p><p id=\"Par15\">\n<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>AVN communication system<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e318\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig1_HTML.jpg\"/></fig></p><p id=\"Par16\">However, operational situations are in complex and unconstrained environments where a timely decision can mean the difference between life and death. For instance, when managing traffic, navigating around a crowded junction, dodging other vehicles on the road, or responding to any obstacles on the road, AVNs must determine various criteria, such as speed limits, surface types, and surrounding traffic. As shown in Fig. <xref rid=\"Fig1\" ref-type=\"fig\">1</xref>, the intelligent communication system allows AVNs to communicate and check the reliability of nearby vehicles to develop collective decision-making<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>. This reduces uncertainty, minimizes risk, and enhances predictability in areas with high traffic density. An XAI-integrated AVN framework enables intelligent and data-driven decision-making, as well as explainable reasoning, to reduce the trust deficit in self-driving cars among passengers, pedestrians, and regulatory authorities.</p><p id=\"Par17\">This study introduces an XAI-driven AVN framework that leverages YOLOv5-based object detection, alongside explainability techniques, to enhance road safety, transparency, and public confidence in autonomous mobility. The significant contributions of this study are as follows: (1) the implementation of the interpretable AVN model for real-time traffic and hazard estimation based on the introduced approach; (2) refining vehicle decision-making using the YOLOv5 algorithm while including the XAI explanation; (3) fulfilling the requirements of the authorities and the public and making the AVN&#8217;s actions traceable. The objective of this research is to design a foundation for a trustworthy, explainable, and safety-focused AI-powered transportation system that facilitates the integration of AVNs into smart city implementation plans.</p><sec id=\"Sec2\"><title>Research questions and objectives</title><p id=\"Par18\">This study addresses:</p><p id=\"Par19\">\n<list list-type=\"bullet\"><list-item><p id=\"Par20\">\n<bold>RQ1.</bold> What level of detection performance and real-time throughput can a YOLOv5-based perception module achieve for AVNs under a unified evaluation protocol (dataset<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>?</p></list-item><list-item><p id=\"Par21\">\n<bold>RQ2.</bold> To what extent do XAI methods (occlusion, perturbation, gradient-based saliency) provide faithful, localized attributions that clarify the model&#8217;s decisions?</p></list-item><list-item><p id=\"Par22\">\n<bold>RQ3.</bold> How sensitive is performance to inference hyperparameters (image size, confidence threshold, NMS IoU) and lightweight preprocessing, and which operating point best balances accuracy and latency (with uncertainty reporting via 95% CIs)?</p></list-item><list-item><p id=\"Par23\">\n<bold>RQ4.</bold> What planner-ready signals (bounding boxes, class labels, confidences, saliency maps) should be exported for downstream planning/control integration?</p></list-item></list></p></sec><sec id=\"Sec3\"><title>Literature review</title><p id=\"Par24\">This research suggests that integrating XAI methods with YOLOv5 object detection enhances the AVN decision-making process by making it more explainable and accurate. In this section, the authors discuss the limitations of black-box models in AVNs and describe how XAI can enhance decision interpretability, real-time perception, and vehicle safety with the help of recent advances.</p><p id=\"Par25\">Researchers in<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> examined semantic segmentation problems for driving environments characterized by difficult-to-identify road areas combined with poor traffic rule compliance and objects that vary significantly in type. The India Driving Dataset (IDD) served as the basis for solving complexities that distinguish it from structured urban road data captured in Cityscapes. To improve segmentation performance in such challenging conditions, the authors proposed Eff-UNet, a model that combines EfficientNet as an encoder for feature extraction with UNet&#8217;s decoder for precise reconstruction of the segmentation map. EfficientNet&#8217;s compound scaling optimizes depth, width, and resolution, while UNet&#8217;s skip connections retain essential spatial details. The proposed Eff-UNet outperformed models such as DeepLabV3 + and standard UNet variants, achieving the highest mean Intersection over Union (mIoU) on the IDD Lite dataset and securing first place in the IDD segmentation challenge.</p><p id=\"Par26\">In this research, Porzi et al. (2019)<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup> propose a Feature Pyramid Network (FPN)-based approach for seamless scene segmentation, addressing the challenges of joint semantic and instance segmentation. Previous methods, such as Mask R-CNN for instance segmentation and DeepLab for semantic segmentation, trained separate models, leading to inconsistent labeling and increased computational cost. To overcome this, the authors introduce a unified model that integrates multi-scale contextual features from FPN with a DeepLab-inspired segmentation module. It enhances segmentation quality and brings improvements in redundancy and efficiency, achieving state-of-the-art performance on the Cityscapes and Mapillary Vistas datasets. It is simultaneously demonstrated that the integration of semantics and instances for segmentation has provided better stability and reduced time consumption, thereby making it suitable for real-world applications such as autonomous driving and urban environment perception.</p><p id=\"Par27\">In this research, Singh et al. (2021)<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> evaluate state-of-the-art deep learning models for object detection, semantic segmentation, and instance segmentation on real-world driving datasets, including Cityscapes, Berkeley DeepDrive (BDD), and the IDD. Unlike previous studies that rely on standard datasets such as PASCAL VOC and MS-COCO, this work highlights the challenges of structured and unstructured driving environments. For object detection, RetinaNet performs best on structured datasets, while Faster R-CNN excels in unstructured conditions. While evaluating semantic segmentation, the efficacy of PSPNet, ERFNet, and DRN is compared to each other. However, it has been realized that an optimal segmentation model should have multi-release robust feature representations. Among the three models assessed, DRN stands out as the most stable with both SVHN and Imagenet data. For example, Mask R-CNN performed better on Cityscapes and IDD, while the different Cascade Mask performed much better on BDD. The study demonstrates the impact of domain shift on model generalization and highlights the scarcity of datasets, underscoring the importance of domain adaptation for enhancing segmentation and detection in autonomous driving environments.</p><p id=\"Par28\">In this research<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, Louati et al. (2024) propose a Multi-Agent Reinforcement Learning (MARL) framework to optimize lane-changing strategies for AVs in smart cities. Unlike traditional rule-based methods, their MA2C algorithm enhances cooperation among AVs using parameter-sharing techniques and integrates human-like driving behaviors through IDM and MOBIL models. The results demonstrate that MA2C outperforms existing MARL models, such as MAPPO and MADQN, in terms of traffic efficiency, energy consumption, and passenger safety, thereby reducing congestion and enhancing sustainability in urban mobility. However, they have limitations in that they do not involve real-world traffic environment settings and testing, and the scenarios used are relatively simple and may require additional testing in a complex urban environment with various styles of driving.</p><p id=\"Par29\">In<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>, the authors tackle the complexities of lane-change decisions for AVs on multi-lane expressways by introducing a robust decision-making framework. This framework enhances lateral vehicle stability through phase-plane analysis and state-machine logic, effectively considering surrounding traffic dynamics and vehicle motion states. To maintain a safe distance at all times, a cruising strategy is designed, Including Key safety distance models, speed changes, and transition procedures in emergency situations. Additionally, they suggest planning time polynomial optimization that integrates stability and rollover constraints into the path-tracking controller. Simulation results validate the approach, demonstrating improved safety, stability, and overall driving efficiency in multilane expressway scenarios.</p><p id=\"Par30\">In<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>, the authors address the challenges of executing safe lane changes and merging maneuvers for AVs in dense traffic environments. They introduce a decentralized, controller-agnostic cooperative strategy leveraging Vehicle-to-Vehicle (V2V) communication to facilitate collision-free lane transitions. By enabling real-time coordination among vehicles, their approach significantly enhances traffic efficiency, demonstrating up to a 26% improvement in flow under congested conditions. However, the study also notes an increase in energy consumption in low-traffic scenarios. Compared to existing methods, their strategy exhibits superior safety and efficiency, particularly as the penetration rate of AVs increases, highlighting its potential for large-scale deployment in intelligent transportation systems.</p><p id=\"Par31\">In<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>, the authors present a decision-making and path-planning framework for AVs based on game theory to address the uncertainties associated with dynamic obstacles. Incorporating the application of Stackelberg game theory, the framework determines the successive strategies of other vehicles in various states and potential threats, thereby increasing the level of decision-making. It adopts a potential-field model of navigation which considers various forms of driving behavior and limitations, hence flexibility in any traffic situation. Furthermore, MPC is used to compute the optimal vehicle behavior in the context of safety and fuel consumption optimization. These results also incorporate this approach to social interactions, and the identification of consecutive and emergent traffic patterns could enhance the planning of AV paths in social and technical environments.</p><p id=\"Par32\">Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref> evaluates AVN models based on their decision-making capabilities, interpretability, and efficiency. While models like Eff-UNet and FPN + DeepLab<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup> excel in segmentation, they lack explainability. Reinforcement learning (MA2C) and V2V strategies<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref>,<xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup> enhance traffic flow but are computationally expensive. Game-theoretic and phase-plane methods<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref>,<xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup> enhance stability but require real-world validation. The proposed XAI-based model addresses these challenges by ensuring explainability, efficiency, and regulatory compliance for the deployment of real-time AVNs.</p><p id=\"Par33\">\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Comparative analysis of previously published approaches.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">References</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Preprocessing</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Decision making</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Interpretable AVN Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">YOLOv5 with XAI</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Regulatory &amp; public acceptance</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Outcome</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Positive aspects</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Negative aspects</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baheti et al., 2020<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eff-UNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Semantic segmentation in unstructured environments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Highest mIoU on IDD Lite, 1&#8201;st in IDD challenge</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Superior accuracy over DeepLabV3+</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Limited to unstructured datasets</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Porzi et al., 2019<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">FPN + DeepLab</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Joint semantic &amp; instance segmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved SOTA on Cityscapes &amp; Mapillary Vistas</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved efficiency &amp; consistency</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Higher computational cost</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Singh et al., 2021<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">RetinaNet, Faster R-CNN, PSPNet, DRN, Mask R-CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Object detection, semantic &amp; instance segmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Model performance varies across Cityscapes, BDD, and IDD</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Identifies best models for structured &amp; unstructured environments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Domain shifts impact generalization</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Louati et al., 2024<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">MA2C (MARL)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">MARL</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Outperforms MAPPO &amp; MADQN in traffic flow, energy efficiency &amp; safety</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Reduces congestion &amp; improves sustainability</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Simplified traffic, lacks real-world validation</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tang et al., 2023<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Phase-plane + State Machine</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lane-change decision framework for AVs</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improves stability, safety &amp; efficiency in expressways</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Integrates stability &amp; rollover constraints</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Requires real-world validation</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Monteiro et al., 2023<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">V2V Cooperative Strategy</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Decentralized lane-change coordination</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improves traffic flow by 26% in congestion</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhances safety &amp; efficiency in AV networks</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Higher energy use in low traffic</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Yuan et al., 2023<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Stackelberg Game Theory + MPC</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Game-theoretic decision-making &amp; path planning</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>X</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Not discussed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhances AV navigation in dynamic environments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Handles social interactions &amp; uncertainty</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Computationally intensive</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Linear Discriminant Analysis</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10004;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improving interpretability in AV models</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Interpretability might come at the cost of reduced model complexity, potentially impacting performance.</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Embedding XAI explanations may increase computational overhead, affecting real-time processing efficiency.</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Regulatory compliance and public trust depend on external factors, such as policy changes and public perception, which are difficult to control</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhance model transparency &amp; decision explainability</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ensures better regulatory compliance &amp; trust</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Potential reduction in model complexity &amp; performance</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec4\"><title>Limitations of existing approaches</title><p id=\"Par34\">Evaluating existing approaches highlights several key challenges and limitations in AVNs&#8217; decision-making, affecting their efficiency, interpretability, and adaptability in real-world driving conditions.</p><sec id=\"Sec5\"><title>Lack of explainability and transparency</title><p id=\"Par35\">As stated in<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>,<xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, most existing models have limited integration of the XAI technique, leading to a black box approach to decision-making. This lack of interpretability causes difficulty in understanding their behavior, which raises issues in making compliance with regulations as well as causing low levels of public confidence in the autonomous systems.</p></sec><sec id=\"Sec6\"><title>High computational complexity</title><p id=\"Par36\">Approaches like [19,24] often employ complex multi-layered decision-making processes, leading to high computational costs that hinder their feasibility for real-time autonomous driving applications.</p></sec><sec id=\"Sec7\"><title>Lack of real-world validation and adaptability</title><p id=\"Par37\">Models in [21,22] have been primarily lacking validation in real-world urban and highway driving conditions, raising concerns about their generalizability.</p></sec></sec><sec id=\"Sec8\"><title>Contributions of the proposed model (addressing limitations)</title><p id=\"Par38\">The proposed XAI-based AVN model directly addresses these limitations, offering an interpretable, efficient, and regulation-compliant solution for real-time AV navigation.</p><sec id=\"Sec9\"><title>Enhancing explainability and transparency</title><p id=\"Par39\">Implementing XAI in the decision-making model addresses regulatory requirements and enhances public confidence in the decision-making process by facilitating the auditing of the AV decisions.</p></sec><sec id=\"Sec10\"><title>Improving computational efficiency</title><p id=\"Par40\">In comparison to deep learning-based decision-making models involving high computational complexity, the proposed XAI-based model is far more efficient, which would enhance its capability for implementation in real-time scenarios.</p></sec><sec id=\"Sec11\"><title>Ensuring real-world validation and adaptability</title><p id=\"Par41\">The model provides auditable decision-making and is fully scalable to meet the requirements specified in specific periods of realistic traffic conditions and smart city systems.</p></sec></sec></sec><sec id=\"Sec12\"><title>Proposed methodology</title><p id=\"Par42\">AVNs have several fundamental issues, including low interpretability, high complexity, and difficulty in applying existing methods to real-world scenarios. Most current models have issues with their decision-making mechanisms, creating a challenge in matters concerning transparency and regulation. Additionally, computationally intensive approaches hinder real-time execution, while reliance on simulated testing environments raises concerns about the effectiveness in real-world settings. To address these issues, this research introduces an XAI-based AVN model that enhances explainability, computational efficiency, and adaptability, ensuring more transparent, efficient, and trustworthy decision-making for AVs. Figures&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> and <xref rid=\"Fig3\" ref-type=\"fig\">3</xref>, and <xref rid=\"Fig4\" ref-type=\"fig\">4</xref> represent the smart city communication framework for AVN operations, the abstract model of the proposed XAI-based AVN, and the proposed XAI-based AVN, respectively.</p><p id=\"Par43\">\n<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Smart city communication for AVs operations.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e796\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig2_HTML.jpg\"/></fig>\n</p><p id=\"Par44\">Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref> illustrates a smart city scenario in which continuous information exchange occurs among vehicles, infrastructure, and smart buildings to enable automated traffic control and safe autonomous driving. The figure illustrates various interaction types: V2V communication (green lines) enables cars to exchange information about their positions and movements, thereby preventing collisions. V2I communication (purple lines) connects vehicles with traffic lights and Road-Side Units (RSUs) for real-time traffic updates. Infrastructure-to-Infrastructure (I2I) communication (red lines) links different components, such as the traffic control center and base stations, to manage traffic flow efficiently. Additionally, the Vehicle Sensing Radius Propagation (blue circles) represents each vehicle&#8217;s detection zone for identifying Vulnerable Road Users (VRUs), such as pedestrians and cyclists, ensuring timely reactions to potential hazards. It helps to improve general situational awareness and enables AVs to work safely within the smart city context, responding to changes in the environment and signals. This communication fabric provides real-time context to perception and distributes its outputs across the AVN. Building upon the communication structure shown in Figs.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> and <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> abstracts the AVN data processing pipeline, which is further detailed through the proposed XAI-based model illustrated in Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref>.</p><p id=\"Par45\">\n<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Abstract model of the proposed XAI-based AVs network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e820\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig3_HTML.jpg\"/></fig>\n</p><p id=\"Par46\">Figure <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> illustrates the data processing pipeline for the AVN, which is built upon the data collected through V2V, V2I, and I2I interactions, as shown in Figs.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref> and <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>. The collected real-time vehicular data undergoes three key stages: Data Pre-Processing to filter noise and extract relevant features, Data Processing to analyze patterns and predict behaviors, and Data Post-Processing to refine the results for accuracy. This processed data is then transmitted to the Cloud Computing platform for storage and computational tasks. In the Validation Phase, the insights are cross-checked with real-time data to ensure reliability before being deployed to the AVN, enabling intelligent, adaptive, and safe vehicle operations within the smart city. Building on this abstraction, Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref> details the concrete YOLOv5-based perception and XAI workflow that instantiates these stages.</p><p id=\"Par47\">\n<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Proposed XAI based AVN model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e844\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig4_HTML.jpg\"/></fig>\n</p><p id=\"Par48\">Figure <xref rid=\"Fig4\" ref-type=\"fig\">4</xref> illustrates the detailed workflow of the proposed model, starting from the Vehicular Data Input Layer and progressing through various stages to generate predictions for the AVN. This operationalizes Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref> as a concrete YOLOv5 &#8594; XAI &#8594; validation pipeline. The process begins with the collection of vehicular data, which is fed into the Preprocessing Layer. This layer prepares vehicular data by applying Red, Green, and Blue (RGB) conversion, image resizing, grayscale/Hue, Saturation, Value (HSV) transformation, bounding box adjustments, histogram processing, normalization/heatmap generation, and torchvision transforms. These steps help maintain data integrity and increase object detection, as well as image separation and contrast. It applies to dataset design, creating an effective dataset for dependable real-time decision-making in AVs. Standard preprocessing steps illustrated in Figs.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, <xref rid=\"Fig6\" ref-type=\"fig\">6</xref>, <xref rid=\"Fig7\" ref-type=\"fig\">7</xref>, <xref rid=\"Fig8\" ref-type=\"fig\">8</xref>, <xref rid=\"Fig9\" ref-type=\"fig\">9</xref> and <xref rid=\"Fig10\" ref-type=\"fig\">10</xref> are essential for ensuring input consistency, robustness, and effective explainability in AV perception. Techniques such as resizing, grayscale conversion, histogram equalization, HSV transformation, and edge detection were applied to improve data quality under real-world conditions and to support downstream detection and XAI analysis. HSV (lighting-robust via chromaticity&#8211;luminance decoupling), histogram equalization (which raises local contrast to enhance small/low-contrast objects), and edge detection (which emphasizes saliency-aligned boundaries) collectively standardize inputs and produce sharper, localized attributions&#8212;improving robustness and interpretability.</p><p id=\"Par49\">Although the proposed model focuses primarily on object detection and explainability, it is designed to integrate with standard decision-making modules in autonomous vehicles seamlessly. The outputs of the YOLOv5&#8201;+&#8201;XAI block (detected objects and their human-interpretable explanations) can be supplied as inputs to conventional path planning and control strategies (e.g., rule-based state machines, model predictive planners, and PID or Pure-Pursuit controllers). In this way, the transparency achieved at the perception layer can also be extended to the vehicle&#8217;s subsequent actions, ensuring that braking, lane changes, or yielding decisions can be traced back to both the detected objects and their XAI-based rationales.</p><p id=\"Par50\">Figure <xref rid=\"Fig5\" ref-type=\"fig\">5</xref> depicts the detected objects, such as vehicles and infrastructure, with red bounding boxes, therefore outlining the preprocessing stage. This step utilizes object recognition operations that allow for recognizing important elements of the driving environment, which forms the basis for further processes such as resizing activity, color space transformation, and feature extraction.</p><p id=\"Par51\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Object detection with red bounding boxes.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e888\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig5_HTML.jpg\"/></fig>\n</p><p id=\"Par52\">\n<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Image resizing to a standard resolution.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e899\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig6_HTML.jpg\"/></fig>\n</p><p id=\"Par53\">\n<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Grayscale conversion for simplified feature extraction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e909\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig7_HTML.jpg\"/></fig>\n</p><p id=\"Par54\">\n<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Histogram processing for color and intensity analysis.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e919\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig8_HTML.jpg\"/></fig>\n</p><p id=\"Par55\">\n<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>HSV conversion for robust object detection in varying lighting.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e929\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig9_HTML.jpg\"/></fig>\n</p><p id=\"Par56\">\n<fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>Edge detection for object boundary identification.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e939\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig10_HTML.jpg\"/></fig>\n</p><p id=\"Par57\">Figure <xref rid=\"Fig6\" ref-type=\"fig\">6</xref> shows the image resizing process, where the original images are resized to a standard 224&#8201;&#215;&#8201;224 resolution. This resizing brings uniformity to the structures, also saves computational costs for training the models, while maintaining important features of the objects as described by the red bounding boxes.</p><p id=\"Par58\">Figure <xref rid=\"Fig7\" ref-type=\"fig\">7</xref> illustrates the conversion process from RGB to grayscale, where color information is discarded while intensity information is retained. This step reduces complexity, improves object detection, and concentrates primarily on the structural and texture data required by AVs.</p><p id=\"Par59\">Figure <xref rid=\"Fig8\" ref-type=\"fig\">8</xref> shows the histogram processing stage, where pixel intensity distributions for the RGB color channels are analyzed. Histograms enable people to define brightness, contrast, and color differences in images, as these factors can affect visibility and the likelihood of detecting an object. This process enables the model to better balance pixel intensity levels, thereby increasing its reliability when operating under various lighting conditions.</p><p id=\"Par60\">Figure <xref rid=\"Fig9\" ref-type=\"fig\">9</xref> represents the conversion to the HSV color space that converts the original images to the HSV color system. This separation divides color content from luminance, which enhances the model&#8217;s capacity to deal with lighting changes and recognize objects irrespective of lighting intensity. The hue component deals with the color, saturation specifies the intensity of the color, and value captures brightness, which also helps in identifying objects in varying illumination.</p><p id=\"Par61\">Figure <xref rid=\"Fig10\" ref-type=\"fig\">10</xref> also illustrates the edge detection process, with the help of which, boundaries of objects within images are extracted using the intensity gradient. This technique helps the model identify various objects by detecting sharp changes in pixel intensity. Edge detection is helpful for tasks such as lane detection, object detection, and obstacle detection, as the edges provide the model with a clearer structure from the images it receives.</p><p id=\"Par62\">These preprocessing techniques were carefully selected to enhance object detection robustness under varying lighting conditions, diverse traffic environments, and to ensure the model&#8217;s adaptability to real-world AVN scenarios. The preprocessed data is then further split into a training set, comprising 70% of the data, and a testing set, which comprises 30%. The training set is used in the selection of deep learning models, while the test set is used in the assessment of the models. The Deep Learning phase utilizes a YOLOv5 algorithm for object identification and prediction. The YOLOv5 model in this study was selected due to its efficiency in real-time detection and its suitability for AVNs. The model divides input images into grids and performs object detection with low latency, a feature critical for making time-sensitive driving decisions. Once trained, the model processes frames in a single forward pass, enabling the detection of vehicles, pedestrians, and traffic signs in real-time. Once predictions are made, various XAI methods come into play to provide a simpler explanation that helps improve the predictability of the model&#8217;s desired outcome.</p><p id=\"Par63\">The YOLOv5 algorithm operates by dividing the input image into an <inline-formula id=\"IEq1\"><tex-math id=\"d33e971\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:S\\times\\:S$$\\end{document}</tex-math></inline-formula> grid, where each grid cell is responsible for predicting bounding boxes, object presence, and class probabilities. The following equations describe the core components of the object detection process:</p><p id=\"Par64\">It predicts bounding boxes with the following parameters:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e977\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\widehat{y}=(x,y,w,h,c)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par65\">Where <inline-formula id=\"IEq2\"><tex-math id=\"d33e983\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:x,y$$\\end{document}</tex-math></inline-formula> represent the center coordinates of the predicted bounding box (normalized in [0,1]), <inline-formula id=\"IEq3\"><tex-math id=\"d33e987\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:w,h$$\\end{document}</tex-math></inline-formula> represent the width and height of the box, and <inline-formula id=\"IEq4\"><tex-math id=\"d33e991\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:c\\in\\:\\left[\\text{0,1}\\right]$$\\end{document}</tex-math></inline-formula> denotes the confidence score indicating the probability of object presence.</p><p id=\"Par66\">The objectness score measures the likelihood of an object being present in a grid cell:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e997\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\text{O}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\text{n}\\text{e}\\text{s}\\text{s}\\:\\text{S}\\text{c}\\text{o}\\text{r}\\text{e}=\\text{P}\\left(\\text{o}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\right)\\text{x}{\\text{I}\\text{O}\\text{U}}_{\\text{p}\\text{r}\\text{e}\\text{d},\\text{t}\\text{r}\\text{u}\\text{t}\\text{h}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par67\">Where <inline-formula id=\"IEq5\"><tex-math id=\"d33e1003\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:P\\left(\\text{o}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\right)$$\\end{document}</tex-math></inline-formula> is the probability of object existence in the grid cell, <inline-formula id=\"IEq6\"><tex-math id=\"d33e1007\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\text{I}\\text{O}\\text{U}}_{\\text{p}\\text{r}\\text{e}\\text{d},\\text{t}\\text{r}\\text{u}\\text{t}\\text{h}}$$\\end{document}</tex-math></inline-formula> represents the Intersection over Union (IoU) between the predicted bounding box <inline-formula id=\"IEq7\"><tex-math id=\"d33e1011\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{pred}$$\\end{document}</tex-math></inline-formula> and the ground truth box <inline-formula id=\"IEq8\"><tex-math id=\"d33e1015\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{b}_{gt}$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par68\">It predicts the class of the detected objects using:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e1021\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:P\\left({class}_{i}\\right|object)\\forall\\:i\\in\\:\\{\\text{1,2},\\dots\\:,k\\}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par69\">Where <inline-formula id=\"IEq9\"><tex-math id=\"d33e1027\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:k$$\\end{document}</tex-math></inline-formula> denotes the number of object classes (e.g., vehicles, pedestrians, traffic signs), and <inline-formula id=\"IEq10\"><tex-math id=\"d33e1031\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:P\\left({class}_{i}\\right|object)$$\\end{document}</tex-math></inline-formula> is the probability of the detected object belonging to class <inline-formula id=\"IEq11\"><tex-math id=\"d33e1035\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:i$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par70\">The loss function minimizes the error across localization, object confidence, and classification:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e1041\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{total}={\\lambda\\:}_{coord}{\\mathcal{L}}_{coord}+{\\mathcal{L}}_{obj}+{\\mathcal{L}}_{noobj}+{\\mathcal{L}}_{class}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par71\">Where <inline-formula id=\"IEq12\"><tex-math id=\"d33e1047\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{coord}$$\\end{document}</tex-math></inline-formula> represents the localization loss, penalizing the errors in bounding box predictions:<disp-formula id=\"Equa\"><tex-math id=\"d33e1051\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{coord}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{obj}[{\\left({x}_{i}-{\\widehat{x}}_{i}\\right)}^{2}+{\\left({y}_{i}-{\\widehat{y}}_{i}\\right)}^{2}+{\\left(\\sqrt{{w}_{i}}-\\sqrt{{\\widehat{w}}_{i}}\\right)}^{2}+{\\left(\\sqrt{{h}_{i}}-\\sqrt{{\\widehat{h}}_{i}}\\right)}^{2}]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par72\"><inline-formula id=\"IEq13\"><tex-math id=\"d33e1055\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{obj}$$\\end{document}</tex-math></inline-formula> denotes objectness loss, measuring prediction confidence:<disp-formula id=\"Equb\"><tex-math id=\"d33e1059\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{obj}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{obj}{\\left({c}_{i}-{\\widehat{c}}_{i}\\right)}^{2}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par73\"><inline-formula id=\"IEq14\"><tex-math id=\"d33e1063\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{noobj}$$\\end{document}</tex-math></inline-formula>&#8203; is the no-object loss, penalizing false positives in empty grid cells:<disp-formula id=\"Equc\"><tex-math id=\"d33e1067\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{noobj}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{noobj}{\\left({c}_{i}-{\\widehat{c}}_{i}\\right)}^{2}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par74\"><inline-formula id=\"IEq15\"><tex-math id=\"d33e1072\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{class}$$\\end{document}</tex-math></inline-formula>&#8203; is the classification loss, which evaluates the predicted class probabilities:<disp-formula id=\"Equd\"><tex-math id=\"d33e1076\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mathcal{L}}_{class}=\\sum\\:_{i=0}^{{S}^{2}}{1}_{i}^{obj}\\sum\\:_{c\\in\\:classes}{({p}_{i}\\left(c\\right)-{\\widehat{p}}_{i}(c\\left)\\right)}^{2}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par75\">Here, <inline-formula id=\"IEq16\"><tex-math id=\"d33e1081\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:S\\times\\:S$$\\end{document}</tex-math></inline-formula> is the grid size, <inline-formula id=\"IEq17\"><tex-math id=\"d33e1085\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:B$$\\end{document}</tex-math></inline-formula> is the number of bounding boxes per grid cell, <inline-formula id=\"IEq18\"><tex-math id=\"d33e1089\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{1}_{ij}^{obj}$$\\end{document}</tex-math></inline-formula>&#8203; is an indicator if an object exists in cell <inline-formula id=\"IEq19\"><tex-math id=\"d33e1093\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:i$$\\end{document}</tex-math></inline-formula> for box <inline-formula id=\"IEq20\"><tex-math id=\"d33e1097\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:j$$\\end{document}</tex-math></inline-formula>, and <inline-formula id=\"IEq21\"><tex-math id=\"d33e1102\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{1}_{ij}^{noobj}$$\\end{document}</tex-math></inline-formula>&#8203; is an indicator if no object exists. <inline-formula id=\"IEq22\"><tex-math id=\"d33e1106\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:({x}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq23\"><tex-math id=\"d33e1110\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{y}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq24\"><tex-math id=\"d33e1114\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{w}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq25\"><tex-math id=\"d33e1118\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{h}_{i})$$\\end{document}</tex-math></inline-formula> are ground-truth parameters, <inline-formula id=\"IEq26\"><tex-math id=\"d33e1122\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:({\\widehat{x}}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq27\"><tex-math id=\"d33e1127\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\widehat{y}}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq28\"><tex-math id=\"d33e1131\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\widehat{w}}_{i}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq29\"><tex-math id=\"d33e1135\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\widehat{h}}_{i})$$\\end{document}</tex-math></inline-formula> are predicted parameters, <inline-formula id=\"IEq30\"><tex-math id=\"d33e1139\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{c}_{i},{\\widehat{c}}_{i}$$\\end{document}</tex-math></inline-formula> denote ground truth vs. predicted confidence, <inline-formula id=\"IEq31\"><tex-math id=\"d33e1143\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{p}_{i}\\left(c\\right),{\\widehat{p}}_{i}\\left(c\\right)$$\\end{document}</tex-math></inline-formula> are true vs. predicted class probabilities, and <inline-formula id=\"IEq32\"><tex-math id=\"d33e1147\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\lambda\\:}_{coord}$$\\end{document}</tex-math></inline-formula>&#8203; is a weighting factor for localization loss.</p><p id=\"Par76\">The loss function aligns with the original YOLOv5 implementation to ensure reproducibility and fair benchmarking. After the YOLOv5 algorithm generates predictions, XAI techniques like Occlusion Sensitivity Heatmaps, Perturbation-Based Saliency Maps, and Gradient-Based Saliency Maps are applied to provide human-understandable insights into the model&#8217;s decision-making process. The XAI methods ensure that the detected objects and their classifications are transparent and interpretable, enhancing trust and facilitating better regulatory compliance in AVNs.</p><p id=\"Par77\">The Occlusion Sensitivity method systematically blocks (occludes) regions of an image and observes how the model&#8217;s confidence in the detected objects changes. The importance of each region is calculated as:<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e1155\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{occ}\\left(x,y\\right)={P}_{orig}-{P}_{occ}(x,y)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par78\">Where <inline-formula id=\"IEq33\"><tex-math id=\"d33e1161\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{occ}\\left(x,y\\right)$$\\end{document}</tex-math></inline-formula> is the occlusion sensitivity score at pixel location <inline-formula id=\"IEq34\"><tex-math id=\"d33e1165\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(x,y)$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq35\"><tex-math id=\"d33e1169\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{P}_{orig}$$\\end{document}</tex-math></inline-formula> is the original YOLOv5 object confidence score before occlusion, and <inline-formula id=\"IEq36\"><tex-math id=\"d33e1173\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{P}_{occ}(x,y)$$\\end{document}</tex-math></inline-formula> is the YOLOv5 confidence score after occluding region <inline-formula id=\"IEq37\"><tex-math id=\"d33e1177\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(x,y)$$\\end{document}</tex-math></inline-formula>. A heatmap is generated based on these scores, highlighting regions crucial for object detection.</p><p id=\"Par79\">The Perturbation-Based method modifies image regions by adding noise or blurring to analyze how the model&#8217;s confidence changes:<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e1183\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{pert}\\left(x,y\\right)={P}_{orig}-{P}_{pert}\\left(x,y\\right)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par80\">Where <inline-formula id=\"IEq38\"><tex-math id=\"d33e1189\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{pert}\\left(x,y\\right)$$\\end{document}</tex-math></inline-formula> is the perturbation saliency score, and <inline-formula id=\"IEq39\"><tex-math id=\"d33e1193\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{P}_{pert}\\left(x,y\\right)$$\\end{document}</tex-math></inline-formula> is the confidence score after perturbing pixel <inline-formula id=\"IEq40\"><tex-math id=\"d33e1197\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(x,y)$$\\end{document}</tex-math></inline-formula>. A saliency map is generated, highlighting areas where modifications have a significant impact on the model&#8217;s predictions.</p><p id=\"Par81\">Gradient-based methods compute the sensitivity of YOLOv5&#8217;s output to input pixel changes using backpropagation gradients:<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e1203\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{grad}\\left(x,y\\right)=\\left|\\frac{\\partial\\:{P}_{obj}}{\\partial\\:\\text{I}(\\text{x},\\text{y})}\\right|$$\\end{document}</tex-math></disp-formula></p><p id=\"Par82\">Where <inline-formula id=\"IEq41\"><tex-math id=\"d33e1209\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{S}_{grad}\\left(x,y\\right)$$\\end{document}</tex-math></inline-formula> is the gradient saliency score for pixel <inline-formula id=\"IEq42\"><tex-math id=\"d33e1213\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(x,y)$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq43\"><tex-math id=\"d33e1217\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{P}_{obj}$$\\end{document}</tex-math></inline-formula>is the object confidence score, <inline-formula id=\"IEq44\"><tex-math id=\"d33e1221\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:I(x,y)$$\\end{document}</tex-math></inline-formula> is the input pixel intensity, and <inline-formula id=\"IEq45\"><tex-math id=\"d33e1225\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\frac{\\partial\\:{P}_{obj}}{\\partial\\:\\text{I}(\\text{x},\\text{y})}$$\\end{document}</tex-math></inline-formula> represents the gradient of the confidence score with respect to input pixel intensity. This method highlights pixels that strongly influence object detection, creating a gradient-based saliency map. These predictions and explanations are then validated, if the model&#8217;s learning rate does not meet performance criteria, the deep learning phase is retrained. If the learning rate meets these explained patterns, they are sent to Cloud Computing for storage and further analysis.</p><p id=\"Par83\">The next step is the Validation Phase, where the trained model is applied to real-time vehicular data. The XAI techniques are imported from the cloud to validate the predictions. If validation is successful, the projections are forwarded to the AVN for decision-making. In cases where predictions are unreliable, the system discards the output to prevent incorrect actions.</p><p id=\"Par84\">Ultimately, this process ensures that the AVN receives accurate, explainable, and reliable predictions based on real-time and historical data, enhancing the safety and efficiency of AVs in dynamic environments.</p></sec><sec id=\"Sec13\"><title>Simulation results</title><p id=\"Par85\">In recent times, there have been enhancements in object detection for AVs, but issues that have lacked a solution include latency, false positives, and explainability. There are several issues with traditional models, including lighting variations, occlusions, and shifts in traffic density, which reduce their reliability. To address this, simulations were conducted using the YOLOv5 model, trained on the Berkeley Labeled Dataset for Autonomous Driving, sourced from the Kaggle dataset<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> with a 70/30 split, using standard settings without architectural changes to ensure reproducibility and fair benchmarking.</p><p id=\"Par86\">The dataset consists of 212 labelled images in YOLOv5 format, including 179 for training and 33 for validation, covering common AVN classes such as vehicles, pedestrians, and traffic signs. The images span urban streets, highways, and intersections under varied illumination conditions (daytime, dusk/night, shadows) with a range of traffic densities and partial occlusions. The class distribution is vehicle-dominant, with fewer instances of pedestrians and signs. This provides a compact yet representative slice of routine AV perception, while coverage of extreme weather (rain/snow/fog) and rare corner cases remains limited. All images were paired with corresponding label files, and class consistency was programmatically verified to ensure data integrity. The modest sample size was mitigated by fine-tuning a pre-trained YOLOv5 model with standard data augmentation techniques, which are more effective compared to existing methods and increase classification accuracy and reliability (horizontal flips, moderate scaling/translations, HSV color jitter) under Ultralytics. Robustness was assessed via a validation sweep across image size and thresholds, with performance peaking at imgsz&#8201;=&#8201;320 (mAP@0.50&#8201;=&#8201;0.623, mAP@0.50&#8211;0.95&#8201;=&#8201;0.278), indicating stable behavior despite limited data. XAI techniques were applied to enhance model transparency. Results confirm the system&#8217;s ability to provide real-time, accurate, and interpretable decisions for safer autonomous navigation. All key detection metrics are reported with variability (mean&#8201;&#177;&#8201;SD) and 95% confidence intervals, as demonstrated in Tables&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref> and <xref rid=\"Tab4\" ref-type=\"table\">4</xref>.</p><p id=\"Par87\">\n<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>YOLOv5-based object detection for AVN driving (<bold>a</bold>). YOLOv5-based object detection for AVN driving (<bold>b</bold>). YOLOv5-based object detection for AVN driving (<bold>c</bold>). YOLOv5-based object detection for AVN driving (<bold>d</bold>). YOLOv5-based object detection for AVN driving (<bold>e</bold>). YOLOv5-based object detection for AVN driving (<bold>f</bold>). YOLOv5-based object detection for AVN driving (<bold>g</bold>). YOLOv5-based object detection for AVN driving (<bold>h</bold>). YOLOv5-based object detection for AVN driving (<bold>i</bold>).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1286\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig11_HTML.jpg\"/></fig>\n</p><p id=\"Par88\">Figure <xref rid=\"Fig11\" ref-type=\"fig\">11</xref>(a&#8211;i) illustrates the YOLOv5-based object detection results across various AVNs driving conditions. The model effectively identifies and classifies multiple objects, primarily vehicles, with red bounding boxes highlighting Class 1 (vehicles) and yellow bounding boxes indicating Class 2 (roadside elements such as traffic signs and poles). Each detection is accompanied by a confidence score ranging from 0.81 to 0.99, indicating a high degree of certainty in object identification.</p><p id=\"Par89\">The results showcase the model&#8217;s robustness across varying traffic conditions and environmental settings, including highways, urban streets, intersections, and low-light scenarios. In dense traffic conditions (Fig.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>d and f, and <xref rid=\"Fig11\" ref-type=\"fig\">11</xref>i), the system effectively distinguishes and tracks multiple vehicles with high confidence, ensuring precise localization. Additionally, the model maintains strong detection performance in nighttime and low-light conditions (Fig.&#160;<xref rid=\"Fig10\" ref-type=\"fig\">10</xref>e), demonstrating its ability to adapt to challenging visibility constraints. The consistently high confidence scores highlight the stability and precision of the YOLOv5 algorithm, reinforcing its suitability for real-world AVN applications.</p><p id=\"Par90\">The proposed YOLOv5-based model achieves an average accuracy of 94.5%, maintaining over 85% even in challenging conditions, ensuring reliable real-time object detection for autonomous driving.</p><p id=\"Par91\">The proposed model employs occlusion sensitivity, perturbation-based, and gradient saliency maps due to their low computational overhead and suitability for real-time object detection. These techniques provide localized visual interpretability and integrate seamlessly with the trained model, supporting the rapid decision-making demands of AVs.</p><p id=\"Par92\">\n<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p>Occlusion sensitivity heatmap highlighting key regions in YOLOv5 detection.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1316\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig12_HTML.jpg\"/></fig>\n</p><p id=\"Par93\">Figure <xref rid=\"Fig12\" ref-type=\"fig\">12</xref> showcases an XAI approach applied to YOLOv5-based object detection using Occlusion Sensitivity Heatmaps. The left side shows the original highway image, while the right displays the Heatmap visualization, where bright yellow and red areas indicate the most critical regions influencing object detection. This technique enhances transparency and trust by revealing which parts of the image contribute most to model predictions. By making the detection process interpretable, this XAI approach ensures safer, more reliable autonomous navigation in real-world driving scenarios.</p><p id=\"Par94\">\n<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig. 13</label><caption><p>Perturbation-based saliency map revealing critical areas in YOLOv5 detection.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1331\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_25993_Fig13_HTML.jpg\"/></fig>\n</p><p id=\"Par95\">Figure <xref rid=\"Fig13\" ref-type=\"fig\">13</xref> presents an XAI approach using a Perturbation-Based Saliency Map to analyze YOLOv5-based object detection. The left side shows the original image, while the right side highlights important regions where pixel perturbations significantly impact detection confidence. The colored grid-like blocks indicate areas where modifications affect the model&#8217;s decisions, making it a crucial tool for understanding model behavior. This approach enhances transparency and reliability, ensuring safer and more interpretable AV navigation.</p><p id=\"Par96\">XAI overlays (occlusion/perturbation/gradient) accompany each detection as rationale artifacts&#8212;heatmaps and scalar scores&#8212;forming an audit-ready trail for incident analysis and regulatory reporting. Sanity checks also curb misinterpretation and strengthen stakeholder trust.</p><p id=\"Par97\">\n<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Comparison of the proposed XAI based AVN model with previously published approaches.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">References</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Miss-rate (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Yara&#351;, N., 2020<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">BIT Vehicle Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet34</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">77%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">33</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tas, S. et al., 2022<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">7.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Chowdhury, S. et al., 2022<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom Bangladeshi Vehicle Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">YOLOv5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.02%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">16.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Won, M., 2020<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">9.15</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mazhar, T. et al., 2023<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Electric Vehicle Charging Dataset Kaggle</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">RF</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">85</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">15</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cao, X. et al., 2011<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Linear SVM Classification</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Yang, B. et al., 2014<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Fixed threshold state machine algorithm</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">6.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed XAI based AVN model</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par98\">Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> shows that, on a dataset<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> under a unified evaluation protocol, the Proposed XAI-based AVN model achieves 99% accuracy with only a 1% miss rate; because the prior approaches<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup> report results on heterogeneous datasets and metrics&#8212;although they pursue the same overarching AVN-perception objective&#8212;Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> is presented as purpose-aligned context rather than a head-to-head benchmark. This highlights its superior reliability in AVN classification and intelligent transportation systems.</p><sec id=\"Sec14\"><title>Ablation and sensitivity analysis</title><p id=\"Par99\">To assess robustness and component-wise contribution, we performed a targeted ablation and sensitivity study using the validation split. Varying the image size and thresholds shows a clear accuracy&#8211;efficiency trade-off: increasing resolution from 224 to 320 improves mAP@0.50 from ~&#8201;0.45 to 0.623 (best at imgsz&#8201;=&#8201;320, conf&#8201;=&#8201;0.70, IoU&#8201;=&#8201;0.45; mAP@0.50&#8201;&#8722;&#8201;0.95&#8201;&#8776;&#8201;0.278), while 416 offers marginal additional accuracy with higher computational cost; therefore, 320 is selected for real-time AVN deployment. Table&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref> summarizes the complete validation sensitivity sweep (imgsz/conf/IoU).</p><p id=\"Par100\">\n<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Validation sensitivity sweep (imgsz/confidence/IoU; validation split).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">imgsz</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">conf</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">mAP@0.50</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">mAP@0.50&#8211;0.95</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.403</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.189</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.508</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.387</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.439</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.402</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.190</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.474</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.387</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.426</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.389</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.185</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.419</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.379</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.398</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.448</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.210</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.508</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.387</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.439</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.439</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.208</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.474</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.387</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.426</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.414</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.198</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.419</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.379</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.398</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.466</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.217</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.503</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.366</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.424</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.452</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.213</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.466</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.366</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.410</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">224</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.432</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.205</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.406</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.370</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.387</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.539</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.246</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.675</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.496</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.572</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.535</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.244</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.673</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.498</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.572</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.508</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.239</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.626</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.496</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.553</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.592</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.266</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.675</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.496</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.572</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.587</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.262</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.673</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.498</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.572</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.539</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.255</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.626</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.496</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.553</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>320</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.70</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.45</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.623</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.278</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.691</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.494</bold>\n</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>0.576</bold>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.622</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.278</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.681</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.498</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.575</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">320</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.70</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.600</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.277</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.620</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.498</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.552</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">416</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.45</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.586</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.278</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.569</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.601</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.585</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">416</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.587</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.279</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.561</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.601</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.580</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par101\">Preprocessing ablation (proxy on 50 images) in Table&#160;<xref rid=\"Tab4\" ref-type=\"table\">4</xref> indicates that grayscale and HSV-V-only decrease mean detections by approximately 3% and 5%, respectively, and slightly reduce mean confidence (baseline mean confidence&#8201;&#8776;&#8201;0.583; grayscale&#8201;&#8776;&#8201;0.572; HSV-V-only&#8201;&#8776;&#8201;0.579). In contrast, histogram equalization increases mean detections by approximately 2.7% (baseline &#8594; 21.30 vs. hist_eq &#8594; 21.88) with comparable confidence (&#8776;&#8201;0.577 vs. baseline&#8201;&#8776;&#8201;0.583), supporting its inclusion under challenging illumination conditions.</p><p id=\"Par102\">\n<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Preprocessing ablation (proxy, <italic toggle=\"yes\">N</italic>&#8201;=&#8201;50 images).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Variant</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Mean detections</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Mean confidence</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">baseline_RGB</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.30</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.583</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">grayscale</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20.66</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.572</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">hist_eq</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.88</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.577</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">HSV_V_only</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20.22</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.579</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par103\">For the explainability cost-benefit analysis, a lightweight XAI method timing comparison in Table&#160;<xref rid=\"Tab5\" ref-type=\"table\">5</xref> shows that occlusion produces a larger average confidence drop (&#8776;&#8201;0.0089) than perturbation (&#8776;&#8201;0.0049), with similar runtime per image (&#8776;&#8201;0.13s vs. 0.12s on CPU). Accordingly, occlusion heatmaps are employed for clearer attribution, and perturbation maps when lower latency is preferred.</p><p id=\"Par104\">\n<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>XAI timing (proxy, <italic toggle=\"yes\">N</italic>&#8201;=&#8201;3 images).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">XAI method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Avg confidence drop</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Time (s/image)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">occlusion</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0089</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.131</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">perturbation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0049</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.118</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par105\">As Table&#160;<xref rid=\"Tab6\" ref-type=\"table\">6</xref> shows, precision and recall are accompanied by two-sided Wilson 95% confidence intervals, whereas mAP@0.50 and mAP@0.50&#8211;0.95 include non-parametric bootstrap 95% intervals computed over the validation images (1,000 resamples); pairwise differences between settings (e.g., imgsz&#8201;=&#8201;224 vs. 320) are assessed with paired bootstrap tests.</p><p id=\"Par106\">\n<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Best configuration (with 95% CIs).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Metric</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Point Estimate</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">CI method</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">mAP@0.50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.623</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bootstrap</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">mAP@0.50&#8211;0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.278</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bootstrap</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.691</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Wilson (95%)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.494</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Wilson (95%)</td></tr></tbody></table></table-wrap>\n</p></sec></sec><sec id=\"Sec15\"><title>Conclusion</title><p id=\"Par107\">In recent times, AVs worldwide are facing significant challenges, including a lack of transparency in decision-making, trust concerns from the public, and ensuring real-time safety in dynamic environments. These issues hinder the seamless integration of AVNs into smart city infrastructures. Traditional models primarily focus on improving accuracy and efficiency, but they fall short in providing explainability, which is crucial for building public trust and achieving widespread adoption. To address these limitations, this research introduces an XAI-based YOLOv5 model that integrates XAI techniques to enhance real-time interpretability and transparency in decision-making. It includes a targeted ablation/sensitivity study with uncertainty reporting (95% confidence intervals) to quantify robustness. To enhance the reliability and safety of the model, YOLOv5&#8217;s strong object detection feature is integrated with the principles of XAI. The proposed model achieves a 99% accuracy rate with a 1% missing rate on the evaluation dataset under a unified protocol, which is more effective than existing techniques and enhances classification accuracy and reliability.</p><sec id=\"Sec16\"><title>Limitations and future work</title><p id=\"Par108\">The Berkeley set spans urban streets, highways, and intersections under varied illumination (day/dusk/night) with mixed traffic densities and partial occlusions; it is vehicle-dominant with fewer pedestrian/sign instances. This yields a concise yet representative slice of routine AV perception, while coverage of extreme weather (rain/snow/fog) and rare hazards remains limited. Under crowding/occlusion, small overlaps lower recall when non-maximum suppression mistakenly removes true positives. These scope limits were partially mitigated via standard augmentation and a validation sensitivity sweep, and variability is reported (mean&#8201;&#177;&#8201;SD; 95% CIs).</p><p id=\"Par109\">Future work will scale evaluation to larger, multi-domain datasets and validate under extreme weather and highly dynamic urban traffic, while exploring federated learning for privacy-preserving distributed training<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup> to strengthen robustness, generalization, and scalability; temporal tracking for occlusion recovery, overlap-aware calibrated non-maximum suppression, and concept/counterfactual XAI to resolve multi-object interactions additionally, edge privacy controls for biometrics and location data<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref>,<xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup>, and decentralized identity and auditability via SSI/blockchain with privacy-aware task governance<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup> will be incorporated to enhance data governance and formalize end-to-end privacy/latency budgets with an edge-first design, restricting cloud use to de-identified, asynchronous analytics.</p><p id=\"Par110\">Ethical Consideration:</p><p id=\"Par111\">This study utilizes a publicly available dataset and focuses on explainability at the perception layer. For deployment, privacy should be enforced via edge-first processing and federated learning for model updates, without centralizing raw data, alongside data minimization and auditable, time-bounded retention, which is marked as future work.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>Ali Zaman Malik, Naila Samar Naz, Fahad Ahmed and Muhammad Saleem, have collected data from different resources and contributed to writing&#8212;original draft preparation. Ali Zaman Malik, Muhammad Sajid Farooq, Ateeq Ur Rehman, Waleed M. Ismael and Muhammad Adnan Khan performed formal analysis and Simulation, writing&#8212;review and editing, Naila Samar Naz, Ateeq Ur Rehman, and Muhammad Adnan Khan; performed supervision, Muhammad Sajid Farooq, Fahad Ahmed, Muhammad Saleem, Muhammad Sajid Farooq.; drafted pictures and tables, Ateeq Ur Rehman, Waleed M. Ismael and Muhammad Adnan Khan; performed revisions and improve the quality of the draft. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The data used to support the findings of this study are available from the corresponding authors upon request.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par112\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><mixed-citation publication-type=\"other\">Singh, B., Kaunert, C., Lal, S., Arora, M. K. &amp; Jermsittiparsert, K. Intelligent mobility assimilating IoT in autonomous vehicles: foster sustainable cities and communities. In Designing Sustainable Internet of Things Solutions for Smart Industries. <italic toggle=\"yes\">IGI Global</italic>. 279&#8211;300 (2025).</mixed-citation></ref><ref id=\"CR2\"><label>2.</label><mixed-citation publication-type=\"other\">Shrimal, H. Integration of AI-powered vehicles with smart City infrastructure to transform the future of automotive world. <italic toggle=\"yes\">SAE Technical Paper</italic>.&#160;(2024)</mixed-citation></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Giannaros</surname><given-names>A</given-names></name><etal/></person-group><article-title>Autonomous vehicles: sophisticated attacks, safety issues, challenges, open topics, blockchain, and future directions</article-title><source>J. Cybersecur. Priv.</source><year>2023</year><volume>3</volume><issue>3</issue><fpage>493</fpage><lpage>543</lpage><pub-id pub-id-type=\"doi\">10.3390/jcp3030025</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Giannaros, A. et al. Autonomous vehicles: sophisticated attacks, safety issues, challenges, open topics, blockchain, and future directions. <italic toggle=\"yes\">J. Cybersecur. Priv.</italic><bold>3</bold> (3), 493&#8211;543 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Mohapatra, H. &amp; Dalai, A. K. February. IoT based V2I framework for accident prevention. In 2022 2nd international conference on artificial intelligence and signal processing (AISP) (pp. 1&#8211;4). IEEE. (2022).</mixed-citation></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mohapatra</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Rath</surname><given-names>AK</given-names></name><name name-style=\"western\"><surname>Panda</surname><given-names>N</given-names></name></person-group><article-title>IoT infrastructure for the accident avoidance: an approach of smart transportation</article-title><source>Int. J. Inform. Technol.</source><year>2022</year><volume>14</volume><issue>2</issue><fpage>761</fpage><lpage>768</lpage></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Mohapatra, H., Rath, A. K. &amp; Panda, N. IoT infrastructure for the accident avoidance: an approach of smart transportation. <italic toggle=\"yes\">Int. J. Inform. Technol.</italic><bold>14</bold> (2), 761&#8211;768 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mohapatra</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Rath</surname><given-names>AK</given-names></name></person-group><article-title>An IoT based efficient multi-objective real-time smart parking system</article-title><source>Int. J. Sens. Networks</source><year>2021</year><volume>37</volume><issue>4</issue><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type=\"doi\">10.1504/IJSNET.2021.119483</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Mohapatra, H. &amp; Rath, A. K. An IoT based efficient multi-objective real-time smart parking system. <italic toggle=\"yes\">Int. J. Sens. Networks</italic>. <bold>37</bold> (4), 219&#8211;232 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Naveed, S. A. et al. Enhancing traffic flow and congestion management in smart cities utilizing SVM-based linear regression approach. <italic toggle=\"yes\">Int. J. Adv. Appl. Sci.</italic><bold>11</bold>(10), 166&#8211;175 (2024).</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saleem</surname><given-names>M</given-names></name><etal/></person-group><article-title>Smart cities: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques</article-title><source>Egypt. Inf. J.</source><year>2022</year><volume>23</volume><issue>3</issue><fpage>417</fpage><lpage>426</lpage></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Saleem, M. et al. Smart cities: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques. <italic toggle=\"yes\">Egypt. Inf. J.</italic><bold>23</bold> (3), 417&#8211;426 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><mixed-citation publication-type=\"other\">Tahir, H. A., Alayed, W., Hassan, W. U. &amp; Haider, A. A Novel Hybrid XAI Solution for autonomous vehicles: real-time interpretability through LIME&#8211;SHAP Integration. Sensors, 24(21), p.6776. (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s24216776</pub-id><pub-id pub-id-type=\"pmcid\">PMC11548085</pub-id><pub-id pub-id-type=\"pmid\">39517672</pub-id></mixed-citation></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Khan, M. A. et al. <italic toggle=\"yes\">Smart Buildings: Federated learning-driven secure, Transparent and Smart Energy Management System Using XAI</italic> Vol. 13, pp.2066&#8211;2081 (Energy Reports, 2025).</mixed-citation></ref><ref id=\"CR11\"><label>11.</label><mixed-citation publication-type=\"other\">Saleem, M. et al. <italic toggle=\"yes\">Secure and Transparent Mobility in Smart Cities: Revolutionizing AVNs To Predict Traffic Congestion Using MapReduce</italic> (Private Blockchain and XAI. IEEE Access, 2024).</mixed-citation></ref><ref id=\"CR12\"><label>12.</label><mixed-citation publication-type=\"other\">Zhang, T., Li, W., Huang, W. &amp; Ma, L. Critical roles of explainability in shaping perception, trust, and acceptance of autonomous vehicles. International Journal of Industrial Ergonomics, 100, p.103568. (2024).</mixed-citation></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Berhanu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Alemayehu</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Schr&#246;der</surname><given-names>D</given-names></name></person-group><article-title>Examining car accident prediction techniques and road traffic congestion: a comparative analysis of road safety and prevention of world challenges in low-income and high&#8208;income countries</article-title><source>J. Adv. Transp.</source><year>2023</year><volume>2023</volume><issue>1</issue><fpage>6643412</fpage></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Berhanu, Y., Alemayehu, E. &amp; Schr&#246;der, D. Examining car accident prediction techniques and road traffic congestion: a comparative analysis of road safety and prevention of world challenges in low-income and high&#8208;income countries. <italic toggle=\"yes\">J. Adv. Transp.</italic><bold>2023</bold> (1), 6643412 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Hossain, S., Maggi, E. &amp; Vezzulli, A. Factors influencing the road accidents in low and middle-income countries: a systematic literature review. <italic toggle=\"yes\">Int. J. Injury Control Saf. Promotion</italic>. <bold>31</bold>(2), 294&#8211;322. 10.1080/17457300.2024.2319618 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1080/17457300.2024.2319618</pub-id><pub-id pub-id-type=\"pmid\">38379460</pub-id></mixed-citation></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Sunitha, G., Priya, V. S., Kumar, V. S., Priya, G. G. &amp; Kumar, T. N. August. road object detection using Yolov8. In 2024 5th international conference on electronics and sustainable communication systems (ICESC) (pp. 847&#8211;853). IEEE. (2024).</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><mixed-citation publication-type=\"other\">P&#233;rez, S., G&#243;mez, C. &amp; Rodr&#237;guez, M. Innovative Deep Learning Techniques for Obstacle Recognition: A Comparative Study of Modern Detection Algorithms. arXiv preprint arXiv:2410.10096. (2024).</mixed-citation></ref><ref id=\"CR17\"><label>17.</label><mixed-citation publication-type=\"other\">Madhav, A. S. &amp; Tyagi, A. K. July. Explainable artificial intelligence (XAI): connecting artificial decision-making and human trust in autonomous vehicles. In Proceedings of Third International Conference on Computing, Communications, and Cyber-Security: IC4S 2021 (pp. 123&#8211;136). Singapore: Springer Nature Singapore. (2022).</mixed-citation></ref><ref id=\"CR18\"><label>18.</label><mixed-citation publication-type=\"other\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/harshsprajapati/berkeley-labelled-dataset-for-autonomous-driving\">https://www.kaggle.com/datasets/harshsprajapati/berkeley-labelled-dataset-for-autonomous-driving</ext-link></mixed-citation></ref><ref id=\"CR19\"><label>19.</label><mixed-citation publication-type=\"other\">Baheti, B., Innani, S., Gajre, S. &amp; Talbar, S. Eff-unet: A novel architecture for semantic segmentation in unstructured environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 358&#8211;359). (2020).</mixed-citation></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">Porzi, L., Bulo, S. R., Colovic, A. &amp; Kontschieder, P. Seamless scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8277&#8211;8286). (2019).</mixed-citation></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Singh, D., Rahane, A., Mondal, A., Subramanian, A. &amp; Jawahar, C. V. December. evaluation of detection and segmentation tasks on driving datasets. In International Conference on Computer Vision and Image Processing (pp. 512&#8211;524). Cham: Springer International Publishing. (2021).</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">Louati, A. et al. Sustainable smart cities through multi-agent reinforcement learning-based cooperative autonomous vehicles. Sustainability, 16(5), p.1779. (2024).</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Tang, C., Pan, L., Xia, J. &amp; Fan, S. Research on lane-change decision and planning in multilane expressway scenarios for autonomous vehicles. Machines, 11(8), p.820. (2023).</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Monteiro, F. V. &amp; Ioannou, P. Safe autonomous lane changes and impact on traffic flow in a connected vehicle environment. Transportation research part C: emerging technologies, 151, p.104138. (2023).</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Yuan, Q. et al. Decision-making and planning methods for autonomous vehicles based on multistate estimations and game theory. Advanced Intelligent Systems, 5(11), p.2300177. (2023).</mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Yara&#351;, N. Vehicle type classification with deep learning (Master&#8217;s thesis, Izmir Institute of Technology (Turkey)). (2020).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Tas, S. et al. Deep learning-based vehicle classification for low quality images. Sensors, 22(13), p.4740. (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s22134740</pub-id><pub-id pub-id-type=\"pmcid\">PMC9268885</pub-id><pub-id pub-id-type=\"pmid\">35808251</pub-id></mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Chowdhury, S., Chowdhury, S., Ifty, J. T. &amp; Khan, R. September. Vehicle detection and classification using deep neural networks. In 2022 International Conference on Electrical and Information Technology (IEIT) (pp. 95&#8211;100). IEEE. (2022).</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Won</surname><given-names>M</given-names></name></person-group><article-title>Intelligent traffic monitoring systems for vehicle classification: A survey</article-title><source>IEEE Access.</source><year>2020</year><volume>8</volume><fpage>73340</fpage><lpage>73358</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2020.2987634</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Won, M. Intelligent traffic monitoring systems for vehicle classification: A survey. <italic toggle=\"yes\">IEEE Access.</italic><bold>8</bold>, 73340&#8211;73358 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Mazhar, T. et al. Electric vehicle charging system in the smart grid using different machine learning methods. Sustainability, 15(3), p.2603. (2023).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Cao, X., Wu, C., Yan, P. &amp; Li, X. September. Linear SVM classification using boosting HOG features for vehicle detection in low-altitude airborne videos. In 2011 18th IEEE international conference on image processing (pp. 2421&#8211;2424). IEEE. (2011).</mixed-citation></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Lei</surname><given-names>Y</given-names></name></person-group><article-title>Vehicle detection and classification for low-speed congested traffic with anisotropic magnetoresistive sensor</article-title><source>IEEE Sens. J.</source><year>2014</year><volume>15</volume><issue>2</issue><fpage>1132</fpage><lpage>1138</lpage><pub-id pub-id-type=\"doi\">10.1109/JSEN.2014.2359014</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Yang, B. &amp; Lei, Y. Vehicle detection and classification for low-speed congested traffic with anisotropic magnetoresistive sensor. <italic toggle=\"yes\">IEEE Sens. J.</italic><bold>15</bold> (2), 1132&#8211;1138 (2014).</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><mixed-citation publication-type=\"other\">Yao, A. et al. FedShufde: A privacy preserving framework of federated learning for edge-based smart UAV delivery system. Future Generation Computer Systems, 166, p.107706. (2025).</mixed-citation></ref><ref id=\"CR34\"><label>34.</label><mixed-citation publication-type=\"other\">Fang, K. et al. April. MoCFL: Mobile cluster federated learning framework for highly dynamic network. In Proceedings of the ACM on Web Conference 2025 (pp. 5065&#8211;5074). (2025).</mixed-citation></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Ali, A., Khan, M. A. &amp; Choi, H. Hydrogen storage prediction in dibenzyltoluene as liquid organic hydrogen carrier empowered with weighted federated machine learning. Mathematics, 10(20), p.3846. (2022).</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><mixed-citation publication-type=\"other\">Ali, Y., Han, K. H., Majeed, A., Lim, J. S. &amp; Hwang, S. O. <italic toggle=\"yes\">An Optimal two-step Approach for Defense against Poisoning Attacks in Federated Learning</italic> (IEEE Access, 2025).</mixed-citation></ref><ref id=\"CR37\"><label>37.</label><citation-alternatives><element-citation id=\"ec-CR37\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Majeed</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Hwang</surname><given-names>SO</given-names></name></person-group><article-title>A multifaceted survey on federated learning: Fundamentals, paradigm shifts, practical issues, recent developments, partnerships, trade-offs, trustworthiness, and ways forward</article-title><source>IEEE Access.</source><year>2024</year><volume>12</volume><fpage>84643</fpage><lpage>84679</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2024.3413069</pub-id></element-citation><mixed-citation id=\"mc-CR37\" publication-type=\"journal\">Majeed, A. &amp; Hwang, S. O. A multifaceted survey on federated learning: Fundamentals, paradigm shifts, practical issues, recent developments, partnerships, trade-offs, trustworthiness, and ways forward. <italic toggle=\"yes\">IEEE Access.</italic><bold>12</bold>, 84643&#8211;84679 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Deebak, B. D. &amp; Hwang, S. O. Federated learning-based lightweight two-factor authentication framework with privacy preservation for mobile sink in the social IoMT. Electronics, 12(5), p.1250. (2023).</mixed-citation></ref><ref id=\"CR39\"><label>39.</label><mixed-citation publication-type=\"other\">Majeed, A., Zhang, X. &amp; Hwang, S. O. Applications and challenges of federated learning paradigm in the big data era with special emphasis on COVID-19. Big Data and Cognitive Computing, 6(4), p.127. (2022).</mixed-citation></ref><ref id=\"CR40\"><label>40.</label><mixed-citation publication-type=\"other\">Yao, A., Pal, S., Dong, C., Li, X. &amp; Liu, X. March. A framework for user biometric privacy protection in UAV delivery systems with edge computing. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 631&#8211;636). IEEE. (2024).</mixed-citation></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Yao, A. et al. A privacy-preserving location data collection framework for intelligent systems in edge computing. Ad Hoc Networks, 161, p.103532. (2024).</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><mixed-citation publication-type=\"other\">Dong, C. et al. May. A blockchain-aided self-sovereign identity framework for edge-based uav delivery system. In 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (pp. 622&#8211;624). IEEE. (2021).</mixed-citation></ref><ref id=\"CR43\"><label>43.</label><mixed-citation publication-type=\"other\">Yao, A. et al. October. A novel security framework for edge computing based uav delivery system. In 2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) (pp. 1031&#8211;1038). IEEE. (2021).</mixed-citation></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Dong, C., Pal, S., Chen, S., Jiang, F. &amp; Liu, X. <italic toggle=\"yes\">A privacy-aware Task Distribution Architecture for UAV Communications System Using Blockchain</italic> (IEEE Internet of Things Journal, 2025).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12658072 PMC12658072.1 12658072 12658072 41298516 10.1038/s41598-025-25993-3 25993 1 Article Enhancing smart city mobility through real time explainable AI in autonomous vehicles Malik Ali Zaman 1 Naz Naila Samar 1 Ahmed Fahad 1 Saleem Muhammad 1 5 Farooq Muhammad Sajid 2 Rehman Ateeq Ur 3 4 Ismael Waleed M. waleed.m@auhd.edu.ye 6 Khan Muhammad Adnan adnan@gachon.ac.kr 7 1 https://ror.org/02my4wj17 grid.444933.d 0000 0004 0608 8111 Department of Computer Science, National College of Business Administration and Economics, Lahore, 54000 Pakistan 2 Department of Cyber Security, NASTP Institute of Information Technology, Lahore, 58810 Pakistan 3 https://ror.org/0034me914 grid.412431.1 0000 0004 0444 045X Computer Science and Engineering, Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai, Tamilnadu India 4 https://ror.org/01ah6nb52 grid.411423.1 0000 0004 0622 534X Applied Science Research Center, Applied Science Private University, Amman, Jordan 5 https://ror.org/05t4pvx35 grid.448792.4 0000 0004 4678 9721 University Center for Research and Development, Chandigarh University, Mohali, 140413 Punjab India 6 https://ror.org/02zv8ns48 Department of Information Technology, Faculty of Engineering, Azal University for Human Development, Sanaa, Yemen 7 https://ror.org/03ryywt80 grid.256155.0 0000 0004 0647 2973 Department of Software, Faculty of Artificial Intelligence and Software, Gachon University, Seongnam-si, 13557 Republic of Korea 26 11 2025 2025 15 478255 42118 7 7 2025 27 10 2025 26 11 2025 28 11 2025 28 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . The quick advancement of Autonomous Vehicular Networks (AVNs) shows remarkable potential to transform urban transportation systems in smart cities. This transformational process faces several crucial issues, primarily due to the lack of clear decision-making, concerns about public confidence, and the need for prompt protective measures in various contexts. The implementation of AVNs depends on resolving current adversities, as these difficulties affect both system safety, user trust, and performance reliability. Traditional AVN development focused on enhancing technical capabilities, such as reliability, but failed to adequately address issues with decision transparency and interpretability. The current systems fall short because they lack an understanding of how Autonomous Vehicles (AVs) generate decisions in real-time urban conditions, which impedes public confidence and broader adoption. To address these limitations, this study integrates You Only Look Once, V5 (YOLOv5), a fast and lightweight object detection model well-suited for AVs, alongside Explainable AI (XAI) techniques to ensure interpretability and transparency. In this research, an XAI-based YOLOv5 model is proposed to enable real-time, explainable decision-making. Its objectives are to increase transparency, increase safety, and gain public acceptance for connecting the AVNs to smart city systems. The proposed model achieves an accuracy of 99% with a miss rate of 1%, thereby enhancing classification accuracy and public confidence. The proposed work also aims to foster public trust in AVNs within smart city ecosystems by making AI decisions more transparent and interpretable. Keywords Autonomous vehicular networks (AVNs) Explainable AI (XAI) Vehicle-to-Vehicle (V2V) Vehicle-to-Infrastructure (V2I) Infrastructure-to-Infrastructure (I2I) Subject terms Engineering Mathematics and computing pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction The advent of AVNs represents a groundbreaking innovation in the urban transport system, incorporating the application of Artificial Intelligence (AI) and modern communication technology to provide smart transport systems 1 &#8211; 6 . These networks have the potential to provide the best possible traffic patterns, less congested roads, improved traffic signals, and enhanced traffic safety. However, these advancements raise the following critical concern in AVNs: the lack of transparency in the decisions made by the AI systems used in AVs. In the case of traditional AVN models, they are black box models where the decision-making process is not amenable to the users or regulators 7 , 8 . Such a lack of explainability leads to public skepticism, risks of legal responsibility, and legislative issues in cases of car crashes or reckless driving. XAI appears to be a currently sought-after solution aimed at enhancing the vague vehicular decisions concerning interpretability, public acceptance, and regulatory approval. Thus, XAI helps to improve the transparency and accountability of these models and can be regarded as a key factor for users&#8217; trust and compliance with the regulation 9 &#8211; 11 . Another significant hurdle hindering the use of AVN is that it is challenging to show precisely how and why AVs make decisions instantaneously 12 . For instance, when an AV decides to change lanes or apply the brakes suddenly, the passengers, pedestrians, and traffic authorities must be able to understand why the car made such a decision. For this reason, when there is no interpretability, the population loses trust in the AVNs, or there may be a fear of how safe these vehicles are. Similarly, road safety continues to be an area of concern, and according to the WHO, global road traffic deaths stand at 1.35 million per year, with the majority resulting from human error 13 , 14 . AVNs can prevent such fatal accidents by removing distractions and impaired decision-making, but since they lack an XAI system, their full potential cannot be harnessed. This can be solved by implementing the YOLOv5 real-time object detection model and incorporating AI explainability approaches to improve the transparency of AVN&#8217;s decision-making. YOLOv5 operates as a lightweight and real-time object detection model that achieves a balance between speed and accuracy for applications like AVNs that require prompt decision-making 15 , 16 . However, YOLOv5 performs well in overall perception tasks in real-time applications, but it does not come with a method for explaining the decision-making process it undertakes. To tackle this, XAI methods such as gradient-based saliency, perturbation-based heatmaps, and occlusion sensitivity are implemented into YOLOv5, allowing the AVNs to provide human-understandable reasons for actions like changing lanes, halting at signal-controlled intersections, or rerouting caused by an obstacle. The integration of explainability into the decision-making process within AVN also enhances the safety factor of the system, as the actions executed by AI can be verified and justified. This is illustrated in Fig.&#160; 1 , which presents an AVN communication system involving Vehicle-to-Infrastructure (V2I) interaction essential for informed decision-making. Fig. 1 AVN communication system 17 . However, operational situations are in complex and unconstrained environments where a timely decision can mean the difference between life and death. For instance, when managing traffic, navigating around a crowded junction, dodging other vehicles on the road, or responding to any obstacles on the road, AVNs must determine various criteria, such as speed limits, surface types, and surrounding traffic. As shown in Fig. 1 , the intelligent communication system allows AVNs to communicate and check the reliability of nearby vehicles to develop collective decision-making 17 . This reduces uncertainty, minimizes risk, and enhances predictability in areas with high traffic density. An XAI-integrated AVN framework enables intelligent and data-driven decision-making, as well as explainable reasoning, to reduce the trust deficit in self-driving cars among passengers, pedestrians, and regulatory authorities. This study introduces an XAI-driven AVN framework that leverages YOLOv5-based object detection, alongside explainability techniques, to enhance road safety, transparency, and public confidence in autonomous mobility. The significant contributions of this study are as follows: (1) the implementation of the interpretable AVN model for real-time traffic and hazard estimation based on the introduced approach; (2) refining vehicle decision-making using the YOLOv5 algorithm while including the XAI explanation; (3) fulfilling the requirements of the authorities and the public and making the AVN&#8217;s actions traceable. The objective of this research is to design a foundation for a trustworthy, explainable, and safety-focused AI-powered transportation system that facilitates the integration of AVNs into smart city implementation plans. Research questions and objectives This study addresses: RQ1. What level of detection performance and real-time throughput can a YOLOv5-based perception module achieve for AVNs under a unified evaluation protocol (dataset 18 ? RQ2. To what extent do XAI methods (occlusion, perturbation, gradient-based saliency) provide faithful, localized attributions that clarify the model&#8217;s decisions? RQ3. How sensitive is performance to inference hyperparameters (image size, confidence threshold, NMS IoU) and lightweight preprocessing, and which operating point best balances accuracy and latency (with uncertainty reporting via 95% CIs)? RQ4. What planner-ready signals (bounding boxes, class labels, confidences, saliency maps) should be exported for downstream planning/control integration? Literature review This research suggests that integrating XAI methods with YOLOv5 object detection enhances the AVN decision-making process by making it more explainable and accurate. In this section, the authors discuss the limitations of black-box models in AVNs and describe how XAI can enhance decision interpretability, real-time perception, and vehicle safety with the help of recent advances. Researchers in 19 examined semantic segmentation problems for driving environments characterized by difficult-to-identify road areas combined with poor traffic rule compliance and objects that vary significantly in type. The India Driving Dataset (IDD) served as the basis for solving complexities that distinguish it from structured urban road data captured in Cityscapes. To improve segmentation performance in such challenging conditions, the authors proposed Eff-UNet, a model that combines EfficientNet as an encoder for feature extraction with UNet&#8217;s decoder for precise reconstruction of the segmentation map. EfficientNet&#8217;s compound scaling optimizes depth, width, and resolution, while UNet&#8217;s skip connections retain essential spatial details. The proposed Eff-UNet outperformed models such as DeepLabV3 + and standard UNet variants, achieving the highest mean Intersection over Union (mIoU) on the IDD Lite dataset and securing first place in the IDD segmentation challenge. In this research, Porzi et al. (2019) 20 propose a Feature Pyramid Network (FPN)-based approach for seamless scene segmentation, addressing the challenges of joint semantic and instance segmentation. Previous methods, such as Mask R-CNN for instance segmentation and DeepLab for semantic segmentation, trained separate models, leading to inconsistent labeling and increased computational cost. To overcome this, the authors introduce a unified model that integrates multi-scale contextual features from FPN with a DeepLab-inspired segmentation module. It enhances segmentation quality and brings improvements in redundancy and efficiency, achieving state-of-the-art performance on the Cityscapes and Mapillary Vistas datasets. It is simultaneously demonstrated that the integration of semantics and instances for segmentation has provided better stability and reduced time consumption, thereby making it suitable for real-world applications such as autonomous driving and urban environment perception. In this research, Singh et al. (2021) 21 evaluate state-of-the-art deep learning models for object detection, semantic segmentation, and instance segmentation on real-world driving datasets, including Cityscapes, Berkeley DeepDrive (BDD), and the IDD. Unlike previous studies that rely on standard datasets such as PASCAL VOC and MS-COCO, this work highlights the challenges of structured and unstructured driving environments. For object detection, RetinaNet performs best on structured datasets, while Faster R-CNN excels in unstructured conditions. While evaluating semantic segmentation, the efficacy of PSPNet, ERFNet, and DRN is compared to each other. However, it has been realized that an optimal segmentation model should have multi-release robust feature representations. Among the three models assessed, DRN stands out as the most stable with both SVHN and Imagenet data. For example, Mask R-CNN performed better on Cityscapes and IDD, while the different Cascade Mask performed much better on BDD. The study demonstrates the impact of domain shift on model generalization and highlights the scarcity of datasets, underscoring the importance of domain adaptation for enhancing segmentation and detection in autonomous driving environments. In this research 22 , Louati et al. (2024) propose a Multi-Agent Reinforcement Learning (MARL) framework to optimize lane-changing strategies for AVs in smart cities. Unlike traditional rule-based methods, their MA2C algorithm enhances cooperation among AVs using parameter-sharing techniques and integrates human-like driving behaviors through IDM and MOBIL models. The results demonstrate that MA2C outperforms existing MARL models, such as MAPPO and MADQN, in terms of traffic efficiency, energy consumption, and passenger safety, thereby reducing congestion and enhancing sustainability in urban mobility. However, they have limitations in that they do not involve real-world traffic environment settings and testing, and the scenarios used are relatively simple and may require additional testing in a complex urban environment with various styles of driving. In 23 , the authors tackle the complexities of lane-change decisions for AVs on multi-lane expressways by introducing a robust decision-making framework. This framework enhances lateral vehicle stability through phase-plane analysis and state-machine logic, effectively considering surrounding traffic dynamics and vehicle motion states. To maintain a safe distance at all times, a cruising strategy is designed, Including Key safety distance models, speed changes, and transition procedures in emergency situations. Additionally, they suggest planning time polynomial optimization that integrates stability and rollover constraints into the path-tracking controller. Simulation results validate the approach, demonstrating improved safety, stability, and overall driving efficiency in multilane expressway scenarios. In 24 , the authors address the challenges of executing safe lane changes and merging maneuvers for AVs in dense traffic environments. They introduce a decentralized, controller-agnostic cooperative strategy leveraging Vehicle-to-Vehicle (V2V) communication to facilitate collision-free lane transitions. By enabling real-time coordination among vehicles, their approach significantly enhances traffic efficiency, demonstrating up to a 26% improvement in flow under congested conditions. However, the study also notes an increase in energy consumption in low-traffic scenarios. Compared to existing methods, their strategy exhibits superior safety and efficiency, particularly as the penetration rate of AVs increases, highlighting its potential for large-scale deployment in intelligent transportation systems. In 25 , the authors present a decision-making and path-planning framework for AVs based on game theory to address the uncertainties associated with dynamic obstacles. Incorporating the application of Stackelberg game theory, the framework determines the successive strategies of other vehicles in various states and potential threats, thereby increasing the level of decision-making. It adopts a potential-field model of navigation which considers various forms of driving behavior and limitations, hence flexibility in any traffic situation. Furthermore, MPC is used to compute the optimal vehicle behavior in the context of safety and fuel consumption optimization. These results also incorporate this approach to social interactions, and the identification of consecutive and emergent traffic patterns could enhance the planning of AV paths in social and technical environments. Table 1 evaluates AVN models based on their decision-making capabilities, interpretability, and efficiency. While models like Eff-UNet and FPN + DeepLab 19 , 20 excel in segmentation, they lack explainability. Reinforcement learning (MA2C) and V2V strategies 22 , 24 enhance traffic flow but are computationally expensive. Game-theoretic and phase-plane methods 23 , 25 enhance stability but require real-world validation. The proposed XAI-based model addresses these challenges by ensuring explainability, efficiency, and regulatory compliance for the deployment of real-time AVNs. Table 1 Comparative analysis of previously published approaches. References Model Preprocessing Decision making Interpretable AVN Model YOLOv5 with XAI Regulatory &amp; public acceptance Outcome Positive aspects Negative aspects Baheti et al., 2020 19 Eff-UNet &#10004; Semantic segmentation in unstructured environments X X Not discussed Highest mIoU on IDD Lite, 1&#8201;st in IDD challenge Superior accuracy over DeepLabV3+ Limited to unstructured datasets Porzi et al., 2019 20 FPN + DeepLab &#10004; Joint semantic &amp; instance segmentation X X Not discussed Achieved SOTA on Cityscapes &amp; Mapillary Vistas Improved efficiency &amp; consistency Higher computational cost Singh et al., 2021 21 RetinaNet, Faster R-CNN, PSPNet, DRN, Mask R-CNN &#10004; Object detection, semantic &amp; instance segmentation X X Not discussed Model performance varies across Cityscapes, BDD, and IDD Identifies best models for structured &amp; unstructured environments Domain shifts impact generalization Louati et al., 2024 22 MA2C (MARL) &#10004; MARL X X Not discussed Outperforms MAPPO &amp; MADQN in traffic flow, energy efficiency &amp; safety Reduces congestion &amp; improves sustainability Simplified traffic, lacks real-world validation Tang et al., 2023 23 Phase-plane + State Machine &#10004; Lane-change decision framework for AVs X X Not discussed Improves stability, safety &amp; efficiency in expressways Integrates stability &amp; rollover constraints Requires real-world validation Monteiro et al., 2023 24 V2V Cooperative Strategy &#10004; Decentralized lane-change coordination X X Not discussed Improves traffic flow by 26% in congestion Enhances safety &amp; efficiency in AV networks Higher energy use in low traffic Yuan et al., 2023 25 Stackelberg Game Theory + MPC &#10004; Game-theoretic decision-making &amp; path planning X X Not discussed Enhances AV navigation in dynamic environments Handles social interactions &amp; uncertainty Computationally intensive Proposed Linear Discriminant Analysis &#10004; Improving interpretability in AV models Interpretability might come at the cost of reduced model complexity, potentially impacting performance. Embedding XAI explanations may increase computational overhead, affecting real-time processing efficiency. Regulatory compliance and public trust depend on external factors, such as policy changes and public perception, which are difficult to control Enhance model transparency &amp; decision explainability Ensures better regulatory compliance &amp; trust Potential reduction in model complexity &amp; performance Limitations of existing approaches Evaluating existing approaches highlights several key challenges and limitations in AVNs&#8217; decision-making, affecting their efficiency, interpretability, and adaptability in real-world driving conditions. Lack of explainability and transparency As stated in 19 , 20 , 22 , most existing models have limited integration of the XAI technique, leading to a black box approach to decision-making. This lack of interpretability causes difficulty in understanding their behavior, which raises issues in making compliance with regulations as well as causing low levels of public confidence in the autonomous systems. High computational complexity Approaches like [19,24] often employ complex multi-layered decision-making processes, leading to high computational costs that hinder their feasibility for real-time autonomous driving applications. Lack of real-world validation and adaptability Models in [21,22] have been primarily lacking validation in real-world urban and highway driving conditions, raising concerns about their generalizability. Contributions of the proposed model (addressing limitations) The proposed XAI-based AVN model directly addresses these limitations, offering an interpretable, efficient, and regulation-compliant solution for real-time AV navigation. Enhancing explainability and transparency Implementing XAI in the decision-making model addresses regulatory requirements and enhances public confidence in the decision-making process by facilitating the auditing of the AV decisions. Improving computational efficiency In comparison to deep learning-based decision-making models involving high computational complexity, the proposed XAI-based model is far more efficient, which would enhance its capability for implementation in real-time scenarios. Ensuring real-world validation and adaptability The model provides auditable decision-making and is fully scalable to meet the requirements specified in specific periods of realistic traffic conditions and smart city systems. Proposed methodology AVNs have several fundamental issues, including low interpretability, high complexity, and difficulty in applying existing methods to real-world scenarios. Most current models have issues with their decision-making mechanisms, creating a challenge in matters concerning transparency and regulation. Additionally, computationally intensive approaches hinder real-time execution, while reliance on simulated testing environments raises concerns about the effectiveness in real-world settings. To address these issues, this research introduces an XAI-based AVN model that enhances explainability, computational efficiency, and adaptability, ensuring more transparent, efficient, and trustworthy decision-making for AVs. Figures&#160; 2 and 3 , and 4 represent the smart city communication framework for AVN operations, the abstract model of the proposed XAI-based AVN, and the proposed XAI-based AVN, respectively. Fig. 2 Smart city communication for AVs operations. Figure 2 illustrates a smart city scenario in which continuous information exchange occurs among vehicles, infrastructure, and smart buildings to enable automated traffic control and safe autonomous driving. The figure illustrates various interaction types: V2V communication (green lines) enables cars to exchange information about their positions and movements, thereby preventing collisions. V2I communication (purple lines) connects vehicles with traffic lights and Road-Side Units (RSUs) for real-time traffic updates. Infrastructure-to-Infrastructure (I2I) communication (red lines) links different components, such as the traffic control center and base stations, to manage traffic flow efficiently. Additionally, the Vehicle Sensing Radius Propagation (blue circles) represents each vehicle&#8217;s detection zone for identifying Vulnerable Road Users (VRUs), such as pedestrians and cyclists, ensuring timely reactions to potential hazards. It helps to improve general situational awareness and enables AVs to work safely within the smart city context, responding to changes in the environment and signals. This communication fabric provides real-time context to perception and distributes its outputs across the AVN. Building upon the communication structure shown in Figs.&#160; 2 and 3 abstracts the AVN data processing pipeline, which is further detailed through the proposed XAI-based model illustrated in Fig.&#160; 4 . Fig. 3 Abstract model of the proposed XAI-based AVs network. Figure 3 illustrates the data processing pipeline for the AVN, which is built upon the data collected through V2V, V2I, and I2I interactions, as shown in Figs.&#160; 1 and 2 . The collected real-time vehicular data undergoes three key stages: Data Pre-Processing to filter noise and extract relevant features, Data Processing to analyze patterns and predict behaviors, and Data Post-Processing to refine the results for accuracy. This processed data is then transmitted to the Cloud Computing platform for storage and computational tasks. In the Validation Phase, the insights are cross-checked with real-time data to ensure reliability before being deployed to the AVN, enabling intelligent, adaptive, and safe vehicle operations within the smart city. Building on this abstraction, Fig.&#160; 4 details the concrete YOLOv5-based perception and XAI workflow that instantiates these stages. Fig. 4 Proposed XAI based AVN model. Figure 4 illustrates the detailed workflow of the proposed model, starting from the Vehicular Data Input Layer and progressing through various stages to generate predictions for the AVN. This operationalizes Fig.&#160; 3 as a concrete YOLOv5 &#8594; XAI &#8594; validation pipeline. The process begins with the collection of vehicular data, which is fed into the Preprocessing Layer. This layer prepares vehicular data by applying Red, Green, and Blue (RGB) conversion, image resizing, grayscale/Hue, Saturation, Value (HSV) transformation, bounding box adjustments, histogram processing, normalization/heatmap generation, and torchvision transforms. These steps help maintain data integrity and increase object detection, as well as image separation and contrast. It applies to dataset design, creating an effective dataset for dependable real-time decision-making in AVs. Standard preprocessing steps illustrated in Figs.&#160; 5 , 6 , 7 , 8 , 9 and 10 are essential for ensuring input consistency, robustness, and effective explainability in AV perception. Techniques such as resizing, grayscale conversion, histogram equalization, HSV transformation, and edge detection were applied to improve data quality under real-world conditions and to support downstream detection and XAI analysis. HSV (lighting-robust via chromaticity&#8211;luminance decoupling), histogram equalization (which raises local contrast to enhance small/low-contrast objects), and edge detection (which emphasizes saliency-aligned boundaries) collectively standardize inputs and produce sharper, localized attributions&#8212;improving robustness and interpretability. Although the proposed model focuses primarily on object detection and explainability, it is designed to integrate with standard decision-making modules in autonomous vehicles seamlessly. The outputs of the YOLOv5&#8201;+&#8201;XAI block (detected objects and their human-interpretable explanations) can be supplied as inputs to conventional path planning and control strategies (e.g., rule-based state machines, model predictive planners, and PID or Pure-Pursuit controllers). In this way, the transparency achieved at the perception layer can also be extended to the vehicle&#8217;s subsequent actions, ensuring that braking, lane changes, or yielding decisions can be traced back to both the detected objects and their XAI-based rationales. Figure 5 depicts the detected objects, such as vehicles and infrastructure, with red bounding boxes, therefore outlining the preprocessing stage. This step utilizes object recognition operations that allow for recognizing important elements of the driving environment, which forms the basis for further processes such as resizing activity, color space transformation, and feature extraction. Fig. 5 Object detection with red bounding boxes. Fig. 6 Image resizing to a standard resolution. Fig. 7 Grayscale conversion for simplified feature extraction. Fig. 8 Histogram processing for color and intensity analysis. Fig. 9 HSV conversion for robust object detection in varying lighting. Fig. 10 Edge detection for object boundary identification. Figure 6 shows the image resizing process, where the original images are resized to a standard 224&#8201;&#215;&#8201;224 resolution. This resizing brings uniformity to the structures, also saves computational costs for training the models, while maintaining important features of the objects as described by the red bounding boxes. Figure 7 illustrates the conversion process from RGB to grayscale, where color information is discarded while intensity information is retained. This step reduces complexity, improves object detection, and concentrates primarily on the structural and texture data required by AVs. Figure 8 shows the histogram processing stage, where pixel intensity distributions for the RGB color channels are analyzed. Histograms enable people to define brightness, contrast, and color differences in images, as these factors can affect visibility and the likelihood of detecting an object. This process enables the model to better balance pixel intensity levels, thereby increasing its reliability when operating under various lighting conditions. Figure 9 represents the conversion to the HSV color space that converts the original images to the HSV color system. This separation divides color content from luminance, which enhances the model&#8217;s capacity to deal with lighting changes and recognize objects irrespective of lighting intensity. The hue component deals with the color, saturation specifies the intensity of the color, and value captures brightness, which also helps in identifying objects in varying illumination. Figure 10 also illustrates the edge detection process, with the help of which, boundaries of objects within images are extracted using the intensity gradient. This technique helps the model identify various objects by detecting sharp changes in pixel intensity. Edge detection is helpful for tasks such as lane detection, object detection, and obstacle detection, as the edges provide the model with a clearer structure from the images it receives. These preprocessing techniques were carefully selected to enhance object detection robustness under varying lighting conditions, diverse traffic environments, and to ensure the model&#8217;s adaptability to real-world AVN scenarios. The preprocessed data is then further split into a training set, comprising 70% of the data, and a testing set, which comprises 30%. The training set is used in the selection of deep learning models, while the test set is used in the assessment of the models. The Deep Learning phase utilizes a YOLOv5 algorithm for object identification and prediction. The YOLOv5 model in this study was selected due to its efficiency in real-time detection and its suitability for AVNs. The model divides input images into grids and performs object detection with low latency, a feature critical for making time-sensitive driving decisions. Once trained, the model processes frames in a single forward pass, enabling the detection of vehicles, pedestrians, and traffic signs in real-time. Once predictions are made, various XAI methods come into play to provide a simpler explanation that helps improve the predictability of the model&#8217;s desired outcome. The YOLOv5 algorithm operates by dividing the input image into an \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:S\\times\\:S$$\\end{document} grid, where each grid cell is responsible for predicting bounding boxes, object presence, and class probabilities. The following equations describe the core components of the object detection process: It predicts bounding boxes with the following parameters: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\widehat{y}=(x,y,w,h,c)$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:x,y$$\\end{document} represent the center coordinates of the predicted bounding box (normalized in [0,1]), \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:w,h$$\\end{document} represent the width and height of the box, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:c\\in\\:\\left[\\text{0,1}\\right]$$\\end{document} denotes the confidence score indicating the probability of object presence. The objectness score measures the likelihood of an object being present in a grid cell: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\text{O}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\text{n}\\text{e}\\text{s}\\text{s}\\:\\text{S}\\text{c}\\text{o}\\text{r}\\text{e}=\\text{P}\\left(\\text{o}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\right)\\text{x}{\\text{I}\\text{O}\\text{U}}_{\\text{p}\\text{r}\\text{e}\\text{d},\\text{t}\\text{r}\\text{u}\\text{t}\\text{h}}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:P\\left(\\text{o}\\text{b}\\text{j}\\text{e}\\text{c}\\text{t}\\right)$$\\end{document} is the probability of object existence in the grid cell, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\text{I}\\text{O}\\text{U}}_{\\text{p}\\text{r}\\text{e}\\text{d},\\text{t}\\text{r}\\text{u}\\text{t}\\text{h}}$$\\end{document} represents the Intersection over Union (IoU) between the predicted bounding box \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{b}_{pred}$$\\end{document} and the ground truth box \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{b}_{gt}$$\\end{document} . It predicts the class of the detected objects using: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:P\\left({class}_{i}\\right|object)\\forall\\:i\\in\\:\\{\\text{1,2},\\dots\\:,k\\}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:k$$\\end{document} denotes the number of object classes (e.g., vehicles, pedestrians, traffic signs), and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:P\\left({class}_{i}\\right|object)$$\\end{document} is the probability of the detected object belonging to class \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:i$$\\end{document} . The loss function minimizes the error across localization, object confidence, and classification: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{total}={\\lambda\\:}_{coord}{\\mathcal{L}}_{coord}+{\\mathcal{L}}_{obj}+{\\mathcal{L}}_{noobj}+{\\mathcal{L}}_{class}$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{coord}$$\\end{document} represents the localization loss, penalizing the errors in bounding box predictions: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{coord}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{obj}[{\\left({x}_{i}-{\\widehat{x}}_{i}\\right)}^{2}+{\\left({y}_{i}-{\\widehat{y}}_{i}\\right)}^{2}+{\\left(\\sqrt{{w}_{i}}-\\sqrt{{\\widehat{w}}_{i}}\\right)}^{2}+{\\left(\\sqrt{{h}_{i}}-\\sqrt{{\\widehat{h}}_{i}}\\right)}^{2}]$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{obj}$$\\end{document} denotes objectness loss, measuring prediction confidence: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{obj}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{obj}{\\left({c}_{i}-{\\widehat{c}}_{i}\\right)}^{2}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{noobj}$$\\end{document} &#8203; is the no-object loss, penalizing false positives in empty grid cells: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{noobj}=\\sum\\:_{i=0}^{{S}^{2}}\\sum\\:_{j=0}^{B}{1}_{ij}^{noobj}{\\left({c}_{i}-{\\widehat{c}}_{i}\\right)}^{2}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{class}$$\\end{document} &#8203; is the classification loss, which evaluates the predicted class probabilities: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mathcal{L}}_{class}=\\sum\\:_{i=0}^{{S}^{2}}{1}_{i}^{obj}\\sum\\:_{c\\in\\:classes}{({p}_{i}\\left(c\\right)-{\\widehat{p}}_{i}(c\\left)\\right)}^{2}$$\\end{document} Here, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:S\\times\\:S$$\\end{document} is the grid size, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:B$$\\end{document} is the number of bounding boxes per grid cell, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{1}_{ij}^{obj}$$\\end{document} &#8203; is an indicator if an object exists in cell \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:i$$\\end{document} for box \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:j$$\\end{document} , and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{1}_{ij}^{noobj}$$\\end{document} &#8203; is an indicator if no object exists. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:({x}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{y}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{w}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{h}_{i})$$\\end{document} are ground-truth parameters, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:({\\widehat{x}}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\widehat{y}}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\widehat{w}}_{i}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\widehat{h}}_{i})$$\\end{document} are predicted parameters, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{c}_{i},{\\widehat{c}}_{i}$$\\end{document} denote ground truth vs. predicted confidence, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{p}_{i}\\left(c\\right),{\\widehat{p}}_{i}\\left(c\\right)$$\\end{document} are true vs. predicted class probabilities, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\lambda\\:}_{coord}$$\\end{document} &#8203; is a weighting factor for localization loss. The loss function aligns with the original YOLOv5 implementation to ensure reproducibility and fair benchmarking. After the YOLOv5 algorithm generates predictions, XAI techniques like Occlusion Sensitivity Heatmaps, Perturbation-Based Saliency Maps, and Gradient-Based Saliency Maps are applied to provide human-understandable insights into the model&#8217;s decision-making process. The XAI methods ensure that the detected objects and their classifications are transparent and interpretable, enhancing trust and facilitating better regulatory compliance in AVNs. The Occlusion Sensitivity method systematically blocks (occludes) regions of an image and observes how the model&#8217;s confidence in the detected objects changes. The importance of each region is calculated as: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{occ}\\left(x,y\\right)={P}_{orig}-{P}_{occ}(x,y)$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{occ}\\left(x,y\\right)$$\\end{document} is the occlusion sensitivity score at pixel location \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(x,y)$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{P}_{orig}$$\\end{document} is the original YOLOv5 object confidence score before occlusion, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{P}_{occ}(x,y)$$\\end{document} is the YOLOv5 confidence score after occluding region \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(x,y)$$\\end{document} . A heatmap is generated based on these scores, highlighting regions crucial for object detection. The Perturbation-Based method modifies image regions by adding noise or blurring to analyze how the model&#8217;s confidence changes: 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{pert}\\left(x,y\\right)={P}_{orig}-{P}_{pert}\\left(x,y\\right)$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{pert}\\left(x,y\\right)$$\\end{document} is the perturbation saliency score, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{P}_{pert}\\left(x,y\\right)$$\\end{document} is the confidence score after perturbing pixel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(x,y)$$\\end{document} . A saliency map is generated, highlighting areas where modifications have a significant impact on the model&#8217;s predictions. Gradient-based methods compute the sensitivity of YOLOv5&#8217;s output to input pixel changes using backpropagation gradients: 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{grad}\\left(x,y\\right)=\\left|\\frac{\\partial\\:{P}_{obj}}{\\partial\\:\\text{I}(\\text{x},\\text{y})}\\right|$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{S}_{grad}\\left(x,y\\right)$$\\end{document} is the gradient saliency score for pixel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(x,y)$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{P}_{obj}$$\\end{document} is the object confidence score, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:I(x,y)$$\\end{document} is the input pixel intensity, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\frac{\\partial\\:{P}_{obj}}{\\partial\\:\\text{I}(\\text{x},\\text{y})}$$\\end{document} represents the gradient of the confidence score with respect to input pixel intensity. This method highlights pixels that strongly influence object detection, creating a gradient-based saliency map. These predictions and explanations are then validated, if the model&#8217;s learning rate does not meet performance criteria, the deep learning phase is retrained. If the learning rate meets these explained patterns, they are sent to Cloud Computing for storage and further analysis. The next step is the Validation Phase, where the trained model is applied to real-time vehicular data. The XAI techniques are imported from the cloud to validate the predictions. If validation is successful, the projections are forwarded to the AVN for decision-making. In cases where predictions are unreliable, the system discards the output to prevent incorrect actions. Ultimately, this process ensures that the AVN receives accurate, explainable, and reliable predictions based on real-time and historical data, enhancing the safety and efficiency of AVs in dynamic environments. Simulation results In recent times, there have been enhancements in object detection for AVs, but issues that have lacked a solution include latency, false positives, and explainability. There are several issues with traditional models, including lighting variations, occlusions, and shifts in traffic density, which reduce their reliability. To address this, simulations were conducted using the YOLOv5 model, trained on the Berkeley Labeled Dataset for Autonomous Driving, sourced from the Kaggle dataset 18 with a 70/30 split, using standard settings without architectural changes to ensure reproducibility and fair benchmarking. The dataset consists of 212 labelled images in YOLOv5 format, including 179 for training and 33 for validation, covering common AVN classes such as vehicles, pedestrians, and traffic signs. The images span urban streets, highways, and intersections under varied illumination conditions (daytime, dusk/night, shadows) with a range of traffic densities and partial occlusions. The class distribution is vehicle-dominant, with fewer instances of pedestrians and signs. This provides a compact yet representative slice of routine AV perception, while coverage of extreme weather (rain/snow/fog) and rare corner cases remains limited. All images were paired with corresponding label files, and class consistency was programmatically verified to ensure data integrity. The modest sample size was mitigated by fine-tuning a pre-trained YOLOv5 model with standard data augmentation techniques, which are more effective compared to existing methods and increase classification accuracy and reliability (horizontal flips, moderate scaling/translations, HSV color jitter) under Ultralytics. Robustness was assessed via a validation sweep across image size and thresholds, with performance peaking at imgsz&#8201;=&#8201;320 (mAP@0.50&#8201;=&#8201;0.623, mAP@0.50&#8211;0.95&#8201;=&#8201;0.278), indicating stable behavior despite limited data. XAI techniques were applied to enhance model transparency. Results confirm the system&#8217;s ability to provide real-time, accurate, and interpretable decisions for safer autonomous navigation. All key detection metrics are reported with variability (mean&#8201;&#177;&#8201;SD) and 95% confidence intervals, as demonstrated in Tables&#160; 3 and 4 . Fig. 11 YOLOv5-based object detection for AVN driving ( a ). YOLOv5-based object detection for AVN driving ( b ). YOLOv5-based object detection for AVN driving ( c ). YOLOv5-based object detection for AVN driving ( d ). YOLOv5-based object detection for AVN driving ( e ). YOLOv5-based object detection for AVN driving ( f ). YOLOv5-based object detection for AVN driving ( g ). YOLOv5-based object detection for AVN driving ( h ). YOLOv5-based object detection for AVN driving ( i ). Figure 11 (a&#8211;i) illustrates the YOLOv5-based object detection results across various AVNs driving conditions. The model effectively identifies and classifies multiple objects, primarily vehicles, with red bounding boxes highlighting Class 1 (vehicles) and yellow bounding boxes indicating Class 2 (roadside elements such as traffic signs and poles). Each detection is accompanied by a confidence score ranging from 0.81 to 0.99, indicating a high degree of certainty in object identification. The results showcase the model&#8217;s robustness across varying traffic conditions and environmental settings, including highways, urban streets, intersections, and low-light scenarios. In dense traffic conditions (Fig.&#160; 11 d and f, and 11 i), the system effectively distinguishes and tracks multiple vehicles with high confidence, ensuring precise localization. Additionally, the model maintains strong detection performance in nighttime and low-light conditions (Fig.&#160; 10 e), demonstrating its ability to adapt to challenging visibility constraints. The consistently high confidence scores highlight the stability and precision of the YOLOv5 algorithm, reinforcing its suitability for real-world AVN applications. The proposed YOLOv5-based model achieves an average accuracy of 94.5%, maintaining over 85% even in challenging conditions, ensuring reliable real-time object detection for autonomous driving. The proposed model employs occlusion sensitivity, perturbation-based, and gradient saliency maps due to their low computational overhead and suitability for real-time object detection. These techniques provide localized visual interpretability and integrate seamlessly with the trained model, supporting the rapid decision-making demands of AVs. Fig. 12 Occlusion sensitivity heatmap highlighting key regions in YOLOv5 detection. Figure 12 showcases an XAI approach applied to YOLOv5-based object detection using Occlusion Sensitivity Heatmaps. The left side shows the original highway image, while the right displays the Heatmap visualization, where bright yellow and red areas indicate the most critical regions influencing object detection. This technique enhances transparency and trust by revealing which parts of the image contribute most to model predictions. By making the detection process interpretable, this XAI approach ensures safer, more reliable autonomous navigation in real-world driving scenarios. Fig. 13 Perturbation-based saliency map revealing critical areas in YOLOv5 detection. Figure 13 presents an XAI approach using a Perturbation-Based Saliency Map to analyze YOLOv5-based object detection. The left side shows the original image, while the right side highlights important regions where pixel perturbations significantly impact detection confidence. The colored grid-like blocks indicate areas where modifications affect the model&#8217;s decisions, making it a crucial tool for understanding model behavior. This approach enhances transparency and reliability, ensuring safer and more interpretable AV navigation. XAI overlays (occlusion/perturbation/gradient) accompany each detection as rationale artifacts&#8212;heatmaps and scalar scores&#8212;forming an audit-ready trail for incident analysis and regulatory reporting. Sanity checks also curb misinterpretation and strengthen stakeholder trust. Table 2 Comparison of the proposed XAI based AVN model with previously published approaches. References Dataset Model Accuracy (%) Miss-rate (%) Yara&#351;, N., 2020 26 BIT Vehicle Dataset ResNet34 77% 33 Tas, S. et al., 2022 27 Custom Dataset Custom CNN 92.9 7.1 Chowdhury, S. et al., 2022 28 Custom Bangladeshi Vehicle Dataset YOLOv5 83.02% 16.98 Won, M., 2020 29 Custom Dataset CNN 90.85 9.15 Mazhar, T. et al., 2023 30 Electric Vehicle Charging Dataset Kaggle RF 85 15 Cao, X. et al., 2011 31 Custom Dataset Linear SVM Classification 90 10 Yang, B. et al., 2014 32 Custom Dataset Fixed threshold state machine algorithm 93.6 6.4 Proposed XAI based AVN model 99 1 Table 2 shows that, on a dataset 18 under a unified evaluation protocol, the Proposed XAI-based AVN model achieves 99% accuracy with only a 1% miss rate; because the prior approaches 26 &#8211; 32 report results on heterogeneous datasets and metrics&#8212;although they pursue the same overarching AVN-perception objective&#8212;Table 2 is presented as purpose-aligned context rather than a head-to-head benchmark. This highlights its superior reliability in AVN classification and intelligent transportation systems. Ablation and sensitivity analysis To assess robustness and component-wise contribution, we performed a targeted ablation and sensitivity study using the validation split. Varying the image size and thresholds shows a clear accuracy&#8211;efficiency trade-off: increasing resolution from 224 to 320 improves mAP@0.50 from ~&#8201;0.45 to 0.623 (best at imgsz&#8201;=&#8201;320, conf&#8201;=&#8201;0.70, IoU&#8201;=&#8201;0.45; mAP@0.50&#8201;&#8722;&#8201;0.95&#8201;&#8776;&#8201;0.278), while 416 offers marginal additional accuracy with higher computational cost; therefore, 320 is selected for real-time AVN deployment. Table&#160; 3 summarizes the complete validation sensitivity sweep (imgsz/conf/IoU). Table 3 Validation sensitivity sweep (imgsz/confidence/IoU; validation split). imgsz conf IoU mAP@0.50 mAP@0.50&#8211;0.95 Precision Recall F1-Score 224 0.25 0.45 0.403 0.189 0.508 0.387 0.439 224 0.25 0.50 0.402 0.190 0.474 0.387 0.426 224 0.25 0.70 0.389 0.185 0.419 0.379 0.398 224 0.50 0.45 0.448 0.210 0.508 0.387 0.439 224 0.50 0.50 0.439 0.208 0.474 0.387 0.426 224 0.50 0.70 0.414 0.198 0.419 0.379 0.398 224 0.70 0.45 0.466 0.217 0.503 0.366 0.424 224 0.70 0.50 0.452 0.213 0.466 0.366 0.410 224 0.70 0.70 0.432 0.205 0.406 0.370 0.387 320 0.25 0.45 0.539 0.246 0.675 0.496 0.572 320 0.25 0.50 0.535 0.244 0.673 0.498 0.572 320 0.25 0.70 0.508 0.239 0.626 0.496 0.553 320 0.50 0.45 0.592 0.266 0.675 0.496 0.572 320 0.50 0.50 0.587 0.262 0.673 0.498 0.572 320 0.50 0.70 0.539 0.255 0.626 0.496 0.553 320 0.70 0.45 0.623 0.278 0.691 0.494 0.576 320 0.70 0.50 0.622 0.278 0.681 0.498 0.575 320 0.70 0.70 0.600 0.277 0.620 0.498 0.552 416 0.25 0.45 0.586 0.278 0.569 0.601 0.585 416 0.25 0.50 0.587 0.279 0.561 0.601 0.580 Preprocessing ablation (proxy on 50 images) in Table&#160; 4 indicates that grayscale and HSV-V-only decrease mean detections by approximately 3% and 5%, respectively, and slightly reduce mean confidence (baseline mean confidence&#8201;&#8776;&#8201;0.583; grayscale&#8201;&#8776;&#8201;0.572; HSV-V-only&#8201;&#8776;&#8201;0.579). In contrast, histogram equalization increases mean detections by approximately 2.7% (baseline &#8594; 21.30 vs. hist_eq &#8594; 21.88) with comparable confidence (&#8776;&#8201;0.577 vs. baseline&#8201;&#8776;&#8201;0.583), supporting its inclusion under challenging illumination conditions. Table 4 Preprocessing ablation (proxy, N &#8201;=&#8201;50 images). Variant Mean detections Mean confidence baseline_RGB 21.30 0.583 grayscale 20.66 0.572 hist_eq 21.88 0.577 HSV_V_only 20.22 0.579 For the explainability cost-benefit analysis, a lightweight XAI method timing comparison in Table&#160; 5 shows that occlusion produces a larger average confidence drop (&#8776;&#8201;0.0089) than perturbation (&#8776;&#8201;0.0049), with similar runtime per image (&#8776;&#8201;0.13s vs. 0.12s on CPU). Accordingly, occlusion heatmaps are employed for clearer attribution, and perturbation maps when lower latency is preferred. Table 5 XAI timing (proxy, N &#8201;=&#8201;3 images). XAI method Avg confidence drop Time (s/image) occlusion 0.0089 0.131 perturbation 0.0049 0.118 As Table&#160; 6 shows, precision and recall are accompanied by two-sided Wilson 95% confidence intervals, whereas mAP@0.50 and mAP@0.50&#8211;0.95 include non-parametric bootstrap 95% intervals computed over the validation images (1,000 resamples); pairwise differences between settings (e.g., imgsz&#8201;=&#8201;224 vs. 320) are assessed with paired bootstrap tests. Table 6 Best configuration (with 95% CIs). Metric Point Estimate CI method mAP@0.50 0.623 Bootstrap mAP@0.50&#8211;0.95 0.278 Bootstrap Precision 0.691 Wilson (95%) Recall 0.494 Wilson (95%) Conclusion In recent times, AVs worldwide are facing significant challenges, including a lack of transparency in decision-making, trust concerns from the public, and ensuring real-time safety in dynamic environments. These issues hinder the seamless integration of AVNs into smart city infrastructures. Traditional models primarily focus on improving accuracy and efficiency, but they fall short in providing explainability, which is crucial for building public trust and achieving widespread adoption. To address these limitations, this research introduces an XAI-based YOLOv5 model that integrates XAI techniques to enhance real-time interpretability and transparency in decision-making. It includes a targeted ablation/sensitivity study with uncertainty reporting (95% confidence intervals) to quantify robustness. To enhance the reliability and safety of the model, YOLOv5&#8217;s strong object detection feature is integrated with the principles of XAI. The proposed model achieves a 99% accuracy rate with a 1% missing rate on the evaluation dataset under a unified protocol, which is more effective than existing techniques and enhances classification accuracy and reliability. Limitations and future work The Berkeley set spans urban streets, highways, and intersections under varied illumination (day/dusk/night) with mixed traffic densities and partial occlusions; it is vehicle-dominant with fewer pedestrian/sign instances. This yields a concise yet representative slice of routine AV perception, while coverage of extreme weather (rain/snow/fog) and rare hazards remains limited. Under crowding/occlusion, small overlaps lower recall when non-maximum suppression mistakenly removes true positives. These scope limits were partially mitigated via standard augmentation and a validation sensitivity sweep, and variability is reported (mean&#8201;&#177;&#8201;SD; 95% CIs). Future work will scale evaluation to larger, multi-domain datasets and validate under extreme weather and highly dynamic urban traffic, while exploring federated learning for privacy-preserving distributed training 33 &#8211; 39 to strengthen robustness, generalization, and scalability; temporal tracking for occlusion recovery, overlap-aware calibrated non-maximum suppression, and concept/counterfactual XAI to resolve multi-object interactions additionally, edge privacy controls for biometrics and location data 40 , 41 , and decentralized identity and auditability via SSI/blockchain with privacy-aware task governance 42 &#8211; 44 will be incorporated to enhance data governance and formalize end-to-end privacy/latency budgets with an edge-first design, restricting cloud use to de-identified, asynchronous analytics. Ethical Consideration: This study utilizes a publicly available dataset and focuses on explainability at the perception layer. For deployment, privacy should be enforced via edge-first processing and federated learning for model updates, without centralizing raw data, alongside data minimization and auditable, time-bounded retention, which is marked as future work. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions Ali Zaman Malik, Naila Samar Naz, Fahad Ahmed and Muhammad Saleem, have collected data from different resources and contributed to writing&#8212;original draft preparation. Ali Zaman Malik, Muhammad Sajid Farooq, Ateeq Ur Rehman, Waleed M. Ismael and Muhammad Adnan Khan performed formal analysis and Simulation, writing&#8212;review and editing, Naila Samar Naz, Ateeq Ur Rehman, and Muhammad Adnan Khan; performed supervision, Muhammad Sajid Farooq, Fahad Ahmed, Muhammad Saleem, Muhammad Sajid Farooq.; drafted pictures and tables, Ateeq Ur Rehman, Waleed M. Ismael and Muhammad Adnan Khan; performed revisions and improve the quality of the draft. All authors have read and agreed to the published version of the manuscript. Data availability The data used to support the findings of this study are available from the corresponding authors upon request. Declarations Competing interests The authors declare no competing interests. References 1. Singh, B., Kaunert, C., Lal, S., Arora, M. K. &amp; Jermsittiparsert, K. Intelligent mobility assimilating IoT in autonomous vehicles: foster sustainable cities and communities. In Designing Sustainable Internet of Things Solutions for Smart Industries. IGI Global . 279&#8211;300 (2025). 2. Shrimal, H. Integration of AI-powered vehicles with smart City infrastructure to transform the future of automotive world. SAE Technical Paper .&#160;(2024) 3. Giannaros A Autonomous vehicles: sophisticated attacks, safety issues, challenges, open topics, blockchain, and future directions J. Cybersecur. Priv. 2023 3 3 493 543 10.3390/jcp3030025 Giannaros, A. et al. Autonomous vehicles: sophisticated attacks, safety issues, challenges, open topics, blockchain, and future directions. J. Cybersecur. Priv. 3 (3), 493&#8211;543 (2023). 4. Mohapatra, H. &amp; Dalai, A. K. February. IoT based V2I framework for accident prevention. In 2022 2nd international conference on artificial intelligence and signal processing (AISP) (pp. 1&#8211;4). IEEE. (2022). 5. Mohapatra H Rath AK Panda N IoT infrastructure for the accident avoidance: an approach of smart transportation Int. J. Inform. Technol. 2022 14 2 761 768 Mohapatra, H., Rath, A. K. &amp; Panda, N. IoT infrastructure for the accident avoidance: an approach of smart transportation. Int. J. Inform. Technol. 14 (2), 761&#8211;768 (2022). 6. Mohapatra H Rath AK An IoT based efficient multi-objective real-time smart parking system Int. J. Sens. Networks 2021 37 4 219 232 10.1504/IJSNET.2021.119483 Mohapatra, H. &amp; Rath, A. K. An IoT based efficient multi-objective real-time smart parking system. Int. J. Sens. Networks . 37 (4), 219&#8211;232 (2021). 7. Naveed, S. A. et al. Enhancing traffic flow and congestion management in smart cities utilizing SVM-based linear regression approach. Int. J. Adv. Appl. Sci. 11 (10), 166&#8211;175 (2024). 8. Saleem M Smart cities: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques Egypt. Inf. J. 2022 23 3 417 426 Saleem, M. et al. Smart cities: Fusion-based intelligent traffic congestion control system for vehicular networks using machine learning techniques. Egypt. Inf. J. 23 (3), 417&#8211;426 (2022). 9. Tahir, H. A., Alayed, W., Hassan, W. U. &amp; Haider, A. A Novel Hybrid XAI Solution for autonomous vehicles: real-time interpretability through LIME&#8211;SHAP Integration. Sensors, 24(21), p.6776. (2024). 10.3390/s24216776 PMC11548085 39517672 10. Khan, M. A. et al. Smart Buildings: Federated learning-driven secure, Transparent and Smart Energy Management System Using XAI Vol. 13, pp.2066&#8211;2081 (Energy Reports, 2025). 11. Saleem, M. et al. Secure and Transparent Mobility in Smart Cities: Revolutionizing AVNs To Predict Traffic Congestion Using MapReduce (Private Blockchain and XAI. IEEE Access, 2024). 12. Zhang, T., Li, W., Huang, W. &amp; Ma, L. Critical roles of explainability in shaping perception, trust, and acceptance of autonomous vehicles. International Journal of Industrial Ergonomics, 100, p.103568. (2024). 13. Berhanu Y Alemayehu E Schr&#246;der D Examining car accident prediction techniques and road traffic congestion: a comparative analysis of road safety and prevention of world challenges in low-income and high&#8208;income countries J. Adv. Transp. 2023 2023 1 6643412 Berhanu, Y., Alemayehu, E. &amp; Schr&#246;der, D. Examining car accident prediction techniques and road traffic congestion: a comparative analysis of road safety and prevention of world challenges in low-income and high&#8208;income countries. J. Adv. Transp. 2023 (1), 6643412 (2023). 14. Hossain, S., Maggi, E. &amp; Vezzulli, A. Factors influencing the road accidents in low and middle-income countries: a systematic literature review. Int. J. Injury Control Saf. Promotion . 31 (2), 294&#8211;322. 10.1080/17457300.2024.2319618 (2024). 10.1080/17457300.2024.2319618 38379460 15. Sunitha, G., Priya, V. S., Kumar, V. S., Priya, G. G. &amp; Kumar, T. N. August. road object detection using Yolov8. In 2024 5th international conference on electronics and sustainable communication systems (ICESC) (pp. 847&#8211;853). IEEE. (2024). 16. P&#233;rez, S., G&#243;mez, C. &amp; Rodr&#237;guez, M. Innovative Deep Learning Techniques for Obstacle Recognition: A Comparative Study of Modern Detection Algorithms. arXiv preprint arXiv:2410.10096. (2024). 17. Madhav, A. S. &amp; Tyagi, A. K. July. Explainable artificial intelligence (XAI): connecting artificial decision-making and human trust in autonomous vehicles. In Proceedings of Third International Conference on Computing, Communications, and Cyber-Security: IC4S 2021 (pp. 123&#8211;136). Singapore: Springer Nature Singapore. (2022). 18. https://www.kaggle.com/datasets/harshsprajapati/berkeley-labelled-dataset-for-autonomous-driving 19. Baheti, B., Innani, S., Gajre, S. &amp; Talbar, S. Eff-unet: A novel architecture for semantic segmentation in unstructured environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 358&#8211;359). (2020). 20. Porzi, L., Bulo, S. R., Colovic, A. &amp; Kontschieder, P. Seamless scene segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8277&#8211;8286). (2019). 21. Singh, D., Rahane, A., Mondal, A., Subramanian, A. &amp; Jawahar, C. V. December. evaluation of detection and segmentation tasks on driving datasets. In International Conference on Computer Vision and Image Processing (pp. 512&#8211;524). Cham: Springer International Publishing. (2021). 22. Louati, A. et al. Sustainable smart cities through multi-agent reinforcement learning-based cooperative autonomous vehicles. Sustainability, 16(5), p.1779. (2024). 23. Tang, C., Pan, L., Xia, J. &amp; Fan, S. Research on lane-change decision and planning in multilane expressway scenarios for autonomous vehicles. Machines, 11(8), p.820. (2023). 24. Monteiro, F. V. &amp; Ioannou, P. Safe autonomous lane changes and impact on traffic flow in a connected vehicle environment. Transportation research part C: emerging technologies, 151, p.104138. (2023). 25. Yuan, Q. et al. Decision-making and planning methods for autonomous vehicles based on multistate estimations and game theory. Advanced Intelligent Systems, 5(11), p.2300177. (2023). 26. Yara&#351;, N. Vehicle type classification with deep learning (Master&#8217;s thesis, Izmir Institute of Technology (Turkey)). (2020). 27. Tas, S. et al. Deep learning-based vehicle classification for low quality images. Sensors, 22(13), p.4740. (2022). 10.3390/s22134740 PMC9268885 35808251 28. Chowdhury, S., Chowdhury, S., Ifty, J. T. &amp; Khan, R. September. Vehicle detection and classification using deep neural networks. In 2022 International Conference on Electrical and Information Technology (IEIT) (pp. 95&#8211;100). IEEE. (2022). 29. Won M Intelligent traffic monitoring systems for vehicle classification: A survey IEEE Access. 2020 8 73340 73358 10.1109/ACCESS.2020.2987634 Won, M. Intelligent traffic monitoring systems for vehicle classification: A survey. IEEE Access. 8 , 73340&#8211;73358 (2020). 30. Mazhar, T. et al. Electric vehicle charging system in the smart grid using different machine learning methods. Sustainability, 15(3), p.2603. (2023). 31. Cao, X., Wu, C., Yan, P. &amp; Li, X. September. Linear SVM classification using boosting HOG features for vehicle detection in low-altitude airborne videos. In 2011 18th IEEE international conference on image processing (pp. 2421&#8211;2424). IEEE. (2011). 32. Yang B Lei Y Vehicle detection and classification for low-speed congested traffic with anisotropic magnetoresistive sensor IEEE Sens. J. 2014 15 2 1132 1138 10.1109/JSEN.2014.2359014 Yang, B. &amp; Lei, Y. Vehicle detection and classification for low-speed congested traffic with anisotropic magnetoresistive sensor. IEEE Sens. J. 15 (2), 1132&#8211;1138 (2014). 33. Yao, A. et al. FedShufde: A privacy preserving framework of federated learning for edge-based smart UAV delivery system. Future Generation Computer Systems, 166, p.107706. (2025). 34. Fang, K. et al. April. MoCFL: Mobile cluster federated learning framework for highly dynamic network. In Proceedings of the ACM on Web Conference 2025 (pp. 5065&#8211;5074). (2025). 35. Ali, A., Khan, M. A. &amp; Choi, H. Hydrogen storage prediction in dibenzyltoluene as liquid organic hydrogen carrier empowered with weighted federated machine learning. Mathematics, 10(20), p.3846. (2022). 36. Ali, Y., Han, K. H., Majeed, A., Lim, J. S. &amp; Hwang, S. O. An Optimal two-step Approach for Defense against Poisoning Attacks in Federated Learning (IEEE Access, 2025). 37. Majeed A Hwang SO A multifaceted survey on federated learning: Fundamentals, paradigm shifts, practical issues, recent developments, partnerships, trade-offs, trustworthiness, and ways forward IEEE Access. 2024 12 84643 84679 10.1109/ACCESS.2024.3413069 Majeed, A. &amp; Hwang, S. O. A multifaceted survey on federated learning: Fundamentals, paradigm shifts, practical issues, recent developments, partnerships, trade-offs, trustworthiness, and ways forward. IEEE Access. 12 , 84643&#8211;84679 (2024). 38. Deebak, B. D. &amp; Hwang, S. O. Federated learning-based lightweight two-factor authentication framework with privacy preservation for mobile sink in the social IoMT. Electronics, 12(5), p.1250. (2023). 39. Majeed, A., Zhang, X. &amp; Hwang, S. O. Applications and challenges of federated learning paradigm in the big data era with special emphasis on COVID-19. Big Data and Cognitive Computing, 6(4), p.127. (2022). 40. Yao, A., Pal, S., Dong, C., Li, X. &amp; Liu, X. March. A framework for user biometric privacy protection in UAV delivery systems with edge computing. In 2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops) (pp. 631&#8211;636). IEEE. (2024). 41. Yao, A. et al. A privacy-preserving location data collection framework for intelligent systems in edge computing. Ad Hoc Networks, 161, p.103532. (2024). 42. Dong, C. et al. May. A blockchain-aided self-sovereign identity framework for edge-based uav delivery system. In 2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (pp. 622&#8211;624). IEEE. (2021). 43. Yao, A. et al. October. A novel security framework for edge computing based uav delivery system. In 2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) (pp. 1031&#8211;1038). IEEE. (2021). 44. Dong, C., Pal, S., Chen, S., Jiang, F. &amp; Liu, X. A privacy-aware Task Distribution Architecture for UAV Communications System Using Blockchain (IEEE Internet of Things Journal, 2025)."
}