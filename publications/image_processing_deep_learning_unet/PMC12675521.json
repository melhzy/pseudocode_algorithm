{
  "pmcid": "PMC12675521",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.020985",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12675521",
    "pmid": "41339372",
    "doi": "10.1038/s41598-025-24174-6",
    "title": "Deep learning approach for crop-weed segmentation in peanut cultivation using PSPEdgeWeedNet",
    "year": "2025",
    "month": "12",
    "day": "3",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "3"
    },
    "authors": [
      "Pai Deepthi G",
      "Balachandra Mamatha",
      "Kamath Radhika"
    ],
    "abstract": "Weed management continues to be a significant challenge in modern agriculture, primarily due to the aggressive growth patterns of weeds and their direct competition with crops for essential resources such as light, water, and nutrients. Although recent developments in precision agriculture have led to the emergence of automated weed detection systems aimed at reducing operational costs and decreasing reliance on chemical herbicides, achieving accurate cropâ€“weed segmentation remains a persistent difficulty. This is largely attributed to high visual similarity between crops and weeds, coupled with variations in illumination and field conditions. To address these challenges, Convolutional Neural Networks (CNNs) have been increasingly adopted for their capability to perform end-to-end, pixel-level classification, particularly when leveraging multi-spectral imagery. In this context, PSPEdgeWeedNet is proposed, a novel edge-aware deep learning architecture tailored for precise semantic segmentation of crops and weeds within peanut cultivation fields. Distinct from the conventional Pyramid Scene Parsing Network (PSPNet) and its boundary-aware variant developed as a baseline in this research, PSPEdgeWeedNet introduces a dedicated edge detection branch. This branch is specifically engineered to enhance boundary localization and improve delineation between adjacent vegetation classes. In post-processing, Conditional Random Fields (CRFs) are used to slightly enhance the segmentation results around object boundaries. Additionally, all models were trained on a curated peanut field dataset using class-weighted loss functions to effectively address inherent class imbalance. Comprehensive experimental evaluations reveal that PSPEdgeWeedNet significantly outperforms existing state-of-the-art architectures including PSPNet, SegNet, UNet, DeepLabv3, Swin-Unet, and light weight transformer model based on ViT across multiple performance metrics such as Intersection over Union (IoU), precision, recall, and F1-score. These results highlight the critical role of incorporating edge-aware mechanisms within semantic segmentation frameworks, thereby enhancing the robustness and accuracy of automated weed detection systems in complex, real-world agricultural environments.",
    "keywords": [
      "Boundary loss",
      "Crop",
      "Edge aware networks",
      "Multiclass segmentation",
      "Object detection",
      "Precision agriculture",
      "PSPNet",
      "Segmentation",
      "Weed",
      "Plant sciences",
      "Environmental sciences",
      "Engineering"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12675521</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12675521.1</article-id><article-id pub-id-type=\"pmcaid\">12675521</article-id><article-id pub-id-type=\"pmcaiid\">12675521</article-id><article-id pub-id-type=\"pmid\">41339372</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-24174-6</article-id><article-id pub-id-type=\"publisher-id\">24174</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep learning approach for crop-weed segmentation in peanut cultivation using PSPEdgeWeedNet</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Pai</surname><given-names initials=\"DG\">Deepthi G</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\"/></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Balachandra</surname><given-names initials=\"M\">Mamatha</given-names></name><address><email>mamtha.bc@manipal.edu</email></address><xref ref-type=\"aff\" rid=\"Aff1\"/></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Kamath</surname><given-names initials=\"R\">Radhika</given-names></name><address><email>radhika.kamath@manipal.edu</email></address><xref ref-type=\"aff\" rid=\"Aff1\"/></contrib><aff id=\"Aff1\"><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02xzytt36</institution-id><institution-id institution-id-type=\"GRID\">grid.411639.8</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0571 5193</institution-id><institution>Manipal Institute of Technology, </institution><institution>Manipal Academy of Higher Education, </institution></institution-wrap>Manipal, Karnataka 576104 India </aff></contrib-group><pub-date pub-type=\"epub\"><day>3</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>43032</elocation-id><history><date date-type=\"received\"><day>16</day><month>5</month><year>2025</year></date><date date-type=\"accepted\"><day>10</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>05</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 00:25:12.533\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_24174.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Weed management continues to be a significant challenge in modern agriculture, primarily due to the aggressive growth patterns of weeds and their direct competition with crops for essential resources such as light, water, and nutrients. Although recent developments in precision agriculture have led to the emergence of automated weed detection systems aimed at reducing operational costs and decreasing reliance on chemical herbicides, achieving accurate crop&#8211;weed segmentation remains a persistent difficulty. This is largely attributed to high visual similarity between crops and weeds, coupled with variations in illumination and field conditions. To address these challenges, Convolutional Neural Networks (CNNs) have been increasingly adopted for their capability to perform end-to-end, pixel-level classification, particularly when leveraging multi-spectral imagery. In this context, PSPEdgeWeedNet is proposed, a novel edge-aware deep learning architecture tailored for precise semantic segmentation of crops and weeds within peanut cultivation fields. Distinct from the conventional Pyramid Scene Parsing Network (PSPNet) and its boundary-aware variant developed as a baseline in this research, PSPEdgeWeedNet introduces a dedicated edge detection branch. This branch is specifically engineered to enhance boundary localization and improve delineation between adjacent vegetation classes. In post-processing, Conditional Random Fields (CRFs) are used to slightly enhance the segmentation results around object boundaries. Additionally, all models were trained on a curated peanut field dataset using class-weighted loss functions to effectively address inherent class imbalance. Comprehensive experimental evaluations reveal that PSPEdgeWeedNet significantly outperforms existing state-of-the-art architectures including PSPNet, SegNet, UNet, DeepLabv3, Swin-Unet, and light weight transformer model based on ViT across multiple performance metrics such as Intersection over Union (IoU), precision, recall, and F1-score. These results highlight the critical role of incorporating edge-aware mechanisms within semantic segmentation frameworks, thereby enhancing the robustness and accuracy of automated weed detection systems in complex, real-world agricultural environments.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Boundary loss</kwd><kwd>Crop</kwd><kwd>Edge aware networks</kwd><kwd>Multiclass segmentation</kwd><kwd>Object detection</kwd><kwd>Precision agriculture</kwd><kwd>PSPNet</kwd><kwd>Segmentation</kwd><kwd>Weed</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Plant sciences</kwd><kwd>Environmental sciences</kwd><kwd>Engineering</kwd></kwd-group><funding-group><award-group><funding-source><institution>Manipal Academy of Higher Education, Manipal</institution></funding-source></award-group><open-access><p>Open access funding provided by Manipal Academy of Higher Education, Manipal</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">Weeds, often referred to as &#8220;plants out of place,&#8221; compete with crops for essential resources such as soil nutrients and water, leading to distinctive crop-weed interactions that can significantly reduce crop yields<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. However, the importance of weed management is often underestimated by both farmers and the general public due to the high costs associated with herbicides and manual weeding. Additionally, weeds serve as hosts for pests that can damage crops, further exacerbating agricultural losses<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. These weeds can also hinder crop growth. Therefore, it is essential to accurately identify and detect them<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>.</p><p id=\"Par3\">Arachis hypogaea L., or peanuts(groundnuts), are a great source of important nutrients and health advantages. However, several factors, including disease, insects, drought, climate change, soil fertility, and weed infestation, have an impact on peanut crop yields. Peanut production losses of up to 70% occur during the crucial 40&#8211;45&#160;day post-sowing phase for crop-weed competition<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. Therefore, detecting and removing weeds from peanut farms is essential to enhance crop yield.</p><p id=\"Par4\">Artificial Intelligence (AI) can play a significant role in agricultural tasks, with Deep Learning models enabling accurate classification and detection of weeds and crops in the field<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref>,<xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. Crop monitoring, yield forecasting, disease detection, soil health management, climate forecasting, supply chain optimization, genomics and breeding, precision agriculture, and market analysis have all been made possible by Deep Learning (DL), which has completely changed the agricultural industry<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>. These technologies analyze data from satellites, drones, and Internet of Things devices to identify weeds and pests, monitor crop health, diagnose crop and leaf diseases, estimate yields, and optimize fertilization and irrigation<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup>. Additionally, they aid in the early identification of crop diseases, enabling timely action to prevent crop loss. DL models support breeding and genomics, supply chain optimization, and climate predictions<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>. DL-based precision agriculture uses real-time data to optimize resource utilization and reduce environmental impacts<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>. Figure&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref> shows different DL models used for weed detection purposes.</p><p id=\"Par5\">\n<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Deep Learning models used in weed detection.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig1_HTML.jpg\"/></fig>\n</p><p id=\"Par6\">Semantic segmentation, a key technique in DL, can be effectively utilized for crop and weed detection purposes<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>. Every pixel in the image is classified into one of the pre existing classes using semantic segmentation. As a result, it offers more accurate crop and weed location data<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>. These Deep Neural Net based techniques, which have many tuning parameters, usually need a few thousand to tens of thousands of images to train to provide sufficient performance and prevent overfitting<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>. However, standard segmentation models often struggle with accurately distinguishing weed boundaries, leading to misclassification errors<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>.</p><p id=\"Par7\">One of the widely used models for semantic segmentation is PSPNet (Pyramid Scene Parsing Network)<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>. However, it has certain limitations, including poor boundary delineation, low Intersection over Union (IoU) for small and rare classes, and high sensitivity to background noise. Despite the growing use of DL models in precision agriculture, accurately segmenting crops and weeds in field conditions remains a challenging task due to high visual similarity between classes, varying illumination, and complex backgrounds. Existing semantic segmentation models such as UNet, SegNet, DeepLabV3, PSPNet, and transformer- based architectures like Swin-UNet and lightweight Vision Transformer (ViT) variants often struggle with precise boundary delineation, leading to misclassifications, particularly in tightly clustered or overlapping vegetation.</p><p id=\"Par8\">To address this gap, the authors propose PSPEdgeWeedNet, a novel semantic segmentation architecture designed specifically for crop&#8211;weed discrimination in peanut fields. The model integrates a boundary-aware loss function and a dedicated edge detection branch to enhance localization at object borders. Furthermore, Conditional Random Fields (CRFs) are used in post-processing to refine the segmentation masks. The model was also qualitatively evaluated on a separate peanut dataset collected from the Udupi region of Karnataka, which contains different weed species and varying lighting conditions. While the model was not trained on this dataset, it successfully identified every crop instance, indicating promising generalization capabilities.Experimental results demonstrate that PSPEdgeWeedNet outperforms the aforementioned models across all major performance metrics, including precision, recall, F1-score, and Intersection over Union (IoU), thereby establishing its effectiveness in handling the nuanced challenges of field-based weed detection.</p><p id=\"Par9\">The model is thoroughly tested on a peanut crop-weed dataset and compared against the following methods:<list list-type=\"bullet\"><list-item><p id=\"Par10\">PSPNet, serving as the baseline semantic segmentation model.</p></list-item><list-item><p id=\"Par11\">Boundary-aware PSPNet, which integrates a boundary-sensitive loss function and investigates different class weighting schemes to address class imbalance.</p></list-item><list-item><p id=\"Par12\">Other widely used semantic segmentation models, including UNet, SegNet, and DeepLabv3, Swin-Unet, light weight transformer model based on ViT.</p></list-item><list-item><p id=\"Par13\">The model was tested on a peanut dataset collected from the Udupi region, which includes diverse weed species and varying lighting conditions.</p></list-item></list></p><p id=\"Par14\">This study aims to develop a novel PSPNet-based architecture augmented with a boundary-aware loss function and an edge detection branch, aiming to improve boundary accuracy while maintaining computational efficiency suitable for real-time deployment.</p><sec id=\"Sec2\"><title>The significance of edge detection in segmentation models</title><p id=\"Par15\">The primary goal of edge detection is to pinpoint the specific regions or pixels within an image that represent boundaries or transitions&#8212;typically where there are sharp changes in intensity or color&#8212;indicating the presence of edges<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup>. The technique&#8217;s working premise states that an edge is any pixel in the chosen band of the RGB image that has a value different from that of its neighbors<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>.</p><p id=\"Par16\">Edge detection is a crucial technique in segmentation models, particularly in tasks like crop and weed detection<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. It enhances object boundaries, reduces misclassification, differentiates objects with similar textures or colors, and prevents background noise interference. Edge detection also improves IoU for small objects, preserving their structure and boosting segmentation performance. It complements global context learning by adding local fine-grained details, leading to more precise segmentation masks. In this study, edge detection refers to the limits of crop, weed, and background, indicating the significant difference between the object and nearby objects. Sobel edge detection is a gradient-based method that determines the rate at which an image&#8217;s intensity changes and uses the gradient magnitude to identify both horizontal and vertical edges<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>. The Sobel filter in the early layers of the proposed PSPEdgeWeedNet model improves segmentation efficiency and reduces noise, thereby boosting edge detection performance.</p></sec></sec><sec id=\"Sec3\"><title>Motivation of this Study</title><p id=\"Par17\">Despite reaching excellent overall accuracy in agricultural applications, current semantic segmentation algorithms often fail to provide the precise border delineation required for successful precision farming. This paper proposes a novel PSPNet-based architecture to bridge the crucial gap in boundary-aware agricultural segmentation, enhanced by boundary-aware loss functions and a dedicated edge detection branch. The proposed approach focuses on the difficulty of accurately identifying crop-weed boundaries, which is important for precision mechanical weeding, focused herbicide application, and autonomous agricultural operations. The suggested framework prioritizes edge-critical areas, as opposed to previous approaches that provide equal weight to every pixel. For real-time agricultural applications, this results in notably improved boundary accuracy while maintaining computing efficiency.</p></sec><sec id=\"Sec4\"><title>Related works</title><p id=\"Par18\">With its exceptional ability to differentiate between crops and weeds in a variety of agricultural settings, semantic segmentation has become a game-changing technique in precision agriculture. Several research projects have achieved F1-scores above 90% and mean Intersection over Union (mIoU) values as high as 92%, demonstrating the technology&#8217;s consistently outstanding performance metrics. Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> summarizes different DL methods utilized in agricultural domains, detailing their aim, model architectures, datasets used, and associated limitations.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Analysis of different deep learning techniques in the agricultural domain.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Citation</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Aim of the paper</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Models or algorithms</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Limitations</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">To improve pixel-level precision in distinguishing crop (tobacco) and weed pixels using a two-stage semantic segmentation approach, enhancing classification performance over traditional one- stage models</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Stage 1(Binary classifier) Stage 2 (three class clas- sifier)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">A newly captured aerial tobacco crop dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">The two-stage approach doubles the computation compared to one-stage models and even though inference time is still acceptable for real-time, it is costlier computationally</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Weed and crop segmentation network that improves performance in recognizing weeds of any shape in complex environments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Stuttgart and Bonn datasets</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lack of Generalization Evidence Resource Demanding</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Detects vegetation outside the crop mask as weeds by combining semantic segmentation and image processing</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>CCNet </p><p>GCNet </p><p>ISANet </p><p>DeepLabV3 DeepLabV3&#8201;+&#8201;</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cornfields near Shenyang, Liaoning Province, China</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Limited Temporal and Geographic Diversity</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">An automated semantic segmentation- based approach for multiclass weed identification in precision agriculture</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net based on Inception-ResNetV2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Brinjal farms in Gorakhpur, Uttar Pradesh, India</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Use of RGB Images only Assumption of Uniform Lighting and Weather Conditions</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Advanced semantic segmentation model for accurate crop&#8211;weed differentiation, enhancing precision weeding under natural field conditions</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">EPAnet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bean sprout cultivation base in Avignon, France</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Limited Generalizability</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lightweight, real-time weed segmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>CCNet</p><p>GCNet</p><p>ISANet</p><p>DeepLabV3 DeepLabV3&#8201;+&#8201;</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cornfields near Shenyang, Liaoning Province, China</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Geographic and Temporal Limitation Indirect Weed Segmentation</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Embeds a SE module for soybean weeds in a genuine field setting and suggests an enhanced UNet structure</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">UNet ResNet34</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Soybean experimental field of Jilin Agricultural University</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Real-time deployment on edge devices for in-field precision spraying remains a challenge due to computational complexity</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Weed recognition model based on improved Swin-Unet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>UNet </p><p>Swin-Unet </p><p>DeepLabv3 </p><p>Mask R-CNN</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Maize test field in Zibo, Shan-dong Province, China</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Approach may be less effective in scenarios where crops and weeds are heavily interwoven or where morphological differences are subtle, leading to potential misclassification</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Suggest an enhanced model based on YOLOv4 for detecting weeds in potato fields</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved YOLOv4 al-gorithm</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Images were taken from the test site in China</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data set collected from an experimental field, not from the real field</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposes a Faster R-CNN network architecture for identifying weeds in cropping region images</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Faster RCNN FPN Improved ResNeXt101</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">V2 Plant Seedlings Dataset</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">The paper lacks details on the potential effects of lighting conditions or image quality on the model&#8217;s performance</td></tr></tbody></table></table-wrap></p><sec id=\"Sec5\"><title>Advanced neural network architectures and performance achievements</title><p id=\"Par19\">Agricultural segmentation challenges have demonstrated the effectiveness of DL systems. On brinjal (eggplant) farm datasets, a U-Net model with Inception-ResNetV2 backbone architecture obtained an F1-score of 96.78%, proving the value of fusing robust feature extraction networks with segmentation frameworks<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>. Researchers created an ERFNet-based multi-decoder architecture with a dual-loss function mechanism, furthering the field&#8217;s advancement<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>. Quantifiable performance increases of 1.91% in mIoU and 1.19% in Frequency Weighted Intersection over Union (FWIoU) were achieved by this novel technique, which focused on improving model attention on important crop and weed categories. The model&#8217;s improved performance in challenging field settings shows how useful it is in actual agricultural scenarios in which environmental influences can have a big influence on identification accuracy.</p></sec><sec id=\"Sec6\"><title>Integration of knowledge distillation and real-time optimization</title><p id=\"Par20\">Researchers are investigating knowledge distillation approaches in conjunction with conventional image processing methods as a result of the desire for real-time agricultural monitoring. One important research, which focused on maize segmentation applications, combined the DeepLabV3&#8201;+&#8201;architecture with traditional image processing methods and knowledge distillation frameworks<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>. The crucial problem of preserving high accuracy while attaining real-time speed and cutting computational overhead through model size optimization was effectively handled by this hybrid technique.</p></sec><sec id=\"Sec7\"><title>Analysis of segmentation architectures in specific crop environments</title><p id=\"Par21\">Research on paddy field applications has shed important light on how well various segmentation frameworks perform in comparison. Extensive comparison analyses shown that in rice field environments, Pyramid Scene Parsing Network (PSPNet) continuously outperformed both UNet and SegNet designs, attaining mIoU values in the 70%&#8211;80% range<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>. The significance of choosing suitable designs depending on particular crop traits and field circumstances is shown by this performance difference. The development of UNet architectures has produced complex variations with cutting-edge training techniques. With mIoU values as high as 92.34%, a UNet++ implementation with deep supervision techniques demonstrated exceptional performance in sugar beet field applications<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup>. This architecture showed the benefits of architectural improvements and sophisticated training approaches in agricultural segmentation tasks by considerably outperforming baseline UNet++ implementations as well as classic UNet models.</p><p id=\"Par22\">In agricultural applications, the combination of object detection and semantic segmentation approaches has demonstrated encouraging outcomes. A rich Sugarbeets dataset with 1,300 images was used to train a hybrid model that combined semantic segmentation with YOLO (You Only Look Once) object recognition<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>. The potential advantages of merging complementing computer vision techniques for improved agricultural monitoring were demonstrated by this integrated system, which achieved 93% accuracy.</p><p id=\"Par23\">Agricultural imaging frequently takes place in difficult settings, especially when drone-based systems are being used, which can cause motion blur. Researchers created the customized architecture DeBlurWeedSeg to deal with motion-blurred drone footage after realizing this constraint<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup>. A major practical issue in aerial agricultural monitoring systems was resolved by this creative method, which much outperformed traditional segmentation algorithms without deblurring capabilities.</p><p id=\"Par24\">Semantic segmentation performs well for weed-crop identification across different crops; mIoU values of up to 92% and F1-scores of over 90% have been observed. Custom designs such as DeBlurWeedSeg, ERFNet with dual-loss, UNet++ with deep supervision, and U-Net based on Inception-ResNetV2 improve accuracy and robustness while addressing real-world issues. Lightweight models and knowledge distillation enable real-time performance, while hybrid approaches improve practical utility. The quality and diversity of datasets are crucial for better learning and broader generalizability. The development of automated, accurate, and effective crop monitoring systems has advanced significantly with these developments in semantic segmentation for agricultural applications. Through more precise and timely weed-crop distinction capabilities, the technology&#8217;s ongoing development promises to improve crop production projections, maximize resource management, and advance sustainable agricultural methods.</p><p id=\"Par25\">Despite the outstanding F1-scores and mIoU metrics that contemporary deep learning models, such U-Net++, DeepLabV3+, and PSPNet, show across a variety of crops and environments, the majority of methods have some significant drawbacks. Without explicit edge modeling, it is still difficult to distinguish between weeds and crops. Additionally, most research rely on single-field or crop-specific data, making it uncommon to examine generality across datasets or geographic regions. Also neglected are post-processing methods that may greatly improve fine-grained segmentation, such as CRF. By combining an edge detection module, boundary-aware loss to sharpen object edges, using CRF-based post-refinement to lower segmentation noise, and verifying generalization ability across datasets, our suggested PSPEdgeWeedNet directly tackles these issues. Combined, these enhancements address the significant flaws in earlier research and help create more accurate semantic segmentation systems for practical agricultural monitoring.</p></sec><sec id=\"Sec8\"><title>Multispectral and NIR-based approaches in agricultural segmentation</title><p id=\"Par26\">Multispectral and near-infrared (NIR) data have been used more and more in precision agriculture in recent years to increase the accuracy of crop and weed segmentation. The pioneering public dataset RafanoSet<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup> includes 85 multispectral images of fields of Triticum aestivum that include Raphanus raphanistrum. The images are annotated at the pixel level to locate weeds. The images, which were taken in 17 different scenarios and contain spectral bands including Blue, Green, Red, NIR, and RedEdge, provide a wealth of information for segmentation model training and evaluation. An improved SegFormer architecture was presented in a related effort to use UAV-based images to extract spatial patterns in agricultural landscapes<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>. With 98.42% pixel accuracy and 96.91% IoU, the model&#8217;s integration of Efficient Channel Attention, BiFPN layers, and MLP-based modules enhances feature extraction and yields noteworthy results, which are especially advantageous for small-scale farms and field borders. Furthermore, as an alternative to conventional data augmentation in crop-weed segmentation tasks, a conditional GAN (cGAN) was used in another work to create realistic multispectral images, encompassing RGB and NIR channels<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>. In addition to improving visual realism, using synthetic data increased semantic segmentation network&#8217;s accuracy. Additionally, hyperspectral imaging has demonstrated potential; in one research, five weed species and soybean plants were classified using a robotic hyperspectral scanning platform in a greenhouse setting<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>. The study highlighted unique chemical fingerprints among species and obtained good classification accuracy by using 983 hyperspectral cubes and Savitzky&#8211;Golay derivative preprocessing. Additionally, early identification of the parasitic weed known as sunflower broomrape was accomplished with hyperspectral imaging, with classification accuracies of 76% and 89% at 31 and 38 days after planting, respectively<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>. The study illustrated the potential of hyperspectral methods for prompt, site-specific weed treatment by examining certain leaf segments. When taken as a whole, these studies highlight how multispectral and hyperspectral imaging are increasingly helping to advance precision farming by improving plant discriminating and detecting weeds early.</p></sec></sec><sec id=\"Sec9\"><title>Materials and methods</title><sec id=\"Sec10\"><title>Dataset</title><p id=\"Par27\">The peanut (groundnut) dataset employed in this study was sourced from a publicly available GitHub repository<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup> and is specifically designed to support research in crop-weed segmentation. It comprises 400 high-resolution RGB images, each with a resolution of 720 &#215; 960 pixels, captured from real-world peanut fields located near Da Nang, Vietnam. These images provide a rich and diverse visual representation of peanut crops alongside several common weed species, most notably goose grass and nut grass.</p><p id=\"Par28\">The dataset is annotated with two semantic classes: crop (representing peanut plants) and weed (aggregating all types of invasive species). The annotation process was meticulously performed under the consultation of domain experts in agriculture, ensuring high-quality ground-truth labels. On average, each image required around 20 minutes to annotate due to the intricate and detailed nature of the scenes.</p><p id=\"Par29\">Figure&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> illustrates the peanut dataset showcasing diverse and complex weed scenarios, emphasizing the challenges in accurate weed detection.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>A peanut example dataset is used to illustrate the dataset&#8217;s diversity and recognition challenges. The images depict a range of field scenarios, including single, tiny, and mixed weed species, as well as crops and weeds with intricate backgrounds. These illustrate the challenges of weed detection technologies by showcasing the seemingly complicated and identical backgrounds of numerous weed types<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig2_HTML.jpg\"/></fig></p><p id=\"Par30\">This dataset presents several complex segmentation scenarios that are representative of actual field conditions. These include the occurrence of small-scale weed patches, inter-plant occlusions, and large differences in plant size, all of which greatly complicate accurate segmentation. Specifically, the visual resemblance and dense spatial distribution of weeds and crops, as well as background clutter, necessitate models that can capture contextual cues and fine-grained limits. The peanut dataset represents a rigorous standard for assessing the efficacy and resilience of contemporary Deep Learning-based semantic segmentation models in precision agriculture because of these features. Sample original and annotated images from the dataset are shown in Figs.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Representative samples from the dataset: (left) original RGB image and (right) corresponding annotated image, where green denotes crop regions and red indicates weed areas<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig3_HTML.jpg\"/></fig></p></sec><sec id=\"Sec11\"><title>Experimental setup</title><p id=\"Par31\">The experiments in this study were conducted on a computational platform equipped with an NVIDIA A100 GPU. The experimental environment utilized the PyTorch framework, running on a Microsoft Windows 11 system with 8 GB of RAM. The platform supported CUDA 11.8, enabling efficient GPU acceleration for Deep Learning tasks. All experiments were implemented using Python 3.10.12.</p></sec><sec id=\"Sec12\"><title>Training parameters</title><p id=\"Par32\">To enhance model performance, the training parameters were systematically fine-tuned through multiple experimental iterations. A resolution of 600&#8201;&#215;&#8201;600 pixels was selected for the input images, as it provides an optimal balance between computational efficiency and the retention of key image features. While smaller image sizes could result in the loss of crucial information, larger sizes would unnecessarily increase the computational load without offering significant performance benefits. The number of epochs was set to 100 to ensure sufficient learning while avoiding overfitting.</p><p id=\"Par33\">The model&#8217;s performance was optimized using an Adam optimizer with a learning rate of 1e-4 and a batch size of 16. Using a pretrained ResNet101 backbone, the model architecture used a custom PSPNet with edge detection included. Dense CRF was used as a post-processing step to further improve the visualization of segmentation findings. By successfully resolving minor errors and enhancing boundary accuracy, this method, when used for visualization purposes after inference, provides cleaner segmentation boundaries.</p><p id=\"Par34\">Effective feature extraction is made possible by using a pretrained Convolutional Neural Network (CNN) as the backbone. This promotes quicker convergence and improved performance with less data. To balance the pace of convergence and minimize overfitting, the learning rate was carefully tuned during training. The dataset was randomly split into 80% for training, 10% for validation, and 10% for testing. Although the split was random and not stratified, class-weighted loss functions were used to address class imbalance across the three semantic classes&#8212;Crop, Weed, and Background. Specifically, class weights of 2 for Crop, 3 for Weed, and 1 for Background were applied to ensure that minority classes (Crop and Weed) had a greater influence during model optimization. Since the usage of augmentation techniques such random rotations, color jitter, and horizontal and vertical flips resulted in lower metric values, including decreased accuracy, the model trained without data augmentation was taken into consideration. Furthermore, in contrast, the visuals of predictions were substantially worse with data augmentation. This might be explained by the model&#8217;s inability to generalize well after being trained on sparse data with augmentation, which resulted to noisy predictions and imprecise limits. The model&#8217;s ability to concentrate on the dataset&#8217;s essential characteristics without augmentation resulted in more consistent and precise performance. A boundary-aware loss was used to improve edge detection performance, and the Cross-Entropy Loss function was used for the segmentation task, with weighted class contributions to rectify the class imbalance.</p><p id=\"Par35\">The model contains a total of 48,271,428 trainable parameters, which are learned during the training process. This large number of parameters indicates an architecture capable of capturing intricate patterns in the data, allowing the model to potentially learn highly detailed features.</p></sec><sec id=\"Sec13\"><title>Evaluation metrics</title><p id=\"Par36\">This research uses important evaluation metrics, including as training and validation accuracy, precision, recall, F1 score, and Intersection over Union (IoU), to thoroughly assess the model&#8217;s effectiveness and performance in semantic segmentation tasks.<list list-type=\"bullet\"><list-item><p id=\"Par37\">Accuracy: Accuracy measures the number of pixels correctly classified as weeds or crops.</p></list-item><list-item><p id=\"Par38\">Precision: Precision quantifies the number of true positive cases that a model predicts, or the accuracy of the model&#8217;s positive predictions.</p></list-item><list-item><p id=\"Par39\">Recall: The frequency with which a model accurately detects positive examples (true positives) out of all the actual positive samples in the dataset is known as recall.</p></list-item><list-item><p id=\"Par40\">F1 score: It is the harmonic mean of precision and recall, giving a fair assessment of a model&#8217;s precision (the capacity to detect positive occurrences) and recall (the ability to minimize false positives).</p></list-item><list-item><p id=\"Par41\">IoU: A metric used to evaluate the accuracy of object detection models and segmentation models.</p></list-item><list-item><p id=\"Par42\">Confusion Matrix: In semantic segmentation, a confusion matrix is used to evaluate how well a model classifies each pixel into the correct class.</p></list-item><list-item><p id=\"Par43\">Visualization: Visualizing segmentation results alongside the ground truth helps understand where the model makes errors, particularly in boundary definitions and tiny object detection.</p></list-item></list></p></sec></sec><sec id=\"Sec14\"><title>Proposed model&#8212;PSPEdgeWeedNet</title><p id=\"Par44\">Figure&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref> depicts basic blocks used in the proposed work.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Basic blocks used in proposed work.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig4_HTML.jpg\"/></fig></p><p id=\"Par45\">The proposed model PSPEdgeWeedNet is a modified version of PSPNet (Pyramid Scene Parsing Network), specifically tailored for semantic segmentation tasks in agriculture, such as accurately distinguishing between crops and weeds. It utilizes ResNet101 as the backbone, a deep convolutional neural network pretrained on ImageNet, which effectively extracts rich feature representations while preserving spatial details essential for segmentation. To enhance contextual understanding, a pyramid pooling module (PSP block) is integrated, enabling the model to capture information at multiple scales. This is particularly valuable for agricultural images, where objects may appear in varied sizes and complex backgrounds are common. An additional edge detection module is incorporated into the architecture to improve the delineation of object boundaries.</p><p id=\"Par46\">This component learns edge features from the same feature maps used for segmentation and generates an edge map to guide&#160;the model in identifying class borders more precisely. To reinforce this behavior during training, a custom Boundary Loss is introduced. It applies a Sobel operator to both the predicted segmentation output and the ground truth mask to extract edge information and then computes the mean squared error between them. In conjunction with the conventional CrossEntropyLoss, this boundary-aware loss encourages more precise boundary predictions and lessens class overlap. To improve the visual depiction of the segmentation precision, a Conditional Random Field (CRF) is also used as a post-processing step during the visualization phase to maintain fine details and sharpen boundaries.</p><p id=\"Par48\">In summary, a dual-loss approach, edge detection for boundary refinement, and pyramid pooling for global context work together to produce more precise and thorough segmentation. This is particularly useful in agricultural settings where accurate and contextually aware segmentation is necessary due to the minor variations between weeds and crops.</p><p id=\"Par49\">This approach ensures that both segmentation accuracy and edge refinement are optimized. Figure&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref> illustrates the block diagram of the proposed PSPEdgeWeedNet model.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Proposed PSPEdgeWeedNet block diagram.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig5_HTML.jpg\"/></fig></p><sec id=\"Sec15\"><title>Edge detection module and boundary aware loss</title><p id=\"Par50\">By including an edge detection module, the proposed PSPEdgeWeedNet enhances localization in high-resolution agricultural images by learning semantic areas such as crop, weed, and background. The module adds an edge detection head for the convolutional layer and pulls feature maps from the ResNet101 backbone. This head highlights changes between semantic classes by learning a 1-channel edge probability map. Edge prediction is guided by using a unique boundary-aware loss based on Sobel edge detection.</p><p id=\"Par51\">To guarantee distinct, precise object borders and oversee edge prediction, the Boundary-aware Loss technique is employed. By extracting gradient magnitude from both ground truth and anticipated segmentation maps using Sobel filters and minimizing the difference using Mean Squared Error, it ensures clear and well-aligned borders in challenging agricultural images.</p><p id=\"Par52\">Boundary-aware loss implementation works by combining traditional semantic segmentation loss with a specialized boundary detection loss to enhance segmentation accuracy at class boundaries. It extracts edge information from predicted segmentation maps and ground truth labels using Sobel edge detection filters. The Sobel operator applies two 3&#8201;&#215;&#8201;3 convolution kernels to compute gradient magnitudes, highlighting boundary regions. The edge strength is calculated as the square root of the sum of squared gradients in both directions, creating edge maps emphasizing transitions between different classes like crops, weeds, and background. The boundary loss is computed as the Mean Squared Error (MSE) between the predicted edge map and the ground truth edge map, measuring the alignment of the model&#8217;s predicted boundaries with the actual class boundaries in the labeled data. This loss function optimizes for correct pixel-wise classification and accurate boundary delineation. By penalizing boundary misalignment, the model learns to pay special attention to boundary regions where segmentation errors are most visually apparent and functionally important, particularly in crop/weed segmentation tasks. This dual-objective approach helps the model achieve better overall segmentation performance by ensuring that class transitions are sharp and well-defined, rather than just focusing on broad regional classification accuracy.</p><sec id=\"Sec16\"><title>Boundary-aware loss function</title><p id=\"Par53\">To enhance the precision of object boundaries in semantic segmentation, the authors introduce a <italic toggle=\"yes\">boundary-aware loss</italic> based on Sobel edge detection. This loss penalizes differences between the predicted and ground truth edge maps.</p><p id=\"Par54\"><italic toggle=\"yes\">Predicted Segmentation</italic> Let the network output be a raw score tensor:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e701\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P(x) \\in {\\text{R}}^{H \\times W \\times C}$$\\end{document}</tex-math></disp-formula>where <italic toggle=\"yes\">H</italic>, <italic toggle=\"yes\">W</italic> , and <italic toggle=\"yes\">C</italic> denote the height, width, and number of classes, respectively.</p><p id=\"Par55\">The predicted class label for each pixel is obtained via:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e717\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{Y}(x) = \\mathop {{\\text{argmax Softmax}}(P(x)) }\\limits_{c}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq1\"><tex-math id=\"d33e722\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\hat{Y}(x) \\in \\{ 0,{1},...,C - {1}\\}^{{^{H \\times W} }}$$\\end{document}</tex-math></inline-formula> is the class prediction map.</p><p id=\"Par56\"><italic toggle=\"yes\">Ground Truth Segmentation</italic> Let the ground truth label map be:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e730\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Y(x) \\in \\{ 0,{1},...,C - {1}\\}^{{^{H \\times W} }}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par57\"><italic toggle=\"yes\">Sobel Edge Detection</italic> To extract edge information, we apply Sobel operators in both horizontal (<italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">x</italic></sub>) and vertical (<italic toggle=\"yes\">S</italic><sub><italic toggle=\"yes\">y</italic></sub>) directions:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e750\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$S_{x} = \\begin{array}{*{20}c} { - {1}} &amp; 0 &amp; {1} \\\\ { - {2}} &amp; 0 &amp; {2} \\\\ { - {1}} &amp; 0 &amp; 1 \\\\ \\end{array} \\;S_{y} = \\begin{array}{*{20}c} { - 1} &amp; { - 2} &amp; { - 1} \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{array}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par58\">The edge maps of the predicted and ground truth masks are calculated as:<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e756\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_{{\\hat{Y}}} = \\sqrt {(S_{x} * \\hat{Y})^{{2}} + (S_{y} * \\hat{Y})^{{2}} }$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e760\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E_{Y} = \\sqrt {(S_{x} * Y)^{{2}} + (S_{y} * Y)^{{2}} }$$\\end{document}</tex-math></disp-formula></p><p id=\"Par59\">where &#8727; denotes 2D convolution, and <inline-formula id=\"IEq2\"><tex-math id=\"d33e766\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$E \\hat{Y},E_{Y} \\in {\\text{R}}^{{^{H \\times W} }}$$\\end{document}</tex-math></inline-formula> represent the edge magnitude maps of the prediction and ground truth<italic toggle=\"yes\">.</italic></p><p id=\"Par61\"><italic toggle=\"yes\">Boundary-aware Loss</italic> The loss is defined as the mean squared error between the predicted and ground truth edge maps:<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e776\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_\\textrm{boundary} =\\frac{1}{H \\times W} \\sum\\limits_{i = 1}^{H} \\sum\\limits_{j = 1}^{W} {(E \\hat{Y}(i,j) - E_{Y} (i,j))^2} $$\\end{document}</tex-math></disp-formula></p><p id=\"Par62\">This encourages the predicted segmentation boundaries to align closely with those in the ground truth.</p><p id=\"Par63\"><italic toggle=\"yes\">Total Loss</italic> The boundary-aware loss is used in conjunction with the standard cross-entropy loss:<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e786\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{{{\\text{total}}}} = L{_\\text{CE}} + \\lambda \\cdot L_{{{\\text{boundary}}}}$$\\end{document}</tex-math></disp-formula>where <italic toggle=\"yes\">L</italic><sub>CE</sub> is the pixel-wise cross-entropy loss, and <italic toggle=\"yes\">&#955;</italic> is a weighting factor controlling the influence of boundary supervision. Based on the performance metrics obtained after training the models and evaluating them on the validation dataset, it was observed that PSPNet with class weights set to crop&#8201;=&#8201;2, weed&#8201;=&#8201;3, and background&#8201;=&#8201;1 (c&#8201;=&#8201;2, w&#8201;=&#8201;3, b&#8201;=&#8201;1) achieved superior results compared to other class weighting strategies. Furthermore, integrating a boundary-aware loss function into PSPNet led to noticeable improvements in segmentation accuracy, precision, recall, and F1-score when compared to the baselinePSPNet model. Building on these findings, a novel model, PSPEdgeWeedNet, was proposed. This model retained the optimal class weights (2, 3, 1) and incorporated both the boundary-aware loss and an additional edge detection module to explicitly refine object boundaries. The inclusion of the edge detection branch further enhanced the segmentation performance across all evaluation metrics, including per-class Intersection over Union (IoU), overall precision, recall, and F1-score as shown in Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>. The trend of validation metrics across epochs indicates a consistent improvement in precision, recall, and F1 scores throughout the training process. However, a slight dip in all three metrics is noticeable around the 30th to 35th epochs, which may be attributed to temporary model instability or overfitting fluctuations. Following this phase, the metrics recover and continue to improve, ultimately reaching approximately 93%, indicating strong generalization performance in later epochs.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Validation metrics over epochs in PSPEdgeWeedNet.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig6_HTML.jpg\"/></fig></p><p id=\"Par64\">Figure.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref> illustrates the radar chart comparing precision, recall, F1 score, and IoU across each class in the proposed model. The background class consistently achieves the highest scores across all metrics, followed by the crop class. The weed class ranks lowest; however, it still attains approximately 60% in both precision and F1 scores, and nearly 70% in recall, indicating reasonable detection performance.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Radar chart using the PSPEdgeWeedNet model that shows the IoU, F1 score, recall, and precision for each class.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig7_HTML.jpg\"/></fig></p><p id=\"Par65\">Conditional Random Fields (CRF) were applied as a post-processing refinement technique during the visualization phase. CRF significantly enhanced the delineation of crop and weed boundaries and reduced inter-class misclassifications, especially in overlapping regions. Although the model occasionally missed very small or sparse weed instances, the overall boundary precision in visual outputs was markedly improved, demonstrating the effectiveness of the integrated architecture and post-processing strategy.</p><p id=\"Par66\">The Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref> shows the visualization of ground truth and prediction where the proposed model failed to detect the smaller instances of weed. It shows that despite its excellent performance, the proposed PSPEdgeWeedNet model had trouble identifying microscopic or tiny weed instances, especially in situations where the weeds were partially obscured or sparsely scattered. In the segmentation result, these smaller weeds were either completely ignored or incorrectly categorized as background. Reduced saliency of fine-scale information in deeper network layers and possible loss of spatial resolution during the encoding process are the causes of this problem. Deep segmentation networks frequently face the difficulty of identifying tiny objects, particularly when the item size is smaller than the network&#8217;s receptive field. The fine-grained patterns linked to smaller weeds might not have been adequately captured by the model&#8217;s edge refinement and pyramid pooling techniques.<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Visualization of ground truth and prediction where the proposed model PSPEdgeWeedNet failed. Green color represents crop, red color for weed, and black color for the background. Misclassifications and missed detections are highlighted using yellow boxes.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig8_HTML.jpg\"/></fig></p><p id=\"Par67\">The model&#8217;s ability to differentiate between the three classes, crop, weed, and background, is demonstrated by the normalized confusion matrix as shown in Fig.&#160;<xref rid=\"Fig9\" ref-type=\"fig\">9</xref>. With a high true positive rate of 91%, the model has a remarkable capacity to recognize crop pixels, whereas just 1% are misclassified as weed and 8% are misclassified as background. The visual elements of weed and the other two classifications overlap, as seen by the fact that weed pixels are properly categorized 69% of the time, although there is a noticeable misunderstanding with background (23%), and 8% are misclassified as crop. With a 95% classification accuracy, the background class shows the least amount of misunderstanding with crop (3%) and weed (2%). Table&#160;<xref rid=\"Tab2\" ref-type=\"table\">2</xref> presents a comparative analysis of PSPNet, PSPEdgeWeedNet without CRF, and PSPEdgeWeedNet with CRF. The results indicate that the integration of CRF in the proposed PSPEdgeWeedNet significantly enhances boundary delineation for both weeds and crops, and effectively preserves leaf structures. Moreover, the model demonstrates zero misclassification, thereby contributing to an overall improvement in segmentation quality.<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>Confusion matrix of proposed model PSPEdgeWeedNet.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO9\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig9_HTML.jpg\"/></fig><table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Comparison of PSPNet and PSPEdgeWeedNet variants.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Aspect</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSPNet</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet without CRF</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet with CRF</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Boundary definition</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Poor; boundaries between crops and weeds are blurred</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved boundary sharpness compared to PSPNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Well-defined and continuous boundaries</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Weed leaf structure preservation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Inadequate; leaf structures are not clearly captured</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Partial preservation; leaf edges are still not sharply structured</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Good preservation; leaf morphology is maintained</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Misclassification</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Present; crops misclassified as weeds</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Minimal; misclassifications largely eliminated</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">No significant misclassification observed</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Edge sharpness</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low; edges appear soft and diffused</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate; improved but not optimal</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High; edges are sharp and detailed</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tiny weed detection</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Missed frequently</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Slight improvement, but still limited</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Some very small weeds missed, but overall better retention</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Overall segmentation quality</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Suboptimal</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate; better than baseline</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Significantly improved</td></tr></tbody></table></table-wrap></p></sec></sec><sec id=\"Sec17\"><title>Limitations and generalization capability</title><p id=\"Par68\">One of the key limitations of this study lies in the relatively small size and geographic scope of the training dataset, which comprises only 400 images of peanut crops collected from a single region. Such a dataset may not fully capture the variability in field conditions, weed species, or lighting scenarios encountered in other agricultural environments. To evaluate the model&#8217;s generalizability, we tested the trained network on an independent groundnut dataset collected from the Udupi region of Karnataka. This dataset contains a diverse range of weed species, including different types of broadleaved weeds and grasses, and was captured under varying lighting conditions. Without any additional retraining or fine-tuning, the model was able to correctly identify groundnut crops and detect certain broadleaved weeds and grasses in several samples, demonstrating a level of robustness and transferability. However, the model struggled to recognize other types of broadleaved weeds, indicating limitations in its ability to generalize to all unseen weed variants. The Udupi dataset was utilized solely for visualization purposes, showcasing side-by-side comparisons of the original image, predicted segmentation map, and overlay to highlight both accurate detections and notable failure cases. Figure <xref rid=\"Fig10\" ref-type=\"fig\">10</xref> illustrates the visualization of the original image, the model&#8217;s prediction, and the overlay between them. The figure demonstrates that the model successfully identifies nearly all peanut crops, even without being retrained on this new dataset from a different region. However, it struggles to detect weeds, as the weed types in this area were not included in the training data, making them unfamiliar to the model. Despite this, the model still performs well in accurately recognizing the crops.<fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>Visualization of original image, prediction, and overall of PSPEdgeweedNet on a new dataset collected from the Udupi region of Karnataka, India.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO10\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig10_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec18\"><title>Comparison with other semantic segmentation models</title><p id=\"Par69\">The performance comparison across various DL models, as shown in Table&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref> for semantic segmentation, reveals that the proposed PSPEdgeWeedNet model significantly outperforms all other architectures. It achieves the highest scores across all evaluation metrics, with a precision of 0.9399, a recall of 0.9335, F1-score of 0.9358, an IoU of 0.8797, and a competitive accuracy of 0.9335. This indicates that the proposed model not only detects relevant classes more accurately but also handles boundary and overlap regions more effectively. Among the baseline models, DeepLabv3 shows strong overall performance (F1-score of 0.8329 and IoU of 0.69), while SegNet and UNet also perform reasonably well. Swin-UNet, though achieving the highest accuracy (0.9511), falls behind in F1-score and IoU, suggesting that it might be biased toward the dominant class. Accuracy alone can be deceptive in semantic segmentation tasks such as crop-weed identification because of class imbalance, where background pixels frequently take up the majority of the image. Measures such as Intersection over Union (IoU), precision, recall, and F1 score provide more accurate and insightful evaluations. By reducing false positives, precision assesses how accurate positive predictions are, whereas recall concentrates on the model&#8217;s capacity to catch all pertinent cases. The F1 Score balances precision and recall, providing a single measure of accuracy that accounts for both types of errors. IoU quantifies the spatial overlap between predicted and actual segmentation, making it valuable for assessing object boundaries and region-based accuracy. Together, these metrics provide a comprehensive evaluation framework that better captures the effectiveness and robustness of segmentation models in detecting and delineating agriculturally significant features. By balancing precision and recall, the F1 Score offers a single accuracy metric that takes into consideration both kinds of errors. IoU is useful for evaluating object boundaries and region-based accuracy since it measures the spatial overlap between expected and actual segmentation. When combined, these metrics offer a thorough assessment methodology that more accurately depicts the efficiency and resilience of segmentation models in identifying and defining traits of agricultural importance. So it can be concluded that the proposed PSPEdgeWeedNet model outperforms all the baseline semantic segmentation models evaluated in this study. Although PSPEdgeWeedNet exhibits a slightly lower inference speed (86 FPS) compared to other models like UNet (166 FPS) and SegNet (145 FPS), it outperforms them in all other key performance metrics such as accuracy, mIoU, F1-score, and boundary delineation. This makes the proposed model a more suitable choice for applications where segmentation quality is prioritized over raw inference speed.<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Performance comparison of different models for semantic segmentation (Significant values are highlighted in bold).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1 Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Inference Speed</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">UNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7942</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8053</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8177</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6887</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9327</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">166 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SegNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8269</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7720</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7866</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6802</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9345</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">145 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6631</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7257</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6756</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5553</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8777</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DeepLabv3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8162</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8537</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8329</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6900</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9481</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">89 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin-UNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7829</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8040</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7925</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6827</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9511</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.92 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Lightweight ViT-based Model</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6029</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5823</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5848</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.4708</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8428</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">150 FPS</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed PSPEdgeWeedNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9399</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9358</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.8797</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9335</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">86 FPS</td></tr></tbody></table></table-wrap></p><p id=\"Par71\">Figure.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref> presents a bar plot comparing the performance of various semantic segmentation models across multiple evaluation metrics, including precision, recall, F1 score, IoU, and accuracy.<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>Performance comparison of semantic segmentation models.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO11\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig11_HTML.jpg\"/></fig></p><p id=\"Par72\">Visualization of ground truth and corresponding predictions generated by various semantic segmentation models, post- processed with Conditional Random Fields (CRF) for boundary refinement, are shown in Fig.&#160;<xref rid=\"Fig12\" ref-type=\"fig\">12</xref>. Each row displays side-by-side comparisons of the input image, ground truth annotation, and CRF-refined predicted mask. Yellow bounding boxes are overlaid to highlight regions of misclassification (e.g., crop predicted as weed or background) and missed detections (e.g., weeds not detected). These visualizations demonstrate the capability of each model to localize and classify crop, weed, and background regions accurately, as well as the effectiveness of CRF refinement in enhancing boundary precision and reducing spatial ambiguity.<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p>Visualization of ground truth and corresponding predictions (CRF refined) generated by different semantic segmentation models. Green color represents crop, red color for weed, and black color for the background. Misclassifications and missed detections are highlighted using yellow bounding boxes.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO12\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig12_HTML.jpg\"/></fig></p><p id=\"Par73\">The comparison of UNet, PSPNet, SegNet, DeepLabV3, Swin-Unet, a lightweight transformer model based on ViT, and PSPEdgeWeedNet is presented in Table&#160;<xref rid=\"Tab4\" ref-type=\"table\">4</xref>, which concludes that PSPEdgeWeedNet provides the optimum performance, accuracy, and efficiency balance, especially for tasks requiring boundary localization and fine-grained segmentation.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Comparison of segmentation models.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Pyramid pooling</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Edge detection focus</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Boundary precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Computational efficiency</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Key strengths</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">UNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Strong performance in small datasets</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SegNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Memory-efficient decoding</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10003;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate to High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multiscale context understanding</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DeepLabv3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10003;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate to High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate to Low</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Captures large context without resolution loss</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Swin-Unet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10003;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Good for dense prediction tasks</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Light weight transformer model based on ViT</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10007;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Excellent for classification and transfer learning</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10003;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#10003;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved boundary localization, efficient deep feature extraction, better fine-grained segmentation</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec19\"><title>Ablation study: model and improvements</title><sec id=\"Sec20\"><title>Impact of class weights and boundary-aware loss on PSPNet model performance</title><p id=\"Par74\">The Table&#160;<xref rid=\"Tab5\" ref-type=\"table\">5</xref> presents experimental results with varying class weights for crop, weed, and background to determine the most optimal weight configuration for each class. The evaluation is based on both training and validation metrics. Upon analyzing the results, it is evident that the class weights of 2 for crop, 3 for weed, and 1 for background yield the highest accuracy compared to all other configurations tested. This combination consistently demonstrated superior performance in both training and validation accuracy, making it the most effective choice for achieving optimal model performance in this context.<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Baseline PSPNet with different class weights. The significant values are highlighted in bold.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class weights</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Training loss</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation loss</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Training accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation accuracy</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Crop</italic>&#8201;=&#8201;2<italic toggle=\"yes\">,Weed</italic>&#8201;=&#8201;5<italic toggle=\"yes\">, Background</italic>&#8201;=&#8201;1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.2777</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.3959</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.898</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8897</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Crop</italic>&#8201;=&#8201;3<italic toggle=\"yes\">,Weed</italic>&#8201;=&#8201;7<italic toggle=\"yes\">, Background</italic>&#8201;=&#8201;1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.2804</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.4073</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8832</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8763</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Crop</italic>&#8201;=&#8201;6<italic toggle=\"yes\">,Weed</italic>&#8201;=&#8201;15<italic toggle=\"yes\">, Background</italic>&#8201;=&#8201;1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.2696</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.4619</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8417</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8455</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Crop</italic>&#8201;=&#8201;2<italic toggle=\"yes\">,Weed</italic>&#8201;=&#8201;3<italic toggle=\"yes\">, Background</italic>&#8201;=&#8201;1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.2482</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.3755</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9098</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.8960</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par75\">The ablation results shown in Table&#160;<xref rid=\"Tab6\" ref-type=\"table\">6</xref> clearly demonstrate the incremental improvements achieved by incorporating boundary-aware loss and edge detection branch into the base PSPNet architecture:<list list-type=\"bullet\"><list-item><p id=\"Par76\">Baseline Improvement with Boundary-Aware Loss: Adding boundary-aware loss alone improves precision, F1 score, and IoU (from baseline values) and moderately enhances overall segmentation quality, achieving an F1 score of 0.7068 and IoU of 0.5875. This suggests it helps the model better capture class boundaries, reducing misclassifications.</p></list-item><list-item><p id=\"Par77\">Impact of Edge Detection Branch: Replacing boundary loss with an edge detection branch significantly boosts all metrics, particularly IoU (0.68) and F1 score (0.7897). This indicates that edge cues contribute more effectively to object separation and delineation than boundary loss alone.</p></list-item><list-item><p id=\"Par78\">Combined Strategy- Proposed Model: The full integration of both edge detection and boundary-aware loss in the proposed PSPEdgeWeedNet model leads to the best performance across all metrics.</p></list-item></list><table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Ablation study showing the impact of boundary-aware loss and edge detection branch on PSPNet-based segmentation model performance (Significant values are highlighted in bold).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model variant</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1 Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Boundary Aware Loss</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8943</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6945</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7236</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7068</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5875</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Edge Detection Branch</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9279</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7800</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8003</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7897</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6800</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Boundary Aware Loss&#8201;+&#8201;</p><p>Edge Detection Branch (Proposed model)</p></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9399</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9358</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.8797</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par79\">This significant performance jump highlights the complementary effect of both strategies in enhancing segmentation quality, particularly in challenging cases involving complex boundaries and overlapping vegetation. Fig&#160;<xref rid=\"Fig13\" ref-type=\"fig\">13</xref> presents the visualization of ground truth and predictions for two models: (a) PSPNet with boundary-aware loss but without the edge detection module, and (b) PSPNet with the edge detection module but without boundary-aware loss. While both models effectively capture the crop regions, instances of weed misclassification as crops and missed weed detections are observed in both cases.<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig. 13</label><caption><p>Visualisation of ground truth and predictions for two PSPNet variations. Green color represents crop, red color for weed, and black color for the background.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO13\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig13_HTML.jpg\"/></fig></p></sec><sec id=\"Sec21\"><title>Impact of different backbone architectures on PSPEdgeWeedNet performance</title><p id=\"Par80\">The performance comparison of PSPEdgeWeedNet with three different backbone networks, ResNet101, ResNet18, and MobileNetv2 are shown in the Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>. Results demonstrates that the ResNet101 backbone yields the best results across all<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Performance of PSPEdgeWeedNet with different backbone architectures (Significant values are highlighted in bold).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1 Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet (ResNet101)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9399</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9358</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.8797</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet (ResNet18)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7580</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8111</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7825</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6444</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9194</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet (MobileNetv2)</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8015</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8074</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8050</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6730</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9320</td></tr></tbody></table></table-wrap></p><p id=\"Par81\">key evaluation metrics. Specifically, it achieves a precision of 0.9399, recall of 0.9335, F1-score of 0.9358, IoU of 0.8797, and accuracy of 0.9335. In contrast, the ResNet18 version shows lower scores (F1-score: 0.7825, IoU: 0.6444), while the MobileNetV2 variant performs slightly better than ResNet18 (F1-score: 0.805, IoU: 0.673), although still below ResNet101. The superior performance of PSPEdgeWeedNet with ResNet101 confirms that deeper and more expressive backbones significantly improve segmentation quality in agricultural applications. However, lighter backbones like ResNet18 or MobileNetV2 may still be suitable where inference speed or resource constraints are critical, at the cost of some accuracy. Fig&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref> shows the visualization of ground truth and predictions generated by PSPEdgeWeedNet using ResNet18 and MobileNetv2 as backbone networks. When ResNet18 is used as the backbone, all crop instances are accurately identified, and a few weed instances are detected. However, some weeds are misclassified as crops. In contrast, with MobileNetV2 as the backbone, the model fails to detect weed instances, as highlighted by the yellow boxes in the visualization.<fig id=\"Fig14\" position=\"float\" orientation=\"portrait\"><label>Fig. 14</label><caption><p>Visualization of ground truth and predictions generated using PSPEdgeWeedNet with ResNet18 and MobileNetv2 as backbone networks. Green color represents crop, red color for weed, and black color for the background. Yellow boxes highlight misclassifications and missed detections.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO14\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig14_HTML.jpg\"/></fig></p></sec><sec id=\"Sec22\"><title>Impact of data augmentation on PSPEdgeWeedNet model performance</title><p id=\"Par82\">A comprehensive set of data augmentation techniques was used on the proposed PSPEdgeWeedNet model. Color-based transformations were applied using ColorJitter, which randomly altered image brightness, contrast, saturation, and hue to mimic the diverse lighting conditions commonly encountered in agricultural environments. Geometric augmentations included horizontal flipping with a 50% probability and vertical flipping with a 30% probability, helping the model learn orientation- invariant features. Additionally, random rotations up to 15 degrees were incorporated to accommodate angular variations during image capture. To further diversify spatial configurations, a RandomAffine transformation was applied, introducing minor translations, scaling, and shearing. Lastly, Gaussian blur was used to simulate image quality degradation such as motion blur or defocus, encouraging the model to learn more robust visual patterns.</p><p id=\"Par83\">From the results obtained as shown in Table&#160;<xref rid=\"Tab8\" ref-type=\"table\">8</xref>, it is clear that data augmentation did not improve the model&#8217;s performance. It led to a noticeable decrease in accuracy, precision, recall, and F1 score. The drop in performance suggests that the augmented data might not have been beneficial in this specific scenario, potentially due to overfitting or a misalignment between the augmented data and the actual data distribution. Therefore, for the PSPEdgeWeedNet model, using no data augmentation seems to lead to better overall performance. On both the training and validation sets, PSPEdgeWeedNet (without data augmentation) performs better than PSPEdgeWeedNet (with data augmentation) in every parameter, including accuracy, precision, recall, and F1 score.<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Ablation experiments comparing PSPEdgeWeedNet performance with and without data augmentation. Significant values are highlighted in bold.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Metric</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet (Without Data Augmentation)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSPEdgeWeedNet (With Data Augmentation)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8293</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9399</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8241</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9335</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8293</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">F1 Score</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.9358</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8263</td></tr></tbody></table></table-wrap></p><p id=\"Par85\">Figure.&#160;<xref rid=\"Fig15\" ref-type=\"fig\">15</xref>a displays the confusion matrix generated for PSPedgeWeedNet with data augmentation. Fig&#160;<xref rid=\"Fig15\" ref-type=\"fig\">15</xref>b illustrates the visualization of ground truth and prediction for the same. Based on the results presented in the above table, as well as insights drawn from the confusion matrix and the visual comparisons of predictions with ground truth, it is evident that applying data augmentation had a detrimental impact on the model&#8217;s performance. Specifically, data augmentation not only led to a decline in key evaluation metrics such as accuracy, precision, recall, and F1-score, but also contributed to an increase in both misclassifications and missed detections. A significant number of weed instances were left undetected in the augmented dataset, indicating that the augmentation techniques may have distorted critical features required for effective weed identification.<fig id=\"Fig15\" position=\"float\" orientation=\"portrait\"><label>Fig. 15</label><caption><p>Evaluation results of PSPEdgeWeedNet with data augmentation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO15\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_24174_Fig15_HTML.jpg\"/></fig></p><p id=\"Par86\">Furthermore, the predictions refined using Conditional Random Fields (CRF) revealed that the boundaries between crop and weed regions were not sharply or accurately delineated when data augmentation was applied. This lack of clear segmentation negatively affected the model&#8217;s ability to distinguish between classes at the object level.</p><p id=\"Par87\">In contrast, our proposed model, PSPEdgeWeedNet without data augmentation, consistently produced more accurate and well-defined predictions. It not only achieved superior quantitative performance but also demonstrated better qualitative outcomes in terms of segmentation clarity and object boundary definition. Hence, it can be concluded that the non-augmented training pipeline yields more reliable results for crop&#8211;weed segmentation tasks in this specific dataset.</p></sec></sec><sec id=\"Sec23\"><title>Analysis of findings</title><p id=\"Par88\">The performance of PSPNet enhanced with boundary-aware loss and no edge detection module revealed inherent limitations in accurately delineating the boundaries between crop and weed classes. Specifically, the model failed to capture sharp edge definitions, resulting in blurred contours and reduced structural fidelity&#8212;particularly in the morphology of weed leaves. This led to imprecise segmentation and instances of class confusion, where crop regions were incorrectly classified as weeds, highlighting difficulties in achieving effective class separation.</p><p id=\"Par89\">In contrast, PSPEdgeWeedNet exhibited notable improvements in boundary localization compared to the baseline PSPNet. Misclassifications were substantially reduced, and edge contours were more distinguishable. However, the segmentation output still lacked the precision required for fine-grained structural representation, especially in the detailed contours of weed foliage, which remained partially undefined and unstructured.</p><p id=\"Par90\">The integration of Conditional Random Fields (CRF) in the visualization module of PSPEdgeWeedNet significantly enhanced the boundary refinement process. This led to more continuous and precise delineation of crop and weed edges, with improved preservation of the morphological and structural characteristics of vegetation. Visual inspection of the outputs confirmed this enhancement, although the model still struggled with detecting extremely small weed instances, which were occasionally omitted during segmentation.</p><p id=\"Par91\">A qualitative visualization comparison was also conducted between UNet, SegNet, DeepLabv3, Swin-Unet, lightweight transformer based on ViT, and the proposed PSPEdgeWeedNet. In the case of UNet, although the model generally succeeded in identifying both crops and weeds, several instances of weed misclassification as crops were observed, highlighted using yellow bounding boxes. SegNet exhibited a higher degree of misclassification than UNet, with weeds frequently mislabeled as crops and, in some cases, background pixels erroneously detected as weeds. Swin-Unet demonstrated relatively high segmentation accuracy; however, visualizations revealed inaccuracies along the boundaries of crops and weeds, with occasional misclassifications where crops were incorrectly segmented as weeds.</p><p id=\"Par92\">In contrast, PSPEdgeWeedNet produced more refined and accurate boundary segmentation than all the aforementioned models. The contours of both crop and weed regions were sharply defined, and no significant misclassifications were observed in the visual outputs. Nevertheless, a minor limitation persisted&#8212;extremely small weed patches were sometimes not detected. Based on both quantitative evaluation (in terms of precision, recall, F1-score, and IoU) and qualitative visual inspection, it can be concluded that PSPEdgeWeedNet outperforms other segmentation architectures. It delivers superior boundary delineation, significantly reduced misclassification, and enhanced structural preservation of vegetation classes, making it highly effective for semantic segmentation tasks in agricultural applications.</p></sec><sec id=\"Sec24\"><title>Future work</title><p id=\"Par93\">The PSPEdgeWeedNet model, which segments peanut field images into crop, weed, and background classes, has shown effectiveness in segmenting images. However, there are several areas for future exploration. Firstly, diversifying the dataset to include images from a wider range of crops which can improve the model&#8217;s generalizability across different agricultural scenarios. Secondly, expanding the dataset to include a wider variety and an increased number of weed species will enhance the model&#8217;s robustness and enable accurate differentiation between diverse weed types. Thirdly, by combining the PSPEdgeWeedNet model with UAVs, ground robots, or smartphone applications, it can be enhanced to detect weeds in actual agricultural settings. To make the model more scalable and appropriate for long-term deployment in dynamic agricultural ecosystems, further research could employ adaptive learning approaches to change with new data over time.</p><p id=\"Par94\">Importantly, the current architecture is optimized for RGB-only inputs, with a focus on real-time deployment in agricultural field conditions. Future work will address the incorporation of spectral diversity (e.g., multispectral and NIR data) and include multi-crop validation to ensure broader applicability and improved performance in varied agricultural systems.</p></sec><sec id=\"Sec25\"><title>Conclusion</title><p id=\"Par95\">This paper presents PSPEdgeWeedNet, a novel semantic segmentation architecture specifically developed for precise crop and weed identification in agricultural field conditions. The proposed model builds upon the foundational Pyramid Scene Parsing Network (PSPNet) by integrating an edge-aware module that enhances the model&#8217;s capability to delineate object boundaries with higher fidelity. This architectural augmentation enables the extraction of both global contextual information and localized edge features, which is particularly critical for distinguishing between visually similar plant structures such as weeds and crops, especially in heterogeneous field environments where occlusion, overlapping foliage, and background clutter are prevalent.</p><p id=\"Par96\">Quantitative and qualitative evaluations demonstrate that PSPEdgeWeedNet surpasses the UNet, SegNet, PSPNet, DeepLabv3, Swin-Unet, and lightweight transformer model based on ViT in both segmentation performance and boundary localization. PSPEdgeWeedNet shows significant improvements in edge sharpness, reduced crop-to-weed misclassification, and improved retention of structural features such as weed leaf morphology. Although Conditional Random Fields (CRF) are not incorporated into the training or inference stages, their application in post-processing for visualization highlights the enhanced boundary preservation achieved by PSPEdgeWeedNet. The visual results suggest that the model is capable of generating continuous, well-preserved boundaries and exhibits reduced fragmentation, independent of any CRF-based post-processing.</p><p id=\"Par97\">Furthermore, PSPEdgeWeedNet demonstrates improved sensitivity to small and sparsely distributed weed instances, al- though some extremely tiny weed structures remain difficult to accurately segment. Despite achieving high overall segmentation accuracy, certain challenges persist, notably suboptimal Intersection over Union (IoU) scores for underrepresented classes. These results confirm the efficacy of the proposed edge-aware enhancements in addressing core limitations of conventional semantic segmentation models and suggest potential avenues for future work in class-imbalance mitigation and fine-grained structure recovery in agricultural vision tasks.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was supported by the Manipal Academy of Higher Education, Dr. T.M.A Pai Research Scholarship under Research Registration No: 230900101-2023.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>DGP: Visualization, Writing&#8212;original draft, review and editing. MB: Validation, Supervision, review and editing. RK: Validation, Formal analysis, Investigation. All authors contributed to the article and approved the submitted version.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>Open access funding provided by Manipal Academy of Higher Education, Manipal. Open access funding is provided by Manipal Academy of Higher Education, Manipal.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The datasets presented in this study can be found in online repositories(<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/ptdkhoa/Peanut-dataset\">https://github.com/ptdkhoa/Peanut-dataset</ext-link>).</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par98\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Arg&#252;elles</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>March</surname><given-names>H</given-names></name></person-group><article-title>Weeds in action: Vegetal political ecology of unwanted plants</article-title><source>Prog. Hum. Geogr.</source><year>2022</year><volume>46</volume><fpage>44</fpage><lpage>66</lpage><pub-id pub-id-type=\"doi\">10.1177/03091325211054966</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Arg&#252;elles, L. &amp; March, H. Weeds in action: Vegetal political ecology of unwanted plants. <italic toggle=\"yes\">Prog. Hum. Geogr.</italic><bold>46</bold>, 44&#8211;66 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ramesh</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Matloob</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Aslam</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Florentine</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Chauhan</surname><given-names>BS</given-names></name></person-group><article-title>Weeds in a changing climate: Vulnerabilities, consequences, and implications for future weed management</article-title><source>Front. plant science</source><year>2017</year><volume>8</volume><fpage>95</fpage><pub-id pub-id-type=\"doi\">10.3389/fpls.2017.00095</pub-id><pub-id pub-id-type=\"pmcid\">PMC5303747</pub-id><pub-id pub-id-type=\"pmid\">28243245</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Ramesh, K., Matloob, A., Aslam, F., Florentine, S. K. &amp; Chauhan, B. S. Weeds in a changing climate: Vulnerabilities, consequences, and implications for future weed management. <italic toggle=\"yes\">Front. plant science</italic><bold>8</bold>, 95 (2017).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fpls.2017.00095</pub-id><pub-id pub-id-type=\"pmcid\">PMC5303747</pub-id><pub-id pub-id-type=\"pmid\">28243245</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><mixed-citation publication-type=\"other\">Liebman. Integration of soil, crop and weed management in low-external-input farming systems. <italic toggle=\"yes\">Weed research</italic><bold>40</bold>, 27&#8211;47 (2000).</mixed-citation></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shittu</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Fagam</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Garba</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sabo</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Gworgwor</surname><given-names>N</given-names></name></person-group><article-title>Weed control efficiency, nodulation and yield response of groundnut as affected by weed control, variety, and season in Bauchi, Nigeria. National innovation and research academia</article-title><source>Int. J. Agribusiness Agric. Sci.</source><year>2022</year><volume>7</volume><fpage>1</fpage><lpage>17</lpage></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Shittu, E., Fagam, A., Garba, A., Sabo, M. &amp; Gworgwor, N. Weed control efficiency, nodulation and yield response of groundnut as affected by weed control, variety, and season in Bauchi, Nigeria. National innovation and research academia. <italic toggle=\"yes\">Int. J. Agribusiness Agric. Sci.</italic><bold>7</bold>, 1&#8211;17 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rakhmatulin</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Kamilaris</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Andreasen</surname><given-names>C</given-names></name></person-group><article-title>Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review</article-title><source>Remote. Sens.</source><year>2021</year><volume>13</volume><fpage>4486</fpage><pub-id pub-id-type=\"doi\">10.3390/rs13214486</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Rakhmatulin, I., Kamilaris, A. &amp; Andreasen, C. Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review. <italic toggle=\"yes\">Remote. Sens.</italic><bold>13</bold>, 4486 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shaikh</surname><given-names>TA</given-names></name><name name-style=\"western\"><surname>Rasool</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Lone</surname><given-names>FR</given-names></name></person-group><article-title>Towards leveraging the role of machine learning and artificial intelligence in precision agriculture and smart farming</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>198</volume><fpage>107119</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2022.107119</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Shaikh, T. A., Rasool, T. &amp; Lone, F. R. Towards leveraging the role of machine learning and artificial intelligence in precision agriculture and smart farming. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>198</bold>, 107119 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><citation-alternatives><element-citation id=\"ec-CR7\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Patr&#237;cio</surname><given-names>DI</given-names></name><name name-style=\"western\"><surname>Rieder</surname><given-names>R</given-names></name></person-group><article-title>Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review</article-title><source>Comput. Electron. Agric.</source><year>2018</year><volume>153</volume><fpage>69</fpage><lpage>81</lpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2018.08.001</pub-id></element-citation><mixed-citation id=\"mc-CR7\" publication-type=\"journal\">Patr&#237;cio, D. I. &amp; Rieder, R. Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>153</bold>, 69&#8211;81 (2018).</mixed-citation></citation-alternatives></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sharma</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Shivandu</surname><given-names>SK</given-names></name></person-group><article-title>Integrating artificial intelligence and internet of things (iot) for enhanced crop monitoring and management in precision agriculture</article-title><source>Sens. Int.</source><year>2024</year><volume>5</volume><fpage>100292</fpage><pub-id pub-id-type=\"doi\">10.1016/j.sintl.2024.100292</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Sharma, K. &amp; Shivandu, S. K. Integrating artificial intelligence and internet of things (iot) for enhanced crop monitoring and management in precision agriculture. <italic toggle=\"yes\">Sens. Int.</italic><bold>5</bold>, 100292 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Niazian</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Niedba&#322;a</surname><given-names>G</given-names></name></person-group><article-title>Machine learning for plant breeding and biotechnology</article-title><source>Agriculture</source><year>2020</year><volume>10</volume><fpage>436</fpage><pub-id pub-id-type=\"doi\">10.3390/agriculture10100436</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Niazian, M. &amp; Niedba&#322;a, G. Machine learning for plant breeding and biotechnology. <italic toggle=\"yes\">Agriculture</italic><bold>10</bold>, 436 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pai</surname><given-names>DG</given-names></name><name name-style=\"western\"><surname>Kamath</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Balachandra</surname><given-names>M</given-names></name></person-group><article-title>Deep learning techniques for weed detection in agricultural environments: A comprehensive review</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>113193</fpage><lpage>113214</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2024.3418454</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Pai, D. G., Kamath, R. &amp; Balachandra, M. Deep learning techniques for weed detection in agricultural environments: A comprehensive review. <italic toggle=\"yes\">IEEE Access</italic><bold>12</bold>, 113193&#8211;113214 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ulku</surname><given-names>I</given-names></name></person-group><article-title>Reslmffnet: A real-time semantic segmentation network for precision agriculture</article-title><source>J. Real.Time Image Process.</source><year>2024</year><volume>21</volume><fpage>101</fpage><pub-id pub-id-type=\"doi\">10.1007/s11554-024-01474-0</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Ulku, I. Reslmffnet: A real-time semantic segmentation network for precision agriculture. <italic toggle=\"yes\">J. Real.Time Image Process.</italic><bold>21</bold>, 101 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sa</surname><given-names>I</given-names></name><etal/></person-group><article-title>weednet: Dense semantic weed classification using multispectral images and mav for smart farming</article-title><source>IEEE Robot. Autom. Lett.</source><year>2017</year><volume>3</volume><fpage>588</fpage><lpage>595</lpage><pub-id pub-id-type=\"doi\">10.1109/LRA.2017.2774979</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Sa, I. et al. weednet: Dense semantic weed classification using multispectral images and mav for smart farming. <italic toggle=\"yes\">IEEE Robot. Autom. Lett.</italic><bold>3</bold>, 588&#8211;595 (2017).</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>You</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J</given-names></name></person-group><article-title>A dnn-based semantic segmentation for detecting weed and crop</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>178</volume><fpage>105750</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2020.105750</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">You, J., Liu, W. &amp; Lee, J. A dnn-based semantic segmentation for detecting weed and crop. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>178</bold>, 105750 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kamath</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Balachandra</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Vardhan</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Maheshwari</surname><given-names>U</given-names></name></person-group><article-title>Classification of paddy crop and weeds using semantic segmentation</article-title><source>Cogent eng.</source><year>2022</year><volume>9</volume><fpage>2018791</fpage><pub-id pub-id-type=\"doi\">10.1080/23311916.2021.2018791</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Kamath, R., Balachandra, M., Vardhan, A. &amp; Maheshwari, U. Classification of paddy crop and weeds using semantic segmentation. <italic toggle=\"yes\">Cogent eng.</italic><bold>9</bold>, 2018791 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Luo</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Gou</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Semantic segmentation of agricultural images: A survey</article-title><source>Inf. Process. Agric.</source><year>2024</year><volume>11</volume><fpage>172</fpage><lpage>186</lpage></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Luo, Z., Yang, W., Yuan, Y., Gou, R. &amp; Li, X. Semantic segmentation of agricultural images: A survey. <italic toggle=\"yes\">Inf. Process. Agric.</italic><bold>11</bold>, 172&#8211;186 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Su</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Kong</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Qiao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Sukkarieh</surname><given-names>S</given-names></name></person-group><article-title>Data augmentation for deep learning based semantic segmentation and crop-weed classification in agricultural robotics</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>190</volume><fpage>106418</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2021.106418</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Su, D., Kong, H., Qiao, Y. &amp; Sukkarieh, S. Data augmentation for deep learning based semantic segmentation and crop-weed classification in agricultural robotics. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>190</bold>, 106418 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>B</given-names></name><etal/></person-group><article-title>Instance segmentation method for weed detection using UAV imagery in soybean fields</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>211</volume><fpage>107994</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2023.107994</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Xu, B. et al. Instance segmentation method for weed detection using UAV imagery in soybean fields. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>211</bold>, 107994 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><mixed-citation publication-type=\"other\">Zhao, H., Shi, J., Qi, X., Wang, X. &amp; Jia, J. Pyramid scene parsing network. In <italic toggle=\"yes\">Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 2881&#8211;2890 (2017).</mixed-citation></ref><ref id=\"CR19\"><label>19.</label><mixed-citation publication-type=\"other\">Kaganami, H. G. &amp; Beiji, Z. Region-based segmentation versus edge detection. In <italic toggle=\"yes\">2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing</italic>, 1217&#8211;1221 (IEEE, 2009).</mixed-citation></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Parra</surname><given-names>L</given-names></name><etal/></person-group><article-title>Edge detection for weed recognition in lawns</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>176</volume><fpage>105684</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2020.105684</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Parra, L. et al. Edge detection for weed recognition in lawns. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>176</bold>, 105684 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Hajare, P. A. &amp; Tijare, P. A. Edge detection techniques for image segmentation. <italic toggle=\"yes\">Int. J. Comput. Sci. Appl.</italic><bold>4</bold> (2011).</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mathur</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Mathur</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Mathur</surname><given-names>D</given-names></name></person-group><article-title>A novel approach to improve sobel edge detector</article-title><source>Procedia Comput. Sci.</source><year>2016</year><volume>93</volume><fpage>431</fpage><lpage>438</lpage><pub-id pub-id-type=\"doi\">10.1016/j.procs.2016.07.230</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Mathur, N., Mathur, S. &amp; Mathur, D. A novel approach to improve sobel edge detector. <italic toggle=\"yes\">Procedia Comput. Sci.</italic><bold>93</bold>, 431&#8211;438 (2016).</mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><citation-alternatives><element-citation id=\"ec-CR23\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Moazzam</surname><given-names>SI</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>US</given-names></name><name name-style=\"western\"><surname>Qureshi</surname><given-names>WS</given-names></name><name name-style=\"western\"><surname>Nawaz</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Kunwar</surname><given-names>F</given-names></name></person-group><article-title>Towards automated weed detection through two-stage semantic segmentation of tobacco and weed pixels in aerial imagery</article-title><source>Smart Agric. Technol.</source><year>2023</year><volume>4</volume><fpage>100142</fpage><pub-id pub-id-type=\"doi\">10.1016/j.atech.2022.100142</pub-id></element-citation><mixed-citation id=\"mc-CR23\" publication-type=\"journal\">Moazzam, S. I., Khan, U. S., Qureshi, W. S., Nawaz, T. &amp; Kunwar, F. Towards automated weed detection through two-stage semantic segmentation of tobacco and weed pixels in aerial imagery. <italic toggle=\"yes\">Smart Agric. Technol.</italic><bold>4</bold>, 100142 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gupta</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Yadav</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Soni</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Shanker</surname><given-names>U</given-names></name><name name-style=\"western\"><surname>Singh</surname><given-names>PK</given-names></name></person-group><article-title>Multiclass weed identification using semantic segmentation: An automated approach for precision agriculture</article-title><source>Ecol. Informatics</source><year>2023</year><volume>78</volume><fpage>102366</fpage><pub-id pub-id-type=\"doi\">10.1016/j.ecoinf.2023.102366</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Gupta, S. K., Yadav, S. K., Soni, S. K., Shanker, U. &amp; Singh, P. K. Multiclass weed identification using semantic segmentation: An automated approach for precision agriculture. <italic toggle=\"yes\">Ecol. Informatics</italic><bold>78</bold>, 102366 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>H</given-names></name><etal/></person-group><article-title>An accurate semantic segmentation model for bean seedlings and weeds identification based on improved erfnet</article-title><source>Sci. Reports</source><year>2024</year><volume>14</volume><fpage>12288</fpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-61981-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC11136954</pub-id><pub-id pub-id-type=\"pmid\">38811674</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Gao, H. et al. An accurate semantic segmentation model for bean seedlings and weeds identification based on improved erfnet. <italic toggle=\"yes\">Sci. Reports</italic><bold>14</bold>, 12288 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-61981-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC11136954</pub-id><pub-id pub-id-type=\"pmid\">38811674</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Liu, T. et al. Semantic segmentation for weed detection in corn. <italic toggle=\"yes\">Pest Manag. Sci.</italic> (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/ps.8554</pub-id><pub-id pub-id-type=\"pmid\">39584373</pub-id></mixed-citation></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Men</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Bi</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name></person-group><article-title>Research on field soybean weed identification based on an improved unet model combined with a channel attention mechanism</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>13</volume><fpage>890051</fpage><pub-id pub-id-type=\"doi\">10.3389/fpls.2022.890051</pub-id><pub-id pub-id-type=\"pmid\">35783959</pub-id><pub-id pub-id-type=\"pmcid\">PMC9240479</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Yu, H., Men, Z., Bi, C. &amp; Liu, H. Research on field soybean weed identification based on an improved unet model combined with a channel attention mechanism. <italic toggle=\"yes\">Front. Plant Sci.</italic><bold>13</bold>, 890051 (2022).<pub-id pub-id-type=\"pmid\">35783959</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fpls.2022.890051</pub-id><pub-id pub-id-type=\"pmcid\">PMC9240479</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Gong</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Mostafa</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>G</given-names></name></person-group><article-title>Weed identification in maize fields based on improved swin-unet</article-title><source>Agronomy</source><year>2023</year><volume>13</volume><fpage>1846</fpage><pub-id pub-id-type=\"doi\">10.3390/agronomy13071846</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Zhang, J., Gong, J., Zhang, Y., Mostafa, K. &amp; Yuan, G. Weed identification in maize fields based on improved swin-unet. <italic toggle=\"yes\">Agronomy</italic><bold>13</bold>, 1846 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>J</given-names></name><etal/></person-group><article-title>Weed detection in potato fields based on improved yolov4: Optimal speed and accuracy of weed detection in potato fields</article-title><source>Electronics</source><year>2022</year><volume>11</volume><fpage>3709</fpage><pub-id pub-id-type=\"doi\">10.3390/electronics11223709</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Zhao, J. et al. Weed detection in potato fields based on improved yolov4: Optimal speed and accuracy of weed detection in potato fields. <italic toggle=\"yes\">Electronics</italic><bold>11</bold>, 3709 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><label>30.</label><citation-alternatives><element-citation id=\"ec-CR30\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A faster R-CNN-based model for the identification of weed seedling</article-title><source>Agronomy</source><year>2022</year><volume>12</volume><fpage>2867</fpage><pub-id pub-id-type=\"doi\">10.3390/agronomy12112867</pub-id></element-citation><mixed-citation id=\"mc-CR30\" publication-type=\"journal\">Mu, Y. et al. A faster R-CNN-based model for the identification of weed seedling. <italic toggle=\"yes\">Agronomy</italic><bold>12</bold>, 2867 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Hu, X.-Z., Jeon, W.-S. &amp; Rhee, S.-Y. Sugar beets and weed detection using semantic segmentation. In <italic toggle=\"yes\">2022 International Conference on Fuzzy Theory and Its Applications (iFUZZY)</italic>, 1&#8211;4 (IEEE, 2022).</mixed-citation></ref><ref id=\"CR32\"><label>32.</label><mixed-citation publication-type=\"other\">Charania, S., Lendave, P., Borwankar, J. &amp; Kadge, S. A novel approach to weed detection using segmentation and image processing techniques. In<italic toggle=\"yes\"> World Conference on Communication &amp; Computing (WCONF)</italic>, 1&#8211;5 (IEEE, 2023).</mixed-citation></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Genze</surname><given-names>N</given-names></name><etal/></person-group><article-title>Improved weed segmentation in uav imagery of sorghum fields with a combined deblurring segmentation model</article-title><source>Plant Methods</source><year>2023</year><volume>19</volume><fpage>87</fpage><pub-id pub-id-type=\"doi\">10.1186/s13007-023-01060-8</pub-id><pub-id pub-id-type=\"pmid\">37608384</pub-id><pub-id pub-id-type=\"pmcid\">PMC10463442</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Genze, N. et al. Improved weed segmentation in uav imagery of sorghum fields with a combined deblurring segmentation model. <italic toggle=\"yes\">Plant Methods</italic><bold>19</bold>, 87 (2023).<pub-id pub-id-type=\"pmid\">37608384</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s13007-023-01060-8</pub-id><pub-id pub-id-type=\"pmcid\">PMC10463442</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rana</surname><given-names>S</given-names></name><etal/></person-group><article-title>Rafanoset: Dataset of raw, manually, and automatically annotated raphanus raphanistrum weed images for object detection and segmentation</article-title><source>Data Brief</source><year>2024</year><volume>54</volume><fpage>110430</fpage><pub-id pub-id-type=\"doi\">10.1016/j.dib.2024.110430</pub-id><pub-id pub-id-type=\"pmid\">38698801</pub-id><pub-id pub-id-type=\"pmcid\">PMC11063987</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Rana, S. et al. Rafanoset: Dataset of raw, manually, and automatically annotated raphanus raphanistrum weed images for object detection and segmentation. <italic toggle=\"yes\">Data Brief</italic><bold>54</bold>, 110430 (2024).<pub-id pub-id-type=\"pmid\">38698801</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.dib.2024.110430</pub-id><pub-id pub-id-type=\"pmcid\">PMC11063987</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><citation-alternatives><element-citation id=\"ec-CR35\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Farmland extraction from UAV remote sensing images based on improved segformer model</article-title><source>J. Indian Soc. Remote. Sens.</source><year>2025</year><volume>53</volume><fpage>421</fpage><lpage>433</lpage><pub-id pub-id-type=\"doi\">10.1007/s12524-024-02004-y</pub-id></element-citation><mixed-citation id=\"mc-CR35\" publication-type=\"journal\">Chen, Y. &amp; Wang, X. Farmland extraction from UAV remote sensing images based on improved segformer model. <italic toggle=\"yes\">J. Indian Soc. Remote. Sens.</italic><bold>53</bold>, 421&#8211;433 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fawakherji</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Potena</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Pretto</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Bloisi</surname><given-names>DD</given-names></name><name name-style=\"western\"><surname>Nardi</surname><given-names>D</given-names></name></person-group><article-title>Multi-spectral image synthesis for crop/weed segmentation in precision farming</article-title><source>Robot. Auton. Syst.</source><year>2021</year><volume>146</volume><fpage>103861</fpage><pub-id pub-id-type=\"doi\">10.1016/j.robot.2021.103861</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Fawakherji, M., Potena, C., Pretto, A., Bloisi, D. D. &amp; Nardi, D. Multi-spectral image synthesis for crop/weed segmentation in precision farming. <italic toggle=\"yes\">Robot. Auton. Syst.</italic><bold>146</bold>, 103861 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Ahmed, M. R. <italic toggle=\"yes\">et al.</italic> Multiclass classification on soybean and weed species using a novel customized greenhouse robotic and hyperspectral combination system. <italic toggle=\"yes\">Available at SSRN 4044574</italic> (2022).</mixed-citation></ref><ref id=\"CR38\"><label>38.</label><citation-alternatives><element-citation id=\"ec-CR38\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Atsmon</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Nehurai</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Kizel</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Eizenberg</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Lati</surname><given-names>RN</given-names></name></person-group><article-title>Hyperspectral imaging facilitates early detection of orobanche cumana below-ground parasitism on sunflower under field conditions</article-title><source>Comput. Electron. Agric.</source><year>2022</year><volume>196</volume><fpage>106881</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2022.106881</pub-id></element-citation><mixed-citation id=\"mc-CR38\" publication-type=\"journal\">Atsmon, G., Nehurai, O., Kizel, F., Eizenberg, H. &amp; Lati, R. N. Hyperspectral imaging facilitates early detection of orobanche cumana below-ground parasitism on sunflower under field conditions. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>196</bold>, 106881 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tran</surname><given-names>THY</given-names></name><name name-style=\"western\"><surname>Phan</surname><given-names>TDK</given-names></name></person-group><article-title>Dense multi-scale convolutional network for plant segmentation</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>82640</fpage><lpage>82651</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2023.3300234</pub-id></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Tran, T. H. Y. &amp; Phan, T. D. K. Dense multi-scale convolutional network for plant segmentation. <italic toggle=\"yes\">IEEE Access</italic><bold>11</bold>, 82640&#8211;82651 (2023).</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12675521 PMC12675521.1 12675521 12675521 41339372 10.1038/s41598-025-24174-6 24174 1 Article Deep learning approach for crop-weed segmentation in peanut cultivation using PSPEdgeWeedNet Pai Deepthi G Balachandra Mamatha mamtha.bc@manipal.edu Kamath Radhika radhika.kamath@manipal.edu https://ror.org/02xzytt36 grid.411639.8 0000 0001 0571 5193 Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka 576104 India 3 12 2025 2025 15 478255 43032 16 5 2025 10 10 2025 03 12 2025 05 12 2025 05 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . Weed management continues to be a significant challenge in modern agriculture, primarily due to the aggressive growth patterns of weeds and their direct competition with crops for essential resources such as light, water, and nutrients. Although recent developments in precision agriculture have led to the emergence of automated weed detection systems aimed at reducing operational costs and decreasing reliance on chemical herbicides, achieving accurate crop&#8211;weed segmentation remains a persistent difficulty. This is largely attributed to high visual similarity between crops and weeds, coupled with variations in illumination and field conditions. To address these challenges, Convolutional Neural Networks (CNNs) have been increasingly adopted for their capability to perform end-to-end, pixel-level classification, particularly when leveraging multi-spectral imagery. In this context, PSPEdgeWeedNet is proposed, a novel edge-aware deep learning architecture tailored for precise semantic segmentation of crops and weeds within peanut cultivation fields. Distinct from the conventional Pyramid Scene Parsing Network (PSPNet) and its boundary-aware variant developed as a baseline in this research, PSPEdgeWeedNet introduces a dedicated edge detection branch. This branch is specifically engineered to enhance boundary localization and improve delineation between adjacent vegetation classes. In post-processing, Conditional Random Fields (CRFs) are used to slightly enhance the segmentation results around object boundaries. Additionally, all models were trained on a curated peanut field dataset using class-weighted loss functions to effectively address inherent class imbalance. Comprehensive experimental evaluations reveal that PSPEdgeWeedNet significantly outperforms existing state-of-the-art architectures including PSPNet, SegNet, UNet, DeepLabv3, Swin-Unet, and light weight transformer model based on ViT across multiple performance metrics such as Intersection over Union (IoU), precision, recall, and F1-score. These results highlight the critical role of incorporating edge-aware mechanisms within semantic segmentation frameworks, thereby enhancing the robustness and accuracy of automated weed detection systems in complex, real-world agricultural environments. Keywords Boundary loss Crop Edge aware networks Multiclass segmentation Object detection Precision agriculture PSPNet Segmentation Weed Subject terms Plant sciences Environmental sciences Engineering Manipal Academy of Higher Education, Manipal Open access funding provided by Manipal Academy of Higher Education, Manipal pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Weeds, often referred to as &#8220;plants out of place,&#8221; compete with crops for essential resources such as soil nutrients and water, leading to distinctive crop-weed interactions that can significantly reduce crop yields 1 . However, the importance of weed management is often underestimated by both farmers and the general public due to the high costs associated with herbicides and manual weeding. Additionally, weeds serve as hosts for pests that can damage crops, further exacerbating agricultural losses 2 . These weeds can also hinder crop growth. Therefore, it is essential to accurately identify and detect them 3 . Arachis hypogaea L., or peanuts(groundnuts), are a great source of important nutrients and health advantages. However, several factors, including disease, insects, drought, climate change, soil fertility, and weed infestation, have an impact on peanut crop yields. Peanut production losses of up to 70% occur during the crucial 40&#8211;45&#160;day post-sowing phase for crop-weed competition 4 . Therefore, detecting and removing weeds from peanut farms is essential to enhance crop yield. Artificial Intelligence (AI) can play a significant role in agricultural tasks, with Deep Learning models enabling accurate classification and detection of weeds and crops in the field 5 , 6 . Crop monitoring, yield forecasting, disease detection, soil health management, climate forecasting, supply chain optimization, genomics and breeding, precision agriculture, and market analysis have all been made possible by Deep Learning (DL), which has completely changed the agricultural industry 7 . These technologies analyze data from satellites, drones, and Internet of Things devices to identify weeds and pests, monitor crop health, diagnose crop and leaf diseases, estimate yields, and optimize fertilization and irrigation 8 . Additionally, they aid in the early identification of crop diseases, enabling timely action to prevent crop loss. DL models support breeding and genomics, supply chain optimization, and climate predictions 9 . DL-based precision agriculture uses real-time data to optimize resource utilization and reduce environmental impacts 10 &#8211; 12 . Figure&#160; 1 shows different DL models used for weed detection purposes. Fig. 1 Deep Learning models used in weed detection. Semantic segmentation, a key technique in DL, can be effectively utilized for crop and weed detection purposes 13 &#8211; 15 . Every pixel in the image is classified into one of the pre existing classes using semantic segmentation. As a result, it offers more accurate crop and weed location data 16 . These Deep Neural Net based techniques, which have many tuning parameters, usually need a few thousand to tens of thousands of images to train to provide sufficient performance and prevent overfitting 16 . However, standard segmentation models often struggle with accurately distinguishing weed boundaries, leading to misclassification errors 17 . One of the widely used models for semantic segmentation is PSPNet (Pyramid Scene Parsing Network) 18 . However, it has certain limitations, including poor boundary delineation, low Intersection over Union (IoU) for small and rare classes, and high sensitivity to background noise. Despite the growing use of DL models in precision agriculture, accurately segmenting crops and weeds in field conditions remains a challenging task due to high visual similarity between classes, varying illumination, and complex backgrounds. Existing semantic segmentation models such as UNet, SegNet, DeepLabV3, PSPNet, and transformer- based architectures like Swin-UNet and lightweight Vision Transformer (ViT) variants often struggle with precise boundary delineation, leading to misclassifications, particularly in tightly clustered or overlapping vegetation. To address this gap, the authors propose PSPEdgeWeedNet, a novel semantic segmentation architecture designed specifically for crop&#8211;weed discrimination in peanut fields. The model integrates a boundary-aware loss function and a dedicated edge detection branch to enhance localization at object borders. Furthermore, Conditional Random Fields (CRFs) are used in post-processing to refine the segmentation masks. The model was also qualitatively evaluated on a separate peanut dataset collected from the Udupi region of Karnataka, which contains different weed species and varying lighting conditions. While the model was not trained on this dataset, it successfully identified every crop instance, indicating promising generalization capabilities.Experimental results demonstrate that PSPEdgeWeedNet outperforms the aforementioned models across all major performance metrics, including precision, recall, F1-score, and Intersection over Union (IoU), thereby establishing its effectiveness in handling the nuanced challenges of field-based weed detection. The model is thoroughly tested on a peanut crop-weed dataset and compared against the following methods: PSPNet, serving as the baseline semantic segmentation model. Boundary-aware PSPNet, which integrates a boundary-sensitive loss function and investigates different class weighting schemes to address class imbalance. Other widely used semantic segmentation models, including UNet, SegNet, and DeepLabv3, Swin-Unet, light weight transformer model based on ViT. The model was tested on a peanut dataset collected from the Udupi region, which includes diverse weed species and varying lighting conditions. This study aims to develop a novel PSPNet-based architecture augmented with a boundary-aware loss function and an edge detection branch, aiming to improve boundary accuracy while maintaining computational efficiency suitable for real-time deployment. The significance of edge detection in segmentation models The primary goal of edge detection is to pinpoint the specific regions or pixels within an image that represent boundaries or transitions&#8212;typically where there are sharp changes in intensity or color&#8212;indicating the presence of edges 19 . The technique&#8217;s working premise states that an edge is any pixel in the chosen band of the RGB image that has a value different from that of its neighbors 20 . Edge detection is a crucial technique in segmentation models, particularly in tasks like crop and weed detection 21 . It enhances object boundaries, reduces misclassification, differentiates objects with similar textures or colors, and prevents background noise interference. Edge detection also improves IoU for small objects, preserving their structure and boosting segmentation performance. It complements global context learning by adding local fine-grained details, leading to more precise segmentation masks. In this study, edge detection refers to the limits of crop, weed, and background, indicating the significant difference between the object and nearby objects. Sobel edge detection is a gradient-based method that determines the rate at which an image&#8217;s intensity changes and uses the gradient magnitude to identify both horizontal and vertical edges 22 . The Sobel filter in the early layers of the proposed PSPEdgeWeedNet model improves segmentation efficiency and reduces noise, thereby boosting edge detection performance. Motivation of this Study Despite reaching excellent overall accuracy in agricultural applications, current semantic segmentation algorithms often fail to provide the precise border delineation required for successful precision farming. This paper proposes a novel PSPNet-based architecture to bridge the crucial gap in boundary-aware agricultural segmentation, enhanced by boundary-aware loss functions and a dedicated edge detection branch. The proposed approach focuses on the difficulty of accurately identifying crop-weed boundaries, which is important for precision mechanical weeding, focused herbicide application, and autonomous agricultural operations. The suggested framework prioritizes edge-critical areas, as opposed to previous approaches that provide equal weight to every pixel. For real-time agricultural applications, this results in notably improved boundary accuracy while maintaining computing efficiency. Related works With its exceptional ability to differentiate between crops and weeds in a variety of agricultural settings, semantic segmentation has become a game-changing technique in precision agriculture. Several research projects have achieved F1-scores above 90% and mean Intersection over Union (mIoU) values as high as 92%, demonstrating the technology&#8217;s consistently outstanding performance metrics. Table&#160; 1 summarizes different DL methods utilized in agricultural domains, detailing their aim, model architectures, datasets used, and associated limitations. Table 1 Analysis of different deep learning techniques in the agricultural domain. Citation Aim of the paper Models or algorithms Dataset Limitations 23 To improve pixel-level precision in distinguishing crop (tobacco) and weed pixels using a two-stage semantic segmentation approach, enhancing classification performance over traditional one- stage models Stage 1(Binary classifier) Stage 2 (three class clas- sifier) A newly captured aerial tobacco crop dataset The two-stage approach doubles the computation compared to one-stage models and even though inference time is still acceptable for real-time, it is costlier computationally 13 Weed and crop segmentation network that improves performance in recognizing weeds of any shape in complex environments DNN Stuttgart and Bonn datasets Lack of Generalization Evidence Resource Demanding 26 Detects vegetation outside the crop mask as weeds by combining semantic segmentation and image processing CCNet GCNet ISANet DeepLabV3 DeepLabV3&#8201;+&#8201; Cornfields near Shenyang, Liaoning Province, China Limited Temporal and Geographic Diversity 24 An automated semantic segmentation- based approach for multiclass weed identification in precision agriculture U-Net based on Inception-ResNetV2 Brinjal farms in Gorakhpur, Uttar Pradesh, India Use of RGB Images only Assumption of Uniform Lighting and Weather Conditions 25 Advanced semantic segmentation model for accurate crop&#8211;weed differentiation, enhancing precision weeding under natural field conditions EPAnet Bean sprout cultivation base in Avignon, France Limited Generalizability 26 Lightweight, real-time weed segmentation CCNet GCNet ISANet DeepLabV3 DeepLabV3&#8201;+&#8201; Cornfields near Shenyang, Liaoning Province, China Geographic and Temporal Limitation Indirect Weed Segmentation 27 Embeds a SE module for soybean weeds in a genuine field setting and suggests an enhanced UNet structure UNet ResNet34 Soybean experimental field of Jilin Agricultural University Real-time deployment on edge devices for in-field precision spraying remains a challenge due to computational complexity 28 Weed recognition model based on improved Swin-Unet UNet Swin-Unet DeepLabv3 Mask R-CNN Maize test field in Zibo, Shan-dong Province, China Approach may be less effective in scenarios where crops and weeds are heavily interwoven or where morphological differences are subtle, leading to potential misclassification 29 Suggest an enhanced model based on YOLOv4 for detecting weeds in potato fields Improved YOLOv4 al-gorithm Images were taken from the test site in China Data set collected from an experimental field, not from the real field 30 Proposes a Faster R-CNN network architecture for identifying weeds in cropping region images Faster RCNN FPN Improved ResNeXt101 V2 Plant Seedlings Dataset The paper lacks details on the potential effects of lighting conditions or image quality on the model&#8217;s performance Advanced neural network architectures and performance achievements Agricultural segmentation challenges have demonstrated the effectiveness of DL systems. On brinjal (eggplant) farm datasets, a U-Net model with Inception-ResNetV2 backbone architecture obtained an F1-score of 96.78%, proving the value of fusing robust feature extraction networks with segmentation frameworks 24 . Researchers created an ERFNet-based multi-decoder architecture with a dual-loss function mechanism, furthering the field&#8217;s advancement 25 . Quantifiable performance increases of 1.91% in mIoU and 1.19% in Frequency Weighted Intersection over Union (FWIoU) were achieved by this novel technique, which focused on improving model attention on important crop and weed categories. The model&#8217;s improved performance in challenging field settings shows how useful it is in actual agricultural scenarios in which environmental influences can have a big influence on identification accuracy. Integration of knowledge distillation and real-time optimization Researchers are investigating knowledge distillation approaches in conjunction with conventional image processing methods as a result of the desire for real-time agricultural monitoring. One important research, which focused on maize segmentation applications, combined the DeepLabV3&#8201;+&#8201;architecture with traditional image processing methods and knowledge distillation frameworks 26 . The crucial problem of preserving high accuracy while attaining real-time speed and cutting computational overhead through model size optimization was effectively handled by this hybrid technique. Analysis of segmentation architectures in specific crop environments Research on paddy field applications has shed important light on how well various segmentation frameworks perform in comparison. Extensive comparison analyses shown that in rice field environments, Pyramid Scene Parsing Network (PSPNet) continuously outperformed both UNet and SegNet designs, attaining mIoU values in the 70%&#8211;80% range 14 . The significance of choosing suitable designs depending on particular crop traits and field circumstances is shown by this performance difference. The development of UNet architectures has produced complex variations with cutting-edge training techniques. With mIoU values as high as 92.34%, a UNet++ implementation with deep supervision techniques demonstrated exceptional performance in sugar beet field applications 31 . This architecture showed the benefits of architectural improvements and sophisticated training approaches in agricultural segmentation tasks by considerably outperforming baseline UNet++ implementations as well as classic UNet models. In agricultural applications, the combination of object detection and semantic segmentation approaches has demonstrated encouraging outcomes. A rich Sugarbeets dataset with 1,300 images was used to train a hybrid model that combined semantic segmentation with YOLO (You Only Look Once) object recognition 32 . The potential advantages of merging complementing computer vision techniques for improved agricultural monitoring were demonstrated by this integrated system, which achieved 93% accuracy. Agricultural imaging frequently takes place in difficult settings, especially when drone-based systems are being used, which can cause motion blur. Researchers created the customized architecture DeBlurWeedSeg to deal with motion-blurred drone footage after realizing this constraint 33 . A major practical issue in aerial agricultural monitoring systems was resolved by this creative method, which much outperformed traditional segmentation algorithms without deblurring capabilities. Semantic segmentation performs well for weed-crop identification across different crops; mIoU values of up to 92% and F1-scores of over 90% have been observed. Custom designs such as DeBlurWeedSeg, ERFNet with dual-loss, UNet++ with deep supervision, and U-Net based on Inception-ResNetV2 improve accuracy and robustness while addressing real-world issues. Lightweight models and knowledge distillation enable real-time performance, while hybrid approaches improve practical utility. The quality and diversity of datasets are crucial for better learning and broader generalizability. The development of automated, accurate, and effective crop monitoring systems has advanced significantly with these developments in semantic segmentation for agricultural applications. Through more precise and timely weed-crop distinction capabilities, the technology&#8217;s ongoing development promises to improve crop production projections, maximize resource management, and advance sustainable agricultural methods. Despite the outstanding F1-scores and mIoU metrics that contemporary deep learning models, such U-Net++, DeepLabV3+, and PSPNet, show across a variety of crops and environments, the majority of methods have some significant drawbacks. Without explicit edge modeling, it is still difficult to distinguish between weeds and crops. Additionally, most research rely on single-field or crop-specific data, making it uncommon to examine generality across datasets or geographic regions. Also neglected are post-processing methods that may greatly improve fine-grained segmentation, such as CRF. By combining an edge detection module, boundary-aware loss to sharpen object edges, using CRF-based post-refinement to lower segmentation noise, and verifying generalization ability across datasets, our suggested PSPEdgeWeedNet directly tackles these issues. Combined, these enhancements address the significant flaws in earlier research and help create more accurate semantic segmentation systems for practical agricultural monitoring. Multispectral and NIR-based approaches in agricultural segmentation Multispectral and near-infrared (NIR) data have been used more and more in precision agriculture in recent years to increase the accuracy of crop and weed segmentation. The pioneering public dataset RafanoSet 34 includes 85 multispectral images of fields of Triticum aestivum that include Raphanus raphanistrum. The images are annotated at the pixel level to locate weeds. The images, which were taken in 17 different scenarios and contain spectral bands including Blue, Green, Red, NIR, and RedEdge, provide a wealth of information for segmentation model training and evaluation. An improved SegFormer architecture was presented in a related effort to use UAV-based images to extract spatial patterns in agricultural landscapes 35 . With 98.42% pixel accuracy and 96.91% IoU, the model&#8217;s integration of Efficient Channel Attention, BiFPN layers, and MLP-based modules enhances feature extraction and yields noteworthy results, which are especially advantageous for small-scale farms and field borders. Furthermore, as an alternative to conventional data augmentation in crop-weed segmentation tasks, a conditional GAN (cGAN) was used in another work to create realistic multispectral images, encompassing RGB and NIR channels 36 . In addition to improving visual realism, using synthetic data increased semantic segmentation network&#8217;s accuracy. Additionally, hyperspectral imaging has demonstrated potential; in one research, five weed species and soybean plants were classified using a robotic hyperspectral scanning platform in a greenhouse setting 37 . The study highlighted unique chemical fingerprints among species and obtained good classification accuracy by using 983 hyperspectral cubes and Savitzky&#8211;Golay derivative preprocessing. Additionally, early identification of the parasitic weed known as sunflower broomrape was accomplished with hyperspectral imaging, with classification accuracies of 76% and 89% at 31 and 38 days after planting, respectively 38 . The study illustrated the potential of hyperspectral methods for prompt, site-specific weed treatment by examining certain leaf segments. When taken as a whole, these studies highlight how multispectral and hyperspectral imaging are increasingly helping to advance precision farming by improving plant discriminating and detecting weeds early. Materials and methods Dataset The peanut (groundnut) dataset employed in this study was sourced from a publicly available GitHub repository 39 and is specifically designed to support research in crop-weed segmentation. It comprises 400 high-resolution RGB images, each with a resolution of 720 &#215; 960 pixels, captured from real-world peanut fields located near Da Nang, Vietnam. These images provide a rich and diverse visual representation of peanut crops alongside several common weed species, most notably goose grass and nut grass. The dataset is annotated with two semantic classes: crop (representing peanut plants) and weed (aggregating all types of invasive species). The annotation process was meticulously performed under the consultation of domain experts in agriculture, ensuring high-quality ground-truth labels. On average, each image required around 20 minutes to annotate due to the intricate and detailed nature of the scenes. Figure&#160; 2 illustrates the peanut dataset showcasing diverse and complex weed scenarios, emphasizing the challenges in accurate weed detection. Fig. 2 A peanut example dataset is used to illustrate the dataset&#8217;s diversity and recognition challenges. The images depict a range of field scenarios, including single, tiny, and mixed weed species, as well as crops and weeds with intricate backgrounds. These illustrate the challenges of weed detection technologies by showcasing the seemingly complicated and identical backgrounds of numerous weed types 39 . This dataset presents several complex segmentation scenarios that are representative of actual field conditions. These include the occurrence of small-scale weed patches, inter-plant occlusions, and large differences in plant size, all of which greatly complicate accurate segmentation. Specifically, the visual resemblance and dense spatial distribution of weeds and crops, as well as background clutter, necessitate models that can capture contextual cues and fine-grained limits. The peanut dataset represents a rigorous standard for assessing the efficacy and resilience of contemporary Deep Learning-based semantic segmentation models in precision agriculture because of these features. Sample original and annotated images from the dataset are shown in Figs.&#160; 3 . Fig. 3 Representative samples from the dataset: (left) original RGB image and (right) corresponding annotated image, where green denotes crop regions and red indicates weed areas 39 . Experimental setup The experiments in this study were conducted on a computational platform equipped with an NVIDIA A100 GPU. The experimental environment utilized the PyTorch framework, running on a Microsoft Windows 11 system with 8 GB of RAM. The platform supported CUDA 11.8, enabling efficient GPU acceleration for Deep Learning tasks. All experiments were implemented using Python 3.10.12. Training parameters To enhance model performance, the training parameters were systematically fine-tuned through multiple experimental iterations. A resolution of 600&#8201;&#215;&#8201;600 pixels was selected for the input images, as it provides an optimal balance between computational efficiency and the retention of key image features. While smaller image sizes could result in the loss of crucial information, larger sizes would unnecessarily increase the computational load without offering significant performance benefits. The number of epochs was set to 100 to ensure sufficient learning while avoiding overfitting. The model&#8217;s performance was optimized using an Adam optimizer with a learning rate of 1e-4 and a batch size of 16. Using a pretrained ResNet101 backbone, the model architecture used a custom PSPNet with edge detection included. Dense CRF was used as a post-processing step to further improve the visualization of segmentation findings. By successfully resolving minor errors and enhancing boundary accuracy, this method, when used for visualization purposes after inference, provides cleaner segmentation boundaries. Effective feature extraction is made possible by using a pretrained Convolutional Neural Network (CNN) as the backbone. This promotes quicker convergence and improved performance with less data. To balance the pace of convergence and minimize overfitting, the learning rate was carefully tuned during training. The dataset was randomly split into 80% for training, 10% for validation, and 10% for testing. Although the split was random and not stratified, class-weighted loss functions were used to address class imbalance across the three semantic classes&#8212;Crop, Weed, and Background. Specifically, class weights of 2 for Crop, 3 for Weed, and 1 for Background were applied to ensure that minority classes (Crop and Weed) had a greater influence during model optimization. Since the usage of augmentation techniques such random rotations, color jitter, and horizontal and vertical flips resulted in lower metric values, including decreased accuracy, the model trained without data augmentation was taken into consideration. Furthermore, in contrast, the visuals of predictions were substantially worse with data augmentation. This might be explained by the model&#8217;s inability to generalize well after being trained on sparse data with augmentation, which resulted to noisy predictions and imprecise limits. The model&#8217;s ability to concentrate on the dataset&#8217;s essential characteristics without augmentation resulted in more consistent and precise performance. A boundary-aware loss was used to improve edge detection performance, and the Cross-Entropy Loss function was used for the segmentation task, with weighted class contributions to rectify the class imbalance. The model contains a total of 48,271,428 trainable parameters, which are learned during the training process. This large number of parameters indicates an architecture capable of capturing intricate patterns in the data, allowing the model to potentially learn highly detailed features. Evaluation metrics This research uses important evaluation metrics, including as training and validation accuracy, precision, recall, F1 score, and Intersection over Union (IoU), to thoroughly assess the model&#8217;s effectiveness and performance in semantic segmentation tasks. Accuracy: Accuracy measures the number of pixels correctly classified as weeds or crops. Precision: Precision quantifies the number of true positive cases that a model predicts, or the accuracy of the model&#8217;s positive predictions. Recall: The frequency with which a model accurately detects positive examples (true positives) out of all the actual positive samples in the dataset is known as recall. F1 score: It is the harmonic mean of precision and recall, giving a fair assessment of a model&#8217;s precision (the capacity to detect positive occurrences) and recall (the ability to minimize false positives). IoU: A metric used to evaluate the accuracy of object detection models and segmentation models. Confusion Matrix: In semantic segmentation, a confusion matrix is used to evaluate how well a model classifies each pixel into the correct class. Visualization: Visualizing segmentation results alongside the ground truth helps understand where the model makes errors, particularly in boundary definitions and tiny object detection. Proposed model&#8212;PSPEdgeWeedNet Figure&#160; 4 depicts basic blocks used in the proposed work. Fig. 4 Basic blocks used in proposed work. The proposed model PSPEdgeWeedNet is a modified version of PSPNet (Pyramid Scene Parsing Network), specifically tailored for semantic segmentation tasks in agriculture, such as accurately distinguishing between crops and weeds. It utilizes ResNet101 as the backbone, a deep convolutional neural network pretrained on ImageNet, which effectively extracts rich feature representations while preserving spatial details essential for segmentation. To enhance contextual understanding, a pyramid pooling module (PSP block) is integrated, enabling the model to capture information at multiple scales. This is particularly valuable for agricultural images, where objects may appear in varied sizes and complex backgrounds are common. An additional edge detection module is incorporated into the architecture to improve the delineation of object boundaries. This component learns edge features from the same feature maps used for segmentation and generates an edge map to guide&#160;the model in identifying class borders more precisely. To reinforce this behavior during training, a custom Boundary Loss is introduced. It applies a Sobel operator to both the predicted segmentation output and the ground truth mask to extract edge information and then computes the mean squared error between them. In conjunction with the conventional CrossEntropyLoss, this boundary-aware loss encourages more precise boundary predictions and lessens class overlap. To improve the visual depiction of the segmentation precision, a Conditional Random Field (CRF) is also used as a post-processing step during the visualization phase to maintain fine details and sharpen boundaries. In summary, a dual-loss approach, edge detection for boundary refinement, and pyramid pooling for global context work together to produce more precise and thorough segmentation. This is particularly useful in agricultural settings where accurate and contextually aware segmentation is necessary due to the minor variations between weeds and crops. This approach ensures that both segmentation accuracy and edge refinement are optimized. Figure&#160; 5 illustrates the block diagram of the proposed PSPEdgeWeedNet model. Fig. 5 Proposed PSPEdgeWeedNet block diagram. Edge detection module and boundary aware loss By including an edge detection module, the proposed PSPEdgeWeedNet enhances localization in high-resolution agricultural images by learning semantic areas such as crop, weed, and background. The module adds an edge detection head for the convolutional layer and pulls feature maps from the ResNet101 backbone. This head highlights changes between semantic classes by learning a 1-channel edge probability map. Edge prediction is guided by using a unique boundary-aware loss based on Sobel edge detection. To guarantee distinct, precise object borders and oversee edge prediction, the Boundary-aware Loss technique is employed. By extracting gradient magnitude from both ground truth and anticipated segmentation maps using Sobel filters and minimizing the difference using Mean Squared Error, it ensures clear and well-aligned borders in challenging agricultural images. Boundary-aware loss implementation works by combining traditional semantic segmentation loss with a specialized boundary detection loss to enhance segmentation accuracy at class boundaries. It extracts edge information from predicted segmentation maps and ground truth labels using Sobel edge detection filters. The Sobel operator applies two 3&#8201;&#215;&#8201;3 convolution kernels to compute gradient magnitudes, highlighting boundary regions. The edge strength is calculated as the square root of the sum of squared gradients in both directions, creating edge maps emphasizing transitions between different classes like crops, weeds, and background. The boundary loss is computed as the Mean Squared Error (MSE) between the predicted edge map and the ground truth edge map, measuring the alignment of the model&#8217;s predicted boundaries with the actual class boundaries in the labeled data. This loss function optimizes for correct pixel-wise classification and accurate boundary delineation. By penalizing boundary misalignment, the model learns to pay special attention to boundary regions where segmentation errors are most visually apparent and functionally important, particularly in crop/weed segmentation tasks. This dual-objective approach helps the model achieve better overall segmentation performance by ensuring that class transitions are sharp and well-defined, rather than just focusing on broad regional classification accuracy. Boundary-aware loss function To enhance the precision of object boundaries in semantic segmentation, the authors introduce a boundary-aware loss based on Sobel edge detection. This loss penalizes differences between the predicted and ground truth edge maps. Predicted Segmentation Let the network output be a raw score tensor: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$P(x) \\in {\\text{R}}^{H \\times W \\times C}$$\\end{document} where H , W , and C denote the height, width, and number of classes, respectively. The predicted class label for each pixel is obtained via: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{Y}(x) = \\mathop {{\\text{argmax Softmax}}(P(x)) }\\limits_{c}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hat{Y}(x) \\in \\{ 0,{1},...,C - {1}\\}^{{^{H \\times W} }}$$\\end{document} is the class prediction map. Ground Truth Segmentation Let the ground truth label map be: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Y(x) \\in \\{ 0,{1},...,C - {1}\\}^{{^{H \\times W} }}$$\\end{document} Sobel Edge Detection To extract edge information, we apply Sobel operators in both horizontal ( S x ) and vertical ( S y ) directions: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$S_{x} = \\begin{array}{*{20}c} { - {1}} &amp; 0 &amp; {1} \\\\ { - {2}} &amp; 0 &amp; {2} \\\\ { - {1}} &amp; 0 &amp; 1 \\\\ \\end{array} \\;S_{y} = \\begin{array}{*{20}c} { - 1} &amp; { - 2} &amp; { - 1} \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{array}$$\\end{document} The edge maps of the predicted and ground truth masks are calculated as: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_{{\\hat{Y}}} = \\sqrt {(S_{x} * \\hat{Y})^{{2}} + (S_{y} * \\hat{Y})^{{2}} }$$\\end{document} 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E_{Y} = \\sqrt {(S_{x} * Y)^{{2}} + (S_{y} * Y)^{{2}} }$$\\end{document} where &#8727; denotes 2D convolution, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$E \\hat{Y},E_{Y} \\in {\\text{R}}^{{^{H \\times W} }}$$\\end{document} represent the edge magnitude maps of the prediction and ground truth . Boundary-aware Loss The loss is defined as the mean squared error between the predicted and ground truth edge maps: 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L_\\textrm{boundary} =\\frac{1}{H \\times W} \\sum\\limits_{i = 1}^{H} \\sum\\limits_{j = 1}^{W} {(E \\hat{Y}(i,j) - E_{Y} (i,j))^2} $$\\end{document} This encourages the predicted segmentation boundaries to align closely with those in the ground truth. Total Loss The boundary-aware loss is used in conjunction with the standard cross-entropy loss: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L_{{{\\text{total}}}} = L{_\\text{CE}} + \\lambda \\cdot L_{{{\\text{boundary}}}}$$\\end{document} where L CE is the pixel-wise cross-entropy loss, and &#955; is a weighting factor controlling the influence of boundary supervision. Based on the performance metrics obtained after training the models and evaluating them on the validation dataset, it was observed that PSPNet with class weights set to crop&#8201;=&#8201;2, weed&#8201;=&#8201;3, and background&#8201;=&#8201;1 (c&#8201;=&#8201;2, w&#8201;=&#8201;3, b&#8201;=&#8201;1) achieved superior results compared to other class weighting strategies. Furthermore, integrating a boundary-aware loss function into PSPNet led to noticeable improvements in segmentation accuracy, precision, recall, and F1-score when compared to the baselinePSPNet model. Building on these findings, a novel model, PSPEdgeWeedNet, was proposed. This model retained the optimal class weights (2, 3, 1) and incorporated both the boundary-aware loss and an additional edge detection module to explicitly refine object boundaries. The inclusion of the edge detection branch further enhanced the segmentation performance across all evaluation metrics, including per-class Intersection over Union (IoU), overall precision, recall, and F1-score as shown in Fig.&#160; 6 . The trend of validation metrics across epochs indicates a consistent improvement in precision, recall, and F1 scores throughout the training process. However, a slight dip in all three metrics is noticeable around the 30th to 35th epochs, which may be attributed to temporary model instability or overfitting fluctuations. Following this phase, the metrics recover and continue to improve, ultimately reaching approximately 93%, indicating strong generalization performance in later epochs. Fig. 6 Validation metrics over epochs in PSPEdgeWeedNet. Figure.&#160; 7 illustrates the radar chart comparing precision, recall, F1 score, and IoU across each class in the proposed model. The background class consistently achieves the highest scores across all metrics, followed by the crop class. The weed class ranks lowest; however, it still attains approximately 60% in both precision and F1 scores, and nearly 70% in recall, indicating reasonable detection performance. Fig. 7 Radar chart using the PSPEdgeWeedNet model that shows the IoU, F1 score, recall, and precision for each class. Conditional Random Fields (CRF) were applied as a post-processing refinement technique during the visualization phase. CRF significantly enhanced the delineation of crop and weed boundaries and reduced inter-class misclassifications, especially in overlapping regions. Although the model occasionally missed very small or sparse weed instances, the overall boundary precision in visual outputs was markedly improved, demonstrating the effectiveness of the integrated architecture and post-processing strategy. The Fig.&#160; 8 shows the visualization of ground truth and prediction where the proposed model failed to detect the smaller instances of weed. It shows that despite its excellent performance, the proposed PSPEdgeWeedNet model had trouble identifying microscopic or tiny weed instances, especially in situations where the weeds were partially obscured or sparsely scattered. In the segmentation result, these smaller weeds were either completely ignored or incorrectly categorized as background. Reduced saliency of fine-scale information in deeper network layers and possible loss of spatial resolution during the encoding process are the causes of this problem. Deep segmentation networks frequently face the difficulty of identifying tiny objects, particularly when the item size is smaller than the network&#8217;s receptive field. The fine-grained patterns linked to smaller weeds might not have been adequately captured by the model&#8217;s edge refinement and pyramid pooling techniques. Fig. 8 Visualization of ground truth and prediction where the proposed model PSPEdgeWeedNet failed. Green color represents crop, red color for weed, and black color for the background. Misclassifications and missed detections are highlighted using yellow boxes. The model&#8217;s ability to differentiate between the three classes, crop, weed, and background, is demonstrated by the normalized confusion matrix as shown in Fig.&#160; 9 . With a high true positive rate of 91%, the model has a remarkable capacity to recognize crop pixels, whereas just 1% are misclassified as weed and 8% are misclassified as background. The visual elements of weed and the other two classifications overlap, as seen by the fact that weed pixels are properly categorized 69% of the time, although there is a noticeable misunderstanding with background (23%), and 8% are misclassified as crop. With a 95% classification accuracy, the background class shows the least amount of misunderstanding with crop (3%) and weed (2%). Table&#160; 2 presents a comparative analysis of PSPNet, PSPEdgeWeedNet without CRF, and PSPEdgeWeedNet with CRF. The results indicate that the integration of CRF in the proposed PSPEdgeWeedNet significantly enhances boundary delineation for both weeds and crops, and effectively preserves leaf structures. Moreover, the model demonstrates zero misclassification, thereby contributing to an overall improvement in segmentation quality. Fig. 9 Confusion matrix of proposed model PSPEdgeWeedNet. Table 2 Comparison of PSPNet and PSPEdgeWeedNet variants. Aspect PSPNet PSPEdgeWeedNet without CRF PSPEdgeWeedNet with CRF Boundary definition Poor; boundaries between crops and weeds are blurred Improved boundary sharpness compared to PSPNet Well-defined and continuous boundaries Weed leaf structure preservation Inadequate; leaf structures are not clearly captured Partial preservation; leaf edges are still not sharply structured Good preservation; leaf morphology is maintained Misclassification Present; crops misclassified as weeds Minimal; misclassifications largely eliminated No significant misclassification observed Edge sharpness Low; edges appear soft and diffused Moderate; improved but not optimal High; edges are sharp and detailed Tiny weed detection Missed frequently Slight improvement, but still limited Some very small weeds missed, but overall better retention Overall segmentation quality Suboptimal Moderate; better than baseline Significantly improved Limitations and generalization capability One of the key limitations of this study lies in the relatively small size and geographic scope of the training dataset, which comprises only 400 images of peanut crops collected from a single region. Such a dataset may not fully capture the variability in field conditions, weed species, or lighting scenarios encountered in other agricultural environments. To evaluate the model&#8217;s generalizability, we tested the trained network on an independent groundnut dataset collected from the Udupi region of Karnataka. This dataset contains a diverse range of weed species, including different types of broadleaved weeds and grasses, and was captured under varying lighting conditions. Without any additional retraining or fine-tuning, the model was able to correctly identify groundnut crops and detect certain broadleaved weeds and grasses in several samples, demonstrating a level of robustness and transferability. However, the model struggled to recognize other types of broadleaved weeds, indicating limitations in its ability to generalize to all unseen weed variants. The Udupi dataset was utilized solely for visualization purposes, showcasing side-by-side comparisons of the original image, predicted segmentation map, and overlay to highlight both accurate detections and notable failure cases. Figure 10 illustrates the visualization of the original image, the model&#8217;s prediction, and the overlay between them. The figure demonstrates that the model successfully identifies nearly all peanut crops, even without being retrained on this new dataset from a different region. However, it struggles to detect weeds, as the weed types in this area were not included in the training data, making them unfamiliar to the model. Despite this, the model still performs well in accurately recognizing the crops. Fig. 10 Visualization of original image, prediction, and overall of PSPEdgeweedNet on a new dataset collected from the Udupi region of Karnataka, India. Comparison with other semantic segmentation models The performance comparison across various DL models, as shown in Table&#160; 3 for semantic segmentation, reveals that the proposed PSPEdgeWeedNet model significantly outperforms all other architectures. It achieves the highest scores across all evaluation metrics, with a precision of 0.9399, a recall of 0.9335, F1-score of 0.9358, an IoU of 0.8797, and a competitive accuracy of 0.9335. This indicates that the proposed model not only detects relevant classes more accurately but also handles boundary and overlap regions more effectively. Among the baseline models, DeepLabv3 shows strong overall performance (F1-score of 0.8329 and IoU of 0.69), while SegNet and UNet also perform reasonably well. Swin-UNet, though achieving the highest accuracy (0.9511), falls behind in F1-score and IoU, suggesting that it might be biased toward the dominant class. Accuracy alone can be deceptive in semantic segmentation tasks such as crop-weed identification because of class imbalance, where background pixels frequently take up the majority of the image. Measures such as Intersection over Union (IoU), precision, recall, and F1 score provide more accurate and insightful evaluations. By reducing false positives, precision assesses how accurate positive predictions are, whereas recall concentrates on the model&#8217;s capacity to catch all pertinent cases. The F1 Score balances precision and recall, providing a single measure of accuracy that accounts for both types of errors. IoU quantifies the spatial overlap between predicted and actual segmentation, making it valuable for assessing object boundaries and region-based accuracy. Together, these metrics provide a comprehensive evaluation framework that better captures the effectiveness and robustness of segmentation models in detecting and delineating agriculturally significant features. By balancing precision and recall, the F1 Score offers a single accuracy metric that takes into consideration both kinds of errors. IoU is useful for evaluating object boundaries and region-based accuracy since it measures the spatial overlap between expected and actual segmentation. When combined, these metrics offer a thorough assessment methodology that more accurately depicts the efficiency and resilience of segmentation models in identifying and defining traits of agricultural importance. So it can be concluded that the proposed PSPEdgeWeedNet model outperforms all the baseline semantic segmentation models evaluated in this study. Although PSPEdgeWeedNet exhibits a slightly lower inference speed (86 FPS) compared to other models like UNet (166 FPS) and SegNet (145 FPS), it outperforms them in all other key performance metrics such as accuracy, mIoU, F1-score, and boundary delineation. This makes the proposed model a more suitable choice for applications where segmentation quality is prioritized over raw inference speed. Table 3 Performance comparison of different models for semantic segmentation (Significant values are highlighted in bold). Model Precision Recall F1 Score IoU Accuracy Inference Speed UNet 0.7942 0.8053 0.8177 0.6887 0.9327 166 FPS SegNet 0.8269 0.7720 0.7866 0.6802 0.9345 145 FPS PSPNet 0.6631 0.7257 0.6756 0.5553 0.8777 95 FPS DeepLabv3 0.8162 0.8537 0.8329 0.6900 0.9481 89 FPS Swin-UNet 0.7829 0.8040 0.7925 0.6827 0.9511 98.92 FPS Lightweight ViT-based Model 0.6029 0.5823 0.5848 0.4708 0.8428 150 FPS Proposed PSPEdgeWeedNet 0.9399 0.9335 0.9358 0.8797 0.9335 86 FPS Figure.&#160; 11 presents a bar plot comparing the performance of various semantic segmentation models across multiple evaluation metrics, including precision, recall, F1 score, IoU, and accuracy. Fig. 11 Performance comparison of semantic segmentation models. Visualization of ground truth and corresponding predictions generated by various semantic segmentation models, post- processed with Conditional Random Fields (CRF) for boundary refinement, are shown in Fig.&#160; 12 . Each row displays side-by-side comparisons of the input image, ground truth annotation, and CRF-refined predicted mask. Yellow bounding boxes are overlaid to highlight regions of misclassification (e.g., crop predicted as weed or background) and missed detections (e.g., weeds not detected). These visualizations demonstrate the capability of each model to localize and classify crop, weed, and background regions accurately, as well as the effectiveness of CRF refinement in enhancing boundary precision and reducing spatial ambiguity. Fig. 12 Visualization of ground truth and corresponding predictions (CRF refined) generated by different semantic segmentation models. Green color represents crop, red color for weed, and black color for the background. Misclassifications and missed detections are highlighted using yellow bounding boxes. The comparison of UNet, PSPNet, SegNet, DeepLabV3, Swin-Unet, a lightweight transformer model based on ViT, and PSPEdgeWeedNet is presented in Table&#160; 4 , which concludes that PSPEdgeWeedNet provides the optimum performance, accuracy, and efficiency balance, especially for tasks requiring boundary localization and fine-grained segmentation. Table 4 Comparison of segmentation models. Model Pyramid pooling Edge detection focus Boundary precision Computational efficiency Key strengths UNet &#10007; &#10007; Moderate High Strong performance in small datasets SegNet &#10007; &#10007; Moderate Moderate Memory-efficient decoding PSPNet &#10003; &#10007; Moderate to High Moderate Multiscale context understanding DeepLabv3 &#10003; &#10007; Moderate to High Moderate to Low Captures large context without resolution loss Swin-Unet &#10003; &#10007; High High Good for dense prediction tasks Light weight transformer model based on ViT &#10007; &#10007; Moderate Low Excellent for classification and transfer learning PSPEdgeWeedNet &#10003; &#10003; High Moderate Improved boundary localization, efficient deep feature extraction, better fine-grained segmentation Ablation study: model and improvements Impact of class weights and boundary-aware loss on PSPNet model performance The Table&#160; 5 presents experimental results with varying class weights for crop, weed, and background to determine the most optimal weight configuration for each class. The evaluation is based on both training and validation metrics. Upon analyzing the results, it is evident that the class weights of 2 for crop, 3 for weed, and 1 for background yield the highest accuracy compared to all other configurations tested. This combination consistently demonstrated superior performance in both training and validation accuracy, making it the most effective choice for achieving optimal model performance in this context. Table 5 Baseline PSPNet with different class weights. The significant values are highlighted in bold. Class weights Training loss Validation loss Training accuracy Validation accuracy Crop &#8201;=&#8201;2 ,Weed &#8201;=&#8201;5 , Background &#8201;=&#8201;1 0.2777 0.3959 0.898 0.8897 Crop &#8201;=&#8201;3 ,Weed &#8201;=&#8201;7 , Background &#8201;=&#8201;1 0.2804 0.4073 0.8832 0.8763 Crop &#8201;=&#8201;6 ,Weed &#8201;=&#8201;15 , Background &#8201;=&#8201;1 0.2696 0.4619 0.8417 0.8455 Crop &#8201;=&#8201;2 ,Weed &#8201;=&#8201;3 , Background &#8201;=&#8201;1 0.2482 0.3755 0.9098 0.8960 The ablation results shown in Table&#160; 6 clearly demonstrate the incremental improvements achieved by incorporating boundary-aware loss and edge detection branch into the base PSPNet architecture: Baseline Improvement with Boundary-Aware Loss: Adding boundary-aware loss alone improves precision, F1 score, and IoU (from baseline values) and moderately enhances overall segmentation quality, achieving an F1 score of 0.7068 and IoU of 0.5875. This suggests it helps the model better capture class boundaries, reducing misclassifications. Impact of Edge Detection Branch: Replacing boundary loss with an edge detection branch significantly boosts all metrics, particularly IoU (0.68) and F1 score (0.7897). This indicates that edge cues contribute more effectively to object separation and delineation than boundary loss alone. Combined Strategy- Proposed Model: The full integration of both edge detection and boundary-aware loss in the proposed PSPEdgeWeedNet model leads to the best performance across all metrics. Table 6 Ablation study showing the impact of boundary-aware loss and edge detection branch on PSPNet-based segmentation model performance (Significant values are highlighted in bold). Model variant Accuracy Precision Recall F1 Score IoU PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Boundary Aware Loss 0.8943 0.6945 0.7236 0.7068 0.5875 PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Edge Detection Branch 0.9279 0.7800 0.8003 0.7897 0.6800 PSPNet&#8201;+&#8201;Class weights&#8201;+&#8201;Boundary Aware Loss&#8201;+&#8201; Edge Detection Branch (Proposed model) 0.9335 0.9399 0.9335 0.9358 0.8797 This significant performance jump highlights the complementary effect of both strategies in enhancing segmentation quality, particularly in challenging cases involving complex boundaries and overlapping vegetation. Fig&#160; 13 presents the visualization of ground truth and predictions for two models: (a) PSPNet with boundary-aware loss but without the edge detection module, and (b) PSPNet with the edge detection module but without boundary-aware loss. While both models effectively capture the crop regions, instances of weed misclassification as crops and missed weed detections are observed in both cases. Fig. 13 Visualisation of ground truth and predictions for two PSPNet variations. Green color represents crop, red color for weed, and black color for the background. Impact of different backbone architectures on PSPEdgeWeedNet performance The performance comparison of PSPEdgeWeedNet with three different backbone networks, ResNet101, ResNet18, and MobileNetv2 are shown in the Table&#160; 7 . Results demonstrates that the ResNet101 backbone yields the best results across all Table 7 Performance of PSPEdgeWeedNet with different backbone architectures (Significant values are highlighted in bold). Model Precision Recall F1 Score IoU Accuracy PSPEdgeWeedNet (ResNet101) 0.9399 0.9335 0.9358 0.8797 0.9335 PSPEdgeWeedNet (ResNet18) 0.7580 0.8111 0.7825 0.6444 0.9194 PSPEdgeWeedNet (MobileNetv2) 0.8015 0.8074 0.8050 0.6730 0.9320 key evaluation metrics. Specifically, it achieves a precision of 0.9399, recall of 0.9335, F1-score of 0.9358, IoU of 0.8797, and accuracy of 0.9335. In contrast, the ResNet18 version shows lower scores (F1-score: 0.7825, IoU: 0.6444), while the MobileNetV2 variant performs slightly better than ResNet18 (F1-score: 0.805, IoU: 0.673), although still below ResNet101. The superior performance of PSPEdgeWeedNet with ResNet101 confirms that deeper and more expressive backbones significantly improve segmentation quality in agricultural applications. However, lighter backbones like ResNet18 or MobileNetV2 may still be suitable where inference speed or resource constraints are critical, at the cost of some accuracy. Fig&#160; 14 shows the visualization of ground truth and predictions generated by PSPEdgeWeedNet using ResNet18 and MobileNetv2 as backbone networks. When ResNet18 is used as the backbone, all crop instances are accurately identified, and a few weed instances are detected. However, some weeds are misclassified as crops. In contrast, with MobileNetV2 as the backbone, the model fails to detect weed instances, as highlighted by the yellow boxes in the visualization. Fig. 14 Visualization of ground truth and predictions generated using PSPEdgeWeedNet with ResNet18 and MobileNetv2 as backbone networks. Green color represents crop, red color for weed, and black color for the background. Yellow boxes highlight misclassifications and missed detections. Impact of data augmentation on PSPEdgeWeedNet model performance A comprehensive set of data augmentation techniques was used on the proposed PSPEdgeWeedNet model. Color-based transformations were applied using ColorJitter, which randomly altered image brightness, contrast, saturation, and hue to mimic the diverse lighting conditions commonly encountered in agricultural environments. Geometric augmentations included horizontal flipping with a 50% probability and vertical flipping with a 30% probability, helping the model learn orientation- invariant features. Additionally, random rotations up to 15 degrees were incorporated to accommodate angular variations during image capture. To further diversify spatial configurations, a RandomAffine transformation was applied, introducing minor translations, scaling, and shearing. Lastly, Gaussian blur was used to simulate image quality degradation such as motion blur or defocus, encouraging the model to learn more robust visual patterns. From the results obtained as shown in Table&#160; 8 , it is clear that data augmentation did not improve the model&#8217;s performance. It led to a noticeable decrease in accuracy, precision, recall, and F1 score. The drop in performance suggests that the augmented data might not have been beneficial in this specific scenario, potentially due to overfitting or a misalignment between the augmented data and the actual data distribution. Therefore, for the PSPEdgeWeedNet model, using no data augmentation seems to lead to better overall performance. On both the training and validation sets, PSPEdgeWeedNet (without data augmentation) performs better than PSPEdgeWeedNet (with data augmentation) in every parameter, including accuracy, precision, recall, and F1 score. Table 8 Ablation experiments comparing PSPEdgeWeedNet performance with and without data augmentation. Significant values are highlighted in bold. Metric PSPEdgeWeedNet (Without Data Augmentation) PSPEdgeWeedNet (With Data Augmentation) Accuracy 0.9335 0.8293 Precision 0.9399 0.8241 Recall 0.9335 0.8293 F1 Score 0.9358 0.8263 Figure.&#160; 15 a displays the confusion matrix generated for PSPedgeWeedNet with data augmentation. Fig&#160; 15 b illustrates the visualization of ground truth and prediction for the same. Based on the results presented in the above table, as well as insights drawn from the confusion matrix and the visual comparisons of predictions with ground truth, it is evident that applying data augmentation had a detrimental impact on the model&#8217;s performance. Specifically, data augmentation not only led to a decline in key evaluation metrics such as accuracy, precision, recall, and F1-score, but also contributed to an increase in both misclassifications and missed detections. A significant number of weed instances were left undetected in the augmented dataset, indicating that the augmentation techniques may have distorted critical features required for effective weed identification. Fig. 15 Evaluation results of PSPEdgeWeedNet with data augmentation. Furthermore, the predictions refined using Conditional Random Fields (CRF) revealed that the boundaries between crop and weed regions were not sharply or accurately delineated when data augmentation was applied. This lack of clear segmentation negatively affected the model&#8217;s ability to distinguish between classes at the object level. In contrast, our proposed model, PSPEdgeWeedNet without data augmentation, consistently produced more accurate and well-defined predictions. It not only achieved superior quantitative performance but also demonstrated better qualitative outcomes in terms of segmentation clarity and object boundary definition. Hence, it can be concluded that the non-augmented training pipeline yields more reliable results for crop&#8211;weed segmentation tasks in this specific dataset. Analysis of findings The performance of PSPNet enhanced with boundary-aware loss and no edge detection module revealed inherent limitations in accurately delineating the boundaries between crop and weed classes. Specifically, the model failed to capture sharp edge definitions, resulting in blurred contours and reduced structural fidelity&#8212;particularly in the morphology of weed leaves. This led to imprecise segmentation and instances of class confusion, where crop regions were incorrectly classified as weeds, highlighting difficulties in achieving effective class separation. In contrast, PSPEdgeWeedNet exhibited notable improvements in boundary localization compared to the baseline PSPNet. Misclassifications were substantially reduced, and edge contours were more distinguishable. However, the segmentation output still lacked the precision required for fine-grained structural representation, especially in the detailed contours of weed foliage, which remained partially undefined and unstructured. The integration of Conditional Random Fields (CRF) in the visualization module of PSPEdgeWeedNet significantly enhanced the boundary refinement process. This led to more continuous and precise delineation of crop and weed edges, with improved preservation of the morphological and structural characteristics of vegetation. Visual inspection of the outputs confirmed this enhancement, although the model still struggled with detecting extremely small weed instances, which were occasionally omitted during segmentation. A qualitative visualization comparison was also conducted between UNet, SegNet, DeepLabv3, Swin-Unet, lightweight transformer based on ViT, and the proposed PSPEdgeWeedNet. In the case of UNet, although the model generally succeeded in identifying both crops and weeds, several instances of weed misclassification as crops were observed, highlighted using yellow bounding boxes. SegNet exhibited a higher degree of misclassification than UNet, with weeds frequently mislabeled as crops and, in some cases, background pixels erroneously detected as weeds. Swin-Unet demonstrated relatively high segmentation accuracy; however, visualizations revealed inaccuracies along the boundaries of crops and weeds, with occasional misclassifications where crops were incorrectly segmented as weeds. In contrast, PSPEdgeWeedNet produced more refined and accurate boundary segmentation than all the aforementioned models. The contours of both crop and weed regions were sharply defined, and no significant misclassifications were observed in the visual outputs. Nevertheless, a minor limitation persisted&#8212;extremely small weed patches were sometimes not detected. Based on both quantitative evaluation (in terms of precision, recall, F1-score, and IoU) and qualitative visual inspection, it can be concluded that PSPEdgeWeedNet outperforms other segmentation architectures. It delivers superior boundary delineation, significantly reduced misclassification, and enhanced structural preservation of vegetation classes, making it highly effective for semantic segmentation tasks in agricultural applications. Future work The PSPEdgeWeedNet model, which segments peanut field images into crop, weed, and background classes, has shown effectiveness in segmenting images. However, there are several areas for future exploration. Firstly, diversifying the dataset to include images from a wider range of crops which can improve the model&#8217;s generalizability across different agricultural scenarios. Secondly, expanding the dataset to include a wider variety and an increased number of weed species will enhance the model&#8217;s robustness and enable accurate differentiation between diverse weed types. Thirdly, by combining the PSPEdgeWeedNet model with UAVs, ground robots, or smartphone applications, it can be enhanced to detect weeds in actual agricultural settings. To make the model more scalable and appropriate for long-term deployment in dynamic agricultural ecosystems, further research could employ adaptive learning approaches to change with new data over time. Importantly, the current architecture is optimized for RGB-only inputs, with a focus on real-time deployment in agricultural field conditions. Future work will address the incorporation of spectral diversity (e.g., multispectral and NIR data) and include multi-crop validation to ensure broader applicability and improved performance in varied agricultural systems. Conclusion This paper presents PSPEdgeWeedNet, a novel semantic segmentation architecture specifically developed for precise crop and weed identification in agricultural field conditions. The proposed model builds upon the foundational Pyramid Scene Parsing Network (PSPNet) by integrating an edge-aware module that enhances the model&#8217;s capability to delineate object boundaries with higher fidelity. This architectural augmentation enables the extraction of both global contextual information and localized edge features, which is particularly critical for distinguishing between visually similar plant structures such as weeds and crops, especially in heterogeneous field environments where occlusion, overlapping foliage, and background clutter are prevalent. Quantitative and qualitative evaluations demonstrate that PSPEdgeWeedNet surpasses the UNet, SegNet, PSPNet, DeepLabv3, Swin-Unet, and lightweight transformer model based on ViT in both segmentation performance and boundary localization. PSPEdgeWeedNet shows significant improvements in edge sharpness, reduced crop-to-weed misclassification, and improved retention of structural features such as weed leaf morphology. Although Conditional Random Fields (CRF) are not incorporated into the training or inference stages, their application in post-processing for visualization highlights the enhanced boundary preservation achieved by PSPEdgeWeedNet. The visual results suggest that the model is capable of generating continuous, well-preserved boundaries and exhibits reduced fragmentation, independent of any CRF-based post-processing. Furthermore, PSPEdgeWeedNet demonstrates improved sensitivity to small and sparsely distributed weed instances, al- though some extremely tiny weed structures remain difficult to accurately segment. Despite achieving high overall segmentation accuracy, certain challenges persist, notably suboptimal Intersection over Union (IoU) scores for underrepresented classes. These results confirm the efficacy of the proposed edge-aware enhancements in addressing core limitations of conventional semantic segmentation models and suggest potential avenues for future work in class-imbalance mitigation and fine-grained structure recovery in agricultural vision tasks. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements This work was supported by the Manipal Academy of Higher Education, Dr. T.M.A Pai Research Scholarship under Research Registration No: 230900101-2023. Author contributions DGP: Visualization, Writing&#8212;original draft, review and editing. MB: Validation, Supervision, review and editing. RK: Validation, Formal analysis, Investigation. All authors contributed to the article and approved the submitted version. Funding Open access funding provided by Manipal Academy of Higher Education, Manipal. Open access funding is provided by Manipal Academy of Higher Education, Manipal. Data availability The datasets presented in this study can be found in online repositories( https://github.com/ptdkhoa/Peanut-dataset ). Declarations Competing interests The authors declare no competing interests. References 1. Arg&#252;elles L March H Weeds in action: Vegetal political ecology of unwanted plants Prog. Hum. Geogr. 2022 46 44 66 10.1177/03091325211054966 Arg&#252;elles, L. &amp; March, H. Weeds in action: Vegetal political ecology of unwanted plants. Prog. Hum. Geogr. 46 , 44&#8211;66 (2022). 2. Ramesh K Matloob A Aslam F Florentine SK Chauhan BS Weeds in a changing climate: Vulnerabilities, consequences, and implications for future weed management Front. plant science 2017 8 95 10.3389/fpls.2017.00095 PMC5303747 28243245 Ramesh, K., Matloob, A., Aslam, F., Florentine, S. K. &amp; Chauhan, B. S. Weeds in a changing climate: Vulnerabilities, consequences, and implications for future weed management. Front. plant science 8 , 95 (2017). 10.3389/fpls.2017.00095 PMC5303747 28243245 3. Liebman. Integration of soil, crop and weed management in low-external-input farming systems. Weed research 40 , 27&#8211;47 (2000). 4. Shittu E Fagam A Garba A Sabo M Gworgwor N Weed control efficiency, nodulation and yield response of groundnut as affected by weed control, variety, and season in Bauchi, Nigeria. National innovation and research academia Int. J. Agribusiness Agric. Sci. 2022 7 1 17 Shittu, E., Fagam, A., Garba, A., Sabo, M. &amp; Gworgwor, N. Weed control efficiency, nodulation and yield response of groundnut as affected by weed control, variety, and season in Bauchi, Nigeria. National innovation and research academia. Int. J. Agribusiness Agric. Sci. 7 , 1&#8211;17 (2022). 5. Rakhmatulin I Kamilaris A Andreasen C Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review Remote. Sens. 2021 13 4486 10.3390/rs13214486 Rakhmatulin, I., Kamilaris, A. &amp; Andreasen, C. Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review. Remote. Sens. 13 , 4486 (2021). 6. Shaikh TA Rasool T Lone FR Towards leveraging the role of machine learning and artificial intelligence in precision agriculture and smart farming Comput. Electron. Agric. 2022 198 107119 10.1016/j.compag.2022.107119 Shaikh, T. A., Rasool, T. &amp; Lone, F. R. Towards leveraging the role of machine learning and artificial intelligence in precision agriculture and smart farming. Comput. Electron. Agric. 198 , 107119 (2022). 7. Patr&#237;cio DI Rieder R Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review Comput. Electron. Agric. 2018 153 69 81 10.1016/j.compag.2018.08.001 Patr&#237;cio, D. I. &amp; Rieder, R. Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review. Comput. Electron. Agric. 153 , 69&#8211;81 (2018). 8. Sharma K Shivandu SK Integrating artificial intelligence and internet of things (iot) for enhanced crop monitoring and management in precision agriculture Sens. Int. 2024 5 100292 10.1016/j.sintl.2024.100292 Sharma, K. &amp; Shivandu, S. K. Integrating artificial intelligence and internet of things (iot) for enhanced crop monitoring and management in precision agriculture. Sens. Int. 5 , 100292 (2024). 9. Niazian M Niedba&#322;a G Machine learning for plant breeding and biotechnology Agriculture 2020 10 436 10.3390/agriculture10100436 Niazian, M. &amp; Niedba&#322;a, G. Machine learning for plant breeding and biotechnology. Agriculture 10 , 436 (2020). 10. Pai DG Kamath R Balachandra M Deep learning techniques for weed detection in agricultural environments: A comprehensive review IEEE Access 2024 12 113193 113214 10.1109/ACCESS.2024.3418454 Pai, D. G., Kamath, R. &amp; Balachandra, M. Deep learning techniques for weed detection in agricultural environments: A comprehensive review. IEEE Access 12 , 113193&#8211;113214 (2024). 11. Ulku I Reslmffnet: A real-time semantic segmentation network for precision agriculture J. Real.Time Image Process. 2024 21 101 10.1007/s11554-024-01474-0 Ulku, I. Reslmffnet: A real-time semantic segmentation network for precision agriculture. J. Real.Time Image Process. 21 , 101 (2024). 12. Sa I weednet: Dense semantic weed classification using multispectral images and mav for smart farming IEEE Robot. Autom. Lett. 2017 3 588 595 10.1109/LRA.2017.2774979 Sa, I. et al. weednet: Dense semantic weed classification using multispectral images and mav for smart farming. IEEE Robot. Autom. Lett. 3 , 588&#8211;595 (2017). 13. You J Liu W Lee J A dnn-based semantic segmentation for detecting weed and crop Comput. Electron. Agric. 2020 178 105750 10.1016/j.compag.2020.105750 You, J., Liu, W. &amp; Lee, J. A dnn-based semantic segmentation for detecting weed and crop. Comput. Electron. Agric. 178 , 105750 (2020). 14. Kamath R Balachandra M Vardhan A Maheshwari U Classification of paddy crop and weeds using semantic segmentation Cogent eng. 2022 9 2018791 10.1080/23311916.2021.2018791 Kamath, R., Balachandra, M., Vardhan, A. &amp; Maheshwari, U. Classification of paddy crop and weeds using semantic segmentation. Cogent eng. 9 , 2018791 (2022). 15. Luo Z Yang W Yuan Y Gou R Li X Semantic segmentation of agricultural images: A survey Inf. Process. Agric. 2024 11 172 186 Luo, Z., Yang, W., Yuan, Y., Gou, R. &amp; Li, X. Semantic segmentation of agricultural images: A survey. Inf. Process. Agric. 11 , 172&#8211;186 (2024). 16. Su D Kong H Qiao Y Sukkarieh S Data augmentation for deep learning based semantic segmentation and crop-weed classification in agricultural robotics Comput. Electron. Agric. 2021 190 106418 10.1016/j.compag.2021.106418 Su, D., Kong, H., Qiao, Y. &amp; Sukkarieh, S. Data augmentation for deep learning based semantic segmentation and crop-weed classification in agricultural robotics. Comput. Electron. Agric. 190 , 106418 (2021). 17. Xu B Instance segmentation method for weed detection using UAV imagery in soybean fields Comput. Electron. Agric. 2023 211 107994 10.1016/j.compag.2023.107994 Xu, B. et al. Instance segmentation method for weed detection using UAV imagery in soybean fields. Comput. Electron. Agric. 211 , 107994 (2023). 18. Zhao, H., Shi, J., Qi, X., Wang, X. &amp; Jia, J. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition , 2881&#8211;2890 (2017). 19. Kaganami, H. G. &amp; Beiji, Z. Region-based segmentation versus edge detection. In 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing , 1217&#8211;1221 (IEEE, 2009). 20. Parra L Edge detection for weed recognition in lawns Comput. Electron. Agric. 2020 176 105684 10.1016/j.compag.2020.105684 Parra, L. et al. Edge detection for weed recognition in lawns. Comput. Electron. Agric. 176 , 105684 (2020). 21. Hajare, P. A. &amp; Tijare, P. A. Edge detection techniques for image segmentation. Int. J. Comput. Sci. Appl. 4 (2011). 22. Mathur N Mathur S Mathur D A novel approach to improve sobel edge detector Procedia Comput. Sci. 2016 93 431 438 10.1016/j.procs.2016.07.230 Mathur, N., Mathur, S. &amp; Mathur, D. A novel approach to improve sobel edge detector. Procedia Comput. Sci. 93 , 431&#8211;438 (2016). 23. Moazzam SI Khan US Qureshi WS Nawaz T Kunwar F Towards automated weed detection through two-stage semantic segmentation of tobacco and weed pixels in aerial imagery Smart Agric. Technol. 2023 4 100142 10.1016/j.atech.2022.100142 Moazzam, S. I., Khan, U. S., Qureshi, W. S., Nawaz, T. &amp; Kunwar, F. Towards automated weed detection through two-stage semantic segmentation of tobacco and weed pixels in aerial imagery. Smart Agric. Technol. 4 , 100142 (2023). 24. Gupta SK Yadav SK Soni SK Shanker U Singh PK Multiclass weed identification using semantic segmentation: An automated approach for precision agriculture Ecol. Informatics 2023 78 102366 10.1016/j.ecoinf.2023.102366 Gupta, S. K., Yadav, S. K., Soni, S. K., Shanker, U. &amp; Singh, P. K. Multiclass weed identification using semantic segmentation: An automated approach for precision agriculture. Ecol. Informatics 78 , 102366 (2023). 25. Gao H An accurate semantic segmentation model for bean seedlings and weeds identification based on improved erfnet Sci. Reports 2024 14 12288 10.1038/s41598-024-61981-9 PMC11136954 38811674 Gao, H. et al. An accurate semantic segmentation model for bean seedlings and weeds identification based on improved erfnet. Sci. Reports 14 , 12288 (2024). 10.1038/s41598-024-61981-9 PMC11136954 38811674 26. Liu, T. et al. Semantic segmentation for weed detection in corn. Pest Manag. Sci. (2025). 10.1002/ps.8554 39584373 27. Yu H Men Z Bi C Liu H Research on field soybean weed identification based on an improved unet model combined with a channel attention mechanism Front. Plant Sci. 2022 13 890051 10.3389/fpls.2022.890051 35783959 PMC9240479 Yu, H., Men, Z., Bi, C. &amp; Liu, H. Research on field soybean weed identification based on an improved unet model combined with a channel attention mechanism. Front. Plant Sci. 13 , 890051 (2022). 35783959 10.3389/fpls.2022.890051 PMC9240479 28. Zhang J Gong J Zhang Y Mostafa K Yuan G Weed identification in maize fields based on improved swin-unet Agronomy 2023 13 1846 10.3390/agronomy13071846 Zhang, J., Gong, J., Zhang, Y., Mostafa, K. &amp; Yuan, G. Weed identification in maize fields based on improved swin-unet. Agronomy 13 , 1846 (2023). 29. Zhao J Weed detection in potato fields based on improved yolov4: Optimal speed and accuracy of weed detection in potato fields Electronics 2022 11 3709 10.3390/electronics11223709 Zhao, J. et al. Weed detection in potato fields based on improved yolov4: Optimal speed and accuracy of weed detection in potato fields. Electronics 11 , 3709 (2022). 30. Mu Y A faster R-CNN-based model for the identification of weed seedling Agronomy 2022 12 2867 10.3390/agronomy12112867 Mu, Y. et al. A faster R-CNN-based model for the identification of weed seedling. Agronomy 12 , 2867 (2022). 31. Hu, X.-Z., Jeon, W.-S. &amp; Rhee, S.-Y. Sugar beets and weed detection using semantic segmentation. In 2022 International Conference on Fuzzy Theory and Its Applications (iFUZZY) , 1&#8211;4 (IEEE, 2022). 32. Charania, S., Lendave, P., Borwankar, J. &amp; Kadge, S. A novel approach to weed detection using segmentation and image processing techniques. In World Conference on Communication &amp; Computing (WCONF) , 1&#8211;5 (IEEE, 2023). 33. Genze N Improved weed segmentation in uav imagery of sorghum fields with a combined deblurring segmentation model Plant Methods 2023 19 87 10.1186/s13007-023-01060-8 37608384 PMC10463442 Genze, N. et al. Improved weed segmentation in uav imagery of sorghum fields with a combined deblurring segmentation model. Plant Methods 19 , 87 (2023). 37608384 10.1186/s13007-023-01060-8 PMC10463442 34. Rana S Rafanoset: Dataset of raw, manually, and automatically annotated raphanus raphanistrum weed images for object detection and segmentation Data Brief 2024 54 110430 10.1016/j.dib.2024.110430 38698801 PMC11063987 Rana, S. et al. Rafanoset: Dataset of raw, manually, and automatically annotated raphanus raphanistrum weed images for object detection and segmentation. Data Brief 54 , 110430 (2024). 38698801 10.1016/j.dib.2024.110430 PMC11063987 35. Chen Y Wang X Farmland extraction from UAV remote sensing images based on improved segformer model J. Indian Soc. Remote. Sens. 2025 53 421 433 10.1007/s12524-024-02004-y Chen, Y. &amp; Wang, X. Farmland extraction from UAV remote sensing images based on improved segformer model. J. Indian Soc. Remote. Sens. 53 , 421&#8211;433 (2025). 36. Fawakherji M Potena C Pretto A Bloisi DD Nardi D Multi-spectral image synthesis for crop/weed segmentation in precision farming Robot. Auton. Syst. 2021 146 103861 10.1016/j.robot.2021.103861 Fawakherji, M., Potena, C., Pretto, A., Bloisi, D. D. &amp; Nardi, D. Multi-spectral image synthesis for crop/weed segmentation in precision farming. Robot. Auton. Syst. 146 , 103861 (2021). 37. Ahmed, M. R. et al. Multiclass classification on soybean and weed species using a novel customized greenhouse robotic and hyperspectral combination system. Available at SSRN 4044574 (2022). 38. Atsmon G Nehurai O Kizel F Eizenberg H Lati RN Hyperspectral imaging facilitates early detection of orobanche cumana below-ground parasitism on sunflower under field conditions Comput. Electron. Agric. 2022 196 106881 10.1016/j.compag.2022.106881 Atsmon, G., Nehurai, O., Kizel, F., Eizenberg, H. &amp; Lati, R. N. Hyperspectral imaging facilitates early detection of orobanche cumana below-ground parasitism on sunflower under field conditions. Comput. Electron. Agric. 196 , 106881 (2022). 39. Tran THY Phan TDK Dense multi-scale convolutional network for plant segmentation IEEE Access 2023 11 82640 82651 10.1109/ACCESS.2023.3300234 Tran, T. H. Y. &amp; Phan, T. D. K. Dense multi-scale convolutional network for plant segmentation. IEEE Access 11 , 82640&#8211;82651 (2023)."
}