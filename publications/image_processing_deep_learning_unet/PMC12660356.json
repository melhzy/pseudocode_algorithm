{
  "pmcid": "PMC12660356",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:29.466218",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12660356",
    "pmid": "41309699",
    "doi": "10.1038/s41598-025-26314-4",
    "title": "Feature fusion context attention gate UNet for detection of polycystic ovary syndrome",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Natarajan Yuvaraj",
      "K. R. Sri Preethaa",
      "M. Shyamala Devi"
    ],
    "abstract": "Polycystic Ovary Syndrome (PCOS) is a prevalent endocrine disorder affecting women of reproductive age, characterized by hormonal imbalance, irregular menstrual cycles, and ovarian cysts. Traditional diagnostic approaches, which include clinical evaluations, radiological studies, and surgical interventions, are often time-consuming, costly, and not always reliable. To improve the accuracy and efficiency of PCOS diagnosis, this research introduces the Feature Fusion Context Attention U-Net (FCAU-Net) model, leveraging deep learning (DL) techniques. This study makes two key contributions. First, it enhances dataset preparation through Fuzzy Contrast Enhanced (FCE) imaging. Second, it integrates a Feature Fusion Context (FFC) module into the Attention U-Net model, optimizing the extraction of context and position weights from feature maps for better classification performance. An openly available PCOS Ultrasound Image Dataset with 3,800 images was partitioned with 80: 20 to ensure that only original images were used for testing, while augmented samples were exclusively utilized for training to enhance model generalization and robustness. The remaining 3040 images was augmented to form 45,600 images and split into training and validation sets in an 80:20 ratio. The augmented images were processed and tested with several DL models, including DenseNet, AlexNet, VGG19, ResNet, U-Net, and Attention U-Net. Among these, the Attention U-Net initially achieved over 80% accuracy in detecting PCOS. The proposed FCAU-Net, which incorporates the FFC module, demonstrated superior performance, achieving a detection accuracy of 99.89%, significantly outperforming existing DL models. This research highlights the potential of FCAU-Net in providing a more accurate and efficient tool for the diagnosis of PCOS.",
    "keywords": [
      "Polycystic ovary syndrome (PCOS)",
      "DL",
      "Feature fusion context attention U-Net (FCAU-Net)",
      "Ultrasound imaging",
      "Medical image classification",
      "Computational biology and bioinformatics",
      "Diseases",
      "Endocrinology",
      "Health care",
      "Mathematics and computing",
      "Medical research"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12660356</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12660356.1</article-id><article-id pub-id-type=\"pmcaid\">12660356</article-id><article-id pub-id-type=\"pmcaiid\">12660356</article-id><article-id pub-id-type=\"pmid\">41309699</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-26314-4</article-id><article-id pub-id-type=\"publisher-id\">26314</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Feature fusion context attention gate UNet for detection of polycystic ovary syndrome</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Natarajan</surname><given-names initials=\"Y\">Yuvaraj</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>K. R.</surname><given-names initials=\"SP\">Sri Preethaa</given-names></name><address><email>sripreethaa.kr@vit.ac.in</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>M.</surname><given-names initials=\"SD\">Shyamala Devi</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00qzypv28</institution-id><institution-id institution-id-type=\"GRID\">grid.412813.d</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0687 4946</institution-id><institution>School of Computer Science and Engineering, </institution><institution>Vellore Institute of Technology, </institution></institution-wrap>Vellore, Tamil Nadu 632014 India </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/040c17130</institution-id><institution-id institution-id-type=\"GRID\">grid.258803.4</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0661 1556</institution-id><institution>Department of Robot and Smart System Engineering, </institution><institution>Kyungpook National University, </institution></institution-wrap>80, Daehak-ro, Buk-gu, Daegu, 41566 Republic of Korea </aff></contrib-group><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>42386</elocation-id><history><date date-type=\"received\"><day>20</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>28</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>29</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-29 00:25:14.510\"><day>29</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_26314.pdf\"/><abstract id=\"Abs1\" abstract-type=\"unstructured\"><p id=\"Par1\">Polycystic Ovary Syndrome (PCOS) is a prevalent endocrine disorder affecting women of reproductive age, characterized by hormonal imbalance, irregular menstrual cycles, and ovarian cysts. Traditional diagnostic approaches, which include clinical evaluations, radiological studies, and surgical interventions, are often time-consuming, costly, and not always reliable. To improve the accuracy and efficiency of PCOS diagnosis, this research introduces the Feature Fusion Context Attention U-Net (FCAU-Net) model, leveraging deep learning (DL) techniques. This study makes two key contributions. First, it enhances dataset preparation through Fuzzy Contrast Enhanced (FCE) imaging. Second, it integrates a Feature Fusion Context (FFC) module into the Attention U-Net model, optimizing the extraction of context and position weights from feature maps for better classification performance. An openly available PCOS Ultrasound Image Dataset with 3,800 images was partitioned with 80: 20 to ensure that only original images were used for testing, while augmented samples were exclusively utilized for training to enhance model generalization and robustness. The remaining 3040 images was augmented to form 45,600 images and split into training and validation sets in an 80:20 ratio. The augmented images were processed and tested with several DL models, including DenseNet, AlexNet, VGG19, ResNet, U-Net, and Attention U-Net. Among these, the Attention U-Net initially achieved over 80% accuracy in detecting PCOS. The proposed FCAU-Net, which incorporates the FFC module, demonstrated superior performance, achieving a detection accuracy of 99.89%, significantly outperforming existing DL models. This research highlights the potential of FCAU-Net in providing a more accurate and efficient tool for the diagnosis of PCOS.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Polycystic ovary syndrome (PCOS)</kwd><kwd>DL</kwd><kwd>Feature fusion context attention U-Net (FCAU-Net)</kwd><kwd>Ultrasound imaging</kwd><kwd>Medical image classification</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Computational biology and bioinformatics</kwd><kwd>Diseases</kwd><kwd>Endocrinology</kwd><kwd>Health care</kwd><kwd>Mathematics and computing</kwd><kwd>Medical research</kwd></kwd-group><funding-group><award-group><funding-source><institution>Vellore Institute of Technology, Vellore</institution></funding-source></award-group><open-access><p>Open access funding provided by Vellore Institute of Technology.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec56\"><title>Introduction</title><p id=\"Par2\">Polycystic Ovary Syndrome (PCOS) is a major medical condition with a high prevalence of morbidity and mortality across the world. Proper care and prevention of related problems including diabetes, heart disease, and infertility depend on early identification of PCOS. Since PCOS appears differently in each person, developing a consistent set of diagnostic criteria is difficult. PCOS diagnostic requirements can cause inconsistent diagnosis amongst medical professionals and different regions, which can affect the precision of treatment plans and occurrence estimations. Medical practitioners may find it difficult to diagnose PCOS because of a number of factors including a patient&#8217;s history, complex hormonal profiles, overlap with other conditions, subjective ultrasound interpretation, diagnostic delay, and varied symptoms. A typical technique for visualizing ovarian morphology in PCOS patients is transvaginal ultrasonography. Interpreting ultrasonography results, such as ovarian volume and cyst presence, can be subjective and operator-specific. The accuracy and consistency of the PCOS diagnosis may be compromised by this variation. DL has the potential to improve diagnostic consistency and accuracy by providing a consistent, objective method of evaluating large, complicated information, such as clinical data and medical images. In recent years, there has been a fast development of DL techniques, which have been employed in medical image analysis and disease classification. DL has demonstrated outstanding performance in a number of medical specialties, including skin diseases, pathological conditions, and radiography. DL models may use massive data sets and complex algorithms to uncover hidden patterns and characteristics from medical imaging. The adaptation of DL methods may result in faster and more accurate diagnosis times. The neural networks are used in SL that extract knowledge from massive volumes of data. Complex characteristics may be automatically extracted from medical images using DL models, which may help with the objective interpretation of ultrasound scans used to diagnose PCOS. In terms of PCOS, DL offers promising opportunities to improve and automate PCOS screening. Automatic PCOS identification speeds up diagnosis and enhances medical practitioners&#8217; diagnostic consistency. Medical professionals may minimize disparities between radiologists&#8217; diagnoses by employing DL technology to automate and standardize the PCOS detection process. This consistency increases the standard of care given to patients, reduces the incidence of diagnostic mistakes, and increases the precision of diagnoses.</p><p id=\"Par3\">CNNs that focus on segmentation, like AResUNet, CR-UNet, and Ocys-Net, have been investigated in recent studies on PCOS detection using ultrasound imaging. These CNNs mainly improve follicle and cyst localization, but they frequently have difficulties with generalized feature representation. Although deep CNN-based classifiers such as VGG16, EfficientNetB6, and SqueezeNet have demonstrated remarkable classification accuracy, their ability to adapt to noisy clinical ultrasound pictures is limited due to their heavy reliance on transfer learning and large-scale data. Although they offer better diagnostic robustness, hybrid models that combine CNNs with ensemble learners (such as SVM, RF, or BiLSTM) or feature selection methods have a greater computational cost. Elman NN and PCOS-WaveConvNet, two wavelet and spectral-based models, have improved spatial-spectral extraction but lack precise contextual awareness. While attention-based designs such as ASPPNet and Attention U-Net have demonstrated superior multiscale feature learning, they frequently lack in their ability to successfully incorporate position-sensitive information and contextual dependencies. With the above challenges, this research proposes FCAU-Net model that offers the PCOS detection with high accuracy. To address these limitations, the proposed FCAU-Net introduce an innovative FFC module that adaptively combines spatial, positional, and contextual cues from feature maps, while FCE preprocessing enhances cystic boundary visibility.</p><sec id=\"Sec57\"><title>Paper organization and research contribution</title><p id=\"Par4\">This research paper is organized as follows: Sect.&#160;<xref rid=\"Sec1\" ref-type=\"sec\">2</xref> explores the background literature review. Section&#160;<xref rid=\"Sec2\" ref-type=\"sec\">3</xref> deals with the design of proposed FCAU-Net model research methodology. Section&#160;<xref rid=\"Sec4\" ref-type=\"sec\">4</xref> covers the mathematical modeling of the proposed FCAU-Net model. Section&#160;<xref rid=\"Sec10\" ref-type=\"sec\">5</xref> discusses the results and implementation analysis of the proposed FCAU-Net model. In the end, Sect.&#160;6 concludes the proposed FCAU-Net model that includes the challenges, novelty and Future work. This research primarily contributes in two ways.</p><p id=\"Par5\">\n<list list-type=\"simple\"><list-item><label>(i)</label><p id=\"Par6\"><italic toggle=\"yes\">Fuzzy Contrast Enhancement</italic>: The first contribution focuses on preparing datasets through image cropping and feature improvement using an adaption of the FCE image. FCE is the novel preprocessing technique that improves the quality of ultrasound images by enhancing contrast and reducing ambiguity, thereby facilitating more accurate feature extraction.</p></list-item><list-item><label>(ii)</label><p id=\"Par7\"><italic toggle=\"yes\">Feature Fusion Context Attention U-Net</italic>: The second contribution emphasizes on integrating FFC module into the Attention U-Net model for supervised learning towards detecting PCOS. Proposes enhanced Attention U-Net architecture FCAU-Net that is integrated with a FFC module that effectively captures both contextual and positional information from feature maps, leading to superior diagnostic performance. The FFC module extracts the Context and position weights of the Feature Maps (FM) that optimizes the deep and shallow features of FM as shown in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>.</p></list-item></list>\n</p><p id=\"Par8\">\n<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Feature fusion context module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e232\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig1_HTML.jpg\"/></fig>\n</p></sec></sec><sec id=\"Sec1\"><title>Background study</title><p id=\"Par9\">Several DL and machine learning approaches have been explored for the detection and classification of PCOS using medical imaging, particularly ultrasound. Early studies applied CNN-based architectures with segmentation and handcrafted preprocessing techniques, such as adaptive bilateral filtering, Otsu thresholding, watershed segmentation, and Gabor wavelet-based feature extraction, to enhance cyst and follicle identification. Models such as AResUNet and CR-UNet demonstrated improved noise reduction, robustness against low-contrast images, and better adaptation to multimodal inputs. The following Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> presents a structured overview of existing works on PCOS detection and related medical image analysis, highlighting the methodology, imaging modality, preprocessing techniques, and performance outcomes.</p><p id=\"Par10\">\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Key outcomes from the related works.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Preprocessing</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Approach</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Key outcome</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Segmentation-focused CNN:</italic> AResUNet<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, CR-UNet<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, Ocys-Net<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>, CNN with Segmentation<sup><xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup>, Threshold and Watershed<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref>,<xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>, Hybrid<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>, GrabCut<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Filtering, Otsu, contour, watershed</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deep U-Net variants, reverse bottleneck, watershed and contour-based segmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Improved follicle, cyst localization and tracking</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Deep CNN classifiers:</italic> VGG16<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>, SqueezeNet<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>, CNN&#8201;+&#8201;FC<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup>, ITL-CNN<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup>, InceptionV3<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref>,<xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup>, EfficientNetB6<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ultrasound preprocessing, TL, normalization</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN-based classification with batch norm, TL, activation optimization</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Higher PCOS detection and classification accuracy</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Hybrid DL models:</italic> VGG16&#8201;+&#8201;Ensemble<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref>,<xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>, CNN with BiLSTM<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>, CNN with KNN<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>, CNN with SVM, DT, RF<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref>,<xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup>, Sequential 2D CNN with FS<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Feature selection, ensemble integration</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN fused with ensembles, clustering, sequential learning, wrapper FS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Robust diagnosis across diverse datasets</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Wavelet and spectral approaches:</italic> PCOS-WaveConvNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>, Elman NN with Wavelet<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>, BPA with Gabor<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Spectral domain, Gabor wavelets</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Wavelet-based ConvNets, Elman NNs, Gabor filters</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Strong spectral, spatial feature extraction</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">GAN and augmentation methods:</italic> GAN with CNN<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data augmentation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Generative adversarial networks with CNN classifier</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Overcame overfitting, improved generalization</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Feature-driven and hybrid Methods</italic> (ESDPCOS<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>, AMCNN<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, MLOD<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, GIST-MDR<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup>, Probabilistic PCOS<sup><xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ultrasound texture, GLCM, fuzzy logic</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Hybrid models combining CNN with GLCM, fuzzy logic, texture scoring</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhanced robustness in classification</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Attention and context learning</italic> (ASPPNet&#8201;+&#8201;ResNet<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup>, AMCNN<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, EfficientNetB6 with Attention UNet<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ultrasound</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention mechanisms, dilated conv, spatial pyramid pooling</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Better multiscale learning and context capture</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">Other imaging modalities:</italic> Ovarian quantification<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>, Eye scleral biomarkers<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>, MRI follicle count<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ultrasound, scleral, MRI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Quantification of follicles, scleral biomarkers, MRI follicle mapping</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multimodal biomarkers for improved PCOS detection</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par11\">To address the limitations of conventional CNNs, attention mechanisms and multi-scale learning architectures were introduced. Examples include ASPPNet, AMCNN, and ensemble frameworks combining EfficientNet and Attention U-Net, which enabled better contextual feature extraction and improved segmentation efficiency. Similarly, transfer learning strategies with InceptionV3, VGG16, and ResNet variants allowed reuse of pre-trained weights for enhanced classification performance on limited datasets. Hybrid approaches also emerged to overcome challenges of feature uncertainty and overfitting. These integrated deep models with traditional techniques such as fuzzy logic, SVMs, clustering, or wavelets. For instance, CNNs combined with fuzzy layers or KNN clustering improved feature reliability, while GAN-based augmentation addressed overfitting by generating synthetic data. Other models, such as PCOS-WaveConvNet and Ocys-Net, explored wavelet transforms and reverse bottleneck designs for richer feature representation. Recent advancements focused on ensemble and optimization-driven methods, including stacking models that merged VGG16, ResNet50, and MobileNet, or HHO-DQN frameworks that optimized hyperparameters for deep networks. Additionally, segmentation-driven workflows, such as hybrid Otsu-Chanvese segmentation, GrabCut with fuzzy SNN models, and probabilistic grid-based analysis, provided more precise localization of follicles. Overall, the existing works achieved varying levels of accuracy, but faced persistent challenges on handcrafted preprocessing, and limited robustness across datasets. This underscores the need for an end-to-end, adaptive, and context-aware architecture, which motivates the development of proposed FCAU-Net. The DL applications towards health care with pre-trained CNN models can be explored<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR46\">46</xref></sup>. The inferences, advantages and the limitations from literature survey were shown in Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref>.</p><p id=\"Par12\">\n<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Inferences from literature Review.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"2\" rowspan=\"1\">Methodology and Inference</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Advantages</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Limitations</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Sequential CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Fuzzy CNN<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>, Confluence CNN<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>, ESDPCOS<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup>, AMCNN<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup>, KNN based CNN<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup>, MLOD<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, Ocys-Net<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>, ITL-CNN<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup>, Ensemble CNN<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>, Hybrid CNN<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>, Sequential CNN<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup>, 2D CNN<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup>, Tri-stage wrapper CNN<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup>, SSFSE-DL<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup>, DLNNSVM<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup>, GrabCut and Fuzzy Logic -SNN Model<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>Best in handling images with permissible noise and scalable to apply the same model to any large number of datasets</p><p>Easy to integrate with other CNN architectures or combined with TL</p><p>Image pixels and the ROI are identified uniquely in each convolution layer operation</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>The model should be refined with labelled data</p><p>Need to validate with various cross-validation methods to fine-tune the accuracy</p><p>Need separate validation to handle augmentation images</p><p>Depends on feature filtering to extract deep features from the ultrasound images</p></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Recurrent CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Complex Spatial Recurrent Neural Network U-Net<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, Elman NN<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup>, Back Propagation Algorithm<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>Spatial relationship between image pixels is effectively correlated</p><p>Relationship between the image frames is reused</p><p>Effectively extract the patterns in ultrasound image</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>Best suited for large dataset</p><p>Less performance while handling image with ling sequences and cannot handle static image data</p><p>Depends on effective feature extraction methods</p></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Pre-Trained CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">AResUNet<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, Inception CNN<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>, ResNet<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>, VGGNet<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>, Inception V3<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref>,<xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup>, PCOS-WaveConvNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>, PCONet<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>, VGG16<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref>,<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>,<xref ref-type=\"bibr\" rid=\"CR24\">24</xref>,<xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>, ASPPNet<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>, HHO-DQN<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup>, SqueezeNet<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup>, UNet and EfficientNet<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>, GIST-MDR<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup>, U-Net<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup> and ResNet<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>AResUNet achieves 98% of accuracy</p><p>Inception CNN achieves 84.81% of accuracy</p><p>Exhibits good performance with little amount of data</p><p>Best in extracting the deep features</p></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><p>Model need to be fine-tuned with several activation function and number of layers to improve the performance</p><p>Depends on balanced data before fitting the model</p></td></tr></tbody></table></table-wrap>\n</p><p id=\"Par13\">Pathological image segmentation method<sup><xref ref-type=\"bibr\" rid=\"CR58\">58</xref></sup> based on multiscale and dual attention mechanisms, aiming to enhance feature representation and improve segmentation precision. The multiscale module allows the network to capture both global contextual information and fine-grained local details, while the dual attention mechanism emphasizes salient spatial and channel-wise features, reducing the influence of irrelevant regions. Improved TransUnet framework<sup><xref ref-type=\"bibr\" rid=\"CR59\">59</xref></sup> for melanoma image segmentation on integrating transformer-based global context modeling with enhanced convolutional modules could capture fine-grained lesion details with high accuracy. The high-order paired-ASPP (Atrous Spatial Pyramid Pooling) Network<sup><xref ref-type=\"bibr\" rid=\"CR60\">60</xref></sup> enhances the semantic segmentation by effectively capturing both global context and fine local structures. By leveraging high-order feature interactions and a paired atrous spatial pyramid pooling design, the method improves boundary delineation and reduces semantic ambiguity. EnsembleEdgeFusion framework<sup><xref ref-type=\"bibr\" rid=\"CR61\">61</xref></sup> designed to advance semantic segmentation in microvascular decompression imaging. By integrating multiple segmentation models with edge-aware fusion strategies, the method enhances boundary precision and structural consistency in complex medical images. Dilated SE-DenseNet<sup><xref ref-type=\"bibr\" rid=\"CR62\">62</xref></sup> framework classifies the brain tumor using MRI scans. By combining dilated convolutions with squeeze-and-excitation modules, the model captures multi-scale contextual information while adaptively emphasizing the most relevant features.</p><p id=\"Par14\">The automated framework for high-precision PCOS detection that leverages the Segment Anything Model (SAM)<sup><xref ref-type=\"bibr\" rid=\"CR63\">63</xref></sup> applied to super-resolution ultrasound ovary images. By combining advanced segmentation with image enhancement, the method achieves more accurate follicle boundary delineation and improved feature representation. HR-ASPP<sup><xref ref-type=\"bibr\" rid=\"CR64\">64</xref></sup>, an enhanced semantic segmentation model for cervical nucleus images builds on DeepLabv3 + with improved atrous spatial pyramid pooling. By focusing on high-resolution spatial localization and robust shape feature extraction, the model achieves more precise nucleus boundary detection. The dual-stage U-Net DSU-Net<sup><xref ref-type=\"bibr\" rid=\"CR65\">65</xref></sup> integrates CNN-based local feature extraction with transformer-based global context modeling for skin lesion segmentation. This hybrid design enhances both boundary precision and contextual understanding, enabling more accurate lesion delineation. The dual-encoder attention network DEAU-Net<sup><xref ref-type=\"bibr\" rid=\"CR66\">66</xref></sup> improve medical image segmentation by combining two encoders with attention mechanisms. It effectively captures multi-scale contextual information and emphasizes salient features while suppressing irrelevant regions. The integration of attention mechanisms with DL enhances the medical image segmentation by feature representation, capturing contextual dependencies, and improving segmentation accuracy across diverse imaging modalities. These approaches have shown remarkable performance in diverse medical imaging domains, including brain tumor classification, melanoma segmentation, cervical nucleus detection, and microvascular imaging, highlighting the potential of attention-guided and hybrid feature extraction frameworks. Inspired by these research works, the proposed FCAU-Net integrates feature-calibrated attention modules with an end-to-end architecture tailored for ovarian ultrasound images, addressing challenges such as small follicle structures, low contrast, and imaging noise. The combination of multiscale feature extraction, attention-guided focus, and computational efficiency in FCAU-Net is directly motivated by these prior works, aiming to achieve high-precision, robust, and clinically applicable PCOS detection, surpassing the limitations of existing CNN, U-Net, and ensemble-based approaches.</p><p id=\"Par15\">Recent advancements in medical image processing have witnessed the emergence of several U-Net derivatives that integrate hybrid architectural components such as Atrous Spatial Pyramid Pooling (ASPP), dual attention schemes, and Squeeze-and-Excitation (SE) modules to enhance feature extraction and boundary precision. For instance, SAP-UNet has been successfully employed for ultrasound-based segmentation by combining ASPP with SE blocks to capture multi-scale contextual representations while refining channel-wise significance<sup><xref ref-type=\"bibr\" rid=\"CR72\">72</xref></sup>. Similarly, DDA-AttResUNet<sup><xref ref-type=\"bibr\" rid=\"CR73\">73</xref></sup> developed for breast and ovarian ultrasound segmentation tasks, utilizes a dual decoder mechanism fused with residual and attention pathways to enhance feature propagation between encoder and decoder stages. Beyond these, architectures such as DA-TransUNet<sup><xref ref-type=\"bibr\" rid=\"CR74\">74</xref></sup> and Hybrid Dilated Residual U-Net<sup><xref ref-type=\"bibr\" rid=\"CR75\">75</xref></sup> have incorporated both spatial and channel-wise attention blocks for fine-grained tissue segmentation. In the specific context of ovarian and PCOS imaging, studies such as CystNet, Enhanced AResU-Net<sup><xref ref-type=\"bibr\" rid=\"CR76\">76</xref></sup>, Follicles-Net<sup><xref ref-type=\"bibr\" rid=\"CR77\">77</xref></sup>, RNN<sup><xref ref-type=\"bibr\" rid=\"CR78\">78</xref></sup> and ML<sup><xref ref-type=\"bibr\" rid=\"CR79\">79</xref></sup> have reported improved cyst recognition through hierarchical or dilated convolutional blocks. These designs, while effective in multiscale feature aggregation, often focus on static pooling or global context enhancement without fully integrating localized spatial relational learning. The proposed FCAU-Net differs by introducing FFC module that dynamically combines spatial and contextual information across scales, thereby refining ovarian cyst segmentation boundaries and follicular classification accuracy. Moreover, through FCE preprocessing, the model ensures improved noise resilience and region smoothness compared to ASPP or SE-based variants.</p></sec><sec id=\"Sec2\"><title>Research methodology of proposed FCAU-Net model</title><p id=\"Par16\">The proposed FCAU-Net model was designed to classify the PCOS infected and Normal healthy images. The FCAU-Net research methodology is shown in Fig. <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>. The overall research methodology of FCAU-Net initiates with stage 1 that performs collection of 3800 PCOS ultrasound Images Dataset from KAGGLE having 1900 PCOS infected images and 1900 Normal healthy images<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup>. Stage 2 deals with dataset preprocessing that segregates the images based on normal and PCOS symptoms. Then the Labelling of the image is done followed by data augmentation by generating 14 augmented images for each image in the dataset resulting with 53,200 images. The data augmentation was performed using horizontal flipping, vertical flipping, rotation with positive and negative angle of 45, 90, 135, 180, 225, 270 degrees. The data augmented cropped images are subjected to generate fuzzy contrast enhanced image vector. Stage 3 fits the HEG images are fitted with the existing CNN models like DenseNet, AlexNet, VGG19, ResNet, Inception, UNet and Attention UNet to select the best CNN model. The Attention U-Net found to detect the existence of PCOS with accuracy above 80%.</p><p id=\"Par17\">\n<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Proposed FCAU-Net research methodology.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e819\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig2_HTML.jpg\"/></fig>\n</p><p id=\"Par18\">So, the Attention U-Net was refined to improve the accuracy by proposing FCAU-Net. The proposed FCAU-Net Overall architecture shown in Fig. <xref rid=\"Fig3\" ref-type=\"fig\">3</xref>. The FCAU-Net framework retrieves the PCOS ultrasound images that are subjected to the image segregation based on the disease class as Normal and PCOS infected images. The PCOS ultrasound images are performed with data preprocessing to generate FCE images by calling Fuzzy contrast enhanced module that is shown in Fig. <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>. The FCE images are fitted to proposed FCAU-Net framework that predicts the mask and classifies the ultrasound images based on the class. Steps involved in generating the enhanced image vector (Fig. <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>) retrieves the segregated labeled PCOS ultrasound images.</p><p id=\"Par19\">\n<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Overall architecture of proposed FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e840\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig3_HTML.jpg\"/></fig>\n</p><p id=\"Par20\">The labelled PCOS ultrasound images subjected to image cropping by finding the biggest contour and extract the extreme points to form the crop images. The image cropping was performed in order to focus on the significant image features. The cropped images are performed with data augmentation that 14 images for each image in the dataset resulting with 53,200 images. All the data augmented images are subjected to enhance the brightness of the image by forming Histogram equalized image, CLAHE image and Fuzzy Contrast Enhanced image. This work forms both Histogram equalized image and CLAHE with the intent of performance evaluation as both of them serves a distinct purpose. The normal Histogram equalization of the image works for the entire image by stretching the pixel intensity. Though CLAHE is a method of histogram equalization, it applies histogram for small adaptive regions that prevents noise in homogenous areas. The FCE images, known for their high brightness, are validated using the PSNR ratio and processed through the Fuzzy Contrast Enhanced module. The FCAU-Net framework, depicted in Fig.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, uses these FCE images as input. The images are passed through four encoder blocks and four decoder blocks. The encoder-decoder feature maps are combined using the Feature Fusion Context (FFC) module, which extracts positional and contextual characteristics to generate optimized fused feature maps.</p><p id=\"Par21\">\n<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Steps in generating enhanced image vector.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e855\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig4_HTML.jpg\"/></fig>\n</p><p id=\"Par22\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>FCAU-Net framework.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e865\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig5_HTML.jpg\"/></fig>\n</p><p id=\"Par23\">In the FCAU-Net (Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>), the FCE images are initially downscaled by a factor of 2 through the encoder blocks, creating contrasted FCE feature maps. These are then passed through decoder blocks integrated with attention gates, which upscale the feature maps, resulting in expanded FCE segmentation feature maps.</p><p id=\"Par24\">\n<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>FCAU-Net Framework (F &#8211; Feature Maps, H, W, D &#8211; Height, width and Depth of the feature maps).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e880\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig6_HTML.jpg\"/></fig>\n</p><p id=\"Par25\">\n<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Attention gate network in FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e890\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig7_HTML.jpg\"/></fig>\n</p><p id=\"Par26\">The attention gate mechanism is illustrated in Fig.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref>. The attention gate in FCAU-Net takes the input feature map and gate signal to calculate the gating coefficient. Batch normalization is applied to the gating coefficient to center the features in the active region while maintaining the relevance of unaligned weights in the feature maps. ReLU activation is then used to introduce nonlinearity, helping the feature vector learn complex representations. Dropout is applied to remove noise from the aligned weights of the feature map. A 1&#8201;&#215;&#8201;1 linear convolution is performed next to generate the attention feature map based on vector concatenation. Finally, the sigmoid activation function is applied, assigning a weight of &#8220;1&#8221; to the aligned features to create the attention coefficient feature map.</p><sec id=\"Sec3\"><title>Feature fusion context module in proposed FCAU-Net</title><p id=\"Par27\">The novelty of this research lies in the integration of the Feature Fusion Context (FFC) Module, positioned between the encoder and decoder blocks of the FCAU-Net, as shown in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>. The FFC module enhances feature maps by extracting positional and contextual information. Positional information is obtained by analysing correlations within feature maps (FMs). Spatial features are extracted through convolution, resulting in a 3D FM comprising query, key, and value components. These features are compared to compute the Energy Score Matrix (ESM), which highlights the relative importance of pixel positions. The ESM is normalized using SoftMax to generate Position Attention Weights (PAW), capturing positional details. Contextual information is derived by identifying interdependencies between FM channels. Attention scores indicating feature importance are computed and normalized with SoftMax to create weighted FMs. These weighted FMs are multiplied with the original FMs to strengthen the cumulative channel content. The weighted and original FMs are then integrated to enhance the model&#8217;s ability to capture contextual details. The FFC module processes both original and small-scale FMs to fuse positional and contextual information, resulting in improved feature map quality.</p><p id=\"Par28\">This fusion significantly enhances prediction performance, validating the module&#8217;s effectiveness in optimizing feature. First, the Position Attention Weights (PAW) are generated for both original and small-scale feature maps (FMs). For the original scale FM, depth-wise convolution with three kernels is applied, followed by batch normalization and single-kernel convolution. For the small-scale FM, convolution with three kernels and two strides is followed by batch normalization and average pooling with three kernels and two strides. Next, the Semantic Attention Weights (SAW) are created. For the original scale FM, convolution with three kernels is applied, followed by batch normalization and up-sampling with a sigmoid activation function. For the small-scale FM, depth-wise convolution with three kernels is followed by batch normalization and single convolution with a sigmoid activation function. The PAW and SAW of the original scale FM are then combined to form the Position Semantic Weight for the original scale FM. Similarly, the PAW and SAW of the small-scale FM are concatenated and up-sampled to create the Position Semantic Weight for the small-scale FM. Finally, the Position Semantic Weights of both scales are fused to produce optimized fused FM.</p></sec></sec><sec id=\"Sec4\"><title>Development of FCAU-Net and mathematical modeling</title><p id=\"Par29\">The FCAU-Net initiates by collecting ultrasound images from publicly available PCOS ultrasound Images Dataset from KAGGLE dataset for classifying the PCOS infected and normal healthy images are denoted in the Eq.&#160;(<xref rid=\"Equ786\" ref-type=\"disp-formula\">1</xref>).</p><p id=\"Par30\">\n<disp-formula id=\"Equ786\"><label>1</label><tex-math id=\"d33e916\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$PCOS_{{3800}} = \\left\\lfloor { \\cup _{{W = 1}}^{{3800}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {PCOS_{{ed}} } } } \\right\\}} \\right\\rfloor$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par31\">Where &#8220;<inline-formula id=\"IEq2\"><tex-math id=\"d33e923\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{PCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; denotes single ultrasound image, &#8220;<inline-formula id=\"IEq3\"><tex-math id=\"d33e927\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e,\\:d$$\\end{document}</tex-math></inline-formula>&#8221; denotes the number of row and column pixels and &#8220;<inline-formula id=\"IEq4\"><tex-math id=\"d33e931\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W$$\\end{document}</tex-math></inline-formula>&#8221; denotes the number of images in the dataset. The single ultrasound image is denoted in Eq.&#160;(<xref rid=\"Equ456\" ref-type=\"disp-formula\">2</xref>).</p><p id=\"Par32\">\n<disp-formula id=\"Equ456\"><label>2</label><tex-math id=\"d33e940\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$PCOS_{{001}} = \\left[ {\\begin{array}{*{20}l} {PCOS\\left( {0,0} \\right)} &amp; {PCOS\\left( {0,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {PCOS\\left( {0,255} \\right)} \\\\ {PCOS\\left( {1,0} \\right)} &amp; {PCOS\\left( {1,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {PCOS\\left( {1,255} \\right)} \\\\ \\vdots &amp; \\vdots &amp; { \\ldots \\ldots \\ldots } &amp; \\vdots \\\\ {PCOS\\left( {255,0} \\right)} &amp; {PCOS\\left( {255,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {\\:PCOS\\left( {255,255} \\right)} \\\\ \\end{array} } \\right]$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par33\">The ultrasound images are applied to the data preprocessing module by processing fuzzy contrast enhancement to generate the FCE images.</p><sec id=\"Sec5\"><title>Data preprocessing modeling</title><p id=\"Par34\">The input ultrasound images are segregated based on the PCOS disease class as shown in the Eq.&#160;(<xref rid=\"Equ987\" ref-type=\"disp-formula\">3</xref>) denoting 1900 &#8220;<inline-formula id=\"IEq6\"><tex-math id=\"d33e954\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Infect$$\\end{document}</tex-math></inline-formula>&#8221; as PCOS infected images and 1900 &#8220;<inline-formula id=\"IEq7\"><tex-math id=\"d33e958\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Norm$$\\end{document}</tex-math></inline-formula>&#8221; as normal healthy images which were labeled.</p><p id=\"Par35\">\n<disp-formula id=\"Equ987\"><label>3</label><tex-math id=\"d33e964\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$PCOS_{{3800}} = \\left\\lfloor { \\cup _{{W = 1}}^{{1900}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {Infect_{{ed}} } } } \\right\\} + \\cup _{{W = 1}}^{{1900}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {Norm_{{ed}} } } } \\right\\}} \\right\\rfloor$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par36\">The labeled ultrasound images are processed with the image cropping and data augmentation.</p></sec><sec id=\"Sec6\"><title>Image cropping and data augmentation modeling</title><p id=\"Par37\">Each labeled &#8220;<inline-formula id=\"IEq9\"><tex-math id=\"d33e975\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{PCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; ultrasound input image is processed with image cropping by extracting the biggest contour and extract the ultrasound image extreme points to form the cropped images &#8220;<inline-formula id=\"IEq10\"><tex-math id=\"d33e979\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{CropImgPCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; as shown in Eq.&#160;(<xref rid=\"Equ1\" ref-type=\"disp-formula\">4</xref>) to Eq.&#160;(<xref rid=\"Equ2\" ref-type=\"disp-formula\">5</xref>)<disp-formula id=\"Equ1\"><label>4</label><tex-math id=\"d33e989\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Contour=maxcontour\\left({{PCOS}_{00}}_{1}\\right)$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ2\"><label>5</label><tex-math id=\"d33e993\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{CropImgPCOS}_{00}}_{1}=Extremepoints\\left(Contour\\right)\\:\\:$$\\end{document}</tex-math></disp-formula></p><p id=\"Par38\">The cropped images &#8220;<inline-formula id=\"IEq11\"><tex-math id=\"d33e999\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{CropImgPCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; are processed with data augmentation to form PCOS augmented images &#8220;<inline-formula id=\"IEq12\"><tex-math id=\"d33e1003\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{AugPCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; resulting with 53,200 images. The data augmentation was performed using horizontal flipping from the Eqs.&#160;(<xref rid=\"Equ3\" ref-type=\"disp-formula\">6</xref>) to (<xref rid=\"Equ4\" ref-type=\"disp-formula\">7</xref>). Here, &#8220;<inline-formula id=\"IEq13\"><tex-math id=\"d33e1013\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:CropImgPCOS$$\\end{document}</tex-math></inline-formula>&#8221; denotes cropped image patch showing the horizontally cropped ROI portion from the main PCOS dataset image, &#8220;<inline-formula id=\"IEq14\"><tex-math id=\"d33e1018\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Horizontal\\:Flip$$\\end{document}</tex-math></inline-formula>&#8221; denotes the horizontal transformation matrix that performs horizontal mirroring. The variables &#8220;<inline-formula id=\"IEq15\"><tex-math id=\"d33e1022\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e,\\:d$$\\end{document}</tex-math></inline-formula>&#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before horizontal flipping transformation. The &#8220;<inline-formula id=\"IEq16\"><tex-math id=\"d33e1026\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[c;e;1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes original homogeneous coordinate vector having the column vector form of the pixel coordinate before horizontal flipping. The &#8220;<inline-formula id=\"IEq17\"><tex-math id=\"d33e1030\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[c{\\prime\\:};e{\\prime\\:};1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes the transformed coordinate vector representing the pixel coordinates after horizontal flipping.<disp-formula id=\"Equ3\"><label>6</label><tex-math id=\"d33e1034\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}c\\\\\\:\\begin{array}{c}e\\\\\\:1\\end{array}\\end{array}\\right]=Horizontal\\:Flip\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}c\\:{\\prime\\:}\\\\\\:\\begin{array}{c}e\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ4\"><label>7</label><tex-math id=\"d33e1038\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Horizontal\\;Flip = \\left[ {\\begin{array}{*{20}c} {c^{\\prime } } \\\\ {\\begin{array}{*{20}c} {e^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} { - 1} \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 1 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} c \\\\ {\\begin{array}{*{20}c} e \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par39\">The vertical flipping operation is shown from Eqs.&#160;(<xref rid=\"Equ5\" ref-type=\"disp-formula\">8</xref>) to <xref rid=\"Equ6\" ref-type=\"disp-formula\">9</xref>). Here, &#8220;<inline-formula id=\"IEq18\"><tex-math id=\"d33e1050\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:CropImgPCOS$$\\end{document}</tex-math></inline-formula>&#8221; denotes cropped image patch showing the vertical cropped ROI portion from the main PCOS dataset image, &#8220;<inline-formula id=\"IEq19\"><tex-math id=\"d33e1054\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Vertical\\:Flip$$\\end{document}</tex-math></inline-formula>&#8221; denotes the vertical transformation matrix that performs vertical mirroring. The variables &#8220;<inline-formula id=\"IEq20\"><tex-math id=\"d33e1058\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f,\\:g$$\\end{document}</tex-math></inline-formula>&#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before vertical flipping transformation. The &#8220;<inline-formula id=\"IEq21\"><tex-math id=\"d33e1063\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[f;g;1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes original homogeneous coordinate vector having the column vector form of the pixel coordinate before vertical flipping. The &#8220;<inline-formula id=\"IEq22\"><tex-math id=\"d33e1067\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[f{\\prime\\:};g{\\prime\\:};1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes the transformed coordinate vector representing the pixel coordinates after vertical flipping.<disp-formula id=\"Equ5\"><label>8</label><tex-math id=\"d33e1071\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}f\\\\\\:\\begin{array}{c}g\\\\\\:1\\end{array}\\end{array}\\right]=Vertical\\:Flip\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}f\\:{\\prime\\:}\\\\\\:\\begin{array}{c}g\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ6\"><label>9</label><tex-math id=\"d33e1075\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Vertical\\;Flip = \\left[ {\\begin{array}{*{20}c} {f^{\\prime } } \\\\ {\\begin{array}{*{20}c} {g^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} 1 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} { - 1} \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} f \\\\ {\\begin{array}{*{20}c} g \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par40\">The rotation operation is shown in the equation Eq.&#160;(<xref rid=\"Equ7\" ref-type=\"disp-formula\">10</xref>) to (11). Here, &#8220;<inline-formula id=\"IEq23\"><tex-math id=\"d33e1084\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:CropImgPCOS$$\\end{document}</tex-math></inline-formula>&#8221; denotes cropped image patch showing the rotated portion from the main PCOS dataset image, &#8220;<inline-formula id=\"IEq24\"><tex-math id=\"d33e1088\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Rotation$$\\end{document}</tex-math></inline-formula>&#8221; denotes the rotation transformation matrix that applies geometric rotation in homogeneous space. The variables &#8220;<inline-formula id=\"IEq25\"><tex-math id=\"d33e1092\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$m,\\:n$$\\end{document}</tex-math></inline-formula>&#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before rotation transformation. The &#8220;<inline-formula id=\"IEq26\"><tex-math id=\"d33e1096\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[m;n;1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes original homogeneous coordinate vector of the pixel coordinate before rotation. The &#8220;<inline-formula id=\"IEq27\"><tex-math id=\"d33e1101\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[m{\\prime\\:};n{\\prime\\:};1]$$\\end{document}</tex-math></inline-formula>&#8221; denotes the transformed coordinate vector representing the pixel coordinates after rotation.<disp-formula id=\"Equ7\"><label>10</label><tex-math id=\"d33e1105\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}m\\\\\\:\\begin{array}{c}n\\\\\\:1\\end{array}\\end{array}\\right]=Rotation\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}m\\:{\\prime\\:}\\\\\\:\\begin{array}{c}n\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par41\"><disp-formula id=\"Equ789\"><label>11</label><tex-math id=\"d33e1110\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Rotation = \\left[ {\\begin{array}{*{20}c} {m^{\\prime } } \\\\ {\\begin{array}{*{20}c} {n^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} {\\cos \\theta } \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} { - \\sin \\theta } \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} {\\sin \\theta } \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} {\\cos \\theta } \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} m \\\\ {\\begin{array}{*{20}c} n \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document}</tex-math></disp-formula>The final resultant data augmentation results \"<inline-formula id=\"IEq29\"><tex-math id=\"d33e1115\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$AugPCOS$$\\end{document}</tex-math></inline-formula> \" are obtained as depicted in Eqs.&#160;(<xref rid=\"Equ8\" ref-type=\"disp-formula\">12</xref>) to (<xref rid=\"Equ11\" ref-type=\"disp-formula\">15</xref>), with &#8220;<inline-formula id=\"IEq30\"><tex-math id=\"d33e1125\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:HFPCOS$$\\end{document}</tex-math></inline-formula>&#8221; denoting the horizontally flipped image, denoting &#8220;<inline-formula id=\"IEq31\"><tex-math id=\"d33e1129\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:VFPCOS$$\\end{document}</tex-math></inline-formula>&#8221; the vertically flipped image and &#8220;<inline-formula id=\"IEq32\"><tex-math id=\"d33e1134\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:RPCOS$$\\end{document}</tex-math></inline-formula>&#8221; denoting the rotated image.</p><p id=\"Par42\">\n<disp-formula id=\"Equ8\"><label>12</label><tex-math id=\"d33e1140\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:HFPCOS=Horizontal\\:Flip\\left(CropImgPCOS\\right)\\:$$\\end{document}</tex-math></disp-formula>\n<disp-formula id=\"Equ9\"><label>13</label><tex-math id=\"d33e1145\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:VFPCOS=Vertical\\:Flip\\left(CropImgPCOS\\right)\\:$$\\end{document}</tex-math></disp-formula>\n<disp-formula id=\"Equ10\"><label>14</label><tex-math id=\"d33e1150\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:RPCOS=Rotation\\:\\left(CropImgPCOS\\right)\\:$$\\end{document}</tex-math></disp-formula>\n<disp-formula id=\"Equ11\"><label>15</label><tex-math id=\"d33e1155\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AugPCOS=\\bigcup\\:\\left\\{\\begin{array}{c}HFPCOS\\\\\\:VFPCOS\\\\\\:RPCOS\\end{array}\\right.$$\\end{document}</tex-math></disp-formula>\n</p><p id=\"Par43\">The data augmented images are processed with fuzzy contrast enhanced module.</p></sec><sec id=\"Sec7\"><title>Fuzzy contrast enhanced module modeling</title><p id=\"Par44\">The data augmented ultrasound images &#8220;<inline-formula id=\"IEq33\"><tex-math id=\"d33e1166\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AugPCOS$$\\end{document}</tex-math></inline-formula>&#8221; are processed to form the Histogram equalized images, CLAHE image and FCE images. The histogram equalized ultrasound image &#8220;<inline-formula id=\"IEq34\"><tex-math id=\"d33e1170\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{HistPCOS}_{00}}_{1}$$\\end{document}</tex-math></inline-formula>&#8221; was formed by applying cumulative distribution function &#8220;<inline-formula id=\"IEq35\"><tex-math id=\"d33e1174\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:cdf\\:$$\\end{document}</tex-math></inline-formula>&#8221; of each image pixel and histogram variance &#8220;<inline-formula id=\"IEq36\"><tex-math id=\"d33e1178\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Histogram$$\\end{document}</tex-math></inline-formula>&#8221; as in Eqs.&#160;(<xref rid=\"Equ12\" ref-type=\"disp-formula\">16</xref>) to (<xref rid=\"Equ13\" ref-type=\"disp-formula\">17</xref>) denoting &#8220;<inline-formula id=\"IEq37\"><tex-math id=\"d33e1189\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(x,y)$$\\end{document}</tex-math></inline-formula>&#8221; are the pixel co-ordinates of data augmented ultrasound image. Here <inline-formula id=\"IEq38\"><tex-math id=\"d33e1193\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\text{min}\\left(cdf\\right)$$\\end{document}</tex-math></inline-formula> denotes the smallest cumulative probability value used for normalization. The &#8220;<inline-formula id=\"IEq39\"><tex-math id=\"d33e1197\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Width\\left(dots\\right)$$\\end{document}</tex-math></inline-formula>&#8221; denotes the number of pixels in the horizontal dimension of the image. The &#8220;<inline-formula id=\"IEq40\"><tex-math id=\"d33e1201\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Height\\left(dots\\right)$$\\end{document}</tex-math></inline-formula>&#8221; denotes the number of pixels in the vertical dimension of the image. The &#8220;<inline-formula id=\"IEq41\"><tex-math id=\"d33e1205\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$RC,\\:GC,\\:BC$$\\end{document}</tex-math></inline-formula>&#8221; denotes the red, green and blue channel intensity component of the augmented image.<disp-formula id=\"Equ12\"><label>16</label><tex-math id=\"d33e1210\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Histogram=round\\:\\left(\\frac{cdf\\left({{AugPCOS}_{00}}_{1}\\left(x,y\\right)-\\text{min}\\left(cdf\\right)\\right)}{Width\\left(dots\\right)\\:\\times\\:Height\\left(dots\\right)-\\text{m}\\text{i}\\text{n}\\left(cdf\\right)}\\times\\:\\left(RC+GC+BC\\right)-1\\right)$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ13\"><label>17</label><tex-math id=\"d33e1214\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{HistPCOS}_{00}}_{1}={{AugPCOS}_{00}}_{1}+Histogram$$\\end{document}</tex-math></disp-formula></p><p id=\"Par45\">As the data augmented ultrasound images &#8220;<inline-formula id=\"IEq42\"><tex-math id=\"d33e1220\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AugPCOS$$\\end{document}</tex-math></inline-formula>&#8221; is an array of pixel values which are denoted by the array of numbers as random dots. Suppose if &#8220;<inline-formula id=\"IEq43\"><tex-math id=\"d33e1224\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(x,y)$$\\end{document}</tex-math></inline-formula>&#8221; are the two random pixel variables and if they are exactly linearly correlated with constant &#8220;<inline-formula id=\"IEq44\"><tex-math id=\"d33e1228\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$c$$\\end{document}</tex-math></inline-formula>&#8221;, then it is shown in Eq.&#160;(<xref rid=\"Equ14\" ref-type=\"disp-formula\">18</xref>).<disp-formula id=\"Equ14\"><label>18</label><tex-math id=\"d33e1235\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{AugPCOS}_{00}}_{1}\\left(x\\right)=c{\\:\\times\\:{AugPCOS}_{00}}_{1}\\left(y\\right)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par46\">Now the probability density function (PDF) of the two random dots is denoted as Eq.&#160;(<xref rid=\"Equ15\" ref-type=\"disp-formula\">19</xref>) where &#8216;r&#8217; is the total number of roots of (31) which is equal to &#8216;1&#8217;. Here <inline-formula id=\"IEq45\"><tex-math id=\"d33e1244\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\frac{d\\left({y}_{j}\\right)}{d\\left(x\\right)}$$\\end{document}</tex-math></inline-formula> represents how intensity values &#8220;<inline-formula id=\"IEq46\"><tex-math id=\"d33e1248\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${y}_{j}$$\\end{document}</tex-math></inline-formula>&#8221; change with respect to &#8220;<inline-formula id=\"IEq47\"><tex-math id=\"d33e1252\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x$$\\end{document}</tex-math></inline-formula>&#8221; during transformation. <disp-formula id=\"Equ15\"><label>19</label><tex-math id=\"d33e1256\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$PDF\\left( {AugPCOS_{{001}} \\left( x \\right)} \\right) = \\sum\\nolimits_{{j = 1}}^{r} f \\left( {AugPCOS_{{001}} (y_{j} )} \\right)\\left| {\\frac{{d\\left( {y_{j} } \\right)}}{{d\\left( x \\right)}}} \\right|$$\\end{document}</tex-math></disp-formula></p><p id=\"Par47\">In CLAHE, the PDF inside the local region of the augmented ultrasound image was found and is denoted by Eq.&#160;(<xref rid=\"Equ16\" ref-type=\"disp-formula\">20</xref>), where &#8220;<inline-formula id=\"IEq48\"><tex-math id=\"d33e1265\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)$$\\end{document}</tex-math></inline-formula>&#8221; represents the contrast of the augmented ultrasound image by CLAHE and is substituted to &#8220;<inline-formula id=\"IEq49\"><tex-math id=\"d33e1269\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Q$$\\end{document}</tex-math></inline-formula>&#8221; as in Eq.&#160;(<xref rid=\"Equ17\" ref-type=\"disp-formula\">21</xref>).<disp-formula id=\"Equ16\"><label>20</label><tex-math id=\"d33e1276\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)\\ne\\:1$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ17\"><label>21</label><tex-math id=\"d33e1280\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)=Q$$\\end{document}</tex-math></disp-formula></p><p id=\"Par48\">The value of <inline-formula id=\"IEq50\"><tex-math id=\"d33e1286\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Q\"$$\\end{document}</tex-math></inline-formula> lies between 0 and 1. If <inline-formula id=\"IEq51\"><tex-math id=\"d33e1290\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Q=1\"$$\\end{document}</tex-math></inline-formula>, then the local histogram stretching is maximum, but in CLAHE the value of <inline-formula id=\"IEq52\"><tex-math id=\"d33e1294\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Q\"$$\\end{document}</tex-math></inline-formula> must be less than &#8216;1&#8217;, since the contrast stretching is limited. The value of <inline-formula id=\"IEq53\"><tex-math id=\"d33e1298\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Q\"$$\\end{document}</tex-math></inline-formula> is denoted in Eq.&#160;(<xref rid=\"Equ18\" ref-type=\"disp-formula\">22</xref>)<disp-formula id=\"Equ18\"><label>22</label><tex-math id=\"d33e1306\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{Q=\\:PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\left|\\frac{d\\left(y\\right)}{d\\left(x\\right)}\\right|LR$$\\end{document}</tex-math></disp-formula></p><p id=\"Par49\">Where<inline-formula id=\"IEq54\"><tex-math id=\"d33e1312\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\:\"PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\"$$\\end{document}</tex-math></inline-formula> is the PDF of the local region &#8220;<inline-formula id=\"IEq55\"><tex-math id=\"d33e1316\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:LR\\:$$\\end{document}</tex-math></inline-formula>&#8221; of original augmented ultrasound image and &#8220;<inline-formula id=\"IEq56\"><tex-math id=\"d33e1320\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:LR$$\\end{document}</tex-math></inline-formula>&#8221;, <inline-formula id=\"IEq57\"><tex-math id=\"d33e1324\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left|\\frac{d\\left(y\\right)}{d\\left(x\\right)}\\right|$$\\end{document}</tex-math></inline-formula> is the ratio <inline-formula id=\"IEq58\"><tex-math id=\"d33e1328\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:d\\left(y\\right),\\:d\\left(y\\right)$$\\end{document}</tex-math></inline-formula> of the image in that local region. By integrating on both sides of Eq.&#160;(<xref rid=\"Equ18\" ref-type=\"disp-formula\">22</xref>), we get the form as in Eq.&#160;(<xref rid=\"Equ19\" ref-type=\"disp-formula\">23</xref>). The transformation function of image contrast with CLAHE is given in Eq.&#160;(<xref rid=\"Equ20\" ref-type=\"disp-formula\">24</xref>) denoting <inline-formula id=\"IEq59\"><tex-math id=\"d33e1342\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"k\"$$\\end{document}</tex-math></inline-formula> as integral constant. The contrast enhanced image by CLAHE <inline-formula id=\"IEq60\"><tex-math id=\"d33e1346\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{\"CLAHEPCOS}_{00}}_{1}\"\\:$$\\end{document}</tex-math></inline-formula>is given in Eq.&#160;(<xref rid=\"Equ21\" ref-type=\"disp-formula\">25</xref>)<disp-formula id=\"Equ19\"><label>23</label><tex-math id=\"d33e1354\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Q*{\\int\\:}_{LR}^{x}dx=\\:{\\int\\:}_{LR}^{x}{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\:dy$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ20\"><label>24</label><tex-math id=\"d33e1358\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{{AugPCOS}_{00}}_{1}\\left(x\\right)}_{LR}\\:\\:=\\frac{1}{Q}*{\\int\\:}_{LR}^{x}{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\:dy+\\:k$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ21\"><label>25</label><tex-math id=\"d33e1362\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{CLAHEPCOS}_{00}}_{1}={{{AugPCOS}_{00}}_{1}\\left(x\\right)}_{LR}\\:\\:$$\\end{document}</tex-math></disp-formula></p><p id=\"Par50\">As the input augmented ultrasound images &#8220;<inline-formula id=\"IEq61\"><tex-math id=\"d33e1368\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AugPCOS\\:$$\\end{document}</tex-math></inline-formula>&#8221; is in the gray scale format of size <inline-formula id=\"IEq62\"><tex-math id=\"d33e1372\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"M\\:\\times\\:\\:N\"$$\\end{document}</tex-math></inline-formula> with &#8220;<inline-formula id=\"IEq63\"><tex-math id=\"d33e1376\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:GL\\:$$\\end{document}</tex-math></inline-formula>&#8221; gray levels with <inline-formula id=\"IEq64\"><tex-math id=\"d33e1380\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"gl=0,\\:\\text{1,2}\\dots\\:.L-1\"$$\\end{document}</tex-math></inline-formula>. The gray levels were defined as the group of fuzzy sets representing the membership pixel value to the image property as in Eq.&#160;(<xref rid=\"Equ22\" ref-type=\"disp-formula\">26</xref>) for the single augmented ultrasound image <inline-formula id=\"IEq65\"><tex-math id=\"d33e1388\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{{AugPCOS}_{00}}_{1}\"$$\\end{document}</tex-math></inline-formula>. The notation of the fuzzy sets <inline-formula id=\"IEq66\"><tex-math id=\"d33e1392\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\frac{{\\mu\\:}_{mn}}{{gl}_{mn}}\"$$\\end{document}</tex-math></inline-formula> represents the fuzzy membership of the <inline-formula id=\"IEq67\"><tex-math id=\"d33e1396\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"M\\:\\times\\:\\:N\"$$\\end{document}</tex-math></inline-formula> pixel.<disp-formula id=\"Equ22\"><label>26</label><tex-math id=\"d33e1400\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$AugPCOS_{{001}} = \\cup _{{m = 1}}^{M} \\cup _{{m = 1}}^{N} \\frac{{\\mu _{{mn}} }}{{gl_{{mn}} }}\\:\\:where\\:\\mu \\:_{{mn}} \\: \\in \\:[0,\\:1]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par51\">The fuzzy contrast image <inline-formula id=\"IEq68\"><tex-math id=\"d33e1406\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"FCE\\left(Y\\right)\"$$\\end{document}</tex-math></inline-formula> of the input augmented ultrasound images &#8220;<inline-formula id=\"IEq69\"><tex-math id=\"d33e1410\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AugPCOS$$\\end{document}</tex-math></inline-formula>&#8221; was formed by performing three processes as fuzzification <inline-formula id=\"IEq70\"><tex-math id=\"d33e1414\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\Phi\\:}\"\\:$$\\end{document}</tex-math></inline-formula>operation, Membership value <inline-formula id=\"IEq71\"><tex-math id=\"d33e1418\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\Gamma\\:}\"$$\\end{document}</tex-math></inline-formula> operation, and defuzzification<inline-formula id=\"IEq72\"><tex-math id=\"d33e1422\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{\\Psi\\:}\"$$\\end{document}</tex-math></inline-formula> operation as denoted by Eq.&#160;(<xref rid=\"Equ23\" ref-type=\"disp-formula\">27</xref>)<disp-formula id=\"Equ23\"><label>27</label><tex-math id=\"d33e1430\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FCE\\left(Y\\right)={\\Psi\\:}\\left(\\:{\\Gamma\\:}\\left(\\:{\\Phi\\:}\\left({{AugPCOS}_{00}}_{1}\\:\\right)\\right)\\right)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par52\">The modified gray levels<inline-formula id=\"IEq73\"><tex-math id=\"d33e1436\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\:\\:\"GL}^{{\\prime\\:}}\"$$\\end{document}</tex-math></inline-formula> of the augmented ultrasound image <inline-formula id=\"IEq74\"><tex-math id=\"d33e1440\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{{AugPCOS}_{00}}_{1}\"$$\\end{document}</tex-math></inline-formula> is computed as in Eq.&#160;(<xref rid=\"Equ24\" ref-type=\"disp-formula\">28</xref>).<disp-formula id=\"Equ24\"><label>28</label><tex-math id=\"d33e1447\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$GL^{\\prime } = f\\left( {AugPCOS_{{001}} } \\right) = (GL - 1)\\sum\\nolimits_{{x = 0}}^{{gl}} {\\frac{{h\\left( i \\right)}}{{MN}}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par53\">The gray level fuzzification <inline-formula id=\"IEq75\"><tex-math id=\"d33e1453\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\Phi\\:}\"\\:$$\\end{document}</tex-math></inline-formula>operation is performed on the image <inline-formula id=\"IEq76\"><tex-math id=\"d33e1457\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\mu\\:}_{mn}\"\\:$$\\end{document}</tex-math></inline-formula>membership value as in Eq.&#160;(<xref rid=\"Equ25\" ref-type=\"disp-formula\">29</xref>) denoting as exponential &#8220;<inline-formula id=\"IEq77\"><tex-math id=\"d33e1464\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FE$$\\end{document}</tex-math></inline-formula>&#8221; and denominational &#8220;<inline-formula id=\"IEq78\"><tex-math id=\"d33e1468\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FD$$\\end{document}</tex-math></inline-formula>&#8221; fuzzifiers that control the amount of grayness level in the FCE image.<disp-formula id=\"Equ25\"><label>29</label><tex-math id=\"d33e1473\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\mu\\:}_{mn}\\left(gl\\right)={\\left[1+\\frac{{gl}_{max}-\\:gl}{FD}\\:\\right]}^{-FE}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par54\">Now, the Membership value <inline-formula id=\"IEq79\"><tex-math id=\"d33e1480\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\Gamma\\:}\\left(gl\\right)\"$$\\end{document}</tex-math></inline-formula> operation on the image is performed as shown in Eq.&#160;(<xref rid=\"Equ26\" ref-type=\"disp-formula\">30</xref>)<disp-formula id=\"Equ26\"><label>30</label><tex-math id=\"d33e1487\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\Gamma\\:}\\left(gl\\right)=\\left\\{\\begin{array}{l}{2\\left[{\\mu\\:}_{mn}\\left(gl\\right)\\right]}^{2}\\\\\\:1-{2\\left[{\\mu\\:}_{mn}\\left(gl\\right)\\right]}^{2}\\end{array}\\right.\\begin{array}{c}\\:\\:\\:\\:\\:\\:\\:if\\:0\\le\\:{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:0.5\\:\\\\\\:\\:\\:\\:\\:\\:if\\:0.5&lt;{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:1\\end{array}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par55\">The defuzzification<inline-formula id=\"IEq80\"><tex-math id=\"d33e1493\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{\\Psi\\:}\"$$\\end{document}</tex-math></inline-formula> operation on the image is performed as shown in Eq.&#160;(<xref rid=\"Equ27\" ref-type=\"disp-formula\">31</xref>) and the FCE image was found from Eq.&#160;(<xref rid=\"Equ28\" ref-type=\"disp-formula\">32</xref>) and Eq.&#160;(<xref rid=\"Equ29\" ref-type=\"disp-formula\">33</xref>).<disp-formula id=\"Equ27\"><label>31</label><tex-math id=\"d33e1506\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\Psi\\:}\\left(gl\\right)=\\left\\{\\begin{array}{c}n-\\left(n-{gl}_{\\text{m}\\text{i}\\text{n}}\\right)(1-2{\\mu\\:}_{mn}\\left(gl\\right))\\\\\\:n+\\left({gl}_{\\text{m}\\text{a}\\text{x}}-n\\right)(2{\\mu\\:}_{mn}\\left(gl\\right)-1)\\end{array}\\right.\\begin{array}{c}\\:\\:\\:\\:\\:\\:\\:if\\:0\\le\\:{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:0.5\\:\\\\\\:\\:\\:\\:\\:\\:if\\:0.5&lt;{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:1\\end{array}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ28\"><label>32</label><tex-math id=\"d33e1510\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{{FCEPCOS}_{00}}_{1}\\left(x,y\\right)=\\:FCE\\left({{AugPCOS}_{00}}_{1}\\right)$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ29\"><label>33</label><tex-math id=\"d33e1515\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FCE\\left({{AugPCOS}_{00}}_{1}\\right)={\\Psi\\:}\\left(\\:{\\Gamma\\:}\\left(\\:{\\Phi\\:}\\left({{{{\\mu\\:}_{mn}(AugPCOS}_{00}}_{1})}_{gl}\\:\\right)\\right)\\right)\\:$$\\end{document}</tex-math></disp-formula></p><p id=\"Par56\">The brightness and Pixel Intensity of the obtained Histogram equalized images, CLAHE image and FCE images are validated to select high pixel intensity image. The pixel intensity <inline-formula id=\"IEq81\"><tex-math id=\"d33e1521\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Int(x,y)\"\\:$$\\end{document}</tex-math></inline-formula>was validated by finding the scene transmission <inline-formula id=\"IEq82\"><tex-math id=\"d33e1525\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"t(x,y)\"\\:$$\\end{document}</tex-math></inline-formula>distant dependent factor of the images. The scene radiance <inline-formula id=\"IEq83\"><tex-math id=\"d33e1529\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"SR(x,\\lambda\\:)\"\\:$$\\end{document}</tex-math></inline-formula>was also estimated denoting<inline-formula id=\"IEq84\"><tex-math id=\"d33e1533\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\lambda\\:\"\\:$$\\end{document}</tex-math></inline-formula>as transmission coefficient. The validation of pixel intensity for histogram equalized image <inline-formula id=\"IEq85\"><tex-math id=\"d33e1537\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"IntHist\\left(x,y\\right)\"\\:$$\\end{document}</tex-math></inline-formula>was done as in Eq.&#160;(<xref rid=\"Equ30\" ref-type=\"disp-formula\">34</xref>) to Eq.&#160;(<xref rid=\"Equ31\" ref-type=\"disp-formula\">35</xref>) with<inline-formula id=\"IEq86\"><tex-math id=\"d33e1548\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\beta\\:\"$$\\end{document}</tex-math></inline-formula> representing as color density, <inline-formula id=\"IEq87\"><tex-math id=\"d33e1552\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"d\"$$\\end{document}</tex-math></inline-formula> distant dependent constant factor and<inline-formula id=\"IEq88\"><tex-math id=\"d33e1556\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\:\"L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\"$$\\end{document}</tex-math></inline-formula> as scattering coefficient.<disp-formula id=\"Equ30\"><label>34</label><tex-math id=\"d33e1560\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t\\left({{HistPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ31\"><label>35</label><tex-math id=\"d33e1565\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:IntHist\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{HistPCOS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par57\">The validation of pixel intensity for CLAHE image <inline-formula id=\"IEq89\"><tex-math id=\"d33e1571\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"IntCLAHE\\left(x,y\\right)\"\\:$$\\end{document}</tex-math></inline-formula>was done as in Eq.&#160;(<xref rid=\"Equ32\" ref-type=\"disp-formula\">36</xref>) to Eq.&#160;(<xref rid=\"Equ33\" ref-type=\"disp-formula\">37</xref>) with<inline-formula id=\"IEq90\"><tex-math id=\"d33e1581\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"\\beta\\:\"$$\\end{document}</tex-math></inline-formula> representing as color density, <inline-formula id=\"IEq91\"><tex-math id=\"d33e1585\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"d\"$$\\end{document}</tex-math></inline-formula> distant dependent constant factor and<inline-formula id=\"IEq92\"><tex-math id=\"d33e1590\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\:\"L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\"$$\\end{document}</tex-math></inline-formula> as scattering coefficient.<disp-formula id=\"Equ32\"><label>36</label><tex-math id=\"d33e1594\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t\\left({{CLAHEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ33\"><label>37</label><tex-math id=\"d33e1598\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:IntCLAHE\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{CLAHEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par58\">The validation of pixel intensity for FCE image<inline-formula id=\"IEq93\"><tex-math id=\"d33e1604\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"IntFCE\\left(x,y\\right)\"$$\\end{document}</tex-math></inline-formula> was done in Eq.&#160;(<xref rid=\"Equ34\" ref-type=\"disp-formula\">38</xref>) to Eq.&#160;(<xref rid=\"Equ35\" ref-type=\"disp-formula\">39</xref>)<disp-formula id=\"Equ34\"><label>38</label><tex-math id=\"d33e1614\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:t\\left({{FCEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ35\"><label>39</label><tex-math id=\"d33e1618\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:IntFCE\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{HighGAS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par59\">The high pixel intensity image was selected based on comparing the obtained pixel intensity of Histogram equalized images, CLAHE image and FCE images as in Eq.&#160;(<xref rid=\"Equ36\" ref-type=\"disp-formula\">40</xref>). The pixel intensity of FCE was found to be high. The FCE images are applied to FCAU-Net module<disp-formula id=\"Equ36\"><label>40</label><tex-math id=\"d33e1627\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:HighPixel=max\\left\\{\\begin{array}{l}IntHist\\left(x,y\\right)\\\\\\:IntCLAHE\\left(x,y\\right)\\\\\\:IntFCE\\left(x,y\\right)\\end{array}\\right.$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec8\"><title>Proposed FCAU-Net PCOS detection modeling</title><p id=\"Par60\">The fuzzy contrast enhanced image <inline-formula id=\"IEq94\"><tex-math id=\"d33e1635\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{{FCEPCOS}_{00}}_{1}\"\\:$$\\end{document}</tex-math></inline-formula>input data is processed with existing CNN models to select the best CNN model. Experiment results portray that Attention UNet offers the classification of PCOS with the accuracy above 80%. Now the Attention UNet was selected to integrate the feature fusion context module. The FCAU-Net consists of four encoder block and four decoder blocks accompanied with the Attention gate. The feature map produced by the convolution after each encoder is shown in Eq.&#160;(<xref rid=\"Equ37\" ref-type=\"disp-formula\">41</xref>) that defines the feature map <inline-formula id=\"IEq95\"><tex-math id=\"d33e1642\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FEMap}_{mn}\\:$$\\end{document}</tex-math></inline-formula>at position <inline-formula id=\"IEq96\"><tex-math id=\"d33e1646\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"(m,n)\"$$\\end{document}</tex-math></inline-formula>, which results from applying the convolution operator <inline-formula id=\"IEq97\"><tex-math id=\"d33e1650\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"Con\\left(i,j\\right)\"\\:$$\\end{document}</tex-math></inline-formula>between the image pixels &#8220;<inline-formula id=\"IEq98\"><tex-math id=\"d33e1655\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PIXx$$\\end{document}</tex-math></inline-formula>&#8221; and the convolution kernel &#8220;<inline-formula id=\"IEq99\"><tex-math id=\"d33e1659\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:KER$$\\end{document}</tex-math></inline-formula>&#8221;.<disp-formula id=\"Equ37\"><label>41</label><tex-math id=\"d33e1663\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FEMap}_{mn}=Con\\left(i,j\\right)={\\left(PIXx*KER\\right)}_{ij}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par61\">The convolution expansion operation is shown in Eq.&#160;(<xref rid=\"Equ38\" ref-type=\"disp-formula\">42</xref>) with <inline-formula id=\"IEq100\"><tex-math id=\"d33e1672\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{PIX}_{i-r,j-c}\"\\:$$\\end{document}</tex-math></inline-formula>denoting the pixel intensity from the input image at position <inline-formula id=\"IEq101\"><tex-math id=\"d33e1676\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left(i-r,j-c\\right)$$\\end{document}</tex-math></inline-formula> in the receptive field with <inline-formula id=\"IEq102\"><tex-math id=\"d33e1680\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(i,\\:j)$$\\end{document}</tex-math></inline-formula> as pixel coordinates. Here <inline-formula id=\"IEq103\"><tex-math id=\"d33e1684\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"KER}_{r,c}\"\\:$$\\end{document}</tex-math></inline-formula>denotes the kernel coefficient that is the weight at position <inline-formula id=\"IEq104\"><tex-math id=\"d33e1689\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:(r,c)$$\\end{document}</tex-math></inline-formula> in the convolution kernel.<disp-formula id=\"Equ38\"><label>42</label><tex-math id=\"d33e1693\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\left( {PIX*KER} \\right)_{{ij}} = \\sum\\nolimits_{{r = 1}}^{{255}} {\\sum\\nolimits_{{c = 1}}^{{255}} {PIX_{{i - r,j - c}} *KER_{{r,c}} } }$$\\end{document}</tex-math></disp-formula></p><p id=\"Par62\">After the convolution, the sigmoid function &#8220;<inline-formula id=\"IEq105\"><tex-math id=\"d33e1699\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Sig$$\\end{document}</tex-math></inline-formula>&#8221; was performed as in Eq.&#160;(<xref rid=\"Equ39\" ref-type=\"disp-formula\">43</xref>) where <inline-formula id=\"IEq106\"><tex-math id=\"d33e1706\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\sigma\\:\\left(.\\right)\"$$\\end{document}</tex-math></inline-formula> Is the Sigmoid function. Here <inline-formula id=\"IEq107\"><tex-math id=\"d33e1710\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"FEM}_{mn}\"\\:$$\\end{document}</tex-math></inline-formula>denotes the feature map intensity which is the raw convolution output before activation. The <inline-formula id=\"IEq108\"><tex-math id=\"d33e1714\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"e}^{-{FEM}_{mn}}\"$$\\end{document}</tex-math></inline-formula> denotes the exponential decay term that controls how steeply the sigmoid transitions between 0 and 1.<disp-formula id=\"Equ39\"><label>43</label><tex-math id=\"d33e1719\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Sig=\\sigma\\:\\left({FEM}_{mn}\\right)\\frac{1}{1+\\:{e}^{-{FEM}_{mn}}}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec9\"><title>Feature fusion context module modeling</title><p id=\"Par63\">The FFC module was integrated between the encoder and decoder block of the FCAU-Net. The FFC module extracts the position and Context information of the FM. The Context information of the FM was computed by finding the interdependencies between different channels and extracts attention scores that indicate the feature importance. The attention scores are normalized with SoftMax to form the attention weights distributions with probabilistic values to form Weighted FM. Assume &#8220;<inline-formula id=\"IEq109\"><tex-math id=\"d33e1727\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FEMap$$\\end{document}</tex-math></inline-formula>&#8221; is the obtained feature map which is shown in Eq.&#160;(<xref rid=\"Equ40\" ref-type=\"disp-formula\">44</xref>) with <inline-formula id=\"IEq110\"><tex-math id=\"d33e1734\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"C\\:\\times\\:\\:H\\:\\times\\:W\"$$\\end{document}</tex-math></inline-formula> denoting the channel, height and width of FM. The context attention scores &#8220;<inline-formula id=\"IEq111\"><tex-math id=\"d33e1738\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AS$$\\end{document}</tex-math></inline-formula>&#8221; of the feature map were formed in the format of channel attention map &#8220;<inline-formula id=\"IEq112\"><tex-math id=\"d33e1742\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:AM$$\\end{document}</tex-math></inline-formula>&#8221; as in Eq.&#160;(<xref rid=\"Equ41\" ref-type=\"disp-formula\">45</xref>).<disp-formula id=\"Equ40\"><label>44</label><tex-math id=\"d33e1750\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:AM={FEMap}^{C\\:\\times\\:\\:H\\:\\times\\:W}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ41\"><label>45</label><tex-math id=\"d33e1754\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{AS}_{ij}=\\frac{\\text{exp}\\left({AM}_{i}{AM}_{j}\\right)}{{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({AM}_{i}{AM}_{j}\\right)}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par64\">The attention scores <inline-formula id=\"IEq113\"><tex-math id=\"d33e1760\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"AS}_{ij}\"\\:$$\\end{document}</tex-math></inline-formula>are multiplied with the original FM to form the cumulative FM channel content. Now, the weighted FM was integrated with the original FM to regulate the FM strength. This validates that the model can learn the context details &#8220;<inline-formula id=\"IEq114\"><tex-math id=\"d33e1764\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:CD$$\\end{document}</tex-math></inline-formula>&#8221; of the FM with the attention distribution of the FM pixels with scale parameter <inline-formula id=\"IEq115\"><tex-math id=\"d33e1768\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\beta\\:\"$$\\end{document}</tex-math></inline-formula> as in Eq.&#160;(<xref rid=\"Equ42\" ref-type=\"disp-formula\">46</xref>).<disp-formula id=\"Equ42\"><label>46</label><tex-math id=\"d33e1775\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:CD=\\beta\\:{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({AS}_{ij}{AM}_{i}\\right)+{AM}_{j}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par65\">The position information of the FM was done by analysing the correlation between the position in the FM. The process starts by acquiring the spatial features from FM by applying convolution that results in 3D FM consisting of query, key, and value. By comparing the 3D FM features, the Energy Score Matrix (ESM) was computed. Assume &#8220;<inline-formula id=\"IEq116\"><tex-math id=\"d33e1781\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:FEMap$$\\end{document}</tex-math></inline-formula>&#8221; is obtained FM. The Position Attention Weights &#8220;<inline-formula id=\"IEq117\"><tex-math id=\"d33e1785\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PW$$\\end{document}</tex-math></inline-formula>&#8221; of the FM were formed in format of &#8220;<inline-formula id=\"IEq118\"><tex-math id=\"d33e1789\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:EM$$\\end{document}</tex-math></inline-formula>&#8221; as in Eq.&#160;(<xref rid=\"Equ43\" ref-type=\"disp-formula\">47</xref>) and Eq.&#160;(<xref rid=\"Equ44\" ref-type=\"disp-formula\">48</xref>)<disp-formula id=\"Equ43\"><label>47</label><tex-math id=\"d33e1800\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:EM={FEMap}^{C\\:\\times\\:\\:H\\:\\times\\:W}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ44\"><label>48</label><tex-math id=\"d33e1804\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PW}_{ij}=\\frac{\\text{exp}\\left({EM}_{i}{EM}_{j}\\right)}{{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({EM}_{i}{EM}_{j}\\right)}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par66\">The ESM represents the relative importance between FM pixel positions. The ESM are then normalized with softmax to form Position Attention Weights &#8220;<inline-formula id=\"IEq119\"><tex-math id=\"d33e1810\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PW$$\\end{document}</tex-math></inline-formula>&#8221; that shows the position information of FM. This validates that model can learn the position details &#8220;<inline-formula id=\"IEq120\"><tex-math id=\"d33e1814\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PD$$\\end{document}</tex-math></inline-formula>&#8221; of the FM with the Position Attention Weights &#8220;<inline-formula id=\"IEq121\"><tex-math id=\"d33e1818\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PW$$\\end{document}</tex-math></inline-formula>&#8221; of the FM pixels with scale parameter <inline-formula id=\"IEq122\"><tex-math id=\"d33e1822\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\beta\\:\"$$\\end{document}</tex-math></inline-formula> as in Eq.&#160;(<xref rid=\"Equ45\" ref-type=\"disp-formula\">49</xref>).<disp-formula id=\"Equ45\"><label>49</label><tex-math id=\"d33e1830\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:PD=\\beta\\:{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({PW}_{ij}{EM}_{i}\\right)+{EM}_{j}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par67\">The FFC module extracts the position and Context information of both the original and small-scale feature maps. First, the PAW of the original and small-scale feature maps is formed. The PAW of the original scale FM is processed with depth wise convolution using 3 kernels followed by batch normalization and single kernel convolution to form PAW of original scale FM. The PAW of original scale FM is denoted by <inline-formula id=\"IEq123\"><tex-math id=\"d33e1836\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMO}_{Pos}\"$$\\end{document}</tex-math></inline-formula> that denotes the &#8220;position block 1&#8221;. The SAW of the small-scale FM is processed with depth wise convolution using 3 kernels followed by batch normalization and single convolution with sigmoid activation function to form SAW of small-scale FM. The SAW of small-scale FM is denoted by <inline-formula id=\"IEq124\"><tex-math id=\"d33e1840\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMS}_{Con}\"$$\\end{document}</tex-math></inline-formula> that denotes the &#8220;context block 1&#8221;. The <inline-formula id=\"IEq125\"><tex-math id=\"d33e1844\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMO}_{Pos}\"$$\\end{document}</tex-math></inline-formula>and<inline-formula id=\"IEq126\"><tex-math id=\"d33e1848\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{Con}\"\\:$$\\end{document}</tex-math></inline-formula>is denoted as in Eq.&#160;(<xref rid=\"Equ46\" ref-type=\"disp-formula\">50</xref>) and Eq.&#160;(<xref rid=\"Equ47\" ref-type=\"disp-formula\">51</xref>). The operation performed on SAW of small-scale FM as in Eq.&#160;(<xref rid=\"Equ48\" ref-type=\"disp-formula\">52</xref>) with <inline-formula id=\"IEq127\"><tex-math id=\"d33e1862\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\text{FM}\"$$\\end{document}</tex-math></inline-formula> denoting FM.<disp-formula id=\"Equ46\"><label>50</label><tex-math id=\"d33e1866\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMO}_{Pos}\\in\\:{FM}^{H\\times\\:W\\times\\:C}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ47\"><label>51</label><tex-math id=\"d33e1870\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{Con}\\in\\:{FM}^{\\frac{H}{4}\\times\\:\\frac{W}{4}\\times\\:C}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ48\"><label>52</label><tex-math id=\"d33e1874\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FM_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} {DK_{{ij}}^{C} FMS_{{Con}}^{C} [m + i,n + j]} }$$\\end{document}</tex-math></disp-formula></p><p id=\"Par68\">The value <inline-formula id=\"IEq128\"><tex-math id=\"d33e1880\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FM}_{CV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> represents the context value of the <inline-formula id=\"IEq129\"><tex-math id=\"d33e1884\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{C}^{th}\"$$\\end{document}</tex-math></inline-formula> channel at position <inline-formula id=\"IEq130\"><tex-math id=\"d33e1888\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"[m,n]\"$$\\end{document}</tex-math></inline-formula>. The <inline-formula id=\"IEq131\"><tex-math id=\"d33e1892\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{DK}_{ij}^{C}$$\\end{document}</tex-math></inline-formula> denotes the depth wise kernel of the <inline-formula id=\"IEq132\"><tex-math id=\"d33e1896\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{C}^{th}\"$$\\end{document}</tex-math></inline-formula> channel at position <inline-formula id=\"IEq133\"><tex-math id=\"d33e1901\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"[i,j]\"$$\\end{document}</tex-math></inline-formula>. The <inline-formula id=\"IEq134\"><tex-math id=\"d33e1905\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMS}_{Con}^{C}\\left[m+i,n+j\\right]\"$$\\end{document}</tex-math></inline-formula> denotes the context value of FM of the <inline-formula id=\"IEq135\"><tex-math id=\"d33e1909\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{C}^{th}\"$$\\end{document}</tex-math></inline-formula> channel at position <inline-formula id=\"IEq136\"><tex-math id=\"d33e1913\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"[i,j]\"$$\\end{document}</tex-math></inline-formula>. Now the pointwise context convolution<inline-formula id=\"IEq137\"><tex-math id=\"d33e1917\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PC}_{j}\"$$\\end{document}</tex-math></inline-formula> is done for <inline-formula id=\"IEq138\"><tex-math id=\"d33e1921\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FM}_{CV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> for adjusting the number of channels as in Eq.&#160;(<xref rid=\"Equ49\" ref-type=\"disp-formula\">53</xref>)<disp-formula id=\"Equ49\"><label>53</label><tex-math id=\"d33e1929\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{FM}_{CV}^{C}[m,n,i]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par69\">The operation performed on PAW of original scale FM is formulated as in Eq.&#160;(<xref rid=\"Equ50\" ref-type=\"disp-formula\">54</xref>). The pointwise position convolution<inline-formula id=\"IEq139\"><tex-math id=\"d33e1938\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PP}_{j}\"$$\\end{document}</tex-math></inline-formula> is done for <inline-formula id=\"IEq140\"><tex-math id=\"d33e1942\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FM}_{PV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> for adjusting the number of channels as in Eq.&#160;(<xref rid=\"Equ51\" ref-type=\"disp-formula\">55</xref>). The value <inline-formula id=\"IEq141\"><tex-math id=\"d33e1949\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FM}_{PV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> represents the position value of the <inline-formula id=\"IEq142\"><tex-math id=\"d33e1954\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{C}^{th}\"$$\\end{document}</tex-math></inline-formula> channel at position <inline-formula id=\"IEq143\"><tex-math id=\"d33e1958\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"[m,n]\"$$\\end{document}</tex-math></inline-formula>. The <inline-formula id=\"IEq144\"><tex-math id=\"d33e1962\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMO}_{Pos}^{C}\\left[m+i,n+j\\right]\"$$\\end{document}</tex-math></inline-formula> denotes the position value of FM of the <inline-formula id=\"IEq145\"><tex-math id=\"d33e1966\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{C}^{th}\"$$\\end{document}</tex-math></inline-formula> channel at position <inline-formula id=\"IEq146\"><tex-math id=\"d33e1970\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"[i,j]\"$$\\end{document}</tex-math></inline-formula>.<disp-formula id=\"Equ50\"><label>54</label><tex-math id=\"d33e1974\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FM_{{PV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} {DK_{{ij}}^{C} FMO_{{Pos}}^{C} [m + i,n + j]} }$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ51\"><label>55</label><tex-math id=\"d33e1979\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PP}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{FM}_{PV}^{C}[m,n,i]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par70\">Now after obtaining the pointwise context convolution<inline-formula id=\"IEq147\"><tex-math id=\"d33e1985\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PC}_{j}\"$$\\end{document}</tex-math></inline-formula> of small-scale and pointwise position convolution<inline-formula id=\"IEq148\"><tex-math id=\"d33e1989\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PP}_{j}\"$$\\end{document}</tex-math></inline-formula> of original scale, both are subjected to batch normalization. Now let us consider the position block 1<inline-formula id=\"IEq149\"><tex-math id=\"d33e1993\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMO}_{PB1}\"$$\\end{document}</tex-math></inline-formula> as denoted in Eq.&#160;(<xref rid=\"Equ52\" ref-type=\"disp-formula\">56</xref>). The convolution with 1&#8201;&#215;&#8201;1 kernel was performed as denoted by Eq.&#160;(<xref rid=\"Equ53\" ref-type=\"disp-formula\">57</xref>). The value <inline-formula id=\"IEq150\"><tex-math id=\"d33e2004\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\gamma\\:$$\\end{document}</tex-math></inline-formula> denotes scaling factor, <inline-formula id=\"IEq151\"><tex-math id=\"d33e2008\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\mu\\:\\:$$\\end{document}</tex-math></inline-formula>denotes the mean of the position block 1, <inline-formula id=\"IEq152\"><tex-math id=\"d33e2012\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\sigma\\:}^{2}\\:$$\\end{document}</tex-math></inline-formula>denotes the variance of the position block 1 and <inline-formula id=\"IEq153\"><tex-math id=\"d33e2016\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\beta\\:$$\\end{document}</tex-math></inline-formula> denotes the offset value. The final FM obtained from the position block 1 is<inline-formula id=\"IEq154\"><tex-math id=\"d33e2020\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMO}_{conv\\_pos1}\"$$\\end{document}</tex-math></inline-formula><disp-formula id=\"Equ52\"><label>56</label><tex-math id=\"d33e2023\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMO_{{PB1}} = \\gamma \\frac{{PP_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ53\"><label>57</label><tex-math id=\"d33e2027\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMO_{{conv\\_pos1}} = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} D } K_{{ij}}^{C} FMO_{{PB1}} [i,j]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par71\">Now let us consider the context block 1<inline-formula id=\"IEq155\"><tex-math id=\"d33e2033\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{CB1}\"$$\\end{document}</tex-math></inline-formula> as denoted in Eq.&#160;(<xref rid=\"Equ54\" ref-type=\"disp-formula\">58</xref>). The convolution with 1&#8201;&#215;&#8201;1 kernel along with sigmoid activation function <inline-formula id=\"IEq156\"><tex-math id=\"d33e2040\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"\\sigma\\:\"$$\\end{document}</tex-math></inline-formula> was performed as denoted by Eq.&#160;(<xref rid=\"Equ55\" ref-type=\"disp-formula\">59</xref>) and Eq.&#160;(<xref rid=\"Equ56\" ref-type=\"disp-formula\">60</xref>). The final FM obtained from the context block 1 is<inline-formula id=\"IEq157\"><tex-math id=\"d33e2051\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{conv\\_con1}\"$$\\end{document}</tex-math></inline-formula>.<disp-formula id=\"Equ54\"><label>58</label><tex-math id=\"d33e2055\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMS_{{CB1}} = \\gamma \\frac{{PC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ55\"><label>59</label><tex-math id=\"d33e2059\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMS_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} {DK_{{ij}}^{C} FMS_{{CB1}} [i,j]} }$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ56\"><label>60</label><tex-math id=\"d33e2063\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{conv\\_con1}=\\sigma\\:\\left({FMS}_{conv}\\right)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par72\">The SAW of original scale is denoted by <inline-formula id=\"IEq158\"><tex-math id=\"d33e2069\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMO}_{Con}\"$$\\end{document}</tex-math></inline-formula> that denotes the &#8220;context block 2&#8221;. The <inline-formula id=\"IEq159\"><tex-math id=\"d33e2073\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMO}_{Con}\"$$\\end{document}</tex-math></inline-formula>and<inline-formula id=\"IEq160\"><tex-math id=\"d33e2077\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{Pos}\"\\:$$\\end{document}</tex-math></inline-formula>is denoted as in Eq.&#160;(<xref rid=\"Equ57\" ref-type=\"disp-formula\">61</xref>) and Eq.&#160;(<xref rid=\"Equ58\" ref-type=\"disp-formula\">62</xref>). The PAW of small-scale FM is denoted by <inline-formula id=\"IEq161\"><tex-math id=\"d33e2088\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FMS}_{Pos}\"$$\\end{document}</tex-math></inline-formula> that denotes the &#8220;position block 2&#8221;.<disp-formula id=\"Equ57\"><label>61</label><tex-math id=\"d33e2092\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMO}_{Con}\\in\\:{FM}^{H\\times\\:W\\times\\:C}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ58\"><label>62</label><tex-math id=\"d33e2096\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{Pos}\\in\\:{FM}^{\\frac{H}{4}\\times\\:\\frac{W}{4}\\times\\:C}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par73\">The SAW of the original scale FM is processed with convolution using 3 kernels as shown in Eq.&#160;(<xref rid=\"Equ59\" ref-type=\"disp-formula\">63</xref>). Now the pointwise context convolution<inline-formula id=\"IEq162\"><tex-math id=\"d33e2106\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PWC}_{j}\"$$\\end{document}</tex-math></inline-formula> is done for <inline-formula id=\"IEq163\"><tex-math id=\"d33e2110\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\text{S}\\text{A}\\text{W}}_{CV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> for adjusting the number of channels as in Eq.&#160;(<xref rid=\"Equ60\" ref-type=\"disp-formula\">64</xref>)<disp-formula id=\"Equ59\"><label>63</label><tex-math id=\"d33e2117\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$SAW_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{2} {\\sum\\nolimits_{{j = 1}}^{2} {\\sum\\nolimits_{{k = 1}}^{C} D } } K_{{k,j,i}}^{C} FMO_{{Con}}^{C} [m + i,n + j]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ60\"><label>64</label><tex-math id=\"d33e2121\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PWC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{\\text{S}\\text{A}\\text{W}}_{CV}^{C}[m,n,i]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par74\">Now after obtaining the pointwise context convolution<inline-formula id=\"IEq164\"><tex-math id=\"d33e2127\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PWC}_{j}\"$$\\end{document}</tex-math></inline-formula> of original scale of the &#8220;context block 2&#8221;, the batch normalization operation was performed. Now let us consider the context block 2<inline-formula id=\"IEq165\"><tex-math id=\"d33e2131\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMO}_{CB2}\"$$\\end{document}</tex-math></inline-formula> as denoted in Eq.&#160;(<xref rid=\"Equ61\" ref-type=\"disp-formula\">65</xref>) and Eq.&#160;(<xref rid=\"Equ62\" ref-type=\"disp-formula\">66</xref>). Then the obtained <inline-formula id=\"IEq166\"><tex-math id=\"d33e2141\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"FMO}_{conv}\"\\:$$\\end{document}</tex-math></inline-formula>was up sampled with sigmoid activation function to form SAW of original scale FM as in Eq.&#160;(<xref rid=\"Equ63\" ref-type=\"disp-formula\">67</xref>) and Eq.&#160;(<xref rid=\"Equ64\" ref-type=\"disp-formula\">68</xref>). The final FM obtained from the context block 2 is<inline-formula id=\"IEq167\"><tex-math id=\"d33e2152\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMO}_{conv\\_con2}\"$$\\end{document}</tex-math></inline-formula>.<disp-formula id=\"Equ61\"><label>65</label><tex-math id=\"d33e2156\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMO_{{CB2}} = \\gamma \\frac{{PWC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ62\"><label>66</label><tex-math id=\"d33e2160\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMO_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} D } K_{{ij}}^{C} FMO_{{CB2}} [i,j]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ63\"><label>67</label><tex-math id=\"d33e2164\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMUP}_{conv}=4*Upsample\\left({FMO}_{conv}\\right)$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ64\"><label>68</label><tex-math id=\"d33e2168\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMO}_{conv\\_con2}=\\sigma\\:\\left({FMUP}_{conv}\\right)$$\\end{document}</tex-math></disp-formula></p><p id=\"Par75\">The PAW of the small-scale FM<inline-formula id=\"IEq168\"><tex-math id=\"d33e2174\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{Pos}\"$$\\end{document}</tex-math></inline-formula> is processed with convolution using 3 kernels and 2 strides as in Eq.&#160;(<xref rid=\"Equ65\" ref-type=\"disp-formula\">69</xref>). Now the pointwise position convolution<inline-formula id=\"IEq169\"><tex-math id=\"d33e2181\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PPC}_{j}\"$$\\end{document}</tex-math></inline-formula> is done for <inline-formula id=\"IEq170\"><tex-math id=\"d33e2185\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{\\text{P}\\text{A}\\text{W}}_{CV}^{C}[m,n]\"$$\\end{document}</tex-math></inline-formula> for adjusting the number of channels as in Eq.&#160;(<xref rid=\"Equ66\" ref-type=\"disp-formula\">70</xref>)<disp-formula id=\"Equ65\"><label>69</label><tex-math id=\"d33e2193\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$PAW_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{2} {\\sum\\nolimits_{{j = 1}}^{2} {\\sum\\nolimits_{{k = 1}}^{C} D } } K_{{k,j,i}}^{C} FMS_{{Pos}}^{C} [m*(2 - 1) + i,n*(2 - 1) + j]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ66\"><label>70</label><tex-math id=\"d33e2197\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{PPC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{\\text{P}\\text{A}\\text{W}}_{CV}^{C}[m,n,i]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par76\">Now after obtaining the pointwise position convolution<inline-formula id=\"IEq171\"><tex-math id=\"d33e2203\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{PPC}_{j}\"$$\\end{document}</tex-math></inline-formula> of small-scale of the &#8220;position block 2&#8221;, the batch normalization operation was performed. Now let us consider the position block 2<inline-formula id=\"IEq172\"><tex-math id=\"d33e2207\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{PB2}\"$$\\end{document}</tex-math></inline-formula> as denoted in Eqs.&#160;(<xref rid=\"Equ67\" ref-type=\"disp-formula\">71</xref>) and (<xref rid=\"Equ68\" ref-type=\"disp-formula\">72</xref>).<disp-formula id=\"Equ67\"><label>71</label><tex-math id=\"d33e2217\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMS_{{PB2}} = \\gamma \\frac{{PPC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ68\"><label>72</label><tex-math id=\"d33e2221\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FMS_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} D } K_{{ij}}^{C} FMS_{{PB2}} [i,j]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par77\">Then the obtained <inline-formula id=\"IEq173\"><tex-math id=\"d33e2227\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"FMS}_{conv}\"\\:$$\\end{document}</tex-math></inline-formula>was performed with average pooling <inline-formula id=\"IEq174\"><tex-math id=\"d33e2231\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"{FM}_{AP}[m,n]\"\\:$$\\end{document}</tex-math></inline-formula>using 3 kernels and 2 strides to form PAW of small-scale FM as in Eq.&#160;(<xref rid=\"Equ69\" ref-type=\"disp-formula\">73</xref>). The final small-scale FM obtained from the position block 2 is<inline-formula id=\"IEq175\"><tex-math id=\"d33e2238\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{FMS}_{conv\\_pos2}\"$$\\end{document}</tex-math></inline-formula> as in Eq.&#160;(<xref rid=\"Equ70\" ref-type=\"disp-formula\">74</xref>).<disp-formula id=\"Equ69\"><label>73</label><tex-math id=\"d33e2246\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FM_{{AP}} [m,n] = \\frac{1}{9}\\sum\\nolimits_{{i = 1}}^{3} {\\sum\\nolimits_{{j = 1}}^{3} F } MS_{{conv}} [m*2 + i,\\:n*2 + j]$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ70\"><label>74</label><tex-math id=\"d33e2250\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{conv\\_pos2}=\\:{FM}_{AP}[m,n]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par78\">The final FM of original scale and small scale of position block 1, context block 1, context block and position block 2 is shown in Eq.&#160;(<xref rid=\"Equ71\" ref-type=\"disp-formula\">75</xref>) to Eq.&#160;(<xref rid=\"Equ74\" ref-type=\"disp-formula\">78</xref>)<disp-formula id=\"Equ71\"><label>75</label><tex-math id=\"d33e2262\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMO}_{conv\\_pos1}=Optimized\\:Original\\:Scale\\:position\\:FM\\:of\\:position\\:block\\:1\\:$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ72\"><label>76</label><tex-math id=\"d33e2266\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{conv\\_con1}\\:=Optimized\\:Small\\:Scale\\:Context\\:FM\\:of\\:context\\:block\\:1\\:$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ73\"><label>77</label><tex-math id=\"d33e2270\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMO}_{conv\\_con2}=Optimized\\:Original\\:Scale\\:Context\\:FM\\:of\\:context\\:block\\:2\\:$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ74\"><label>78</label><tex-math id=\"d33e2274\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FMS}_{conv\\_pos2}=Optimized\\:Small\\:Scale\\:\\:position\\:FM\\:of\\:position\\:block\\:2\\:$$\\end{document}</tex-math></disp-formula></p><p id=\"Par79\">The output feature map of original scale is <inline-formula id=\"IEq177\"><tex-math id=\"d33e2280\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{OS\\_Out}\\:$$\\end{document}</tex-math></inline-formula>denoted in Eq.&#160;(<xref rid=\"Equ75\" ref-type=\"disp-formula\">79</xref>) and the output feature map of small-scale is <inline-formula id=\"IEq178\"><tex-math id=\"d33e2287\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{SS\\_Out}$$\\end{document}</tex-math></inline-formula> denoted in Eq.&#160;(80).<disp-formula id=\"Equ75\"><label>79</label><tex-math id=\"d33e2291\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{OS\\_Out}={FMO}_{conv\\_pos1}\\times\\:{FMO}_{conv\\_con2}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par80\"><inline-formula id=\"IEq179\"><tex-math id=\"d33e2296\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{SS\\_Out}={FMS}_{conv\\_con1}\\times\\:{FMS}_{conv\\_pos2}$$\\end{document}</tex-math></inline-formula> (80) The optimized fused feature map is <inline-formula id=\"IEq180\"><tex-math id=\"d33e2300\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\"OFM}_{Out}\"$$\\end{document}</tex-math></inline-formula> obtained by combining the <inline-formula id=\"IEq181\"><tex-math id=\"d33e2304\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{OS\\_Out}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq182\"><tex-math id=\"d33e2308\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{FM}_{SS\\_Out}$$\\end{document}</tex-math></inline-formula> as in Eq.&#160;(<xref rid=\"Equ76\" ref-type=\"disp-formula\">81</xref>). The &#8220;<inline-formula id=\"IEq183\"><tex-math id=\"d33e2315\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:Upsample$$\\end{document}</tex-math></inline-formula>&#8221; operation resizes the lower-resolution FM <inline-formula id=\"IEq184\"><tex-math id=\"d33e2320\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:F{M}_{\\left(S{S}_{Out}\\right)}$$\\end{document}</tex-math></inline-formula>to match the dimensions of <inline-formula id=\"IEq185\"><tex-math id=\"d33e2324\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:F{M}_{\\left(O{S}_{Out}\\right)}$$\\end{document}</tex-math></inline-formula>, and the scaling factor (&#215;4) ensures the intensities or channel magnitudes are balanced before fusion. The addition <inline-formula id=\"IEq186\"><tex-math id=\"d33e2328\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\"+\"$$\\end{document}</tex-math></inline-formula> operation merges these two maps to form the final output feature map<inline-formula id=\"IEq187\"><tex-math id=\"d33e2332\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\:\"{OFM}_{Out\\:}\"$$\\end{document}</tex-math></inline-formula><disp-formula id=\"Equ76\"><label>81</label><tex-math id=\"d33e2335\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{OFM}_{Out}={FM}_{OS\\_Out}+4*Upsample\\left({FM}_{SS\\_Out}\\right)$$\\end{document}</tex-math></disp-formula></p></sec></sec><sec id=\"Sec10\"><title>Experimental setup and result analysis</title><p id=\"Par81\">The FCAU-Net initiates by collecting have 3800 ultrasound ovary images of PCOS Ultrasound (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images\">https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images</ext-link>) dataset for classifying the PCOS infected and healthy class type. The sample normal and PCOS infected images from the dataset are shown in the Fig. <xref rid=\"Fig8\" ref-type=\"fig\">8</xref>. The implementation was carried out in python by using keras, tensorflow, pandas, numpy, algorithms, utils, skimage, neupy, matplotlib and Theano library. The PCOS Ultrasound from dataset are segregated based on the disease class. The segregated PCOS Ultrasound are performed with labeling and the results are shown in Fig. <xref rid=\"Fig9\" ref-type=\"fig\">9</xref>.</p><p id=\"Par82\">\n<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Sample PCOS Ultrasound Dataset Images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2361\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig8_HTML.jpg\"/></fig>\n</p><p id=\"Par83\">\n<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>Results of Labeled PCOS Ultrasound Dataset Images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2371\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig9_HTML.jpg\"/></fig>\n</p><p id=\"Par84\">After labeling, the image cropping was done to form the cropped image. The results of the PCOS Ultrasound images before and after Image cropping are shown in Fig. <xref rid=\"Fig10\" ref-type=\"fig\">10</xref>. The cropped PCOS Ultrasound images are subjected to data augmentation to form 14 images for each image resulting with 53,200 images data augmented cropped images. The results obtained from the data augmented PCOS infected and normal healthy ultrasound images are shown in Figs.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref> and <xref rid=\"Fig12\" ref-type=\"fig\">12</xref> respectively.</p><p id=\"Par85\">\n<fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>Step by step breakdown of cropping the image.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2392\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig10_HTML.jpg\"/></fig>\n</p><p id=\"Par86\">\n<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>Results of PCOS infected Data Augmentation Images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2402\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig11_HTML.jpg\"/></fig>\n</p><p id=\"Par87\">\n<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p>Results of Normal healthy ultrasound Data Augmentation Images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2412\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig12_HTML.jpg\"/></fig>\n</p><p id=\"Par88\">The original dataset consisted of 3,800 ultrasound images with 1,900 healthy and 1,900 PCOS-infected. The testing dataset was formed with 80: 20 to extract 360 testing images. Through augmentation techniques, the dataset was expanded to 42,560 images, resulting in a total of 45,600 images. Before performing the augmentation process, the dataset was initially divided into separate subsets for training and testing in the ratio of 80:20 to ensure unbiased model evaluation. Specifically, the testing dataset was formed exclusively from the original, unaltered ultrasound 360 images prior to any augmentation procedures. This approach guarantees that the test data remains completely independent from the augmented samples used during model training, thereby preserving the integrity of performance assessment. Once the testing set was isolated, data augmentation was applied only to the training subset to artificially expand the number and diversity of training samples. The augmentation operations were implemented to simulate realistic variations in ultrasound imaging conditions. By using augmented data solely for training, the FCAU-Net model benefits from improved generalization and robustness to image variability, while the testing phases are conducted strictly on original data to ensure an accurate reflection of the model&#8217;s real-world diagnostic performance. This methodological separation between augmentation and testing provides a clear, reliable framework for evaluating model effectiveness without data leakage or overfitting bias. Table&#160;<xref rid=\"Tab3\" ref-type=\"table\">3</xref> summarizes the distribution of PCOS classes across the augmented datasets. The large augmented dataset not only enhances the model&#8217;s generalization capability but also reduces the risk of overfitting, ensuring that the proposed FCAU-Net can robustly distinguish PCOS from healthy cases.</p><p id=\"Par89\">\n<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Dataset distribution for FCAU-Net training, validation and testing.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"/><th align=\"left\" colspan=\"7\" rowspan=\"1\">Data distribution</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Actual</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Testing</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Actual</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Augmentation</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Total</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Train</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1,900</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">380</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1,520</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">21,280</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">22,800</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">18,240</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4560</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS infected</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1,900</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">380</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1,520</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">21,280</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">22,800</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">18,240</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4560</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Total</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3,800</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">760</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3040</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">42,560</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">45,600</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">36,480</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9120</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par90\">The brightness estimation for Histogram equalized images, CLAHE images and FCE images was done and its analysis is shown in Table. <xref rid=\"Tab4\" ref-type=\"table\">4</xref>. The brightness was analyzed for some sample images from the PCOS ultrasound image dataset. From the Table. <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, it is evident that the brightness value of FCE images was found to be high compared to other two methods. From the Table. <xref rid=\"Tab4\" ref-type=\"table\">4</xref>, it is evident that while HE and CLAHE produce only slight improvements in brightness typically within 1 to 3% of each other, the FCE method significantly outperforms both, achieving brightness values consistently above 94%. This sharp increase indicates that FCE not only enhances the global contrast but also preserves finer structural details, making ovarian features more distinguishable. Importantly, both PCOS-infected and healthy images benefit equally from FCE, showing a consistent improvement trend, which implies that the technique is effective. Such a substantial enhancement in brightness directly supports improved feature visibility, thereby facilitating better feature extraction and classification performance.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4.</label><caption><p>Brightness analysis of HE, CLAHE and FCE images.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">S. NO</th><th align=\"left\" rowspan=\"2\" colspan=\"1\">Sample Image</th><th align=\"left\" colspan=\"3\" rowspan=\"1\">Brightness Value (%)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">HE Images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">CLAHE Images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE Images</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS Infected Image 1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">61.43</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">64.44</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.43</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS Infected Image 2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">62.33</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63.32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">94.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS Infected Image 3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63.63</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65.61</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS Infected Image 4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">67.22</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.21</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.22</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS Infected Image 5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65.66</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">66.34</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.66</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy ultrasound Image 1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">66.34</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">67.32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.34</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy ultrasound Image 2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">61.55</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">64.22</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.55</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy ultrasound Image 3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">62.35</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63.44</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.35</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy ultrasound Image 4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63.45</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65.46</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.45</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\"/><td align=\"left\" colspan=\"1\" rowspan=\"1\">Healthy ultrasound Image 5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">66.23</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.52</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.23</td></tr></tbody></table></table-wrap></p><p id=\"Par91\">The Data augmentation images are subjected to form the Histogram equalized images, CLAHE images and FCE images and the obtained results are shown in Fig.&#160;<xref rid=\"Fig13\" ref-type=\"fig\">13</xref>.</p><p id=\"Par92\">\n<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig. 13</label><caption><p>Results of Traditional HE, CLAHE and FCE images of both classes.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2662\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig13_HTML.jpg\"/></fig>\n</p><p id=\"Par93\">The training images was fitted with proposed FCAU-Net and tested with the existing CNN to analyze the performance and is shown in Table&#160;<xref rid=\"Tab5\" ref-type=\"table\">5</xref>; Fig. <xref rid=\"Fig14\" ref-type=\"fig\">14</xref>. It is observed that Attention U-Net was found to exhibit the accuracy above 80%. So, the Attention U-Net was refined by integrating the feature fusion context module to propose FCAU-Net model. To further validate the classification capability of the proposed FCAU-Net model, the confusion matrix was generated for the test dataset.</p><p id=\"Par94\">\n<fig id=\"Fig14\" position=\"float\" orientation=\"portrait\"><label>Fig. 14</label><caption><p>Performance analysis of Accuracy of FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2680\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig14_HTML.jpg\"/></fig>\n</p><p id=\"Par95\">\n<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Performance analysis of FCAU-Net.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">CNN Type</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"/><th align=\"left\" colspan=\"1\" rowspan=\"1\">Directly applying raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">\n<bold>Applying FCE images</bold>\n</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">68.27</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">69.43</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref>,<xref ref-type=\"bibr\" rid=\"CR52\">52</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">71.23</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">72.62</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet<sup><xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">72.91</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">73.51</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref>,<xref ref-type=\"bibr\" rid=\"CR55\">55</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">76.25</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">77.44</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net<sup><xref ref-type=\"bibr\" rid=\"CR56\">56</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">78.64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">79.32</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">82.36</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.78</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90.51</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.89</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par96\">The confusion matrix provides a detailed breakdown of correctly and incorrectly classified samples across the two classes healthy and PCOS-infected. As shown in Figure. 15, FCAU-Net demonstrates near-perfect classification performance, correctly identifying the vast majority of cases with only a very small number of misclassifications. Out of 5,700 test images, the model achieved 5,693 correct predictions, with just 7 errors, corresponding to an overall accuracy of 99.89%. From the Fig. <xref rid=\"Fig15\" ref-type=\"fig\">15</xref>, it is evident that both Healthy and PCOS classes are classified with almost equal precision, ensuring the model does not suffer from class imbalance bias. The prediction results obtained from proposed FCAU-Net is shown in Figs.&#160;<xref rid=\"Fig16\" ref-type=\"fig\">16</xref> and <xref rid=\"Fig17\" ref-type=\"fig\">17</xref>.</p><p id=\"Par97\">\n<fig id=\"Fig15\" position=\"float\" orientation=\"portrait\"><label>Fig. 15</label><caption><p>Confusion matrix of the proposed FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2813\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig15_HTML.jpg\"/></fig>\n</p><p id=\"Par98\">\n<fig id=\"Fig16\" position=\"float\" orientation=\"portrait\"><label>Fig. 16</label><caption><p>Performance analysis of Accuracy of FCAU-Net with and without applying FCE.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2823\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig16_HTML.jpg\"/></fig>\n</p><p id=\"Par99\">\n<fig id=\"Fig17\" position=\"float\" orientation=\"portrait\"><label>Fig. 17</label><caption><p>Prediction Mask Results of Proposed FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2833\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig17_HTML.jpg\"/></fig>\n</p><p id=\"Par100\">To quantitatively assess the alignment of Grad-CAM heatmaps with ground-truth masks, four explainability metrics were used to analyze the performance like Intersection over Union (IoU), Dice Similarity Coefficient (DSC), Pointing Game Accuracy (PGA), and Energy-based Localization Score (ELS) and is shown in Table&#160;<xref rid=\"Tab6\" ref-type=\"table\">6</xref>. IoU measures the overlap between the predicted Grad-CAM heatmap region P and the ground-truth mask G. Here <inline-formula id=\"IEq188\"><tex-math id=\"d33e2840\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left|\\text{P}\\text{&#8745;}\\text{G}\\right|$$\\end{document}</tex-math></inline-formula>is the number of pixels common to both prediction and ground truth, and <inline-formula id=\"IEq189\"><tex-math id=\"d33e2844\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left|\\text{P}\\text{&#8746;}\\text{G}\\right|$$\\end{document}</tex-math></inline-formula> is the total number of unique pixels. The DSC evaluates the similarity between predicted and ground-truth regions, emphasizing balanced overlap where <inline-formula id=\"IEq190\"><tex-math id=\"d33e2848\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left|\\text{P}\\right|$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq191\"><tex-math id=\"d33e2852\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\left|\\text{G}\\right|$$\\end{document}</tex-math></inline-formula>denote the number of pixels in the predicted region and ground truth, respectively. PGA assesses whether the most activated pixel from the Grad-CAM heatmap lies within the ground-truth region. ELS evaluates the proportion of activation energy concentrated within the ground-truth region compared to the total energy in the heatmap.</p><p id=\"Par101\">\n<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Key metrics of quantitative evaluation of Explainability.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Metrics</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Formula</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq192\"><tex-math id=\"d33e2879\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{IoU = }}\\frac{{\\left| {{\\text{P}} \\cap {\\text{G}}} \\right|}}{{\\left| {{\\text{P}} \\cup {\\text{G}}} \\right|}}$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DSC</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq193\"><tex-math id=\"d33e2889\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{DSC}} = \\frac{{2\\left| {{\\text{P}} \\cap {\\text{G}}} \\right|}}{{\\left| {\\text{P}} \\right| + \\left| {\\text{G}} \\right|}}$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PGA</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq194\"><tex-math id=\"d33e2899\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Var}}}} = \\frac{{{\\text{No}}{\\text{.}}\\:{\\text{of}}\\:{\\text{hits}}}}{{{\\text{Total}}\\:{\\text{samples}}}} \\times 100$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ELS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq299\"><tex-math id=\"d33e2909\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{SD}} = \\frac{{\\sum\\nolimits_{{i \\in G}} {{\\text{GRADCAM}}_{i} } }}{{\\sum\\nolimits_{{i \\in I}} {{\\text{GRADCAM}}_{i} } }} \\times 100$$\\end{document}</tex-math></inline-formula>\n</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par102\">The numerical assessment of GRAD-CAM performance is shown in Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>. The Table&#160;7 performance analysis provides a numerical assessment of how well the highlighted regions in Grad-CAM align with the ovarian regions. The proposed FCAU-Net consistently demonstrates higher alignment scores compared to baseline CNN models, reinforcing that the attention learned by FCAU-Net is both accurate and meaningful.</p><p id=\"Par103\">\n<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Quantitative evaluation of explainability Grad-CAM of FCAU-Net.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">DSC (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PGA (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ELS (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">58.12</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">62.45</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">67.21</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">64.38</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">60.27</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">64.88</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.14</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65.02</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">61.33</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65.74</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">70.29</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">66.41</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">66.41</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">70.83</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.56</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.22</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.12</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.35</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.64</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.25</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.46</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.14</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.71</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">92.67</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.03</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.81</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.42</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par104\">From the quantitative analysis of Table&#160;<xref rid=\"Tab7\" ref-type=\"table\">7</xref>, it is evident that FCAU-Net achieves the highest explainability scores across all metrics. Specifically, the DSC (95.03%) and IoU (92.67%) indicate that Grad-CAM heatmaps produced by FCAU-Net strongly overlap with the true follicular regions in the ultrasound images. Furthermore, the PGA (97.81%) highlights that FCAU-Net almost always localizes the clinically relevant region, while the high ELS (95.42%) reflects the models focus intensity on these target areas. Compared to conventional CNNs and even the Attention U-Net, FCAU-Net shows a substantial margin of improvement, validating that its FFCM module not only enhances classification accuracy but also improves interpretability in meaningful way. Fig. <xref rid=\"Fig18\" ref-type=\"fig\">18</xref> shows the ROC curve and PR curve of the proposed FCAU-Net and it almost reaches the top-left corner in ROC and near-perfect PR, reflecting its very high accuracy of 99.89% on FCE images. PR curves show how the proposed FCAU-Net maintains high precision even at high recall, critical for reducing false positives in PCOS diagnosis.</p><p id=\"Par105\">\n<fig id=\"Fig18\" position=\"float\" orientation=\"portrait\"><label>Fig. 18</label><caption><p>ROC curve and PR curve of Proposed FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3040\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig18_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec11\"><title>Cross-validation generalization performance on FCAU-Net</title><p id=\"Par106\">To further validate the robustness and generalization ability of the proposed FCAU-Net, the 5-fold cross-validation was conducted on the augmented PCOS images. Unlike a single train&#8211;test split, k-fold cross-validation systematically partitions the dataset into k equally sized folds, where in each iteration one-fold is used for testing and the remaining k-1 folds for training. This process is repeated until every fold has been used once as the test set. The final performance is obtained by averaging across all folds, ensuring that the reported results are not biased by a particular train&#8211;test division. So, k-fold cross-validation ensures that every sample in the dataset is used for both training and testing, thereby minimizing bias and reducing the risk of overfitting. By systematically rotating training and testing folds, the performance of FCAU-Net can be reliably assessed under different data partitions. For the k-fold cross-validation experiments, the augmented PCOS 45,600 images with 22,800 Healthy and 22,800 PCOS images. The dataset was partitioned into five equal folds, each containing 9120 images, ensuring that every sample contributed to both training and testing across different iterations. In each fold, approximately 45,600 images were used for training while the 360 actual images were used for testing. Table&#160;<xref rid=\"Tab8\" ref-type=\"table\">8</xref> shows the performance of Fold-1 of cross-validation results. The results of Fold-1 demonstrate that the proposed FCAU-Net outperforms all baseline CNN architectures across every evaluation metric. However, the proposed FCAU-Net clearly establishes superiority, reaching nearly perfect performance in Fold 1 with accuracy and F1-scores above 99%. This fold-1 thus validates the effectiveness of the FFCM module in capturing both positional and contextual information, results in a more reliable diagnostic tool. The ROC and PR curves for Fold-1 in Figure. <xref rid=\"Fig19\" ref-type=\"fig\">19</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification.</p><p id=\"Par107\">\n<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Performance analysis of fold-1 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par108\">\n<fig id=\"Fig19\" position=\"float\" orientation=\"portrait\"><label>Fig. 19</label><caption><p>ROC curve and PR curve of FCAU-Net Fold-1.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3181\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig19_HTML.jpg\"/></fig>\n</p><p id=\"Par109\">Table&#160;<xref rid=\"Tab9\" ref-type=\"table\">9</xref> shows the performance of Fold-2 of cross-validation results. In Fold 2, the performance trend observed in Fold 1 is consistently replicated. Nevertheless, FCAU-Net exhibits the most robust performance across all metrics, maintaining balanced precision, recall, specificity, and F1-scores above 99%. The high specificity indicates that the model effectively reduces false positives, avoiding over-diagnosis of healthy ovaries as PCOS. This balance between sensitivity and specificity is vital, where both false negatives and false positives carry serious consequences. This fold-2 reinforces the generalization ability of FCAU-Net, suggesting that its superior accuracy is not dataset-specific but consistent across splits.</p><p id=\"Par110\">\n<table-wrap id=\"Tab9\" position=\"float\" orientation=\"portrait\"><label>Table 9</label><caption><p>Performance analysis of fold-2 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">70.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par111\">\n<fig id=\"Fig20\" position=\"float\" orientation=\"portrait\"><label>Fig. 20</label><caption><p>ROC curve and PR curve of FCAU-Net Fold-2.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3317\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig20_HTML.jpg\"/></fig>\n</p><p id=\"Par112\">The ROC and PR curves for Fold-2 in Fig. <xref rid=\"Fig20\" ref-type=\"fig\">20</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. The Fold-3 performance analysis of FCAU-Net shown in Table&#160;<xref rid=\"Tab10\" ref-type=\"table\">10</xref> further illustrates the robustness of FCAU-Net in comparison with other baseline models. FCAU-Net maintains its dominance, achieving near-perfect classification with F1-scores and recall emphasizes the model&#8217;s ability to correctly identify nearly all PCOS cases, ensuring minimal risk of underdiagnosis. Additionally, the stability of results across folds highlights that FCAU-Net does not overfit to a particular data partition. The ROC and PR curves for Fold-3 in Fig. <xref rid=\"Fig21\" ref-type=\"fig\">21</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification.</p><p id=\"Par113\">\n<table-wrap id=\"Tab10\" position=\"float\" orientation=\"portrait\"><label>Table 10</label><caption><p>Performance analysis of fold-3 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par114\">\n<fig id=\"Fig21\" position=\"float\" orientation=\"portrait\"><label>Fig. 21</label><caption><p>ROC curve and PR curve of FCAU-Net Fold-3.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3459\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig21_HTML.jpg\"/></fig>\n</p><p id=\"Par115\">The Fold-4 performance analysis of FCAU-Net shown in Table&#160;<xref rid=\"Tab11\" ref-type=\"table\">11</xref> confirms the FCAU-Net&#8217;s superiority. The proposed FCAU-Net achieves accuracies above 99% with correspondingly high precision and specificity, signifying a substantial reduction in both false positives and false negatives. Such stability across folds demonstrates that FCAU-Net is not only accurate but also highly generalizable for real time deployment.</p><p id=\"Par116\">\n<table-wrap id=\"Tab11\" position=\"float\" orientation=\"portrait\"><label>Table 11</label><caption><p>Performance analysis of fold-4 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">70.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par117\">\n<fig id=\"Fig22\" position=\"float\" orientation=\"portrait\"><label>Fig. 22</label><caption><p>ROC curve and PR curve of FCAU-Net Fold-4.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3596\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig22_HTML.jpg\"/></fig>\n</p><p id=\"Par118\">The ROC and PR curves for Fold-4 in Fig. <xref rid=\"Fig22\" ref-type=\"fig\">22</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. The Fold-5 performance analysis of FCAU-Net shown in Table&#160;<xref rid=\"Tab12\" ref-type=\"table\">12</xref> results provide further evidence of FCAU-Net&#8217;s robustness, as the model consistently surpasses all baselines in every performance metric. While traditional CNNs, including DenseNet and AlexNet, remain constrained to accuracies below 75%, U-Net and ResNet achieve moderate improvements yet fall short in sensitivity.</p><p id=\"Par119\">\n<table-wrap id=\"Tab12\" position=\"float\" orientation=\"portrait\"><label>Table 12</label><caption><p>Performance analysis of fold-5 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par120\">\n<fig id=\"Fig23\" position=\"float\" orientation=\"portrait\"><label>Fig. 23</label><caption><p>ROC curve and PR curve of FCAU-Net Fold-5.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3735\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig23_HTML.jpg\"/></fig>\n</p><p id=\"Par121\">The ROC and PR curves for Fold-5 in Fig. <xref rid=\"Fig23\" ref-type=\"fig\">23</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. Attention U-Net&#8217;s ability to surpass 83% accuracy highlights the incremental gains of attention mechanisms but also underscores the gap left unaddressed in terms of feature fusion. FCAU-Net addresses these limitations by achieving almost flawless classification, with balanced precision, recall, specificity, and F1-scores near 100%. This demonstrates its capacity to reliably capture both fine-grained local features and broader contextual dependencies within ovarian ultrasound images. The mean performance across all five folds provides the validation of FCAU-Net&#8217;s generalization capabilities and is shown in Table&#160;<xref rid=\"Tab13\" ref-type=\"table\">13</xref>. Baseline models such as DenseNet, VGG, and AlexNet display consistently low averages across metrics, with accuracies around 70% to 74%. ResNet and U-Net improve average accuracy to the high 70%, but their lower recall values suggest persistent vulnerability to false negatives. Attention U-Net consistently achieves above 83% accuracy, marking a significant advancement through the use of attention mechanisms. However, FCAU-Net outperforms all models by a wide margin, with mean accuracies above 99% and near-perfect scores across precision, recall, specificity, and F1-score. This consistency across folds confirms that FCAU-Net&#8217;s results are not due to random partitioning effects but rather from its architecture, which integrates feature fusion and contextual attention. Figure <xref rid=\"Fig24\" ref-type=\"fig\">24</xref> shows the 5-fold cross validation box plot of proposed FCAU-Net. The consolidated ROC and PR curves for all the five folds in Fig. <xref rid=\"Fig25\" ref-type=\"fig\">25</xref> shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification.</p><p id=\"Par122\">\n<table-wrap id=\"Tab13\" position=\"float\" orientation=\"portrait\"><label>Table 13</label><caption><p>Performance analysis of fold-5 cross-validation.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Specificity</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-Score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">74.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par123\">\n<fig id=\"Fig24\" position=\"float\" orientation=\"portrait\"><label>Fig. 24</label><caption><p>K-Fold cross validation plot of Proposed FCAU-Net.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3880\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig24_HTML.jpg\"/></fig>\n</p><p id=\"Par124\">\n<fig id=\"Fig25\" position=\"float\" orientation=\"portrait\"><label>Fig. 25</label><caption><p>Consolidated ROC curve and PR curve of FCAU-Net K-fold cross validation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e3890\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26314_Fig25_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec12\"><title>Statistical significance testing on FCAU-Net</title><p id=\"Par125\">To ensure that the superior performance of the proposed FCAU-Net with 99.89% is not due to random variation, the statistical significance testing was conducted. To evaluate the effectiveness of the proposed FCAU-Net model, a statistical significance analysis was conducted comparing its performance on raw ultrasound images and FCE enhanced ultrasound images. While the absolute accuracy of FCAU-Net increased from 90.51% to 99.89%, it is essential to determine whether this improvement is statistically meaningful. Accuracy alone cannot fully establish the robustness of a model, hence statistical significance tests was employed to validate the consistency of FCAU-Net compared with baseline models such as DenseNet, VGG, AlexNet, ResNet, U-Net, and Attention U-Net.For statistical comparison. Paired t-test was used to compare the performance of FCAU-Net with each baseline model. McNemar test was also conducted on confusion matrices to evaluate whether the observed differences in misclassification distributions were statistically significant. A 95% confidence level (<italic toggle=\"yes\">p</italic>&#8201;&lt;&#8201;0.05) was set as the threshold for significance. Table&#160;<xref rid=\"Tab14\" ref-type=\"table\">14</xref> shows the performance of statistical significance testing analysis results on confusion matrix with FCE images.</p><p id=\"Par126\">\n<table-wrap id=\"Tab14\" position=\"float\" orientation=\"portrait\"><label>Table 14</label><caption><p>Statistical significance testing on confusion matrix with FCE images.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"5\" rowspan=\"1\">With FCE images (%)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Misclassifications</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.43</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.54</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.73</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,742</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.62</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.01</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,561</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.51</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.28</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.08</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,508</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.44</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.20</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.06</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,286</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.11</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.72</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.91</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1,178</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.78</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.50</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.19</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.34</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">926</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.86</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.87</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">7</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par127\">The statistical paired t-test was employed to assess the differences in model performance, considering repeated measurements across multiple runs on raw ultrasound images and FCE ultrasound images. Key metrics computed include the mean difference, variance, standard deviation, t-value, degrees of freedom (DoF), <italic toggle=\"yes\">p</italic>-value, and 95% confidence interval of the observed improvement and is shown in Table&#160;<xref rid=\"Tab15\" ref-type=\"table\">15</xref>.</p><p id=\"Par128\">\n<table-wrap id=\"Tab15\" position=\"float\" orientation=\"portrait\"><label>Table 15</label><caption><p>Key metrics of statistical significance testing.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Metrics</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Formula</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy difference<inline-formula id=\"IEq196\"><tex-math id=\"d33e4055\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:{\\text{}\\text{Diff}}_{\\text{i}}$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq197\"><tex-math id=\"d33e4061\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{Diff}}_{i} = {\\text{Accuracy}}\\;({\\text{FCE}}\\;{\\text{images}} - {\\text{Raw}}\\;{\\text{images}})$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Maean difference <inline-formula id=\"IEq198\"><tex-math id=\"d33e4069\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Diff}}}}$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq199\"><tex-math id=\"d33e4075\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Diff}}}} {\\text{ = }}\\frac{{\\sum _{{{\\text{i = 1}}}}^{n} {\\text{Diff}}_{{\\text{i}}} }}{{\\text{n}}}$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Varaiance of difference <inline-formula id=\"IEq200\"><tex-math id=\"d33e4083\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Var}}}}$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq201\"><tex-math id=\"d33e4089\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Var}}}} = \\frac{{\\sum _{{i = 1}}^{n} ({\\text{Diff}}_{i} - \\overline{{{\\text{Diff}}}} )^{2} }}{{n - 1}}$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Standard deviation <inline-formula id=\"IEq202\"><tex-math id=\"d33e4097\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\text{SD}$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq203\"><tex-math id=\"d33e4103\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{SD = }}\\sqrt {\\overline{{{\\text{Var}}}} }$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">T-value</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq204\"><tex-math id=\"d33e4113\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{gathered} {\\text{t}}\\;{\\text{ = }}\\frac{{\\overline{{{\\text{Diff}}}} }}{{\\frac{{{\\text{SD}}}}{{\\sqrt {\\text{n}} }}}} \\hfill \\\\ \\overline{{{\\text{Var}}}} \\hfill \\\\ \\end{gathered}$$\\end{document}</tex-math></inline-formula>\n</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Confidence interval <inline-formula id=\"IEq205\"><tex-math id=\"d33e4121\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\:\\text{CI}$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq206\"><tex-math id=\"d33e4127\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{CI}} = \\overline{{{\\text{Diff}}}} \\pm t \\cdot \\frac{{{\\text{SD}}}}{{\\sqrt n }}$$\\end{document}</tex-math></inline-formula>\n</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par129\">The initial step of the statistical paired t-test starts by organizing the data. Then calculate the basic difference between the accuracy of FCE images with raw images. Table&#160;<xref rid=\"Tab16\" ref-type=\"table\">16</xref> shows the performance of statistical paired t-test of FCAU-Net compared with the baseline models.</p><p id=\"Par130\">\n<table-wrap id=\"Tab16\" position=\"float\" orientation=\"portrait\"><label>Table 16</label><caption><p>Performance analysis of statistical significance testing with FCE images.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw accuracy (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE accuracy (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq207\"><tex-math id=\"d33e4166\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Diff}}}}$$\\end{document}</tex-math></inline-formula>\n</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">\n<inline-formula id=\"IEq208\"><tex-math id=\"d33e4173\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\overline{{{\\text{Var}}}}$$\\end{document}</tex-math></inline-formula>\n</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SD</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">t-value</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">DOF</th><th align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">p</italic>-value</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">95% CI (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">68.27</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">69.43</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.90</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.045</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.03&#8211;2.29</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">71.23</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.62</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.39</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.47</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.025</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.31&#8211;2.47</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">72.91</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">73.51</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.60</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.50</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.20</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">&#8722;0.44&#8211;1.64</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">76.25</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">77.44</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.19</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">2.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.042</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.12&#8211;2.26</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.64</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.68</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.70</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">&#8722;0.30&#8211;1.66</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.36</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.78</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">1.42</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.16</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.55</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.023</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.36&#8211;2.48</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">90.51</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.38</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.25</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.50</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">42.00</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">&lt;&#8201;0.001</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.50&#8211;10.26</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par131\">The analysis shows that FCAU-Net achieved a mean improvement of 9.38%, with low variance across repeated runs. The resulting t-value of 42.0 and a <italic toggle=\"yes\">p</italic>-value&#8201;&lt;&#8201;0.001 confirm that the improvement is highly statistically significant. The 95% confidence interval [8.50%, 10.26%] further reinforces that FCAU-Net consistently outperforms other models when leveraging FCE ultrasound images.</p></sec><sec id=\"Sec13\"><title>Computational complexity analysis of FCAU-Net</title><p id=\"Par132\">In addition to superior accuracy in PCOS detection, the computational efficiency of the proposed FCAU-Net is a critical factor in evaluating its practical applicability of the model. In order to comprehensively evaluate the computational efficiency of the proposed FCAU-Net, several key metrics were considered across both raw and FCE ultrasound images. Training metrics included the training time per epoch, convergence rate, and CPU utilization during training. These metrics provide insights into how quickly the network learns from data, how efficiently it uses hardware resources, and how long it takes to reach optimal performance. Inference metrics encompassed inference time per image, inference time per batch, throughput with the number of images processed per second, and latency. These inference metrics measure the speed and responsiveness of the model. The training and inference metrics performance comparison is shown in Tables&#160;<xref rid=\"Tab17\" ref-type=\"table\">17</xref> and <xref rid=\"Tab18\" ref-type=\"table\">18</xref> respectively. The inference performance of the proposed FCAU-Net demonstrates significant improvements over conventional CNN models in terms of speed, throughput, and latency, highlighting its suitability for real-time PCOS diagnosis. On raw images, FCAU-Net achieves an inference time of 9.1 ms per image, which is considerably faster than other CNN models (Table&#160;<xref rid=\"Tab18\" ref-type=\"table\">18</xref>). This reduced processing time allows for higher throughput, with FCAU-Net processing approximately 11.1 images per second, outperforming all other models in practical efficiency.</p><p id=\"Par133\">\n<table-wrap id=\"Tab17\" position=\"float\" orientation=\"portrait\"><label>Table 17</label><caption><p>Training metrics of FCAU-Net model.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Training time per epoch (minutes)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Convergence rate (epochs)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">CPU utilization (%)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">35</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">34</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">60</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">62</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">39</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">65</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">30</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">29</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">55</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">56</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">31</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">58</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">59</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">38</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">37</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">60</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">61</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">40</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">39</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">62</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">28</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">27</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">54</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">55</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par134\">Similarly, for FCE-processed images, FCAU-Net maintains a rapid inference time (Table&#160;<xref rid=\"Tab18\" ref-type=\"table\">18</xref>) of 8.7 ms per image, which is again lower than the other CNN, resulting in a throughput of 11.5 images per second. The superior inference performance is attributed to the optimized architecture of FCAU-Net, which combines lightweight attention modules with feature-calibrated fusion, reducing redundant computations while focusing on the most informative regions of the ultrasound images.</p><p id=\"Par135\">\n<table-wrap id=\"Tab18\" position=\"float\" orientation=\"portrait\"><label>Table 18</label><caption><p>Inference metrics of FCAU-Net model.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Inferenc time per image (ms)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Inferenc time per batch (ms)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Throughput (images/sec)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Latency (ms)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.1</td><td char=\".\" align=\"left\" colspan=\"1\" rowspan=\"1\">120</td><td char=\".\" align=\"left\" colspan=\"1\" rowspan=\"1\">115</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">13.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">135</td><td char=\".\" align=\"left\" colspan=\"1\" rowspan=\"1\">130</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">13.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">100</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.4</td><td char=\".\" align=\"left\" colspan=\"1\" rowspan=\"1\">110</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">108</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">15.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">15.0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">150</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">145</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">15.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">15.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">16.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">16.0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">160</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">155</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">16.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">16.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">90</td><td char=\".\" align=\"left\" colspan=\"1\" rowspan=\"1\">85</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.7</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par136\">To evaluate model complexity and computational load, the analysis included the total number of learnable parameters, floating-point operations (FLOPs), and computational overhead introduced by network components such as attention modules and feature-calibrated fusion layers. Table&#160;<xref rid=\"Tab19\" ref-type=\"table\">19</xref> shows the analysis of model complexity and efficiency metrics that highlights the computational advantages of the proposed FCAU-Net over conventional CNN models for both raw and FCE ultrasound images. In terms of number of parameters, FCAU-Net maintains a relatively compact size of 7.3&#160;million, which is lower than other CNN, reflecting its efficient design without compromising representational power. Regarding computational operations, FCAU-Net achieves a low FLOPs count for FCE images, which is significantly lower than other CNN, indicating that it requires fewer floating-point operations to generate predictions. This reduced computational load directly translates into faster processing and lower energy consumption. Furthermore, the computational overhead of FCAU-Net is categorized as low, in contrast to the high overhead observed in other CNN models, due to the optimized integration of attention mechanisms and feature-calibrated fusion modules that selectively process the most informative regions of the images. Overall, these metrics demonstrate that FCAU-Net achieves a balanced trade-off between high accuracy and computational efficiency, making it well-suited for automated PCOS detection.</p><p id=\"Par137\">\n<table-wrap id=\"Tab19\" position=\"float\" orientation=\"portrait\"><label>Table 19</label><caption><p>FCAU-Net model complexity and efficiency metrics.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Number of parameters (M)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">FLOPS ( X 10<sup>9</sup>)</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Computation overhead</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Raw images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">FCE images</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DenseNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">8.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AlexNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">5.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">3.4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">11.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">6.6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">12.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">9.3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">14.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">10.1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">High</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">7.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">4.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Low</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec14\"><title>Performance comparison with State-of-the-art models</title><p id=\"Par138\">To evaluate the effectiveness of the proposed FCAU-Net, a detailed comparison is done with existing state-of-the-art (SOTA) approaches for PCOS detection with accuracy metrics. Table&#160;<xref rid=\"Tab20\" ref-type=\"table\">20</xref> summarizes the performance of several recent techniques, including conventional CNNs, hybrid networks, attention-based models, and ensemble methods compared with the proposed FCAU-Net.</p><p id=\"Par139\">\n<table-wrap id=\"Tab20\" position=\"float\" orientation=\"portrait\"><label>Table 20</label><caption><p>Performance analysis of SOTA methods.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AResUNet<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhanced U-Net&#8201;+&#8201;ResNet<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.80</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GAN&#8201;+&#8201;CNN<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCOS-WaveConvNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.90</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">PCONet<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.12</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGGNet16&#8201;+&#8201;Stacking Ensemble<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.90</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG16 (modified last 4 layers)<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">92.11</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ASPPNet&#8201;+&#8201;ResNet<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.79</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN&#8201;+&#8201;BiLSTM<sup><xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.74</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ESDPCOS (CNN&#8201;+&#8201;GLCM)<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.06</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AMCNN<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.79</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN&#8201;+&#8201;KNN clustering<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">MLOD<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ocys-Net<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.93</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">HHO-DQN<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.50</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ITL-CNN<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.90</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ensemble (VGG16, ResNet50, MobileNet)<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Watershed&#8201;+&#8201;contour analysis<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.80</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CR-UNet<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">91.20</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Hybrid CNN<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SqueezeNet<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.63</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">InceptionV3&#8201;+&#8201;TL<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.48</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">2D CNN&#8201;+&#8201;SVM, DT, RF<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.07</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Sequential 2D CNN&#8201;+&#8201;wrapper FS<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.67</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Elman NN&#8201;+&#8201;Gabor Wavelet<sup><xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.10</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BPA (modified LM optimization)<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">93.92</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNetB6&#8201;+&#8201;Attention UNet<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.12</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Threshold-based segmentation<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.00</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DLNNSVM<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.32</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GrabCut&#8201;+&#8201;FL-SNNM<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.99</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">GIST-MDR<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">93.82</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">QEI-SAM<sup><xref ref-type=\"bibr\" rid=\"CR63\">63</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.31</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deeplabv3<sup><xref ref-type=\"bibr\" rid=\"CR64\">64</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">94.60</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">CystNet<sup><xref ref-type=\"bibr\" rid=\"CR68\">68</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.82</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">TL-CNN<sup><xref ref-type=\"bibr\" rid=\"CR69\">69</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.20</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DC-UNet<sup><xref ref-type=\"bibr\" rid=\"CR70\">70</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.54</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AdaResU-Net<sup><xref ref-type=\"bibr\" rid=\"CR71\">71</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">98.47</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par140\">The SOTA methods demonstrates extensive exploration of neural-based systems for PCOS detection from ultrasound images, largely involving U-Net variants, hybrid segmentation-classification designs, and feature fusion strategies. However, existing approaches such as AResU-Net<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup> and CystNet<sup><xref ref-type=\"bibr\" rid=\"CR68\">68</xref></sup> primarily focus on architectural modifications or preprocessing filters without deeply modeling contextual dependencies and multi-level feature interactions. The proposed CAU-Net uniquely addresses these deficits by integrating a FFC module that jointly encodes spatial and contextual cues, enhancing discriminative representation across ovarian regions. Unlike CystNet threshold-based segmentation or AResU-Net residual attention layers focused on feature refinement, the FFC module adaptively weighs local-global dependencies through contextual recalibration, achieving superior follicle delineation and classification precision. Moreover, adding FCE preprocessing distinguishes FCAU-Net from conventional CLAHE preprocessing schemes, improving cyst boundary clarity and model generalization. As summarized below, FCAU-Net&#8217;s 99.89% detection accuracy notably exceeds SOTA methods, underscoring its advancements in both architectural and preprocessing aspects.</p><p id=\"Par141\">The results of fold-5 cross-validation demonstrate that the proposed FCAU-Net outperforms a wide range of SOTA techniques in PCOS detection from ultrasound images. Conventional CNN-based architectures were surpassed by FCAU-Net by margins ranging from 1% to 3%, highlighting its superior ability to extract and utilize discriminative features from ovarian ultrasound images. Furthermore, models relying on classical feature extraction methods performed significantly lower, emphasizing the advantage of FCAU-Net with attention and feature calibration mechanisms. The proposed FCAU-Net integrates feature-calibrated attention modules that selectively focus on informative regions of the ultrasound images, which likely accounts for its superior performance. In addition, its robust architecture ensures consistent performance across different folds, demonstrating both high accuracy and reliability.</p></sec><sec id=\"Sec15\"><title>Ablation study on FCAU-Net</title><p id=\"Par142\">To rigorously evaluate the effectiveness of the architectural components integrated into the proposed FCAU-Net, an extensive ablation study is done to analyze the performance. The purpose of this analysis is to isolate and quantify the contribution of each key component namely FFCM, the modified attention gate, and the skip connections towards the overall performance of the network in detecting PCOS from ultrasound images. While the baseline U-Net serves as the foundational reference, progressive modifications and module additions allow us to systematically examine how each enhancement improves the network&#8217;s learning capability and discriminative power. This ablation study was performed on raw ultrasound images and FCE images. The ablation framework involved testing eleven different model variants, ranging from a simple baseline U-Net to progressively enhanced versions with either FFCM, default attention gates, modified attention gates. Table&#160;<xref rid=\"Tab21\" ref-type=\"table\">21</xref> shows the ablation study performance analysis of the FCAU-Net for raw images. Table&#160;<xref rid=\"Tab22\" ref-type=\"table\">22</xref> shows the ablation study performance analysis of the FCAU-Net for FCE images. The ablation results on both raw and FCE enhanced ultrasound images highlight the incremental contributions of each architectural component in FCAU-Net.</p><p id=\"Par143\">\n<table-wrap id=\"Tab21\" position=\"float\" orientation=\"portrait\"><label>Table 21</label><caption><p>Ablation study performance with Raw images.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"4\" rowspan=\"1\">With FCE images (%)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.64</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net without FFCM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.10</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with FFCM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.25</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.36</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.10</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.02</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">85.12</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">85.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">85.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.27</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">87.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">85.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net with FFCM and without Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.15</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">87.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net with FFCM and without Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">89.10</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">89.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">89.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">90.51</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">91.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">90.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">90.6</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par144\">Starting from the baseline U-Net (Table&#160;<xref rid=\"Tab22\" ref-type=\"table\">22</xref>) providing modest accuracy, the addition of FFCM or attention gates individually improves performance by enabling more effective feature representation and contextual learning. The U-Net variants with default or modified attention gates perform better than those with FFCM alone, underscoring the importance of selective focus in ovarian structure segmentation.</p><p id=\"Par145\">\n<table-wrap id=\"Tab22\" position=\"float\" orientation=\"portrait\"><label>Table 22</label><caption><p>Ablation study performance with FCE images.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" rowspan=\"2\" colspan=\"1\">Model</th><th align=\"left\" colspan=\"4\" rowspan=\"1\">With FCE images (%)</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.32</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">78.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net without FFCM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.10</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">79.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">80.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with FFCM</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.56</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">81.4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.74</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">82.7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net with Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.56</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.2</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">83.6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.60</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">85.1</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.0</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">84.5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.78</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">87.4</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.3</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">86.8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net without FFCM and Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.15</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">87.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">88.1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net with FFCM and without Default Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.24</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">95.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">FCAU-Net with FFCM and without Modified Attention Gate</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.12</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.6</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">96.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">97.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed FCAU-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.89</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.8</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">99.9</td></tr></tbody></table></table-wrap>\n</p><p id=\"Par146\">The modified attention gate consistently outperforms the default gate, reflecting the benefit of refining the gating mechanisms for ultrasound images. The highest performance is obtained with the full FCAU-Net, achieving 90.51% on raw images and an impressive 99.89% on FCE images, demonstrating that integrating both FFCM and modified attention gates yields the most discriminative and robust feature learning.</p></sec><sec id=\"Sec16\"><title>Conclusion and future enhancements</title><p id=\"Par147\">This study proposes the FCAU-Net model, an enhanced Attention U-Net integrated with a Feature Fusion Context Module (FFCM), for classifying PCOS-infected ultrasound images with high accuracy. The methodology introduces two significant contributions. First, the dataset was preprocessed using image cropping, focusing on the main contextual regions by identifying extreme points and contours, followed by enhancement through FCE imaging. These steps emphasize high-intensity pixel features, ensuring better input quality for classification. Second, the FFCM was integrated into the Attention U-Net to optimize feature maps by fusing positional and contextual information, enhancing both deep and shallow features. Before augmentation, the dataset was partitioned to ensure that only original images were used for testing with 360 images, while augmented samples were exclusively utilized for training to enhance model generalization and robustness. The refined pipeline included data augmentation, resulting in a dataset of 45,600 images, divided into 80:20 for training and validation. Comparative evaluation against models like DenseNet, VGG, AlexNet, UNet, and Attention U-Net demonstrated the superior performance of FCAU-Net, achieving a classification accuracy of 99.89%, significantly outperforming existing approaches.</p><p id=\"Par148\">While FCAU-Net exhibits remarkable performance, challenges remain in further optimizing the encoding and decoding blocks with alternative loss functions and advanced optimizers. Although FCAU-Net incorporates feature-calibrated attention modules to focus on informative regions, very small or overlapping follicles with subtle intensity differences can still pose challenges, leading to occasional misclassification or missed detections. The proposed FCAU-Net is highly depended on high-quality ultrasound images, so the segmentation accuracy and detection of follicle is affected for images with severe noise, motion artifacts, or poor contrast. Additionally, FCAU-Net primarily focuses on morphological features visible in 2D ultrasound images and may not fully leverage temporal or volumetric information available in 3D or cine ultrasound scans, which could provide richer diagnostic cues. To overcome these limitations, future research could explore the integration of self-supervised or semi-supervised learning strategies that may enhance feature robustness. Additionally, hybrid architectures combining FCAU-Net with lightweight transformer modules or adaptive post-processing techniques could further improve the detection of subtle and overlapping follicles. The future work could focus on robust pre-processing and denoising techniques to enhance performance on low-quality or noisy images. Integrating 3D ultrasound data or temporal sequences into FCAU-Net could capture additional structural and dynamic information, potentially improving detection of small or overlapping follicles. Furthermore, incorporating explainable AI techniques such as attention heatmaps or feature attribution maps can enhance model interpretability. The proposed FCAU-Net may also focus on extending the FFCM with additional position and context blocks to further refine feature map optimization. By addressing these limitations, future iterations of FCAU-Net could achieve even higher reliability, generalizability, and practical usability for automated PCOS diagnosis.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>Conceptualization, N.Y, M.S.D. and K.R.S.P; Data curation, M.S.D. and K.R.S.P.; Formal analysis, K.R.S.P, M.S.D., and N.Y; Methodology, M.S.D., and N.Y; Project administration, K.R.S.P and N.Y; Resources, N.Y .; Software, M.S.D, and K.R.S.P.; Supervision, N.Y.; Validation, N.Y.; Visualization, M.S.D. and K.R.S.P ; Writing&#8212;original draft, M.S.D, K.R.S.P. and N.Y.; Writing&#8212;review &amp; editing, K.R.S.P. and N.Y.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>Open access funding provided by Vellore Institute of Technology.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The dataset used in and associated metadata used for model development and evaluation can be accessed and this study is publicly available on Kaggle at the following link: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images\">https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images</ext-link> All ultrasound images downloaded from this repository under the terms and conditions specified by the dataset provider</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par149\">The authors declare no competing interests.</p></notes><notes id=\"FPar76\"><title>Ethical approval</title><p id=\"Par150\">Not applicable.</p></notes><notes id=\"FPar2\"><title>Consent to publish</title><p id=\"Par151\">Not applicable.</p></notes><notes id=\"FPar3\"><title>Consent to participate</title><p id=\"Par152\">Not applicable.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bedi</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Goyal</surname><given-names>SB</given-names></name><name name-style=\"western\"><surname>Rajawat</surname><given-names>AS</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>M</given-names></name></person-group><article-title>An integrated adaptive bilateral filter-based framework and attention residual U-net for detecting polycystic ovary syndrome</article-title><source>Decis. Anal. J.</source><year>2024</year><volume>10</volume><fpage>100366</fpage><pub-id pub-id-type=\"doi\">10.1016/j.dajour.2023.100366</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Bedi, P., Goyal, S. B., Rajawat, A. S. &amp; Kumar, M. An integrated adaptive bilateral filter-based framework and attention residual U-net for detecting polycystic ovary syndrome. <italic toggle=\"yes\">Decis. Anal. J.</italic><bold>10</bold>, 100366. 10.1016/j.dajour.2023.100366 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><mixed-citation publication-type=\"other\">Alamoudi, A. et al. M. &amp; Al Bahrani, R. A deep learning fusion approach to diagnosis the polycystic ovary syndrome (PCOS). <italic toggle=\"yes\">Appl. Comput. Intell. Soft Comput.</italic> 9686697 (2023). (2023). 10.1155/2023/9686697</mixed-citation></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lv</surname><given-names>W</given-names></name><etal/></person-group><article-title>Deep learning algorithm for automated detection of polycystic ovary syndrome using scleral images</article-title><source>Front. Endocrinol.</source><year>2021</year><volume>12</volume><fpage>789878</fpage><pub-id pub-id-type=\"doi\">10.3389/fendo.2021.789878</pub-id><pub-id pub-id-type=\"pmcid\">PMC8828568</pub-id><pub-id pub-id-type=\"pmid\">35154003</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Lv, W. et al. Deep learning algorithm for automated detection of polycystic ovary syndrome using scleral images. <italic toggle=\"yes\">Front. Endocrinol.</italic><bold>12</bold>, 789878. 10.3389/fendo.2021.789878 (2021).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fendo.2021.789878</pub-id><pub-id pub-id-type=\"pmcid\">PMC8828568</pub-id><pub-id pub-id-type=\"pmid\">35154003</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Reka, S. et al. Expeditious prognosis of PCOS with ultrasonography images &#8211; a convolutional neural network approach. <italic toggle=\"yes\">In Artificial Intelligence of Things. ICAIoT 2023. Communications in Computer and Information Science (eds. Challa, R. K.) vol. 315&#8211;326 (Springer, Cham, 2024).</italic>10.1007/978-3-031-48774-3_26 (1929).</mixed-citation></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hossain</surname><given-names>MM</given-names></name><etal/></person-group><article-title>Particle swarm optimized fuzzy CNN with quantitative feature fusion for ultrasound image quality identification</article-title><source>IEEE J. Transl Eng. Health Med.</source><year>2022</year><volume>10</volume><fpage>1800712</fpage><pub-id pub-id-type=\"doi\">10.1109/JTEHM.2022.3197923</pub-id><pub-id pub-id-type=\"pmid\">36226132</pub-id><pub-id pub-id-type=\"pmcid\">PMC9550163</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Hossain, M. M. et al. Particle swarm optimized fuzzy CNN with quantitative feature fusion for ultrasound image quality identification. <italic toggle=\"yes\">IEEE J. Transl Eng. Health Med.</italic><bold>10</bold>, 1800712. 10.1109/JTEHM.2022.3197923 (2022).<pub-id pub-id-type=\"pmid\">36226132</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JTEHM.2022.3197923</pub-id><pub-id pub-id-type=\"pmcid\">PMC9550163</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Tiwari, S., Maheshwari, P. &amp; PCOS-WaveConvNet: A wavelet convolutional neural network for polycystic ovary syndrome detection using ultrasound images. <italic toggle=\"yes\">In 9th International Conference on Information Technology Trends (ITT)</italic> 117&#8211;122 (IEEE, 2023). 117&#8211;122 (IEEE, 2023). (2023). 10.1109/ITT59889.2023.10184271</mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Hosain, A. K. M. S., Mehedi, M. H. K., Kabir, I. E. &amp; PCONet A convolutional neural network architecture to detect polycystic ovary syndrome (PCOS) from ovarian ultrasound images. <italic toggle=\"yes\">In International Conference on Engineering and Emerging Technologies (ICEET)</italic> 1&#8211;6 (IEEE, 2022). 1&#8211;6 (IEEE, 2022). (2022). 10.1109/ICEET56468.2022.10007353</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Suha</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>Islam</surname><given-names>MN</given-names></name></person-group><article-title>An extended machine learning technique for polycystic ovary syndrome detection using ovary ultrasound image</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>17123</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-022-21724-0</pub-id><pub-id pub-id-type=\"pmid\">36224353</pub-id><pub-id pub-id-type=\"pmcid\">PMC9556522</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Suha, S. A. &amp; Islam, M. N. An extended machine learning technique for polycystic ovary syndrome detection using ovary ultrasound image. <italic toggle=\"yes\">Sci. Rep.</italic><bold>12</bold>, 17123. 10.1038/s41598-022-21724-0 (2022).<pub-id pub-id-type=\"pmid\">36224353</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-022-21724-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC9556522</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Soni</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Vashisht</surname><given-names>S</given-names></name></person-group><article-title>Image segmentation for detecting polycystic ovarian disease using deep neural networks</article-title><source>Int. J. Comput. Sci. Eng.</source><year>2019</year><volume>7</volume><fpage>534</fpage><lpage>537</lpage><pub-id pub-id-type=\"doi\">10.26438/ijcse/v7i3.534537</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Soni, P. &amp; Vashisht, S. Image segmentation for detecting polycystic ovarian disease using deep neural networks. <italic toggle=\"yes\">Int. J. Comput. Sci. Eng.</italic><bold>7</bold>, 534&#8211;537. 10.26438/ijcse/v7i3.534537 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Srivastava</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Kumar</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Chaudhry</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Singh</surname><given-names>A</given-names></name></person-group><article-title>Detection of ovarian cyst in ultrasound images using fine-tuned VGG-16 deep learning network</article-title><source>SN Comput. Sci.</source><year>2020</year><volume>1</volume><fpage>109</fpage><pub-id pub-id-type=\"doi\">10.1007/s42979-020-0109-6</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Srivastava, S., Kumar, P., Chaudhry, V. &amp; Singh, A. Detection of ovarian cyst in ultrasound images using fine-tuned VGG-16 deep learning network. <italic toggle=\"yes\">SN Comput. Sci.</italic><bold>1</bold>, 109. 10.1007/s42979-020-0109-6 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><mixed-citation publication-type=\"other\">Sahu, G. et al. Attention-based transfer learning approach using spatial pyramid pooling for diagnosis of polycystic ovary syndrome. <italic toggle=\"yes\">In 9th International Conference on Signal Processing and Communication (ICSC)</italic> 238&#8211;243 (IEEE, 2023). 238&#8211;243 (IEEE, 2023). (2023). 10.1109/ICSC60394.2023.10441101</mixed-citation></ref><ref id=\"CR12\"><label>12.</label><mixed-citation publication-type=\"other\">Diptho, R. A. et al. I. PCOS diagnosis with confluence CNN: A revolution in women&#8217;s health. <italic toggle=\"yes\">In 26th International Conference on Computer and Information Technology (ICCIT)</italic> 1&#8211;5 (IEEE, 2023). 1&#8211;5 (IEEE, 2023). (2023). 10.1109/ICCIT60459.2023.10441010</mixed-citation></ref><ref id=\"CR13\"><label>13.</label><mixed-citation publication-type=\"other\">Rousanuzzaman, Biswas, S. K. et al. ESDPCOS: Effectiveness of combined CNN and handcrafted features for ovarian cyst detection in PCOS patients using ultrasound images. <italic toggle=\"yes\">In 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)</italic> 846&#8211;851 (IEEE, 2023). 846&#8211;851 (IEEE, 2023). (2023). 10.1109/UPCON59197.2023.10434294</mixed-citation></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Rashid, S. et al. Attention-based multiscale deep neural network for diagnosis of polycystic ovary syndrome using ovarian ultrasound images. <italic toggle=\"yes\">In 15th International Congress on Ultra-Modern Telecommunications and Control Systems and Workshops (ICUMT)</italic> 44&#8211;49 (IEEE, 2023). 44&#8211;49 (IEEE, 2023). (2023). 10.1109/ICUMT61075.2023.10333275</mixed-citation></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Rachana, B. et al. Detection of polycystic ovarian syndrome using follicle recognition technique. <italic toggle=\"yes\">Glob. Transit. Proc.</italic> 2, 304&#8211;308 (2021). 10.1016/j.gltp.2021.08.010</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kiruthika</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Sathiya</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ramya</surname><given-names>MM</given-names></name></person-group><article-title>Machine learning based ovarian detection in ultrasound images</article-title><source>Int. J. Adv. Mechatron. Syst.</source><year>2020</year><volume>8</volume><fpage>109</fpage><lpage>115</lpage><pub-id pub-id-type=\"doi\">10.1504/IJAMECHS.2020.111306</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Kiruthika, V., Sathiya, S. &amp; Ramya, M. M. Machine learning based ovarian detection in ultrasound images. <italic toggle=\"yes\">Int. J. Adv. Mechatron. Syst.</italic><bold>8</bold>, 109&#8211;115. 10.1504/IJAMECHS.2020.111306 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fan</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Y</given-names></name></person-group><article-title>Accurate ovarian cyst classification with a lightweight deep learning model for ultrasound images</article-title><source>IEEE Access.</source><year>2023</year><volume>11</volume><fpage>110681</fpage><lpage>110691</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2023.3321408</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Fan, J., Liu, J., Chen, Q., Wang, W. &amp; Wu, Y. Accurate ovarian cyst classification with a lightweight deep learning model for ultrasound images. <italic toggle=\"yes\">IEEE Access.</italic><bold>11</bold>, 110681&#8211;110691. 10.1109/ACCESS.2023.3321408 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Narmatha</surname><given-names>C</given-names></name><etal/></person-group><article-title>Ovarian cysts classification using novel deep reinforcement learning with Harris Hawks optimization method</article-title><source>J. Supercomput</source><year>2023</year><volume>79</volume><fpage>1374</fpage><lpage>1397</lpage><pub-id pub-id-type=\"doi\">10.1007/s11227-022-04709-8</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Narmatha, C. et al. Ovarian cysts classification using novel deep reinforcement learning with Harris Hawks optimization method. <italic toggle=\"yes\">J. Supercomput</italic>. <bold>79</bold>, 1374&#8211;1397. 10.1007/s11227-022-04709-8 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gopalakrishnan</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Iyapparaja</surname><given-names>MITLCNN</given-names></name></person-group><article-title>Integrated transfer learning-based Convolution neural network for ultrasound PCOS image classification</article-title><source>Int. J. Pattern Recognit. Artif. Intell.</source><year>2022</year><volume>36</volume><fpage>2240002</fpage><pub-id pub-id-type=\"doi\">10.1142/S021800142240002X</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Gopalakrishnan, C. &amp; Iyapparaja, M. I. T. L. C. N. N. Integrated transfer learning-based Convolution neural network for ultrasound PCOS image classification. <italic toggle=\"yes\">Int. J. Pattern Recognit. Artif. Intell.</italic><bold>36</bold>, 2240002. 10.1142/S021800142240002X (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Christiansen</surname><given-names>F</given-names></name><etal/></person-group><article-title>Ultrasound image analysis using deep neural networks for discriminating between benign and malignant ovarian tumors: comparison with expert subjective assessment</article-title><source>Ultrasound Obstet. Gynecol.</source><year>2021</year><volume>57</volume><fpage>155</fpage><lpage>163</lpage><pub-id pub-id-type=\"doi\">10.1002/uog.23530</pub-id><pub-id pub-id-type=\"pmid\">33142359</pub-id><pub-id pub-id-type=\"pmcid\">PMC7839489</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Christiansen, F. et al. Ultrasound image analysis using deep neural networks for discriminating between benign and malignant ovarian tumors: comparison with expert subjective assessment. <italic toggle=\"yes\">Ultrasound Obstet. Gynecol.</italic><bold>57</bold>, 155&#8211;163. 10.1002/uog.23530 (2021).<pub-id pub-id-type=\"pmid\">33142359</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/uog.23530</pub-id><pub-id pub-id-type=\"pmcid\">PMC7839489</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Nabilah, A., Sigit, R., Harsono, T. &amp; Anwar, A. Classification of ovarian cysts on ultrasound images using watershed segmentation and contour analysis. <italic toggle=\"yes\">In 2020 International Electronics Symposium (IES)</italic> 513&#8211;519 (IEEE, 2020). 10.1109/IES50839.2020.9231695</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><etal/></person-group><article-title>CR-Unet: A composite network for ovary and follicle segmentation in ultrasound images</article-title><source>IEEE J. Biomed. Health Inf.</source><year>2019</year><volume>24</volume><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type=\"doi\">10.1109/JBHI.2019.2946092</pub-id><pub-id pub-id-type=\"pmid\">31603808</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Li, H. et al. CR-Unet: A composite network for ovary and follicle segmentation in ultrasound images. <italic toggle=\"yes\">IEEE J. Biomed. Health Inf.</italic><bold>24</bold>, 974&#8211;983. 10.1109/JBHI.2019.2946092 (2019).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JBHI.2019.2946092</pub-id><pub-id pub-id-type=\"pmid\">31603808</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Chitra, P. et al. Classification of ultrasound PCOS image using deep learning-based hybrid models. <italic toggle=\"yes\">In Second International Conference on Electronics and Renewable Systems (ICEARS)</italic> 1389&#8211;1394 (IEEE, 2023). 1389&#8211;1394 (IEEE, 2023). (2023). 10.1109/ICEARS56392.2023.10085400</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jha</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Saxena</surname><given-names>R</given-names></name></person-group><article-title>Noise cancellation of polycystic ovarian syndrome ultrasound images using robust two-dimensional fractional fourier transform filter and VGG-16 model</article-title><source>Int. J. Inf. Technol.</source><year>2024</year><volume>16</volume><fpage>2497</fpage><lpage>2504</lpage><pub-id pub-id-type=\"doi\">10.1007/s41870-024-01773-6</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Jha, M., Gupta, R. &amp; Saxena, R. Noise cancellation of polycystic ovarian syndrome ultrasound images using robust two-dimensional fractional fourier transform filter and VGG-16 model. <italic toggle=\"yes\">Int. J. Inf. Technol.</italic><bold>16</bold>, 2497&#8211;2504. 10.1007/s41870-024-01773-6 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Choubey</surname><given-names>SB</given-names></name><name name-style=\"western\"><surname>Choubey</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Nandan</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Mahajan</surname><given-names>A</given-names></name></person-group><article-title>Polycystic ovarian syndrome detection by using two-stage image denoising</article-title><source>Trait Signal.</source><year>2021</year><volume>38</volume><fpage>1217</fpage><lpage>1227</lpage><pub-id pub-id-type=\"doi\">10.18280/ts.380433</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Choubey, S. B., Choubey, A., Nandan, D. &amp; Mahajan, A. Polycystic ovarian syndrome detection by using two-stage image denoising. <italic toggle=\"yes\">Trait Signal.</italic><bold>38</bold>, 1217&#8211;1227. 10.18280/ts.380433 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Gulhan, P. G., &#214;zmen, G. &amp; Alptekin, H. CNN based determination of polycystic ovarian syndrome using automatic follicle detection methods. <italic toggle=\"yes\">Politeknik Dergisi</italic> 1&#8211;1 (2023). (2023). 10.2339/politeknik.1263520</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Panicker, P. H., Shah, K. &amp; Karamchandani, S. CNN based image descriptor for polycystic ovarian morphology from transvaginal ultrasound. <italic toggle=\"yes\">In International Conference on Communication System, Computing and IT Applications (CSCITA)</italic> 148&#8211;152 (IEEE, 2023). 148&#8211;152 (IEEE, 2023). (2023). 10.1109/CSCITA55725.2023.10104931</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Kaur, N., Gupta, G. &amp; Kaur, P. Transfer-based deep learning technique for PCOS detection using ultrasound images. <italic toggle=\"yes\">In International Conference on Network, Multimedia and Information Technology (NMITCON)</italic> 1&#8211;6 (IEEE, 2023). 1&#8211;6 (IEEE, 2023). (2023). 10.1109/NMITCON58196.2023.10276245</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Gupta, K. &amp; Prasad, R. Polycystic ovary syndrome detection using deep learning. <italic toggle=\"yes\">In 6th International Conference on Contemporary Computing and Informatics (IC3I)</italic> 1465&#8211;1468 (IEEE, 2023). 1465&#8211;1468 (IEEE, 2023). (2023). 10.1109/IC3I59117.2023.10397615</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Nagodavithana, B. &amp; Ullah, A. Diagnosis of polycystic ovarian syndrome (PCOS) using deep learning. In Proceedings of International Conference on Information Technology and Applications (eds. Anwar, S., Ullah, A., Rocha, &#193;. &amp; Sousa, M. J.) <italic toggle=\"yes\">Lecture Notes in Networks and Systems</italic> vol. 614Springer, (2023). 10.1007/978-981-19-9331-2_5</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abouhawwash</surname><given-names>M</given-names></name><etal/></person-group><article-title>Automatic diagnosis of polycystic ovarian syndrome using wrapper methodology with deep learning techniques</article-title><source>Comput. Syst. Sci. Eng.</source><year>2023</year><volume>47</volume><fpage>239</fpage><lpage>253</lpage><pub-id pub-id-type=\"doi\">10.32604/csse.2023.037812</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Abouhawwash, M. et al. Automatic diagnosis of polycystic ovarian syndrome using wrapper methodology with deep learning techniques. <italic toggle=\"yes\">Comput. Syst. Sci. Eng.</italic><bold>47</bold>, 239&#8211;253. 10.32604/csse.2023.037812 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Thufailah</surname><given-names>IF</given-names></name><name name-style=\"western\"><surname>Adiwijaya, Wisesty</surname><given-names>UN</given-names></name><name name-style=\"western\"><surname>Jondri</surname></name></person-group><article-title>An implementation of Elman neural network for polycystic ovary classification based on ultrasound images</article-title><source>J. Phys. Conf. Ser.</source><year>2018</year><volume>971</volume><fpage>012</fpage><lpage>016</lpage><pub-id pub-id-type=\"doi\">10.1088/1742-6596/971/1/012016</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Thufailah, I. F., Adiwijaya, Wisesty, U. N. &amp; Jondri An implementation of Elman neural network for polycystic ovary classification based on ultrasound images. <italic toggle=\"yes\">J. Phys. Conf. Ser.</italic><bold>971</bold>, 012&#8211;016. 10.1088/1742-6596/971/1/012016 (2018).</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Miao</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Tong</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Sheng</surname><given-names>M</given-names></name></person-group><article-title>Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel</article-title><source>J. Vis. Commun. Image Represent</source><year>2019</year><volume>63</volume><fpage>102587</fpage><pub-id pub-id-type=\"doi\">10.1016/j.jvcir.2019.102587</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">He, D., Liu, L., Miao, S., Tong, X. &amp; Sheng, M. Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel. <italic toggle=\"yes\">J. Vis. Commun. Image Represent</italic>. <bold>63</bold>, 102587. 10.1016/j.jvcir.2019.102587 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kumar</surname><given-names>HP</given-names></name><name name-style=\"western\"><surname>Srinivasan</surname><given-names>S</given-names></name></person-group><article-title>Fast automatic segmentation of polycystic ovary in ultrasound images using improved Chan-Vese with split-Bregman optimization</article-title><source>J. Med. Imaging Health Inf.</source><year>2015</year><volume>5</volume><fpage>57</fpage><lpage>62</lpage><pub-id pub-id-type=\"doi\">10.1166/jmihi.2015.1355</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Kumar, H. P. &amp; Srinivasan, S. Fast automatic segmentation of polycystic ovary in ultrasound images using improved Chan-Vese with split-Bregman optimization. <italic toggle=\"yes\">J. Med. Imaging Health Inf.</italic><bold>5</bold>, 57&#8211;62. 10.1166/jmihi.2015.1355 (2015).</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><citation-alternatives><element-citation id=\"ec-CR35\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wisesty</surname><given-names>UN</given-names></name><name name-style=\"western\"><surname>Nasri</surname><given-names>J</given-names></name></person-group><article-title>Adiwijaya. Modified backpropagation algorithm for polycystic ovary syndrome detection based on ultrasound images</article-title><source>Adv. Intell. Syst. Comput.</source><year>2016</year><volume>2</volume><fpage>141</fpage><lpage>151</lpage><pub-id pub-id-type=\"doi\">10.1007/978-3-319-51281-5_15</pub-id></element-citation><mixed-citation id=\"mc-CR35\" publication-type=\"journal\">Wisesty, U. N. &amp; Nasri, J. Adiwijaya. Modified backpropagation algorithm for polycystic ovary syndrome detection based on ultrasound images. <italic toggle=\"yes\">Adv. Intell. Syst. Comput.</italic><bold>2</bold>, 141&#8211;151. 10.1007/978-3-319-51281-5_15 (2016).</mixed-citation></citation-alternatives></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>R</given-names></name><etal/></person-group><article-title>High-resolution single-shot fast spin-echo MR imaging with deep learning reconstruction algorithm can improve repeatability and reproducibility of follicle counting</article-title><source>J. Clin. Med.</source><year>2023</year><volume>12</volume><fpage>3234</fpage><pub-id pub-id-type=\"doi\">10.3390/jcm12093234</pub-id><pub-id pub-id-type=\"pmid\">37176674</pub-id><pub-id pub-id-type=\"pmcid\">PMC10179356</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Yang, R. et al. High-resolution single-shot fast spin-echo MR imaging with deep learning reconstruction algorithm can improve repeatability and reproducibility of follicle counting. <italic toggle=\"yes\">J. Clin. Med.</italic><bold>12</bold>, 3234. 10.3390/jcm12093234 (2023).<pub-id pub-id-type=\"pmid\">37176674</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/jcm12093234</pub-id><pub-id pub-id-type=\"pmcid\">PMC10179356</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><citation-alternatives><element-citation id=\"ec-CR37\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nazarudin</surname><given-names>AA</given-names></name><etal/></person-group><article-title>Performance analysis of a novel hybrid segmentation method for polycystic ovarian syndrome monitoring</article-title><source>Diagnostics</source><year>2023</year><volume>13</volume><fpage>750</fpage><pub-id pub-id-type=\"doi\">10.3390/diagnostics13040750</pub-id><pub-id pub-id-type=\"pmid\">36832237</pub-id><pub-id pub-id-type=\"pmcid\">PMC9954948</pub-id></element-citation><mixed-citation id=\"mc-CR37\" publication-type=\"journal\">Nazarudin, A. A. et al. Performance analysis of a novel hybrid segmentation method for polycystic ovarian syndrome monitoring. <italic toggle=\"yes\">Diagnostics</italic><bold>13</bold>, 750. 10.3390/diagnostics13040750 (2023).<pub-id pub-id-type=\"pmid\">36832237</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/diagnostics13040750</pub-id><pub-id pub-id-type=\"pmcid\">PMC9954948</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR38\"><label>38.</label><citation-alternatives><element-citation id=\"ec-CR38\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kodipalli</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Devi</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Dasar</surname><given-names>S</given-names></name></person-group><article-title>Semantic segmentation and classification of polycystic ovarian disease using attention U-Net, PySpark, and ensemble learning model</article-title><source>Expert Syst.</source><year>2024</year><volume>41</volume><fpage>e13498</fpage><pub-id pub-id-type=\"doi\">10.1111/exsy.13498</pub-id></element-citation><mixed-citation id=\"mc-CR38\" publication-type=\"journal\">Kodipalli, A., Devi, S. &amp; Dasar, S. Semantic segmentation and classification of polycystic ovarian disease using attention U-Net, PySpark, and ensemble learning model. <italic toggle=\"yes\">Expert Syst.</italic><bold>41</bold>, e13498. 10.1111/exsy.13498 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR39\"><label>39.</label><mixed-citation publication-type=\"other\">Poorani, B. &amp; Khilar, R. Identification of polycystic ovary syndrome in ultrasound images of ovaries using distinct threshold based image segmentation. <italic toggle=\"yes\">In International Conference on Advancement in Computation &amp; Computer Technologies (InCACCT)</italic> 570&#8211;575 (IEEE, 2023). 570&#8211;575 (IEEE, 2023). (2023). 10.1109/InCACCT57535.2023.10141800</mixed-citation></ref><ref id=\"CR40\"><label>40.</label><citation-alternatives><element-citation id=\"ec-CR40\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Suganya</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ganesan</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Valarmathi</surname><given-names>P</given-names></name></person-group><article-title>Ultrasound ovary cyst image classification with deep learning neural network with support vector machine</article-title><source>Int. J. Health Sci.</source><year>2022</year><volume>6</volume><fpage>8811</fpage><lpage>8818</lpage><pub-id pub-id-type=\"doi\">10.53730/ijhs.v6ns2.7304</pub-id></element-citation><mixed-citation id=\"mc-CR40\" publication-type=\"journal\">Suganya, Y., Ganesan, S. &amp; Valarmathi, P. Ultrasound ovary cyst image classification with deep learning neural network with support vector machine. <italic toggle=\"yes\">Int. J. Health Sci.</italic><bold>6</bold>, 8811&#8211;8818. 10.53730/ijhs.v6ns2.7304 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Srilatha, K. &amp; Ulagamuthalvi, V. Performance analysis of ultrasound ovarian tumour segmentation using GrabCut and FL-SNNM. <italic toggle=\"yes\">In 2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)</italic> 1&#8211;7IEEE, (2021). 10.1109/ICAECT49130.2021.9392630</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><citation-alternatives><element-citation id=\"ec-CR42\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gopalakrishnan</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Iyapparaja</surname><given-names>M</given-names></name></person-group><article-title>Multilevel thresholding based follicle detection and classification of polycystic ovary syndrome from the ultrasound images using machine learning</article-title><source>Int. J. Syst. Assur. Eng. Manag</source><year>2021</year><pub-id pub-id-type=\"doi\">10.1007/s13198-021-01203-x</pub-id></element-citation><mixed-citation id=\"mc-CR42\" publication-type=\"journal\">Gopalakrishnan, C. &amp; Iyapparaja, M. Multilevel thresholding based follicle detection and classification of polycystic ovary syndrome from the ultrasound images using machine learning. <italic toggle=\"yes\">Int. J. Syst. Assur. Eng. Manag</italic>. 10.1007/s13198-021-01203-x (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yuvaraj</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Preethi</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Sumathi</surname><given-names>AC</given-names></name><name name-style=\"western\"><surname>Preethaa</surname><given-names>KRS</given-names></name></person-group><article-title>Alzheimer disease classification based on multimodel deep convolutional neural network using MRI images</article-title><source>AIP Conf. Proc.</source><year>2023</year><volume>2764</volume><fpage>060008</fpage><pub-id pub-id-type=\"doi\">10.1063/5.0144082</pub-id></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Yuvaraj, N., Preethi, T., Sumathi, A. C. &amp; Preethaa, K. R. S. Alzheimer disease classification based on multimodel deep convolutional neural network using MRI images. <italic toggle=\"yes\">AIP Conf. Proc.</italic><bold>2764</bold>, 060008. 10.1063/5.0144082 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Yuvaraj, N., Mouthami, K., Wadhwa, G., Sundarraj, S. &amp; Srinivasan, S. A. Deep learning system of naturalistic communication in brain&#8211;computer interface for quadriplegic patient. <italic toggle=\"yes\">In Computational Intelligence and Deep Learning Methods for Neuro-rehabilitation Applications</italic> 215&#8211;238Academic Press, (2024). 10.1016/B978-0-443-13772-3.00009-1</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><citation-alternatives><element-citation id=\"ec-CR45\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Thangavel</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Natarajan</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Preethaa</surname><given-names>KRS</given-names></name></person-group><article-title>EAD-DNN: early alzheimer&#8217;s disease prediction using deep neural networks</article-title><source>Biomed. Signal. Process. Control</source><year>2023</year><volume>86</volume><fpage>105215</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2023.105215</pub-id></element-citation><mixed-citation id=\"mc-CR45\" publication-type=\"journal\">Thangavel, P., Natarajan, Y. &amp; Preethaa, K. R. S. EAD-DNN: early alzheimer&#8217;s disease prediction using deep neural networks. <italic toggle=\"yes\">Biomed. Signal. Process. Control</italic>. <bold>86</bold>, 105215. 10.1016/j.bspc.2023.105215 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><label>46.</label><mixed-citation publication-type=\"other\">Rathinakumar, A. P. et al. CovidXDetector: deep learning-based chest abnormality detection for Covid radiography diagnosis. <italic toggle=\"yes\">AIP Conf. Proc.</italic><bold>2853</bold> (020260). 10.1063/5.0197394 (2024).</mixed-citation></ref><ref id=\"CR47\"><label>47.</label><mixed-citation publication-type=\"other\">Choudhari, A. PCOS detection using ultrasound images. <italic toggle=\"yes\">Kaggle</italic> (2023). <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images\">https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images</ext-link></mixed-citation></ref><ref id=\"CR48\"><label>48.</label><citation-alternatives><element-citation id=\"ec-CR48\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hasan</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Bao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Shawon</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name></person-group><article-title>DenseNet convolutional neural networks application for predicting COVID-19 using CT image</article-title><source>SN Comput. Sci.</source><year>2021</year><volume>2</volume><fpage>389</fpage><pub-id pub-id-type=\"doi\">10.1007/s42979-021-00782-7</pub-id><pub-id pub-id-type=\"pmid\">34337432</pub-id><pub-id pub-id-type=\"pmcid\">PMC8300985</pub-id></element-citation><mixed-citation id=\"mc-CR48\" publication-type=\"journal\">Hasan, N., Bao, Y., Shawon, A. &amp; Huang, Y. DenseNet convolutional neural networks application for predicting COVID-19 using CT image. <italic toggle=\"yes\">SN Comput. Sci.</italic><bold>2</bold>, 389. 10.1007/s42979-021-00782-7 (2021).<pub-id pub-id-type=\"pmid\">34337432</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s42979-021-00782-7</pub-id><pub-id pub-id-type=\"pmcid\">PMC8300985</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR49\"><label>49.</label><mixed-citation publication-type=\"other\">Santhi, S. &amp; Chairman, M. Oral disease detection from dental X-ray images using DenseNet. <italic toggle=\"yes\">In 4th International Conference on Inventive Research in Computing Applications (ICIRCA)</italic> 1280&#8211;1286 (IEEE, 2022). 1280&#8211;1286 (IEEE, 2022). (2022). 10.1109/ICIRCA54612.2022.9985687</mixed-citation></ref><ref id=\"CR50\"><label>50.</label><mixed-citation publication-type=\"other\">Sai, S. S. M., Tinnaluri, B. C. &amp; Tella, T. Y. Accurate prediction of classification score using DenseNet for acute pneumonia. <italic toggle=\"yes\">In 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)</italic> 1293&#8211;1299 (IEEE, 2024). 1293&#8211;1299 (IEEE, 2024). (2024). 10.1109/IDCIoT59759.2024.10467683</mixed-citation></ref><ref id=\"CR51\"><label>51.</label><mixed-citation publication-type=\"other\">Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv (2015). <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1409.1556\">http://arxiv.org/abs/1409.1556</ext-link></mixed-citation></ref><ref id=\"CR52\"><label>52.</label><mixed-citation publication-type=\"other\">Hlawa, S. &amp; Romdhane, N. B. Deep learning-based Alzheimer&#8217;s disease prediction for smart health system. <italic toggle=\"yes\">In 3rd International Conference on Distributed Sensing and Intelligent Systems (ICDSIS)</italic> 128&#8211;137 (IET, 2022). 10.1049/icp.2022.2427</mixed-citation></ref><ref id=\"CR53\"><label>53.</label><citation-alternatives><element-citation id=\"ec-CR53\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ravi</surname><given-names>D</given-names></name><etal/></person-group><article-title>Deep learning for health informatics</article-title><source>IEEE J. Biomed. Health Inf.</source><year>2017</year><volume>21</volume><fpage>4</fpage><lpage>21</lpage><pub-id pub-id-type=\"doi\">10.1109/JBHI.2016.2636665</pub-id><pub-id pub-id-type=\"pmid\">28055930</pub-id></element-citation><mixed-citation id=\"mc-CR53\" publication-type=\"journal\">Ravi, D. et al. Deep learning for health informatics. <italic toggle=\"yes\">IEEE J. Biomed. Health Inf.</italic><bold>21</bold>, 4&#8211;21. 10.1109/JBHI.2016.2636665 (2017).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/JBHI.2016.2636665</pub-id><pub-id pub-id-type=\"pmid\">28055930</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR54\"><label>54.</label><citation-alternatives><element-citation id=\"ec-CR54\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Tao</surname><given-names>D</given-names></name></person-group><article-title>Why ResNet works? Residuals generalize</article-title><source>IEEE Trans. Neural Netw. Learn. Syst</source><year>2020</year><volume>31</volume><fpage>5349</fpage><lpage>5362</lpage><pub-id pub-id-type=\"doi\">10.1109/TNNLS.2020.2966319</pub-id><pub-id pub-id-type=\"pmid\">32031953</pub-id></element-citation><mixed-citation id=\"mc-CR54\" publication-type=\"journal\">He, F., Liu, T. &amp; Tao, D. Why ResNet works? Residuals generalize. <italic toggle=\"yes\">IEEE Trans. Neural Netw. Learn. Syst</italic>. <bold>31</bold>, 5349&#8211;5362. 10.1109/TNNLS.2020.2966319 (2020).<pub-id pub-id-type=\"pmid\">32031953</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNNLS.2020.2966319</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR55\"><label>55.</label><citation-alternatives><element-citation id=\"ec-CR55\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Dou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Bao</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>Diffusion mechanism in residual neural network: theory and applications</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>667</fpage><lpage>680</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2023.3272341</pub-id><pub-id pub-id-type=\"pmid\">37130245</pub-id></element-citation><mixed-citation id=\"mc-CR55\" publication-type=\"journal\">Wang, T., Dou, Z., Bao, C. &amp; Shi, Z. Diffusion mechanism in residual neural network: theory and applications. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>46</bold>, 667&#8211;680. 10.1109/TPAMI.2023.3272341 (2024).<pub-id pub-id-type=\"pmid\">37130245</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2023.3272341</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR56\"><label>56.</label><citation-alternatives><element-citation id=\"ec-CR56\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Suri</surname><given-names>JS</given-names></name><etal/></person-group><article-title>U-Net deep learning architecture for segmentation of vascular and non-vascular images: A microscopic look at U-Net components buffered with pruning, explainable artificial intelligence, and bias</article-title><source>IEEE Access.</source><year>2023</year><volume>11</volume><fpage>595</fpage><lpage>645</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2022.3232561</pub-id></element-citation><mixed-citation id=\"mc-CR56\" publication-type=\"journal\">Suri, J. S. et al. U-Net deep learning architecture for segmentation of vascular and non-vascular images: A microscopic look at U-Net components buffered with pruning, explainable artificial intelligence, and bias. <italic toggle=\"yes\">IEEE Access.</italic><bold>11</bold>, 595&#8211;645. 10.1109/ACCESS.2022.3232561 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR57\"><label>57.</label><citation-alternatives><element-citation id=\"ec-CR57\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>LT</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y</given-names></name></person-group><article-title>Attention U-Net based on Bi-ConvLSTM and its optimization for smart healthcare</article-title><source>IEEE Trans. Comput. Soc. Syst.</source><year>2023</year><volume>10</volume><fpage>1966</fpage><lpage>1974</lpage><pub-id pub-id-type=\"doi\">10.1109/TCSS.2023.3237923</pub-id></element-citation><mixed-citation id=\"mc-CR57\" publication-type=\"journal\">Gao, Y., Yang, L. T., Yang, J., Wang, H. &amp; Zhao, Y. Attention U-Net based on Bi-ConvLSTM and its optimization for smart healthcare. <italic toggle=\"yes\">IEEE Trans. Comput. Soc. Syst.</italic><bold>10</bold>, 1966&#8211;1974. 10.1109/TCSS.2023.3237923 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR58\"><label>58.</label><mixed-citation publication-type=\"other\">Wu, J., Niu, Y., Ling, Z., Zhu, J. &amp; Gou, F. Pathological image segmentation method based on multiscale and dual attention. <italic toggle=\"yes\">Int. J. Intell. Syst.</italic><bold>2024</bold> (9987190). 10.1155/int/9987190 (2024).</mixed-citation></ref><ref id=\"CR59\"><label>59.</label><mixed-citation publication-type=\"other\">Xu, X., Yue, X., Huang, Z., Wang, Q. &amp; Liu, Y. Melanoma image segmentation based on improved TransUnet. In Proc. 7th Int. Conf. Artif. Intell. Pattern Recognit. (AIPR &#8217;24) 139&#8211;147ACM, New York, NY, USA, (2025). 10.1145/3703935.3703990</mixed-citation></ref><ref id=\"CR60\"><label>60.</label><citation-alternatives><element-citation id=\"ec-CR60\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>J</given-names></name></person-group><article-title>High-order paired-ASPP for deep semantic segmentation networks</article-title><source>Inf. Sci.</source><year>2023</year><volume>646</volume><fpage>119364</fpage><pub-id pub-id-type=\"doi\">10.1016/j.ins.2023.119364</pub-id></element-citation><mixed-citation id=\"mc-CR60\" publication-type=\"journal\">Sun, X., Zhang, Y., Chen, C., Xie, S. &amp; Dong, J. High-order paired-ASPP for deep semantic segmentation networks. <italic toggle=\"yes\">Inf. Sci.</italic><bold>646</bold>, 119364. 10.1016/j.ins.2023.119364 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR61\"><label>61.</label><citation-alternatives><element-citation id=\"ec-CR61\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dhiyanesh</surname><given-names>B</given-names></name><etal/></person-group><article-title>EnsembleEdgeFusion: advancing semantic segmentation in microvascular decompression imaging with innovative ensemble techniques</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>17892</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-02470-5</pub-id><pub-id pub-id-type=\"pmid\">40410312</pub-id><pub-id pub-id-type=\"pmcid\">PMC12102392</pub-id></element-citation><mixed-citation id=\"mc-CR61\" publication-type=\"journal\">Dhiyanesh, B. et al. EnsembleEdgeFusion: advancing semantic segmentation in microvascular decompression imaging with innovative ensemble techniques. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold>, 17892. 10.1038/s41598-025-02470-5 (2025).<pub-id pub-id-type=\"pmid\">40410312</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-02470-5</pub-id><pub-id pub-id-type=\"pmcid\">PMC12102392</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR62\"><label>62.</label><citation-alternatives><element-citation id=\"ec-CR62\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Dilated SE-DenseNet for brain tumor MRI classification</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>3596</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-86752-y</pub-id><pub-id pub-id-type=\"pmid\">39875423</pub-id><pub-id pub-id-type=\"pmcid\">PMC11775108</pub-id></element-citation><mixed-citation id=\"mc-CR62\" publication-type=\"journal\">Mao, Y. et al. Dilated SE-DenseNet for brain tumor MRI classification. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold>, 3596. 10.1038/s41598-025-86752-y (2025).<pub-id pub-id-type=\"pmid\">39875423</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-86752-y</pub-id><pub-id pub-id-type=\"pmcid\">PMC11775108</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR63\"><label>63.</label><citation-alternatives><element-citation id=\"ec-CR63\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Reka</surname><given-names>S</given-names></name><etal/></person-group><article-title>Automated high precision PCOS detection through a segment anything model on super resolution ultrasound ovary images</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>16832</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-01744-2</pub-id><pub-id pub-id-type=\"pmid\">40369044</pub-id><pub-id pub-id-type=\"pmcid\">PMC12078606</pub-id></element-citation><mixed-citation id=\"mc-CR63\" publication-type=\"journal\">Reka, S. et al. Automated high precision PCOS detection through a segment anything model on super resolution ultrasound ovary images. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold>, 16832. 10.1038/s41598-025-01744-2 (2025).<pub-id pub-id-type=\"pmid\">40369044</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-01744-2</pub-id><pub-id pub-id-type=\"pmcid\">PMC12078606</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR64\"><label>64.</label><mixed-citation publication-type=\"other\">Zhang, J. et al. HR-ASPP: An improved semantic segmentation model of cervical nucleus images with accurate spatial localization and better shape feature extraction based on Deeplabv3+. In Proc. 15th Int. Conf. Digit. Image Process. (ICDIP &#8217;23), Article 16, 1&#8211;8ACM, New York, NY, USA, (2023). 10.1145/3604078.3604094</mixed-citation></ref><ref id=\"CR65\"><label>65.</label><citation-alternatives><element-citation id=\"ec-CR65\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhong</surname><given-names>L</given-names></name><etal/></person-group><article-title>DSU-Net: Dual-Stage U-Net based on CNN and transformer for skin lesion segmentation</article-title><source>Biomed. Signal. Process. Control</source><year>2024</year><volume>100</volume><fpage>107090</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.107090</pub-id></element-citation><mixed-citation id=\"mc-CR65\" publication-type=\"journal\">Zhong, L. et al. DSU-Net: Dual-Stage U-Net based on CNN and transformer for skin lesion segmentation. <italic toggle=\"yes\">Biomed. Signal. Process. Control</italic>. <bold>100</bold>, 107090. 10.1016/j.bspc.2024.107090 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR66\"><label>66.</label><citation-alternatives><element-citation id=\"ec-CR66\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Hua</surname><given-names>Z</given-names></name></person-group><article-title>DEAU-Net: attention networks based on dual encoder for medical image segmentation</article-title><source>Comput. Biol. Med.</source><year>2022</year><volume>150</volume><fpage>106197</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2022.106197</pub-id><pub-id pub-id-type=\"pmid\">37859289</pub-id></element-citation><mixed-citation id=\"mc-CR66\" publication-type=\"journal\">Fu, Z., Li, J. &amp; Hua, Z. DEAU-Net: attention networks based on dual encoder for medical image segmentation. <italic toggle=\"yes\">Comput. Biol. Med.</italic><bold>150</bold>, 106197. 10.1016/j.compbiomed.2022.106197 (2022).<pub-id pub-id-type=\"pmid\">37859289</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2022.106197</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR67\"><label>67.</label><citation-alternatives><element-citation id=\"ec-CR67\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Advances in attention mechanisms for medical image segmentation</article-title><source>Comput. Sci. Rev.</source><year>2025</year><volume>56</volume><fpage>100721</fpage><pub-id pub-id-type=\"doi\">10.1016/j.cosrev.2024.100721</pub-id></element-citation><mixed-citation id=\"mc-CR67\" publication-type=\"journal\">Zhang, J. et al. Advances in attention mechanisms for medical image segmentation. <italic toggle=\"yes\">Comput. Sci. Rev.</italic><bold>56</bold>, 100721. 10.1016/j.cosrev.2024.100721 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR68\"><label>68.</label><citation-alternatives><element-citation id=\"ec-CR68\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Moral</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Mustafi</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Mustafi</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sahana</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>CystNet</surname></name></person-group><article-title>An AI driven model for PCOS detection using multilevel thresholding of ultrasound images</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>25012</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-75964-3</pub-id><pub-id pub-id-type=\"pmid\">39443622</pub-id><pub-id pub-id-type=\"pmcid\">PMC11499604</pub-id></element-citation><mixed-citation id=\"mc-CR68\" publication-type=\"journal\">Moral, P., Mustafi, D., Mustafi, A., Sahana, S. K. &amp; CystNet An AI driven model for PCOS detection using multilevel thresholding of ultrasound images. <italic toggle=\"yes\">Sci. Rep.</italic><bold>14</bold> (1), 25012. 10.1038/s41598-024-75964-3 (2024).<pub-id pub-id-type=\"pmid\">39443622</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-75964-3</pub-id><pub-id pub-id-type=\"pmcid\">PMC11499604</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR69\"><label>69.</label><citation-alternatives><element-citation id=\"ec-CR69\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sundari</surname><given-names>MS</given-names></name><etal/></person-group><article-title>Transfer learning-enhanced CNN model for integrative ultrasound and biomarker-based diagnosis of polycystic ovarian disease</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><issue>1</issue><fpage>34519</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-025-17711-w</pub-id><pub-id pub-id-type=\"pmid\">41044309</pub-id><pub-id pub-id-type=\"pmcid\">PMC12494765</pub-id></element-citation><mixed-citation id=\"mc-CR69\" publication-type=\"journal\">Sundari, M. S. et al. Transfer learning-enhanced CNN model for integrative ultrasound and biomarker-based diagnosis of polycystic ovarian disease. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold> (1), 34519. 10.1038/s41598-025-17711-w (2025).<pub-id pub-id-type=\"pmid\">41044309</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-17711-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC12494765</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR70\"><label>70.</label><citation-alternatives><element-citation id=\"ec-CR70\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sarkar</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Mandal</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Tudu</surname><given-names>A</given-names></name></person-group><article-title>DC-UNet: looking for follicles in the ovarian ultrasound images</article-title><source>Frankl. Open.</source><year>2024</year><volume>8</volume><fpage>100149</fpage><pub-id pub-id-type=\"doi\">10.1016/j.fraope.2024.100149</pub-id></element-citation><mixed-citation id=\"mc-CR70\" publication-type=\"journal\">Sarkar, M., Mandal, A. &amp; Tudu, A. DC-UNet: looking for follicles in the ovarian ultrasound images. <italic toggle=\"yes\">Frankl. Open.</italic><bold>8</bold>, 100149. 10.1016/j.fraope.2024.100149 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR71\"><label>71.</label><citation-alternatives><element-citation id=\"ec-CR71\" publication-type=\"journal\"><person-group person-group-type=\"author\"><collab>71</collab><name name-style=\"western\"><surname>Sha</surname><given-names>M</given-names></name></person-group><article-title>Segmentation of ovarian cyst in ultrasound images using AdaResU-net with optimization algorithm and deep learning model</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>18868</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-69427-y</pub-id><pub-id pub-id-type=\"pmid\">39143122</pub-id><pub-id pub-id-type=\"pmcid\">PMC11325020</pub-id></element-citation><mixed-citation id=\"mc-CR71\" publication-type=\"journal\">71 &amp; Sha, M. Segmentation of ovarian cyst in ultrasound images using AdaResU-net with optimization algorithm and deep learning model. <italic toggle=\"yes\">Sci. Rep.</italic><bold>14</bold> (1), 18868. 10.1038/s41598-024-69427-y (2024).<pub-id pub-id-type=\"pmid\">39143122</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-69427-y</pub-id><pub-id pub-id-type=\"pmcid\">PMC11325020</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR72\"><label>72.</label><citation-alternatives><element-citation id=\"ec-CR72\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Lv</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>SAR-U-Net</surname></name></person-group><article-title>Squeeze-and-excitation block and atrous Spatial pyramid pooling based residual U-Net for automatic liver segmentation in computed tomography</article-title><source>Comput. Methods Programs Biomed.</source><year>2021</year><volume>208</volume><fpage>106268</fpage><pub-id pub-id-type=\"doi\">10.1016/j.cmpb.2021.106268</pub-id><pub-id pub-id-type=\"pmid\">34274611</pub-id></element-citation><mixed-citation id=\"mc-CR72\" publication-type=\"journal\">Wang, J., Lv, P., Wang, H., Shi, C. &amp; SAR-U-Net Squeeze-and-excitation block and atrous Spatial pyramid pooling based residual U-Net for automatic liver segmentation in computed tomography. <italic toggle=\"yes\">Comput. Methods Programs Biomed.</italic><bold>208</bold>, 106268. 10.1016/j.cmpb.2021.106268 (2021).<pub-id pub-id-type=\"pmid\">34274611</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.cmpb.2021.106268</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR73\"><label>73.</label><citation-alternatives><element-citation id=\"ec-CR73\" publication-type=\"journal\"><person-group person-group-type=\"author\"><collab>73</collab><name name-style=\"western\"><surname>Hekal</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Elnakib</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Moustafa</surname><given-names>HE</given-names></name><name name-style=\"western\"><surname>Amer</surname><given-names>HM</given-names></name></person-group><article-title>Breast cancer segmentation from ultrasound images using deep Dual-Decoder technology with attention network</article-title><source>IEEE Access.</source><year>2024</year><volume>12</volume><fpage>10087</fpage><lpage>10101</lpage><pub-id pub-id-type=\"doi\">10.1109/access.2024.3351564</pub-id></element-citation><mixed-citation id=\"mc-CR73\" publication-type=\"journal\">73, Hekal, A. A., Elnakib, A., Moustafa, H. E. &amp; Amer, H. M. Breast cancer segmentation from ultrasound images using deep Dual-Decoder technology with attention network. <italic toggle=\"yes\">IEEE Access.</italic><bold>12</bold>, 10087&#8211;10101. 10.1109/access.2024.3351564 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR74\"><label>74.</label><mixed-citation publication-type=\"other\">Sun, G. et al. DA-TransUNet: integrating Spatial and channel dual attention with transformer U-net for medical image segmentation. <italic toggle=\"yes\">Front. Bioeng. Biotechnol.</italic><bold>12</bold>10.3389/fbioe.2024.1398237 (2024).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fbioe.2024.1398237</pub-id><pub-id pub-id-type=\"pmcid\">PMC11141164</pub-id><pub-id pub-id-type=\"pmid\">38827037</pub-id></mixed-citation></ref><ref id=\"CR75\"><label>75.</label><citation-alternatives><element-citation id=\"ec-CR75\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname></name><name name-style=\"western\"><surname>Zou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>PX</given-names></name></person-group><article-title>Hybrid dilation and attention residual U-Net for medical image segmentation</article-title><source>Comput. Biol. Med.</source><year>2021</year><volume>134</volume><fpage>104449</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2021.104449</pub-id><pub-id pub-id-type=\"pmid\">33993015</pub-id></element-citation><mixed-citation id=\"mc-CR75\" publication-type=\"journal\">Wang, Zou, Z., Liu, P. X. &amp; Y., &amp; Hybrid dilation and attention residual U-Net for medical image segmentation. <italic toggle=\"yes\">Comput. Biol. Med.</italic><bold>134</bold>, 104449. 10.1016/j.compbiomed.2021.104449 (2021).<pub-id pub-id-type=\"pmid\">33993015</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2021.104449</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR76\"><label>76.</label><citation-alternatives><element-citation id=\"ec-CR76\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chowdhury</surname><given-names>SR</given-names></name><etal/></person-group><article-title>Atrous attention U-Net with repeated ASPP for medical image segmentation</article-title><source>ArXiv Preprint arXiv:2501 13129</source><year>2025</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2501.13129</pub-id></element-citation><mixed-citation id=\"mc-CR76\" publication-type=\"journal\">Chowdhury, S. R. et al. Atrous attention U-Net with repeated ASPP for medical image segmentation. <italic toggle=\"yes\">ArXiv Preprint arXiv:2501 13129</italic>. 10.48550/arXiv.2501.13129 (2025).</mixed-citation></citation-alternatives></ref><ref id=\"CR77\"><label>77.</label><citation-alternatives><element-citation id=\"ec-CR77\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Umapathy</surname><given-names>SS</given-names></name><name name-style=\"western\"><surname>Alhajlah</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Almutairi</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Aslam</surname><given-names>F</given-names></name></person-group><article-title>F-Net: follicles net an efficient tool for the diagnosis of polycystic ovarian syndrome using deep learning techniques</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><issue>8</issue><fpage>e0307571</fpage><pub-id pub-id-type=\"doi\">10.1371/journal.pone.030757</pub-id><pub-id pub-id-type=\"pmid\">39146307</pub-id><pub-id pub-id-type=\"pmcid\">PMC11326594</pub-id></element-citation><mixed-citation id=\"mc-CR77\" publication-type=\"journal\">Umapathy, S. S., Alhajlah, S., Almutairi, O. &amp; Aslam, F. F-Net: follicles net an efficient tool for the diagnosis of polycystic ovarian syndrome using deep learning techniques. <italic toggle=\"yes\">PLoS ONE</italic>. <bold>19</bold> (8), e0307571. 10.1371/journal.pone.030757 (2024).<pub-id pub-id-type=\"pmid\">39146307</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1371/journal.pone.0307571</pub-id><pub-id pub-id-type=\"pmcid\">PMC11326594</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR78\"><label>78.</label><mixed-citation publication-type=\"other\">Pulluparambil, S. J. &amp; B, S. B. Detection and prediction of polycystic ovary syndrome using Attention-Based CNN-RNN classification model. <italic toggle=\"yes\">Int. J. Adv. Comput. Sci. Appl.</italic><bold>16</bold> (2). 10.14569/ijacsa.2025.0160270 (2025).</mixed-citation></ref><ref id=\"CR79\"><label>79.</label><mixed-citation publication-type=\"other\">Agirsoy, M. &amp; Oehlschlaeger, M. A. A machine learning approach for non-invasive PCOS diagnosis from ultrasound and clinical features. <italic toggle=\"yes\">Sci. Rep.</italic><bold>15</bold> (1). 10.1038/s41598-025-10453-9 (2025).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-10453-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC12479987</pub-id><pub-id pub-id-type=\"pmid\">41022847</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12660356 PMC12660356.1 12660356 12660356 41309699 10.1038/s41598-025-26314-4 26314 1 Article Feature fusion context attention gate UNet for detection of polycystic ovary syndrome Natarajan Yuvaraj 1 K. R. Sri Preethaa sripreethaa.kr@vit.ac.in 1 M. Shyamala Devi 2 1 https://ror.org/00qzypv28 grid.412813.d 0000 0001 0687 4946 School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu 632014 India 2 https://ror.org/040c17130 grid.258803.4 0000 0001 0661 1556 Department of Robot and Smart System Engineering, Kyungpook National University, 80, Daehak-ro, Buk-gu, Daegu, 41566 Republic of Korea 27 11 2025 2025 15 478255 42386 20 8 2025 28 10 2025 27 11 2025 29 11 2025 29 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . Polycystic Ovary Syndrome (PCOS) is a prevalent endocrine disorder affecting women of reproductive age, characterized by hormonal imbalance, irregular menstrual cycles, and ovarian cysts. Traditional diagnostic approaches, which include clinical evaluations, radiological studies, and surgical interventions, are often time-consuming, costly, and not always reliable. To improve the accuracy and efficiency of PCOS diagnosis, this research introduces the Feature Fusion Context Attention U-Net (FCAU-Net) model, leveraging deep learning (DL) techniques. This study makes two key contributions. First, it enhances dataset preparation through Fuzzy Contrast Enhanced (FCE) imaging. Second, it integrates a Feature Fusion Context (FFC) module into the Attention U-Net model, optimizing the extraction of context and position weights from feature maps for better classification performance. An openly available PCOS Ultrasound Image Dataset with 3,800 images was partitioned with 80: 20 to ensure that only original images were used for testing, while augmented samples were exclusively utilized for training to enhance model generalization and robustness. The remaining 3040 images was augmented to form 45,600 images and split into training and validation sets in an 80:20 ratio. The augmented images were processed and tested with several DL models, including DenseNet, AlexNet, VGG19, ResNet, U-Net, and Attention U-Net. Among these, the Attention U-Net initially achieved over 80% accuracy in detecting PCOS. The proposed FCAU-Net, which incorporates the FFC module, demonstrated superior performance, achieving a detection accuracy of 99.89%, significantly outperforming existing DL models. This research highlights the potential of FCAU-Net in providing a more accurate and efficient tool for the diagnosis of PCOS. Keywords Polycystic ovary syndrome (PCOS) DL Feature fusion context attention U-Net (FCAU-Net) Ultrasound imaging Medical image classification Subject terms Computational biology and bioinformatics Diseases Endocrinology Health care Mathematics and computing Medical research Vellore Institute of Technology, Vellore Open access funding provided by Vellore Institute of Technology. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Polycystic Ovary Syndrome (PCOS) is a major medical condition with a high prevalence of morbidity and mortality across the world. Proper care and prevention of related problems including diabetes, heart disease, and infertility depend on early identification of PCOS. Since PCOS appears differently in each person, developing a consistent set of diagnostic criteria is difficult. PCOS diagnostic requirements can cause inconsistent diagnosis amongst medical professionals and different regions, which can affect the precision of treatment plans and occurrence estimations. Medical practitioners may find it difficult to diagnose PCOS because of a number of factors including a patient&#8217;s history, complex hormonal profiles, overlap with other conditions, subjective ultrasound interpretation, diagnostic delay, and varied symptoms. A typical technique for visualizing ovarian morphology in PCOS patients is transvaginal ultrasonography. Interpreting ultrasonography results, such as ovarian volume and cyst presence, can be subjective and operator-specific. The accuracy and consistency of the PCOS diagnosis may be compromised by this variation. DL has the potential to improve diagnostic consistency and accuracy by providing a consistent, objective method of evaluating large, complicated information, such as clinical data and medical images. In recent years, there has been a fast development of DL techniques, which have been employed in medical image analysis and disease classification. DL has demonstrated outstanding performance in a number of medical specialties, including skin diseases, pathological conditions, and radiography. DL models may use massive data sets and complex algorithms to uncover hidden patterns and characteristics from medical imaging. The adaptation of DL methods may result in faster and more accurate diagnosis times. The neural networks are used in SL that extract knowledge from massive volumes of data. Complex characteristics may be automatically extracted from medical images using DL models, which may help with the objective interpretation of ultrasound scans used to diagnose PCOS. In terms of PCOS, DL offers promising opportunities to improve and automate PCOS screening. Automatic PCOS identification speeds up diagnosis and enhances medical practitioners&#8217; diagnostic consistency. Medical professionals may minimize disparities between radiologists&#8217; diagnoses by employing DL technology to automate and standardize the PCOS detection process. This consistency increases the standard of care given to patients, reduces the incidence of diagnostic mistakes, and increases the precision of diagnoses. CNNs that focus on segmentation, like AResUNet, CR-UNet, and Ocys-Net, have been investigated in recent studies on PCOS detection using ultrasound imaging. These CNNs mainly improve follicle and cyst localization, but they frequently have difficulties with generalized feature representation. Although deep CNN-based classifiers such as VGG16, EfficientNetB6, and SqueezeNet have demonstrated remarkable classification accuracy, their ability to adapt to noisy clinical ultrasound pictures is limited due to their heavy reliance on transfer learning and large-scale data. Although they offer better diagnostic robustness, hybrid models that combine CNNs with ensemble learners (such as SVM, RF, or BiLSTM) or feature selection methods have a greater computational cost. Elman NN and PCOS-WaveConvNet, two wavelet and spectral-based models, have improved spatial-spectral extraction but lack precise contextual awareness. While attention-based designs such as ASPPNet and Attention U-Net have demonstrated superior multiscale feature learning, they frequently lack in their ability to successfully incorporate position-sensitive information and contextual dependencies. With the above challenges, this research proposes FCAU-Net model that offers the PCOS detection with high accuracy. To address these limitations, the proposed FCAU-Net introduce an innovative FFC module that adaptively combines spatial, positional, and contextual cues from feature maps, while FCE preprocessing enhances cystic boundary visibility. Paper organization and research contribution This research paper is organized as follows: Sect.&#160; 2 explores the background literature review. Section&#160; 3 deals with the design of proposed FCAU-Net model research methodology. Section&#160; 4 covers the mathematical modeling of the proposed FCAU-Net model. Section&#160; 5 discusses the results and implementation analysis of the proposed FCAU-Net model. In the end, Sect.&#160;6 concludes the proposed FCAU-Net model that includes the challenges, novelty and Future work. This research primarily contributes in two ways. (i) Fuzzy Contrast Enhancement : The first contribution focuses on preparing datasets through image cropping and feature improvement using an adaption of the FCE image. FCE is the novel preprocessing technique that improves the quality of ultrasound images by enhancing contrast and reducing ambiguity, thereby facilitating more accurate feature extraction. (ii) Feature Fusion Context Attention U-Net : The second contribution emphasizes on integrating FFC module into the Attention U-Net model for supervised learning towards detecting PCOS. Proposes enhanced Attention U-Net architecture FCAU-Net that is integrated with a FFC module that effectively captures both contextual and positional information from feature maps, leading to superior diagnostic performance. The FFC module extracts the Context and position weights of the Feature Maps (FM) that optimizes the deep and shallow features of FM as shown in Fig.&#160; 1 . Fig. 1 Feature fusion context module. Background study Several DL and machine learning approaches have been explored for the detection and classification of PCOS using medical imaging, particularly ultrasound. Early studies applied CNN-based architectures with segmentation and handcrafted preprocessing techniques, such as adaptive bilateral filtering, Otsu thresholding, watershed segmentation, and Gabor wavelet-based feature extraction, to enhance cyst and follicle identification. Models such as AResUNet and CR-UNet demonstrated improved noise reduction, robustness against low-contrast images, and better adaptation to multimodal inputs. The following Table&#160; 1 presents a structured overview of existing works on PCOS detection and related medical image analysis, highlighting the methodology, imaging modality, preprocessing techniques, and performance outcomes. Table 1 Key outcomes from the related works. Model Preprocessing Approach Key outcome Segmentation-focused CNN: AResUNet 1 , CR-UNet 22 , Ocys-Net 17 , CNN with Segmentation 9 , Threshold and Watershed 21 , 39 , Hybrid 37 , GrabCut 41 Filtering, Otsu, contour, watershed Deep U-Net variants, reverse bottleneck, watershed and contour-based segmentation Improved follicle, cyst localization and tracking Deep CNN classifiers: VGG16 10 , SqueezeNet 26 , CNN&#8201;+&#8201;FC 27 , ITL-CNN 19 , InceptionV3 7 , 28 , EfficientNetB6 38 Ultrasound preprocessing, TL, normalization CNN-based classification with batch norm, TL, activation optimization Higher PCOS detection and classification accuracy Hybrid DL models: VGG16&#8201;+&#8201;Ensemble 8 , 20 , CNN with BiLSTM 12 , CNN with KNN 15 , CNN with SVM, DT, RF 29 , 40 , Sequential 2D CNN with FS 31 Feature selection, ensemble integration CNN fused with ensembles, clustering, sequential learning, wrapper FS Robust diagnosis across diverse datasets Wavelet and spectral approaches: PCOS-WaveConvNet 6 , Elman NN with Wavelet 32 , BPA with Gabor 35 Spectral domain, Gabor wavelets Wavelet-based ConvNets, Elman NNs, Gabor filters Strong spectral, spatial feature extraction GAN and augmentation methods: GAN with CNN 4 Data augmentation Generative adversarial networks with CNN classifier Overcame overfitting, improved generalization Feature-driven and hybrid Methods (ESDPCOS 13 , AMCNN 14 , MLOD 16 , GIST-MDR 42 , Probabilistic PCOS 33 Ultrasound texture, GLCM, fuzzy logic Hybrid models combining CNN with GLCM, fuzzy logic, texture scoring Enhanced robustness in classification Attention and context learning (ASPPNet&#8201;+&#8201;ResNet 11 , AMCNN 14 , EfficientNetB6 with Attention UNet 38 Ultrasound Attention mechanisms, dilated conv, spatial pyramid pooling Better multiscale learning and context capture Other imaging modalities: Ovarian quantification 2 , Eye scleral biomarkers 3 , MRI follicle count 36 Ultrasound, scleral, MRI Quantification of follicles, scleral biomarkers, MRI follicle mapping Multimodal biomarkers for improved PCOS detection To address the limitations of conventional CNNs, attention mechanisms and multi-scale learning architectures were introduced. Examples include ASPPNet, AMCNN, and ensemble frameworks combining EfficientNet and Attention U-Net, which enabled better contextual feature extraction and improved segmentation efficiency. Similarly, transfer learning strategies with InceptionV3, VGG16, and ResNet variants allowed reuse of pre-trained weights for enhanced classification performance on limited datasets. Hybrid approaches also emerged to overcome challenges of feature uncertainty and overfitting. These integrated deep models with traditional techniques such as fuzzy logic, SVMs, clustering, or wavelets. For instance, CNNs combined with fuzzy layers or KNN clustering improved feature reliability, while GAN-based augmentation addressed overfitting by generating synthetic data. Other models, such as PCOS-WaveConvNet and Ocys-Net, explored wavelet transforms and reverse bottleneck designs for richer feature representation. Recent advancements focused on ensemble and optimization-driven methods, including stacking models that merged VGG16, ResNet50, and MobileNet, or HHO-DQN frameworks that optimized hyperparameters for deep networks. Additionally, segmentation-driven workflows, such as hybrid Otsu-Chanvese segmentation, GrabCut with fuzzy SNN models, and probabilistic grid-based analysis, provided more precise localization of follicles. Overall, the existing works achieved varying levels of accuracy, but faced persistent challenges on handcrafted preprocessing, and limited robustness across datasets. This underscores the need for an end-to-end, adaptive, and context-aware architecture, which motivates the development of proposed FCAU-Net. The DL applications towards health care with pre-trained CNN models can be explored 43 &#8211; 46 . The inferences, advantages and the limitations from literature survey were shown in Table 2 . Table 2 Inferences from literature Review. Methodology and Inference Advantages Limitations Sequential CNN Fuzzy CNN 5 , Confluence CNN 12 , ESDPCOS 13 , AMCNN 14 , KNN based CNN 15 , MLOD 16 , Ocys-Net 17 , ITL-CNN 19 , Ensemble CNN 20 , Hybrid CNN 23 , Sequential CNN 27 , 2D CNN 29 , Tri-stage wrapper CNN 31 , SSFSE-DL 36 , DLNNSVM 40 , GrabCut and Fuzzy Logic -SNN Model 41 Best in handling images with permissible noise and scalable to apply the same model to any large number of datasets Easy to integrate with other CNN architectures or combined with TL Image pixels and the ROI are identified uniquely in each convolution layer operation The model should be refined with labelled data Need to validate with various cross-validation methods to fine-tune the accuracy Need separate validation to handle augmentation images Depends on feature filtering to extract deep features from the ultrasound images Recurrent CNN Complex Spatial Recurrent Neural Network U-Net 22 , Elman NN 32 , Back Propagation Algorithm 35 Spatial relationship between image pixels is effectively correlated Relationship between the image frames is reused Effectively extract the patterns in ultrasound image Best suited for large dataset Less performance while handling image with ling sequences and cannot handle static image data Depends on effective feature extraction methods Pre-Trained CNN AResUNet 1 , Inception CNN 2 , ResNet 4 , VGGNet 4 , Inception V3 4 , 28 , PCOS-WaveConvNet 6 , PCONet 7 , VGG16 8 , 9 , 24 , 30 , ASPPNet 10 , HHO-DQN 18 , SqueezeNet 26 , UNet and EfficientNet 38 , GIST-MDR 42 , U-Net 3 and ResNet 3 AResUNet achieves 98% of accuracy Inception CNN achieves 84.81% of accuracy Exhibits good performance with little amount of data Best in extracting the deep features Model need to be fine-tuned with several activation function and number of layers to improve the performance Depends on balanced data before fitting the model Pathological image segmentation method 58 based on multiscale and dual attention mechanisms, aiming to enhance feature representation and improve segmentation precision. The multiscale module allows the network to capture both global contextual information and fine-grained local details, while the dual attention mechanism emphasizes salient spatial and channel-wise features, reducing the influence of irrelevant regions. Improved TransUnet framework 59 for melanoma image segmentation on integrating transformer-based global context modeling with enhanced convolutional modules could capture fine-grained lesion details with high accuracy. The high-order paired-ASPP (Atrous Spatial Pyramid Pooling) Network 60 enhances the semantic segmentation by effectively capturing both global context and fine local structures. By leveraging high-order feature interactions and a paired atrous spatial pyramid pooling design, the method improves boundary delineation and reduces semantic ambiguity. EnsembleEdgeFusion framework 61 designed to advance semantic segmentation in microvascular decompression imaging. By integrating multiple segmentation models with edge-aware fusion strategies, the method enhances boundary precision and structural consistency in complex medical images. Dilated SE-DenseNet 62 framework classifies the brain tumor using MRI scans. By combining dilated convolutions with squeeze-and-excitation modules, the model captures multi-scale contextual information while adaptively emphasizing the most relevant features. The automated framework for high-precision PCOS detection that leverages the Segment Anything Model (SAM) 63 applied to super-resolution ultrasound ovary images. By combining advanced segmentation with image enhancement, the method achieves more accurate follicle boundary delineation and improved feature representation. HR-ASPP 64 , an enhanced semantic segmentation model for cervical nucleus images builds on DeepLabv3 + with improved atrous spatial pyramid pooling. By focusing on high-resolution spatial localization and robust shape feature extraction, the model achieves more precise nucleus boundary detection. The dual-stage U-Net DSU-Net 65 integrates CNN-based local feature extraction with transformer-based global context modeling for skin lesion segmentation. This hybrid design enhances both boundary precision and contextual understanding, enabling more accurate lesion delineation. The dual-encoder attention network DEAU-Net 66 improve medical image segmentation by combining two encoders with attention mechanisms. It effectively captures multi-scale contextual information and emphasizes salient features while suppressing irrelevant regions. The integration of attention mechanisms with DL enhances the medical image segmentation by feature representation, capturing contextual dependencies, and improving segmentation accuracy across diverse imaging modalities. These approaches have shown remarkable performance in diverse medical imaging domains, including brain tumor classification, melanoma segmentation, cervical nucleus detection, and microvascular imaging, highlighting the potential of attention-guided and hybrid feature extraction frameworks. Inspired by these research works, the proposed FCAU-Net integrates feature-calibrated attention modules with an end-to-end architecture tailored for ovarian ultrasound images, addressing challenges such as small follicle structures, low contrast, and imaging noise. The combination of multiscale feature extraction, attention-guided focus, and computational efficiency in FCAU-Net is directly motivated by these prior works, aiming to achieve high-precision, robust, and clinically applicable PCOS detection, surpassing the limitations of existing CNN, U-Net, and ensemble-based approaches. Recent advancements in medical image processing have witnessed the emergence of several U-Net derivatives that integrate hybrid architectural components such as Atrous Spatial Pyramid Pooling (ASPP), dual attention schemes, and Squeeze-and-Excitation (SE) modules to enhance feature extraction and boundary precision. For instance, SAP-UNet has been successfully employed for ultrasound-based segmentation by combining ASPP with SE blocks to capture multi-scale contextual representations while refining channel-wise significance 72 . Similarly, DDA-AttResUNet 73 developed for breast and ovarian ultrasound segmentation tasks, utilizes a dual decoder mechanism fused with residual and attention pathways to enhance feature propagation between encoder and decoder stages. Beyond these, architectures such as DA-TransUNet 74 and Hybrid Dilated Residual U-Net 75 have incorporated both spatial and channel-wise attention blocks for fine-grained tissue segmentation. In the specific context of ovarian and PCOS imaging, studies such as CystNet, Enhanced AResU-Net 76 , Follicles-Net 77 , RNN 78 and ML 79 have reported improved cyst recognition through hierarchical or dilated convolutional blocks. These designs, while effective in multiscale feature aggregation, often focus on static pooling or global context enhancement without fully integrating localized spatial relational learning. The proposed FCAU-Net differs by introducing FFC module that dynamically combines spatial and contextual information across scales, thereby refining ovarian cyst segmentation boundaries and follicular classification accuracy. Moreover, through FCE preprocessing, the model ensures improved noise resilience and region smoothness compared to ASPP or SE-based variants. Research methodology of proposed FCAU-Net model The proposed FCAU-Net model was designed to classify the PCOS infected and Normal healthy images. The FCAU-Net research methodology is shown in Fig. 2 . The overall research methodology of FCAU-Net initiates with stage 1 that performs collection of 3800 PCOS ultrasound Images Dataset from KAGGLE having 1900 PCOS infected images and 1900 Normal healthy images 47 . Stage 2 deals with dataset preprocessing that segregates the images based on normal and PCOS symptoms. Then the Labelling of the image is done followed by data augmentation by generating 14 augmented images for each image in the dataset resulting with 53,200 images. The data augmentation was performed using horizontal flipping, vertical flipping, rotation with positive and negative angle of 45, 90, 135, 180, 225, 270 degrees. The data augmented cropped images are subjected to generate fuzzy contrast enhanced image vector. Stage 3 fits the HEG images are fitted with the existing CNN models like DenseNet, AlexNet, VGG19, ResNet, Inception, UNet and Attention UNet to select the best CNN model. The Attention U-Net found to detect the existence of PCOS with accuracy above 80%. Fig. 2 Proposed FCAU-Net research methodology. So, the Attention U-Net was refined to improve the accuracy by proposing FCAU-Net. The proposed FCAU-Net Overall architecture shown in Fig. 3 . The FCAU-Net framework retrieves the PCOS ultrasound images that are subjected to the image segregation based on the disease class as Normal and PCOS infected images. The PCOS ultrasound images are performed with data preprocessing to generate FCE images by calling Fuzzy contrast enhanced module that is shown in Fig. 4 . The FCE images are fitted to proposed FCAU-Net framework that predicts the mask and classifies the ultrasound images based on the class. Steps involved in generating the enhanced image vector (Fig. 4 ) retrieves the segregated labeled PCOS ultrasound images. Fig. 3 Overall architecture of proposed FCAU-Net. The labelled PCOS ultrasound images subjected to image cropping by finding the biggest contour and extract the extreme points to form the crop images. The image cropping was performed in order to focus on the significant image features. The cropped images are performed with data augmentation that 14 images for each image in the dataset resulting with 53,200 images. All the data augmented images are subjected to enhance the brightness of the image by forming Histogram equalized image, CLAHE image and Fuzzy Contrast Enhanced image. This work forms both Histogram equalized image and CLAHE with the intent of performance evaluation as both of them serves a distinct purpose. The normal Histogram equalization of the image works for the entire image by stretching the pixel intensity. Though CLAHE is a method of histogram equalization, it applies histogram for small adaptive regions that prevents noise in homogenous areas. The FCE images, known for their high brightness, are validated using the PSNR ratio and processed through the Fuzzy Contrast Enhanced module. The FCAU-Net framework, depicted in Fig.&#160; 5 , uses these FCE images as input. The images are passed through four encoder blocks and four decoder blocks. The encoder-decoder feature maps are combined using the Feature Fusion Context (FFC) module, which extracts positional and contextual characteristics to generate optimized fused feature maps. Fig. 4 Steps in generating enhanced image vector. Fig. 5 FCAU-Net framework. In the FCAU-Net (Fig.&#160; 6 ), the FCE images are initially downscaled by a factor of 2 through the encoder blocks, creating contrasted FCE feature maps. These are then passed through decoder blocks integrated with attention gates, which upscale the feature maps, resulting in expanded FCE segmentation feature maps. Fig. 6 FCAU-Net Framework (F &#8211; Feature Maps, H, W, D &#8211; Height, width and Depth of the feature maps). Fig. 7 Attention gate network in FCAU-Net. The attention gate mechanism is illustrated in Fig.&#160; 7 . The attention gate in FCAU-Net takes the input feature map and gate signal to calculate the gating coefficient. Batch normalization is applied to the gating coefficient to center the features in the active region while maintaining the relevance of unaligned weights in the feature maps. ReLU activation is then used to introduce nonlinearity, helping the feature vector learn complex representations. Dropout is applied to remove noise from the aligned weights of the feature map. A 1&#8201;&#215;&#8201;1 linear convolution is performed next to generate the attention feature map based on vector concatenation. Finally, the sigmoid activation function is applied, assigning a weight of &#8220;1&#8221; to the aligned features to create the attention coefficient feature map. Feature fusion context module in proposed FCAU-Net The novelty of this research lies in the integration of the Feature Fusion Context (FFC) Module, positioned between the encoder and decoder blocks of the FCAU-Net, as shown in Fig.&#160; 1 . The FFC module enhances feature maps by extracting positional and contextual information. Positional information is obtained by analysing correlations within feature maps (FMs). Spatial features are extracted through convolution, resulting in a 3D FM comprising query, key, and value components. These features are compared to compute the Energy Score Matrix (ESM), which highlights the relative importance of pixel positions. The ESM is normalized using SoftMax to generate Position Attention Weights (PAW), capturing positional details. Contextual information is derived by identifying interdependencies between FM channels. Attention scores indicating feature importance are computed and normalized with SoftMax to create weighted FMs. These weighted FMs are multiplied with the original FMs to strengthen the cumulative channel content. The weighted and original FMs are then integrated to enhance the model&#8217;s ability to capture contextual details. The FFC module processes both original and small-scale FMs to fuse positional and contextual information, resulting in improved feature map quality. This fusion significantly enhances prediction performance, validating the module&#8217;s effectiveness in optimizing feature. First, the Position Attention Weights (PAW) are generated for both original and small-scale feature maps (FMs). For the original scale FM, depth-wise convolution with three kernels is applied, followed by batch normalization and single-kernel convolution. For the small-scale FM, convolution with three kernels and two strides is followed by batch normalization and average pooling with three kernels and two strides. Next, the Semantic Attention Weights (SAW) are created. For the original scale FM, convolution with three kernels is applied, followed by batch normalization and up-sampling with a sigmoid activation function. For the small-scale FM, depth-wise convolution with three kernels is followed by batch normalization and single convolution with a sigmoid activation function. The PAW and SAW of the original scale FM are then combined to form the Position Semantic Weight for the original scale FM. Similarly, the PAW and SAW of the small-scale FM are concatenated and up-sampled to create the Position Semantic Weight for the small-scale FM. Finally, the Position Semantic Weights of both scales are fused to produce optimized fused FM. Development of FCAU-Net and mathematical modeling The FCAU-Net initiates by collecting ultrasound images from publicly available PCOS ultrasound Images Dataset from KAGGLE dataset for classifying the PCOS infected and normal healthy images are denoted in the Eq.&#160;( 1 ). 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$PCOS_{{3800}} = \\left\\lfloor { \\cup _{{W = 1}}^{{3800}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {PCOS_{{ed}} } } } \\right\\}} \\right\\rfloor$$\\end{document} Where &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{PCOS}_{00}}_{1}$$\\end{document} &#8221; denotes single ultrasound image, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$e,\\:d$$\\end{document} &#8221; denotes the number of row and column pixels and &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$W$$\\end{document} &#8221; denotes the number of images in the dataset. The single ultrasound image is denoted in Eq.&#160;( 2 ). 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$PCOS_{{001}} = \\left[ {\\begin{array}{*{20}l} {PCOS\\left( {0,0} \\right)} &amp; {PCOS\\left( {0,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {PCOS\\left( {0,255} \\right)} \\\\ {PCOS\\left( {1,0} \\right)} &amp; {PCOS\\left( {1,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {PCOS\\left( {1,255} \\right)} \\\\ \\vdots &amp; \\vdots &amp; { \\ldots \\ldots \\ldots } &amp; \\vdots \\\\ {PCOS\\left( {255,0} \\right)} &amp; {PCOS\\left( {255,1} \\right)} &amp; { \\ldots \\ldots \\ldots } &amp; {\\:PCOS\\left( {255,255} \\right)} \\\\ \\end{array} } \\right]$$\\end{document} The ultrasound images are applied to the data preprocessing module by processing fuzzy contrast enhancement to generate the FCE images. Data preprocessing modeling The input ultrasound images are segregated based on the PCOS disease class as shown in the Eq.&#160;( 3 ) denoting 1900 &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Infect$$\\end{document} &#8221; as PCOS infected images and 1900 &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Norm$$\\end{document} &#8221; as normal healthy images which were labeled. 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$PCOS_{{3800}} = \\left\\lfloor { \\cup _{{W = 1}}^{{1900}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {Infect_{{ed}} } } } \\right\\} + \\cup _{{W = 1}}^{{1900}} \\left\\{ {\\sum\\nolimits_{{e = 1}}^{{255}} {\\sum\\nolimits_{{d = 1}}^{{255}} {Norm_{{ed}} } } } \\right\\}} \\right\\rfloor$$\\end{document} The labeled ultrasound images are processed with the image cropping and data augmentation. Image cropping and data augmentation modeling Each labeled &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{PCOS}_{00}}_{1}$$\\end{document} &#8221; ultrasound input image is processed with image cropping by extracting the biggest contour and extract the ultrasound image extreme points to form the cropped images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{CropImgPCOS}_{00}}_{1}$$\\end{document} &#8221; as shown in Eq.&#160;( 4 ) to Eq.&#160;( 5 ) 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Contour=maxcontour\\left({{PCOS}_{00}}_{1}\\right)$$\\end{document} 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{CropImgPCOS}_{00}}_{1}=Extremepoints\\left(Contour\\right)\\:\\:$$\\end{document} The cropped images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{CropImgPCOS}_{00}}_{1}$$\\end{document} &#8221; are processed with data augmentation to form PCOS augmented images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{AugPCOS}_{00}}_{1}$$\\end{document} &#8221; resulting with 53,200 images. The data augmentation was performed using horizontal flipping from the Eqs.&#160;( 6 ) to ( 7 ). Here, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:CropImgPCOS$$\\end{document} &#8221; denotes cropped image patch showing the horizontally cropped ROI portion from the main PCOS dataset image, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Horizontal\\:Flip$$\\end{document} &#8221; denotes the horizontal transformation matrix that performs horizontal mirroring. The variables &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$e,\\:d$$\\end{document} &#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before horizontal flipping transformation. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[c;e;1]$$\\end{document} &#8221; denotes original homogeneous coordinate vector having the column vector form of the pixel coordinate before horizontal flipping. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[c{\\prime\\:};e{\\prime\\:};1]$$\\end{document} &#8221; denotes the transformed coordinate vector representing the pixel coordinates after horizontal flipping. 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}c\\\\\\:\\begin{array}{c}e\\\\\\:1\\end{array}\\end{array}\\right]=Horizontal\\:Flip\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}c\\:{\\prime\\:}\\\\\\:\\begin{array}{c}e\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document} 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Horizontal\\;Flip = \\left[ {\\begin{array}{*{20}c} {c^{\\prime } } \\\\ {\\begin{array}{*{20}c} {e^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} { - 1} \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 1 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} c \\\\ {\\begin{array}{*{20}c} e \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document} The vertical flipping operation is shown from Eqs.&#160;( 8 ) to 9 ). Here, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:CropImgPCOS$$\\end{document} &#8221; denotes cropped image patch showing the vertical cropped ROI portion from the main PCOS dataset image, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Vertical\\:Flip$$\\end{document} &#8221; denotes the vertical transformation matrix that performs vertical mirroring. The variables &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$f,\\:g$$\\end{document} &#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before vertical flipping transformation. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[f;g;1]$$\\end{document} &#8221; denotes original homogeneous coordinate vector having the column vector form of the pixel coordinate before vertical flipping. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[f{\\prime\\:};g{\\prime\\:};1]$$\\end{document} &#8221; denotes the transformed coordinate vector representing the pixel coordinates after vertical flipping. 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}f\\\\\\:\\begin{array}{c}g\\\\\\:1\\end{array}\\end{array}\\right]=Vertical\\:Flip\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}f\\:{\\prime\\:}\\\\\\:\\begin{array}{c}g\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document} 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Vertical\\;Flip = \\left[ {\\begin{array}{*{20}c} {f^{\\prime } } \\\\ {\\begin{array}{*{20}c} {g^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} 1 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} { - 1} \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} f \\\\ {\\begin{array}{*{20}c} g \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document} The rotation operation is shown in the equation Eq.&#160;( 10 ) to (11). Here, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:CropImgPCOS$$\\end{document} &#8221; denotes cropped image patch showing the rotated portion from the main PCOS dataset image, &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Rotation$$\\end{document} &#8221; denotes the rotation transformation matrix that applies geometric rotation in homogeneous space. The variables &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m,\\:n$$\\end{document} &#8221; denotes the coordinate variables representing the column and row pixel positions respectively in the image that define the location of each pixel before rotation transformation. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[m;n;1]$$\\end{document} &#8221; denotes original homogeneous coordinate vector of the pixel coordinate before rotation. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[m{\\prime\\:};n{\\prime\\:};1]$$\\end{document} &#8221; denotes the transformed coordinate vector representing the pixel coordinates after rotation. 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left[{{CropImgPCOS}_{00}}_{1}\\right]={\\left[\\begin{array}{c}m\\\\\\:\\begin{array}{c}n\\\\\\:1\\end{array}\\end{array}\\right]=Rotation\\left[{{CropImgPCOS}_{00}}_{1}\\right]}^{{\\prime\\:}}=\\left[\\begin{array}{c}m\\:{\\prime\\:}\\\\\\:\\begin{array}{c}n\\:{\\prime\\:}\\\\\\:1\\end{array}\\end{array}\\right]$$\\end{document} 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Rotation = \\left[ {\\begin{array}{*{20}c} {m^{\\prime } } \\\\ {\\begin{array}{*{20}c} {n^{\\prime } } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] = \\left[ {\\begin{array}{*{20}c} {\\cos \\theta } \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} { - \\sin \\theta } \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} {\\sin \\theta } \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} {\\cos \\theta } \\\\ \\vdots \\\\ \\end{array} } \\\\ 0 \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} \\cdots \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} \\ldots \\\\ \\ldots \\\\ \\end{array} } \\\\ \\ldots \\\\ \\end{array} } \\\\ \\end{array} \\begin{array}{*{20}c} 0 \\\\ {\\begin{array}{*{20}c} {\\begin{array}{*{20}c} 0 \\\\ \\vdots \\\\ \\end{array} } \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right] \\times \\left[ {\\begin{array}{*{20}c} m \\\\ {\\begin{array}{*{20}c} n \\\\ 1 \\\\ \\end{array} } \\\\ \\end{array} } \\right]$$\\end{document} The final resultant data augmentation results \" \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$AugPCOS$$\\end{document} \" are obtained as depicted in Eqs.&#160;( 12 ) to ( 15 ), with &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:HFPCOS$$\\end{document} &#8221; denoting the horizontally flipped image, denoting &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:VFPCOS$$\\end{document} &#8221; the vertically flipped image and &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:RPCOS$$\\end{document} &#8221; denoting the rotated image. 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:HFPCOS=Horizontal\\:Flip\\left(CropImgPCOS\\right)\\:$$\\end{document} 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:VFPCOS=Vertical\\:Flip\\left(CropImgPCOS\\right)\\:$$\\end{document} 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:RPCOS=Rotation\\:\\left(CropImgPCOS\\right)\\:$$\\end{document} 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AugPCOS=\\bigcup\\:\\left\\{\\begin{array}{c}HFPCOS\\\\\\:VFPCOS\\\\\\:RPCOS\\end{array}\\right.$$\\end{document} The data augmented images are processed with fuzzy contrast enhanced module. Fuzzy contrast enhanced module modeling The data augmented ultrasound images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AugPCOS$$\\end{document} &#8221; are processed to form the Histogram equalized images, CLAHE image and FCE images. The histogram equalized ultrasound image &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{HistPCOS}_{00}}_{1}$$\\end{document} &#8221; was formed by applying cumulative distribution function &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:cdf\\:$$\\end{document} &#8221; of each image pixel and histogram variance &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Histogram$$\\end{document} &#8221; as in Eqs.&#160;( 16 ) to ( 17 ) denoting &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(x,y)$$\\end{document} &#8221; are the pixel co-ordinates of data augmented ultrasound image. Here \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\text{min}\\left(cdf\\right)$$\\end{document} denotes the smallest cumulative probability value used for normalization. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Width\\left(dots\\right)$$\\end{document} &#8221; denotes the number of pixels in the horizontal dimension of the image. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Height\\left(dots\\right)$$\\end{document} &#8221; denotes the number of pixels in the vertical dimension of the image. The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$RC,\\:GC,\\:BC$$\\end{document} &#8221; denotes the red, green and blue channel intensity component of the augmented image. 16 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Histogram=round\\:\\left(\\frac{cdf\\left({{AugPCOS}_{00}}_{1}\\left(x,y\\right)-\\text{min}\\left(cdf\\right)\\right)}{Width\\left(dots\\right)\\:\\times\\:Height\\left(dots\\right)-\\text{m}\\text{i}\\text{n}\\left(cdf\\right)}\\times\\:\\left(RC+GC+BC\\right)-1\\right)$$\\end{document} 17 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{HistPCOS}_{00}}_{1}={{AugPCOS}_{00}}_{1}+Histogram$$\\end{document} As the data augmented ultrasound images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AugPCOS$$\\end{document} &#8221; is an array of pixel values which are denoted by the array of numbers as random dots. Suppose if &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(x,y)$$\\end{document} &#8221; are the two random pixel variables and if they are exactly linearly correlated with constant &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$c$$\\end{document} &#8221;, then it is shown in Eq.&#160;( 18 ). 18 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{AugPCOS}_{00}}_{1}\\left(x\\right)=c{\\:\\times\\:{AugPCOS}_{00}}_{1}\\left(y\\right)$$\\end{document} Now the probability density function (PDF) of the two random dots is denoted as Eq.&#160;( 19 ) where &#8216;r&#8217; is the total number of roots of (31) which is equal to &#8216;1&#8217;. Here \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\frac{d\\left({y}_{j}\\right)}{d\\left(x\\right)}$$\\end{document} represents how intensity values &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${y}_{j}$$\\end{document} &#8221; change with respect to &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x$$\\end{document} &#8221; during transformation. 19 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$PDF\\left( {AugPCOS_{{001}} \\left( x \\right)} \\right) = \\sum\\nolimits_{{j = 1}}^{r} f \\left( {AugPCOS_{{001}} (y_{j} )} \\right)\\left| {\\frac{{d\\left( {y_{j} } \\right)}}{{d\\left( x \\right)}}} \\right|$$\\end{document} In CLAHE, the PDF inside the local region of the augmented ultrasound image was found and is denoted by Eq.&#160;( 20 ), where &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)$$\\end{document} &#8221; represents the contrast of the augmented ultrasound image by CLAHE and is substituted to &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Q$$\\end{document} &#8221; as in Eq.&#160;( 21 ). 20 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)\\ne\\:1$$\\end{document} 21 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(x\\right)\\right)=Q$$\\end{document} The value of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Q\"$$\\end{document} lies between 0 and 1. If \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Q=1\"$$\\end{document} , then the local histogram stretching is maximum, but in CLAHE the value of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Q\"$$\\end{document} must be less than &#8216;1&#8217;, since the contrast stretching is limited. The value of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Q\"$$\\end{document} is denoted in Eq.&#160;( 22 ) 22 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{Q=\\:PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\left|\\frac{d\\left(y\\right)}{d\\left(x\\right)}\\right|LR$$\\end{document} Where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\:\"PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\"$$\\end{document} is the PDF of the local region &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:LR\\:$$\\end{document} &#8221; of original augmented ultrasound image and &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:LR$$\\end{document} &#8221;, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left|\\frac{d\\left(y\\right)}{d\\left(x\\right)}\\right|$$\\end{document} is the ratio \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:d\\left(y\\right),\\:d\\left(y\\right)$$\\end{document} of the image in that local region. By integrating on both sides of Eq.&#160;( 22 ), we get the form as in Eq.&#160;( 23 ). The transformation function of image contrast with CLAHE is given in Eq.&#160;( 24 ) denoting \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"k\"$$\\end{document} as integral constant. The contrast enhanced image by CLAHE \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{\"CLAHEPCOS}_{00}}_{1}\"\\:$$\\end{document} is given in Eq.&#160;( 25 ) 23 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Q*{\\int\\:}_{LR}^{x}dx=\\:{\\int\\:}_{LR}^{x}{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\:dy$$\\end{document} 24 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{{AugPCOS}_{00}}_{1}\\left(x\\right)}_{LR}\\:\\:=\\frac{1}{Q}*{\\int\\:}_{LR}^{x}{PDF}_{LR}\\left({{AugPCOS}_{00}}_{1}\\left(y\\right)\\right)\\:dy+\\:k$$\\end{document} 25 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{CLAHEPCOS}_{00}}_{1}={{{AugPCOS}_{00}}_{1}\\left(x\\right)}_{LR}\\:\\:$$\\end{document} As the input augmented ultrasound images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AugPCOS\\:$$\\end{document} &#8221; is in the gray scale format of size \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"M\\:\\times\\:\\:N\"$$\\end{document} with &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:GL\\:$$\\end{document} &#8221; gray levels with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"gl=0,\\:\\text{1,2}\\dots\\:.L-1\"$$\\end{document} . The gray levels were defined as the group of fuzzy sets representing the membership pixel value to the image property as in Eq.&#160;( 26 ) for the single augmented ultrasound image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{{AugPCOS}_{00}}_{1}\"$$\\end{document} . The notation of the fuzzy sets \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\frac{{\\mu\\:}_{mn}}{{gl}_{mn}}\"$$\\end{document} represents the fuzzy membership of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"M\\:\\times\\:\\:N\"$$\\end{document} pixel. 26 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$AugPCOS_{{001}} = \\cup _{{m = 1}}^{M} \\cup _{{m = 1}}^{N} \\frac{{\\mu _{{mn}} }}{{gl_{{mn}} }}\\:\\:where\\:\\mu \\:_{{mn}} \\: \\in \\:[0,\\:1]$$\\end{document} The fuzzy contrast image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"FCE\\left(Y\\right)\"$$\\end{document} of the input augmented ultrasound images &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AugPCOS$$\\end{document} &#8221; was formed by performing three processes as fuzzification \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\Phi\\:}\"\\:$$\\end{document} operation, Membership value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\Gamma\\:}\"$$\\end{document} operation, and defuzzification \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{\\Psi\\:}\"$$\\end{document} operation as denoted by Eq.&#160;( 27 ) 27 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FCE\\left(Y\\right)={\\Psi\\:}\\left(\\:{\\Gamma\\:}\\left(\\:{\\Phi\\:}\\left({{AugPCOS}_{00}}_{1}\\:\\right)\\right)\\right)$$\\end{document} The modified gray levels \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\:\\:\"GL}^{{\\prime\\:}}\"$$\\end{document} of the augmented ultrasound image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{{AugPCOS}_{00}}_{1}\"$$\\end{document} is computed as in Eq.&#160;( 28 ). 28 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$GL^{\\prime } = f\\left( {AugPCOS_{{001}} } \\right) = (GL - 1)\\sum\\nolimits_{{x = 0}}^{{gl}} {\\frac{{h\\left( i \\right)}}{{MN}}}$$\\end{document} The gray level fuzzification \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\Phi\\:}\"\\:$$\\end{document} operation is performed on the image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\mu\\:}_{mn}\"\\:$$\\end{document} membership value as in Eq.&#160;( 29 ) denoting as exponential &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FE$$\\end{document} &#8221; and denominational &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FD$$\\end{document} &#8221; fuzzifiers that control the amount of grayness level in the FCE image. 29 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\mu\\:}_{mn}\\left(gl\\right)={\\left[1+\\frac{{gl}_{max}-\\:gl}{FD}\\:\\right]}^{-FE}$$\\end{document} Now, the Membership value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\Gamma\\:}\\left(gl\\right)\"$$\\end{document} operation on the image is performed as shown in Eq.&#160;( 30 ) 30 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\Gamma\\:}\\left(gl\\right)=\\left\\{\\begin{array}{l}{2\\left[{\\mu\\:}_{mn}\\left(gl\\right)\\right]}^{2}\\\\\\:1-{2\\left[{\\mu\\:}_{mn}\\left(gl\\right)\\right]}^{2}\\end{array}\\right.\\begin{array}{c}\\:\\:\\:\\:\\:\\:\\:if\\:0\\le\\:{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:0.5\\:\\\\\\:\\:\\:\\:\\:\\:if\\:0.5&lt;{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:1\\end{array}$$\\end{document} The defuzzification \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{\\Psi\\:}\"$$\\end{document} operation on the image is performed as shown in Eq.&#160;( 31 ) and the FCE image was found from Eq.&#160;( 32 ) and Eq.&#160;( 33 ). 31 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\Psi\\:}\\left(gl\\right)=\\left\\{\\begin{array}{c}n-\\left(n-{gl}_{\\text{m}\\text{i}\\text{n}}\\right)(1-2{\\mu\\:}_{mn}\\left(gl\\right))\\\\\\:n+\\left({gl}_{\\text{m}\\text{a}\\text{x}}-n\\right)(2{\\mu\\:}_{mn}\\left(gl\\right)-1)\\end{array}\\right.\\begin{array}{c}\\:\\:\\:\\:\\:\\:\\:if\\:0\\le\\:{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:0.5\\:\\\\\\:\\:\\:\\:\\:\\:if\\:0.5&lt;{\\mu\\:}_{mn}\\left(gl\\right)\\le\\:1\\end{array}$$\\end{document} 32 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{{FCEPCOS}_{00}}_{1}\\left(x,y\\right)=\\:FCE\\left({{AugPCOS}_{00}}_{1}\\right)$$\\end{document} 33 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FCE\\left({{AugPCOS}_{00}}_{1}\\right)={\\Psi\\:}\\left(\\:{\\Gamma\\:}\\left(\\:{\\Phi\\:}\\left({{{{\\mu\\:}_{mn}(AugPCOS}_{00}}_{1})}_{gl}\\:\\right)\\right)\\right)\\:$$\\end{document} The brightness and Pixel Intensity of the obtained Histogram equalized images, CLAHE image and FCE images are validated to select high pixel intensity image. The pixel intensity \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Int(x,y)\"\\:$$\\end{document} was validated by finding the scene transmission \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"t(x,y)\"\\:$$\\end{document} distant dependent factor of the images. The scene radiance \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"SR(x,\\lambda\\:)\"\\:$$\\end{document} was also estimated denoting \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\lambda\\:\"\\:$$\\end{document} as transmission coefficient. The validation of pixel intensity for histogram equalized image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"IntHist\\left(x,y\\right)\"\\:$$\\end{document} was done as in Eq.&#160;( 34 ) to Eq.&#160;( 35 ) with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\beta\\:\"$$\\end{document} representing as color density, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"d\"$$\\end{document} distant dependent constant factor and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\:\"L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\"$$\\end{document} as scattering coefficient. 34 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:t\\left({{HistPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document} 35 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:IntHist\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{HistPCOS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document} The validation of pixel intensity for CLAHE image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"IntCLAHE\\left(x,y\\right)\"\\:$$\\end{document} was done as in Eq.&#160;( 36 ) to Eq.&#160;( 37 ) with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"\\beta\\:\"$$\\end{document} representing as color density, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"d\"$$\\end{document} distant dependent constant factor and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\:\"L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\"$$\\end{document} as scattering coefficient. 36 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:t\\left({{CLAHEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document} 37 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:IntCLAHE\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{CLAHEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document} The validation of pixel intensity for FCE image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"IntFCE\\left(x,y\\right)\"$$\\end{document} was done in Eq.&#160;( 38 ) to Eq.&#160;( 39 ) 38 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:t\\left({{FCEPCOS}_{00}}_{1}\\left(x,y\\right)\\right)=\\:{e}^{-\\beta\\:\\:.\\:\\:d}$$\\end{document} 39 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:IntFCE\\left(x,y\\right)=SR\\left(x,\\lambda\\:\\right)+{L}_{\\alpha\\:}\\left(\\lambda\\:\\right)\\:\\left[1-t\\left({{HighGAS}_{00}}_{1}\\left(x,y\\right)\\right)\\right]$$\\end{document} The high pixel intensity image was selected based on comparing the obtained pixel intensity of Histogram equalized images, CLAHE image and FCE images as in Eq.&#160;( 40 ). The pixel intensity of FCE was found to be high. The FCE images are applied to FCAU-Net module 40 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:HighPixel=max\\left\\{\\begin{array}{l}IntHist\\left(x,y\\right)\\\\\\:IntCLAHE\\left(x,y\\right)\\\\\\:IntFCE\\left(x,y\\right)\\end{array}\\right.$$\\end{document} Proposed FCAU-Net PCOS detection modeling The fuzzy contrast enhanced image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{{FCEPCOS}_{00}}_{1}\"\\:$$\\end{document} input data is processed with existing CNN models to select the best CNN model. Experiment results portray that Attention UNet offers the classification of PCOS with the accuracy above 80%. Now the Attention UNet was selected to integrate the feature fusion context module. The FCAU-Net consists of four encoder block and four decoder blocks accompanied with the Attention gate. The feature map produced by the convolution after each encoder is shown in Eq.&#160;( 41 ) that defines the feature map \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FEMap}_{mn}\\:$$\\end{document} at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"(m,n)\"$$\\end{document} , which results from applying the convolution operator \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"Con\\left(i,j\\right)\"\\:$$\\end{document} between the image pixels &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PIXx$$\\end{document} &#8221; and the convolution kernel &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:KER$$\\end{document} &#8221;. 41 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FEMap}_{mn}=Con\\left(i,j\\right)={\\left(PIXx*KER\\right)}_{ij}$$\\end{document} The convolution expansion operation is shown in Eq.&#160;( 42 ) with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{PIX}_{i-r,j-c}\"\\:$$\\end{document} denoting the pixel intensity from the input image at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left(i-r,j-c\\right)$$\\end{document} in the receptive field with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(i,\\:j)$$\\end{document} as pixel coordinates. Here \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"KER}_{r,c}\"\\:$$\\end{document} denotes the kernel coefficient that is the weight at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:(r,c)$$\\end{document} in the convolution kernel. 42 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\left( {PIX*KER} \\right)_{{ij}} = \\sum\\nolimits_{{r = 1}}^{{255}} {\\sum\\nolimits_{{c = 1}}^{{255}} {PIX_{{i - r,j - c}} *KER_{{r,c}} } }$$\\end{document} After the convolution, the sigmoid function &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Sig$$\\end{document} &#8221; was performed as in Eq.&#160;( 43 ) where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\sigma\\:\\left(.\\right)\"$$\\end{document} Is the Sigmoid function. Here \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"FEM}_{mn}\"\\:$$\\end{document} denotes the feature map intensity which is the raw convolution output before activation. The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"e}^{-{FEM}_{mn}}\"$$\\end{document} denotes the exponential decay term that controls how steeply the sigmoid transitions between 0 and 1. 43 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Sig=\\sigma\\:\\left({FEM}_{mn}\\right)\\frac{1}{1+\\:{e}^{-{FEM}_{mn}}}$$\\end{document} Feature fusion context module modeling The FFC module was integrated between the encoder and decoder block of the FCAU-Net. The FFC module extracts the position and Context information of the FM. The Context information of the FM was computed by finding the interdependencies between different channels and extracts attention scores that indicate the feature importance. The attention scores are normalized with SoftMax to form the attention weights distributions with probabilistic values to form Weighted FM. Assume &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FEMap$$\\end{document} &#8221; is the obtained feature map which is shown in Eq.&#160;( 44 ) with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"C\\:\\times\\:\\:H\\:\\times\\:W\"$$\\end{document} denoting the channel, height and width of FM. The context attention scores &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AS$$\\end{document} &#8221; of the feature map were formed in the format of channel attention map &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:AM$$\\end{document} &#8221; as in Eq.&#160;( 45 ). 44 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:AM={FEMap}^{C\\:\\times\\:\\:H\\:\\times\\:W}$$\\end{document} 45 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{AS}_{ij}=\\frac{\\text{exp}\\left({AM}_{i}{AM}_{j}\\right)}{{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({AM}_{i}{AM}_{j}\\right)}$$\\end{document} The attention scores \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"AS}_{ij}\"\\:$$\\end{document} are multiplied with the original FM to form the cumulative FM channel content. Now, the weighted FM was integrated with the original FM to regulate the FM strength. This validates that the model can learn the context details &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:CD$$\\end{document} &#8221; of the FM with the attention distribution of the FM pixels with scale parameter \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\beta\\:\"$$\\end{document} as in Eq.&#160;( 46 ). 46 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:CD=\\beta\\:{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({AS}_{ij}{AM}_{i}\\right)+{AM}_{j}$$\\end{document} The position information of the FM was done by analysing the correlation between the position in the FM. The process starts by acquiring the spatial features from FM by applying convolution that results in 3D FM consisting of query, key, and value. By comparing the 3D FM features, the Energy Score Matrix (ESM) was computed. Assume &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:FEMap$$\\end{document} &#8221; is obtained FM. The Position Attention Weights &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PW$$\\end{document} &#8221; of the FM were formed in format of &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:EM$$\\end{document} &#8221; as in Eq.&#160;( 47 ) and Eq.&#160;( 48 ) 47 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:EM={FEMap}^{C\\:\\times\\:\\:H\\:\\times\\:W}$$\\end{document} 48 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PW}_{ij}=\\frac{\\text{exp}\\left({EM}_{i}{EM}_{j}\\right)}{{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({EM}_{i}{EM}_{j}\\right)}$$\\end{document} The ESM represents the relative importance between FM pixel positions. The ESM are then normalized with softmax to form Position Attention Weights &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PW$$\\end{document} &#8221; that shows the position information of FM. This validates that model can learn the position details &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PD$$\\end{document} &#8221; of the FM with the Position Attention Weights &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PW$$\\end{document} &#8221; of the FM pixels with scale parameter \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\beta\\:\"$$\\end{document} as in Eq.&#160;( 49 ). 49 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:PD=\\beta\\:{\\sum\\:}_{i=1}^{c}\\text{exp}\\left({PW}_{ij}{EM}_{i}\\right)+{EM}_{j}$$\\end{document} The FFC module extracts the position and Context information of both the original and small-scale feature maps. First, the PAW of the original and small-scale feature maps is formed. The PAW of the original scale FM is processed with depth wise convolution using 3 kernels followed by batch normalization and single kernel convolution to form PAW of original scale FM. The PAW of original scale FM is denoted by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMO}_{Pos}\"$$\\end{document} that denotes the &#8220;position block 1&#8221;. The SAW of the small-scale FM is processed with depth wise convolution using 3 kernels followed by batch normalization and single convolution with sigmoid activation function to form SAW of small-scale FM. The SAW of small-scale FM is denoted by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMS}_{Con}\"$$\\end{document} that denotes the &#8220;context block 1&#8221;. The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMO}_{Pos}\"$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{Con}\"\\:$$\\end{document} is denoted as in Eq.&#160;( 50 ) and Eq.&#160;( 51 ). The operation performed on SAW of small-scale FM as in Eq.&#160;( 52 ) with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\text{FM}\"$$\\end{document} denoting FM. 50 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMO}_{Pos}\\in\\:{FM}^{H\\times\\:W\\times\\:C}$$\\end{document} 51 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{Con}\\in\\:{FM}^{\\frac{H}{4}\\times\\:\\frac{W}{4}\\times\\:C}$$\\end{document} 52 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FM_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} {DK_{{ij}}^{C} FMS_{{Con}}^{C} [m + i,n + j]} }$$\\end{document} The value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FM}_{CV}^{C}[m,n]\"$$\\end{document} represents the context value of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{C}^{th}\"$$\\end{document} channel at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"[m,n]\"$$\\end{document} . The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{DK}_{ij}^{C}$$\\end{document} denotes the depth wise kernel of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{C}^{th}\"$$\\end{document} channel at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"[i,j]\"$$\\end{document} . The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMS}_{Con}^{C}\\left[m+i,n+j\\right]\"$$\\end{document} denotes the context value of FM of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{C}^{th}\"$$\\end{document} channel at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"[i,j]\"$$\\end{document} . Now the pointwise context convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PC}_{j}\"$$\\end{document} is done for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FM}_{CV}^{C}[m,n]\"$$\\end{document} for adjusting the number of channels as in Eq.&#160;( 53 ) 53 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{FM}_{CV}^{C}[m,n,i]$$\\end{document} The operation performed on PAW of original scale FM is formulated as in Eq.&#160;( 54 ). The pointwise position convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PP}_{j}\"$$\\end{document} is done for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FM}_{PV}^{C}[m,n]\"$$\\end{document} for adjusting the number of channels as in Eq.&#160;( 55 ). The value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FM}_{PV}^{C}[m,n]\"$$\\end{document} represents the position value of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{C}^{th}\"$$\\end{document} channel at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"[m,n]\"$$\\end{document} . The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMO}_{Pos}^{C}\\left[m+i,n+j\\right]\"$$\\end{document} denotes the position value of FM of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{C}^{th}\"$$\\end{document} channel at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"[i,j]\"$$\\end{document} . 54 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FM_{{PV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} {DK_{{ij}}^{C} FMO_{{Pos}}^{C} [m + i,n + j]} }$$\\end{document} 55 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PP}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{FM}_{PV}^{C}[m,n,i]$$\\end{document} Now after obtaining the pointwise context convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PC}_{j}\"$$\\end{document} of small-scale and pointwise position convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PP}_{j}\"$$\\end{document} of original scale, both are subjected to batch normalization. Now let us consider the position block 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMO}_{PB1}\"$$\\end{document} as denoted in Eq.&#160;( 56 ). The convolution with 1&#8201;&#215;&#8201;1 kernel was performed as denoted by Eq.&#160;( 57 ). The value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\gamma\\:$$\\end{document} denotes scaling factor, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\mu\\:\\:$$\\end{document} denotes the mean of the position block 1, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\sigma\\:}^{2}\\:$$\\end{document} denotes the variance of the position block 1 and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\beta\\:$$\\end{document} denotes the offset value. The final FM obtained from the position block 1 is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMO}_{conv\\_pos1}\"$$\\end{document} 56 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMO_{{PB1}} = \\gamma \\frac{{PP_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document} 57 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMO_{{conv\\_pos1}} = \\sum\\nolimits_{{i = 1}}^{M} {\\sum\\nolimits_{{j = 1}}^{N} D } K_{{ij}}^{C} FMO_{{PB1}} [i,j]$$\\end{document} Now let us consider the context block 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{CB1}\"$$\\end{document} as denoted in Eq.&#160;( 58 ). The convolution with 1&#8201;&#215;&#8201;1 kernel along with sigmoid activation function \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"\\sigma\\:\"$$\\end{document} was performed as denoted by Eq.&#160;( 59 ) and Eq.&#160;( 60 ). The final FM obtained from the context block 1 is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{conv\\_con1}\"$$\\end{document} . 58 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMS_{{CB1}} = \\gamma \\frac{{PC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document} 59 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMS_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} {DK_{{ij}}^{C} FMS_{{CB1}} [i,j]} }$$\\end{document} 60 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{conv\\_con1}=\\sigma\\:\\left({FMS}_{conv}\\right)$$\\end{document} The SAW of original scale is denoted by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMO}_{Con}\"$$\\end{document} that denotes the &#8220;context block 2&#8221;. The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMO}_{Con}\"$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{Pos}\"\\:$$\\end{document} is denoted as in Eq.&#160;( 61 ) and Eq.&#160;( 62 ). The PAW of small-scale FM is denoted by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FMS}_{Pos}\"$$\\end{document} that denotes the &#8220;position block 2&#8221;. 61 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMO}_{Con}\\in\\:{FM}^{H\\times\\:W\\times\\:C}$$\\end{document} 62 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{Pos}\\in\\:{FM}^{\\frac{H}{4}\\times\\:\\frac{W}{4}\\times\\:C}$$\\end{document} The SAW of the original scale FM is processed with convolution using 3 kernels as shown in Eq.&#160;( 63 ). Now the pointwise context convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PWC}_{j}\"$$\\end{document} is done for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\text{S}\\text{A}\\text{W}}_{CV}^{C}[m,n]\"$$\\end{document} for adjusting the number of channels as in Eq.&#160;( 64 ) 63 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$SAW_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{2} {\\sum\\nolimits_{{j = 1}}^{2} {\\sum\\nolimits_{{k = 1}}^{C} D } } K_{{k,j,i}}^{C} FMO_{{Con}}^{C} [m + i,n + j]$$\\end{document} 64 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PWC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{\\text{S}\\text{A}\\text{W}}_{CV}^{C}[m,n,i]$$\\end{document} Now after obtaining the pointwise context convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PWC}_{j}\"$$\\end{document} of original scale of the &#8220;context block 2&#8221;, the batch normalization operation was performed. Now let us consider the context block 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMO}_{CB2}\"$$\\end{document} as denoted in Eq.&#160;( 65 ) and Eq.&#160;( 66 ). Then the obtained \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"FMO}_{conv}\"\\:$$\\end{document} was up sampled with sigmoid activation function to form SAW of original scale FM as in Eq.&#160;( 67 ) and Eq.&#160;( 68 ). The final FM obtained from the context block 2 is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMO}_{conv\\_con2}\"$$\\end{document} . 65 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMO_{{CB2}} = \\gamma \\frac{{PWC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document} 66 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMO_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} D } K_{{ij}}^{C} FMO_{{CB2}} [i,j]$$\\end{document} 67 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMUP}_{conv}=4*Upsample\\left({FMO}_{conv}\\right)$$\\end{document} 68 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMO}_{conv\\_con2}=\\sigma\\:\\left({FMUP}_{conv}\\right)$$\\end{document} The PAW of the small-scale FM \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{Pos}\"$$\\end{document} is processed with convolution using 3 kernels and 2 strides as in Eq.&#160;( 69 ). Now the pointwise position convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PPC}_{j}\"$$\\end{document} is done for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{\\text{P}\\text{A}\\text{W}}_{CV}^{C}[m,n]\"$$\\end{document} for adjusting the number of channels as in Eq.&#160;( 70 ) 69 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$PAW_{{CV}}^{C} [m,n] = \\sum\\nolimits_{{i = 1}}^{2} {\\sum\\nolimits_{{j = 1}}^{2} {\\sum\\nolimits_{{k = 1}}^{C} D } } K_{{k,j,i}}^{C} FMS_{{Pos}}^{C} [m*(2 - 1) + i,n*(2 - 1) + j]$$\\end{document} 70 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{PPC}_{j}={\\sum\\:}_{i=1}^{C}{DK}_{ij}^{C}{\\text{P}\\text{A}\\text{W}}_{CV}^{C}[m,n,i]$$\\end{document} Now after obtaining the pointwise position convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{PPC}_{j}\"$$\\end{document} of small-scale of the &#8220;position block 2&#8221;, the batch normalization operation was performed. Now let us consider the position block 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{PB2}\"$$\\end{document} as denoted in Eqs.&#160;( 71 ) and ( 72 ). 71 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMS_{{PB2}} = \\gamma \\frac{{PPC_{j} - \\mu }}{{\\sqrt {\\sigma ^{2} + \\in } }} + \\beta$$\\end{document} 72 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FMS_{{conv}} = \\sum\\nolimits_{{i = 1}}^{1} {\\sum\\nolimits_{{j = 1}}^{1} D } K_{{ij}}^{C} FMS_{{PB2}} [i,j]$$\\end{document} Then the obtained \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"FMS}_{conv}\"\\:$$\\end{document} was performed with average pooling \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"{FM}_{AP}[m,n]\"\\:$$\\end{document} using 3 kernels and 2 strides to form PAW of small-scale FM as in Eq.&#160;( 73 ). The final small-scale FM obtained from the position block 2 is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{FMS}_{conv\\_pos2}\"$$\\end{document} as in Eq.&#160;( 74 ). 73 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FM_{{AP}} [m,n] = \\frac{1}{9}\\sum\\nolimits_{{i = 1}}^{3} {\\sum\\nolimits_{{j = 1}}^{3} F } MS_{{conv}} [m*2 + i,\\:n*2 + j]$$\\end{document} 74 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{conv\\_pos2}=\\:{FM}_{AP}[m,n]$$\\end{document} The final FM of original scale and small scale of position block 1, context block 1, context block and position block 2 is shown in Eq.&#160;( 75 ) to Eq.&#160;( 78 ) 75 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMO}_{conv\\_pos1}=Optimized\\:Original\\:Scale\\:position\\:FM\\:of\\:position\\:block\\:1\\:$$\\end{document} 76 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{conv\\_con1}\\:=Optimized\\:Small\\:Scale\\:Context\\:FM\\:of\\:context\\:block\\:1\\:$$\\end{document} 77 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMO}_{conv\\_con2}=Optimized\\:Original\\:Scale\\:Context\\:FM\\:of\\:context\\:block\\:2\\:$$\\end{document} 78 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FMS}_{conv\\_pos2}=Optimized\\:Small\\:Scale\\:\\:position\\:FM\\:of\\:position\\:block\\:2\\:$$\\end{document} The output feature map of original scale is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{OS\\_Out}\\:$$\\end{document} denoted in Eq.&#160;( 79 ) and the output feature map of small-scale is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{SS\\_Out}$$\\end{document} denoted in Eq.&#160;(80). 79 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{OS\\_Out}={FMO}_{conv\\_pos1}\\times\\:{FMO}_{conv\\_con2}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{SS\\_Out}={FMS}_{conv\\_con1}\\times\\:{FMS}_{conv\\_pos2}$$\\end{document} (80) The optimized fused feature map is \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\"OFM}_{Out}\"$$\\end{document} obtained by combining the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{OS\\_Out}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{FM}_{SS\\_Out}$$\\end{document} as in Eq.&#160;( 81 ). The &#8220; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:Upsample$$\\end{document} &#8221; operation resizes the lower-resolution FM \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:F{M}_{\\left(S{S}_{Out}\\right)}$$\\end{document} to match the dimensions of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:F{M}_{\\left(O{S}_{Out}\\right)}$$\\end{document} , and the scaling factor (&#215;4) ensures the intensities or channel magnitudes are balanced before fusion. The addition \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\"+\"$$\\end{document} operation merges these two maps to form the final output feature map \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\:\"{OFM}_{Out\\:}\"$$\\end{document} 81 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{OFM}_{Out}={FM}_{OS\\_Out}+4*Upsample\\left({FM}_{SS\\_Out}\\right)$$\\end{document} Experimental setup and result analysis The FCAU-Net initiates by collecting have 3800 ultrasound ovary images of PCOS Ultrasound ( https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images ) dataset for classifying the PCOS infected and healthy class type. The sample normal and PCOS infected images from the dataset are shown in the Fig. 8 . The implementation was carried out in python by using keras, tensorflow, pandas, numpy, algorithms, utils, skimage, neupy, matplotlib and Theano library. The PCOS Ultrasound from dataset are segregated based on the disease class. The segregated PCOS Ultrasound are performed with labeling and the results are shown in Fig. 9 . Fig. 8 Sample PCOS Ultrasound Dataset Images. Fig. 9 Results of Labeled PCOS Ultrasound Dataset Images. After labeling, the image cropping was done to form the cropped image. The results of the PCOS Ultrasound images before and after Image cropping are shown in Fig. 10 . The cropped PCOS Ultrasound images are subjected to data augmentation to form 14 images for each image resulting with 53,200 images data augmented cropped images. The results obtained from the data augmented PCOS infected and normal healthy ultrasound images are shown in Figs.&#160; 11 and 12 respectively. Fig. 10 Step by step breakdown of cropping the image. Fig. 11 Results of PCOS infected Data Augmentation Images. Fig. 12 Results of Normal healthy ultrasound Data Augmentation Images. The original dataset consisted of 3,800 ultrasound images with 1,900 healthy and 1,900 PCOS-infected. The testing dataset was formed with 80: 20 to extract 360 testing images. Through augmentation techniques, the dataset was expanded to 42,560 images, resulting in a total of 45,600 images. Before performing the augmentation process, the dataset was initially divided into separate subsets for training and testing in the ratio of 80:20 to ensure unbiased model evaluation. Specifically, the testing dataset was formed exclusively from the original, unaltered ultrasound 360 images prior to any augmentation procedures. This approach guarantees that the test data remains completely independent from the augmented samples used during model training, thereby preserving the integrity of performance assessment. Once the testing set was isolated, data augmentation was applied only to the training subset to artificially expand the number and diversity of training samples. The augmentation operations were implemented to simulate realistic variations in ultrasound imaging conditions. By using augmented data solely for training, the FCAU-Net model benefits from improved generalization and robustness to image variability, while the testing phases are conducted strictly on original data to ensure an accurate reflection of the model&#8217;s real-world diagnostic performance. This methodological separation between augmentation and testing provides a clear, reliable framework for evaluating model effectiveness without data leakage or overfitting bias. Table&#160; 3 summarizes the distribution of PCOS classes across the augmented datasets. The large augmented dataset not only enhances the model&#8217;s generalization capability but also reduces the risk of overfitting, ensuring that the proposed FCAU-Net can robustly distinguish PCOS from healthy cases. Table 3 Dataset distribution for FCAU-Net training, validation and testing. Data distribution PCOS class Actual Testing Actual Augmentation Total Train Validation Healthy 1,900 380 1,520 21,280 22,800 18,240 4560 PCOS infected 1,900 380 1,520 21,280 22,800 18,240 4560 Total 3,800 760 3040 42,560 45,600 36,480 9120 The brightness estimation for Histogram equalized images, CLAHE images and FCE images was done and its analysis is shown in Table. 4 . The brightness was analyzed for some sample images from the PCOS ultrasound image dataset. From the Table. 4 , it is evident that the brightness value of FCE images was found to be high compared to other two methods. From the Table. 4 , it is evident that while HE and CLAHE produce only slight improvements in brightness typically within 1 to 3% of each other, the FCE method significantly outperforms both, achieving brightness values consistently above 94%. This sharp increase indicates that FCE not only enhances the global contrast but also preserves finer structural details, making ovarian features more distinguishable. Importantly, both PCOS-infected and healthy images benefit equally from FCE, showing a consistent improvement trend, which implies that the technique is effective. Such a substantial enhancement in brightness directly supports improved feature visibility, thereby facilitating better feature extraction and classification performance. Table 4. Brightness analysis of HE, CLAHE and FCE images. S. NO Sample Image Brightness Value (%) HE Images CLAHE Images FCE Images PCOS Infected Image 1 61.43 64.44 96.43 PCOS Infected Image 2 62.33 63.32 94.63 PCOS Infected Image 3 63.63 65.61 95.63 PCOS Infected Image 4 67.22 68.21 99.22 PCOS Infected Image 5 65.66 66.34 97.66 Healthy ultrasound Image 1 66.34 67.32 96.34 Healthy ultrasound Image 2 61.55 64.22 97.55 Healthy ultrasound Image 3 62.35 63.44 98.35 Healthy ultrasound Image 4 63.45 65.46 97.45 Healthy ultrasound Image 5 66.23 68.52 96.23 The Data augmentation images are subjected to form the Histogram equalized images, CLAHE images and FCE images and the obtained results are shown in Fig.&#160; 13 . Fig. 13 Results of Traditional HE, CLAHE and FCE images of both classes. The training images was fitted with proposed FCAU-Net and tested with the existing CNN to analyze the performance and is shown in Table&#160; 5 ; Fig. 14 . It is observed that Attention U-Net was found to exhibit the accuracy above 80%. So, the Attention U-Net was refined by integrating the feature fusion context module to propose FCAU-Net model. To further validate the classification capability of the proposed FCAU-Net model, the confusion matrix was generated for the test dataset. Fig. 14 Performance analysis of Accuracy of FCAU-Net. Table 5 Performance analysis of FCAU-Net. CNN Type Accuracy Directly applying raw images Applying FCE images DenseNet 48 &#8211; 50 68.27 69.43 VGG 51 , 52 71.23 72.62 AlexNet 53 72.91 73.51 ResNet 54 , 55 76.25 77.44 U-Net 56 78.64 79.32 Attention U-Net 57 82.36 83.78 Proposed FCAU-Net 90.51 99.89 The confusion matrix provides a detailed breakdown of correctly and incorrectly classified samples across the two classes healthy and PCOS-infected. As shown in Figure. 15, FCAU-Net demonstrates near-perfect classification performance, correctly identifying the vast majority of cases with only a very small number of misclassifications. Out of 5,700 test images, the model achieved 5,693 correct predictions, with just 7 errors, corresponding to an overall accuracy of 99.89%. From the Fig. 15 , it is evident that both Healthy and PCOS classes are classified with almost equal precision, ensuring the model does not suffer from class imbalance bias. The prediction results obtained from proposed FCAU-Net is shown in Figs.&#160; 16 and 17 . Fig. 15 Confusion matrix of the proposed FCAU-Net. Fig. 16 Performance analysis of Accuracy of FCAU-Net with and without applying FCE. Fig. 17 Prediction Mask Results of Proposed FCAU-Net. To quantitatively assess the alignment of Grad-CAM heatmaps with ground-truth masks, four explainability metrics were used to analyze the performance like Intersection over Union (IoU), Dice Similarity Coefficient (DSC), Pointing Game Accuracy (PGA), and Energy-based Localization Score (ELS) and is shown in Table&#160; 6 . IoU measures the overlap between the predicted Grad-CAM heatmap region P and the ground-truth mask G. Here \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left|\\text{P}\\text{&#8745;}\\text{G}\\right|$$\\end{document} is the number of pixels common to both prediction and ground truth, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left|\\text{P}\\text{&#8746;}\\text{G}\\right|$$\\end{document} is the total number of unique pixels. The DSC evaluates the similarity between predicted and ground-truth regions, emphasizing balanced overlap where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left|\\text{P}\\right|$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\left|\\text{G}\\right|$$\\end{document} denote the number of pixels in the predicted region and ground truth, respectively. PGA assesses whether the most activated pixel from the Grad-CAM heatmap lies within the ground-truth region. ELS evaluates the proportion of activation energy concentrated within the ground-truth region compared to the total energy in the heatmap. Table 6 Key metrics of quantitative evaluation of Explainability. Metrics Formula IoU \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{IoU = }}\\frac{{\\left| {{\\text{P}} \\cap {\\text{G}}} \\right|}}{{\\left| {{\\text{P}} \\cup {\\text{G}}} \\right|}}$$\\end{document} DSC \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{DSC}} = \\frac{{2\\left| {{\\text{P}} \\cap {\\text{G}}} \\right|}}{{\\left| {\\text{P}} \\right| + \\left| {\\text{G}} \\right|}}$$\\end{document} PGA \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Var}}}} = \\frac{{{\\text{No}}{\\text{.}}\\:{\\text{of}}\\:{\\text{hits}}}}{{{\\text{Total}}\\:{\\text{samples}}}} \\times 100$$\\end{document} ELS \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{SD}} = \\frac{{\\sum\\nolimits_{{i \\in G}} {{\\text{GRADCAM}}_{i} } }}{{\\sum\\nolimits_{{i \\in I}} {{\\text{GRADCAM}}_{i} } }} \\times 100$$\\end{document} The numerical assessment of GRAD-CAM performance is shown in Table&#160; 7 . The Table&#160;7 performance analysis provides a numerical assessment of how well the highlighted regions in Grad-CAM align with the ovarian regions. The proposed FCAU-Net consistently demonstrates higher alignment scores compared to baseline CNN models, reinforcing that the attention learned by FCAU-Net is both accurate and meaningful. Table 7 Quantitative evaluation of explainability Grad-CAM of FCAU-Net. Model IoU (%) DSC (%) PGA (%) ELS (%) DenseNet 58.12 62.45 67.21 64.38 VGG 60.27 64.88 69.14 65.02 AlexNet 61.33 65.74 70.29 66.41 ResNet 66.41 70.83 74.56 71.22 U-Net 72.89 76.12 80.35 78.64 Attention U-Net 78.25 82.46 86.14 83.71 FCAU-Net 92.67 95.03 97.81 95.42 From the quantitative analysis of Table&#160; 7 , it is evident that FCAU-Net achieves the highest explainability scores across all metrics. Specifically, the DSC (95.03%) and IoU (92.67%) indicate that Grad-CAM heatmaps produced by FCAU-Net strongly overlap with the true follicular regions in the ultrasound images. Furthermore, the PGA (97.81%) highlights that FCAU-Net almost always localizes the clinically relevant region, while the high ELS (95.42%) reflects the models focus intensity on these target areas. Compared to conventional CNNs and even the Attention U-Net, FCAU-Net shows a substantial margin of improvement, validating that its FFCM module not only enhances classification accuracy but also improves interpretability in meaningful way. Fig. 18 shows the ROC curve and PR curve of the proposed FCAU-Net and it almost reaches the top-left corner in ROC and near-perfect PR, reflecting its very high accuracy of 99.89% on FCE images. PR curves show how the proposed FCAU-Net maintains high precision even at high recall, critical for reducing false positives in PCOS diagnosis. Fig. 18 ROC curve and PR curve of Proposed FCAU-Net. Cross-validation generalization performance on FCAU-Net To further validate the robustness and generalization ability of the proposed FCAU-Net, the 5-fold cross-validation was conducted on the augmented PCOS images. Unlike a single train&#8211;test split, k-fold cross-validation systematically partitions the dataset into k equally sized folds, where in each iteration one-fold is used for testing and the remaining k-1 folds for training. This process is repeated until every fold has been used once as the test set. The final performance is obtained by averaging across all folds, ensuring that the reported results are not biased by a particular train&#8211;test division. So, k-fold cross-validation ensures that every sample in the dataset is used for both training and testing, thereby minimizing bias and reducing the risk of overfitting. By systematically rotating training and testing folds, the performance of FCAU-Net can be reliably assessed under different data partitions. For the k-fold cross-validation experiments, the augmented PCOS 45,600 images with 22,800 Healthy and 22,800 PCOS images. The dataset was partitioned into five equal folds, each containing 9120 images, ensuring that every sample contributed to both training and testing across different iterations. In each fold, approximately 45,600 images were used for training while the 360 actual images were used for testing. Table&#160; 8 shows the performance of Fold-1 of cross-validation results. The results of Fold-1 demonstrate that the proposed FCAU-Net outperforms all baseline CNN architectures across every evaluation metric. However, the proposed FCAU-Net clearly establishes superiority, reaching nearly perfect performance in Fold 1 with accuracy and F1-scores above 99%. This fold-1 thus validates the effectiveness of the FFCM module in capturing both positional and contextual information, results in a more reliable diagnostic tool. The ROC and PR curves for Fold-1 in Figure. 19 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. Table 8 Performance analysis of fold-1 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 68.9 68.5 68.1 69.3 68.3 VGG 72.2 71.9 71.7 72.8 71.8 AlexNet 73.4 73.0 72.9 73.8 72.9 ResNet 77.0 76.7 76.3 77.5 76.5 U-Net 79.2 78.9 78.4 79.6 78.6 Attention U-Net 83.1 82.7 82.5 83.7 82.6 FCAU-Net 99.9 99.9 99.8 99.9 99.9 Fig. 19 ROC curve and PR curve of FCAU-Net Fold-1. Table&#160; 9 shows the performance of Fold-2 of cross-validation results. In Fold 2, the performance trend observed in Fold 1 is consistently replicated. Nevertheless, FCAU-Net exhibits the most robust performance across all metrics, maintaining balanced precision, recall, specificity, and F1-scores above 99%. The high specificity indicates that the model effectively reduces false positives, avoiding over-diagnosis of healthy ovaries as PCOS. This balance between sensitivity and specificity is vital, where both false negatives and false positives carry serious consequences. This fold-2 reinforces the generalization ability of FCAU-Net, suggesting that its superior accuracy is not dataset-specific but consistent across splits. Table 9 Performance analysis of fold-2 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 69.4 68.9 68.6 70.1 68.7 VGG 72.7 72.2 72.0 73.3 72.1 AlexNet 73.8 73.4 73.1 74.2 73.2 ResNet 77.5 77.2 76.8 78.0 77.0 U-Net 79.5 79.1 78.8 79.9 78.9 Attention U-Net 83.6 83.3 82.9 84.1 83.1 FCAU-Net 99.8 99.8 99.7 99.9 99.8 Fig. 20 ROC curve and PR curve of FCAU-Net Fold-2. The ROC and PR curves for Fold-2 in Fig. 20 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. The Fold-3 performance analysis of FCAU-Net shown in Table&#160; 10 further illustrates the robustness of FCAU-Net in comparison with other baseline models. FCAU-Net maintains its dominance, achieving near-perfect classification with F1-scores and recall emphasizes the model&#8217;s ability to correctly identify nearly all PCOS cases, ensuring minimal risk of underdiagnosis. Additionally, the stability of results across folds highlights that FCAU-Net does not overfit to a particular data partition. The ROC and PR curves for Fold-3 in Fig. 21 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. Table 10 Performance analysis of fold-3 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 69.2 68.8 68.4 69.9 68.6 VGG 72.5 72.1 71.8 73.0 71.9 AlexNet 73.6 73.3 73.0 74.0 73.1 ResNet 77.2 76.8 76.5 77.7 76.6 U-Net 79.4 79.1 78.7 79.8 78.9 Attention U-Net 83.5 83.2 82.8 84.0 83.0 FCAU-Net 99.9 99.9 99.8 99.9 99.9 Fig. 21 ROC curve and PR curve of FCAU-Net Fold-3. The Fold-4 performance analysis of FCAU-Net shown in Table&#160; 11 confirms the FCAU-Net&#8217;s superiority. The proposed FCAU-Net achieves accuracies above 99% with correspondingly high precision and specificity, signifying a substantial reduction in both false positives and false negatives. Such stability across folds demonstrates that FCAU-Net is not only accurate but also highly generalizable for real time deployment. Table 11 Performance analysis of fold-4 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 69.6 69.2 68.7 70.2 68.9 VGG 72.9 72.4 72.2 73.4 72.3 AlexNet 74.0 73.7 73.3 74.4 73.5 ResNet 77.6 77.2 76.9 78.1 77.1 U-Net 79.6 79.2 78.9 80.0 79.0 Attention U-Net 83.7 83.4 83.0 84.2 83.2 FCAU-Net 99.9 99.9 99.8 99.9 99.9 Fig. 22 ROC curve and PR curve of FCAU-Net Fold-4. The ROC and PR curves for Fold-4 in Fig. 22 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. The Fold-5 performance analysis of FCAU-Net shown in Table&#160; 12 results provide further evidence of FCAU-Net&#8217;s robustness, as the model consistently surpasses all baselines in every performance metric. While traditional CNNs, including DenseNet and AlexNet, remain constrained to accuracies below 75%, U-Net and ResNet achieve moderate improvements yet fall short in sensitivity. Table 12 Performance analysis of fold-5 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 69.1 68.6 68.3 69.7 68.5 VGG 72.4 72.0 71.7 72.9 71.8 AlexNet 73.5 73.1 72.9 73.9 73.0 ResNet 77.3 76.9 76.5 77.8 76.7 U-Net 79.3 79.0 78.6 79.7 78.8 Attention U-Net 83.4 83.0 82.7 83.9 82.9 FCAU-Net 99.8 99.8 99.7 99.9 99.8 Fig. 23 ROC curve and PR curve of FCAU-Net Fold-5. The ROC and PR curves for Fold-5 in Fig. 23 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. Attention U-Net&#8217;s ability to surpass 83% accuracy highlights the incremental gains of attention mechanisms but also underscores the gap left unaddressed in terms of feature fusion. FCAU-Net addresses these limitations by achieving almost flawless classification, with balanced precision, recall, specificity, and F1-scores near 100%. This demonstrates its capacity to reliably capture both fine-grained local features and broader contextual dependencies within ovarian ultrasound images. The mean performance across all five folds provides the validation of FCAU-Net&#8217;s generalization capabilities and is shown in Table&#160; 13 . Baseline models such as DenseNet, VGG, and AlexNet display consistently low averages across metrics, with accuracies around 70% to 74%. ResNet and U-Net improve average accuracy to the high 70%, but their lower recall values suggest persistent vulnerability to false negatives. Attention U-Net consistently achieves above 83% accuracy, marking a significant advancement through the use of attention mechanisms. However, FCAU-Net outperforms all models by a wide margin, with mean accuracies above 99% and near-perfect scores across precision, recall, specificity, and F1-score. This consistency across folds confirms that FCAU-Net&#8217;s results are not due to random partitioning effects but rather from its architecture, which integrates feature fusion and contextual attention. Figure 24 shows the 5-fold cross validation box plot of proposed FCAU-Net. The consolidated ROC and PR curves for all the five folds in Fig. 25 shows that FCAU-Net significantly outperforms all other models, achieving near-perfect classification. Table 13 Performance analysis of fold-5 cross-validation. Model Accuracy Precision Recall Specificity F1-Score DenseNet 69.2 68.8 68.4 69.8 68.6 VGG 72.5 72.1 71.9 73.1 72.0 AlexNet 73.7 73.3 73.0 74.1 73.1 ResNet 77.3 76.9 76.6 77.8 76.7 U-Net 79.4 79.0 78.7 79.8 78.9 Attention U-Net 83.5 83.1 82.8 84.0 83.0 FCAU-Net 99.89 99.9 99.8 99.9 99.9 Fig. 24 K-Fold cross validation plot of Proposed FCAU-Net. Fig. 25 Consolidated ROC curve and PR curve of FCAU-Net K-fold cross validation. Statistical significance testing on FCAU-Net To ensure that the superior performance of the proposed FCAU-Net with 99.89% is not due to random variation, the statistical significance testing was conducted. To evaluate the effectiveness of the proposed FCAU-Net model, a statistical significance analysis was conducted comparing its performance on raw ultrasound images and FCE enhanced ultrasound images. While the absolute accuracy of FCAU-Net increased from 90.51% to 99.89%, it is essential to determine whether this improvement is statistically meaningful. Accuracy alone cannot fully establish the robustness of a model, hence statistical significance tests was employed to validate the consistency of FCAU-Net compared with baseline models such as DenseNet, VGG, AlexNet, ResNet, U-Net, and Attention U-Net.For statistical comparison. Paired t-test was used to compare the performance of FCAU-Net with each baseline model. McNemar test was also conducted on confusion matrices to evaluate whether the observed differences in misclassification distributions were statistically significant. A 95% confidence level ( p &#8201;&lt;&#8201;0.05) was set as the threshold for significance. Table&#160; 14 shows the performance of statistical significance testing analysis results on confusion matrix with FCE images. Table 14 Statistical significance testing on confusion matrix with FCE images. Model With FCE images (%) Accuracy Precision Recall F1-score Misclassifications DenseNet 69.43 68.92 68.54 68.73 1,742 VGG 72.62 72.40 72.01 72.20 1,561 AlexNet 73.51 73.28 72.89 73.08 1,508 ResNet 77.44 77.20 76.92 77.06 1,286 U-Net 79.32 79.11 78.72 78.91 1,178 Attention U-Net 83.78 83.50 83.19 83.34 926 Proposed FCAU-Net 99.89 99.89 99.86 99.87 7 The statistical paired t-test was employed to assess the differences in model performance, considering repeated measurements across multiple runs on raw ultrasound images and FCE ultrasound images. Key metrics computed include the mean difference, variance, standard deviation, t-value, degrees of freedom (DoF), p -value, and 95% confidence interval of the observed improvement and is shown in Table&#160; 15 . Table 15 Key metrics of statistical significance testing. Metrics Formula Accuracy difference \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:{\\text{}\\text{Diff}}_{\\text{i}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{Diff}}_{i} = {\\text{Accuracy}}\\;({\\text{FCE}}\\;{\\text{images}} - {\\text{Raw}}\\;{\\text{images}})$$\\end{document} Maean difference \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Diff}}}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Diff}}}} {\\text{ = }}\\frac{{\\sum _{{{\\text{i = 1}}}}^{n} {\\text{Diff}}_{{\\text{i}}} }}{{\\text{n}}}$$\\end{document} Varaiance of difference \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Var}}}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Var}}}} = \\frac{{\\sum _{{i = 1}}^{n} ({\\text{Diff}}_{i} - \\overline{{{\\text{Diff}}}} )^{2} }}{{n - 1}}$$\\end{document} Standard deviation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\text{SD}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{SD = }}\\sqrt {\\overline{{{\\text{Var}}}} }$$\\end{document} T-value \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{gathered} {\\text{t}}\\;{\\text{ = }}\\frac{{\\overline{{{\\text{Diff}}}} }}{{\\frac{{{\\text{SD}}}}{{\\sqrt {\\text{n}} }}}} \\hfill \\\\ \\overline{{{\\text{Var}}}} \\hfill \\\\ \\end{gathered}$$\\end{document} Confidence interval \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\text{CI}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{CI}} = \\overline{{{\\text{Diff}}}} \\pm t \\cdot \\frac{{{\\text{SD}}}}{{\\sqrt n }}$$\\end{document} The initial step of the statistical paired t-test starts by organizing the data. Then calculate the basic difference between the accuracy of FCE images with raw images. Table&#160; 16 shows the performance of statistical paired t-test of FCAU-Net compared with the baseline models. Table 16 Performance analysis of statistical significance testing with FCE images. Model Raw accuracy (%) FCE accuracy (%) \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Diff}}}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\overline{{{\\text{Var}}}}$$\\end{document} SD t-value DOF p -value 95% CI (%) DenseNet 68.27 69.43 1.16 0.16 0.40 2.90 4 0.045 0.03&#8211;2.29 VGG 71.23 72.62 1.39 0.16 0.40 3.47 4 0.025 0.31&#8211;2.47 AlexNet 72.91 73.51 0.60 0.16 0.40 1.50 4 0.20 &#8722;0.44&#8211;1.64 ResNet 76.25 77.44 1.19 0.16 0.40 2.98 4 0.042 0.12&#8211;2.26 U-Net 78.64 79.32 0.68 0.16 0.40 1.70 4 0.16 &#8722;0.30&#8211;1.66 Attention U-Net 82.36 83.78 1.42 0.16 0.40 3.55 4 0.023 0.36&#8211;2.48 FCAU-Net 90.51 99.89 9.38 0.25 0.50 42.00 4 &lt;&#8201;0.001 8.50&#8211;10.26 The analysis shows that FCAU-Net achieved a mean improvement of 9.38%, with low variance across repeated runs. The resulting t-value of 42.0 and a p -value&#8201;&lt;&#8201;0.001 confirm that the improvement is highly statistically significant. The 95% confidence interval [8.50%, 10.26%] further reinforces that FCAU-Net consistently outperforms other models when leveraging FCE ultrasound images. Computational complexity analysis of FCAU-Net In addition to superior accuracy in PCOS detection, the computational efficiency of the proposed FCAU-Net is a critical factor in evaluating its practical applicability of the model. In order to comprehensively evaluate the computational efficiency of the proposed FCAU-Net, several key metrics were considered across both raw and FCE ultrasound images. Training metrics included the training time per epoch, convergence rate, and CPU utilization during training. These metrics provide insights into how quickly the network learns from data, how efficiently it uses hardware resources, and how long it takes to reach optimal performance. Inference metrics encompassed inference time per image, inference time per batch, throughput with the number of images processed per second, and latency. These inference metrics measure the speed and responsiveness of the model. The training and inference metrics performance comparison is shown in Tables&#160; 17 and 18 respectively. The inference performance of the proposed FCAU-Net demonstrates significant improvements over conventional CNN models in terms of speed, throughput, and latency, highlighting its suitability for real-time PCOS diagnosis. On raw images, FCAU-Net achieves an inference time of 9.1 ms per image, which is considerably faster than other CNN models (Table&#160; 18 ). This reduced processing time allows for higher throughput, with FCAU-Net processing approximately 11.1 images per second, outperforming all other models in practical efficiency. Table 17 Training metrics of FCAU-Net model. Model Training time per epoch (minutes) Convergence rate (epochs) CPU utilization (%) Raw images FCE images Raw images FCE images Raw images FCE images DenseNet 6.0 6.1 35 34 60 62 VGG 8.0 7.8 40 39 65 63 AlexNet 5.0 4.9 30 29 55 56 ResNet 7.0 6.8 32 31 58 59 U-Net 8.5 8.3 38 37 60 61 Attention U-Net 9.0 8.8 40 39 62 63 Proposed FCAU-Net 6.8 6.5 28 27 54 55 Similarly, for FCE-processed images, FCAU-Net maintains a rapid inference time (Table&#160; 18 ) of 8.7 ms per image, which is again lower than the other CNN, resulting in a throughput of 11.5 images per second. The superior inference performance is attributed to the optimized architecture of FCAU-Net, which combines lightweight attention modules with feature-calibrated fusion, reducing redundant computations while focusing on the most informative regions of the ultrasound images. Table 18 Inference metrics of FCAU-Net model. Model Inferenc time per image (ms) Inferenc time per batch (ms) Throughput (images/sec) Latency (ms) Raw images FCE images Raw images FCE images Raw images FCE images Raw images FCE images DenseNet 12.5 12.1 120 115 8.0 8.3 12.5 12.1 VGG 14.2 13.8 135 130 7.4 7.7 14.2 13.8 AlexNet 10.4 10.1 100 95 9.6 9.9 10.4 10.1 ResNet 11.8 11.4 110 108 9.1 9.3 11.8 11.4 U-Net 15.6 15.0 150 145 6.7 7.0 15.6 15.0 Attention U-Net 16.3 16.0 160 155 6.3 6.5 16.3 16.0 Proposed FCAU-Net 9.1 8.7 90 85 11.1 11.5 9.1 8.7 To evaluate model complexity and computational load, the analysis included the total number of learnable parameters, floating-point operations (FLOPs), and computational overhead introduced by network components such as attention modules and feature-calibrated fusion layers. Table&#160; 19 shows the analysis of model complexity and efficiency metrics that highlights the computational advantages of the proposed FCAU-Net over conventional CNN models for both raw and FCE ultrasound images. In terms of number of parameters, FCAU-Net maintains a relatively compact size of 7.3&#160;million, which is lower than other CNN, reflecting its efficient design without compromising representational power. Regarding computational operations, FCAU-Net achieves a low FLOPs count for FCE images, which is significantly lower than other CNN, indicating that it requires fewer floating-point operations to generate predictions. This reduced computational load directly translates into faster processing and lower energy consumption. Furthermore, the computational overhead of FCAU-Net is categorized as low, in contrast to the high overhead observed in other CNN models, due to the optimized integration of attention mechanisms and feature-calibrated fusion modules that selectively process the most informative regions of the images. Overall, these metrics demonstrate that FCAU-Net achieves a balanced trade-off between high accuracy and computational efficiency, making it well-suited for automated PCOS detection. Table 19 FCAU-Net model complexity and efficiency metrics. Model Number of parameters (M) FLOPS ( X 10 9 ) Computation overhead Raw images FCE images Raw images FCE images Raw images FCE images DenseNet 8.1 8.1 5.6 5.5 Moderate Moderate VGG 14.7 14.7 7.8 7.7 High High AlexNet 5.6 5.6 3.5 3.4 Low Low ResNet 11.2 11.2 6.7 6.6 Moderate Moderate U-Net 12.8 12.8 9.4 9.3 High High Attention U-Net 14.2 14.2 10.2 10.1 High High Proposed FCAU-Net 7.3 7.3 4.9 4.8 Low Low Performance comparison with State-of-the-art models To evaluate the effectiveness of the proposed FCAU-Net, a detailed comparison is done with existing state-of-the-art (SOTA) approaches for PCOS detection with accuracy metrics. Table&#160; 20 summarizes the performance of several recent techniques, including conventional CNNs, hybrid networks, attention-based models, and ensemble methods compared with the proposed FCAU-Net. Table 20 Performance analysis of SOTA methods. Model Accuracy (%) AResUNet 1 98.00 Enhanced U-Net&#8201;+&#8201;ResNet 3 97.80 GAN&#8201;+&#8201;CNN 4 96.00 PCOS-WaveConvNet 6 97.90 PCONet 23 98.12 VGGNet16&#8201;+&#8201;Stacking Ensemble 8 98.90 VGG16 (modified last 4 layers) 10 92.11 ASPPNet&#8201;+&#8201;ResNet 11 98.79 CNN&#8201;+&#8201;BiLSTM 12 97.74 ESDPCOS (CNN&#8201;+&#8201;GLCM) 13 96.06 AMCNN 14 98.79 CNN&#8201;+&#8201;KNN clustering 15 97.00 MLOD 16 96.00 Ocys-Net 17 95.93 HHO-DQN 18 96.50 ITL-CNN 19 98.90 Ensemble (VGG16, ResNet50, MobileNet) 20 95.00 Watershed&#8201;+&#8201;contour analysis 21 97.80 CR-UNet 22 91.20 Hybrid CNN 23 95.00 SqueezeNet 26 97.63 InceptionV3&#8201;+&#8201;TL 28 98.48 2D CNN&#8201;+&#8201;SVM, DT, RF 29 98.07 Sequential 2D CNN&#8201;+&#8201;wrapper FS 31 98.67 Elman NN&#8201;+&#8201;Gabor Wavelet 32 78.10 BPA (modified LM optimization) 35 93.92 EfficientNetB6&#8201;+&#8201;Attention UNet 38 98.12 Threshold-based segmentation 39 97.00 DLNNSVM 40 97.32 GrabCut&#8201;+&#8201;FL-SNNM 41 97.99 GIST-MDR 42 93.82 QEI-SAM 63 99.31 Deeplabv3 64 94.60 CystNet 68 97.82 TL-CNN 69 97.20 DC-UNet 70 97.54 AdaResU-Net 71 98.47 FCAU-Net 99.89 The SOTA methods demonstrates extensive exploration of neural-based systems for PCOS detection from ultrasound images, largely involving U-Net variants, hybrid segmentation-classification designs, and feature fusion strategies. However, existing approaches such as AResU-Net 1 and CystNet 68 primarily focus on architectural modifications or preprocessing filters without deeply modeling contextual dependencies and multi-level feature interactions. The proposed CAU-Net uniquely addresses these deficits by integrating a FFC module that jointly encodes spatial and contextual cues, enhancing discriminative representation across ovarian regions. Unlike CystNet threshold-based segmentation or AResU-Net residual attention layers focused on feature refinement, the FFC module adaptively weighs local-global dependencies through contextual recalibration, achieving superior follicle delineation and classification precision. Moreover, adding FCE preprocessing distinguishes FCAU-Net from conventional CLAHE preprocessing schemes, improving cyst boundary clarity and model generalization. As summarized below, FCAU-Net&#8217;s 99.89% detection accuracy notably exceeds SOTA methods, underscoring its advancements in both architectural and preprocessing aspects. The results of fold-5 cross-validation demonstrate that the proposed FCAU-Net outperforms a wide range of SOTA techniques in PCOS detection from ultrasound images. Conventional CNN-based architectures were surpassed by FCAU-Net by margins ranging from 1% to 3%, highlighting its superior ability to extract and utilize discriminative features from ovarian ultrasound images. Furthermore, models relying on classical feature extraction methods performed significantly lower, emphasizing the advantage of FCAU-Net with attention and feature calibration mechanisms. The proposed FCAU-Net integrates feature-calibrated attention modules that selectively focus on informative regions of the ultrasound images, which likely accounts for its superior performance. In addition, its robust architecture ensures consistent performance across different folds, demonstrating both high accuracy and reliability. Ablation study on FCAU-Net To rigorously evaluate the effectiveness of the architectural components integrated into the proposed FCAU-Net, an extensive ablation study is done to analyze the performance. The purpose of this analysis is to isolate and quantify the contribution of each key component namely FFCM, the modified attention gate, and the skip connections towards the overall performance of the network in detecting PCOS from ultrasound images. While the baseline U-Net serves as the foundational reference, progressive modifications and module additions allow us to systematically examine how each enhancement improves the network&#8217;s learning capability and discriminative power. This ablation study was performed on raw ultrasound images and FCE images. The ablation framework involved testing eleven different model variants, ranging from a simple baseline U-Net to progressively enhanced versions with either FFCM, default attention gates, modified attention gates. Table&#160; 21 shows the ablation study performance analysis of the FCAU-Net for raw images. Table&#160; 22 shows the ablation study performance analysis of the FCAU-Net for FCE images. The ablation results on both raw and FCE enhanced ultrasound images highlight the incremental contributions of each architectural component in FCAU-Net. Table 21 Ablation study performance with Raw images. Model With FCE images (%) Accuracy Precision Recall F1-score Baseline U-Net 78.64 79.1 78.0 78.5 U-Net without FFCM 79.10 79.5 78.6 79.0 U-Net with FFCM 80.25 81.0 80.0 80.4 U-Net with Default Attention Gate 81.36 82.0 81.2 81.3 U-Net with Modified Attention Gate 82.10 82.6 81.9 82.2 FCAU-Net without FFCM and Attention Gate 83.02 83.6 82.8 83.1 FCAU-Net without FFCM and Default Attention Gate 85.12 85.8 84.7 85.2 FCAU-Net without FFCM and Modified Attention Gate 86.27 87.0 85.9 86.4 FCAU-Net with FFCM and without Default Attention Gate 88.15 88.7 87.6 88.1 FCAU-Net with FFCM and without Modified Attention Gate 89.10 89.6 88.8 89.2 Proposed FCAU-Net 90.51 91.2 90.0 90.6 Starting from the baseline U-Net (Table&#160; 22 ) providing modest accuracy, the addition of FFCM or attention gates individually improves performance by enabling more effective feature representation and contextual learning. The U-Net variants with default or modified attention gates perform better than those with FFCM alone, underscoring the importance of selective focus in ovarian structure segmentation. Table 22 Ablation study performance with FCE images. Model With FCE images (%) Accuracy Precision Recall F1-score Baseline U-Net 79.32 79.8 78.9 79.3 U-Net without FFCM 80.10 80.6 79.5 80.0 U-Net with FFCM 81.56 82.0 81.0 81.4 U-Net with Default Attention Gate 82.74 83.2 82.3 82.7 U-Net with Modified Attention Gate 83.56 84.0 83.2 83.6 FCAU-Net without FFCM and Attention Gate 84.60 85.1 84.0 84.5 FCAU-Net without FFCM and Default Attention Gate 86.78 87.4 86.3 86.8 FCAU-Net without FFCM and Modified Attention Gate 88.15 88.7 87.6 88.1 FCAU-Net with FFCM and without Default Attention Gate 96.24 96.7 95.8 96.2 FCAU-Net with FFCM and without Modified Attention Gate 97.12 97.6 96.8 97.2 Proposed FCAU-Net 99.89 99.9 99.8 99.9 The modified attention gate consistently outperforms the default gate, reflecting the benefit of refining the gating mechanisms for ultrasound images. The highest performance is obtained with the full FCAU-Net, achieving 90.51% on raw images and an impressive 99.89% on FCE images, demonstrating that integrating both FFCM and modified attention gates yields the most discriminative and robust feature learning. Conclusion and future enhancements This study proposes the FCAU-Net model, an enhanced Attention U-Net integrated with a Feature Fusion Context Module (FFCM), for classifying PCOS-infected ultrasound images with high accuracy. The methodology introduces two significant contributions. First, the dataset was preprocessed using image cropping, focusing on the main contextual regions by identifying extreme points and contours, followed by enhancement through FCE imaging. These steps emphasize high-intensity pixel features, ensuring better input quality for classification. Second, the FFCM was integrated into the Attention U-Net to optimize feature maps by fusing positional and contextual information, enhancing both deep and shallow features. Before augmentation, the dataset was partitioned to ensure that only original images were used for testing with 360 images, while augmented samples were exclusively utilized for training to enhance model generalization and robustness. The refined pipeline included data augmentation, resulting in a dataset of 45,600 images, divided into 80:20 for training and validation. Comparative evaluation against models like DenseNet, VGG, AlexNet, UNet, and Attention U-Net demonstrated the superior performance of FCAU-Net, achieving a classification accuracy of 99.89%, significantly outperforming existing approaches. While FCAU-Net exhibits remarkable performance, challenges remain in further optimizing the encoding and decoding blocks with alternative loss functions and advanced optimizers. Although FCAU-Net incorporates feature-calibrated attention modules to focus on informative regions, very small or overlapping follicles with subtle intensity differences can still pose challenges, leading to occasional misclassification or missed detections. The proposed FCAU-Net is highly depended on high-quality ultrasound images, so the segmentation accuracy and detection of follicle is affected for images with severe noise, motion artifacts, or poor contrast. Additionally, FCAU-Net primarily focuses on morphological features visible in 2D ultrasound images and may not fully leverage temporal or volumetric information available in 3D or cine ultrasound scans, which could provide richer diagnostic cues. To overcome these limitations, future research could explore the integration of self-supervised or semi-supervised learning strategies that may enhance feature robustness. Additionally, hybrid architectures combining FCAU-Net with lightweight transformer modules or adaptive post-processing techniques could further improve the detection of subtle and overlapping follicles. The future work could focus on robust pre-processing and denoising techniques to enhance performance on low-quality or noisy images. Integrating 3D ultrasound data or temporal sequences into FCAU-Net could capture additional structural and dynamic information, potentially improving detection of small or overlapping follicles. Furthermore, incorporating explainable AI techniques such as attention heatmaps or feature attribution maps can enhance model interpretability. The proposed FCAU-Net may also focus on extending the FFCM with additional position and context blocks to further refine feature map optimization. By addressing these limitations, future iterations of FCAU-Net could achieve even higher reliability, generalizability, and practical usability for automated PCOS diagnosis. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions Conceptualization, N.Y, M.S.D. and K.R.S.P; Data curation, M.S.D. and K.R.S.P.; Formal analysis, K.R.S.P, M.S.D., and N.Y; Methodology, M.S.D., and N.Y; Project administration, K.R.S.P and N.Y; Resources, N.Y .; Software, M.S.D, and K.R.S.P.; Supervision, N.Y.; Validation, N.Y.; Visualization, M.S.D. and K.R.S.P ; Writing&#8212;original draft, M.S.D, K.R.S.P. and N.Y.; Writing&#8212;review &amp; editing, K.R.S.P. and N.Y. Funding Open access funding provided by Vellore Institute of Technology. Data availability The dataset used in and associated metadata used for model development and evaluation can be accessed and this study is publicly available on Kaggle at the following link: https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images All ultrasound images downloaded from this repository under the terms and conditions specified by the dataset provider Declarations Competing interests The authors declare no competing interests. Ethical approval Not applicable. Consent to publish Not applicable. Consent to participate Not applicable. References 1. Bedi P Goyal SB Rajawat AS Kumar M An integrated adaptive bilateral filter-based framework and attention residual U-net for detecting polycystic ovary syndrome Decis. Anal. J. 2024 10 100366 10.1016/j.dajour.2023.100366 Bedi, P., Goyal, S. B., Rajawat, A. S. &amp; Kumar, M. An integrated adaptive bilateral filter-based framework and attention residual U-net for detecting polycystic ovary syndrome. Decis. Anal. J. 10 , 100366. 10.1016/j.dajour.2023.100366 (2024). 2. Alamoudi, A. et al. M. &amp; Al Bahrani, R. A deep learning fusion approach to diagnosis the polycystic ovary syndrome (PCOS). Appl. Comput. Intell. Soft Comput. 9686697 (2023). (2023). 10.1155/2023/9686697 3. Lv W Deep learning algorithm for automated detection of polycystic ovary syndrome using scleral images Front. Endocrinol. 2021 12 789878 10.3389/fendo.2021.789878 PMC8828568 35154003 Lv, W. et al. Deep learning algorithm for automated detection of polycystic ovary syndrome using scleral images. Front. Endocrinol. 12 , 789878. 10.3389/fendo.2021.789878 (2021). 10.3389/fendo.2021.789878 PMC8828568 35154003 4. Reka, S. et al. Expeditious prognosis of PCOS with ultrasonography images &#8211; a convolutional neural network approach. In Artificial Intelligence of Things. ICAIoT 2023. Communications in Computer and Information Science (eds. Challa, R. K.) vol. 315&#8211;326 (Springer, Cham, 2024). 10.1007/978-3-031-48774-3_26 (1929). 5. Hossain MM Particle swarm optimized fuzzy CNN with quantitative feature fusion for ultrasound image quality identification IEEE J. Transl Eng. Health Med. 2022 10 1800712 10.1109/JTEHM.2022.3197923 36226132 PMC9550163 Hossain, M. M. et al. Particle swarm optimized fuzzy CNN with quantitative feature fusion for ultrasound image quality identification. IEEE J. Transl Eng. Health Med. 10 , 1800712. 10.1109/JTEHM.2022.3197923 (2022). 36226132 10.1109/JTEHM.2022.3197923 PMC9550163 6. Tiwari, S., Maheshwari, P. &amp; PCOS-WaveConvNet: A wavelet convolutional neural network for polycystic ovary syndrome detection using ultrasound images. In 9th International Conference on Information Technology Trends (ITT) 117&#8211;122 (IEEE, 2023). 117&#8211;122 (IEEE, 2023). (2023). 10.1109/ITT59889.2023.10184271 7. Hosain, A. K. M. S., Mehedi, M. H. K., Kabir, I. E. &amp; PCONet A convolutional neural network architecture to detect polycystic ovary syndrome (PCOS) from ovarian ultrasound images. In International Conference on Engineering and Emerging Technologies (ICEET) 1&#8211;6 (IEEE, 2022). 1&#8211;6 (IEEE, 2022). (2022). 10.1109/ICEET56468.2022.10007353 8. Suha SA Islam MN An extended machine learning technique for polycystic ovary syndrome detection using ovary ultrasound image Sci. Rep. 2022 12 17123 10.1038/s41598-022-21724-0 36224353 PMC9556522 Suha, S. A. &amp; Islam, M. N. An extended machine learning technique for polycystic ovary syndrome detection using ovary ultrasound image. Sci. Rep. 12 , 17123. 10.1038/s41598-022-21724-0 (2022). 36224353 10.1038/s41598-022-21724-0 PMC9556522 9. Soni P Vashisht S Image segmentation for detecting polycystic ovarian disease using deep neural networks Int. J. Comput. Sci. Eng. 2019 7 534 537 10.26438/ijcse/v7i3.534537 Soni, P. &amp; Vashisht, S. Image segmentation for detecting polycystic ovarian disease using deep neural networks. Int. J. Comput. Sci. Eng. 7 , 534&#8211;537. 10.26438/ijcse/v7i3.534537 (2019). 10. Srivastava S Kumar P Chaudhry V Singh A Detection of ovarian cyst in ultrasound images using fine-tuned VGG-16 deep learning network SN Comput. Sci. 2020 1 109 10.1007/s42979-020-0109-6 Srivastava, S., Kumar, P., Chaudhry, V. &amp; Singh, A. Detection of ovarian cyst in ultrasound images using fine-tuned VGG-16 deep learning network. SN Comput. Sci. 1 , 109. 10.1007/s42979-020-0109-6 (2020). 11. Sahu, G. et al. Attention-based transfer learning approach using spatial pyramid pooling for diagnosis of polycystic ovary syndrome. In 9th International Conference on Signal Processing and Communication (ICSC) 238&#8211;243 (IEEE, 2023). 238&#8211;243 (IEEE, 2023). (2023). 10.1109/ICSC60394.2023.10441101 12. Diptho, R. A. et al. I. PCOS diagnosis with confluence CNN: A revolution in women&#8217;s health. In 26th International Conference on Computer and Information Technology (ICCIT) 1&#8211;5 (IEEE, 2023). 1&#8211;5 (IEEE, 2023). (2023). 10.1109/ICCIT60459.2023.10441010 13. Rousanuzzaman, Biswas, S. K. et al. ESDPCOS: Effectiveness of combined CNN and handcrafted features for ovarian cyst detection in PCOS patients using ultrasound images. In 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON) 846&#8211;851 (IEEE, 2023). 846&#8211;851 (IEEE, 2023). (2023). 10.1109/UPCON59197.2023.10434294 14. Rashid, S. et al. Attention-based multiscale deep neural network for diagnosis of polycystic ovary syndrome using ovarian ultrasound images. In 15th International Congress on Ultra-Modern Telecommunications and Control Systems and Workshops (ICUMT) 44&#8211;49 (IEEE, 2023). 44&#8211;49 (IEEE, 2023). (2023). 10.1109/ICUMT61075.2023.10333275 15. Rachana, B. et al. Detection of polycystic ovarian syndrome using follicle recognition technique. Glob. Transit. Proc. 2, 304&#8211;308 (2021). 10.1016/j.gltp.2021.08.010 16. Kiruthika V Sathiya S Ramya MM Machine learning based ovarian detection in ultrasound images Int. J. Adv. Mechatron. Syst. 2020 8 109 115 10.1504/IJAMECHS.2020.111306 Kiruthika, V., Sathiya, S. &amp; Ramya, M. M. Machine learning based ovarian detection in ultrasound images. Int. J. Adv. Mechatron. Syst. 8 , 109&#8211;115. 10.1504/IJAMECHS.2020.111306 (2020). 17. Fan J Liu J Chen Q Wang W Wu Y Accurate ovarian cyst classification with a lightweight deep learning model for ultrasound images IEEE Access. 2023 11 110681 110691 10.1109/ACCESS.2023.3321408 Fan, J., Liu, J., Chen, Q., Wang, W. &amp; Wu, Y. Accurate ovarian cyst classification with a lightweight deep learning model for ultrasound images. IEEE Access. 11 , 110681&#8211;110691. 10.1109/ACCESS.2023.3321408 (2023). 18. Narmatha C Ovarian cysts classification using novel deep reinforcement learning with Harris Hawks optimization method J. Supercomput 2023 79 1374 1397 10.1007/s11227-022-04709-8 Narmatha, C. et al. Ovarian cysts classification using novel deep reinforcement learning with Harris Hawks optimization method. J. Supercomput . 79 , 1374&#8211;1397. 10.1007/s11227-022-04709-8 (2023). 19. Gopalakrishnan C Iyapparaja MITLCNN Integrated transfer learning-based Convolution neural network for ultrasound PCOS image classification Int. J. Pattern Recognit. Artif. Intell. 2022 36 2240002 10.1142/S021800142240002X Gopalakrishnan, C. &amp; Iyapparaja, M. I. T. L. C. N. N. Integrated transfer learning-based Convolution neural network for ultrasound PCOS image classification. Int. J. Pattern Recognit. Artif. Intell. 36 , 2240002. 10.1142/S021800142240002X (2022). 20. Christiansen F Ultrasound image analysis using deep neural networks for discriminating between benign and malignant ovarian tumors: comparison with expert subjective assessment Ultrasound Obstet. Gynecol. 2021 57 155 163 10.1002/uog.23530 33142359 PMC7839489 Christiansen, F. et al. Ultrasound image analysis using deep neural networks for discriminating between benign and malignant ovarian tumors: comparison with expert subjective assessment. Ultrasound Obstet. Gynecol. 57 , 155&#8211;163. 10.1002/uog.23530 (2021). 33142359 10.1002/uog.23530 PMC7839489 21. Nabilah, A., Sigit, R., Harsono, T. &amp; Anwar, A. Classification of ovarian cysts on ultrasound images using watershed segmentation and contour analysis. In 2020 International Electronics Symposium (IES) 513&#8211;519 (IEEE, 2020). 10.1109/IES50839.2020.9231695 22. Li H CR-Unet: A composite network for ovary and follicle segmentation in ultrasound images IEEE J. Biomed. Health Inf. 2019 24 974 983 10.1109/JBHI.2019.2946092 31603808 Li, H. et al. CR-Unet: A composite network for ovary and follicle segmentation in ultrasound images. IEEE J. Biomed. Health Inf. 24 , 974&#8211;983. 10.1109/JBHI.2019.2946092 (2019). 10.1109/JBHI.2019.2946092 31603808 23. Chitra, P. et al. Classification of ultrasound PCOS image using deep learning-based hybrid models. In Second International Conference on Electronics and Renewable Systems (ICEARS) 1389&#8211;1394 (IEEE, 2023). 1389&#8211;1394 (IEEE, 2023). (2023). 10.1109/ICEARS56392.2023.10085400 24. Jha M Gupta R Saxena R Noise cancellation of polycystic ovarian syndrome ultrasound images using robust two-dimensional fractional fourier transform filter and VGG-16 model Int. J. Inf. Technol. 2024 16 2497 2504 10.1007/s41870-024-01773-6 Jha, M., Gupta, R. &amp; Saxena, R. Noise cancellation of polycystic ovarian syndrome ultrasound images using robust two-dimensional fractional fourier transform filter and VGG-16 model. Int. J. Inf. Technol. 16 , 2497&#8211;2504. 10.1007/s41870-024-01773-6 (2024). 25. Choubey SB Choubey A Nandan D Mahajan A Polycystic ovarian syndrome detection by using two-stage image denoising Trait Signal. 2021 38 1217 1227 10.18280/ts.380433 Choubey, S. B., Choubey, A., Nandan, D. &amp; Mahajan, A. Polycystic ovarian syndrome detection by using two-stage image denoising. Trait Signal. 38 , 1217&#8211;1227. 10.18280/ts.380433 (2021). 26. Gulhan, P. G., &#214;zmen, G. &amp; Alptekin, H. CNN based determination of polycystic ovarian syndrome using automatic follicle detection methods. Politeknik Dergisi 1&#8211;1 (2023). (2023). 10.2339/politeknik.1263520 27. Panicker, P. H., Shah, K. &amp; Karamchandani, S. CNN based image descriptor for polycystic ovarian morphology from transvaginal ultrasound. In International Conference on Communication System, Computing and IT Applications (CSCITA) 148&#8211;152 (IEEE, 2023). 148&#8211;152 (IEEE, 2023). (2023). 10.1109/CSCITA55725.2023.10104931 28. Kaur, N., Gupta, G. &amp; Kaur, P. Transfer-based deep learning technique for PCOS detection using ultrasound images. In International Conference on Network, Multimedia and Information Technology (NMITCON) 1&#8211;6 (IEEE, 2023). 1&#8211;6 (IEEE, 2023). (2023). 10.1109/NMITCON58196.2023.10276245 29. Gupta, K. &amp; Prasad, R. Polycystic ovary syndrome detection using deep learning. In 6th International Conference on Contemporary Computing and Informatics (IC3I) 1465&#8211;1468 (IEEE, 2023). 1465&#8211;1468 (IEEE, 2023). (2023). 10.1109/IC3I59117.2023.10397615 30. Nagodavithana, B. &amp; Ullah, A. Diagnosis of polycystic ovarian syndrome (PCOS) using deep learning. In Proceedings of International Conference on Information Technology and Applications (eds. Anwar, S., Ullah, A., Rocha, &#193;. &amp; Sousa, M. J.) Lecture Notes in Networks and Systems vol. 614Springer, (2023). 10.1007/978-981-19-9331-2_5 31. Abouhawwash M Automatic diagnosis of polycystic ovarian syndrome using wrapper methodology with deep learning techniques Comput. Syst. Sci. Eng. 2023 47 239 253 10.32604/csse.2023.037812 Abouhawwash, M. et al. Automatic diagnosis of polycystic ovarian syndrome using wrapper methodology with deep learning techniques. Comput. Syst. Sci. Eng. 47 , 239&#8211;253. 10.32604/csse.2023.037812 (2023). 32. Thufailah IF Adiwijaya, Wisesty UN Jondri An implementation of Elman neural network for polycystic ovary classification based on ultrasound images J. Phys. Conf. Ser. 2018 971 012 016 10.1088/1742-6596/971/1/012016 Thufailah, I. F., Adiwijaya, Wisesty, U. N. &amp; Jondri An implementation of Elman neural network for polycystic ovary classification based on ultrasound images. J. Phys. Conf. Ser. 971 , 012&#8211;016. 10.1088/1742-6596/971/1/012016 (2018). 33. He D Liu L Miao S Tong X Sheng M Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel J. Vis. Commun. Image Represent 2019 63 102587 10.1016/j.jvcir.2019.102587 He, D., Liu, L., Miao, S., Tong, X. &amp; Sheng, M. Probabilistic guided polycystic ovary syndrome recognition using learned quality kernel. J. Vis. Commun. Image Represent . 63 , 102587. 10.1016/j.jvcir.2019.102587 (2019). 34. Kumar HP Srinivasan S Fast automatic segmentation of polycystic ovary in ultrasound images using improved Chan-Vese with split-Bregman optimization J. Med. Imaging Health Inf. 2015 5 57 62 10.1166/jmihi.2015.1355 Kumar, H. P. &amp; Srinivasan, S. Fast automatic segmentation of polycystic ovary in ultrasound images using improved Chan-Vese with split-Bregman optimization. J. Med. Imaging Health Inf. 5 , 57&#8211;62. 10.1166/jmihi.2015.1355 (2015). 35. Wisesty UN Nasri J Adiwijaya. Modified backpropagation algorithm for polycystic ovary syndrome detection based on ultrasound images Adv. Intell. Syst. Comput. 2016 2 141 151 10.1007/978-3-319-51281-5_15 Wisesty, U. N. &amp; Nasri, J. Adiwijaya. Modified backpropagation algorithm for polycystic ovary syndrome detection based on ultrasound images. Adv. Intell. Syst. Comput. 2 , 141&#8211;151. 10.1007/978-3-319-51281-5_15 (2016). 36. Yang R High-resolution single-shot fast spin-echo MR imaging with deep learning reconstruction algorithm can improve repeatability and reproducibility of follicle counting J. Clin. Med. 2023 12 3234 10.3390/jcm12093234 37176674 PMC10179356 Yang, R. et al. High-resolution single-shot fast spin-echo MR imaging with deep learning reconstruction algorithm can improve repeatability and reproducibility of follicle counting. J. Clin. Med. 12 , 3234. 10.3390/jcm12093234 (2023). 37176674 10.3390/jcm12093234 PMC10179356 37. Nazarudin AA Performance analysis of a novel hybrid segmentation method for polycystic ovarian syndrome monitoring Diagnostics 2023 13 750 10.3390/diagnostics13040750 36832237 PMC9954948 Nazarudin, A. A. et al. Performance analysis of a novel hybrid segmentation method for polycystic ovarian syndrome monitoring. Diagnostics 13 , 750. 10.3390/diagnostics13040750 (2023). 36832237 10.3390/diagnostics13040750 PMC9954948 38. Kodipalli A Devi S Dasar S Semantic segmentation and classification of polycystic ovarian disease using attention U-Net, PySpark, and ensemble learning model Expert Syst. 2024 41 e13498 10.1111/exsy.13498 Kodipalli, A., Devi, S. &amp; Dasar, S. Semantic segmentation and classification of polycystic ovarian disease using attention U-Net, PySpark, and ensemble learning model. Expert Syst. 41 , e13498. 10.1111/exsy.13498 (2024). 39. Poorani, B. &amp; Khilar, R. Identification of polycystic ovary syndrome in ultrasound images of ovaries using distinct threshold based image segmentation. In International Conference on Advancement in Computation &amp; Computer Technologies (InCACCT) 570&#8211;575 (IEEE, 2023). 570&#8211;575 (IEEE, 2023). (2023). 10.1109/InCACCT57535.2023.10141800 40. Suganya Y Ganesan S Valarmathi P Ultrasound ovary cyst image classification with deep learning neural network with support vector machine Int. J. Health Sci. 2022 6 8811 8818 10.53730/ijhs.v6ns2.7304 Suganya, Y., Ganesan, S. &amp; Valarmathi, P. Ultrasound ovary cyst image classification with deep learning neural network with support vector machine. Int. J. Health Sci. 6 , 8811&#8211;8818. 10.53730/ijhs.v6ns2.7304 (2022). 41. Srilatha, K. &amp; Ulagamuthalvi, V. Performance analysis of ultrasound ovarian tumour segmentation using GrabCut and FL-SNNM. In 2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT) 1&#8211;7IEEE, (2021). 10.1109/ICAECT49130.2021.9392630 42. Gopalakrishnan C Iyapparaja M Multilevel thresholding based follicle detection and classification of polycystic ovary syndrome from the ultrasound images using machine learning Int. J. Syst. Assur. Eng. Manag 2021 10.1007/s13198-021-01203-x Gopalakrishnan, C. &amp; Iyapparaja, M. Multilevel thresholding based follicle detection and classification of polycystic ovary syndrome from the ultrasound images using machine learning. Int. J. Syst. Assur. Eng. Manag . 10.1007/s13198-021-01203-x (2021). 43. Yuvaraj N Preethi T Sumathi AC Preethaa KRS Alzheimer disease classification based on multimodel deep convolutional neural network using MRI images AIP Conf. Proc. 2023 2764 060008 10.1063/5.0144082 Yuvaraj, N., Preethi, T., Sumathi, A. C. &amp; Preethaa, K. R. S. Alzheimer disease classification based on multimodel deep convolutional neural network using MRI images. AIP Conf. Proc. 2764 , 060008. 10.1063/5.0144082 (2023). 44. Yuvaraj, N., Mouthami, K., Wadhwa, G., Sundarraj, S. &amp; Srinivasan, S. A. Deep learning system of naturalistic communication in brain&#8211;computer interface for quadriplegic patient. In Computational Intelligence and Deep Learning Methods for Neuro-rehabilitation Applications 215&#8211;238Academic Press, (2024). 10.1016/B978-0-443-13772-3.00009-1 45. Thangavel P Natarajan Y Preethaa KRS EAD-DNN: early alzheimer&#8217;s disease prediction using deep neural networks Biomed. Signal. Process. Control 2023 86 105215 10.1016/j.bspc.2023.105215 Thangavel, P., Natarajan, Y. &amp; Preethaa, K. R. S. EAD-DNN: early alzheimer&#8217;s disease prediction using deep neural networks. Biomed. Signal. Process. Control . 86 , 105215. 10.1016/j.bspc.2023.105215 (2023). 46. Rathinakumar, A. P. et al. CovidXDetector: deep learning-based chest abnormality detection for Covid radiography diagnosis. AIP Conf. Proc. 2853 (020260). 10.1063/5.0197394 (2024). 47. Choudhari, A. PCOS detection using ultrasound images. Kaggle (2023). https://www.kaggle.com/datasets/anaghachoudhari/pcos-detection-using-ultrasound-images 48. Hasan N Bao Y Shawon A Huang Y DenseNet convolutional neural networks application for predicting COVID-19 using CT image SN Comput. Sci. 2021 2 389 10.1007/s42979-021-00782-7 34337432 PMC8300985 Hasan, N., Bao, Y., Shawon, A. &amp; Huang, Y. DenseNet convolutional neural networks application for predicting COVID-19 using CT image. SN Comput. Sci. 2 , 389. 10.1007/s42979-021-00782-7 (2021). 34337432 10.1007/s42979-021-00782-7 PMC8300985 49. Santhi, S. &amp; Chairman, M. Oral disease detection from dental X-ray images using DenseNet. In 4th International Conference on Inventive Research in Computing Applications (ICIRCA) 1280&#8211;1286 (IEEE, 2022). 1280&#8211;1286 (IEEE, 2022). (2022). 10.1109/ICIRCA54612.2022.9985687 50. Sai, S. S. M., Tinnaluri, B. C. &amp; Tella, T. Y. Accurate prediction of classification score using DenseNet for acute pneumonia. In 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT) 1293&#8211;1299 (IEEE, 2024). 1293&#8211;1299 (IEEE, 2024). (2024). 10.1109/IDCIoT59759.2024.10467683 51. Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv (2015). http://arxiv.org/abs/1409.1556 52. Hlawa, S. &amp; Romdhane, N. B. Deep learning-based Alzheimer&#8217;s disease prediction for smart health system. In 3rd International Conference on Distributed Sensing and Intelligent Systems (ICDSIS) 128&#8211;137 (IET, 2022). 10.1049/icp.2022.2427 53. Ravi D Deep learning for health informatics IEEE J. Biomed. Health Inf. 2017 21 4 21 10.1109/JBHI.2016.2636665 28055930 Ravi, D. et al. Deep learning for health informatics. IEEE J. Biomed. Health Inf. 21 , 4&#8211;21. 10.1109/JBHI.2016.2636665 (2017). 10.1109/JBHI.2016.2636665 28055930 54. He F Liu T Tao D Why ResNet works? Residuals generalize IEEE Trans. Neural Netw. Learn. Syst 2020 31 5349 5362 10.1109/TNNLS.2020.2966319 32031953 He, F., Liu, T. &amp; Tao, D. Why ResNet works? Residuals generalize. IEEE Trans. Neural Netw. Learn. Syst . 31 , 5349&#8211;5362. 10.1109/TNNLS.2020.2966319 (2020). 32031953 10.1109/TNNLS.2020.2966319 55. Wang T Dou Z Bao C Shi Z Diffusion mechanism in residual neural network: theory and applications IEEE Trans. Pattern Anal. Mach. Intell. 2024 46 667 680 10.1109/TPAMI.2023.3272341 37130245 Wang, T., Dou, Z., Bao, C. &amp; Shi, Z. Diffusion mechanism in residual neural network: theory and applications. IEEE Trans. Pattern Anal. Mach. Intell. 46 , 667&#8211;680. 10.1109/TPAMI.2023.3272341 (2024). 37130245 10.1109/TPAMI.2023.3272341 56. Suri JS U-Net deep learning architecture for segmentation of vascular and non-vascular images: A microscopic look at U-Net components buffered with pruning, explainable artificial intelligence, and bias IEEE Access. 2023 11 595 645 10.1109/ACCESS.2022.3232561 Suri, J. S. et al. U-Net deep learning architecture for segmentation of vascular and non-vascular images: A microscopic look at U-Net components buffered with pruning, explainable artificial intelligence, and bias. IEEE Access. 11 , 595&#8211;645. 10.1109/ACCESS.2022.3232561 (2023). 57. Gao Y Yang LT Yang J Wang H Zhao Y Attention U-Net based on Bi-ConvLSTM and its optimization for smart healthcare IEEE Trans. Comput. Soc. Syst. 2023 10 1966 1974 10.1109/TCSS.2023.3237923 Gao, Y., Yang, L. T., Yang, J., Wang, H. &amp; Zhao, Y. Attention U-Net based on Bi-ConvLSTM and its optimization for smart healthcare. IEEE Trans. Comput. Soc. Syst. 10 , 1966&#8211;1974. 10.1109/TCSS.2023.3237923 (2023). 58. Wu, J., Niu, Y., Ling, Z., Zhu, J. &amp; Gou, F. Pathological image segmentation method based on multiscale and dual attention. Int. J. Intell. Syst. 2024 (9987190). 10.1155/int/9987190 (2024). 59. Xu, X., Yue, X., Huang, Z., Wang, Q. &amp; Liu, Y. Melanoma image segmentation based on improved TransUnet. In Proc. 7th Int. Conf. Artif. Intell. Pattern Recognit. (AIPR &#8217;24) 139&#8211;147ACM, New York, NY, USA, (2025). 10.1145/3703935.3703990 60. Sun X Zhang Y Chen C Xie S Dong J High-order paired-ASPP for deep semantic segmentation networks Inf. Sci. 2023 646 119364 10.1016/j.ins.2023.119364 Sun, X., Zhang, Y., Chen, C., Xie, S. &amp; Dong, J. High-order paired-ASPP for deep semantic segmentation networks. Inf. Sci. 646 , 119364. 10.1016/j.ins.2023.119364 (2023). 61. Dhiyanesh B EnsembleEdgeFusion: advancing semantic segmentation in microvascular decompression imaging with innovative ensemble techniques Sci. Rep. 2025 15 17892 10.1038/s41598-025-02470-5 40410312 PMC12102392 Dhiyanesh, B. et al. EnsembleEdgeFusion: advancing semantic segmentation in microvascular decompression imaging with innovative ensemble techniques. Sci. Rep. 15 , 17892. 10.1038/s41598-025-02470-5 (2025). 40410312 10.1038/s41598-025-02470-5 PMC12102392 62. Mao Y Dilated SE-DenseNet for brain tumor MRI classification Sci. Rep. 2025 15 3596 10.1038/s41598-025-86752-y 39875423 PMC11775108 Mao, Y. et al. Dilated SE-DenseNet for brain tumor MRI classification. Sci. Rep. 15 , 3596. 10.1038/s41598-025-86752-y (2025). 39875423 10.1038/s41598-025-86752-y PMC11775108 63. Reka S Automated high precision PCOS detection through a segment anything model on super resolution ultrasound ovary images Sci. Rep. 2025 15 16832 10.1038/s41598-025-01744-2 40369044 PMC12078606 Reka, S. et al. Automated high precision PCOS detection through a segment anything model on super resolution ultrasound ovary images. Sci. Rep. 15 , 16832. 10.1038/s41598-025-01744-2 (2025). 40369044 10.1038/s41598-025-01744-2 PMC12078606 64. Zhang, J. et al. HR-ASPP: An improved semantic segmentation model of cervical nucleus images with accurate spatial localization and better shape feature extraction based on Deeplabv3+. In Proc. 15th Int. Conf. Digit. Image Process. (ICDIP &#8217;23), Article 16, 1&#8211;8ACM, New York, NY, USA, (2023). 10.1145/3604078.3604094 65. Zhong L DSU-Net: Dual-Stage U-Net based on CNN and transformer for skin lesion segmentation Biomed. Signal. Process. Control 2024 100 107090 10.1016/j.bspc.2024.107090 Zhong, L. et al. DSU-Net: Dual-Stage U-Net based on CNN and transformer for skin lesion segmentation. Biomed. Signal. Process. Control . 100 , 107090. 10.1016/j.bspc.2024.107090 (2024). 66. Fu Z Li J Hua Z DEAU-Net: attention networks based on dual encoder for medical image segmentation Comput. Biol. Med. 2022 150 106197 10.1016/j.compbiomed.2022.106197 37859289 Fu, Z., Li, J. &amp; Hua, Z. DEAU-Net: attention networks based on dual encoder for medical image segmentation. Comput. Biol. Med. 150 , 106197. 10.1016/j.compbiomed.2022.106197 (2022). 37859289 10.1016/j.compbiomed.2022.106197 67. Zhang J Advances in attention mechanisms for medical image segmentation Comput. Sci. Rev. 2025 56 100721 10.1016/j.cosrev.2024.100721 Zhang, J. et al. Advances in attention mechanisms for medical image segmentation. Comput. Sci. Rev. 56 , 100721. 10.1016/j.cosrev.2024.100721 (2025). 68. Moral P Mustafi D Mustafi A Sahana SK CystNet An AI driven model for PCOS detection using multilevel thresholding of ultrasound images Sci. Rep. 2024 14 1 25012 10.1038/s41598-024-75964-3 39443622 PMC11499604 Moral, P., Mustafi, D., Mustafi, A., Sahana, S. K. &amp; CystNet An AI driven model for PCOS detection using multilevel thresholding of ultrasound images. Sci. Rep. 14 (1), 25012. 10.1038/s41598-024-75964-3 (2024). 39443622 10.1038/s41598-024-75964-3 PMC11499604 69. Sundari MS Transfer learning-enhanced CNN model for integrative ultrasound and biomarker-based diagnosis of polycystic ovarian disease Sci. Rep. 2025 15 1 34519 10.1038/s41598-025-17711-w 41044309 PMC12494765 Sundari, M. S. et al. Transfer learning-enhanced CNN model for integrative ultrasound and biomarker-based diagnosis of polycystic ovarian disease. Sci. Rep. 15 (1), 34519. 10.1038/s41598-025-17711-w (2025). 41044309 10.1038/s41598-025-17711-w PMC12494765 70. Sarkar M Mandal A Tudu A DC-UNet: looking for follicles in the ovarian ultrasound images Frankl. Open. 2024 8 100149 10.1016/j.fraope.2024.100149 Sarkar, M., Mandal, A. &amp; Tudu, A. DC-UNet: looking for follicles in the ovarian ultrasound images. Frankl. Open. 8 , 100149. 10.1016/j.fraope.2024.100149 (2024). 71. 71 Sha M Segmentation of ovarian cyst in ultrasound images using AdaResU-net with optimization algorithm and deep learning model Sci. Rep. 2024 14 1 18868 10.1038/s41598-024-69427-y 39143122 PMC11325020 71 &amp; Sha, M. Segmentation of ovarian cyst in ultrasound images using AdaResU-net with optimization algorithm and deep learning model. Sci. Rep. 14 (1), 18868. 10.1038/s41598-024-69427-y (2024). 39143122 10.1038/s41598-024-69427-y PMC11325020 72. Wang J Lv P Wang H Shi C SAR-U-Net Squeeze-and-excitation block and atrous Spatial pyramid pooling based residual U-Net for automatic liver segmentation in computed tomography Comput. Methods Programs Biomed. 2021 208 106268 10.1016/j.cmpb.2021.106268 34274611 Wang, J., Lv, P., Wang, H., Shi, C. &amp; SAR-U-Net Squeeze-and-excitation block and atrous Spatial pyramid pooling based residual U-Net for automatic liver segmentation in computed tomography. Comput. Methods Programs Biomed. 208 , 106268. 10.1016/j.cmpb.2021.106268 (2021). 34274611 10.1016/j.cmpb.2021.106268 73. 73 Hekal AA Elnakib A Moustafa HE Amer HM Breast cancer segmentation from ultrasound images using deep Dual-Decoder technology with attention network IEEE Access. 2024 12 10087 10101 10.1109/access.2024.3351564 73, Hekal, A. A., Elnakib, A., Moustafa, H. E. &amp; Amer, H. M. Breast cancer segmentation from ultrasound images using deep Dual-Decoder technology with attention network. IEEE Access. 12 , 10087&#8211;10101. 10.1109/access.2024.3351564 (2024). 74. Sun, G. et al. DA-TransUNet: integrating Spatial and channel dual attention with transformer U-net for medical image segmentation. Front. Bioeng. Biotechnol. 12 10.3389/fbioe.2024.1398237 (2024). 10.3389/fbioe.2024.1398237 PMC11141164 38827037 75. Wang Zou Z Liu PX Hybrid dilation and attention residual U-Net for medical image segmentation Comput. Biol. Med. 2021 134 104449 10.1016/j.compbiomed.2021.104449 33993015 Wang, Zou, Z., Liu, P. X. &amp; Y., &amp; Hybrid dilation and attention residual U-Net for medical image segmentation. Comput. Biol. Med. 134 , 104449. 10.1016/j.compbiomed.2021.104449 (2021). 33993015 10.1016/j.compbiomed.2021.104449 76. Chowdhury SR Atrous attention U-Net with repeated ASPP for medical image segmentation ArXiv Preprint arXiv:2501 13129 2025 10.48550/arXiv.2501.13129 Chowdhury, S. R. et al. Atrous attention U-Net with repeated ASPP for medical image segmentation. ArXiv Preprint arXiv:2501 13129 . 10.48550/arXiv.2501.13129 (2025). 77. Umapathy SS Alhajlah S Almutairi O Aslam F F-Net: follicles net an efficient tool for the diagnosis of polycystic ovarian syndrome using deep learning techniques PLoS ONE 2024 19 8 e0307571 10.1371/journal.pone.030757 39146307 PMC11326594 Umapathy, S. S., Alhajlah, S., Almutairi, O. &amp; Aslam, F. F-Net: follicles net an efficient tool for the diagnosis of polycystic ovarian syndrome using deep learning techniques. PLoS ONE . 19 (8), e0307571. 10.1371/journal.pone.030757 (2024). 39146307 10.1371/journal.pone.0307571 PMC11326594 78. Pulluparambil, S. J. &amp; B, S. B. Detection and prediction of polycystic ovary syndrome using Attention-Based CNN-RNN classification model. Int. J. Adv. Comput. Sci. Appl. 16 (2). 10.14569/ijacsa.2025.0160270 (2025). 79. Agirsoy, M. &amp; Oehlschlaeger, M. A. A machine learning approach for non-invasive PCOS diagnosis from ultrasound and clinical features. Sci. Rep. 15 (1). 10.1038/s41598-025-10453-9 (2025). 10.1038/s41598-025-10453-9 PMC12479987 41022847"
}