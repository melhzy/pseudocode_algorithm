{
  "pmcid": "PMC12682491",
  "source": "PMC",
  "download_date": "2025-12-09T16:06:40.896585",
  "metadata": {
    "journal_title": "Quantitative Imaging in Medicine and Surgery",
    "journal_nlm_ta": "Quant Imaging Med Surg",
    "journal_iso_abbrev": "Quant Imaging Med Surg",
    "journal": "Quantitative Imaging in Medicine and Surgery",
    "pmcid": "PMC12682491",
    "doi": "10.21037/qims-2025-576",
    "title": "A dual-branch network for lesion segmentation in medical images using state space models",
    "year": "2025",
    "month": "11",
    "day": "14",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "14"
    },
    "authors": [
      "Chen Hao",
      "Min Byung-Won",
      "Zhang Haifei"
    ],
    "abstract": "Background Lesion segmentation in medical images is crucial for clinical diagnosis and treatment planning. However, existing methods often struggle to effectively extract both local and global features, limiting segmentation accuracy. To address this challenge, we propose a dual-branch network that integrates state space models (SSMs) with deep convolutional networks to enhance the extraction of both local and global features, thus improving lesion segmentation performance. Methods The proposed model employs a dual-branch encoder: one branch incorporates the visual state space encoder to efficiently model long-range contextual dependencies, while the other branch, based on the residual network, extracts hierarchical local features. To refine feature representation, we introduce a lightweight multi-scale depth-wise separable convolution block, ensuring adaptability to varying lesion sizes while maintaining computational efficiency. The fused features are processed by the decoder for high-precision segmentation. Results Extensive experiments on the Kaggle_3M and Kvasir-SEG datasets demonstrated that the proposed model outperformed existing state-of-the-art models. Specifically, it achieved a dice similarity coefficient (Dice) of 0.9140 and a false negative rate (FNR) of 0.0800 on Kaggle_3M dataset, and a Dice of 0.9173 and an FNR of 0.0788 on Kvasir-SEG dataset. Compared to other models, our model delivered superior quantitative results and visual segmentation performance. In addition, when trained on Kvasir-SEG and tested on two external datasets, our model demonstrated superior cross-dataset generalization. Conclusions The proposed model integrates SSMs and deep convolutional networks to improve lesion segmentation by effectively capturing both local and global features. It offers new insights for medical image segmentation with potential clinical applications.",
    "keywords": [
      "Lesion segmentation",
      "state space models (SSMs)",
      "visual state space (VSS)",
      "long-range dependencies"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"iso-abbrev\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1905</journal-id><journal-id journal-id-type=\"pmc-domain\">qims</journal-id><journal-id journal-id-type=\"publisher-id\">QIMS</journal-id><journal-title-group><journal-title>Quantitative Imaging in Medicine and Surgery</journal-title></journal-title-group><issn pub-type=\"ppub\">2223-4292</issn><issn pub-type=\"epub\">2223-4306</issn><publisher><publisher-name>AME Publications</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682491</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682491.1</article-id><article-id pub-id-type=\"pmcaid\">12682491</article-id><article-id pub-id-type=\"pmcaiid\">12682491</article-id><article-id pub-id-type=\"doi\">10.21037/qims-2025-576</article-id><article-id pub-id-type=\"publisher-id\">qims-15-12-11977</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>A dual-branch network for lesion segmentation in medical images using state space models</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"H\">Hao</given-names></name><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0009-0005-6986-4805</contrib-id><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Min</surname><given-names initials=\"BW\">Byung-Won</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"H\">Haifei</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><aff id=\"aff1\"><label>1</label><institution>School of Information Engineering, Nantong Institute of Technology, Nantong</institution>, <country country=\"cn\">China</country>;</aff><aff id=\"aff2\"><label>2</label><institution content-type=\"dept\">Division of Information and Communication Convergence Engineering</institution>, <institution>Mokwon University</institution>, <addr-line>Daejeon</addr-line>, <country country=\"kr\">Republic of Korea</country></aff></contrib-group><author-notes><fn id=\"afn1\"><p><italic toggle=\"yes\">Contributions:</italic> (I) Conception and design: H Chen; (II) Administrative support: BW Min; (III) Provision of study materials or patients: None; (IV) Collection and assembly of data: H Chen, H Zhang; (V) Data analysis and interpretation: H Chen; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors.</p></fn><corresp id=\"cor1\"><italic toggle=\"yes\">Correspondence to:</italic> Hao Chen, PhD. School of Information Engineering, Nantong Institute of Technology, 211 Yongxing Road, Chongchuan District, Nantong 226002, China; Division of Information and Communication Convergence Engineering, Mokwon University, Daejeon, Republic of Korea. Email: <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"chenhao@ntit.edu.cn\">chenhao@ntit.edu.cn</email>.</corresp></author-notes><pub-date pub-type=\"epub\"><day>14</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"ppub\"><day>01</day><month>12</month><year>2025</year></pub-date><volume>15</volume><issue>12</issue><issue-id pub-id-type=\"pmc-issue-id\">502028</issue-id><fpage>11977</fpage><lpage>11991</lpage><history><date date-type=\"received\"><day>07</day><month>3</month><year>2025</year></date><date date-type=\"accepted\"><day>25</day><month>9</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 00:25:14.317\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 AME Publishing Company.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>AME Publishing Company.</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><italic toggle=\"yes\">Open Access Statement:</italic> This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">https://creativecommons.org/licenses/by-nc-nd/4.0</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"qims-15-12-11977.pdf\"/><abstract><sec><title>Background</title><p>Lesion segmentation in medical images is crucial for clinical diagnosis and treatment planning. However, existing methods often struggle to effectively extract both local and global features, limiting segmentation accuracy. To address this challenge, we propose a dual-branch network that integrates state space models (SSMs) with deep convolutional networks to enhance the extraction of both local and global features, thus improving lesion segmentation performance.</p></sec><sec><title>Methods</title><p>The proposed model employs a dual-branch encoder: one branch incorporates the visual state space encoder to efficiently model long-range contextual dependencies, while the other branch, based on the residual network, extracts hierarchical local features. To refine feature representation, we introduce a lightweight multi-scale depth-wise separable convolution block, ensuring adaptability to varying lesion sizes while maintaining computational efficiency. The fused features are processed by the decoder for high-precision segmentation.</p></sec><sec><title>Results</title><p>Extensive experiments on the Kaggle_3M and Kvasir-SEG datasets demonstrated that the proposed model outperformed existing state-of-the-art models. Specifically, it achieved a dice similarity coefficient (Dice) of 0.9140 and a false negative rate (FNR) of 0.0800 on Kaggle_3M dataset, and a Dice of 0.9173 and an FNR of 0.0788 on Kvasir-SEG dataset. Compared to other models, our model delivered superior quantitative results and visual segmentation performance. In addition, when trained on Kvasir-SEG and tested on two external datasets, our model demonstrated superior cross-dataset generalization.</p></sec><sec><title>Conclusions</title><p>The proposed model integrates SSMs and deep convolutional networks to improve lesion segmentation by effectively capturing both local and global features. It offers new insights for medical image segmentation with potential clinical applications.</p></sec></abstract><kwd-group kwd-group-type=\"author\"><title>Keywords: </title><kwd>Lesion segmentation</kwd><kwd>state space models (SSMs)</kwd><kwd>visual state space (VSS)</kwd><kwd>long-range dependencies</kwd></kwd-group><funding-group><award-group><funding-source id=\"sp1\">the Research Topic on Educational Informatization in Jiangsu Higher Education Institutions</funding-source><award-id rid=\"sp1\">grant No. 2025JSETKT172</award-id></award-group></funding-group><funding-group><award-group><funding-source id=\"sp2\">Nantong Social Livelihood Science and Technology Plan (Directive Project)</funding-source><award-id rid=\"sp2\">grant No. MS2024019</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\"><title>Introduction</title><p>Medical image segmentation plays a crucial role in disease diagnosis and treatment. For example, deep learning has been applied to brain tumor magnetic resonance imaging for feature extraction and segmentation, showing its effectiveness in clinical decision-making (<xref rid=\"r1\" ref-type=\"bibr\">1</xref>). In recent years, deep learning, particularly convolutional neural network (CNN), has become the mainstream method in this field (<xref rid=\"r2\" ref-type=\"bibr\">2</xref>,<xref rid=\"r3\" ref-type=\"bibr\">3</xref>). The classic U-Net, with its efficient encoder-decoder structure and skip connections, has been widely applied to medical image segmentation (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>,<xref rid=\"r5\" ref-type=\"bibr\">5</xref>). Recent studies have proposed various U-Net-based approaches to further improve medical image segmentation performance (<xref rid=\"r6\" ref-type=\"bibr\">6</xref>,<xref rid=\"r7\" ref-type=\"bibr\">7</xref>). However, CNNs still face limitations in multi-scale feature fusion and long-range dependency modeling, making it challenging to fully capture global contextual information.</p><p>To address this issue, Transformer-based models utilize self-attention mechanisms to effectively capture long-range dependencies, which enhances global context modeling (<xref rid=\"r8\" ref-type=\"bibr\">8</xref>). However, the computational complexity of Transformer models increases significantly with the input image size, limiting their application in large-scale medical image segmentation tasks (<xref rid=\"r9\" ref-type=\"bibr\">9</xref>).</p><p>To address the limitations of CNNs and Transformer, state space model (SSM)-based methods (<xref rid=\"r10\" ref-type=\"bibr\">10</xref>) have gained increasing attention. With its exceptional global modeling capabilities and efficient computation, SSM has emerged as a promising alternative for modeling long-range dependencies and capturing spatial context. SSM can effectively handle long-range dependencies while outperforming traditional CNNs and Transformer in computational efficiency. In recent years, SSM-based image segmentation methods have achieved significant success in medical and remote sensing images, demonstrating their potential for precise segmentation and efficient computation (<xref rid=\"r11\" ref-type=\"bibr\">11</xref>-<xref rid=\"r13\" ref-type=\"bibr\">13</xref>).</p><p>Even though existing methods have partially addressed the limitations of CNNs and Transformer, effectively capturing and integrating both global and local information, while balancing computational complexity, remains a major challenge in improving segmentation performance. To tackle this challenge, this paper proposes a novel dual-branch network-based medical image lesion segmentation model. The specific contributions are as follows:</p><list list-type=\"simple\" id=\"L1\"><list-item><p>&#10070; A dual-branch encoder is designed in which residual network (ResNet) extracts hierarchical semantic features and visual state space (VSS) encoder captures long-range dependencies, enabling complementary local-global feature integration for lesion segmentation.</p></list-item><list-item><p>&#10070; The VSS encoder is introduced into medical image segmentation, achieving efficient global context modeling with lower complexity compared to Transformer-based approaches.</p></list-item><list-item><p>&#10070; A lightweight multi-scale depth-wise separable convolution block (LM-DSCB) is proposed. It combines depth-wise separable convolution (DSConv) with multiple dilation rates and multi-scale pooling to capture lesions of varying sizes and morphologies, while maintaining computational efficiency.</p></list-item></list><p>The rest of the paper is organized as follows: <italic toggle=\"yes\">Related</italic>\n<italic toggle=\"yes\">work</italic> section introduces the related work, <italic toggle=\"yes\">Methods</italic> section describes the proposed method in detail, <italic toggle=\"yes\">Results </italic>section presents the experiments and results, <italic toggle=\"yes\">Discussion </italic>section provides a discussion, and <italic toggle=\"yes\">Conclusions</italic> section concludes the paper.</p><sec><title>Related work</title><p>U-Net (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>) is a classic encoder-decoder architecture that is widely used in medical image segmentation. However, U-Net has limitations when handling multi-scale feature fusion. To address this, U-Net 3+ (<xref rid=\"r14\" ref-type=\"bibr\">14</xref>) effectively combines low-level details and high-level semantic information from different scales through full-scale skip connections, further improving segmentation accuracy. To tackle the issue of spatial information loss, CE-Net (<xref rid=\"r15\" ref-type=\"bibr\">15</xref>) introduces a dense atrous convolution block and a residual multi-kernel pooling block, enhancing the ability to extract high-level features. Researchers have found that incorporating attention mechanisms into U-Net helps the model focus on key features, which improves segmentation accuracy (<xref rid=\"r16\" ref-type=\"bibr\">16</xref>-<xref rid=\"r18\" ref-type=\"bibr\">18</xref>). By using self-attention mechanisms and multi-scale feature fusion, the traditional fusion methods have been improved, successfully capturing rich contextual information. To enhance the adaptability of the attention mechanism, channel prior convolutional attention (<xref rid=\"r19\" ref-type=\"bibr\">19</xref>) dynamically allocates attention weights in both the channel and spatial dimensions, further improving the extraction of spatial relationships. To overcome the limitations of the dual-branch network architecture in real-time semantic segmentation, PIDNet (<xref rid=\"r20\" ref-type=\"bibr\">20</xref>) proposes a three-branch network architecture and introduces the concept of proportional-integral-derivative control. It effectively guides the fusion of detail and contextual information through a boundary attention mechanism. However, traditional CNNs, which primarily learn local features, struggle to capture long-range dependencies and incorporate global context.</p><p>To overcome these limitations, Transformer-based models have shown superior performance in medical image segmentation. Transunet is one of the first models to introduce Transformer into medical image segmentation. By incorporating the self-attention mechanism of Transformer into the U-Net architecture, it improves the extraction of global features (<xref rid=\"r8\" ref-type=\"bibr\">8</xref>). Swin-unet (<xref rid=\"r21\" ref-type=\"bibr\">21</xref>) further enhances the application of Transformer in medical image segmentation. It combines the learning of both local and global features, overcoming the limitations of CNNs in capturing global semantic information. Scribformer (<xref rid=\"r22\" ref-type=\"bibr\">22</xref>) introduces a three-branch structure, combining CNNs, Transformer, and an attention-guided class activation map branch, which further strengthens the synergy between local and global features. LM-Net (<xref rid=\"r23\" ref-type=\"bibr\">23</xref>) addresses the issue of blurry segmentation boundaries in medical image segmentation by combining local and global feature transformers. AFC-Unet (<xref rid=\"r24\" ref-type=\"bibr\">24</xref>), built on the fusion of CNNs and Transformer, employs full-scale feature block fusion and pyramid sampling modules. However, Transformer-based models face the challenge of increased computational complexity as the input size grows, leading to a heavier computational burden.</p><p>Long-range dependency modeling and computational complexity have always been challenges in medical image segmentation. Traditional CNNs have limitations in modeling long-range dependencies, and although Transformer have global modeling capabilities, their computational complexity grows quadratically. To overcome these issues, SSM has gradually shown their advantages. Mamba (<xref rid=\"r10\" ref-type=\"bibr\">10</xref>) has advanced the development of SSM by introducing a data-dependent SSM layer. Vision Mamba (<xref rid=\"r25\" ref-type=\"bibr\">25</xref>) and Vmambair (<xref rid=\"r26\" ref-type=\"bibr\">26</xref>) further applied SSM to the visual domain, achieving significant results. Vm-unet (<xref rid=\"r11\" ref-type=\"bibr\">11</xref>) is the first medical image segmentation network based on SSM. It captures long-range dependency information through the VSS block and uses an asymmetric encoder-decoder structure to reduce the number of convolutional layers, thus lowering computational complexity. VM-UNetV2 (<xref rid=\"r27\" ref-type=\"bibr\">27</xref>) combines the VSS block with the semantics and detail infusion method to further enhance segmentation performance. RS<sup>3</sup>Mamba (<xref rid=\"r12\" ref-type=\"bibr\">12</xref>) provides global information to the convolution-based main branch using VSS block, improving remote sensing image segmentation accuracy. Segmamba (<xref rid=\"r28\" ref-type=\"bibr\">28</xref>) addresses computational issues in three-dimensional medical image segmentation by efficiently capturing long-range dependencies and modeling them at different scales using VSS block. H-vmunet (<xref rid=\"r29\" ref-type=\"bibr\">29</xref>) designs the high-order two-dimensional-selective-scan (SS2D) and local-SS2D modules to enhance the modeling of both global and local features. Based on these works, this paper proposes a dual-branch feature enhancement extraction network based on SSM, which effectively captures extensive contextual information for medical image segmentation.</p></sec></sec><sec sec-type=\"methods\"><title>Methods</title><sec><title>Dual-branch network architecture</title><p>This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. The proposed segmentation model adopts a dual-branch encoder architecture, as illustrated in <xref rid=\"f1\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 1</italic></xref>, and consists of four stages. Stage 1 employs a ResNet stem and encoder to capture local structural features in a hierarchical manner. Stage 2 introduces a parallel VSS stem and encoder to model global contextual dependencies through long-range information propagation. Stage 3 fuses the complementary local and global features from both encoders and further refines them through a LM-DSCB for feature enhancement. Finally, Stage 4 uses a decoder to progressively restore spatial resolution, and the segmentation head produces the final pixel-wise predictions.</p><fig position=\"float\" id=\"f1\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Dual-branch network architecture. BN, batch normalization; Conv, convolution; ReLU, rectified linear unit; ResNet, residual network; VSS, visual state space.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f1.jpg\"/></fig></sec><sec><title>SS2D</title><p>SSM has been introduced into visual tasks, where the input <inline-formula><mml:math id=\"m1\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mi>&#8477;</mml:mi></mml:mrow></mml:math></inline-formula> is projected to the output <inline-formula><mml:math id=\"m2\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:mi>&#8477;</mml:mi></mml:mrow></mml:math></inline-formula> through an intermediate hidden state <inline-formula><mml:math id=\"m3\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. SSM can be represented using a system of linear ordinary differential equations:</p><disp-formula id=\"e1\"><mml:math id=\"m4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[1]</label></disp-formula><disp-formula id=\"e2\"><mml:math id=\"m5\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[2]</label></disp-formula><p>where N is the hidden dimension, <inline-formula><mml:math id=\"m6\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes its derivative with respect to time, <inline-formula><mml:math id=\"m7\" overflow=\"scroll\"><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the state matrix, and <inline-formula><mml:math id=\"m8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are projection parameters.</p><p>To apply SSM in discrete settings, the ordinary differential equations must be discretized. This process involves introducing a timescale parameter <inline-formula><mml:math id=\"m10\" overflow=\"scroll\"><mml:mi>&#916;</mml:mi></mml:math></inline-formula> and transforming <inline-formula><mml:math id=\"m11\" display=\"inline\" overflow=\"scroll\"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"m12\" display=\"inline\" overflow=\"scroll\"><mml:mi>B</mml:mi></mml:math></inline-formula> into discrete counterparts <inline-formula><mml:math id=\"m13\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m14\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> using a fixed discretization rule. The zero-order hold method is adopted for this transformation, defined as:</p><disp-formula id=\"e3\"><mml:math id=\"m15\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mrow><mml:mi>&#916;</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><label>[3]</label></disp-formula><disp-formula id=\"e4\"><mml:math id=\"m16\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#916;</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mrow><mml:mi>&#916;</mml:mi><mml:mstyle mathvariant=\"bold\" mathsize=\"normal\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8901;</mml:mo><mml:mi>&#916;</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:math><label>[4]</label></disp-formula><p><xref rid=\"f2\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 2</italic></xref> illustrates the SS2D process, which serves as the core computational unit of the VSS encoder. It comprises three key components: scan expansion, an S6 block, and scan merging. First, the scan expansion operation unfolds the input image into sequences along four different directions. Then, these sequences are processed through the S6 block, which extracts features while ensuring comprehensive scanning in each direction to capture diverse information. Finally, the scan merging operation aggregates and combines the processed sequences, restoring the output image to the same spatial dimensions as the input. The overall process can be mathematically formulated as follows:</p><disp-formula id=\"e5\"><mml:math id=\"m17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[5]</label></disp-formula><disp-formula id=\"e6\"><mml:math id=\"m18\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mn>6</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[6]</label></disp-formula><disp-formula id=\"e7\"><mml:math id=\"m19\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mtext>Merging</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[7]</label></disp-formula><fig position=\"float\" id=\"f2\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 2</label><caption><p>SS2D process. SS2D, two-dimensional-selective-scan.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f2.jpg\"/></fig><p>Here, <inline-formula><mml:math id=\"m20\" overflow=\"scroll\"><mml:mtext>x</mml:mtext></mml:math></inline-formula> denotes the input feature map, and <inline-formula><mml:math id=\"m21\" overflow=\"scroll\"><mml:mrow><mml:mtext>Expansion</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> unfolds <inline-formula><mml:math id=\"m22\" overflow=\"scroll\"><mml:mtext>x</mml:mtext></mml:math></inline-formula> into directional sequences <inline-formula><mml:math id=\"m23\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>v</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id=\"m24\" overflow=\"scroll\"><mml:mrow><mml:mtext>v</mml:mtext><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the four scanning directions. The operator <inline-formula><mml:math id=\"m25\" overflow=\"scroll\"><mml:mrow><mml:mtext>S6</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> processes each sequence to produce direction-specific representations <inline-formula><mml:math id=\"m26\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:msub><mml:mtext>x</mml:mtext><mml:mtext>v</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Finally, <inline-formula><mml:math id=\"m27\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mtext>Merging</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> aggregates the four directional outputs and reconstructs the feature map <inline-formula><mml:math id=\"m28\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mtext>x</mml:mtext><mml:mo stretchy=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.</p></sec><sec><title>Dual-branch encoder architecture</title><p>The detailed workflow of the dual-branch encoder is illustrated in <xref rid=\"f3\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 3</italic></xref>. The input image is first processed by two parallel stems: the ResNet stem produces the feature map <inline-formula><mml:math id=\"m29\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mtext>W</mml:mtext><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and the VSS stem generates the feature map <inline-formula><mml:math id=\"m30\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mtext>W</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>48</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The ResNet encoder takes <inline-formula><mml:math id=\"m31\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> as input and progressively extracts local structural features through four convolutional stages, producing <inline-formula><mml:math id=\"m32\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In parallel, the VSS encoder processes <inline-formula><mml:math id=\"m33\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, yielding five feature maps <inline-formula><mml:math id=\"m34\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mtext>v</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mtext>,v</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. At each level, the ResNet and VSS features are fused to obtain <inline-formula><mml:math id=\"m35\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which integrate the local features captured by the ResNet encoder and the global dependencies modeled by the VSS encoder. This fusion process is mathematically expressed as follows:</p><disp-formula id=\"e8\"><mml:math id=\"m36\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><label>[8]</label></disp-formula><disp-formula id=\"e9\"><mml:math id=\"m37\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><label>[9]</label></disp-formula><disp-formula id=\"e10\"><mml:math id=\"m38\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><label>[10]</label></disp-formula><disp-formula id=\"e11\"><mml:math id=\"m39\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mtext>rv</mml:mtext></mml:mrow><mml:mn>4</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>v</mml:mtext><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><label>[11]</label></disp-formula><fig position=\"float\" id=\"f3\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Encoder architecture. ResNet, residual network; VSS, visual state space.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f3.jpg\"/></fig><p>These fused multi-scale representations provide complementary information for subsequent feature enhancement.</p></sec><sec><title>LM-DSCB</title><p>To effectively capture multi-scale features and reduce computational cost, this paper proposes LM-DSCB. <xref rid=\"f4\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 4</italic></xref> illustrates the structure of LM-DSCB. This block integrates DSConv and multi-scale pooling to efficiently extract features, enhance the model&#8217;s ability to perceive information at different scales, and simultaneously reduce computational complexity.</p><fig position=\"float\" id=\"f4\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 4</label><caption><p>LM-DSCB architecture. Conv, convolution; DSConv, depthwise separable convolution; DWConv, depthwise convolution; LM-DSCB, lightweight multi-scale depth-wise separable convolution block; PWConv, pointwise convolution.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f4.jpg\"/></fig><p>The input feature map <inline-formula><mml:math id=\"m40\" overflow=\"scroll\"><mml:mrow><mml:mtext>rv</mml:mtext><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> of LM-DSCB originates from the final fused features of the ResNet and VSS encoder. <inline-formula><mml:math id=\"m41\" overflow=\"scroll\"><mml:mrow><mml:mtext>rv</mml:mtext><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> integrates deep global and local semantic information, providing rich feature representation for subsequent convolution operations. The LM-DSCB consists of three sets of DSConv units and two pooling operations. Each convolution unit includes a depthwise convolution (DWConv) and a pointwise convolution (PWConv). The first set adopts a 3&#215;3 DWConv and a 1&#215;1 PWConv, while the second and third sets use dilated 3&#215;3 DWConv along with 1&#215;1 PWConv. This hierarchical convolution design captures features at different receptive fields, enhancing the network&#8217;s multi-scale perception capability.</p><p>DWConv reduces computation by performing convolution independently on each input channel, while PWConv fuses the output of each channel using a 1&#215;1 convolution operation. Let <inline-formula><mml:math id=\"m42\" overflow=\"scroll\"><mml:mrow><mml:mtext>m</mml:mtext><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mtext>H</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>W</mml:mtext><mml:mo>&#215;</mml:mo><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the input feature map with height <inline-formula><mml:math id=\"m43\" overflow=\"scroll\"><mml:mtext>H</mml:mtext></mml:math></inline-formula>, width <inline-formula><mml:math id=\"m44\" overflow=\"scroll\"><mml:mtext>W</mml:mtext></mml:math></inline-formula>, and <inline-formula><mml:math id=\"m45\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> channels. DWConv applies a separate <inline-formula><mml:math id=\"m46\" overflow=\"scroll\"><mml:mrow><mml:mtext>K</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>K</mml:mtext></mml:mrow></mml:math></inline-formula> kernel <inline-formula><mml:math id=\"m47\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>&#969;</mml:mtext><mml:mtext>d</mml:mtext></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mtext>K</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>K</mml:mtext><mml:mo>&#215;</mml:mo><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to each channel <inline-formula><mml:math id=\"m48\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, producing:</p><disp-formula id=\"e12\"><mml:math id=\"m49\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#8727;</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#8195;</mml:mtext><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext>&#8195;</mml:mtext><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><label>[12]</label></disp-formula><p>where <inline-formula><mml:math id=\"m50\" overflow=\"scroll\"><mml:mo>&#8727;</mml:mo></mml:math></inline-formula> represents the convolution operation, and <inline-formula><mml:math id=\"m51\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mtext>d,c</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id=\"m52\" overflow=\"scroll\"><mml:mrow><mml:mtext>K</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>K</mml:mtext></mml:mrow></mml:math></inline-formula> convolution kernel applied to the input channel <inline-formula><mml:math id=\"m53\" overflow=\"scroll\"><mml:mtext>c</mml:mtext></mml:math></inline-formula>. The output <inline-formula><mml:math id=\"m54\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is processed using a 1&#215;1 convolution kernel <inline-formula><mml:math id=\"m55\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>&#969;</mml:mtext><mml:mtext>f</mml:mtext></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mtext>1</mml:mtext><mml:mo>&#215;</mml:mo><mml:mtext>1</mml:mtext><mml:mo>&#215;</mml:mo><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to obtain the final output <inline-formula><mml:math id=\"m56\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>&#8477;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which is expressed as:</p><disp-formula id=\"e13\"><mml:math id=\"m57\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8727;</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math><label>[13]</label></disp-formula><p>where <inline-formula><mml:math id=\"m58\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>&#969;</mml:mtext><mml:mtext>f</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is the convolution kernel for the PWConv, and <inline-formula><mml:math id=\"m59\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>C</mml:mtext><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the number of output channels.</p><p><xref rid=\"f4\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 4</italic></xref> shows the detailed configuration of the three sets of DSConv units. Each set of convolution operations processes the input features with different receptive fields to extract multi-scale information. The formula for calculating the receptive field of dilated convolution is as follows:</p><disp-formula id=\"e14\"><mml:math id=\"m60\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8901;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[14]</label></disp-formula><p>where <inline-formula><mml:math id=\"m61\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>K</mml:mtext><mml:mrow><mml:mtext>size</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the size of the dilated convolution kernel, d denotes the dilation rate, and <inline-formula><mml:math id=\"m62\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mtext>K</mml:mtext><mml:mrow><mml:mtext>rf</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the kernel size of the standard convolution that corresponds to the same receptive field as the dilated convolution. Additionally, to further enhance the model&#8217;s ability to extract multi-scale features, two pooling operations are introduced: the first is 2&#215;2 max pooling, and the second is 3&#215;3 max pooling.</p><p>After the features are processed by DSConv and pooling, the multiple feature maps <inline-formula><mml:math id=\"m63\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mtext>g</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>g</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mtext>g</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are concatenated along the channel dimension by the operator <inline-formula><mml:math id=\"m64\" overflow=\"scroll\"><mml:mrow><mml:mtext>concat</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to obtain a fused representation. The resulting feature map is denoted by <inline-formula><mml:math id=\"m65\" display=\"inline\" overflow=\"scroll\"><mml:msup><mml:mi>p</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup></mml:math></inline-formula>. The mathematical expression for the concatenation operation is as follows:</p><disp-formula id=\"e15\"><mml:math id=\"m66\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[15]</label></disp-formula><p>The concatenation operation is performed along the channel dimension, and the concatenated feature maps undergo a channel-wise feature transformation using a 1&#215;1 convolution to enhance information interaction and adjust the number of channels.</p></sec></sec><sec sec-type=\"results\"><title>Results</title><sec><title>Datasets</title><p>Kaggle_3M (<xref rid=\"r30\" ref-type=\"bibr\">30</xref>): this dataset contains preoperative brain magnetic resonance images from 110 patients with lower-grade gliomas, sourced from The Cancer Genome Atlas and hosted by the cancer imaging archive. The dataset includes fluid-attenuated inversion recovery sequences, which are preferred for the assessment of lower-grade gliomas due to their sensitivity in delineating tumor infiltration and edema in non-enhancing tumors. Unlike high-grade gliomas, lower-grade gliomas rarely show contrast enhancement, making fluid-attenuated inversion recovery a more suitable modality for preoperative imaging. We manually annotated fluid-attenuated inversion recovery hyperintensity regions to create segmentation masks. The images were divided into 1,095 training, 137 validation, and 137 test images, each with a resolution of 256&#215;256 pixels (px), with over 20 slices per patient.</p><p>Kvasir-SEG (<xref rid=\"r31\" ref-type=\"bibr\">31</xref>): this dataset contains 1,000 polyp images with resolutions ranging from 332&#215;487 to 1,920&#215;1,072 px. It was divided into 800 training images, 100 validation images, and 100 test images. The dataset is designed for research on polyp detection, segmentation, and classification, and focuses specifically on polyps.</p><p>These datasets were chosen to assess the model&#8217;s performance across varied clinical scenarios. The Kaggle_3M dataset, focused on brain tumor segmentation from magnetic resonance images, presented challenges due to complex lesion structures. The Kvasir-SEG dataset, featuring colon polyps from colonoscopy, offered a distinct lesion type and imaging modality. Together, they provided a comprehensive evaluation of the model&#8217;s generalizability and robustness. Furthermore, we followed common practice in polyp segmentation literature by training on Kvasir-SEG and testing the trained models on two external datasets, CVC-ColonDB (<xref rid=\"r32\" ref-type=\"bibr\">32</xref>) and ETIS (<xref rid=\"r33\" ref-type=\"bibr\">33</xref>), to evaluate cross-dataset generalization.</p></sec><sec><title>Experimental details</title><p>This paper compared U-Net (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>), CE-Net (<xref rid=\"r15\" ref-type=\"bibr\">15</xref>), Ma-net (<xref rid=\"r18\" ref-type=\"bibr\">18</xref>), Transunet (<xref rid=\"r8\" ref-type=\"bibr\">8</xref>), PIDNet (<xref rid=\"r20\" ref-type=\"bibr\">20</xref>), Vm-unet (<xref rid=\"r11\" ref-type=\"bibr\">11</xref>), RS<sup>3</sup>Mamba (<xref rid=\"r12\" ref-type=\"bibr\">12</xref>), and the proposed model. The experiments were conducted on an RTX 4090D (24 GB) with Ubuntu 22.04, Python 3.10, Cuda 11.8, and PyTorch 2.1.2.</p><p>The models were optimized using the Adam optimizer with an initial learning rate of 0.0001, and the learning rate was adjusted with cosine annealing. The training process lasted 400 epochs, with an input size of 256&#215;256 for training, validation, and testing. In this work, ResNet-34 is chosen as one of the encoders because it offers a favorable trade-off between accuracy and efficiency and has been widely validated in medical image segmentation tasks.</p><p>For the experiments on Kaggle_3M and Kvasir-SEG, segmentation performance was assessed using false negative rate (FNR), dice similarity coefficient (Dice), and mean surface distance (MSD). FNR reflects missed lesion px, Dice measures region overlaps, and MSD evaluates boundary accuracy. In addition, frames per second (FPS) (<xref rid=\"r34\" ref-type=\"bibr\">34</xref>) was reported to indicate inference speed. For the cross-dataset validation on CVC-ColonDB and ETIS, we followed common practice in polyp segmentation research (<xref rid=\"r35\" ref-type=\"bibr\">35</xref>,<xref rid=\"r36\" ref-type=\"bibr\">36</xref>) and reported mean FNR and mean Dice to enable consistent benchmarking across datasets.</p></sec><sec><title>Comparative experiments</title><p>On the Kaggle_3M dataset, this paper provided a comprehensive evaluation of the proposed model and compared it with several mainstream segmentation models. <xref rid=\"t1\" ref-type=\"table\"><italic toggle=\"yes\">Table 1</italic></xref> presented the quantitative metrics of different models on this dataset. The experimental results showed that the proposed model outperformed all other models in all evaluation metrics. In contrast, Ma-net and Transunet exhibited more stable segmentation performance, but their Dice values were lower than those of ours. Moreover, compared to U-Net, it showed a 1.82% reduction in FNR and a 0.98% improvement in Dice.</p><table-wrap position=\"float\" id=\"t1\" orientation=\"portrait\"><label>Table 1</label><caption><title>Comparative experimental results on the Kaggle_3M dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"33.34%\" span=\"1\"/><col width=\"33.34%\" span=\"1\"/><col width=\"33.32%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0982</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9042</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">CE-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0945</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9081</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ma-net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0924</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9133</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Transunet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0885</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9127</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">PIDNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1171</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8903</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Vm-unet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1011</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9019</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">RS<sup>3</sup>Mamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1004</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9043</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0800</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9140</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate.</p></table-wrap-foot></table-wrap><p>As shown in <xref rid=\"f5\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 5</italic></xref>, our proposed method achieved the lowest MSD of 2.12 px, outperforming all compared baselines including U-Net (2.73 px), CE-Net (2.32 px), and Transunet (2.65 px). This indicated a more accurate boundary alignment between the predicted and ground truth lesion contours. In contrast, methods such as PIDNet and Vm-unet exhibited higher MSD, suggesting less precise segmentation boundaries.</p><fig position=\"float\" id=\"f5\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Comparison of MSD among different methods. MSD, mean surface distance; px, pixels.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f5.jpg\"/></fig><p><xref rid=\"f6\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 6</italic></xref> illustrated the visual segmentation results of different models on the Kaggle_3M dataset. Due to the diverse morphological characteristics of lesion regions in this dataset, some models exhibited over-segmentation or under-segmentation. In contrast, our model accurately identified lesion regions and produced complete segmentation results, demonstrating a high degree of alignment with the ground truth. CE-Net showed instances of lesion omission in certain samples, whereas Ma-net and Transunet occasionally suffered from over-segmentation, leading to the misclassification of non-lesion areas. Overall, the proposed model achieved clearer boundaries and more comprehensive lesion segmentation, further confirming its effectiveness in medical image segmentation tasks.</p><fig position=\"float\" id=\"f6\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Visual comparisons on the Kaggle_3M dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) U-Net. (E) CE-Net. (F) Ma-net. (G) Transunet. (H) PIDNet. (I) Vm-unet. (J) RS<sup>3</sup>Mamba.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f6.jpg\"/></fig><p>The quantitative evaluation results on the Kvasir-SEG dataset were shown in <xref rid=\"t2\" ref-type=\"table\"><italic toggle=\"yes\">Table 2</italic></xref>. The proposed model achieved the best performance across all evaluation metrics, with a Dice of 0.9173 and an FNR of only 0.0788. In comparison, CE-Net, the best-performing baseline, achieved a Dice of 0.8879, which is still 2.94% lower than the proposed model. Additionally, U-Net achieved a Dice of 0.8112, which is significantly lower than that of the proposed model. Overall, the proposed model demonstrated the best balance among all models, not only improving Dice but also significantly reducing FNR, enhancing segmentation accuracy.</p><table-wrap position=\"float\" id=\"t2\" orientation=\"portrait\"><label>Table 2</label><caption><title>Comparative experimental results on the Kvasir-SEG dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"33.34%\" span=\"1\"/><col width=\"33.34%\" span=\"1\"/><col width=\"33.32%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1837</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8112</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">CE-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1120</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8879</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ma-net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1364</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8763</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Transunet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2531</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7873</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">PIDNet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.3093</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7448</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Vm-unet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2657</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8009</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">RS<sup>3</sup>Mamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.3372</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7201</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0788</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9173</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate.</p></table-wrap-foot></table-wrap><p><xref rid=\"f7\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 7</italic></xref> presented the visual segmentation results of different models on the Kvasir-SEG dataset. The segmentation results of our model exhibited the closest resemblance to the ground truth, accurately capturing lesion boundaries while effectively reducing mis segmentation. In contrast, U-Net, Ma-net, Transunet, PIDNet, Vm-unet, and RS<sup>3</sup>Mamba demonstrated varying degrees of boundary blurring and incomplete lesion segmentation, particularly in cases involving small or complex-shaped lesions. CE-Net performed relatively well in recovering lesion regions but still exhibited certain mis segmentation artifacts. Overall, the proposed model achieved more precise boundary delineation and more complete lesion segmentation, further validating its effectiveness in medical image segmentation tasks.</p><fig position=\"float\" id=\"f7\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Visual comparisons on the Kvasir-SEG dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) U-Net. (E) CE-Net. (F) Ma-net. (G) Transunet. (H) PIDNet. (I) Vm-unet. (J) RS<sup>3</sup>Mamba.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f7.jpg\"/></fig><p><xref rid=\"f8\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 8</italic></xref> showed the inference speed of the comparison models on two datasets. On both the Kaggle_3M and Kvasir-SEG datasets, the proposed model achieved an inference speed of 78 FPS, outperforming models such as CE-Net, Transunet, and RS<sup>3</sup>Mamba. Although U-Net has the highest inference speed, it maintained a relatively high speed while still demonstrating strong segmentation accuracy.</p><fig position=\"float\" id=\"f8\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Comparative experiments: inference speed of models across different datasets. FPS, frames per second.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f8.jpg\"/></fig><p>To further examine the robustness of the proposed method, we extended the evaluation beyond the training domain. Specifically, the models were trained on the Kvasir-SEG dataset and subsequently applied to two additional public datasets, CVC-ColonDB and ETIS. The outcomes are summarized in <xref rid=\"t3\" ref-type=\"table\"><italic toggle=\"yes\">Table 3</italic></xref>. Across both benchmarks, our approach produced the lowest mean FNR together with the highest mean Dice. Compared with widely used segmentation frameworks such as U-Net, Ma-net, Transunet, and Vm-unet, the proposed method maintained more stable performance when transferred to unseen data. The results showed that our model worked well on the training data and still performed reliably on new datasets.</p><table-wrap position=\"float\" id=\"t3\" orientation=\"portrait\"><label>Table 3</label><caption><title>Comparative experimental results on the two datasets</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"26.86%\" span=\"1\"/><col width=\"25.03%\" span=\"1\"/><col width=\"23.1%\" span=\"1\"/><col width=\"25.01%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dataset</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mean FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mean Dice</th></tr></thead><tbody><tr><td rowspan=\"5\" valign=\"top\" align=\"left\" scope=\"row\" colspan=\"1\">CVC-ColonD</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.3000</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7337</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Ma-net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2700</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7926</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Transunet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2519</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.6955</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Vm-unet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2621</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7565</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2403</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8175</td></tr><tr><td rowspan=\"5\" valign=\"top\" align=\"left\" scope=\"row\" colspan=\"1\">ETIS</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2094</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7462</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Ma-net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1650</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8451</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Transunet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2081</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7494</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Vm-unet</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.2671</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7775</td></tr><tr><td valign=\"top\" colspan=\"1\" align=\"center\" scope=\"row\" rowspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1446</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8909</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate.</p></table-wrap-foot></table-wrap></sec><sec><title>Ablation experiments</title><p>The results of the ablation experiments on the Kaggle_3M dataset were presented in <xref rid=\"t4\" ref-type=\"table\"><italic toggle=\"yes\">Table 4</italic></xref>, with the corresponding segmentation effects shown in <xref rid=\"f9\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 9</italic></xref>. The experiments indicated that when using ResNet alone as the encoder, the Dice reached 0.9107. In contrast, using VSS encoder alone resulted in lower Dice of 0.8911, suggesting that VSS encoder alone is less effective than ResNet encoder in recognizing target regions. Building on this, the basic dual-branch fusion method enhanced segmentation performance. It increased Dice to 0.9127, confirming the effectiveness of multi-branch feature extraction. Furthermore, incorporating LM-DSCB for feature enhancement further improved the Dice to 0.9140 and reduced the FNR to 0.0800, demonstrating more accurate and complete lesion delineation. The actual segmentation results in <xref rid=\"f9\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 9</italic></xref> further supported this conclusion, as it generated finer segmentation boundaries and more complete target regions.</p><table-wrap position=\"float\" id=\"t4\" orientation=\"portrait\"><label>Table 4</label><caption><title>Results of ablation experiments on Kaggle_3M dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"39.75%\" span=\"1\"/><col width=\"30.13%\" span=\"1\"/><col width=\"30.12%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ResNet encoder</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0794</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9107</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">VSS encoder</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0986</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8911</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Basic fusion</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0861</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9127</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0800</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9140</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate; ResNet, residual network; VSS, visual state space.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f9\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Visual comparisons on the Kaggle_3M dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) ResNet encoder. (E) VSS encoder. (F) Basic fusion. ResNet, residual network; VSS, visual state space.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f9.jpg\"/></fig><p>The results of the ablation experiments on the Kvasir-SEG dataset were shown in <xref rid=\"t5\" ref-type=\"table\"><italic toggle=\"yes\">Table 5</italic></xref>, with the corresponding segmentation effects presented in <xref rid=\"f10\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 10</italic></xref>. The results indicated a significant performance gap between using ResNet encoder and VSS encoder alone. When VSS was used as the encoder, its performance was poor, with a Dice of 0.7647 and an FNR as high as 0.3127. After applying the basic dual-branch fusion method, Dice improved to 0.8882, demonstrating the effectiveness of multi-branch feature fusion. Furthermore, incorporating LM-DSCB enhanced the proposed model&#8217;s performance, increasing Dice to 0.9173 and reducing FNR to 0.0788, achieving the best results. The segmentation comparison in <xref rid=\"f10\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 10</italic></xref> further supports this conclusion, showing that the proposed model achieves more precise lesion segmentation with higher completeness of target regions.</p><table-wrap position=\"float\" id=\"t5\" orientation=\"portrait\"><label>Table 5</label><caption><title>Results of ablation experiments on Kvasir-SEG dataset</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"41.3%\" span=\"1\"/><col width=\"29.36%\" span=\"1\"/><col width=\"29.34%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">ResNet encoder</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.1440</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8763</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">VSS encoder</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.3127</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.7647</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Basic fusion</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0947</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8882</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0788</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9173</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate; ResNet, residual network; VSS, visual state space.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f10\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Visual comparisons on the Kvasir-SEG dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) ResNet encoder. (E) VSS encoder. (F) Basic fusion. ResNet, residual network; VSS, visual state space.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11977-f10.jpg\"/></fig><p>To further validate the effectiveness of the proposed LM-DSCB module, we replaced it with atrous spatial pyramid pooling (<xref rid=\"r37\" ref-type=\"bibr\">37</xref>), a widely used multi-scale feature extraction module. The results were shown in <xref rid=\"t6\" ref-type=\"table\"><italic toggle=\"yes\">Table 6</italic></xref>. LM-DSCB achieved a higher Dice and a lower FNR, demonstrating its superior capability in capturing lesions of varying sizes and morphologies compared with conventional multi-scale approaches.</p><table-wrap position=\"float\" id=\"t6\" orientation=\"portrait\"><label>Table 6</label><caption><title>Comparison of LM-DSCB and atrous spatial pyramid pooling</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"46.62%\" span=\"1\"/><col width=\"28.36%\" span=\"1\"/><col width=\"25.02%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Module</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">FNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Dice</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Atrous spatial pyramid pooling</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0902</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.8746</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">LM-DSCB (ours)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0788</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9173</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient; FNR, false negative rate; LM-DSCB, lightweight multi-scale depth-wise separable convolution block.</p></table-wrap-foot></table-wrap></sec></sec><sec sec-type=\"discussion\"><title>Discussion</title><p>This paper verifies the effectiveness of the proposed model in medical image lesion segmentation through both quantitative and qualitative analyses. It achieves the best performance in terms of MSD, Dice, and FNR. The ablation experiments demonstrate that the dual-branch encoder, feature fusion strategy, and LM-DSCB all play a critical role in improving the final segmentation results. Moreover, the visual segmentation results show a high degree of consistency between its outputs and the ground truth. It accurately identifies lesion regions while effectively preventing over-segmentation and under-segmentation. This capability is crucial in clinical applications, as it reduces the risk of missing subtle lesions where diagnostic sensitivity matters most.</p><p>As a classic segmentation network, U-Net restores spatial image information through an encoder-decoder structure. However, due to the locality of convolution operations, it struggles to effectively capture global context and long-range dependencies. Experimental results indicate that U-Net exhibits certain limitations in segmenting specific lesion regions. CE-Net enhances contextual modeling by incorporating dense atrous convolution and a residual multi-kernel pooling block. Ma-net improves local and global feature integration through self-attention mechanisms and multi-scale feature fusion. However, their ability to model long-range dependencies remains limited. Transunet combines the global modeling capability of Transformer with the precise localization of U-Net. On the Kaggle_3M dataset, Transunet demonstrates superior performance. However, its computational complexity increases significantly when handling high-resolution images. In contrast, Vm-unet and RS<sup>3</sup>Mamba enhance long-range modeling capabilities through VSS model while reducing computational costs. Nevertheless, their segmentation performance on our datasets does not surpass that of Transunet. Additionally, PIDNet mitigates the loss of high-resolution information during feature fusion through a three-branch network architecture. However, it still faces challenges in recognizing lesions with complex morphologies. Compared to these models, our method integrates long-range dependency modeling with lightweight computation and enhanced feature fusion, which enhances its ability to segment lesions with irregular shapes, blurred boundaries, and varying scales. Nonetheless, the performance varies across datasets. The model achieves better results on the Kvasir-SEG dataset than on the Kaggle_3M dataset, due to the more distinct boundaries and consistent visual patterns in colonoscopy images. In contrast, magnetic resonance images often contain fragmented or low-contrast lesions, which remain more challenging for accurate segmentation.</p><p>In addition to accuracy, our model also demonstrates competitive efficiency. The proposed model achieves an inference speed of 78 FPS on a single NVIDIA GeForce RTX 4090D GPU. The computational complexity is evaluated with one 256&#215;256 input images, yielding 19.3 giga floating-point operations and 34.4M parameters. When the LM-DSCB is removed, the computational cost decreases slightly by 0.3 giga floating-point operations, while the number of parameters remains nearly unchanged. This observation indicates that LM-DSCB is lightweight, introducing only negligible overhead. Compared with the baseline U-Net, our method introduces only a moderate increase in parameters while significantly improving computational efficiency.</p><p>Beyond the within-dataset experiments, we also carried out cross-dataset validation, where the model was trained on Kvasir-SEG and evaluated on CVC-ColonDB and ETIS. As reported in <xref rid=\"t3\" ref-type=\"table\"><italic toggle=\"yes\">Table 3</italic></xref>, the proposed method achieved higher Dice while reducing FNR in comparison with typical baseline networks. These results suggest that the architecture performs reliably not only on the training data but also under domain variations, which is important for practical clinical use.</p><p>To assess whether the observed performance improvements are statistically significant, we conducted Wilcoxon signed-rank tests on the Kvasir-SEG dataset using the Dice values obtained from 9-fold cross-validation. Our method was compared with U-Net and CE-Net. The resulting probability values are 0.0039 and 0.0273, respectively, both indicating statistically significant improvements. We also reported the mean and standard deviation of the fold-wise Dice differences to further illustrate the stability of the improvements. The detailed results were summarized in <xref rid=\"t7\" ref-type=\"table\"><italic toggle=\"yes\">Table 7</italic></xref>.</p><table-wrap position=\"float\" id=\"t7\" orientation=\"portrait\"><label>Table 7</label><caption><title>Wilcoxon signed-rank test results for Dice values</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"32.74%\" span=\"1\"/><col width=\"39.37%\" span=\"1\"/><col width=\"27.89%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Comparison</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Mean &#177; standard deviation</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Probability value</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours and U-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.095&#177;0.015</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0039</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Ours and CE-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.019&#177;0.019</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0273</td></tr></tbody></table><table-wrap-foot><p>Dice, dice similarity coefficient.</p></table-wrap-foot></table-wrap><p>Our approach achieves promising results overall. However, its performance can still be influenced by data complexity (<xref rid=\"r38\" ref-type=\"bibr\">38</xref>), and concerns regarding privacy preservation remain central to clinical deployment. The model shows good capability in handling small lesions and those with moderate irregularity, yet it encounters difficulties when boundaries are highly fragmented or when lesion shapes become extremely irregular. In such cases, under-segmentation along lesion edges or missed detection of very small targets may occur. While the results obtained on the Kaggle datasets are encouraging, clinical evidence is still insufficient. Broader validation across different imaging modalities and disease types is necessary to establish robustness, with particular emphasis on lesion heterogeneity in future evaluations.</p></sec><sec sec-type=\"conclusions\"><title>Conclusions</title><p>This paper proposes an innovative dual-encoder network architecture for medical image lesion segmentation. The architecture combines the advantages of ResNet encoder and VSS encoder, effectively capturing both local semantic details and global contextual dependencies. The ResNet encoder extracts hierarchical local features, while the VSS encoder models long-range dependencies for global context. After fusing the features from both encoders, complementary information is fully leveraged to obtain richer feature representations. To further enhance feature processing, a LM-DSCB is introduced, combining DSConv with multi-scale pooling operations to improve lesion detection at different scales. Experimental results on the Kaggle_3M and Kvasir-SEG datasets demonstrate that the proposed model achieves outstanding segmentation performance and competitive efficiency. Moreover, when trained on Kvasir-SEG and tested on two external datasets (CVC-ColonDB and ETIS), the model shows superior cross-dataset generalization, further confirming its robustness.</p><p>Future work will emphasize few-shot learning to strengthen performance in limited-data settings, supported by transfer learning and data augmentation. To address irregular boundaries and small structures, edge-aware and multi-scale feature fusion approaches will be investigated. Multi-center studies across varied imaging modalities and patient cohorts will be carried out to validate clinical applicability, while efforts toward workflow integration and regulatory compliance will further promote real-world deployment.</p></sec><sec sec-type=\"supplementary-material\"><title>Supplementary</title><supplementary-material position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><p>The article&#8217;s supplementary files as</p></supplementary-material><supplementary-material position=\"anchor\" id=\"su1\" content-type=\"local-data\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qims-15-12-11977-coif.pdf\" id=\"d67e2445\" position=\"anchor\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.21037/qims-2025-576</object-id></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank all the personnel who provided technical support or assisted with data collection. We also acknowledge the reviewers for their valuable comments and suggestions.</p></ack><fn-group><fn fn-type=\"financial-disclosure\"><p><italic toggle=\"yes\">Funding:</italic> This research was funded by <funding-source rid=\"sp1\">the Research Topic on Educational Informatization in Jiangsu Higher Education Institutions</funding-source> (<award-id rid=\"sp1\">grant No. 2025JSETKT172</award-id>) and <funding-source rid=\"sp2\">Nantong Social Livelihood Science and Technology Plan (Directive Project)</funding-source> (<award-id rid=\"sp2\">grant No. MS2024019</award-id>).</p></fn><fn fn-type=\"COI-statement\"><p><italic toggle=\"yes\">Conflicts of Interest:</italic> All authors have completed the ICMJE uniform disclosure form (available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-576/coif\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-576/coif</ext-link>). The authors have no conflicts of interest to declare.</p></fn></fn-group><notes><p content-type=\"note added in proof\"><italic toggle=\"yes\">Ethical Statement:</italic> The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments.</p></notes><ref-list><title>References</title><ref id=\"r1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rastogi</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Johri</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Donelli</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Kadry</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Espa</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Feraco</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>J</given-names></name></person-group>. <article-title>Deep learning-integrated MRI brain tumor analysis: feature extraction, segmentation, and Survival Prediction using Replicator and volumetric networks.</article-title><source>Sci Rep</source><year>2025</year>;<volume>15</volume>:<fpage>1437</fpage>. <pub-id pub-id-type=\"doi\">10.1038/s41598-024-84386-0</pub-id><pub-id pub-id-type=\"pmid\">39789043</pub-id><pub-id pub-id-type=\"pmcid\">PMC11718254</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>C</given-names></name></person-group>. <article-title>Deep learning in pulmonary nodule detection and segmentation: a systematic review.</article-title><source>Eur Radiol</source><year>2025</year>;<volume>35</volume>:<fpage>255</fpage>-<lpage>66</lpage>. <pub-id pub-id-type=\"doi\">10.1007/s00330-024-10907-0</pub-id><pub-id pub-id-type=\"pmid\">38985185</pub-id><pub-id pub-id-type=\"pmcid\">PMC11632000</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Sheng</surname><given-names>VS</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name></person-group>. <article-title>Deep semi-supervised learning for medical image segmentation: a review.</article-title><source>Expert Syst Appl</source><year>2024</year>;<volume>245</volume>:<elocation-id>123052</elocation-id>.</mixed-citation></ref><ref id=\"r4\"><label>4</label><mixed-citation publication-type=\"book\">Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells W, Frangi A. editors. Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015. Cham: Springer; 2015:234-41.</mixed-citation></ref><ref id=\"r5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azad</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Aghdam</surname><given-names>EK</given-names></name><name name-style=\"western\"><surname>Rauland</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Avval</surname><given-names>AH</given-names></name><name name-style=\"western\"><surname>Bozorgpour</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Karimijafarbigloo</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Cohen</surname><given-names>JP</given-names></name><name name-style=\"western\"><surname>Adeli</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D</given-names></name></person-group>. <article-title>Medical Image Segmentation Review: The Success of U-Net.</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2024</year>;<volume>46</volume>:<fpage>10076</fpage>-<lpage>95</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TPAMI.2024.3435571</pub-id><pub-id pub-id-type=\"pmid\">39167505</pub-id></mixed-citation></ref><ref id=\"r6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Thapar</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Rakhra</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Prashar</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Mrsic</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Kadry</surname><given-names>S</given-names></name></person-group>. <article-title>Skin cancer segmentation and classification by implementing a hybrid FrCN-(U-NeT) technique with machine learning.</article-title><source>PLoS One</source><year>2025</year>;<volume>20</volume>:<elocation-id>e0322659</elocation-id>. <pub-id pub-id-type=\"doi\">10.1371/journal.pone.0322659</pub-id><pub-id pub-id-type=\"pmid\">40455780</pub-id><pub-id pub-id-type=\"pmcid\">PMC12129148</pub-id></mixed-citation></ref><ref id=\"r7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alqarafi</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Mahendran</surname><given-names>RK</given-names></name><name name-style=\"western\"><surname>Al-Sarem</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Albalwy</surname><given-names>F</given-names></name></person-group>. <article-title>Multi-scale GC-T2: Automated region of interest assisted skin cancer detection using multi-scale graph convolution and tri-movement based attention mechanism.</article-title><source>Biomed Signal Process Control</source><year>2024</year>;<volume>95</volume>:<elocation-id>106313</elocation-id>.</mixed-citation></ref><ref id=\"r8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Mei</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Adeli</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Lungren</surname><given-names>MP</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Xing</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Yuille</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y</given-names></name></person-group>. <article-title>TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers.</article-title><source>Med Image Anal</source><year>2024</year>;<volume>97</volume>:<elocation-id>103280</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.media.2024.103280</pub-id><pub-id pub-id-type=\"pmid\">39096845</pub-id></mixed-citation></ref><ref id=\"r9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>JW</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>AU</given-names></name><name name-style=\"western\"><surname>Banerjee</surname><given-names>I</given-names></name></person-group>. <article-title>Systematic Review of Hybrid Vision Transformer Architectures for Radiological Image Analysis.</article-title><source>J Imaging Inform Med</source><year>2025</year>;<volume>38</volume>:<fpage>3248</fpage>-<lpage>62</lpage>. <pub-id pub-id-type=\"doi\">10.1007/s10278-024-01322-4</pub-id><pub-id pub-id-type=\"pmid\">39871042</pub-id><pub-id pub-id-type=\"pmcid\">PMC12572492</pub-id></mixed-citation></ref><ref id=\"r10\"><label>10</label><mixed-citation publication-type=\"preprint\">Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752 (Accessed September 17, 2025). Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2312.00752\" ext-link-type=\"uri\">https://arxiv.org/abs/2312.00752</ext-link></mixed-citation></ref><ref id=\"r11\"><label>11</label><mixed-citation publication-type=\"preprint\">Ruan J, Li J, Xiang S. VM-Unet: Vision mamba unet for medical image segmentation. arXiv:2402.02491 (Accessed September 17, 2025). Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2402.02491\" ext-link-type=\"uri\">https://arxiv.org/abs/2402.02491</ext-link></mixed-citation></ref><ref id=\"r12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Pun</surname><given-names>MO</given-names></name></person-group>. <article-title>RS3Mamba: Visual state space model for remote sensing image semantic segmentation.</article-title><source>IEEE Geosci Remote Sens Lett</source><year>2024</year>;<volume>21</volume>:<elocation-id>6011405</elocation-id>.</mixed-citation></ref><ref id=\"r13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>L</given-names></name></person-group>. <article-title>Selective and multi-scale fusion Mamba for medical image segmentation.</article-title><source>Expert Syst Appl</source><year>2025</year>;<volume>261</volume>:<elocation-id>125518</elocation-id>.</mixed-citation></ref><ref id=\"r14\"><label>14</label><mixed-citation publication-type=\"confproc\">Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y, Han X, Chen YW, Wu J. UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Barcelona: IEEE; 2020:1055-9.</mixed-citation></ref><ref id=\"r15\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Hao</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name></person-group><article-title>CE-Net: Context Encoder Network for 2D Medical Image Segmentation.</article-title><source>IEEE Trans Med Imaging</source><year>2019</year>;<volume>38</volume>:<fpage>2281</fpage>-<lpage>92</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2019.2903562</pub-id><pub-id pub-id-type=\"pmid\">30843824</pub-id></mixed-citation></ref><ref id=\"r16\"><label>16</label><mixed-citation publication-type=\"preprint\">Oktay O, Schlemper J, Le Folgoc L, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, Glocker B, Rueckert D. Attention U-Net: Learning where to look for the pancreas. arXiv:1804.03999 (Accessed September 17, 2025). Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/1804.03999\" ext-link-type=\"uri\">https://arxiv.org/abs/1804.03999</ext-link></mixed-citation></ref><ref id=\"r17\"><label>17</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Das</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Das</surname><given-names>S</given-names></name></person-group>. <article-title>Attention-UNet architectures with pretrained backbones for multi-class cardiac MR image segmentation.</article-title><source>Curr Probl Cardiol</source><year>2024</year>;<volume>49</volume>:<elocation-id>102129</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.cpcardiol.2023.102129</pub-id><pub-id pub-id-type=\"pmid\">37866419</pub-id></mixed-citation></ref><ref id=\"r18\"><label>18</label><mixed-citation publication-type=\"other\">Fan T, Wang G, Li Y, Wang H. MA-Net: A multi-scale attention network for liver and tumor segmentation. IEEE Access 2020;8:179656-65.</mixed-citation></ref><ref id=\"r19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zou</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>F</given-names></name></person-group>. <article-title>Channel prior convolutional attention for medical image segmentation.</article-title><source>Comput Biol Med</source><year>2024</year>;<volume>178</volume>:<elocation-id>108784</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2024.108784</pub-id><pub-id pub-id-type=\"pmid\">38941900</pub-id></mixed-citation></ref><ref id=\"r20\"><label>20</label><mixed-citation publication-type=\"confproc\">Xu J, Xiong Z, Bhattacharyya SP. PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver: IEEE; 2023:19529-39.</mixed-citation></ref><ref id=\"r21\"><label>21</label><mixed-citation publication-type=\"book\">Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-Unet: Unet-like pure transformer for medical image segmentation. In: Karlinsky L, Michaeli T, Nishino K. editors. Computer Vision &#8211; ECCV 2022 Workshops. Cham: Springer; 2023:205-18.</mixed-citation></ref><ref id=\"r22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Shan</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Hong</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>D.</given-names></name></person-group><article-title>ScribFormer: Transformer Makes CNN Work Better for Scribble-Based Medical Image Segmentation.</article-title><source>IEEE Trans Med Imaging</source><year>2024</year>;<volume>43</volume>:<fpage>2254</fpage>-<lpage>65</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2024.3363190</pub-id><pub-id pub-id-type=\"pmid\">38324425</pub-id></mixed-citation></ref><ref id=\"r23\"><label>23</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>She</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Q.</given-names></name></person-group><article-title>LM-Net: A light-weight and multi-scale network for medical image segmentation.</article-title><source>Comput Biol Med</source><year>2024</year>;<volume>168</volume>:<elocation-id>107717</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2023.107717</pub-id><pub-id pub-id-type=\"pmid\">38007973</pub-id></mixed-citation></ref><ref id=\"r24\"><label>24</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Meng</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name></person-group><article-title>AFC-Unet: Attention-fused full-scale CNN-transformer unet for medical image segmentation.</article-title><source>Biomed Signal Process Control</source><year>2025</year>;<volume>99</volume>:<elocation-id>106839</elocation-id>.</mixed-citation></ref><ref id=\"r25\"><label>25</label><mixed-citation publication-type=\"preprint\">Zhu L, Liao B, Zhang Q, Wang X, Liu W, Wang X. Vision Mamba: Efficient visual representation learning with bidirectional state space model. arXiv:2401.09417 (Accessed September 17, 2025). Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2401.09417\" ext-link-type=\"uri\">https://arxiv.org/abs/2401.09417</ext-link></mixed-citation></ref><ref id=\"r26\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Jin</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>X.</given-names></name></person-group><article-title>Vmambair: Visual state space model for image restoration.</article-title><source>IEEE Trans Circuits Syst Video Technol</source><year>2025</year>;<volume>35</volume>:<fpage>5560</fpage>-<lpage>74</lpage>.</mixed-citation></ref><ref id=\"r27\"><label>27</label><mixed-citation publication-type=\"book\">Zhang M, Yu Y, Jin S, Gu L, Ling T, Tao X. VM-UNET-V2: Rethinking vision mamba UNet for medical image segmentation. In: Peng W, Cai Z, Skums P. editors. Bioinformatics Research and Applications. ISBRA 2024. Singapore: Springer; 2024:335-46.</mixed-citation></ref><ref id=\"r28\"><label>28</label><mixed-citation publication-type=\"book\">Xing Z, Ye T, Yang Y, Liu G, Zhu L. Segmamba: Long-range sequential modeling mamba for 3D medical image segmentation. In: Linguraru MG, Dou Q, Feragen A, Giannarou S, Glocker B, Lekadir K, Schnabel JA. editors. Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2024. Cham: Springer; 2024:578-88.</mixed-citation></ref><ref id=\"r29\"><label>29</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name></person-group><article-title>H-vmunet: High-order vision mamba unet for medical image segmentation.</article-title><source>Neurocomputing</source><year>2025</year>;<volume>624</volume>:<elocation-id>129447</elocation-id>.</mixed-citation></ref><ref id=\"r30\"><label>30</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Buda</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Saha</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mazurowski</surname><given-names>MA</given-names></name></person-group>. <article-title>Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm.</article-title><source>Comput Biol Med</source><year>2019</year>;<volume>109</volume>:<fpage>218</fpage>-<lpage>25</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2019.05.002</pub-id><pub-id pub-id-type=\"pmid\">31078126</pub-id></mixed-citation></ref><ref id=\"r31\"><label>31</label><mixed-citation publication-type=\"book\">Jha D, Smedsrud PH, Riegler MA, Halvorsen P, De Lange T, Johansen D, Johansen HD. Kvasir-SEG: A Segmented Polyp Dataset. In: Ro Y, Cheng WH, Kim J, Chu WT, Cui P, Choi JW, Hu MC, De Neve W, editors. MultiMedia Modeling. MMM 2020. Cham: Springer; 2020:451-62.</mixed-citation></ref><ref id=\"r32\"><label>32</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bernal</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>S&#225;nchez</surname><given-names>FJ</given-names></name><name name-style=\"western\"><surname>Fern&#225;ndez-Esparrach</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Gil</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Rodr&#237;guez</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Vilari&#241;o</surname><given-names>F</given-names></name></person-group>. <article-title>WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians.</article-title><source>Comput Med Imaging Graph</source><year>2015</year>;<volume>43</volume>:<fpage>99</fpage>-<lpage>111</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.compmedimag.2015.02.007</pub-id><pub-id pub-id-type=\"pmid\">25863519</pub-id></mixed-citation></ref><ref id=\"r33\"><label>33</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Silva</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Histace</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Romain</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Dray</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Granado</surname><given-names>B</given-names></name></person-group>. <article-title>Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer.</article-title><source>Int J Comput Assist Radiol Surg</source><year>2014</year>;<volume>9</volume>:<fpage>283</fpage>-<lpage>93</lpage>. <pub-id pub-id-type=\"doi\">10.1007/s11548-013-0926-3</pub-id><pub-id pub-id-type=\"pmid\">24037504</pub-id></mixed-citation></ref><ref id=\"r34\"><label>34</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>W.</given-names></name></person-group><article-title>LGCE-Net: A local and global contextual encoding network for effective and efficient medical image segmentation.</article-title><source>Appl Intell</source><year>2025</year>. doi: <pub-id pub-id-type=\"doi\">10.1007/s10489-024-05900-5</pub-id></mixed-citation></ref><ref id=\"r35\"><label>35</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Hua</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>L.</given-names></name></person-group><article-title>DBMF: Dual Branch Multiscale Feature Fusion Network for polyp segmentation.</article-title><source>Comput Biol Med</source><year>2022</year>;<volume>151</volume>:<elocation-id>106304</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2022.106304</pub-id><pub-id pub-id-type=\"pmid\">36401969</pub-id></mixed-citation></ref><ref id=\"r36\"><label>36</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nanni</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Lumini</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Fantozzi</surname><given-names>C</given-names></name></person-group>. <article-title>Exploring the potential of ensembles of deep learning networks for image segmentation.</article-title><source>Information</source><year>2023</year>;<volume>14</volume>:<fpage>657</fpage>.</mixed-citation></ref><ref id=\"r37\"><label>37</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>LC</given-names></name><name name-style=\"western\"><surname>Papandreou</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Kokkinos</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Murphy</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Yuille</surname><given-names>AL</given-names></name></person-group>. <article-title>DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2018</year>;<volume>40</volume>:<fpage>834</fpage>-<lpage>48</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type=\"pmid\">28463186</pub-id></mixed-citation></ref><ref id=\"r38\"><label>38</label><mixed-citation publication-type=\"other\">Kujur A, Raza Z, Khan AA, Wechtaisong C. Data complexity based evaluation of the model dependence of brain MRI images for classification of brain tumor and Alzheimer&#8217;s disease. IEEE Access 2022;10:112117-33.</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Quant Imaging Med Surg Quant Imaging Med Surg 1905 qims QIMS Quantitative Imaging in Medicine and Surgery 2223-4292 2223-4306 AME Publications PMC12682491 PMC12682491.1 12682491 12682491 10.21037/qims-2025-576 qims-15-12-11977 1 Original Article A dual-branch network for lesion segmentation in medical images using state space models Chen Hao https://orcid.org/0009-0005-6986-4805 1 2 Min Byung-Won 2 Zhang Haifei 1 1 School of Information Engineering, Nantong Institute of Technology, Nantong , China ; 2 Division of Information and Communication Convergence Engineering , Mokwon University , Daejeon , Republic of Korea Contributions: (I) Conception and design: H Chen; (II) Administrative support: BW Min; (III) Provision of study materials or patients: None; (IV) Collection and assembly of data: H Chen, H Zhang; (V) Data analysis and interpretation: H Chen; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors. Correspondence to: Hao Chen, PhD. School of Information Engineering, Nantong Institute of Technology, 211 Yongxing Road, Chongchuan District, Nantong 226002, China; Division of Information and Communication Convergence Engineering, Mokwon University, Daejeon, Republic of Korea. Email: chenhao@ntit.edu.cn . 14 11 2025 01 12 2025 15 12 502028 11977 11991 07 3 2025 25 9 2025 01 12 2025 09 12 2025 09 12 2025 &#169; 2025 AME Publishing Company. 2025 AME Publishing Company. https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access Statement: This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: https://creativecommons.org/licenses/by-nc-nd/4.0 . Background Lesion segmentation in medical images is crucial for clinical diagnosis and treatment planning. However, existing methods often struggle to effectively extract both local and global features, limiting segmentation accuracy. To address this challenge, we propose a dual-branch network that integrates state space models (SSMs) with deep convolutional networks to enhance the extraction of both local and global features, thus improving lesion segmentation performance. Methods The proposed model employs a dual-branch encoder: one branch incorporates the visual state space encoder to efficiently model long-range contextual dependencies, while the other branch, based on the residual network, extracts hierarchical local features. To refine feature representation, we introduce a lightweight multi-scale depth-wise separable convolution block, ensuring adaptability to varying lesion sizes while maintaining computational efficiency. The fused features are processed by the decoder for high-precision segmentation. Results Extensive experiments on the Kaggle_3M and Kvasir-SEG datasets demonstrated that the proposed model outperformed existing state-of-the-art models. Specifically, it achieved a dice similarity coefficient (Dice) of 0.9140 and a false negative rate (FNR) of 0.0800 on Kaggle_3M dataset, and a Dice of 0.9173 and an FNR of 0.0788 on Kvasir-SEG dataset. Compared to other models, our model delivered superior quantitative results and visual segmentation performance. In addition, when trained on Kvasir-SEG and tested on two external datasets, our model demonstrated superior cross-dataset generalization. Conclusions The proposed model integrates SSMs and deep convolutional networks to improve lesion segmentation by effectively capturing both local and global features. It offers new insights for medical image segmentation with potential clinical applications. Keywords: Lesion segmentation state space models (SSMs) visual state space (VSS) long-range dependencies the Research Topic on Educational Informatization in Jiangsu Higher Education Institutions grant No. 2025JSETKT172 Nantong Social Livelihood Science and Technology Plan (Directive Project) grant No. MS2024019 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Introduction Medical image segmentation plays a crucial role in disease diagnosis and treatment. For example, deep learning has been applied to brain tumor magnetic resonance imaging for feature extraction and segmentation, showing its effectiveness in clinical decision-making ( 1 ). In recent years, deep learning, particularly convolutional neural network (CNN), has become the mainstream method in this field ( 2 , 3 ). The classic U-Net, with its efficient encoder-decoder structure and skip connections, has been widely applied to medical image segmentation ( 4 , 5 ). Recent studies have proposed various U-Net-based approaches to further improve medical image segmentation performance ( 6 , 7 ). However, CNNs still face limitations in multi-scale feature fusion and long-range dependency modeling, making it challenging to fully capture global contextual information. To address this issue, Transformer-based models utilize self-attention mechanisms to effectively capture long-range dependencies, which enhances global context modeling ( 8 ). However, the computational complexity of Transformer models increases significantly with the input image size, limiting their application in large-scale medical image segmentation tasks ( 9 ). To address the limitations of CNNs and Transformer, state space model (SSM)-based methods ( 10 ) have gained increasing attention. With its exceptional global modeling capabilities and efficient computation, SSM has emerged as a promising alternative for modeling long-range dependencies and capturing spatial context. SSM can effectively handle long-range dependencies while outperforming traditional CNNs and Transformer in computational efficiency. In recent years, SSM-based image segmentation methods have achieved significant success in medical and remote sensing images, demonstrating their potential for precise segmentation and efficient computation ( 11 - 13 ). Even though existing methods have partially addressed the limitations of CNNs and Transformer, effectively capturing and integrating both global and local information, while balancing computational complexity, remains a major challenge in improving segmentation performance. To tackle this challenge, this paper proposes a novel dual-branch network-based medical image lesion segmentation model. The specific contributions are as follows: &#10070; A dual-branch encoder is designed in which residual network (ResNet) extracts hierarchical semantic features and visual state space (VSS) encoder captures long-range dependencies, enabling complementary local-global feature integration for lesion segmentation. &#10070; The VSS encoder is introduced into medical image segmentation, achieving efficient global context modeling with lower complexity compared to Transformer-based approaches. &#10070; A lightweight multi-scale depth-wise separable convolution block (LM-DSCB) is proposed. It combines depth-wise separable convolution (DSConv) with multiple dilation rates and multi-scale pooling to capture lesions of varying sizes and morphologies, while maintaining computational efficiency. The rest of the paper is organized as follows: Related work section introduces the related work, Methods section describes the proposed method in detail, Results section presents the experiments and results, Discussion section provides a discussion, and Conclusions section concludes the paper. Related work U-Net ( 4 ) is a classic encoder-decoder architecture that is widely used in medical image segmentation. However, U-Net has limitations when handling multi-scale feature fusion. To address this, U-Net 3+ ( 14 ) effectively combines low-level details and high-level semantic information from different scales through full-scale skip connections, further improving segmentation accuracy. To tackle the issue of spatial information loss, CE-Net ( 15 ) introduces a dense atrous convolution block and a residual multi-kernel pooling block, enhancing the ability to extract high-level features. Researchers have found that incorporating attention mechanisms into U-Net helps the model focus on key features, which improves segmentation accuracy ( 16 - 18 ). By using self-attention mechanisms and multi-scale feature fusion, the traditional fusion methods have been improved, successfully capturing rich contextual information. To enhance the adaptability of the attention mechanism, channel prior convolutional attention ( 19 ) dynamically allocates attention weights in both the channel and spatial dimensions, further improving the extraction of spatial relationships. To overcome the limitations of the dual-branch network architecture in real-time semantic segmentation, PIDNet ( 20 ) proposes a three-branch network architecture and introduces the concept of proportional-integral-derivative control. It effectively guides the fusion of detail and contextual information through a boundary attention mechanism. However, traditional CNNs, which primarily learn local features, struggle to capture long-range dependencies and incorporate global context. To overcome these limitations, Transformer-based models have shown superior performance in medical image segmentation. Transunet is one of the first models to introduce Transformer into medical image segmentation. By incorporating the self-attention mechanism of Transformer into the U-Net architecture, it improves the extraction of global features ( 8 ). Swin-unet ( 21 ) further enhances the application of Transformer in medical image segmentation. It combines the learning of both local and global features, overcoming the limitations of CNNs in capturing global semantic information. Scribformer ( 22 ) introduces a three-branch structure, combining CNNs, Transformer, and an attention-guided class activation map branch, which further strengthens the synergy between local and global features. LM-Net ( 23 ) addresses the issue of blurry segmentation boundaries in medical image segmentation by combining local and global feature transformers. AFC-Unet ( 24 ), built on the fusion of CNNs and Transformer, employs full-scale feature block fusion and pyramid sampling modules. However, Transformer-based models face the challenge of increased computational complexity as the input size grows, leading to a heavier computational burden. Long-range dependency modeling and computational complexity have always been challenges in medical image segmentation. Traditional CNNs have limitations in modeling long-range dependencies, and although Transformer have global modeling capabilities, their computational complexity grows quadratically. To overcome these issues, SSM has gradually shown their advantages. Mamba ( 10 ) has advanced the development of SSM by introducing a data-dependent SSM layer. Vision Mamba ( 25 ) and Vmambair ( 26 ) further applied SSM to the visual domain, achieving significant results. Vm-unet ( 11 ) is the first medical image segmentation network based on SSM. It captures long-range dependency information through the VSS block and uses an asymmetric encoder-decoder structure to reduce the number of convolutional layers, thus lowering computational complexity. VM-UNetV2 ( 27 ) combines the VSS block with the semantics and detail infusion method to further enhance segmentation performance. RS 3 Mamba ( 12 ) provides global information to the convolution-based main branch using VSS block, improving remote sensing image segmentation accuracy. Segmamba ( 28 ) addresses computational issues in three-dimensional medical image segmentation by efficiently capturing long-range dependencies and modeling them at different scales using VSS block. H-vmunet ( 29 ) designs the high-order two-dimensional-selective-scan (SS2D) and local-SS2D modules to enhance the modeling of both global and local features. Based on these works, this paper proposes a dual-branch feature enhancement extraction network based on SSM, which effectively captures extensive contextual information for medical image segmentation. Methods Dual-branch network architecture This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. The proposed segmentation model adopts a dual-branch encoder architecture, as illustrated in Figure 1 , and consists of four stages. Stage 1 employs a ResNet stem and encoder to capture local structural features in a hierarchical manner. Stage 2 introduces a parallel VSS stem and encoder to model global contextual dependencies through long-range information propagation. Stage 3 fuses the complementary local and global features from both encoders and further refines them through a LM-DSCB for feature enhancement. Finally, Stage 4 uses a decoder to progressively restore spatial resolution, and the segmentation head produces the final pixel-wise predictions. Figure 1 Dual-branch network architecture. BN, batch normalization; Conv, convolution; ReLU, rectified linear unit; ResNet, residual network; VSS, visual state space. SS2D SSM has been introduced into visual tasks, where the input x ( t ) &#8712; &#8477; is projected to the output y ( t ) &#8712; &#8477; through an intermediate hidden state h ( t ) &#8712; &#8477; N . SSM can be represented using a system of linear ordinary differential equations: h &#8242; ( t ) = A h ( t ) + B x ( t ) [1] y ( t ) = C h ( t ) [2] where N is the hidden dimension, h &#8242; ( t ) denotes its derivative with respect to time, A &#8712; &#8477; N &#215; N is the state matrix, and B &#8712; &#8477; N &#215; 1 and C &#8712; &#8477; 1 &#215; N are projection parameters. To apply SSM in discrete settings, the ordinary differential equations must be discretized. This process involves introducing a timescale parameter &#916; and transforming A and B into discrete counterparts A &#175; and B &#175; using a fixed discretization rule. The zero-order hold method is adopted for this transformation, defined as: A &#175; = e &#916; A [3] B &#175; = ( &#916; A ) &#8722; 1 ( e &#916; A &#8722; 1 ) &#8901; &#916; B [4] Figure 2 illustrates the SS2D process, which serves as the core computational unit of the VSS encoder. It comprises three key components: scan expansion, an S6 block, and scan merging. First, the scan expansion operation unfolds the input image into sequences along four different directions. Then, these sequences are processed through the S6 block, which extracts features while ensuring comprehensive scanning in each direction to capture diverse information. Finally, the scan merging operation aggregates and combines the processed sequences, restoring the output image to the same spatial dimensions as the input. The overall process can be mathematically formulated as follows: x v = E x p a n s i o n ( x , v ) [5] x v &#175; = S 6 ( x v ) [6] x &#175; = Merging ( x &#175; 1 , x &#175; 2 , x &#175; 3 , x &#175; 4 ) [7] Figure 2 SS2D process. SS2D, two-dimensional-selective-scan. Here, x denotes the input feature map, and Expansion ( &#8901; ) unfolds x into directional sequences x v , where v &#8712; { 1 , 2 , 3 , 4 } indicates the four scanning directions. The operator S6 ( &#8901; ) processes each sequence to produce direction-specific representations x v &#175; . Finally, Merging ( &#8901; ) aggregates the four directional outputs and reconstructs the feature map x &#175; . Dual-branch encoder architecture The detailed workflow of the dual-branch encoder is illustrated in Figure 3 . The input image is first processed by two parallel stems: the ResNet stem produces the feature map X 0 &#8712; &#8477; H / 4 &#215; W / 4 &#215; 64 , and the VSS stem generates the feature map X 1 &#8712; &#8477; H / 2 &#215; W / 2 &#215; 48 . The ResNet encoder takes X 0 as input and progressively extracts local structural features through four convolutional stages, producing { r 1 , r 2 , r 3 , r 4 } . In parallel, the VSS encoder processes X 1 , yielding five feature maps { v 0 ,v 1 , v 2 , v 3 , v 4 } . At each level, the ResNet and VSS features are fused to obtain { rv 1 , rv 2 , rv 3 , rv 4 } , which integrate the local features captured by the ResNet encoder and the global dependencies modeled by the VSS encoder. This fusion process is mathematically expressed as follows: rv 1 = [ r 1 , v 1 ] [8] rv 2 = [ r 2 , v 2 ] [9] rv 3 = [ r 3 , v 3 ] [10] rv 4 = [ r 4 , v 4 ] [11] Figure 3 Encoder architecture. ResNet, residual network; VSS, visual state space. These fused multi-scale representations provide complementary information for subsequent feature enhancement. LM-DSCB To effectively capture multi-scale features and reduce computational cost, this paper proposes LM-DSCB. Figure 4 illustrates the structure of LM-DSCB. This block integrates DSConv and multi-scale pooling to efficiently extract features, enhance the model&#8217;s ability to perceive information at different scales, and simultaneously reduce computational complexity. Figure 4 LM-DSCB architecture. Conv, convolution; DSConv, depthwise separable convolution; DWConv, depthwise convolution; LM-DSCB, lightweight multi-scale depth-wise separable convolution block; PWConv, pointwise convolution. The input feature map rv 4 of LM-DSCB originates from the final fused features of the ResNet and VSS encoder. rv 4 integrates deep global and local semantic information, providing rich feature representation for subsequent convolution operations. The LM-DSCB consists of three sets of DSConv units and two pooling operations. Each convolution unit includes a depthwise convolution (DWConv) and a pointwise convolution (PWConv). The first set adopts a 3&#215;3 DWConv and a 1&#215;1 PWConv, while the second and third sets use dilated 3&#215;3 DWConv along with 1&#215;1 PWConv. This hierarchical convolution design captures features at different receptive fields, enhancing the network&#8217;s multi-scale perception capability. DWConv reduces computation by performing convolution independently on each input channel, while PWConv fuses the output of each channel using a 1&#215;1 convolution operation. Let m &#8712; &#8477; H &#215; W &#215; C in denotes the input feature map with height H , width W , and C in channels. DWConv applies a separate K &#215; K kernel &#969; d &#8712; &#8477; K &#215; K &#215; C in to each channel c &#8712; { 1 , ... , C in } , producing: m &#8242; = m c &#8727; &#969; d , c , &#8195; f o r &#8195; c &#8712; { 1 , &#8230; , C i n } [12] where &#8727; represents the convolution operation, and &#969; d,c is the K &#215; K convolution kernel applied to the input channel c . The output m &#8242; &#8712; &#8477; H &#215; W &#215; C i n is processed using a 1&#215;1 convolution kernel &#969; f &#8712; &#8477; 1 &#215; 1 &#215; C in &#215; C out to obtain the final output p &#8712; &#8477; H &#215; W &#215; C o u t , which is expressed as: p = m &#8242; &#8727; &#969; f [13] where &#969; f is the convolution kernel for the PWConv, and C out is the number of output channels. Figure 4 shows the detailed configuration of the three sets of DSConv units. Each set of convolution operations processes the input features with different receptive fields to extract multi-scale information. The formula for calculating the receptive field of dilated convolution is as follows: K r f = K s i z e + ( K s i z e &#8722; 1 ) &#8901; ( d &#8722; 1 ) [14] where K size represents the size of the dilated convolution kernel, d denotes the dilation rate, and K rf represents the kernel size of the standard convolution that corresponds to the same receptive field as the dilated convolution. Additionally, to further enhance the model&#8217;s ability to extract multi-scale features, two pooling operations are introduced: the first is 2&#215;2 max pooling, and the second is 3&#215;3 max pooling. After the features are processed by DSConv and pooling, the multiple feature maps { g 1 , g 2 , ... , g n } are concatenated along the channel dimension by the operator concat ( &#8901; ) to obtain a fused representation. The resulting feature map is denoted by p &#8242; . The mathematical expression for the concatenation operation is as follows: p &#8242; = c o n c a t ( g 1 , g 2 , &#8230; , g n ) [15] The concatenation operation is performed along the channel dimension, and the concatenated feature maps undergo a channel-wise feature transformation using a 1&#215;1 convolution to enhance information interaction and adjust the number of channels. Results Datasets Kaggle_3M ( 30 ): this dataset contains preoperative brain magnetic resonance images from 110 patients with lower-grade gliomas, sourced from The Cancer Genome Atlas and hosted by the cancer imaging archive. The dataset includes fluid-attenuated inversion recovery sequences, which are preferred for the assessment of lower-grade gliomas due to their sensitivity in delineating tumor infiltration and edema in non-enhancing tumors. Unlike high-grade gliomas, lower-grade gliomas rarely show contrast enhancement, making fluid-attenuated inversion recovery a more suitable modality for preoperative imaging. We manually annotated fluid-attenuated inversion recovery hyperintensity regions to create segmentation masks. The images were divided into 1,095 training, 137 validation, and 137 test images, each with a resolution of 256&#215;256 pixels (px), with over 20 slices per patient. Kvasir-SEG ( 31 ): this dataset contains 1,000 polyp images with resolutions ranging from 332&#215;487 to 1,920&#215;1,072 px. It was divided into 800 training images, 100 validation images, and 100 test images. The dataset is designed for research on polyp detection, segmentation, and classification, and focuses specifically on polyps. These datasets were chosen to assess the model&#8217;s performance across varied clinical scenarios. The Kaggle_3M dataset, focused on brain tumor segmentation from magnetic resonance images, presented challenges due to complex lesion structures. The Kvasir-SEG dataset, featuring colon polyps from colonoscopy, offered a distinct lesion type and imaging modality. Together, they provided a comprehensive evaluation of the model&#8217;s generalizability and robustness. Furthermore, we followed common practice in polyp segmentation literature by training on Kvasir-SEG and testing the trained models on two external datasets, CVC-ColonDB ( 32 ) and ETIS ( 33 ), to evaluate cross-dataset generalization. Experimental details This paper compared U-Net ( 4 ), CE-Net ( 15 ), Ma-net ( 18 ), Transunet ( 8 ), PIDNet ( 20 ), Vm-unet ( 11 ), RS 3 Mamba ( 12 ), and the proposed model. The experiments were conducted on an RTX 4090D (24 GB) with Ubuntu 22.04, Python 3.10, Cuda 11.8, and PyTorch 2.1.2. The models were optimized using the Adam optimizer with an initial learning rate of 0.0001, and the learning rate was adjusted with cosine annealing. The training process lasted 400 epochs, with an input size of 256&#215;256 for training, validation, and testing. In this work, ResNet-34 is chosen as one of the encoders because it offers a favorable trade-off between accuracy and efficiency and has been widely validated in medical image segmentation tasks. For the experiments on Kaggle_3M and Kvasir-SEG, segmentation performance was assessed using false negative rate (FNR), dice similarity coefficient (Dice), and mean surface distance (MSD). FNR reflects missed lesion px, Dice measures region overlaps, and MSD evaluates boundary accuracy. In addition, frames per second (FPS) ( 34 ) was reported to indicate inference speed. For the cross-dataset validation on CVC-ColonDB and ETIS, we followed common practice in polyp segmentation research ( 35 , 36 ) and reported mean FNR and mean Dice to enable consistent benchmarking across datasets. Comparative experiments On the Kaggle_3M dataset, this paper provided a comprehensive evaluation of the proposed model and compared it with several mainstream segmentation models. Table 1 presented the quantitative metrics of different models on this dataset. The experimental results showed that the proposed model outperformed all other models in all evaluation metrics. In contrast, Ma-net and Transunet exhibited more stable segmentation performance, but their Dice values were lower than those of ours. Moreover, compared to U-Net, it showed a 1.82% reduction in FNR and a 0.98% improvement in Dice. Table 1 Comparative experimental results on the Kaggle_3M dataset Method FNR Dice U-Net 0.0982 0.9042 CE-Net 0.0945 0.9081 Ma-net 0.0924 0.9133 Transunet 0.0885 0.9127 PIDNet 0.1171 0.8903 Vm-unet 0.1011 0.9019 RS 3 Mamba 0.1004 0.9043 Ours 0.0800 0.9140 Dice, dice similarity coefficient; FNR, false negative rate. As shown in Figure 5 , our proposed method achieved the lowest MSD of 2.12 px, outperforming all compared baselines including U-Net (2.73 px), CE-Net (2.32 px), and Transunet (2.65 px). This indicated a more accurate boundary alignment between the predicted and ground truth lesion contours. In contrast, methods such as PIDNet and Vm-unet exhibited higher MSD, suggesting less precise segmentation boundaries. Figure 5 Comparison of MSD among different methods. MSD, mean surface distance; px, pixels. Figure 6 illustrated the visual segmentation results of different models on the Kaggle_3M dataset. Due to the diverse morphological characteristics of lesion regions in this dataset, some models exhibited over-segmentation or under-segmentation. In contrast, our model accurately identified lesion regions and produced complete segmentation results, demonstrating a high degree of alignment with the ground truth. CE-Net showed instances of lesion omission in certain samples, whereas Ma-net and Transunet occasionally suffered from over-segmentation, leading to the misclassification of non-lesion areas. Overall, the proposed model achieved clearer boundaries and more comprehensive lesion segmentation, further confirming its effectiveness in medical image segmentation tasks. Figure 6 Visual comparisons on the Kaggle_3M dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) U-Net. (E) CE-Net. (F) Ma-net. (G) Transunet. (H) PIDNet. (I) Vm-unet. (J) RS 3 Mamba. The quantitative evaluation results on the Kvasir-SEG dataset were shown in Table 2 . The proposed model achieved the best performance across all evaluation metrics, with a Dice of 0.9173 and an FNR of only 0.0788. In comparison, CE-Net, the best-performing baseline, achieved a Dice of 0.8879, which is still 2.94% lower than the proposed model. Additionally, U-Net achieved a Dice of 0.8112, which is significantly lower than that of the proposed model. Overall, the proposed model demonstrated the best balance among all models, not only improving Dice but also significantly reducing FNR, enhancing segmentation accuracy. Table 2 Comparative experimental results on the Kvasir-SEG dataset Method FNR Dice U-Net 0.1837 0.8112 CE-Net 0.1120 0.8879 Ma-net 0.1364 0.8763 Transunet 0.2531 0.7873 PIDNet 0.3093 0.7448 Vm-unet 0.2657 0.8009 RS 3 Mamba 0.3372 0.7201 Ours 0.0788 0.9173 Dice, dice similarity coefficient; FNR, false negative rate. Figure 7 presented the visual segmentation results of different models on the Kvasir-SEG dataset. The segmentation results of our model exhibited the closest resemblance to the ground truth, accurately capturing lesion boundaries while effectively reducing mis segmentation. In contrast, U-Net, Ma-net, Transunet, PIDNet, Vm-unet, and RS 3 Mamba demonstrated varying degrees of boundary blurring and incomplete lesion segmentation, particularly in cases involving small or complex-shaped lesions. CE-Net performed relatively well in recovering lesion regions but still exhibited certain mis segmentation artifacts. Overall, the proposed model achieved more precise boundary delineation and more complete lesion segmentation, further validating its effectiveness in medical image segmentation tasks. Figure 7 Visual comparisons on the Kvasir-SEG dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) U-Net. (E) CE-Net. (F) Ma-net. (G) Transunet. (H) PIDNet. (I) Vm-unet. (J) RS 3 Mamba. Figure 8 showed the inference speed of the comparison models on two datasets. On both the Kaggle_3M and Kvasir-SEG datasets, the proposed model achieved an inference speed of 78 FPS, outperforming models such as CE-Net, Transunet, and RS 3 Mamba. Although U-Net has the highest inference speed, it maintained a relatively high speed while still demonstrating strong segmentation accuracy. Figure 8 Comparative experiments: inference speed of models across different datasets. FPS, frames per second. To further examine the robustness of the proposed method, we extended the evaluation beyond the training domain. Specifically, the models were trained on the Kvasir-SEG dataset and subsequently applied to two additional public datasets, CVC-ColonDB and ETIS. The outcomes are summarized in Table 3 . Across both benchmarks, our approach produced the lowest mean FNR together with the highest mean Dice. Compared with widely used segmentation frameworks such as U-Net, Ma-net, Transunet, and Vm-unet, the proposed method maintained more stable performance when transferred to unseen data. The results showed that our model worked well on the training data and still performed reliably on new datasets. Table 3 Comparative experimental results on the two datasets Dataset Method Mean FNR Mean Dice CVC-ColonD U-Net 0.3000 0.7337 Ma-net 0.2700 0.7926 Transunet 0.2519 0.6955 Vm-unet 0.2621 0.7565 Ours 0.2403 0.8175 ETIS U-Net 0.2094 0.7462 Ma-net 0.1650 0.8451 Transunet 0.2081 0.7494 Vm-unet 0.2671 0.7775 Ours 0.1446 0.8909 Dice, dice similarity coefficient; FNR, false negative rate. Ablation experiments The results of the ablation experiments on the Kaggle_3M dataset were presented in Table 4 , with the corresponding segmentation effects shown in Figure 9 . The experiments indicated that when using ResNet alone as the encoder, the Dice reached 0.9107. In contrast, using VSS encoder alone resulted in lower Dice of 0.8911, suggesting that VSS encoder alone is less effective than ResNet encoder in recognizing target regions. Building on this, the basic dual-branch fusion method enhanced segmentation performance. It increased Dice to 0.9127, confirming the effectiveness of multi-branch feature extraction. Furthermore, incorporating LM-DSCB for feature enhancement further improved the Dice to 0.9140 and reduced the FNR to 0.0800, demonstrating more accurate and complete lesion delineation. The actual segmentation results in Figure 9 further supported this conclusion, as it generated finer segmentation boundaries and more complete target regions. Table 4 Results of ablation experiments on Kaggle_3M dataset Method FNR Dice ResNet encoder 0.0794 0.9107 VSS encoder 0.0986 0.8911 Basic fusion 0.0861 0.9127 Ours 0.0800 0.9140 Dice, dice similarity coefficient; FNR, false negative rate; ResNet, residual network; VSS, visual state space. Figure 9 Visual comparisons on the Kaggle_3M dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) ResNet encoder. (E) VSS encoder. (F) Basic fusion. ResNet, residual network; VSS, visual state space. The results of the ablation experiments on the Kvasir-SEG dataset were shown in Table 5 , with the corresponding segmentation effects presented in Figure 10 . The results indicated a significant performance gap between using ResNet encoder and VSS encoder alone. When VSS was used as the encoder, its performance was poor, with a Dice of 0.7647 and an FNR as high as 0.3127. After applying the basic dual-branch fusion method, Dice improved to 0.8882, demonstrating the effectiveness of multi-branch feature fusion. Furthermore, incorporating LM-DSCB enhanced the proposed model&#8217;s performance, increasing Dice to 0.9173 and reducing FNR to 0.0788, achieving the best results. The segmentation comparison in Figure 10 further supports this conclusion, showing that the proposed model achieves more precise lesion segmentation with higher completeness of target regions. Table 5 Results of ablation experiments on Kvasir-SEG dataset Method FNR Dice ResNet encoder 0.1440 0.8763 VSS encoder 0.3127 0.7647 Basic fusion 0.0947 0.8882 Ours 0.0788 0.9173 Dice, dice similarity coefficient; FNR, false negative rate; ResNet, residual network; VSS, visual state space. Figure 10 Visual comparisons on the Kvasir-SEG dataset. (A) Origin image. (B) Ground truth. (C) Ours. (D) ResNet encoder. (E) VSS encoder. (F) Basic fusion. ResNet, residual network; VSS, visual state space. To further validate the effectiveness of the proposed LM-DSCB module, we replaced it with atrous spatial pyramid pooling ( 37 ), a widely used multi-scale feature extraction module. The results were shown in Table 6 . LM-DSCB achieved a higher Dice and a lower FNR, demonstrating its superior capability in capturing lesions of varying sizes and morphologies compared with conventional multi-scale approaches. Table 6 Comparison of LM-DSCB and atrous spatial pyramid pooling Module FNR Dice Atrous spatial pyramid pooling 0.0902 0.8746 LM-DSCB (ours) 0.0788 0.9173 Dice, dice similarity coefficient; FNR, false negative rate; LM-DSCB, lightweight multi-scale depth-wise separable convolution block. Discussion This paper verifies the effectiveness of the proposed model in medical image lesion segmentation through both quantitative and qualitative analyses. It achieves the best performance in terms of MSD, Dice, and FNR. The ablation experiments demonstrate that the dual-branch encoder, feature fusion strategy, and LM-DSCB all play a critical role in improving the final segmentation results. Moreover, the visual segmentation results show a high degree of consistency between its outputs and the ground truth. It accurately identifies lesion regions while effectively preventing over-segmentation and under-segmentation. This capability is crucial in clinical applications, as it reduces the risk of missing subtle lesions where diagnostic sensitivity matters most. As a classic segmentation network, U-Net restores spatial image information through an encoder-decoder structure. However, due to the locality of convolution operations, it struggles to effectively capture global context and long-range dependencies. Experimental results indicate that U-Net exhibits certain limitations in segmenting specific lesion regions. CE-Net enhances contextual modeling by incorporating dense atrous convolution and a residual multi-kernel pooling block. Ma-net improves local and global feature integration through self-attention mechanisms and multi-scale feature fusion. However, their ability to model long-range dependencies remains limited. Transunet combines the global modeling capability of Transformer with the precise localization of U-Net. On the Kaggle_3M dataset, Transunet demonstrates superior performance. However, its computational complexity increases significantly when handling high-resolution images. In contrast, Vm-unet and RS 3 Mamba enhance long-range modeling capabilities through VSS model while reducing computational costs. Nevertheless, their segmentation performance on our datasets does not surpass that of Transunet. Additionally, PIDNet mitigates the loss of high-resolution information during feature fusion through a three-branch network architecture. However, it still faces challenges in recognizing lesions with complex morphologies. Compared to these models, our method integrates long-range dependency modeling with lightweight computation and enhanced feature fusion, which enhances its ability to segment lesions with irregular shapes, blurred boundaries, and varying scales. Nonetheless, the performance varies across datasets. The model achieves better results on the Kvasir-SEG dataset than on the Kaggle_3M dataset, due to the more distinct boundaries and consistent visual patterns in colonoscopy images. In contrast, magnetic resonance images often contain fragmented or low-contrast lesions, which remain more challenging for accurate segmentation. In addition to accuracy, our model also demonstrates competitive efficiency. The proposed model achieves an inference speed of 78 FPS on a single NVIDIA GeForce RTX 4090D GPU. The computational complexity is evaluated with one 256&#215;256 input images, yielding 19.3 giga floating-point operations and 34.4M parameters. When the LM-DSCB is removed, the computational cost decreases slightly by 0.3 giga floating-point operations, while the number of parameters remains nearly unchanged. This observation indicates that LM-DSCB is lightweight, introducing only negligible overhead. Compared with the baseline U-Net, our method introduces only a moderate increase in parameters while significantly improving computational efficiency. Beyond the within-dataset experiments, we also carried out cross-dataset validation, where the model was trained on Kvasir-SEG and evaluated on CVC-ColonDB and ETIS. As reported in Table 3 , the proposed method achieved higher Dice while reducing FNR in comparison with typical baseline networks. These results suggest that the architecture performs reliably not only on the training data but also under domain variations, which is important for practical clinical use. To assess whether the observed performance improvements are statistically significant, we conducted Wilcoxon signed-rank tests on the Kvasir-SEG dataset using the Dice values obtained from 9-fold cross-validation. Our method was compared with U-Net and CE-Net. The resulting probability values are 0.0039 and 0.0273, respectively, both indicating statistically significant improvements. We also reported the mean and standard deviation of the fold-wise Dice differences to further illustrate the stability of the improvements. The detailed results were summarized in Table 7 . Table 7 Wilcoxon signed-rank test results for Dice values Comparison Mean &#177; standard deviation Probability value Ours and U-Net 0.095&#177;0.015 0.0039 Ours and CE-Net 0.019&#177;0.019 0.0273 Dice, dice similarity coefficient. Our approach achieves promising results overall. However, its performance can still be influenced by data complexity ( 38 ), and concerns regarding privacy preservation remain central to clinical deployment. The model shows good capability in handling small lesions and those with moderate irregularity, yet it encounters difficulties when boundaries are highly fragmented or when lesion shapes become extremely irregular. In such cases, under-segmentation along lesion edges or missed detection of very small targets may occur. While the results obtained on the Kaggle datasets are encouraging, clinical evidence is still insufficient. Broader validation across different imaging modalities and disease types is necessary to establish robustness, with particular emphasis on lesion heterogeneity in future evaluations. Conclusions This paper proposes an innovative dual-encoder network architecture for medical image lesion segmentation. The architecture combines the advantages of ResNet encoder and VSS encoder, effectively capturing both local semantic details and global contextual dependencies. The ResNet encoder extracts hierarchical local features, while the VSS encoder models long-range dependencies for global context. After fusing the features from both encoders, complementary information is fully leveraged to obtain richer feature representations. To further enhance feature processing, a LM-DSCB is introduced, combining DSConv with multi-scale pooling operations to improve lesion detection at different scales. Experimental results on the Kaggle_3M and Kvasir-SEG datasets demonstrate that the proposed model achieves outstanding segmentation performance and competitive efficiency. Moreover, when trained on Kvasir-SEG and tested on two external datasets (CVC-ColonDB and ETIS), the model shows superior cross-dataset generalization, further confirming its robustness. Future work will emphasize few-shot learning to strengthen performance in limited-data settings, supported by transfer learning and data augmentation. To address irregular boundaries and small structures, edge-aware and multi-scale feature fusion approaches will be investigated. Multi-center studies across varied imaging modalities and patient cohorts will be carried out to validate clinical applicability, while efforts toward workflow integration and regulatory compliance will further promote real-world deployment. Supplementary The article&#8217;s supplementary files as 10.21037/qims-2025-576 Acknowledgments The authors would like to thank all the personnel who provided technical support or assisted with data collection. We also acknowledge the reviewers for their valuable comments and suggestions. Funding: This research was funded by the Research Topic on Educational Informatization in Jiangsu Higher Education Institutions ( grant No. 2025JSETKT172 ) and Nantong Social Livelihood Science and Technology Plan (Directive Project) ( grant No. MS2024019 ). Conflicts of Interest: All authors have completed the ICMJE uniform disclosure form (available at https://qims.amegroups.com/article/view/10.21037/qims-2025-576/coif ). The authors have no conflicts of interest to declare. Ethical Statement: The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. References 1 Rastogi D Johri P Donelli M Kadry S Khan AA Espa G Feraco P Kim J . Deep learning-integrated MRI brain tumor analysis: feature extraction, segmentation, and Survival Prediction using Replicator and volumetric networks. Sci Rep 2025 ; 15 : 1437 . 10.1038/s41598-024-84386-0 39789043 PMC11718254 2 Gao C Wu L Wu W Huang Y Wang X Sun Z Xu M Gao C . Deep learning in pulmonary nodule detection and segmentation: a systematic review. Eur Radiol 2025 ; 35 : 255 - 66 . 10.1007/s00330-024-10907-0 38985185 PMC11632000 3 Han K Sheng VS Song Y Liu Y Qiu C Ma S Liu Z . Deep semi-supervised learning for medical image segmentation: a review. Expert Syst Appl 2024 ; 245 : 123052 . 4 Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells W, Frangi A. editors. Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015. Cham: Springer; 2015:234-41. 5 Azad R Aghdam EK Rauland A Jia Y Avval AH Bozorgpour A Karimijafarbigloo S Cohen JP Adeli E Merhof D . Medical Image Segmentation Review: The Success of U-Net. IEEE Trans Pattern Anal Mach Intell 2024 ; 46 : 10076 - 95 . 10.1109/TPAMI.2024.3435571 39167505 6 Thapar P Rakhra M Prashar D Mrsic L Khan AA Kadry S . Skin cancer segmentation and classification by implementing a hybrid FrCN-(U-NeT) technique with machine learning. PLoS One 2025 ; 20 : e0322659 . 10.1371/journal.pone.0322659 40455780 PMC12129148 7 Alqarafi A Khan AA Mahendran RK Al-Sarem M Albalwy F . Multi-scale GC-T2: Automated region of interest assisted skin cancer detection using multi-scale graph convolution and tri-movement based attention mechanism. Biomed Signal Process Control 2024 ; 95 : 106313 . 8 Chen J Mei J Li X Lu Y Yu Q Wei Q Luo X Xie Y Adeli E Wang Y Lungren MP Zhang S Xing L Lu L Yuille A Zhou Y . TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers. Med Image Anal 2024 ; 97 : 103280 . 10.1016/j.media.2024.103280 39096845 9 Kim JW Khan AU Banerjee I . Systematic Review of Hybrid Vision Transformer Architectures for Radiological Image Analysis. J Imaging Inform Med 2025 ; 38 : 3248 - 62 . 10.1007/s10278-024-01322-4 39871042 PMC12572492 10 Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752 (Accessed September 17, 2025). Available online: https://arxiv.org/abs/2312.00752 11 Ruan J, Li J, Xiang S. VM-Unet: Vision mamba unet for medical image segmentation. arXiv:2402.02491 (Accessed September 17, 2025). Available online: https://arxiv.org/abs/2402.02491 12 Ma X Zhang X Pun MO . RS3Mamba: Visual state space model for remote sensing image semantic segmentation. IEEE Geosci Remote Sens Lett 2024 ; 21 : 6011405 . 13 Li G Huang Q Wang W Liu L . Selective and multi-scale fusion Mamba for medical image segmentation. Expert Syst Appl 2025 ; 261 : 125518 . 14 Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y, Han X, Chen YW, Wu J. UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Barcelona: IEEE; 2020:1055-9. 15 Gu Z Cheng J Fu H Zhou K Hao H Zhao Y Zhang T Gao S Liu J. CE-Net: Context Encoder Network for 2D Medical Image Segmentation. IEEE Trans Med Imaging 2019 ; 38 : 2281 - 92 . 10.1109/TMI.2019.2903562 30843824 16 Oktay O, Schlemper J, Le Folgoc L, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, Glocker B, Rueckert D. Attention U-Net: Learning where to look for the pancreas. arXiv:1804.03999 (Accessed September 17, 2025). Available online: https://arxiv.org/abs/1804.03999 17 Das N Das S . Attention-UNet architectures with pretrained backbones for multi-class cardiac MR image segmentation. Curr Probl Cardiol 2024 ; 49 : 102129 . 10.1016/j.cpcardiol.2023.102129 37866419 18 Fan T, Wang G, Li Y, Wang H. MA-Net: A multi-scale attention network for liver and tumor segmentation. IEEE Access 2020;8:179656-65. 19 Huang H Chen Z Zou Y Lu M Chen C Song Y Zhang H Yan F . Channel prior convolutional attention for medical image segmentation. Comput Biol Med 2024 ; 178 : 108784 . 10.1016/j.compbiomed.2024.108784 38941900 20 Xu J, Xiong Z, Bhattacharyya SP. PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver: IEEE; 2023:19529-39. 21 Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-Unet: Unet-like pure transformer for medical image segmentation. In: Karlinsky L, Michaeli T, Nishino K. editors. Computer Vision &#8211; ECCV 2022 Workshops. Cham: Springer; 2023:205-18. 22 Li Z Zheng Y Shan D Yang S Li Q Wang B Zhang Y Hong Q Shen D. ScribFormer: Transformer Makes CNN Work Better for Scribble-Based Medical Image Segmentation. IEEE Trans Med Imaging 2024 ; 43 : 2254 - 65 . 10.1109/TMI.2024.3363190 38324425 23 Lu Z She C Wang W Huang Q. LM-Net: A light-weight and multi-scale network for medical image segmentation. Comput Biol Med 2024 ; 168 : 107717 . 10.1016/j.compbiomed.2023.107717 38007973 24 Meng W Liu S Wang H. AFC-Unet: Attention-fused full-scale CNN-transformer unet for medical image segmentation. Biomed Signal Process Control 2025 ; 99 : 106839 . 25 Zhu L, Liao B, Zhang Q, Wang X, Liu W, Wang X. Vision Mamba: Efficient visual representation learning with bidirectional state space model. arXiv:2401.09417 (Accessed September 17, 2025). Available online: https://arxiv.org/abs/2401.09417 26 Shi Y Xia B Jin X Wang X Zhao T Xia X. Vmambair: Visual state space model for image restoration. IEEE Trans Circuits Syst Video Technol 2025 ; 35 : 5560 - 74 . 27 Zhang M, Yu Y, Jin S, Gu L, Ling T, Tao X. VM-UNET-V2: Rethinking vision mamba UNet for medical image segmentation. In: Peng W, Cai Z, Skums P. editors. Bioinformatics Research and Applications. ISBRA 2024. Singapore: Springer; 2024:335-46. 28 Xing Z, Ye T, Yang Y, Liu G, Zhu L. Segmamba: Long-range sequential modeling mamba for 3D medical image segmentation. In: Linguraru MG, Dou Q, Feragen A, Giannarou S, Glocker B, Lekadir K, Schnabel JA. editors. Medical Image Computing and Computer Assisted Intervention &#8211; MICCAI 2024. Cham: Springer; 2024:578-88. 29 Wu R Liu Y Liang P Chang Q. H-vmunet: High-order vision mamba unet for medical image segmentation. Neurocomputing 2025 ; 624 : 129447 . 30 Buda M Saha A Mazurowski MA . Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm. Comput Biol Med 2019 ; 109 : 218 - 25 . 10.1016/j.compbiomed.2019.05.002 31078126 31 Jha D, Smedsrud PH, Riegler MA, Halvorsen P, De Lange T, Johansen D, Johansen HD. Kvasir-SEG: A Segmented Polyp Dataset. In: Ro Y, Cheng WH, Kim J, Chu WT, Cui P, Choi JW, Hu MC, De Neve W, editors. MultiMedia Modeling. MMM 2020. Cham: Springer; 2020:451-62. 32 Bernal J S&#225;nchez FJ Fern&#225;ndez-Esparrach G Gil D Rodr&#237;guez C Vilari&#241;o F . WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Comput Med Imaging Graph 2015 ; 43 : 99 - 111 . 10.1016/j.compmedimag.2015.02.007 25863519 33 Silva J Histace A Romain O Dray X Granado B . Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer. Int J Comput Assist Radiol Surg 2014 ; 9 : 283 - 93 . 10.1007/s11548-013-0926-3 24037504 34 Zhu Y Peng M Wang X Huang X Xia M Shen X Jiang W. LGCE-Net: A local and global contextual encoding network for effective and efficient medical image segmentation. Appl Intell 2025 . doi: 10.1007/s10489-024-05900-5 35 Liu F Hua Z Li J Fan L. DBMF: Dual Branch Multiscale Feature Fusion Network for polyp segmentation. Comput Biol Med 2022 ; 151 : 106304 . 10.1016/j.compbiomed.2022.106304 36401969 36 Nanni L Lumini A Fantozzi C . Exploring the potential of ensembles of deep learning networks for image segmentation. Information 2023 ; 14 : 657 . 37 Chen LC Papandreou G Kokkinos I Murphy K Yuille AL . DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE Trans Pattern Anal Mach Intell 2018 ; 40 : 834 - 48 . 10.1109/TPAMI.2017.2699184 28463186 38 Kujur A, Raza Z, Khan AA, Wechtaisong C. Data complexity based evaluation of the model dependence of brain MRI images for classification of brain tumor and Alzheimer&#8217;s disease. IEEE Access 2022;10:112117-33."
}