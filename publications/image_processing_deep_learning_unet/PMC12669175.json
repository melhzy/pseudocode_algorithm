{
  "pmcid": "PMC12669175",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.731555",
  "metadata": {
    "journal_title": "Frontiers in Neuroscience",
    "journal_nlm_ta": "Front Neurosci",
    "journal_iso_abbrev": "Front Neurosci",
    "journal": "Frontiers in Neuroscience",
    "pmcid": "PMC12669175",
    "pmid": "41341263",
    "doi": "10.3389/fnins.2025.1699700",
    "title": "Detection of leptomeningeal angiomas in brain MRI of Sturge-Weber syndrome using multi-scale multi-scan Mamba",
    "authors": [
      "Bao Weiqun",
      "Xue Chenghao",
      "Su Ruisheng",
      "Hu Xindan",
      "Li Yuanning",
      "Wang Xiaoqiang",
      "Tan Tao",
      "He Dake",
      "Xu Lin"
    ],
    "abstract": "Objectives Sturge-Weber syndrome (SWS) is a congenital neurological disorder occurring in the early childhood. Timely diagnosis of SWS is essential for proper medical intervention that prevents the development of various neurological issues. Leptomeningeal angiomas (LA) are the clinical manifestation of SWS. Detection of LA is currently performed by manual inspection of the magnetic resonance images (MRI) by experienced neurologist, which is time-consuming and lack of inter-rater consistency. The aim of the present study is to investigate automated LA detection in MRI of SWS patients. Methods A Mamba-based encoder-decoder architecture was employed in the present study. Particularly, a multi-scale multi-scan strategy was proposed to convert 3-D volume into 1-D sequence, enabling capturing long-range dependency with reduced computation complexity. Our dataset consists of 40 SWS patients with T1-enhanced MRI. The proposed model was first pre-trained on a public brain tumor segmentation (BraTS) dataset and then fine-tuned and tested on the SWS dataset using 5-fold cross validation. Results and conclusion Our results show excellent performance of the proposed method, e.g., Dice score of 91.53% and 78.67% for BraTS and SWS, respectively, outperforming several state-of-the-art methods as well as two neurologists. Mamba-based deep learning method can automatically identify LA in MRI images, enabling automated SWS diagnosis in clinical settings.",
    "keywords": [
      "Sturge-Weber syndrome",
      "leptomeningeal angiomas",
      "magnetic resonance imaging",
      "Mamba",
      "multi-scale multi-scan"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Front Neurosci</journal-id><journal-id journal-id-type=\"iso-abbrev\">Front Neurosci</journal-id><journal-id journal-id-type=\"pmc-domain-id\">670</journal-id><journal-id journal-id-type=\"pmc-domain\">frontneurosci</journal-id><journal-id journal-id-type=\"publisher-id\">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type=\"ppub\">1662-4548</issn><issn pub-type=\"epub\">1662-453X</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12669175</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12669175.1</article-id><article-id pub-id-type=\"pmcaid\">12669175</article-id><article-id pub-id-type=\"pmcaiid\">12669175</article-id><article-id pub-id-type=\"pmid\">41341263</article-id><article-id pub-id-type=\"doi\">10.3389/fnins.2025.1699700</article-id><article-version-alternatives><article-version article-version-type=\"pmc-version\">1</article-version><article-version article-version-type=\"Version of Record\" vocab=\"NISO-RP-8-2008\"/></article-version-alternatives><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Detection of leptomeningeal angiomas in brain MRI of Sturge-Weber syndrome using multi-scale multi-scan Mamba</article-title></title-group><contrib-group><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Bao</surname><given-names initials=\"W\">Weiqun</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"fn001\" ref-type=\"author-notes\">\n<sup>&#8224;</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role></contrib><contrib contrib-type=\"author\" equal-contrib=\"yes\"><name name-style=\"western\"><surname>Xue</surname><given-names initials=\"C\">Chenghao</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"fn001\" ref-type=\"author-notes\">\n<sup>&#8224;</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3194460\"/></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Su</surname><given-names initials=\"R\">Ruisheng</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">\n<sup>3</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names initials=\"X\">Xindan</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"Y\">Yuanning</given-names></name><xref rid=\"aff4\" ref-type=\"aff\">\n<sup>4</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"X\">Xiaoqiang</given-names></name><xref rid=\"aff5\" ref-type=\"aff\">\n<sup>5</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Tan</surname><given-names initials=\"T\">Tao</given-names></name><xref rid=\"aff6\" ref-type=\"aff\">\n<sup>6</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>He</surname><given-names initials=\"D\">Dake</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"L\">Lin</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/933756\"/></contrib></contrib-group><aff id=\"aff1\"><label>1</label><institution>Department of Pediatric Neurology, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</institution>, <city>Shanghai</city>, <country country=\"cn\">China</country></aff><aff id=\"aff2\"><label>2</label><institution>School of Information Science and Technology, ShanghaiTech University</institution>, <city>Shanghai</city>, <country country=\"cn\">China</country></aff><aff id=\"aff3\"><label>3</label><institution>Department of Biomedical Engineering, Eindhoven University of Technology</institution>, <city>Eindhoven</city>, <country country=\"nl\">Netherlands</country></aff><aff id=\"aff4\"><label>4</label><institution>School of Biomedical Engineering, ShanghaiTech University</institution>, <city>Shanghai</city>, <country country=\"cn\">China</country></aff><aff id=\"aff5\"><label>5</label><institution>Department of Pediatric Neurosurgery, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</institution>, <city>Shanghai</city>, <country country=\"cn\">China</country></aff><aff id=\"aff6\"><label>6</label><institution>Faculty of Applied Sciences, Macao Polytechnic University</institution>, <city>Macao, Macao SAR</city>, <country country=\"cn\">China</country></aff><author-notes><corresp id=\"c001\"><label>*</label>Correspondence: Lin Xu, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:xulin1@shanghaitech.edu.cn\">xulin1@shanghaitech.edu.cn</email>; Xiaoqiang Wang, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:Wangxiaoqiang419@163.com\">Wangxiaoqiang419@163.com</email>; Tao Tan, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:taotanjs@gmail.com\">taotanjs@gmail.com</email>; Dake He, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:hedake@139.com\">hedake@139.com</email></corresp><fn fn-type=\"equal\" id=\"fn001\"><label>&#8224;</label><p>These authors have contributed equally to this work</p></fn></author-notes><pub-date publication-format=\"electronic\" date-type=\"pub\" iso-8601-date=\"2025-11-18\"><day>18</day><month>11</month><year>2025</year></pub-date><pub-date publication-format=\"electronic\" date-type=\"collection\"><year>2025</year></pub-date><volume>19</volume><issue-id pub-id-type=\"pmc-issue-id\">480891</issue-id><elocation-id>1699700</elocation-id><history><date date-type=\"received\"><day>05</day><month>9</month><year>2025</year></date><date date-type=\"accepted\"><day>27</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>18</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-04 09:25:13.720\"><day>04</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Bao, Xue, Su, Hu, Li, Wang, Tan, He and Xu.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Bao, Xue, Su, Hu, Li, Wang, Tan, He and Xu</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\" start_date=\"2025-11-18\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"fnins-19-1699700.pdf\"/><abstract><sec><title>Objectives</title><p>Sturge-Weber syndrome (SWS) is a congenital neurological disorder occurring in the early childhood. Timely diagnosis of SWS is essential for proper medical intervention that prevents the development of various neurological issues. Leptomeningeal angiomas (LA) are the clinical manifestation of SWS. Detection of LA is currently performed by manual inspection of the magnetic resonance images (MRI) by experienced neurologist, which is time-consuming and lack of inter-rater consistency. The aim of the present study is to investigate automated LA detection in MRI of SWS patients.</p></sec><sec><title>Methods</title><p>A Mamba-based encoder-decoder architecture was employed in the present study. Particularly, a multi-scale multi-scan strategy was proposed to convert 3-D volume into 1-D sequence, enabling capturing long-range dependency with reduced computation complexity. Our dataset consists of 40 SWS patients with T1-enhanced MRI. The proposed model was first pre-trained on a public brain tumor segmentation (BraTS) dataset and then fine-tuned and tested on the SWS dataset using 5-fold cross validation.</p></sec><sec><title>Results and conclusion</title><p>Our results show excellent performance of the proposed method, e.g., Dice score of 91.53% and 78.67% for BraTS and SWS, respectively, outperforming several state-of-the-art methods as well as two neurologists. Mamba-based deep learning method can automatically identify LA in MRI images, enabling automated SWS diagnosis in clinical settings.</p></sec></abstract><kwd-group><kwd>Sturge-Weber syndrome</kwd><kwd>leptomeningeal angiomas</kwd><kwd>magnetic resonance imaging</kwd><kwd>Mamba</kwd><kwd>multi-scale multi-scan</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported in part by the National Natural Science Foundation of China under Grant 62171284.</funding-statement></funding-group><counts><fig-count count=\"8\"/><table-count count=\"7\"/><equation-count count=\"10\"/><ref-count count=\"40\"/><page-count count=\"14\"/><word-count count=\"8548\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Brain Imaging Methods</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"s1\"><label>1</label><title>Introduction</title><p>Sturge-Weber syndrome (SWS) is a rare congenital neurological disorder with an incidence rate of approximately 1 in 20,000 to 50,000 live births (<xref rid=\"B6\" ref-type=\"bibr\">Comi, 2007</xref>). It is often associated with facial port-wine stains (PWS), glaucoma, and ipsilateral leptomeningeal angiomas (LA) (<xref rid=\"B31\" ref-type=\"bibr\">Sudarsanam and Ardern-Holmes, 2014</xref>; <xref rid=\"B34\" ref-type=\"bibr\">Thomas-Sohl et al., 2004</xref>). SWS can lead to various neurological issues such as seizures, hemiparesis, headaches, and cognitive impairments, particularly in children under five years (<xref rid=\"B32\" ref-type=\"bibr\">Sujansky and Conradi, 1995</xref>). Given the high susceptibility to seizures in this age group, early detection of SWS and proper medical intervention are crucial. Although facial PWS is a major characteristic of SWS, not everyone with facial PWS suffers from SWS (<xref rid=\"B34\" ref-type=\"bibr\">Thomas-Sohl et al., 2004</xref>). Therefore, it is recommended that children with PWS undergo imaging evaluation, such as magnetic resonance imaging (MRI), to determine whether they have SWS (<xref rid=\"B1\" ref-type=\"bibr\">Bar et al., 2020</xref>).</p><p>Clinical manifestation of SWS is the presence of intracranial vascular anomaly, i.e., LA (<xref rid=\"B24\" ref-type=\"bibr\">Ramirez and J&#252;lich, 2024</xref>). Currently, the identification of LA relies on visual inspection of the imaging data, particularly MRI, by experienced clinicians. However, the expertise of experienced doctors may not always be readily available, and manual labeling of lesions is time-consuming. Therefore, an automatic diagnosis tool is required for accurate and efficient diagnosis of SWS.</p><p>With the advance in deep learning, encoder-decoder-based frameworks such as U-Net and its variants have become the dominant architectures for medical image segmentation (<xref rid=\"B25\" ref-type=\"bibr\">Ronneberger et al., 2015</xref>; <xref rid=\"B5\" ref-type=\"bibr\">&#199;i&#231;ek et al., 2016</xref>; <xref rid=\"B29\" ref-type=\"bibr\">Soh and Rajapakse, 2023</xref>; <xref rid=\"B2\" ref-type=\"bibr\">Chen et al., 2025</xref>), playing a key role in computer-aided diagnosis (CAD) systems. U-Net primarily utilizes the encoder-decoder structure with convolutional neural networks (CNNs) in each layer to extract hierarchical image features, enabling precise segmentation of the image in pixel level (<xref rid=\"B5\" ref-type=\"bibr\">&#199;i&#231;ek et al., 2016</xref>). Moreover, nnU-Net automates hyperparameter tuning and data preprocessing for U-Net, and therefore enhances the performance in many segmentation tasks (<xref rid=\"B14\" ref-type=\"bibr\">Isensee et al., 2021</xref>). However, while CNNs are efficient in feature extraction, they are constrained by their local receptive fields, making it difficult to capture long-range dependencies that are crucial for tasks where global context is essential (<xref rid=\"B29\" ref-type=\"bibr\">Soh and Rajapakse, 2023</xref>).</p><p>In addition to CNNs, Transformer has emerged as a powerful alternative, demonstrating exceptional ability to model global relationships and capture long-range dependencies (<xref rid=\"B7\" ref-type=\"bibr\">Dosovitskiy et al., 2021</xref>; <xref rid=\"B23\" ref-type=\"bibr\">Raghu et al., 2021</xref>). Vision Transformer(ViT) employs the transformer architecture for image processing, capturing global dependencies in images through self-attention mechanisms. Nevertheless, their quadratic complexity associated with the length of the input sequence renders them computationally intensive and impractical for handling high-dimensional medical images (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>). This computational burden hinders their applicability in real-world settings with limited resources.</p><p>Mamba has been recently proposed to capture long-range dependence with reduced computational burden. It is built on the state space models (SSMs) with optimized state space matrices (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>), and has shown promising performance in natural language processing and also been adapted for computer vision tasks (<xref rid=\"B40\" ref-type=\"bibr\">Zhu et al., 2025</xref>). However, as the SSMs work initially on 1-D sequence, adapting Mamba for image tasks requires to convert the 2- or 3-D images into 1-D sequence. Several strategies have been proposed for such adaptation. U-Mamba flattens image patches using a original scan order, i.e., row by row sequentially (<xref rid=\"B21\" ref-type=\"bibr\">Ma et al., 2024</xref>). VMamba introduces a cross-scan module to decompose 2-D images into four distinct 1-D scanning paths (horizontal, vertical, and their reverse directions) to better preserve spatial relationships (<xref rid=\"B20\" ref-type=\"bibr\">Liu Y. et al., 2024</xref>). Swin-UMamba integrates a shifted window partitioning strategy (inspired by Swin Transformers) with VMamba's directional scanning, while also leveraging ImageNet pre-training for enhanced feature representation (Liu et al., <xref rid=\"B19\" ref-type=\"bibr\">J. 2024</xref>). In addition, SegMamba proposes a tri-orientated scanning scheme for 3-D images, i.e., scanning along axial, sagittal, and coronal planes separately to capture volumetric dependencies (<xref rid=\"B36\" ref-type=\"bibr\">Xing et al., 2024</xref>).</p><p>Despite their wide application, deep-learning-based CAD techniques have not yet been applied to the diagnosis of SWS. The aim of the present study is therefore to develop a deep-learning-based tool for automatic detection of LA in the brain MRI of SWS patients. The study is designed as a retrospective investigation of a clinical dataset consisting of 40 SWS patients. The encoder-decoder structure and Mamba model are considered. In order to deal with the random location of LAs in MRI scans, a novel multi-scale multi-scan (MSMS) strategy is proposed to convert 3-D volumes into 1-D sequences. Besides, due to the rareness of the disease and thus limited sample size, transfer learning is considered by pre-training the model with a public dataset, i.e, the brain tumor segmentation (BraTS) benchmark dataset, and then fun-tuning and testing it on our SWS dataset. The performance of the proposed method is compared with CNNs, Transformers, and state-of-the-art (SOTA) Mamba-based models in terms of a number of evaluation metrics.</p><p>The main novelty and contributions of the present study are summarized hereafter.</p><list list-type=\"bullet\"><list-item><p>An encoder-decoder deep learning method was employed, for the first time, for automatic identification of LA in the brain MRI images, and thus for automated diagnosis of SWS.</p></list-item><list-item><p>Mamba model is embedded in the encoder in order to capture long-rang dependency in 3-D volumes.</p></list-item><list-item><p>A multi-scale multi-scan strategy is proposed in the Mamba model to extract multi-scale futures along different directions of the 3-D volume.</p></list-item><list-item><p>Pre-training on the BraTS dataset is employed to address the challenge of limited sample size.</p></list-item></list></sec><sec id=\"s2\"><label>2</label><title>Material and methods</title><sec><label>2.1</label><title>Dataset and pre-processing</title><p>This study was designed as a retrospective study of existing data collected during clinical practice at Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine (XH-SJTU-SM). Patients diagnosed as SWS between June 2008 and August 2024 were considered. The exclusion criteria include: (a) younger than 3 months; (b) syndrome type II, i.e., without LA; (c) missing clinical information or poor-quality images impeding manual annotation. Consequently, 40 SWS patients (13 girls and 27 boys, age between 5-696 months) were involved in this study. The study protocol was approved by the Ethics Committee at XH-SJTU-SM with the number XHEC-D-2024-090. Written informed consent of each subject was waived by the ethics committee due to its retrospective nature.</p><p>T1-weighted MRI images were analyzed in the present study. They were recorded using different scanners, i.e., Siemens, Philips, and General Electric (GE), with two different magnetic field strengths, i.e., 1.5T and 3T. Details of the scanners and their parameters are summarized in <xref rid=\"T1\" ref-type=\"table\">Table 1</xref>. In each MRI image, the brain areas containing LA were annotated as the ground truth by one experienced neuroradiologist from XH-SJTU-SM using the well-known ITK-SNAP toolbox (<xref rid=\"B38\" ref-type=\"bibr\">Yushkevich et al., 2006</xref>). An representative examples of LA in T1-enhanced MRI and the corresponding annotations is shown in <xref rid=\"F1\" ref-type=\"fig\">Figure 1</xref>. Note that marking only the abnormal intracranial vessels in the MRI image was quite difficult and therefore was not considered in the present study.</p><table-wrap position=\"float\" id=\"T1\" orientation=\"portrait\"><label>Table 1</label><caption><p>MRI scanners and corresponding parameters.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Scanner</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>NOP</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>TR/TE</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>FOV</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>MZ</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>ST</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Si 1.5T</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1,600/8.9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">200 &#215; 200</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">256 &#215; 180</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Ph 3T</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1,800/20</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">230 &#215; 187</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">260 &#215; 189</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">GE 3T</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">17</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1,750/24</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">200 &#215; 200</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#215; 224</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">GE 1.5T</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">15</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">520/9.9</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">240 &#215; 240</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#215; 160</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5</td></tr></tbody></table><table-wrap-foot><p>Si, Siemens; Ph, Philips; NOP, number of patients; TR, repetition time (ms); TE, echo time (ms); FOV, Filed of view (mm<sup>2</sup>); MZ, Matrix size (pixel<sup>2</sup>); ST, Slice thickness (mm).</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"F1\" orientation=\"portrait\"><label>Figure 1</label><caption><p>LA in T1-enhanced brain MRI of the SWS patients and the corresponding annotations.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0001.jpg\"><alt-text content-type=\"machine-generated\">T1-enhanced MRI images of the brain in four columns. The top row shows grayscale MRI scans, while the bottom row displays the same scans with red annotations highlighting specific regions of interest in the brain.</alt-text></graphic></fig><p>In each MRI image, the skull was first removed using the FSL toolbox (<xref rid=\"B38\" ref-type=\"bibr\">Yushkevich et al., 2006</xref>) to avoid its affects on the identification of contrast-enhanced LA. Then the images recorded with different scanners and protocols were resampled to uniform physical spacing of 0.47 mm &#215; 0.47 mm &#215; 6.5 mm. In addition, each image was cropped to contain only the brain area. Due to different brain sizes of the subjects, the largest size of the cropped image was determined as 320 pixels &#215; 320 pixels, and all cropped images were zero-padded to this unique size to keep consistency for further analysis.</p><p>Furthermore, a harmonization technique (<xref rid=\"B33\" ref-type=\"bibr\">Tan et al., 2023</xref>) was applied to the cropped images to compensate the influence of different scanners and imaging parameters. To this end, a high-quality MRI scan was first selected among all the images as a reference image (<italic toggle=\"yes\">I</italic><sub>ref</sub>) based on visual inspection. Then each brain image <italic toggle=\"yes\">I</italic> was smoothed using <italic toggle=\"yes\">k</italic> Gaussian kernels with different standard deviations:</p><disp-formula id=\"EQ1\"><mml:math id=\"M1\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(1)</label></disp-formula><p>where <italic toggle=\"yes\">G</italic><sub>&#963;<sub><italic toggle=\"yes\">k</italic></sub></sub> denotes the <italic toggle=\"yes\">k</italic><sup><italic toggle=\"yes\">th</italic></sup> Gaussian kernel, and &#963;<sub><italic toggle=\"yes\">k</italic></sub> = 0, 2, 4, 6, for <italic toggle=\"yes\">k</italic> = 0, 1, 2, 3, respectively. Note that &#963;<sub>0</sub> = 0 corresponds to the original (non-smoothed) image.</p><p>Each smoothed image <italic toggle=\"yes\">I</italic><sup>(<italic toggle=\"yes\">k</italic>)</sup> was then normalized with respected to the reference image, given by</p><disp-formula id=\"EQ2\"><mml:math id=\"M2\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mover accent=\"true\"><mml:mi>I</mml:mi><mml:mo>&#732;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msup><mml:mo>&#8722;</mml:mo><mml:msup><mml:mi>&#956;</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>&#183;</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula><p>where &#956; and &#963; indicates respectively mean and standard deviation, and <inline-formula><mml:math id=\"M3\" overflow=\"scroll\"><mml:msubsup><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id=\"M4\" overflow=\"scroll\"><mml:msubsup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> are computed from the smoothed reference image <inline-formula><mml:math id=\"M5\" overflow=\"scroll\"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> calculated as <inline-formula><mml:math id=\"M6\" overflow=\"scroll\"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>ref</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. Finally, a harmonized image was obtained as</p><disp-formula id=\"EQ3\"><mml:math id=\"M7\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>harm</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle=\"true\"><mml:munderover accentunder=\"false\" accent=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mover accent=\"true\"><mml:mi>I</mml:mi><mml:mo>&#732;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(3)</label></disp-formula></sec><sec><label>2.2</label><title>Mamba-based segmentation framework</title><sec><label>2.2.1</label><title>Overall framework</title><p>The overall architecture of the proposed segmentation framework is illustrated in <xref rid=\"F2\" ref-type=\"fig\">Figure 2</xref>. It is based on the U-Net structure with six encoding and decoding layers. In the encoder, each layer consists of two 3D CNN modules and a multi-scale multi-scan (MSMS) Mamba block in between. The first CNN is used to extract 3-D features from the input volume. It is composed of a depthwise convolution followed by a pointwise convolution for efficient channel-wise feature mixing (<xref rid=\"B39\" ref-type=\"bibr\">Zhang et al., 2019</xref>), followed by an instance normalization (<xref rid=\"B35\" ref-type=\"bibr\">Ulyanov et al., 2016</xref>), and a Leaky ReLU activation function (<xref rid=\"B37\" ref-type=\"bibr\">Xu et al., 2020</xref>). The MSMS-Mamba block is connected to the output of the residual CNN to enhance the model's ability to capture long-range dependencies and spatial correlations. The second 3D CNN is employed as an efficient alternative to traditional max-pooling to progressively down-sample the feature map in each encoding layer.</p><fig position=\"float\" id=\"F2\" orientation=\"portrait\"><label>Figure 2</label><caption><p>The overall architecture of the proposed segmentation framework.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0002.jpg\"><alt-text content-type=\"machine-generated\">Diagram showing a 3D convolutional neural network architecture for image segmentation. It begins with a 3D input, processed through MSMS-Mamba modules and blocks, layer normalization, and convolution feed-forward networks (ConvFFN). The Mamba Encoder and Decoder units are used, along with skip connections. The network outputs a segmented 3D image.</alt-text></graphic></fig><p>In the decoder, each layer comprises a 3D CNN module and an up-sampling module, i.e., transposed 3D convolution. The 3D CNN module is the same as the encoder. The up-sampling module leverages 3D transposed convolutions to progressively restore the spatial resolution of feature maps. At the end of the encoder, i.e., the final layer, a segmentation head with 1 7 1 convolution is employed to perform pixel-wise classification, producing the final segmentation output. Besides, following established practices in medical image segmentation (<xref rid=\"B14\" ref-type=\"bibr\">Isensee et al., 2021</xref>; <xref rid=\"B10\" ref-type=\"bibr\">Hatamizadeh et al., 2021</xref>), skip connections are introduced between corresponding encoding and decoding layers to retain spatial detail and low-level features.</p><p>The main novelty and contribution of the present work lies in the integration of the MSMS-Mamba block in the encoder to enhance long-range dependency modeling and improve feature fusion. The scheme of the MSMS-Mamba block is shown in the left of <xref rid=\"F2\" ref-type=\"fig\">Figure 2</xref>. It is composed of four consecutive components: an initial layer normalization, the MSMS-Mamba module for long-range dependency and multi-scale modeling, a second layer normalization, and a convolutional feed-forward network (<xref rid=\"B28\" ref-type=\"bibr\">Shi et al., 2024</xref>) for feature refinement. Among the four components, the MSMS-Mamba module is the key of the MSMS-Mamba block, whose scheme is shown in <xref rid=\"F3\" ref-type=\"fig\">Figure 3</xref>. It is built based on a SSM model with dedicated MSMS strategy, as shown in <xref rid=\"F4\" ref-type=\"fig\">Figure 4</xref>. Details of the SSM model and the MSMS strategy are introduced hereafter.</p><fig position=\"float\" id=\"F3\" orientation=\"portrait\"><label>Figure 3</label><caption><p>MSMS-Mamba module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0003.jpg\"><alt-text content-type=\"machine-generated\">Flowchart depicting a neural network architecture. It starts with two parallel linear projection layers, followed by a SiLU activation function. One branch includes a convolution layer before the SiLU function. Both paths merge into a block labeled &#8220;SSM with MSMS&#8221;, followed by another linear projection layer and SiLU. The flow terminates with a final linear projection.</alt-text></graphic></fig><fig position=\"float\" id=\"F4\" orientation=\"portrait\"><label>Figure 4</label><caption><p>SSM with multi-scale multi-scan strategy.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0004.jpg\"><alt-text content-type=\"machine-generated\">Diagram showing a process flow with subsampling, multi-scan, multi-merge, flatten, and reshape steps for data in colored matrices. Subsampling is applied to inputs (SV1 to SV8), processed by SSM blocks, merged, and fused into a final structure.</alt-text></graphic></fig></sec><sec><label>2.2.2</label><title>Selective state space model</title><p>SSM is a class of sequence-to-sequence architecture that has undergone significant evolution in recent years. The development of SSMs (<xref rid=\"B9\" ref-type=\"bibr\">Gu et al., 2022</xref>) originates from their foundational formulation as a linear time-invariant (LTI) system, which maps an input sequence <italic toggle=\"yes\">x</italic>(<italic toggle=\"yes\">t</italic>) &#8712; &#8477;<sup><italic toggle=\"yes\">L</italic></sup> to an output sequence <italic toggle=\"yes\">y</italic>(<italic toggle=\"yes\">t</italic>) &#8712; &#8477;<sup><italic toggle=\"yes\">L</italic></sup> through a hidden state <italic toggle=\"yes\">h</italic>(<italic toggle=\"yes\">t</italic>) &#8712; &#8450;<sup><italic toggle=\"yes\">N</italic></sup>. The continuous form of SSMs is defined as:</p><disp-formula id=\"EQ4\"><mml:math id=\"M8\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>&#8242;</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#160;</mml:mtext><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(4)</label></disp-formula><p>where <italic toggle=\"yes\"><bold>A</bold></italic> &#8712; &#8450;<sup><italic toggle=\"yes\">N</italic>&#215;<italic toggle=\"yes\">N</italic></sup>, <italic toggle=\"yes\"><bold>B</bold></italic> &#8712; &#8450;<sup><italic toggle=\"yes\">N</italic></sup>, and <italic toggle=\"yes\"><bold>C</bold></italic> &#8712; &#8450;<sup><italic toggle=\"yes\">N</italic></sup> are the system matrices, and <italic toggle=\"yes\">h</italic>(<italic toggle=\"yes\">t</italic>) &#8712; &#8477;<sup><italic toggle=\"yes\">N</italic></sup> represents the implicit latent state.</p><p>To make SSMs computationally tractable for discrete-time sequences, the bilinear transform method is employed to discretize the continuous ordinary differential equations. This results in the discrete form of SSMs,</p><disp-formula id=\"EQ5\"><mml:math id=\"M10\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:mi>h</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mstyle><mml:mi>h</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mstyle><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mover accent=\"true\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mstyle><mml:mi>h</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(5)</label></disp-formula><p>where <inline-formula><mml:math id=\"M12\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mo>&#916;</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mo>&#916;</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M13\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>-</mml:mo><mml:mo>&#916;</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#183;</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&#916;</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M14\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula>. Here, &#916; denotes the step size for converting a continuous sequence into a discrete sequence (<xref rid=\"B9\" ref-type=\"bibr\">Gu et al., 2022</xref>).</p><p>The discrete SSM can be further expressed in a convolutional form, shown as</p><disp-formula id=\"EQ6\"><mml:math id=\"M15\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:mi>y</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>&#8230;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#8195;&#8195;</mml:mtext><mml:mo>+</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#8195;</mml:mtext><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>K</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#8195;</mml:mtext><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>K</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover><mml:mo>*</mml:mo><mml:mi>x</mml:mi><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(6)</label></disp-formula><p>The matrices <inline-formula><mml:math id=\"M17\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"M18\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>B</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M19\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> remain fixed within every iteration. This allows pre-computing the convolution kernel <inline-formula><mml:math id=\"M20\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, significantly speeding up the training stage, as reported in <xref rid=\"B9\" ref-type=\"bibr\">Gu et al. (2022)</xref>.</p><p>SSMs are the core of a Mamba model. However, the pre-computing of <inline-formula><mml:math id=\"M21\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>K</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> results in a static representation, where the same matrices are applied to all tokens regardless of the input content, limiting the model's ability to perform context-aware reasoning. To address this limitation, the Mamba model (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>) introduces a selective mechanism to SSMs, making the system matrices input-dependent. This is achieved by constraining the system matrixes <italic toggle=\"yes\">B</italic>, <italic toggle=\"yes\">C</italic>, and &#916; as linear projection of the input sequence <italic toggle=\"yes\">x</italic> &#8712; &#8477;<sup><italic toggle=\"yes\">d</italic></sup> using a weight matrix <italic toggle=\"yes\">W</italic> &#8712; &#8477;<sup><italic toggle=\"yes\">d</italic>&#215;<italic toggle=\"yes\">N</italic></sup>. This makes <italic toggle=\"yes\">B</italic>, <italic toggle=\"yes\">C</italic>, and &#916; input-dependent. Consequently, the input dependency of &#916; causes <inline-formula><mml:math id=\"M22\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>A</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to also become input-dependent, rendering the pre-computed <inline-formula><mml:math id=\"M23\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"false\" class=\"mml-overline\"><mml:mrow><mml:mstyle mathvariant=\"bold-italic\"><mml:mi>K</mml:mi></mml:mstyle></mml:mrow><mml:mo accent=\"true\">&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> inapplicable. To address this, a parallel scanning algorithm is proposed in Mamba for efficient computation of these matrices (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>).</p></sec><sec><label>2.2.3</label><title>Multi-scale multi-scan strategy</title><p>Mamba is initially designed for processing 1-D series, but can be adapted for image processing tasks by serializing the 2-D or 3-D image into a 1-D sequence through flattening operation. However, directly applying Mamba to flattened 3-D medical images leads to limited receptive fields, hindering the model's ability to effectively capture spatial correlations in higher-dimensional data. This limitation becomes particularly evident in 3-D medical imaging, where preserving spatial structure is essential for accurate analysis. To address this challenges, we propose a MSMS strategy to convert our 3-D MRI data into 1D sequence, which is designed as a dual-branch structure in order to extract multi-scale features, as shown in <xref rid=\"F4\" ref-type=\"fig\">Figure 4</xref>. The upper branch performs feature extraction across different directions through a combination of sub-sampling and multi-scan operations. Meanwhile, the parallel lower branch maintains the original resolution to preserve fine-grained spatial information.</p><p>In the upper branch, sub-sampling is first employed to divide the input 3D volume <italic toggle=\"yes\">V</italic> into several smaller sub-volumes (SVs) in order to alleviate long-range forgetting (<xref rid=\"B28\" ref-type=\"bibr\">Shi et al., 2024</xref>). To this end, the 3D volume <italic toggle=\"yes\">V</italic> is uniformly partitioned into <italic toggle=\"yes\">N</italic> segments along each axis, resulting in <italic toggle=\"yes\">N</italic><sup>3</sup> basic elements: <inline-formula><mml:math id=\"M24\" overflow=\"scroll\"><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. In the present study, <italic toggle=\"yes\">N</italic> = 4 thus 64 basic elements are obtained. Then 8 SVs can be derived, with each consisting of 8 basic elements sampled from the entire volume (two elements from each axis), as shown in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>. For instance, the SV<sub>1</sub> in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref> is sampled from {<italic toggle=\"yes\">x</italic><sub>1</sub>, <italic toggle=\"yes\">x</italic><sub>3</sub>}, {<italic toggle=\"yes\">y</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>3</sub>}, and {<italic toggle=\"yes\">z</italic><sub>1</sub>, <italic toggle=\"yes\">z</italic><sub>3</sub>}, denoted as</p><disp-formula id=\"EQ7\"><mml:math id=\"M25\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:mtext class=\"textrm\" mathvariant=\"normal\">SV_1</mml:mtext><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(7)</label></disp-formula><p>Each SV is numbered in the same way as shown in <xref rid=\"F4\" ref-type=\"fig\">Figure 4</xref>. Different scanning orders along different spatial directions are then employed to flatten each SV into 1-D sequence, as listed in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>. The multi-scan operation is designed to extract multi-scale features from each SV while effectively expanding the receptive field. Instead of bidirectional scanning, only unidirectional scanning for each SV is considered to achieve a trade-off between segmentation performance and computational complexity. However, different scanning orders are employed for different SVs, equalling to multi-directional scanning of the original volume with reduced spatial sample rate. Such diverse scanning patterns across different SVs enable complementary coverage among sequences, facilitating joint expansion of context without increasing computational cost. SSM is then applied to each flattened 1-D sequence, which is followed by the multi-merge operation, an inverse operation of multi-scan, reconstructs the 3-D volume based on the output of SSMs.</p><table-wrap position=\"float\" id=\"T2\" orientation=\"portrait\"><label>Table 2</label><caption><p>Sub-volumes obtained by down-sampling and the corresponding scan order.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>SV</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Sub-sample</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Scan order</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>1</sub>, <italic toggle=\"yes\">x</italic><sub>3</sub>; <italic toggle=\"yes\">y</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>3</sub>; <italic toggle=\"yes\">z</italic><sub>1</sub>, <italic toggle=\"yes\">z</italic><sub>3</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[1, 2, 3, 4, 5, 6, 7, 8]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>2</sub>, <italic toggle=\"yes\">x</italic><sub>4</sub>; <italic toggle=\"yes\">y</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>3</sub>; <italic toggle=\"yes\">z</italic><sub>1</sub>, <italic toggle=\"yes\">z</italic><sub>3</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[8, 7, 6, 5, 4, 3, 2, 1]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_3</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>1</sub>, <italic toggle=\"yes\">x</italic><sub>3</sub>; <italic toggle=\"yes\">y</italic><sub>2</sub>, <italic toggle=\"yes\">y</italic><sub>4</sub>; <italic toggle=\"yes\">z</italic><sub>1</sub>, <italic toggle=\"yes\">z</italic><sub>3</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[1, 3, 2, 4, 5, 7, 6, 8]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>2</sub>, <italic toggle=\"yes\">x</italic><sub>4</sub>; <italic toggle=\"yes\">y</italic><sub>2</sub>, <italic toggle=\"yes\">y</italic><sub>4</sub>; <italic toggle=\"yes\">z</italic><sub>1</sub>, <italic toggle=\"yes\">z</italic><sub>3</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[8, 6, 7, 5, 4, 2, 3, 1]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_5</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>1</sub>, <italic toggle=\"yes\">x</italic><sub>3</sub>; <italic toggle=\"yes\">y</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>3</sub>; <italic toggle=\"yes\">z</italic><sub>2</sub>, <italic toggle=\"yes\">z</italic><sub>4</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[1, 5, 2, 6, 3, 7, 4, 8]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_6</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>2</sub>, <italic toggle=\"yes\">x</italic><sub>4</sub>; <italic toggle=\"yes\">y</italic><sub>1</sub>, <italic toggle=\"yes\">y</italic><sub>3</sub>; <italic toggle=\"yes\">z</italic><sub>2</sub>, <italic toggle=\"yes\">z</italic><sub>4</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[8, 4, 7, 3, 6, 2, 5, 1]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_7</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>1</sub>, <italic toggle=\"yes\">x</italic><sub>3</sub>; <italic toggle=\"yes\">y</italic><sub>2</sub>, <italic toggle=\"yes\">y</italic><sub>4</sub>; <italic toggle=\"yes\">z</italic><sub>2</sub>, <italic toggle=\"yes\">z</italic><sub>4</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[1, 2, 4, 3, 5, 6, 8, 7]</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SV_8</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><italic toggle=\"yes\">V</italic>{<italic toggle=\"yes\">x</italic><sub>2</sub>, <italic toggle=\"yes\">x</italic><sub>4</sub>; <italic toggle=\"yes\">y</italic><sub>2</sub>, <italic toggle=\"yes\">y</italic><sub>4</sub>; <italic toggle=\"yes\">z</italic><sub>2</sub>, <italic toggle=\"yes\">z</italic><sub>4</sub>}</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">[7, 8, 6, 5, 3, 4, 2, 1]</td></tr></tbody></table></table-wrap></sec></sec><sec><label>2.3</label><title>Evaluation</title><sec><label>2.3.1</label><title>Evaluation on public BraTS dataset</title><p>The performance of the proposed segmentation framework was first evaluated on a public dataset from the BraTS2023 Challenge (<xref rid=\"B15\" ref-type=\"bibr\">Kazerooni et al., 2024</xref>), which includes 1251 3D brain MRI images with gliomas. This dataset was acquired from multiple institutions under standard clinical conditions using four modalities (T1, T1Gd, T2, and T2-FLAIR). Whole tumor (WT), tumor core (TC), and enhancing tumor (ET) were manually annotated by experienced medical experts. Only T1-enhanced images were considered, the same as our SWS dataset. We adopted a 7:1:2 split for training, validation, and testing, and trained the model for 1,000 epochs to ensure efficient learning and reliable performance evaluation.</p></sec><sec><label>2.3.2</label><title>Evaluation on SWS dataset with pre-training strategy</title><p>While evaluating the model on the SWS dataset, a pre-training strategy was considered due to the limited subject size. In this scenario, the entire BraTS2023 dataset (<xref rid=\"B15\" ref-type=\"bibr\">Kazerooni et al., 2024</xref>) was adopted to pre-train the model for 1,000 epochs. Although gliomas in BraTS differ morphologically from SWS lesions, both share common segmentation principles&#8212;particularly the enhanced intensity patterns in T1-weighted images. Thus, pre-training on BraTS enables the model to leverage relevant priors and helps alleviate the limitations imposed by the small scale of the SWS dataset.</p><p>Given the fact that the spatial sampling distance of the SWS dataset in the z-axis is significantly larger than that of the x- and y-axis, down-sampling in the z-axis of the BraTS dataset was performed as it is originally equally sampled in all directions. After pre-training, the obtained model was fine-tuned and tested on the SWS dataset for 250 epochs, which is smaller than that of the BraTS dataset (1,000) due to the much smaller size of the SWS dataset. We utilized a 5-fold cross-validation strategy to evaluate the performance of the proposed method.</p></sec><sec><label>2.3.3</label><title>Performance metrics</title><p>To quantitatively evaluate the segmentation performance of the proposed model on both datasets, the widely adopted dice similarity (DS) coefficient was employed as the primary metric, which measures the spatial overlap between model predictions and ground truth annotations. This metric has become the de facto standard in medical image segmentation due to its robustness in assessing volumetric agreement. However, note that in the present study the SWS dataset are severely imbalanced, i.e., much smaller LA regions as compared the non-LA regions. Such imbalance may lead to a low Dice score.</p><p>Therefore, other metrics such as sensitivity (Sen), specificity (Spe), balanced accuracy (BAcc), accuracy (Acc), Jaccard Index (JI) (<xref rid=\"B25\" ref-type=\"bibr\">Ronneberger et al., 2015</xref>), Kappa coefficient (<xref rid=\"B4\" ref-type=\"bibr\">Chmura Kraemer et al., 2002</xref>), volumetric similarity (VS), Matthews correlation coefficient (MCC) (<xref rid=\"B3\" ref-type=\"bibr\">Chicco and Jurman, 2020</xref>), and 95% Hausdorff distance (HD95) (<xref rid=\"B13\" ref-type=\"bibr\">Huttenlocher et al., 1993</xref>) were also calculated to measure the similarity between the segmented tumor and the ground truth in order to achieve a comprehensive understanding of the model performance.</p></sec><sec><label>2.3.4</label><title>Implementation details</title><p>The PyTorch deep learning framework was utilized in our implementation. The training and evaluation processes were executed on a high-performance computing node featuring NVIDIA GeForce RTX 3090 graphics processor with CUDA 11.8 acceleration. We have implemented MSMSMamba on top of the well-established nnU-Net framework (<xref rid=\"B14\" ref-type=\"bibr\">Isensee et al., 2021</xref>). Its self-configuring feature has allowed us to focus on network design rather than other trivial details. The loss function was a combination of Dice loss and cross-entropy loss, defined as</p><disp-formula id=\"EQ8\"><mml:math id=\"M26\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mstyle mathvariant=\"script\"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mtext class=\"textrm\" mathvariant=\"normal\">total</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant=\"script\"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mtext class=\"textrm\" mathvariant=\"normal\">CE</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>&#183;</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant=\"script\"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mtext class=\"textrm\" mathvariant=\"normal\">Dice</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(8)</label></disp-formula><p>The Cross Entropy Loss &#8466;<sub>CE</sub> and Dice Loss &#8466;<sub>Dice</sub> are respectively defined as</p><disp-formula id=\"EQ9\"><mml:math id=\"M27\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mstyle mathvariant=\"script\"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mtext class=\"textrm\" mathvariant=\"normal\">CE</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle=\"true\"><mml:munderover accentunder=\"false\" accent=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo class=\"qopname\">log</mml:mo><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo class=\"qopname\">log</mml:mo><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(9)</label></disp-formula><p>and</p><disp-formula id=\"EQ10\"><mml:math id=\"M28\" overflow=\"scroll\"><mml:mtable class=\"eqnarray\" columnalign=\"left\"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mstyle mathvariant=\"script\"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mtext class=\"textrm\" mathvariant=\"normal\">Dice</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mstyle displaystyle=\"false\"><mml:munderover accentunder=\"false\" accent=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow><mml:mrow><mml:mstyle displaystyle=\"false\"><mml:munderover accentunder=\"false\" accent=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle=\"false\"><mml:munderover accentunder=\"false\" accent=\"false\"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(10)</label></disp-formula><p>where <italic toggle=\"yes\">N</italic> is the number of samples (or pixels), <italic toggle=\"yes\">y</italic><sub><italic toggle=\"yes\">i</italic></sub> &#8712; {0, 1} denotes the ground truth label of the <italic toggle=\"yes\">i</italic>-th sample, <italic toggle=\"yes\">p</italic><sub><italic toggle=\"yes\">i</italic></sub> &#8712; [0, 1] denotes the predicted probability for the positive class, and &#1013; is a small constant (e.g., 10<sup>&#8722;6</sup>) to prevent division by zero. The combined loss was supervised at each layer of the decoder to achieve deep supervision, as suggested in previous studies (<xref rid=\"B17\" ref-type=\"bibr\">Lee et al., 2015</xref>), with exponentially decayed weights (1/2<sup><italic toggle=\"yes\">i</italic></sup>) assigned to lower-resolution outputs. The AdamW optimizer with a weight decay of 0.05 was considered (<xref rid=\"B20\" ref-type=\"bibr\">Liu Y. et al., 2024</xref>). A cosine learning rate decay was adopted with an initial learning rate of 0.001.</p><p>Detailed model parameters of the CNN and Mamba blocks are reported in <xref rid=\"T3\" ref-type=\"table\">Table 3</xref>. Note that each CNN block is composed of a depthwise convolution and a pointwise convolution, and only the parameters for depthwise convolution are shown here. The pointwise convolution employed a fixed kernel (1,1,1) with stride (1,1,1) in all layers. For the MSMS-Mamba blocks, the key parameters, i.e., state dimension, convolutional kernel size, and expansion ratio were set as default configuration in Mamba (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>).</p><table-wrap position=\"float\" id=\"T3\" orientation=\"portrait\"><label>Table 3</label><caption><p>Detailed model parameters of the proposed architecture.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"3\" colspan=\"1\">\n<bold>Layer</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"6\" rowspan=\"1\">\n<bold>Encoder</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"3\" rowspan=\"1\">\n<bold>Decoder</bold>\n</th></tr><tr><th valign=\"top\" align=\"center\" colspan=\"3\" rowspan=\"1\">\n<bold>3D CNN</bold>\n<sup>*</sup>\n</th><th valign=\"top\" align=\"center\" colspan=\"3\" rowspan=\"1\">\n<bold>Mamba</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"3\" rowspan=\"1\">\n<bold>Transposed Conv</bold>\n</th></tr><tr><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Kernel</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Stride</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Channel</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<italic toggle=\"yes\">d</italic>\n<sub>state</sub>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<italic toggle=\"yes\">d</italic>\n<sub>conv</sub>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>ER</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Kernel</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Stride</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Channel</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(1,1,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2 &#8594; 32</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(1,1,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64 &#8594; 32</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">32 &#8594; 64</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(1,1,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">128 &#8594; 64</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">3</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64 &#8594; 128</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(1,1,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">256 &#8594; 128</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,3)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">128 &#8594; 256</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#8594; 256</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">5</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,3)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">256 &#8594; 320</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,2)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#8594; 320</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">6</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(3,3,3)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#8594; 320</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">(2,2,1)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">320 &#8594; 320</td></tr></tbody></table><table-wrap-foot><p><sup>*</sup>Each CNN block is composed of a depthwise convolution and a pointwise convolution, and only depthwise convolution parameters are shown here. The pointwise convolution uses a fixed kernel (1,1,1) with stride (1,1,1) in all layers. <italic toggle=\"yes\">d</italic><sub>state</sub>: state dimension; <italic toggle=\"yes\">d</italic><sub>conv</sub>: convolution kernel size; ER, Expansion ratio.</p></table-wrap-foot></table-wrap></sec></sec></sec><sec sec-type=\"results\" id=\"s3\"><label>3</label><title>Results</title><sec><label>3.1</label><title>Segmentation results on BraTS dataset</title><p><xref rid=\"F5\" ref-type=\"fig\">Figure 5</xref> shows an example of the segmentation results of the WT, TC, and ET on the BraTS dataset, produced by our model as well as several SOTA methods such as nnU-Net (<xref rid=\"B14\" ref-type=\"bibr\">Isensee et al., 2021</xref>), UMamba (<xref rid=\"B21\" ref-type=\"bibr\">Ma et al., 2024</xref>), and SegMamba (<xref rid=\"B36\" ref-type=\"bibr\">Xing et al., 2024</xref>), which are re-implemented in the present study based on the open-source code. All methods seem to produce very good segmentation results. However, the quantitative metrics over the entire testing set, showing in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>, suggest that our model outperforming nnU-Net, UMamba, and SegMamba except for Dice in TC and HD95 in ET.</p><fig position=\"float\" id=\"F5\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Example of the segmentation results on the BraTS dataset. Red, WT; Blue, TC; Green, ET.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0005.jpg\"><alt-text content-type=\"machine-generated\">Axial, coronal, and sagittal MRI slices showing brain tumor segmentations. Each row represents a different slice view. Columns display results from various segmentation methods: Ground Truth, Proposed, nnU-Net, nnU-Net + Transformer, SegMamba, and UMamba. Tumors are marked in colored regions, mainly red, green, and blue, indicating different tissue types or segmentation areas.</alt-text></graphic></fig><table-wrap position=\"float\" id=\"T4\" orientation=\"portrait\"><label>Table 4</label><caption><p>Segmentation results on the BraTS2023 dataset.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"2\" colspan=\"1\">\n<bold>Methods</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"2\" rowspan=\"1\">\n<bold>WT</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"2\" rowspan=\"1\">\n<bold>TC</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"2\" rowspan=\"1\">\n<bold>ET</bold>\n</th><th valign=\"top\" align=\"center\" colspan=\"2\" rowspan=\"1\">\n<bold>Average</bold>\n</th></tr><tr><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Dice (%)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>HD95 (mm)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Dice (%)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>HD95 (mm)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Dice (%)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>HD95 (mm)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Dice (%)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>HD95 (mm)</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SegresNet<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.02</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.07</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.10</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.08</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">83.66</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.88</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.26</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.01</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">UX-Net<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.13</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.56</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.03</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.68</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.91</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.19</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.69</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.81</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">MedNeXt<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.41</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.98</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.75</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.67</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">83.96</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.51</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.04</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.72</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">UNETR<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.19</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.17</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.39</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.29</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.48</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.03</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.68</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.49</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SwinUNETR<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.71</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.22</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.79</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.42</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.21</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.42</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.24</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.70</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SwinUNETR-V2<sup>&#8224;</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.35</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.01</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.65</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.41</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.17</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.41</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.39</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.51</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.45</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.34</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.93</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.30</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.59</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>3.74</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.99</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.79</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">UMamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.18</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.07</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.78</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.19</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.89</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.01</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.95</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.76</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SegMamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">93.30</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.12</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>92.49</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.14</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.21</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.91</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.00</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.72</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Proposed</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>93.56</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>4.06</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.45</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>3.06</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>88.58</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.83</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>91.53</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>3.65</bold>\n</td></tr></tbody></table><table-wrap-foot><p><sup>&#8224;</sup>Results reported in SegMamba (<xref rid=\"B36\" ref-type=\"bibr\">Xing et al., 2024</xref>). The bold values indicate the best performance.</p></table-wrap-foot></table-wrap><p>Besides, the performance of the proposed model is further compared with several other models that were employed as baseline models in a recent publication on the BraTS dataset (<xref rid=\"B36\" ref-type=\"bibr\">Xing et al., 2024</xref>), including SegresNet (<xref rid=\"B22\" ref-type=\"bibr\">Myronenko, 2019</xref>), UX-Net (<xref rid=\"B18\" ref-type=\"bibr\">Lee et al., 2023</xref>), MedNeXt (<xref rid=\"B26\" ref-type=\"bibr\">Roy et al., 2023</xref>), UNETR (<xref rid=\"B11\" ref-type=\"bibr\">Hatamizadeh et al., 2022</xref>), SwinUNETR (<xref rid=\"B10\" ref-type=\"bibr\">Hatamizadeh et al., 2021</xref>), and SwinUNETR-V2 (<xref rid=\"B12\" ref-type=\"bibr\">He et al., 2023</xref>). Note that, for these methods, no re-implementation is performed in the present study due to the lack of open-source code. Consequently, the results reported in <xref rid=\"B36\" ref-type=\"bibr\">Xing et al. (2024)</xref> are used in the present study. Besides, only Dice and HD95 are reported in <xref rid=\"B36\" ref-type=\"bibr\">Xing et al. (2024)</xref>. These two metrics are therefore considered as metrics of the BraTS dataset for all the methods listed in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>. It is clear that the performance of our method is superior to those baseline models.</p></sec><sec><label>3.2</label><title>Segmentation results on SWS dataset</title><p><xref rid=\"F6\" ref-type=\"fig\">Figure 6</xref> shows an representation example of the segmentation results on the SWS dataset, produced by the proposed methods as well as the comparison methods including nnU-Net, UMamba, and SegMamba (re-implemented in the present study). It can be observed that the proposed method produces more accurate and coherent LA segmentations, particularly in regions with irregular boundaries.</p><fig position=\"float\" id=\"F6\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Example of the segmentation results on the SWS dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0006.jpg\"><alt-text content-type=\"machine-generated\">Comparison of MRI brain slices with highlighted segments in red. Rows indicate different segmentation methods: Ground Truth, nnU-Net, nnU-Net with Transformer, UMamba, SegMamba, and Proposed. Each column shows a distinct slice.</alt-text></graphic></fig><p>We have employed Grad-CAM (<xref rid=\"B27\" ref-type=\"bibr\">Selvaraju et al., 2017</xref>) to visualize the feature activation maps. As shown in the <xref rid=\"F7\" ref-type=\"fig\">Figure 7</xref>, the proposed method exhibits the most distinct and concentrated response in the lesion regions, indicating strong capability for LA localization. In contrast, the nnU-Net and nnU-Net+Transformer models show relatively diffuse activations, while SegMamba and UMamba fail to accurately highlight the lesion areas. However, our model's stronger capability for LA localization may also lead to increased false positive rate in the worst case shown in <xref rid=\"F8\" ref-type=\"fig\">Figure 8</xref>, where the boundary between two different brain areas has similar characteristic as LA, i.e., highlighted (bright) pixels, and is therefore identified as LA area.</p><fig position=\"float\" id=\"F7\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Feature activation maps.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0007.jpg\"><alt-text content-type=\"machine-generated\">Six-panel image showing brain scan segmentation comparisons. Top left: Original image with a highlighted section in red. Top middle: nnU-Net result. Top right: nnU-Net with Transformer. Bottom left: Proposed method. Bottom middle: SegMamba result. Bottom right: UMamba result. Color scales are included next to each result for intensity reference.</alt-text></graphic></fig><fig position=\"float\" id=\"F8\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Worst case scenario. Red, Ground Truth; Green, Prediction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fnins-19-1699700-g0008.jpg\"><alt-text content-type=\"machine-generated\">Five brain MRI scans each labeled with different segmentation approaches: Proposed, nnU-Net, nnU-Net + Transformer, UMamba, and SegMamba. Colored regions indicate different segmentation results on each scan.</alt-text></graphic></fig><p>The quantitative metrics of the 5-fold cross validation are shown in <xref rid=\"T5\" ref-type=\"table\">Table 5</xref>. The proposed method achieves the highest mean Dice score of 78.67%, outperforming nnUNet, UMamba, and SegMamba by 1.87%, 2.24%, and 1.62%, respectively. Additionally, the proposed method yields the highest scores in all the other metrics except for specificity and HD95: i.e., sensitivity (78.57%), balanced accuracy (88.87%), accuracy (98.34%), Jaccard index (66.59%), Kappa coefficient (77.95%), volume similarity (92.14%) and Matthews correlation coefficient (78.38%). These results demonstrate the effectiveness of the proposed method in segmenting LA in the MRI images of SWS patients. The slightly lower specificity (99.16%) and HD95 (15.49) may be explained by the higher false positive rate of our model in the worst case scenario, as shown in <xref rid=\"F8\" ref-type=\"fig\">Figure 8</xref>.</p><table-wrap position=\"float\" id=\"T5\" orientation=\"portrait\"><label>Table 5</label><caption><p>Segmentation results on the SWS dataset.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Metric</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>nnU-Net</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>nnU-Net+T<sup>*</sup></bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>UMamba</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>SegMamba</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Proposed</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Reader-1</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Reader-2</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Sen (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.31 &#177; 14.58</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.23 &#177; 14.98</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.85 &#177; 15.33</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">74.79 &#177; 17.70</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>78.57</bold>\n<bold>&#177;14.80</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.38 &#177; 14.25</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.42 &#177; 14.12</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Spe (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.18 &#177; 0.61</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.19 &#177; 0.52</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.91 &#177; 1.09</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>99.29</bold>\n<bold>&#177;0.58</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.16 &#177; 0.65</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.90 &#177; 0.98</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">99.12 &#177; 1.33</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">BAcc (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.24 &#177; 7.23</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.21 &#177; 7.43</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.38 &#177; 7.51</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.04 &#177; 8.78</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>88.87</bold>\n<bold>&#177;7.30</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.64 &#177; 7.26</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.27 &#177; 6.91</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Acc (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.21 &#177; 1.02</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.26 &#177; 0.96</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.05 &#177; 1.23</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.27 &#177; 1.03</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>98.34</bold>\n<bold>&#177;0.95</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.70 &#177; 0.99</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.85 &#177; 1.36</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">JI (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64.01 &#177; 15.59</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64.32 &#177; 16.73</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">63.55 &#177; 15.57</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64.77 &#177; 16.65</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>66.59</bold>\n<bold>&#177;14.81</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">48.66 &#177; 18.77</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">51.41 &#177; 18.10</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Kappa (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.87 &#177; 13.73</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.92 &#177; 14.87</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.41 &#177; 14.07</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.14 &#177; 16.21</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>77.95</bold>\n<bold>&#177;13.12</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">62.41 &#177; 19.72</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">65.28 &#177; 17.96</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">VS (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.73 &#177; 9.48</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">91.79 &#177; 9.37</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.56 &#177; 9.47</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.31 &#177; 8.33</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>92.14</bold>\n<bold>&#177;7.86</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.63 &#177; 22.29</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.31 &#177; 20.98</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">MCC (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.53 &#177; 13.19</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.43 &#177; 14.41</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.10 &#177; 13.62</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.68 &#177; 16.00</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>78.38</bold>\n<bold>&#177;12.81</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">65.06 &#177; 17.46</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">67.54 &#177; 15.03</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">HD95 (mm)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">17.16 &#177; 35.93</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">17.27 &#177; 38.26</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">16.83 &#177; 36.84</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>12.83</bold>\n<bold>&#177;32.16</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">15.49 &#177; 35.39</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.81 &#177; 41.41</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">19.87 &#177; 25.38</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">Dice (%)</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.80 &#177; 13.74</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.83 &#177; 13.83</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.43 &#177; 14.03</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.05 &#177; 16.23</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>78.67</bold>\n<bold>&#177;13.11</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">62.94 &#177; 19.98</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">65.76 &#177; 18.02</td></tr></tbody></table><table-wrap-foot><p><sup>*</sup>nnU-Net+Transformer (Transformer is only used in the last layer of the encoder). The bold values indicate the best performance.</p></table-wrap-foot></table-wrap><p>Note that all the models yield very high specificity and accuracy. A high specificity indicates good ability of the models to detect negative samples (non-LA pixels). Besides, given the relatively low sensitivity (below 80%), the observed high accuracy reveals imbalanced distribution of the two classes of samples (LA and non-LA pixels). This is indeed the case of the present study. As shown in <xref rid=\"F6\" ref-type=\"fig\">Figure 6</xref>, non-LA areas in the MRI images are much larger than LA areas.</p><p>To further evaluate the performance of the proposed method for LA detection, two readers (neurologists) were asked to annotate the LA independently, and their results were compared with that of our model. As shown in <xref rid=\"T5\" ref-type=\"table\">Table 5</xref>, the proposed deep learning model outperforms the two readers in many evaluation metrics, except for sensitivity and accuracy where the readers achieve slightly higher values. These results suggest that while the readers may detect more true positive regions, our method provides more accurate and consistent segmentation results overall.</p></sec><sec><label>3.3</label><title>Ablation study</title><p>Ablation study was performed on the SWS dataset in the present study in order to demonstrate the effectiveness of the proposed MSMS architecture as well as the pre-training strategy. The results are reported in <xref rid=\"T6\" ref-type=\"table\">Table 6</xref>. All these models are embedded in the encoder-decoder architecture shown in <xref rid=\"F2\" ref-type=\"fig\">Figure 2</xref>. Considering only the original Mamba, a Dice score of 76.01% is observed for LA segmentation on the SWS dataset. By adding the multi-scaling (down-sampling) module to the original Mamba, a Dice score of 76.01% (with improvements of 0.89%) is achieved. A combination of the original Mamba and the multi-scanning module yields a Dice score of 75.90% (0.78% improvements compared to the original Mamba).</p><table-wrap position=\"float\" id=\"T6\" orientation=\"portrait\"><label>Table 6</label><caption><p>Results of ablation study on the SWS dataset.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Original Mamba</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Multi-scale</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Multi-scan</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Pre-training</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Dice (%)</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.12</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.01</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.90</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.43</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td rowspan=\"1\" colspan=\"1\"/><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.18</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>78.67</bold>\n</td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best performance.</p></table-wrap-foot></table-wrap><p>Furthermore, the original Mamba with pre-training on the BraTS dataset produces a Dice score of 76.43% on the SWS dataset, improving 1.31% compared to original Mamba only. Without pre-training, the combination of multi-scale and multi-scan strategies yields a Dice score of 77.18%. Finally, as reported in <xref rid=\"T6\" ref-type=\"table\">Table 6</xref>, the proposed method integrating multi-scale, multi-scan, and pre-training achieves the best performance, i.e, with a Dice score of 78.67%. These results demonstrate the effectiveness of the MSMS architecture and the pre-training strategy.</p></sec><sec><label>3.4</label><title>Computational complexity</title><p>The computational complexity of the proposed method as well as the comparison methods are reported in <xref rid=\"T7\" ref-type=\"table\">Table 7</xref>. The training and evaluation processes were executed on a high-performance computing node featuring NVIDIA GeForce RTX 3090 graphics processor with CUDA 11.8 acceleration. Due to the addition of Mamba module in each encoding layer, the number of parameters of the proposed method is higher than that of nnU-Net and nnU-Net+Transformer, resulting in slightly higher inference time (0.169 s). However, our model has the lowest Floating Point Operations (892.44 Giga). Notably, for nnU-Net+Transformer, our preliminary results indicate that incorporating a Transformer module at every encoder layer leads to out-of-memory issue. As a consequency, Transformer is only used in the last layer of the encoder, e.g., at the bottleneck.</p><table-wrap position=\"float\" id=\"T7\" orientation=\"portrait\"><label>Table 7</label><caption><p>Comparison of model's computational complexity.</p></caption><table frame=\"box\" rules=\"all\"><thead><tr><th valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Model</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Inference time<sup>&#8224;</sup> (s/subject)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Floating Point Operations (Giga)</bold>\n</th><th valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>Parameters (Million)</bold>\n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">nnU-Net</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.055 &#177; 0.001</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">928.51</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>361.04</bold>\n</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">nnU-Net + Transformer<sup>*</sup></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\"><bold>0.040</bold>\n<bold>&#177;0.023</bold></td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">928.95</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">376.61</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">UMamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.120 &#177; 0.001</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1210.00</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">728.69</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">SegMamba</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.172 &#177; 0.000</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">965.34</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">594.40</td></tr><tr><td valign=\"top\" align=\"left\" rowspan=\"1\" colspan=\"1\">\n<bold>Proposed</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.169 &#177; 0.000</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">\n<bold>892.44</bold>\n</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">570.28</td></tr></tbody></table><table-wrap-foot><p><sup><sup>&#8224;</sup></sup>Mean &#177; stanard deviation over 10 repeated tests;<sup><sup>*</sup></sup>Transformer is only used in the last layer of the encoder. The bold values indicate the best performance.</p></table-wrap-foot></table-wrap></sec></sec><sec sec-type=\"discussion\" id=\"s4\"><label>4</label><title>Discussion</title><p>The aim of the present study is to develop an artificial intelligence (AI) model for automatic identification of LA in the MRI images and thus for automated SWS diagnosis. Such model is of great clinical importance as, currently, the detection of LA in MRI for SWS diagnosis relies on manual inspection by neuroradiologists, which is not only time-consuming and experience-demanding but also lead to low inter-rater agreement. To this end, the encoder-decoder architecture particularly nnU-Net is employed as the basic framework of our model due to its excellent performance in many segmentation tasks (<xref rid=\"B14\" ref-type=\"bibr\">Isensee et al., 2021</xref>).</p><p>Different from traditional nnU-Net using CNNs in each encoding and decoding layer, Mamba is embedded in each encoding layer in the present study in order to capture long-range dependencies. Based on a selective SSM, Mamba has lower computational complexity as compared to conventional sequential modeling approaches such as RNN and Transformer (<xref rid=\"B8\" ref-type=\"bibr\">Gu and Dao, 2024</xref>). Besides, a novel MSMS strategy is proposed to convert the 3-D volumes into sequences as input of the SSM. The MSMS strategy integrates multi-resolution feature fusion with multi-scanning order, enabling robust global context modeling while maintaining linear complexity by explicitly decoupling spatial dependencies across reduced scales and different directions. This lies in the main novelty of the present study.</p><p>The ablation results shown in <xref rid=\"T6\" ref-type=\"table\">Table 6</xref> demonstrate the effectiveness of the MSMS strategy. Besides, the entire model proposed in the present study outperforms several SOTA models, including nnUNet, UMamba, and SegMamba, in both brain tumor segmentation on the BraTS dataset and LA segmentation on the SWS dataset. Our framework significantly enhances Mamba's feature extraction capability for 3D medical images through multi-scale and multi-scan hierarchical integration, which systematically aggregates discriminative features across varying receptive fields. However, note that the reproduced results for SegMamba on the BraTS dataset (<xref rid=\"T4\" ref-type=\"table\">Table 4</xref>) are slightly lower than that originally reported in <xref rid=\"B36\" ref-type=\"bibr\">Xing et al. (2024)</xref>, which may be due to possible difference in the implementation hardware between the present study and <xref rid=\"B36\" ref-type=\"bibr\">Xing et al. (2024)</xref>.</p><p>Furthermore, a main challenge for the application of deep learning to medical data is limited subject size, particularly for rare diseases such as SWS. To overcome this challenge, we introduce a transfer learning approach by pre-training the proposed model on the BraTS dataset and fine-tuning it on the SWS dataset. The ablation results shown in <xref rid=\"T6\" ref-type=\"table\">Table 6</xref> illustrate the effectiveness of the pre-training strategy. However, note that the characteristics of the brain tumor (gliomas) in the pre-training dataset (BraTS) differ significantly from that of the LA in the SWS dataset. It is therefore reasonable to expect improved results after pre-training on a dataset similar to SWS. On the other hand, our results may also suggest that pre-training on the BraTS dataset may benefit lesion segmentation in other rare brain diseases with limited dataset but similar imaging madality.</p><p>Interesting also to note that the MRI images of the 40 SWS patients are collected with different equipments over a time span of 16 years, which can be ascribed to the rareness of the disease. Nevertheless, with such diverges in the recording equipments and time span, the proposed method produces remarkable segmentation results for the LA, suggesting good generalization ability of the proposed method. In fact, the harmonization technique (<xref rid=\"B33\" ref-type=\"bibr\">Tan et al., 2023</xref>) implemented in the pre-processing may contribute significantly to such good generalization ability of the proposed method.</p><p>Our results show that the proposed deep learning method outperforms two neurologist in a number of metrics particularly Dice core, which has been widely employed as the primary metric in segmentation tasks. These results suggest that deep learning method may be an alternative to neurologist for the identification of LA and thus the diagnosis of SWS. In fact, The proposed method can either be integrated in a MRI machine or work off-line on a personal computer, indicating the feasibility of auto-segmentation of LA for SWS diagnosis in clinical practice.</p><p>Interesting to note that while successfully locating LAs, our model produces higher false positive rate, leading to over-diagnosis in clinical settings. In general, false positives may be more preferable than false negatives in clinical practice, as false negatives yield missed diagnosis but false positives can be excluded by post screening of neuroradiologists. Although neuroradiologists is involved for post screening in case of false positives, the working load is significantly reduced by using our model as a pre-screening tool.</p><p>Note also that fully supervised learning is employed in the present study in order to train the coefficients of the deep learning network. The performance of a supervised deep learning model relies strongly on the ground truth. In the present study, annotating the abnormal intracranial vessels in the MRI image is quite difficult and therefore not considered. Instead, the brain area containing the LA is annotated. Besides, the annotation is performed by only one neuroradiologist, limiting the accuracy of the ground truth and therefore the performance of the proposed model for LA segmentation. On the one hand, this may partially explain the observed lowered segmentation results on the SWS dataset as compared with that on the BraTS dataset. On the other hand, self-supervised learning (<xref rid=\"B16\" ref-type=\"bibr\">Krishnan et al., 2022</xref>) or few-shot learning (<xref rid=\"B30\" ref-type=\"bibr\">Song et al., 2023</xref>) with less dependence on the ground truth annotation may be an interesting direction for future studies on LA segmentation for SWS diagnosis.</p></sec><sec sec-type=\"conclusion\" id=\"s5\"><label>5</label><title>Conclusion</title><p>In the present study, we propose a MSMS-based Mamba, embedded into an encoder-decoder architecture, for the identification of LA in the brain MRI images and thus for automated diagnosis of SWS. A pre-training strategy is employed to overcome the challenges introduced by small subject size. Our results show promising segmentation performance of the proposed model on the public BraTS dataset as well as our 40 SWS dataset, outperforming several SOTA models and two independent readers. These findings suggest the feasibility of using AI tool for automatic identification of LA, providing useful information for clinical SWS diagnosis that is currently performed by time-consuming manual inspection. Future studies may focus on self-supervised learning and/or few-shot learning that depend less on the ground truth in order to reduce the requirement of accurate annotations.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI) and MoE Key Laboratory of Intelligence Perception and Human-Machine Collaboration (KLIP-HuMaCo) for their support of research resources.</p></ack><fn-group><fn fn-type=\"edited-by\" id=\"fn0001\"><p>Edited by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/237737/overview\" ext-link-type=\"uri\">Tie-Qiang Li</ext-link>, Karolinska University Hospital, Sweden</p></fn><fn fn-type=\"reviewed-by\" id=\"fn0002\"><p>Reviewed by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/2919119/overview\" ext-link-type=\"uri\">Santosh I. Gore</ext-link>, Sai Info Solution, India</p><p><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3213843/overview\" ext-link-type=\"uri\">GongPeng Cao</ext-link>, Beijing University of Posts and Telecommunications (BUPT), China</p></fn></fn-group><sec sec-type=\"data-availability\" id=\"s6\"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors under reasonable request.</p></sec><sec sec-type=\"ethics-statement\" id=\"s7\"><title>Ethics statement</title><p>The studies involving humans were approved by the Ethics Committee at Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine. The studies were conducted in accordance with the local legislation and institutional requirements. The ethics committee/institutional review board waived the requirement of written informed consent for participation from the participants or the participants' legal guardians/next of kin because this study is a retrospective study using existing data recorded during regular clinical practice.</p></sec><sec sec-type=\"author-contributions\" id=\"s8\"><title>Author contributions</title><p>WB: Validation, Data curation, Writing &#8211; original draft, Methodology. CX: Methodology, Software, Writing &#8211; original draft, Visualization. RS: Writing &#8211; review &amp; editing, Supervision. XH: Visualization, Methodology, Writing &#8211; review &amp; editing. YL: Validation, Writing &#8211; review &amp; editing, Formal analysis. XW: Data curation, Writing &#8211; review &amp; editing, Resources. TT: Supervision, Investigation, Writing &#8211; review &amp; editing. DH: Supervision, Project administration, Writing &#8211; review &amp; editing, Conceptualization, Resources, Investigation. LX: Investigation, Conceptualization, Writing &#8211; review &amp; editing, Funding acquisition, Supervision, Formal analysis, Project administration, Resources.</p></sec><sec sec-type=\"COI-statement\" id=\"conf1\"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type=\"ai-statement\" id=\"s10\"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p><p>Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us.</p></sec><sec sec-type=\"disclaimer\" id=\"s11\"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id=\"B1\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bar</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Pedespan</surname><given-names>J.-M.</given-names></name><name name-style=\"western\"><surname>Boccara</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Garcelon</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Levy</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Gr&#233;vent</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Early magnetic resonance imaging to detect presymptomatic leptomeningeal angioma in children with suspected sturge-weber syndrome</article-title>. <source>Dev. Med. Child Neurol</source>. <volume>62</volume>, <fpage>227</fpage>&#8211;<lpage>233</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1111/dmcn.14253</pub-id><pub-id pub-id-type=\"pmid\">31050360</pub-id></mixed-citation></ref><ref id=\"B2\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Tan</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>H.</given-names></name></person-group> (<year>2025</year>). <article-title>Mlg: A mixed local and global model for brain tumor classification</article-title>. <source>Front. Neurosci</source>. <volume>19</volume>:<fpage>1618514</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fnins.2025.1618514</pub-id><pub-id pub-id-type=\"pmid\">40678755</pub-id><pub-id pub-id-type=\"pmcid\">PMC12267167</pub-id></mixed-citation></ref><ref id=\"B3\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chicco</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Jurman</surname><given-names>G.</given-names></name></person-group> (<year>2020</year>). <article-title>The advantages of the matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>. <source>BMC Genomics</source><volume>21</volume>:<fpage>6</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1186/s12864-019-6413-7</pub-id><pub-id pub-id-type=\"pmid\">31898477</pub-id><pub-id pub-id-type=\"pmcid\">PMC6941312</pub-id></mixed-citation></ref><ref id=\"B4\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chmura Kraemer</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Periyakoil</surname><given-names>V. S.</given-names></name><name name-style=\"western\"><surname>Noda</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Kappa coefficients in medical research</article-title>. <source>Stat. Med</source>. <volume>21</volume>, <fpage>2109</fpage>&#8211;<lpage>2129</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1002/sim.1180</pub-id><pub-id pub-id-type=\"pmid\">12111890</pub-id></mixed-citation></ref><ref id=\"B5\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>&#199;i&#231;ek</surname><given-names>&#214;.</given-names></name><name name-style=\"western\"><surname>Abdulkadir</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Lienkamp</surname><given-names>S. S.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group> (<year>2016</year>). <article-title>&#8220;3D U-Net: learning dense volumetric segmentation from sparse annotation,&#8221;</article-title> in <source>Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Athens</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>424</fpage>&#8211;<lpage>432</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-319-46723-8_49</pub-id></mixed-citation></ref><ref id=\"B6\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Comi</surname><given-names>A. M.</given-names></name></person-group> (<year>2007</year>). <article-title>Update on sturge-weber syndrome: diagnosis, treatment, quantitative measures, and controversies</article-title>. <source>Lymphat. Res. Biol</source>. <volume>5</volume>, <fpage>257</fpage>&#8211;<lpage>264</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1089/lrb.2007.1016</pub-id><pub-id pub-id-type=\"pmid\">18370916</pub-id></mixed-citation></ref><ref id=\"B7\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#8220;An image is worth 16x16 words: transformers for image recognition at scale,&#8221;</article-title> in <source>International Conference on Learning Representations</source>, <fpage>611</fpage>&#8211;<lpage>631</lpage>.</mixed-citation></ref><ref id=\"B8\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name></person-group> (<year>2024</year>). <article-title>&#8220;Mamba: linear-time sequence modeling with selective state spaces,&#8221;</article-title> in <source>First Conference on Language Modeling</source> (<publisher-loc>Philadelphia, PA</publisher-loc>), <fpage>1</fpage>&#8211;<lpage>32</lpage>.</mixed-citation></ref><ref id=\"B9\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Goel</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Re</surname><given-names>C.</given-names></name></person-group> (<year>2022</year>). <article-title>&#8220;Efficiently modeling long sequences with structured state spaces,&#8221;</article-title> in <source>International Conference on Learning Representations</source>, <fpage>14323</fpage>&#8211;<lpage>14343</lpage>.</mixed-citation></ref><ref id=\"B10\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H. R.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>&#8220;Swin unetr: Swin transformers for semantic segmentation of brain tumors in MRI images,&#8221;</article-title> in <source>International MICCAI Brainlesion Workshop</source> (<publisher-loc>Springer</publisher-loc>), <fpage>272</fpage>&#8211;<lpage>284</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-031-08999-2_22</pub-id></mixed-citation></ref><ref id=\"B11\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>&#8220;Unetr: transformers for 3d medical image segmentation,&#8221;</article-title> in <source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source> (<publisher-loc>Waikoloa, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>574</fpage>&#8211;<lpage>584</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/WACV51458.2022.00181</pub-id></mixed-citation></ref><ref id=\"B12\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Nath</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group> (<year>2023</year>). <article-title>&#8220;Swinunetr-v2: Stronger swin transformers with stagewise convolutions for 3d medical image segmentation,&#8221;</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Vancouver, BC</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>416</fpage>&#8211;<lpage>426</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-031-43901-8_40</pub-id></mixed-citation></ref><ref id=\"B13\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huttenlocher</surname><given-names>D. P.</given-names></name><name name-style=\"western\"><surname>Klanderman</surname><given-names>G. A.</given-names></name><name name-style=\"western\"><surname>Rucklidge</surname><given-names>W. J.</given-names></name></person-group> (<year>1993</year>). <article-title>Comparing images using the Hausdorff distance</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <volume>15</volume>, <fpage>850</fpage>&#8211;<lpage>863</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/34.232073</pub-id></mixed-citation></ref><ref id=\"B14\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Jaeger</surname><given-names>P. F.</given-names></name><name name-style=\"western\"><surname>Kohl</surname><given-names>S. A.</given-names></name><name name-style=\"western\"><surname>Petersen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Maier-Hein</surname><given-names>K. H.</given-names></name></person-group> (<year>2021</year>). <article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>. <source>Nat. Methods</source><volume>18</volume>, <fpage>203</fpage>&#8211;<lpage>211</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type=\"pmid\">33288961</pub-id></mixed-citation></ref><ref id=\"B15\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kazerooni</surname><given-names>A. F.</given-names></name><name name-style=\"western\"><surname>Khalili</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Haldar</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Anwar</surname><given-names>S. M.</given-names></name><etal/></person-group>. (<year>2024</year>). <article-title>The brain tumor segmentation (brats) challenge 2023: focus on pediatrics (cbtn-connect-dipgr-asnr-miccai brats-peds)</article-title>. <source>ArXiv:2305.17033</source>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2305.17033</pub-id>37292481\n</mixed-citation></ref><ref id=\"B16\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Krishnan</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Rajpurkar</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Topol</surname><given-names>E. J.</given-names></name></person-group> (<year>2022</year>). <article-title>Self-supervised learning in medicine and healthcare</article-title>. <source>Nat. Biomed. Eng</source>. <volume>6</volume>, <fpage>1346</fpage>&#8211;<lpage>1352</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1038/s41551-022-00914-1</pub-id><pub-id pub-id-type=\"pmid\">35953649</pub-id></mixed-citation></ref><ref id=\"B17\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lee</surname><given-names>C.-Y.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gallagher</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Tu</surname><given-names>Z.</given-names></name></person-group> (<year>2015</year>). <article-title>&#8220;Deeply-supervised nets,&#8221;</article-title> in <source>Artificial Intelligence and Statistics</source> (<publisher-loc>Lille</publisher-loc>: <publisher-name>Pmlr</publisher-name>), <fpage>562</fpage>&#8211;<lpage>570</lpage>.</mixed-citation></ref><ref id=\"B18\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lee</surname><given-names>H. H.</given-names></name><name name-style=\"western\"><surname>Bao</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Huo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B. A.</given-names></name></person-group> (<year>2023</year>). <article-title>&#8220;3D UX-Net: a large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation,&#8221;</article-title> in <source>International Conference on Learning Representations</source> (<publisher-loc>Kigali</publisher-loc>), <fpage>21891</fpage>&#8211;<lpage>21905</lpage>.</mixed-citation></ref><ref id=\"B19\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>H.-Y.</given-names></name><name name-style=\"western\"><surname>Xi</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2024</year>). <article-title>&#8220;Swin-umamba: Mamba-based unet with imagenet-based pretraining,&#8221;</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Marrakesh</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>615</fpage>&#8211;<lpage>625</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-031-72114-4_59</pub-id></mixed-citation></ref><ref id=\"B20\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2024</year>). <article-title>Vmamba: visual state space model</article-title>. <source>Adv. Neural Inform. Process. Syst</source>. <volume>37</volume>, <fpage>103031</fpage>&#8211;<lpage>103063</lpage>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.10166</pub-id></mixed-citation></ref><ref id=\"B21\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name></person-group> (<year>2024</year>). <article-title>U-Mamba: enhancing long-range dependency for biomedical image segmentation</article-title>. <source>arXiv:2401.04722</source>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.04722</pub-id></mixed-citation></ref><ref id=\"B22\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Myronenko</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>&#8220;3D MRI brain tumor segmentation using autoencoder regularization,&#8221;</article-title> in <source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4</source> (<publisher-loc>Granada</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>311</fpage>&#8211;<lpage>320</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-030-11726-9_28</pub-id></mixed-citation></ref><ref id=\"B23\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Raghu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Kornblith</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). <article-title>Do vision transformers see like convolutional neural networks?</article-title><source>Adv. Neural Inform. Process. Syst</source>. <volume>34</volume>, <fpage>12116</fpage>&#8211;<lpage>12128</lpage>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2108.08810</pub-id></mixed-citation></ref><ref id=\"B24\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ramirez</surname><given-names>E. L.</given-names></name><name name-style=\"western\"><surname>J&#252;lich</surname><given-names>K.</given-names></name></person-group> (<year>2024</year>). <article-title>&#8220;Sturge-weber syndrome: an overview of history, genetics, clinical manifestations, and management,&#8221;</article-title> in <source>Seminars in Pediatric Neurology, Volume 51</source> (<publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Elsevier</publisher-name>), <fpage>101151</fpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.spen.2024.101151</pub-id><pub-id pub-id-type=\"pmid\">39389653</pub-id></mixed-citation></ref><ref id=\"B25\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>&#8220;U-Net: convolutional networks for biomedical image segmentation,&#8221;</article-title> in <source>Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Munich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>234</fpage>&#8211;<lpage>241</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation></ref><ref id=\"B26\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roy</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Koehler</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Ulrich</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Baumgartner</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Petersen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Isensee</surname><given-names>F.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>&#8220;Mednext: transformer-driven scaling of convnets for medical image segmentation,&#8221;</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Vancouver, BC</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>405</fpage>&#8211;<lpage>415</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-031-43901-8_39</pub-id></mixed-citation></ref><ref id=\"B27\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Selvaraju</surname><given-names>R. R.</given-names></name><name name-style=\"western\"><surname>Cogswell</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Das</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Vedantam</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Parikh</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Batra</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>&#8220;Grad-cam: visual explanations from deep networks via gradient-based localization,&#8221;</article-title> in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Venice</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>618</fpage>&#8211;<lpage>626</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/ICCV.2017.74</pub-id></mixed-citation></ref><ref id=\"B28\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C.</given-names></name></person-group> (<year>2024</year>). <article-title>Multi-scale vmamba: hierarchy in hierarchy visual state space model</article-title>. <source>Adv. Neural Inform. Process. Syst</source>. <volume>37</volume>, <fpage>25687</fpage>&#8211;<lpage>25708</lpage>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.2405.14174</pub-id></mixed-citation></ref><ref id=\"B29\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Soh</surname><given-names>W. K.</given-names></name><name name-style=\"western\"><surname>Rajapakse</surname><given-names>J. C.</given-names></name></person-group> (<year>2023</year>). <article-title>Hybrid unet transformer architecture for ischemic stoke segmentation with mri and ct datasets</article-title>. <source>Front. Neurosci</source>. <volume>17</volume>:<fpage>1298514</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3389/fnins.2023.1298514</pub-id><pub-id pub-id-type=\"pmid\">38105927</pub-id><pub-id pub-id-type=\"pmcid\">PMC10723803</pub-id></mixed-citation></ref><ref id=\"B30\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Cai</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Mondal</surname><given-names>S. K.</given-names></name><name name-style=\"western\"><surname>Sahoo</surname><given-names>J. P.</given-names></name></person-group> (<year>2023</year>). <article-title>A comprehensive survey of few-shot learning: evolution, applications, challenges, and opportunities</article-title>. <source>ACM Comput. Surv</source>. <volume>55</volume>, <fpage>1</fpage>&#8211;<lpage>40</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1145/3582688</pub-id></mixed-citation></ref><ref id=\"B31\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sudarsanam</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Ardern-Holmes</surname><given-names>S. L.</given-names></name></person-group> (<year>2014</year>). <article-title>Sturge-weber syndrome: from the past to the present</article-title>. <source>Eur. J. Paediatr. Neurol</source>. <volume>18</volume>, <fpage>257</fpage>&#8211;<lpage>266</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.ejpn.2013.10.003</pub-id><pub-id pub-id-type=\"pmid\">24275166</pub-id></mixed-citation></ref><ref id=\"B32\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sujansky</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Conradi</surname><given-names>S.</given-names></name></person-group> (<year>1995</year>). <article-title>Outcome of sturge-weber syndrome in 52 adults</article-title>. <source>Am. J. Med. Genet</source>. <volume>57</volume>, <fpage>35</fpage>&#8211;<lpage>45</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1002/ajmg.1320570110</pub-id><pub-id pub-id-type=\"pmid\">7645596</pub-id></mixed-citation></ref><ref id=\"B33\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tan</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Tegzes</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>T&#246;r&#246;k</surname><given-names>L. I.</given-names></name><name name-style=\"western\"><surname>Ferenczi</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Avinash</surname><given-names>G. B.</given-names></name><name name-style=\"western\"><surname>Rusk&#243;</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2023</year>). <source>Image Harmonization for Deep Learning Model Optimization. Technical Report</source>. US Patent 11669945.</mixed-citation></ref><ref id=\"B34\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Thomas-Sohl</surname><given-names>K. A.</given-names></name><name name-style=\"western\"><surname>Vaslow</surname><given-names>D. F.</given-names></name><name name-style=\"western\"><surname>Maria</surname><given-names>B. L.</given-names></name></person-group> (<year>2004</year>). <article-title>Sturge-weber syndrome: a review</article-title>. <source>Pediatr. Neurol</source>. <volume>30</volume>, <fpage>303</fpage>&#8211;<lpage>310</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.pediatrneurol.2003.12.015</pub-id><pub-id pub-id-type=\"pmid\">15165630</pub-id></mixed-citation></ref><ref id=\"B35\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ulyanov</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Vedaldi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Lempitsky</surname><given-names>V.</given-names></name></person-group> (<year>2016</year>). <article-title>Instance normalization: the missing ingredient for fast stylization</article-title>. <source>arXiv:1607.08022</source>. doi: <pub-id pub-id-type=\"doi\">10.48550/arXiv.1607.08022</pub-id></mixed-citation></ref><ref id=\"B36\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xing</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name></person-group> (<year>2024</year>). <article-title>&#8220;Segmamba: long-range sequential modeling mamba for 3d medical image segmentation,&#8221;</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Marrakesh</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>578</fpage>&#8211;<lpage>588</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1007/978-3-031-72111-3_54</pub-id></mixed-citation></ref><ref id=\"B37\"><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). <article-title>&#8220;Reluplex made more practical: leaky ReLU,&#8221;</article-title> in <source>2020 IEEE Symposium on Computers and communications (ISCC)</source> (<publisher-loc>Rennes</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>&#8211;<lpage>7</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/ISCC50000.2020.9219587</pub-id></mixed-citation></ref><ref id=\"B38\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yushkevich</surname><given-names>P. A.</given-names></name><name name-style=\"western\"><surname>Piven</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Hazlett</surname><given-names>H. C.</given-names></name><name name-style=\"western\"><surname>Smith</surname><given-names>R. G.</given-names></name><name name-style=\"western\"><surname>Ho</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Gee</surname><given-names>J. C.</given-names></name><etal/></person-group>. (<year>2006</year>). <article-title>User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title>. <source>Neuroimage</source><volume>31</volume>, <fpage>1116</fpage>&#8211;<lpage>1128</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1016/j.neuroimage.2006.01.015</pub-id><pub-id pub-id-type=\"pmid\">16545965</pub-id></mixed-citation></ref><ref id=\"B39\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Depthwise separable convolution neural network for high-speed sar ship detection</article-title>. <source>Remote Sens</source>. <volume>11</volume>:<fpage>2483</fpage>. doi: <pub-id pub-id-type=\"doi\">10.3390/rs11212483</pub-id></mixed-citation></ref><ref id=\"B40\"><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group> (<year>2025</year>). <article-title>Visually stabilized mamba U-shaped network with strong inductive bias for 3-D brain tumor segmentation</article-title>. <source>IEEE Trans. Instrument. Measur</source>. <volume>74</volume>, <fpage>1</fpage>&#8211;<lpage>11</lpage>. doi: <pub-id pub-id-type=\"doi\">10.1109/TIM.2025.3551581</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Front Neurosci Front Neurosci 670 frontneurosci Front. Neurosci. Frontiers in Neuroscience 1662-4548 1662-453X Frontiers Media SA PMC12669175 PMC12669175.1 12669175 12669175 41341263 10.3389/fnins.2025.1699700 1 Original Research Detection of leptomeningeal angiomas in brain MRI of Sturge-Weber syndrome using multi-scale multi-scan Mamba Bao Weiqun 1 &#8224; Validation Data curation Writing &#8211; original draft Methodology Xue Chenghao 2 &#8224; Methodology Software Writing &#8211; original draft Visualization Su Ruisheng 3 Writing &#8211; review &amp; editing Supervision Hu Xindan 2 Visualization Methodology Writing &#8211; review &amp; editing Li Yuanning 4 Validation Writing &#8211; review &amp; editing Formal analysis Wang Xiaoqiang 5 * Data curation Writing &#8211; review &amp; editing Resources Tan Tao 6 * Supervision Investigation Writing &#8211; review &amp; editing He Dake 1 * Supervision Project administration Writing &#8211; review &amp; editing Conceptualization Resources Investigation Xu Lin 2 * Investigation Conceptualization Writing &#8211; review &amp; editing Funding acquisition Supervision Formal analysis Project administration Resources 1 Department of Pediatric Neurology, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine , Shanghai , China 2 School of Information Science and Technology, ShanghaiTech University , Shanghai , China 3 Department of Biomedical Engineering, Eindhoven University of Technology , Eindhoven , Netherlands 4 School of Biomedical Engineering, ShanghaiTech University , Shanghai , China 5 Department of Pediatric Neurosurgery, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine , Shanghai , China 6 Faculty of Applied Sciences, Macao Polytechnic University , Macao, Macao SAR , China * Correspondence: Lin Xu, xulin1@shanghaitech.edu.cn ; Xiaoqiang Wang, Wangxiaoqiang419@163.com ; Tao Tan, taotanjs@gmail.com ; Dake He, hedake@139.com &#8224; These authors have contributed equally to this work 18 11 2025 2025 19 480891 1699700 05 9 2025 27 10 2025 18 11 2025 03 12 2025 04 12 2025 Copyright &#169; 2025 Bao, Xue, Su, Hu, Li, Wang, Tan, He and Xu. 2025 Bao, Xue, Su, Hu, Li, Wang, Tan, He and Xu https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY) . The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Objectives Sturge-Weber syndrome (SWS) is a congenital neurological disorder occurring in the early childhood. Timely diagnosis of SWS is essential for proper medical intervention that prevents the development of various neurological issues. Leptomeningeal angiomas (LA) are the clinical manifestation of SWS. Detection of LA is currently performed by manual inspection of the magnetic resonance images (MRI) by experienced neurologist, which is time-consuming and lack of inter-rater consistency. The aim of the present study is to investigate automated LA detection in MRI of SWS patients. Methods A Mamba-based encoder-decoder architecture was employed in the present study. Particularly, a multi-scale multi-scan strategy was proposed to convert 3-D volume into 1-D sequence, enabling capturing long-range dependency with reduced computation complexity. Our dataset consists of 40 SWS patients with T1-enhanced MRI. The proposed model was first pre-trained on a public brain tumor segmentation (BraTS) dataset and then fine-tuned and tested on the SWS dataset using 5-fold cross validation. Results and conclusion Our results show excellent performance of the proposed method, e.g., Dice score of 91.53% and 78.67% for BraTS and SWS, respectively, outperforming several state-of-the-art methods as well as two neurologists. Mamba-based deep learning method can automatically identify LA in MRI images, enabling automated SWS diagnosis in clinical settings. Sturge-Weber syndrome leptomeningeal angiomas magnetic resonance imaging Mamba multi-scale multi-scan The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported in part by the National Natural Science Foundation of China under Grant 62171284. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Brain Imaging Methods 1 Introduction Sturge-Weber syndrome (SWS) is a rare congenital neurological disorder with an incidence rate of approximately 1 in 20,000 to 50,000 live births ( Comi, 2007 ). It is often associated with facial port-wine stains (PWS), glaucoma, and ipsilateral leptomeningeal angiomas (LA) ( Sudarsanam and Ardern-Holmes, 2014 ; Thomas-Sohl et al., 2004 ). SWS can lead to various neurological issues such as seizures, hemiparesis, headaches, and cognitive impairments, particularly in children under five years ( Sujansky and Conradi, 1995 ). Given the high susceptibility to seizures in this age group, early detection of SWS and proper medical intervention are crucial. Although facial PWS is a major characteristic of SWS, not everyone with facial PWS suffers from SWS ( Thomas-Sohl et al., 2004 ). Therefore, it is recommended that children with PWS undergo imaging evaluation, such as magnetic resonance imaging (MRI), to determine whether they have SWS ( Bar et al., 2020 ). Clinical manifestation of SWS is the presence of intracranial vascular anomaly, i.e., LA ( Ramirez and J&#252;lich, 2024 ). Currently, the identification of LA relies on visual inspection of the imaging data, particularly MRI, by experienced clinicians. However, the expertise of experienced doctors may not always be readily available, and manual labeling of lesions is time-consuming. Therefore, an automatic diagnosis tool is required for accurate and efficient diagnosis of SWS. With the advance in deep learning, encoder-decoder-based frameworks such as U-Net and its variants have become the dominant architectures for medical image segmentation ( Ronneberger et al., 2015 ; &#199;i&#231;ek et al., 2016 ; Soh and Rajapakse, 2023 ; Chen et al., 2025 ), playing a key role in computer-aided diagnosis (CAD) systems. U-Net primarily utilizes the encoder-decoder structure with convolutional neural networks (CNNs) in each layer to extract hierarchical image features, enabling precise segmentation of the image in pixel level ( &#199;i&#231;ek et al., 2016 ). Moreover, nnU-Net automates hyperparameter tuning and data preprocessing for U-Net, and therefore enhances the performance in many segmentation tasks ( Isensee et al., 2021 ). However, while CNNs are efficient in feature extraction, they are constrained by their local receptive fields, making it difficult to capture long-range dependencies that are crucial for tasks where global context is essential ( Soh and Rajapakse, 2023 ). In addition to CNNs, Transformer has emerged as a powerful alternative, demonstrating exceptional ability to model global relationships and capture long-range dependencies ( Dosovitskiy et al., 2021 ; Raghu et al., 2021 ). Vision Transformer(ViT) employs the transformer architecture for image processing, capturing global dependencies in images through self-attention mechanisms. Nevertheless, their quadratic complexity associated with the length of the input sequence renders them computationally intensive and impractical for handling high-dimensional medical images ( Gu and Dao, 2024 ). This computational burden hinders their applicability in real-world settings with limited resources. Mamba has been recently proposed to capture long-range dependence with reduced computational burden. It is built on the state space models (SSMs) with optimized state space matrices ( Gu and Dao, 2024 ), and has shown promising performance in natural language processing and also been adapted for computer vision tasks ( Zhu et al., 2025 ). However, as the SSMs work initially on 1-D sequence, adapting Mamba for image tasks requires to convert the 2- or 3-D images into 1-D sequence. Several strategies have been proposed for such adaptation. U-Mamba flattens image patches using a original scan order, i.e., row by row sequentially ( Ma et al., 2024 ). VMamba introduces a cross-scan module to decompose 2-D images into four distinct 1-D scanning paths (horizontal, vertical, and their reverse directions) to better preserve spatial relationships ( Liu Y. et al., 2024 ). Swin-UMamba integrates a shifted window partitioning strategy (inspired by Swin Transformers) with VMamba's directional scanning, while also leveraging ImageNet pre-training for enhanced feature representation (Liu et al., J. 2024 ). In addition, SegMamba proposes a tri-orientated scanning scheme for 3-D images, i.e., scanning along axial, sagittal, and coronal planes separately to capture volumetric dependencies ( Xing et al., 2024 ). Despite their wide application, deep-learning-based CAD techniques have not yet been applied to the diagnosis of SWS. The aim of the present study is therefore to develop a deep-learning-based tool for automatic detection of LA in the brain MRI of SWS patients. The study is designed as a retrospective investigation of a clinical dataset consisting of 40 SWS patients. The encoder-decoder structure and Mamba model are considered. In order to deal with the random location of LAs in MRI scans, a novel multi-scale multi-scan (MSMS) strategy is proposed to convert 3-D volumes into 1-D sequences. Besides, due to the rareness of the disease and thus limited sample size, transfer learning is considered by pre-training the model with a public dataset, i.e, the brain tumor segmentation (BraTS) benchmark dataset, and then fun-tuning and testing it on our SWS dataset. The performance of the proposed method is compared with CNNs, Transformers, and state-of-the-art (SOTA) Mamba-based models in terms of a number of evaluation metrics. The main novelty and contributions of the present study are summarized hereafter. An encoder-decoder deep learning method was employed, for the first time, for automatic identification of LA in the brain MRI images, and thus for automated diagnosis of SWS. Mamba model is embedded in the encoder in order to capture long-rang dependency in 3-D volumes. A multi-scale multi-scan strategy is proposed in the Mamba model to extract multi-scale futures along different directions of the 3-D volume. Pre-training on the BraTS dataset is employed to address the challenge of limited sample size. 2 Material and methods 2.1 Dataset and pre-processing This study was designed as a retrospective study of existing data collected during clinical practice at Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine (XH-SJTU-SM). Patients diagnosed as SWS between June 2008 and August 2024 were considered. The exclusion criteria include: (a) younger than 3 months; (b) syndrome type II, i.e., without LA; (c) missing clinical information or poor-quality images impeding manual annotation. Consequently, 40 SWS patients (13 girls and 27 boys, age between 5-696 months) were involved in this study. The study protocol was approved by the Ethics Committee at XH-SJTU-SM with the number XHEC-D-2024-090. Written informed consent of each subject was waived by the ethics committee due to its retrospective nature. T1-weighted MRI images were analyzed in the present study. They were recorded using different scanners, i.e., Siemens, Philips, and General Electric (GE), with two different magnetic field strengths, i.e., 1.5T and 3T. Details of the scanners and their parameters are summarized in Table 1 . In each MRI image, the brain areas containing LA were annotated as the ground truth by one experienced neuroradiologist from XH-SJTU-SM using the well-known ITK-SNAP toolbox ( Yushkevich et al., 2006 ). An representative examples of LA in T1-enhanced MRI and the corresponding annotations is shown in Figure 1 . Note that marking only the abnormal intracranial vessels in the MRI image was quite difficult and therefore was not considered in the present study. Table 1 MRI scanners and corresponding parameters. Scanner NOP TR/TE FOV MZ ST Si 1.5T 3 1,600/8.9 200 &#215; 200 256 &#215; 180 5 Ph 3T 5 1,800/20 230 &#215; 187 260 &#215; 189 5 GE 3T 17 1,750/24 200 &#215; 200 320 &#215; 224 5 GE 1.5T 15 520/9.9 240 &#215; 240 320 &#215; 160 5 Si, Siemens; Ph, Philips; NOP, number of patients; TR, repetition time (ms); TE, echo time (ms); FOV, Filed of view (mm 2 ); MZ, Matrix size (pixel 2 ); ST, Slice thickness (mm). Figure 1 LA in T1-enhanced brain MRI of the SWS patients and the corresponding annotations. T1-enhanced MRI images of the brain in four columns. The top row shows grayscale MRI scans, while the bottom row displays the same scans with red annotations highlighting specific regions of interest in the brain. In each MRI image, the skull was first removed using the FSL toolbox ( Yushkevich et al., 2006 ) to avoid its affects on the identification of contrast-enhanced LA. Then the images recorded with different scanners and protocols were resampled to uniform physical spacing of 0.47 mm &#215; 0.47 mm &#215; 6.5 mm. In addition, each image was cropped to contain only the brain area. Due to different brain sizes of the subjects, the largest size of the cropped image was determined as 320 pixels &#215; 320 pixels, and all cropped images were zero-padded to this unique size to keep consistency for further analysis. Furthermore, a harmonization technique ( Tan et al., 2023 ) was applied to the cropped images to compensate the influence of different scanners and imaging parameters. To this end, a high-quality MRI scan was first selected among all the images as a reference image ( I ref ) based on visual inspection. Then each brain image I was smoothed using k Gaussian kernels with different standard deviations: I ( k ) = G &#963; k * I , (1) where G &#963; k denotes the k th Gaussian kernel, and &#963; k = 0, 2, 4, 6, for k = 0, 1, 2, 3, respectively. Note that &#963; 0 = 0 corresponds to the original (non-smoothed) image. Each smoothed image I ( k ) was then normalized with respected to the reference image, given by I &#732; ( k ) = I ( k ) &#8722; &#956; ( k ) &#963; ( k ) &#183; &#963; ref ( k ) + &#956; ref ( k ) , (2) where &#956; and &#963; indicates respectively mean and standard deviation, and &#956; ref ( k ) , &#963; ref ( k ) are computed from the smoothed reference image I ref ( k ) calculated as I ref ( k ) = G &#963; k * I ref . Finally, a harmonized image was obtained as I harm = &#8721; k = 0 3 I &#732; ( k ) . (3) 2.2 Mamba-based segmentation framework 2.2.1 Overall framework The overall architecture of the proposed segmentation framework is illustrated in Figure 2 . It is based on the U-Net structure with six encoding and decoding layers. In the encoder, each layer consists of two 3D CNN modules and a multi-scale multi-scan (MSMS) Mamba block in between. The first CNN is used to extract 3-D features from the input volume. It is composed of a depthwise convolution followed by a pointwise convolution for efficient channel-wise feature mixing ( Zhang et al., 2019 ), followed by an instance normalization ( Ulyanov et al., 2016 ), and a Leaky ReLU activation function ( Xu et al., 2020 ). The MSMS-Mamba block is connected to the output of the residual CNN to enhance the model's ability to capture long-range dependencies and spatial correlations. The second 3D CNN is employed as an efficient alternative to traditional max-pooling to progressively down-sample the feature map in each encoding layer. Figure 2 The overall architecture of the proposed segmentation framework. Diagram showing a 3D convolutional neural network architecture for image segmentation. It begins with a 3D input, processed through MSMS-Mamba modules and blocks, layer normalization, and convolution feed-forward networks (ConvFFN). The Mamba Encoder and Decoder units are used, along with skip connections. The network outputs a segmented 3D image. In the decoder, each layer comprises a 3D CNN module and an up-sampling module, i.e., transposed 3D convolution. The 3D CNN module is the same as the encoder. The up-sampling module leverages 3D transposed convolutions to progressively restore the spatial resolution of feature maps. At the end of the encoder, i.e., the final layer, a segmentation head with 1 7 1 convolution is employed to perform pixel-wise classification, producing the final segmentation output. Besides, following established practices in medical image segmentation ( Isensee et al., 2021 ; Hatamizadeh et al., 2021 ), skip connections are introduced between corresponding encoding and decoding layers to retain spatial detail and low-level features. The main novelty and contribution of the present work lies in the integration of the MSMS-Mamba block in the encoder to enhance long-range dependency modeling and improve feature fusion. The scheme of the MSMS-Mamba block is shown in the left of Figure 2 . It is composed of four consecutive components: an initial layer normalization, the MSMS-Mamba module for long-range dependency and multi-scale modeling, a second layer normalization, and a convolutional feed-forward network ( Shi et al., 2024 ) for feature refinement. Among the four components, the MSMS-Mamba module is the key of the MSMS-Mamba block, whose scheme is shown in Figure 3 . It is built based on a SSM model with dedicated MSMS strategy, as shown in Figure 4 . Details of the SSM model and the MSMS strategy are introduced hereafter. Figure 3 MSMS-Mamba module. Flowchart depicting a neural network architecture. It starts with two parallel linear projection layers, followed by a SiLU activation function. One branch includes a convolution layer before the SiLU function. Both paths merge into a block labeled &#8220;SSM with MSMS&#8221;, followed by another linear projection layer and SiLU. The flow terminates with a final linear projection. Figure 4 SSM with multi-scale multi-scan strategy. Diagram showing a process flow with subsampling, multi-scan, multi-merge, flatten, and reshape steps for data in colored matrices. Subsampling is applied to inputs (SV1 to SV8), processed by SSM blocks, merged, and fused into a final structure. 2.2.2 Selective state space model SSM is a class of sequence-to-sequence architecture that has undergone significant evolution in recent years. The development of SSMs ( Gu et al., 2022 ) originates from their foundational formulation as a linear time-invariant (LTI) system, which maps an input sequence x ( t ) &#8712; &#8477; L to an output sequence y ( t ) &#8712; &#8477; L through a hidden state h ( t ) &#8712; &#8450; N . The continuous form of SSMs is defined as: h &#8242; ( t ) = A h ( t ) + B x ( t ) , &#160; y ( t ) = C h ( t ) , (4) where A &#8712; &#8450; N &#215; N , B &#8712; &#8450; N , and C &#8712; &#8450; N are the system matrices, and h ( t ) &#8712; &#8477; N represents the implicit latent state. To make SSMs computationally tractable for discrete-time sequences, the bilinear transform method is employed to discretize the continuous ordinary differential equations. This results in the discrete form of SSMs, h [ n ] = A &#175; h [ n - 1 ] + B &#175; x [ n ] , y [ n ] = C &#175; h [ n ] , (5) where A &#175; = ( I - &#916; / 2 &#183; A ) - 1 ( I + &#916; / 2 &#183; A ) , B &#175; = ( I - &#916; / 2 &#183; A ) - 1 &#916; B , and C &#175; = C . Here, &#916; denotes the step size for converting a continuous sequence into a discrete sequence ( Gu et al., 2022 ). The discrete SSM can be further expressed in a convolutional form, shown as y [ n ] = C A &#175; n B &#175; x [ 0 ] + C A &#175; n - 1 B &#175; x [ 1 ] + &#8230; &#8195;&#8195; + C A &#175; B &#175; x [ n - 1 ] + C B &#175; x [ n ] , &#8195; K &#175; = ( C B &#175; , C A &#175; B &#175; , &#8230; , C A &#175; n - 1 B &#175; , C A &#175; n B &#175; ) , &#8195; y = K &#175; * x . (6) The matrices A &#175; , B &#175; , and C &#175; remain fixed within every iteration. This allows pre-computing the convolution kernel K &#175; , significantly speeding up the training stage, as reported in Gu et al. (2022) . SSMs are the core of a Mamba model. However, the pre-computing of K &#175; results in a static representation, where the same matrices are applied to all tokens regardless of the input content, limiting the model's ability to perform context-aware reasoning. To address this limitation, the Mamba model ( Gu and Dao, 2024 ) introduces a selective mechanism to SSMs, making the system matrices input-dependent. This is achieved by constraining the system matrixes B , C , and &#916; as linear projection of the input sequence x &#8712; &#8477; d using a weight matrix W &#8712; &#8477; d &#215; N . This makes B , C , and &#916; input-dependent. Consequently, the input dependency of &#916; causes A &#175; to also become input-dependent, rendering the pre-computed K &#175; inapplicable. To address this, a parallel scanning algorithm is proposed in Mamba for efficient computation of these matrices ( Gu and Dao, 2024 ). 2.2.3 Multi-scale multi-scan strategy Mamba is initially designed for processing 1-D series, but can be adapted for image processing tasks by serializing the 2-D or 3-D image into a 1-D sequence through flattening operation. However, directly applying Mamba to flattened 3-D medical images leads to limited receptive fields, hindering the model's ability to effectively capture spatial correlations in higher-dimensional data. This limitation becomes particularly evident in 3-D medical imaging, where preserving spatial structure is essential for accurate analysis. To address this challenges, we propose a MSMS strategy to convert our 3-D MRI data into 1D sequence, which is designed as a dual-branch structure in order to extract multi-scale features, as shown in Figure 4 . The upper branch performs feature extraction across different directions through a combination of sub-sampling and multi-scan operations. Meanwhile, the parallel lower branch maintains the original resolution to preserve fine-grained spatial information. In the upper branch, sub-sampling is first employed to divide the input 3D volume V into several smaller sub-volumes (SVs) in order to alleviate long-range forgetting ( Shi et al., 2024 ). To this end, the 3D volume V is uniformly partitioned into N segments along each axis, resulting in N 3 basic elements: V = { x i ; y j ; z k } i , j , k = 1 N . In the present study, N = 4 thus 64 basic elements are obtained. Then 8 SVs can be derived, with each consisting of 8 basic elements sampled from the entire volume (two elements from each axis), as shown in Table 2 . For instance, the SV 1 in Table 2 is sampled from { x 1 , x 3 }, { y 1 , y 3 }, and { z 1 , z 3 }, denoted as SV_1 = V { x 1 , x 3 ; y 1 , y 3 ; z 1 , z 3 } . (7) Each SV is numbered in the same way as shown in Figure 4 . Different scanning orders along different spatial directions are then employed to flatten each SV into 1-D sequence, as listed in Table 2 . The multi-scan operation is designed to extract multi-scale features from each SV while effectively expanding the receptive field. Instead of bidirectional scanning, only unidirectional scanning for each SV is considered to achieve a trade-off between segmentation performance and computational complexity. However, different scanning orders are employed for different SVs, equalling to multi-directional scanning of the original volume with reduced spatial sample rate. Such diverse scanning patterns across different SVs enable complementary coverage among sequences, facilitating joint expansion of context without increasing computational cost. SSM is then applied to each flattened 1-D sequence, which is followed by the multi-merge operation, an inverse operation of multi-scan, reconstructs the 3-D volume based on the output of SSMs. Table 2 Sub-volumes obtained by down-sampling and the corresponding scan order. SV Sub-sample Scan order SV_1 V { x 1 , x 3 ; y 1 , y 3 ; z 1 , z 3 } [1, 2, 3, 4, 5, 6, 7, 8] SV_2 V { x 2 , x 4 ; y 1 , y 3 ; z 1 , z 3 } [8, 7, 6, 5, 4, 3, 2, 1] SV_3 V { x 1 , x 3 ; y 2 , y 4 ; z 1 , z 3 } [1, 3, 2, 4, 5, 7, 6, 8] SV_4 V { x 2 , x 4 ; y 2 , y 4 ; z 1 , z 3 } [8, 6, 7, 5, 4, 2, 3, 1] SV_5 V { x 1 , x 3 ; y 1 , y 3 ; z 2 , z 4 } [1, 5, 2, 6, 3, 7, 4, 8] SV_6 V { x 2 , x 4 ; y 1 , y 3 ; z 2 , z 4 } [8, 4, 7, 3, 6, 2, 5, 1] SV_7 V { x 1 , x 3 ; y 2 , y 4 ; z 2 , z 4 } [1, 2, 4, 3, 5, 6, 8, 7] SV_8 V { x 2 , x 4 ; y 2 , y 4 ; z 2 , z 4 } [7, 8, 6, 5, 3, 4, 2, 1] 2.3 Evaluation 2.3.1 Evaluation on public BraTS dataset The performance of the proposed segmentation framework was first evaluated on a public dataset from the BraTS2023 Challenge ( Kazerooni et al., 2024 ), which includes 1251 3D brain MRI images with gliomas. This dataset was acquired from multiple institutions under standard clinical conditions using four modalities (T1, T1Gd, T2, and T2-FLAIR). Whole tumor (WT), tumor core (TC), and enhancing tumor (ET) were manually annotated by experienced medical experts. Only T1-enhanced images were considered, the same as our SWS dataset. We adopted a 7:1:2 split for training, validation, and testing, and trained the model for 1,000 epochs to ensure efficient learning and reliable performance evaluation. 2.3.2 Evaluation on SWS dataset with pre-training strategy While evaluating the model on the SWS dataset, a pre-training strategy was considered due to the limited subject size. In this scenario, the entire BraTS2023 dataset ( Kazerooni et al., 2024 ) was adopted to pre-train the model for 1,000 epochs. Although gliomas in BraTS differ morphologically from SWS lesions, both share common segmentation principles&#8212;particularly the enhanced intensity patterns in T1-weighted images. Thus, pre-training on BraTS enables the model to leverage relevant priors and helps alleviate the limitations imposed by the small scale of the SWS dataset. Given the fact that the spatial sampling distance of the SWS dataset in the z-axis is significantly larger than that of the x- and y-axis, down-sampling in the z-axis of the BraTS dataset was performed as it is originally equally sampled in all directions. After pre-training, the obtained model was fine-tuned and tested on the SWS dataset for 250 epochs, which is smaller than that of the BraTS dataset (1,000) due to the much smaller size of the SWS dataset. We utilized a 5-fold cross-validation strategy to evaluate the performance of the proposed method. 2.3.3 Performance metrics To quantitatively evaluate the segmentation performance of the proposed model on both datasets, the widely adopted dice similarity (DS) coefficient was employed as the primary metric, which measures the spatial overlap between model predictions and ground truth annotations. This metric has become the de facto standard in medical image segmentation due to its robustness in assessing volumetric agreement. However, note that in the present study the SWS dataset are severely imbalanced, i.e., much smaller LA regions as compared the non-LA regions. Such imbalance may lead to a low Dice score. Therefore, other metrics such as sensitivity (Sen), specificity (Spe), balanced accuracy (BAcc), accuracy (Acc), Jaccard Index (JI) ( Ronneberger et al., 2015 ), Kappa coefficient ( Chmura Kraemer et al., 2002 ), volumetric similarity (VS), Matthews correlation coefficient (MCC) ( Chicco and Jurman, 2020 ), and 95% Hausdorff distance (HD95) ( Huttenlocher et al., 1993 ) were also calculated to measure the similarity between the segmented tumor and the ground truth in order to achieve a comprehensive understanding of the model performance. 2.3.4 Implementation details The PyTorch deep learning framework was utilized in our implementation. The training and evaluation processes were executed on a high-performance computing node featuring NVIDIA GeForce RTX 3090 graphics processor with CUDA 11.8 acceleration. We have implemented MSMSMamba on top of the well-established nnU-Net framework ( Isensee et al., 2021 ). Its self-configuring feature has allowed us to focus on network design rather than other trivial details. The loss function was a combination of Dice loss and cross-entropy loss, defined as L total = 1 2 &#183; L CE + 1 2 &#183; L Dice . (8) The Cross Entropy Loss &#8466; CE and Dice Loss &#8466; Dice are respectively defined as L CE = - 1 N &#8721; i = 1 N [ y i log ( p i ) + ( 1 - y i ) log ( 1 - p i ) ] , (9) and L Dice = 1 - 2 &#8721; i = 1 N p i y i + &#1013; &#8721; i = 1 N p i + &#8721; i = 1 N y i + &#1013; , (10) where N is the number of samples (or pixels), y i &#8712; {0, 1} denotes the ground truth label of the i -th sample, p i &#8712; [0, 1] denotes the predicted probability for the positive class, and &#1013; is a small constant (e.g., 10 &#8722;6 ) to prevent division by zero. The combined loss was supervised at each layer of the decoder to achieve deep supervision, as suggested in previous studies ( Lee et al., 2015 ), with exponentially decayed weights (1/2 i ) assigned to lower-resolution outputs. The AdamW optimizer with a weight decay of 0.05 was considered ( Liu Y. et al., 2024 ). A cosine learning rate decay was adopted with an initial learning rate of 0.001. Detailed model parameters of the CNN and Mamba blocks are reported in Table 3 . Note that each CNN block is composed of a depthwise convolution and a pointwise convolution, and only the parameters for depthwise convolution are shown here. The pointwise convolution employed a fixed kernel (1,1,1) with stride (1,1,1) in all layers. For the MSMS-Mamba blocks, the key parameters, i.e., state dimension, convolutional kernel size, and expansion ratio were set as default configuration in Mamba ( Gu and Dao, 2024 ). Table 3 Detailed model parameters of the proposed architecture. Layer Encoder Decoder 3D CNN * Mamba Transposed Conv Kernel Stride Channel d state d conv ER Kernel Stride Channel 1 (3,3,1) (1,1,1) 2 &#8594; 32 16 4 2 (2,2,1) (1,1,1) 64 &#8594; 32 2 (3,3,1) (2,2,1) 32 &#8594; 64 16 4 2 (2,2,1) (1,1,1) 128 &#8594; 64 3 (3,3,1) (2,2,1) 64 &#8594; 128 16 4 2 (2,2,1) (1,1,1) 256 &#8594; 128 4 (3,3,3) (2,2,2) 128 &#8594; 256 16 4 2 (2,2,2) (2,2,2) 320 &#8594; 256 5 (3,3,3) (2,2,2) 256 &#8594; 320 16 4 2 (2,2,2) (2,2,2) 320 &#8594; 320 6 (3,3,3) (2,2,1) 320 &#8594; 320 16 4 2 (2,2,1) (2,2,1) 320 &#8594; 320 * Each CNN block is composed of a depthwise convolution and a pointwise convolution, and only depthwise convolution parameters are shown here. The pointwise convolution uses a fixed kernel (1,1,1) with stride (1,1,1) in all layers. d state : state dimension; d conv : convolution kernel size; ER, Expansion ratio. 3 Results 3.1 Segmentation results on BraTS dataset Figure 5 shows an example of the segmentation results of the WT, TC, and ET on the BraTS dataset, produced by our model as well as several SOTA methods such as nnU-Net ( Isensee et al., 2021 ), UMamba ( Ma et al., 2024 ), and SegMamba ( Xing et al., 2024 ), which are re-implemented in the present study based on the open-source code. All methods seem to produce very good segmentation results. However, the quantitative metrics over the entire testing set, showing in Table 4 , suggest that our model outperforming nnU-Net, UMamba, and SegMamba except for Dice in TC and HD95 in ET. Figure 5 Example of the segmentation results on the BraTS dataset. Red, WT; Blue, TC; Green, ET. Axial, coronal, and sagittal MRI slices showing brain tumor segmentations. Each row represents a different slice view. Columns display results from various segmentation methods: Ground Truth, Proposed, nnU-Net, nnU-Net + Transformer, SegMamba, and UMamba. Tumors are marked in colored regions, mainly red, green, and blue, indicating different tissue types or segmentation areas. Table 4 Segmentation results on the BraTS2023 dataset. Methods WT TC ET Average Dice (%) HD95 (mm) Dice (%) HD95 (mm) Dice (%) HD95 (mm) Dice (%) HD95 (mm) SegresNet &#8224; 92.02 4.07 89.10 4.08 83.66 3.88 88.26 4.01 UX-Net &#8224; 93.13 4.56 90.03 5.68 85.91 4.19 89.69 4.81 MedNeXt &#8224; 92.41 4.98 87.75 4.67 83.96 4.51 88.04 4.72 UNETR &#8224; 92.19 6.17 86.39 5.29 84.48 5.03 87.68 5.49 SwinUNETR &#8224; 92.71 5.22 87.79 4.42 84.21 4.42 88.24 4.70 SwinUNETR-V2 &#8224; 93.35 5.01 89.65 4.41 85.17 4.41 89.39 4.51 nnU-Net 93.45 4.34 91.93 3.30 87.59 3.74 90.99 3.79 UMamba 93.18 4.07 91.78 3.19 87.89 4.01 90.95 3.76 SegMamba 93.30 4.12 92.49 3.14 87.21 3.91 91.00 3.72 Proposed 93.56 4.06 92.45 3.06 88.58 3.83 91.53 3.65 &#8224; Results reported in SegMamba ( Xing et al., 2024 ). The bold values indicate the best performance. Besides, the performance of the proposed model is further compared with several other models that were employed as baseline models in a recent publication on the BraTS dataset ( Xing et al., 2024 ), including SegresNet ( Myronenko, 2019 ), UX-Net ( Lee et al., 2023 ), MedNeXt ( Roy et al., 2023 ), UNETR ( Hatamizadeh et al., 2022 ), SwinUNETR ( Hatamizadeh et al., 2021 ), and SwinUNETR-V2 ( He et al., 2023 ). Note that, for these methods, no re-implementation is performed in the present study due to the lack of open-source code. Consequently, the results reported in Xing et al. (2024) are used in the present study. Besides, only Dice and HD95 are reported in Xing et al. (2024) . These two metrics are therefore considered as metrics of the BraTS dataset for all the methods listed in Table 4 . It is clear that the performance of our method is superior to those baseline models. 3.2 Segmentation results on SWS dataset Figure 6 shows an representation example of the segmentation results on the SWS dataset, produced by the proposed methods as well as the comparison methods including nnU-Net, UMamba, and SegMamba (re-implemented in the present study). It can be observed that the proposed method produces more accurate and coherent LA segmentations, particularly in regions with irregular boundaries. Figure 6 Example of the segmentation results on the SWS dataset. Comparison of MRI brain slices with highlighted segments in red. Rows indicate different segmentation methods: Ground Truth, nnU-Net, nnU-Net with Transformer, UMamba, SegMamba, and Proposed. Each column shows a distinct slice. We have employed Grad-CAM ( Selvaraju et al., 2017 ) to visualize the feature activation maps. As shown in the Figure 7 , the proposed method exhibits the most distinct and concentrated response in the lesion regions, indicating strong capability for LA localization. In contrast, the nnU-Net and nnU-Net+Transformer models show relatively diffuse activations, while SegMamba and UMamba fail to accurately highlight the lesion areas. However, our model's stronger capability for LA localization may also lead to increased false positive rate in the worst case shown in Figure 8 , where the boundary between two different brain areas has similar characteristic as LA, i.e., highlighted (bright) pixels, and is therefore identified as LA area. Figure 7 Feature activation maps. Six-panel image showing brain scan segmentation comparisons. Top left: Original image with a highlighted section in red. Top middle: nnU-Net result. Top right: nnU-Net with Transformer. Bottom left: Proposed method. Bottom middle: SegMamba result. Bottom right: UMamba result. Color scales are included next to each result for intensity reference. Figure 8 Worst case scenario. Red, Ground Truth; Green, Prediction. Five brain MRI scans each labeled with different segmentation approaches: Proposed, nnU-Net, nnU-Net + Transformer, UMamba, and SegMamba. Colored regions indicate different segmentation results on each scan. The quantitative metrics of the 5-fold cross validation are shown in Table 5 . The proposed method achieves the highest mean Dice score of 78.67%, outperforming nnUNet, UMamba, and SegMamba by 1.87%, 2.24%, and 1.62%, respectively. Additionally, the proposed method yields the highest scores in all the other metrics except for specificity and HD95: i.e., sensitivity (78.57%), balanced accuracy (88.87%), accuracy (98.34%), Jaccard index (66.59%), Kappa coefficient (77.95%), volume similarity (92.14%) and Matthews correlation coefficient (78.38%). These results demonstrate the effectiveness of the proposed method in segmenting LA in the MRI images of SWS patients. The slightly lower specificity (99.16%) and HD95 (15.49) may be explained by the higher false positive rate of our model in the worst case scenario, as shown in Figure 8 . Table 5 Segmentation results on the SWS dataset. Metric nnU-Net nnU-Net+T * UMamba SegMamba Proposed Reader-1 Reader-2 Sen (%) 77.31 &#177; 14.58 77.23 &#177; 14.98 77.85 &#177; 15.33 74.79 &#177; 17.70 78.57 &#177;14.80 82.38 &#177; 14.25 79.42 &#177; 14.12 Spe (%) 99.18 &#177; 0.61 99.19 &#177; 0.52 98.91 &#177; 1.09 99.29 &#177;0.58 99.16 &#177; 0.65 98.90 &#177; 0.98 99.12 &#177; 1.33 BAcc (%) 88.24 &#177; 7.23 88.21 &#177; 7.43 88.38 &#177; 7.51 87.04 &#177; 8.78 88.87 &#177;7.30 90.64 &#177; 7.26 89.27 &#177; 6.91 Acc (%) 98.21 &#177; 1.02 98.26 &#177; 0.96 98.05 &#177; 1.23 98.27 &#177; 1.03 98.34 &#177;0.95 98.70 &#177; 0.99 98.85 &#177; 1.36 JI (%) 64.01 &#177; 15.59 64.32 &#177; 16.73 63.55 &#177; 15.57 64.77 &#177; 16.65 66.59 &#177;14.81 48.66 &#177; 18.77 51.41 &#177; 18.10 Kappa (%) 75.87 &#177; 13.73 75.92 &#177; 14.87 75.41 &#177; 14.07 76.14 &#177; 16.21 77.95 &#177;13.12 62.41 &#177; 19.72 65.28 &#177; 17.96 VS (%) 89.73 &#177; 9.48 91.79 &#177; 9.37 89.56 &#177; 9.47 90.31 &#177; 8.33 92.14 &#177;7.86 73.63 &#177; 22.29 79.31 &#177; 20.98 MCC (%) 76.53 &#177; 13.19 76.43 &#177; 14.41 76.10 &#177; 13.62 76.68 &#177; 16.00 78.38 &#177;12.81 65.06 &#177; 17.46 67.54 &#177; 15.03 HD95 (mm) 17.16 &#177; 35.93 17.27 &#177; 38.26 16.83 &#177; 36.84 12.83 &#177;32.16 15.49 &#177; 35.39 27.81 &#177; 41.41 19.87 &#177; 25.38 Dice (%) 76.80 &#177; 13.74 76.83 &#177; 13.83 76.43 &#177; 14.03 77.05 &#177; 16.23 78.67 &#177;13.11 62.94 &#177; 19.98 65.76 &#177; 18.02 * nnU-Net+Transformer (Transformer is only used in the last layer of the encoder). The bold values indicate the best performance. Note that all the models yield very high specificity and accuracy. A high specificity indicates good ability of the models to detect negative samples (non-LA pixels). Besides, given the relatively low sensitivity (below 80%), the observed high accuracy reveals imbalanced distribution of the two classes of samples (LA and non-LA pixels). This is indeed the case of the present study. As shown in Figure 6 , non-LA areas in the MRI images are much larger than LA areas. To further evaluate the performance of the proposed method for LA detection, two readers (neurologists) were asked to annotate the LA independently, and their results were compared with that of our model. As shown in Table 5 , the proposed deep learning model outperforms the two readers in many evaluation metrics, except for sensitivity and accuracy where the readers achieve slightly higher values. These results suggest that while the readers may detect more true positive regions, our method provides more accurate and consistent segmentation results overall. 3.3 Ablation study Ablation study was performed on the SWS dataset in the present study in order to demonstrate the effectiveness of the proposed MSMS architecture as well as the pre-training strategy. The results are reported in Table 6 . All these models are embedded in the encoder-decoder architecture shown in Figure 2 . Considering only the original Mamba, a Dice score of 76.01% is observed for LA segmentation on the SWS dataset. By adding the multi-scaling (down-sampling) module to the original Mamba, a Dice score of 76.01% (with improvements of 0.89%) is achieved. A combination of the original Mamba and the multi-scanning module yields a Dice score of 75.90% (0.78% improvements compared to the original Mamba). Table 6 Results of ablation study on the SWS dataset. Original Mamba Multi-scale Multi-scan Pre-training Dice (%) &#10003; 75.12 &#10003; &#10003; 76.01 &#10003; &#10003; 75.90 &#10003; &#10003; 76.43 &#10003; &#10003; &#10003; 77.18 &#10003; &#10003; &#10003; &#10003; 78.67 The bold values indicate the best performance. Furthermore, the original Mamba with pre-training on the BraTS dataset produces a Dice score of 76.43% on the SWS dataset, improving 1.31% compared to original Mamba only. Without pre-training, the combination of multi-scale and multi-scan strategies yields a Dice score of 77.18%. Finally, as reported in Table 6 , the proposed method integrating multi-scale, multi-scan, and pre-training achieves the best performance, i.e, with a Dice score of 78.67%. These results demonstrate the effectiveness of the MSMS architecture and the pre-training strategy. 3.4 Computational complexity The computational complexity of the proposed method as well as the comparison methods are reported in Table 7 . The training and evaluation processes were executed on a high-performance computing node featuring NVIDIA GeForce RTX 3090 graphics processor with CUDA 11.8 acceleration. Due to the addition of Mamba module in each encoding layer, the number of parameters of the proposed method is higher than that of nnU-Net and nnU-Net+Transformer, resulting in slightly higher inference time (0.169 s). However, our model has the lowest Floating Point Operations (892.44 Giga). Notably, for nnU-Net+Transformer, our preliminary results indicate that incorporating a Transformer module at every encoder layer leads to out-of-memory issue. As a consequency, Transformer is only used in the last layer of the encoder, e.g., at the bottleneck. Table 7 Comparison of model's computational complexity. Model Inference time &#8224; (s/subject) Floating Point Operations (Giga) Parameters (Million) nnU-Net 0.055 &#177; 0.001 928.51 361.04 nnU-Net + Transformer * 0.040 &#177;0.023 928.95 376.61 UMamba 0.120 &#177; 0.001 1210.00 728.69 SegMamba 0.172 &#177; 0.000 965.34 594.40 Proposed 0.169 &#177; 0.000 892.44 570.28 &#8224; Mean &#177; stanard deviation over 10 repeated tests; * Transformer is only used in the last layer of the encoder. The bold values indicate the best performance. 4 Discussion The aim of the present study is to develop an artificial intelligence (AI) model for automatic identification of LA in the MRI images and thus for automated SWS diagnosis. Such model is of great clinical importance as, currently, the detection of LA in MRI for SWS diagnosis relies on manual inspection by neuroradiologists, which is not only time-consuming and experience-demanding but also lead to low inter-rater agreement. To this end, the encoder-decoder architecture particularly nnU-Net is employed as the basic framework of our model due to its excellent performance in many segmentation tasks ( Isensee et al., 2021 ). Different from traditional nnU-Net using CNNs in each encoding and decoding layer, Mamba is embedded in each encoding layer in the present study in order to capture long-range dependencies. Based on a selective SSM, Mamba has lower computational complexity as compared to conventional sequential modeling approaches such as RNN and Transformer ( Gu and Dao, 2024 ). Besides, a novel MSMS strategy is proposed to convert the 3-D volumes into sequences as input of the SSM. The MSMS strategy integrates multi-resolution feature fusion with multi-scanning order, enabling robust global context modeling while maintaining linear complexity by explicitly decoupling spatial dependencies across reduced scales and different directions. This lies in the main novelty of the present study. The ablation results shown in Table 6 demonstrate the effectiveness of the MSMS strategy. Besides, the entire model proposed in the present study outperforms several SOTA models, including nnUNet, UMamba, and SegMamba, in both brain tumor segmentation on the BraTS dataset and LA segmentation on the SWS dataset. Our framework significantly enhances Mamba's feature extraction capability for 3D medical images through multi-scale and multi-scan hierarchical integration, which systematically aggregates discriminative features across varying receptive fields. However, note that the reproduced results for SegMamba on the BraTS dataset ( Table 4 ) are slightly lower than that originally reported in Xing et al. (2024) , which may be due to possible difference in the implementation hardware between the present study and Xing et al. (2024) . Furthermore, a main challenge for the application of deep learning to medical data is limited subject size, particularly for rare diseases such as SWS. To overcome this challenge, we introduce a transfer learning approach by pre-training the proposed model on the BraTS dataset and fine-tuning it on the SWS dataset. The ablation results shown in Table 6 illustrate the effectiveness of the pre-training strategy. However, note that the characteristics of the brain tumor (gliomas) in the pre-training dataset (BraTS) differ significantly from that of the LA in the SWS dataset. It is therefore reasonable to expect improved results after pre-training on a dataset similar to SWS. On the other hand, our results may also suggest that pre-training on the BraTS dataset may benefit lesion segmentation in other rare brain diseases with limited dataset but similar imaging madality. Interesting also to note that the MRI images of the 40 SWS patients are collected with different equipments over a time span of 16 years, which can be ascribed to the rareness of the disease. Nevertheless, with such diverges in the recording equipments and time span, the proposed method produces remarkable segmentation results for the LA, suggesting good generalization ability of the proposed method. In fact, the harmonization technique ( Tan et al., 2023 ) implemented in the pre-processing may contribute significantly to such good generalization ability of the proposed method. Our results show that the proposed deep learning method outperforms two neurologist in a number of metrics particularly Dice core, which has been widely employed as the primary metric in segmentation tasks. These results suggest that deep learning method may be an alternative to neurologist for the identification of LA and thus the diagnosis of SWS. In fact, The proposed method can either be integrated in a MRI machine or work off-line on a personal computer, indicating the feasibility of auto-segmentation of LA for SWS diagnosis in clinical practice. Interesting to note that while successfully locating LAs, our model produces higher false positive rate, leading to over-diagnosis in clinical settings. In general, false positives may be more preferable than false negatives in clinical practice, as false negatives yield missed diagnosis but false positives can be excluded by post screening of neuroradiologists. Although neuroradiologists is involved for post screening in case of false positives, the working load is significantly reduced by using our model as a pre-screening tool. Note also that fully supervised learning is employed in the present study in order to train the coefficients of the deep learning network. The performance of a supervised deep learning model relies strongly on the ground truth. In the present study, annotating the abnormal intracranial vessels in the MRI image is quite difficult and therefore not considered. Instead, the brain area containing the LA is annotated. Besides, the annotation is performed by only one neuroradiologist, limiting the accuracy of the ground truth and therefore the performance of the proposed model for LA segmentation. On the one hand, this may partially explain the observed lowered segmentation results on the SWS dataset as compared with that on the BraTS dataset. On the other hand, self-supervised learning ( Krishnan et al., 2022 ) or few-shot learning ( Song et al., 2023 ) with less dependence on the ground truth annotation may be an interesting direction for future studies on LA segmentation for SWS diagnosis. 5 Conclusion In the present study, we propose a MSMS-based Mamba, embedded into an encoder-decoder architecture, for the identification of LA in the brain MRI images and thus for automated diagnosis of SWS. A pre-training strategy is employed to overcome the challenges introduced by small subject size. Our results show promising segmentation performance of the proposed model on the public BraTS dataset as well as our 40 SWS dataset, outperforming several SOTA models and two independent readers. These findings suggest the feasibility of using AI tool for automatic identification of LA, providing useful information for clinical SWS diagnosis that is currently performed by time-consuming manual inspection. Future studies may focus on self-supervised learning and/or few-shot learning that depend less on the ground truth in order to reduce the requirement of accurate annotations. Acknowledgments The authors would like to thank Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI) and MoE Key Laboratory of Intelligence Perception and Human-Machine Collaboration (KLIP-HuMaCo) for their support of research resources. Edited by: Tie-Qiang Li , Karolinska University Hospital, Sweden Reviewed by: Santosh I. Gore , Sai Info Solution, India GongPeng Cao , Beijing University of Posts and Telecommunications (BUPT), China Data availability statement The raw data supporting the conclusions of this article will be made available by the authors under reasonable request. Ethics statement The studies involving humans were approved by the Ethics Committee at Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine. The studies were conducted in accordance with the local legislation and institutional requirements. The ethics committee/institutional review board waived the requirement of written informed consent for participation from the participants or the participants' legal guardians/next of kin because this study is a retrospective study using existing data recorded during regular clinical practice. Author contributions WB: Validation, Data curation, Writing &#8211; original draft, Methodology. CX: Methodology, Software, Writing &#8211; original draft, Visualization. RS: Writing &#8211; review &amp; editing, Supervision. XH: Visualization, Methodology, Writing &#8211; review &amp; editing. YL: Validation, Writing &#8211; review &amp; editing, Formal analysis. XW: Data curation, Writing &#8211; review &amp; editing, Resources. TT: Supervision, Investigation, Writing &#8211; review &amp; editing. DH: Supervision, Project administration, Writing &#8211; review &amp; editing, Conceptualization, Resources, Investigation. LX: Investigation, Conceptualization, Writing &#8211; review &amp; editing, Funding acquisition, Supervision, Formal analysis, Project administration, Resources. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The author(s) declare that no Gen AI was used in the creation of this manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher's note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References Bar C. Pedespan J.-M. Boccara O. Garcelon N. Levy R. Gr&#233;vent D. . ( 2020 ). Early magnetic resonance imaging to detect presymptomatic leptomeningeal angioma in children with suspected sturge-weber syndrome . Dev. Med. Child Neurol . 62 , 227 &#8211; 233 . doi: 10.1111/dmcn.14253 31050360 Chen W. Tan X. Zhang J. Du G. Fu Q. Jiang H. ( 2025 ). Mlg: A mixed local and global model for brain tumor classification . Front. Neurosci . 19 : 1618514 . doi: 10.3389/fnins.2025.1618514 40678755 PMC12267167 Chicco D. Jurman G. ( 2020 ). The advantages of the matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation . BMC Genomics 21 : 6 . doi: 10.1186/s12864-019-6413-7 31898477 PMC6941312 Chmura Kraemer H. Periyakoil V. S. Noda A. ( 2002 ). Kappa coefficients in medical research . Stat. Med . 21 , 2109 &#8211; 2129 . doi: 10.1002/sim.1180 12111890 &#199;i&#231;ek &#214;. Abdulkadir A. Lienkamp S. S. Brox T. Ronneberger O. ( 2016 ). &#8220;3D U-Net: learning dense volumetric segmentation from sparse annotation,&#8221; in Medical Image Computing and Computer-Assisted Intervention ( Athens : Springer ), 424 &#8211; 432 . doi: 10.1007/978-3-319-46723-8_49 Comi A. M. ( 2007 ). Update on sturge-weber syndrome: diagnosis, treatment, quantitative measures, and controversies . Lymphat. Res. Biol . 5 , 257 &#8211; 264 . doi: 10.1089/lrb.2007.1016 18370916 Dosovitskiy A. Beyer L. Kolesnikov A. Weissenborn D. Zhai X. Unterthiner T. . ( 2021 ). &#8220;An image is worth 16x16 words: transformers for image recognition at scale,&#8221; in International Conference on Learning Representations , 611 &#8211; 631 . Gu A. Dao T. ( 2024 ). &#8220;Mamba: linear-time sequence modeling with selective state spaces,&#8221; in First Conference on Language Modeling ( Philadelphia, PA ), 1 &#8211; 32 . Gu A. Goel K. Re C. ( 2022 ). &#8220;Efficiently modeling long sequences with structured state spaces,&#8221; in International Conference on Learning Representations , 14323 &#8211; 14343 . Hatamizadeh A. Nath V. Tang Y. Yang D. Roth H. R. Xu D. ( 2021 ). &#8220;Swin unetr: Swin transformers for semantic segmentation of brain tumors in MRI images,&#8221; in International MICCAI Brainlesion Workshop ( Springer ), 272 &#8211; 284 . doi: 10.1007/978-3-031-08999-2_22 Hatamizadeh A. Tang Y. Nath V. Yang D. Myronenko A. Landman B. . ( 2022 ). &#8220;Unetr: transformers for 3d medical image segmentation,&#8221; in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision ( Waikoloa, HI : IEEE ), 574 &#8211; 584 . doi: 10.1109/WACV51458.2022.00181 He Y. Nath V. Yang D. Tang Y. Myronenko A. Xu D. ( 2023 ). &#8220;Swinunetr-v2: Stronger swin transformers with stagewise convolutions for 3d medical image segmentation,&#8221; in International Conference on Medical Image Computing and Computer-Assisted Intervention ( Vancouver, BC : Springer ), 416 &#8211; 426 . doi: 10.1007/978-3-031-43901-8_40 Huttenlocher D. P. Klanderman G. A. Rucklidge W. J. ( 1993 ). Comparing images using the Hausdorff distance . IEEE Trans. Pattern Anal. Mach. Intell . 15 , 850 &#8211; 863 . doi: 10.1109/34.232073 Isensee F. Jaeger P. F. Kohl S. A. Petersen J. Maier-Hein K. H. ( 2021 ). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation . Nat. Methods 18 , 203 &#8211; 211 . doi: 10.1038/s41592-020-01008-z 33288961 Kazerooni A. F. Khalili N. Liu X. Haldar D. Jiang Z. Anwar S. M. . ( 2024 ). The brain tumor segmentation (brats) challenge 2023: focus on pediatrics (cbtn-connect-dipgr-asnr-miccai brats-peds) . ArXiv:2305.17033 . doi: 10.48550/arXiv.2305.17033 37292481 Krishnan R. Rajpurkar P. Topol E. J. ( 2022 ). Self-supervised learning in medicine and healthcare . Nat. Biomed. Eng . 6 , 1346 &#8211; 1352 . doi: 10.1038/s41551-022-00914-1 35953649 Lee C.-Y. Xie S. Gallagher P. Zhang Z. Tu Z. ( 2015 ). &#8220;Deeply-supervised nets,&#8221; in Artificial Intelligence and Statistics ( Lille : Pmlr ), 562 &#8211; 570 . Lee H. H. Bao S. Huo Y. Landman B. A. ( 2023 ). &#8220;3D UX-Net: a large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation,&#8221; in International Conference on Learning Representations ( Kigali ), 21891 &#8211; 21905 . Liu J. Yang H. Zhou H.-Y. Xi Y. Yu L. Li C. . ( 2024 ). &#8220;Swin-umamba: Mamba-based unet with imagenet-based pretraining,&#8221; in International Conference on Medical Image Computing and Computer-Assisted Intervention ( Marrakesh : Springer ), 615 &#8211; 625 . doi: 10.1007/978-3-031-72114-4_59 Liu Y. Tian Y. Zhao Y. Yu H. Xie L. Wang Y. . ( 2024 ). Vmamba: visual state space model . Adv. Neural Inform. Process. Syst . 37 , 103031 &#8211; 103063 . doi: 10.48550/arXiv.2401.10166 Ma J. Li F. Wang B. ( 2024 ). U-Mamba: enhancing long-range dependency for biomedical image segmentation . arXiv:2401.04722 . doi: 10.48550/arXiv.2401.04722 Myronenko A. ( 2019 ). &#8220;3D MRI brain tumor segmentation using autoencoder regularization,&#8221; in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part II 4 ( Granada : Springer ), 311 &#8211; 320 . doi: 10.1007/978-3-030-11726-9_28 Raghu M. Unterthiner T. Kornblith S. Zhang C. Dosovitskiy A. ( 2021 ). Do vision transformers see like convolutional neural networks? Adv. Neural Inform. Process. Syst . 34 , 12116 &#8211; 12128 . doi: 10.48550/arXiv.2108.08810 Ramirez E. L. J&#252;lich K. ( 2024 ). &#8220;Sturge-weber syndrome: an overview of history, genetics, clinical manifestations, and management,&#8221; in Seminars in Pediatric Neurology, Volume 51 ( Philadelphia, PA : Elsevier ), 101151 . doi: 10.1016/j.spen.2024.101151 39389653 Ronneberger O. Fischer P. Brox T. ( 2015 ). &#8220;U-Net: convolutional networks for biomedical image segmentation,&#8221; in Medical Image Computing and Computer-Assisted Intervention ( Munich : Springer ), 234 &#8211; 241 . doi: 10.1007/978-3-319-24574-4_28 Roy S. Koehler G. Ulrich C. Baumgartner M. Petersen J. Isensee F. . ( 2023 ). &#8220;Mednext: transformer-driven scaling of convnets for medical image segmentation,&#8221; in International Conference on Medical Image Computing and Computer-Assisted Intervention ( Vancouver, BC : Springer ), 405 &#8211; 415 . doi: 10.1007/978-3-031-43901-8_39 Selvaraju R. R. Cogswell M. Das A. Vedantam R. Parikh D. Batra D. ( 2017 ). &#8220;Grad-cam: visual explanations from deep networks via gradient-based localization,&#8221; in Proceedings of the IEEE International Conference on Computer Vision ( Venice : IEEE ), 618 &#8211; 626 . doi: 10.1109/ICCV.2017.74 Shi Y. Dong M. Xu C. ( 2024 ). Multi-scale vmamba: hierarchy in hierarchy visual state space model . Adv. Neural Inform. Process. Syst . 37 , 25687 &#8211; 25708 . doi: 10.48550/arXiv.2405.14174 Soh W. K. Rajapakse J. C. ( 2023 ). Hybrid unet transformer architecture for ischemic stoke segmentation with mri and ct datasets . Front. Neurosci . 17 : 1298514 . doi: 10.3389/fnins.2023.1298514 38105927 PMC10723803 Song Y. Wang T. Cai P. Mondal S. K. Sahoo J. P. ( 2023 ). A comprehensive survey of few-shot learning: evolution, applications, challenges, and opportunities . ACM Comput. Surv . 55 , 1 &#8211; 40 . doi: 10.1145/3582688 Sudarsanam A. Ardern-Holmes S. L. ( 2014 ). Sturge-weber syndrome: from the past to the present . Eur. J. Paediatr. Neurol . 18 , 257 &#8211; 266 . doi: 10.1016/j.ejpn.2013.10.003 24275166 Sujansky E. Conradi S. ( 1995 ). Outcome of sturge-weber syndrome in 52 adults . Am. J. Med. Genet . 57 , 35 &#8211; 45 . doi: 10.1002/ajmg.1320570110 7645596 Tan T. Tegzes P. T&#246;r&#246;k L. I. Ferenczi L. Avinash G. B. Rusk&#243; L. . ( 2023 ). Image Harmonization for Deep Learning Model Optimization. Technical Report . US Patent 11669945. Thomas-Sohl K. A. Vaslow D. F. Maria B. L. ( 2004 ). Sturge-weber syndrome: a review . Pediatr. Neurol . 30 , 303 &#8211; 310 . doi: 10.1016/j.pediatrneurol.2003.12.015 15165630 Ulyanov D. Vedaldi A. Lempitsky V. ( 2016 ). Instance normalization: the missing ingredient for fast stylization . arXiv:1607.08022 . doi: 10.48550/arXiv.1607.08022 Xing Z. Ye T. Yang Y. Liu G. Zhu L. ( 2024 ). &#8220;Segmamba: long-range sequential modeling mamba for 3d medical image segmentation,&#8221; in International Conference on Medical Image Computing and Computer-Assisted Intervention ( Marrakesh : Springer ), 578 &#8211; 588 . doi: 10.1007/978-3-031-72111-3_54 Xu J. Li Z. Du B. Zhang M. Liu J. ( 2020 ). &#8220;Reluplex made more practical: leaky ReLU,&#8221; in 2020 IEEE Symposium on Computers and communications (ISCC) ( Rennes : IEEE ), 1 &#8211; 7 . doi: 10.1109/ISCC50000.2020.9219587 Yushkevich P. A. Piven J. Hazlett H. C. Smith R. G. Ho S. Gee J. C. . ( 2006 ). User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability . Neuroimage 31 , 1116 &#8211; 1128 . doi: 10.1016/j.neuroimage.2006.01.015 16545965 Zhang T. Zhang X. Shi J. Wei S. ( 2019 ). Depthwise separable convolution neural network for high-speed sar ship detection . Remote Sens . 11 : 2483 . doi: 10.3390/rs11212483 Zhu Z. Wang Z. Qi G. Zhao Y. Liu Y. ( 2025 ). Visually stabilized mamba U-shaped network with strong inductive bias for 3-D brain tumor segmentation . IEEE Trans. Instrument. Measur . 74 , 1 &#8211; 11 . doi: 10.1109/TIM.2025.3551581"
}