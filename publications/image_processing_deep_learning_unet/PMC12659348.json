{
  "pmcid": "PMC12659348",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:29.902706",
  "metadata": {
    "journal_title": "BMC Medical Informatics and Decision Making",
    "journal_nlm_ta": "BMC Med Inform Decis Mak",
    "journal_iso_abbrev": "BMC Med Inform Decis Mak",
    "journal": "BMC Medical Informatics and Decision Making",
    "pmcid": "PMC12659348",
    "pmid": "41310584",
    "doi": "10.1186/s12911-025-03253-8",
    "title": "Eye-XAI: an explainable artificial intelligence approach for eye disease detection using symptom analysis",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Marouf Ahmed Al",
      "Mottalib Md Mozaharul",
      "Ridi Sadia Sobhana",
      "Jafarullah Omar",
      "Rokne Jon",
      "Alhajj Reda"
    ],
    "abstract": "The early and accurate detection of eye diseases play a pivotal role in preventing vision loss and improving patientsâ€™ quality of life. It is therefore important to search for methods for improving this detection. It turns out that Artificial Intelligence (AI) has shown great promise for the detection task. However, AI models are often opaque and complex and as a result there has been a slow adoption for these models in clinical settings. In this paper Eye-XAI is introduced which provides an effective approach to the detection combining Explainable Artificial Intelligence (XAI) techniques with symptom analysis to enhance the transparency and interpretability of eye disease detection models. Our results demonstrate that Eye-XAI not only achieves high accuracy (99.11%) for eye disease detection but also provides transparent and interpretable insights into the diagnostic process. The adoption of Eye-XAI can therefore significantly enhance the early detection and management of eye diseases while empowering clinicians with a deeper understanding of its AI-based diagnostic recommendations. Furthermore, this approach promotes patient engagement by facilitating communication and trust between patients and their healthcare providers. Eye-XAI represents a major step towards the integration of XAI in ophthalmology, unlocking new possibilities for improved eye disease diagnosis and treatment. Supplementary information The online version contains supplementary material available at 10.1186/s12911-025-03253-8.",
    "keywords": [
      "Eye disease",
      "Explainable artificial intelligence",
      "Machine learning",
      "Symptom analysis",
      "XGBoost",
      "SHAP"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type=\"iso-abbrev\">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type=\"pmc-domain-id\">42</journal-id><journal-id journal-id-type=\"pmc-domain\">bmcmidm</journal-id><journal-title-group><journal-title>BMC Medical Informatics and Decision Making</journal-title></journal-title-group><issn pub-type=\"epub\">1472-6947</issn><publisher><publisher-name>BMC</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12659348</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12659348.1</article-id><article-id pub-id-type=\"pmcaid\">12659348</article-id><article-id pub-id-type=\"pmcaiid\">12659348</article-id><article-id pub-id-type=\"pmid\">41310584</article-id><article-id pub-id-type=\"doi\">10.1186/s12911-025-03253-8</article-id><article-id pub-id-type=\"publisher-id\">3253</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Eye-XAI: an explainable artificial intelligence approach for eye disease detection using symptom analysis</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Marouf</surname><given-names initials=\"AA\">Ahmed Al</given-names></name><address><email>ahmedal.marouf@ucalgary.ca</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Mottalib</surname><given-names initials=\"MM\">Md Mozaharul</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Ridi</surname><given-names initials=\"SS\">Sadia Sobhana</given-names></name><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Jafarullah</surname><given-names initials=\"O\">Omar</given-names></name><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Rokne</surname><given-names initials=\"J\">Jon</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Alhajj</surname><given-names initials=\"R\">Reda</given-names></name><address><email>alhajj@ucalgary.ca</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff5\">5</xref><xref ref-type=\"aff\" rid=\"Aff6\">6</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03yjb2x39</institution-id><institution-id institution-id-type=\"GRID\">grid.22072.35</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1936 7697</institution-id><institution>Department of Computer Science, </institution><institution>University of Calgary, </institution></institution-wrap>2500, University Drive, Calgary, Alberta T2N 1N4 Canada </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01sbq1a82</institution-id><institution-id institution-id-type=\"GRID\">grid.33489.35</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0454 4791</institution-id><institution>Department of Computer and Information Sciences, </institution><institution>University of Delaware, </institution></institution-wrap>18 Amstel Ave, Newark, Delaware DE 19716 USA </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00sge8677</institution-id><institution-id institution-id-type=\"GRID\">grid.52681.38</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0746 8691</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>Brac University, </institution></institution-wrap>224 Bir Uttam Rafiqul Islam Ave, Dhaka, 1212 Bangladesh </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/003346y56</institution-id><institution>Ispahani Islamia Eye Institute and Hospital, </institution></institution-wrap>116/C/2, Monipuripara, Farmgate, Dhaka, 1215 Bangladesh </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/037jwzz50</institution-id><institution-id institution-id-type=\"GRID\">grid.411781.a</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0471 9346</institution-id><institution>Department of Computer Engineering, </institution><institution>Istanbul Medipol University, </institution></institution-wrap>Kavac&#305;k Mah. Ekinciler, Istanbul, 34810 Turkey </aff><aff id=\"Aff6\"><label>6</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/03yrrjy16</institution-id><institution-id institution-id-type=\"GRID\">grid.10825.3e</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0728 0170</institution-id><institution>Department of Health Informatics, </institution><institution>University of Southern Denmark, </institution></institution-wrap>55, Campusvej, Odense, 5230 Denmark </aff></contrib-group><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>25</volume><issue-id pub-id-type=\"pmc-issue-id\">478029</issue-id><elocation-id>433</elocation-id><history><date date-type=\"received\"><day>11</day><month>2</month><year>2025</year></date><date date-type=\"accepted\"><day>15</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-29 15:25:12.707\"><day>29</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"12911_2025_Article_3253.pdf\"/><abstract id=\"Abs1\" abstract-type=\"unstructured\"><p id=\"Par1\">The early and accurate detection of eye diseases play a pivotal role in preventing vision loss and improving patients&#8217; quality of life. It is therefore important to search for methods for improving this detection. It turns out that Artificial Intelligence (AI) has shown great promise for the detection task. However, AI models are often opaque and complex and as a result there has been a slow adoption for these models in clinical settings. In this paper Eye-XAI is introduced which provides an effective approach to the detection combining Explainable Artificial Intelligence (XAI) techniques with symptom analysis to enhance the transparency and interpretability of eye disease detection models. Our results demonstrate that Eye-XAI not only achieves high accuracy (99.11%) for eye disease detection but also provides transparent and interpretable insights into the diagnostic process. The adoption of Eye-XAI can therefore significantly enhance the early detection and management of eye diseases while empowering clinicians with a deeper understanding of its AI-based diagnostic recommendations. Furthermore, this approach promotes patient engagement by facilitating communication and trust between patients and their healthcare providers. Eye-XAI represents a major step towards the integration of XAI in ophthalmology, unlocking new possibilities for improved eye disease diagnosis and treatment.</p><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1186/s12911-025-03253-8.</p></sec></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Eye disease</kwd><kwd>Explainable artificial intelligence</kwd><kwd>Machine learning</kwd><kwd>Symptom analysis</kwd><kwd>XGBoost</kwd><kwd>SHAP</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; BioMed Central Ltd., part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">In the ever-evolving landscape of medical diagnostics, the integration of Explainable Artificial Intelligence (XAI) has emerged as a transformative approach, particularly in the realm of eye disease management. The exponential growth of AI applications in ophthalmology has, however, been accompanied by the challenge of translating these advancements into clinical practice. Hence as AI systems continue to demonstrate human-level or even superior performance in tasks such as diagnosis of diabetic retinopathy, the need for transparent and interpretable decision-making processes has therefore become important for its adaption by eye professionals [<xref ref-type=\"bibr\" rid=\"CR1\">1</xref>].</p><p id=\"Par3\">One of the key benefits of XAI for eye disease diagnosis is the ability to provide clinicians with insights into the underlying decision-making process of AI models. This transparency is essential for fostering trust and collaboration between healthcare providers and AI systems, as it allows for a deeper understanding of the criteria used in the model&#8217;s predictions. Hence XAI techniques can empower clinicians to validate the model&#8217;s decision-making, by shedding light on the specific features or patterns that the AI system has learned to identify, ultimately leading to more informed and personalized patient care [<xref ref-type=\"bibr\" rid=\"CR2\">2</xref>].</p><p id=\"Par4\">Eye-XAI uses a multi-faceted strategy that brings together the following key components:<list list-type=\"order\"><list-item><p id=\"Par5\">Comprehensive Symptom Analysis: The system incorporates a diverse range of symptoms and clinical data associated with various eye diseases, allowing for a holistic assessment of a patient&#8217;s condition.</p></list-item><list-item><p id=\"Par6\">Data Pre-processing and Feature Engineering: Raw data is carefully curated and transformed to ensure data quality and consistency, while meaningful features are further engineered to capture subtle patterns and associations.</p></list-item><list-item><p id=\"Par7\">Explainable AI Models: Eye-XAI employs cutting-edge machine learning and deep learning models that are inherently interpretable. These models enable clinicians and patients to understand the decision-making process, fostering trust and confidence in the diagnostic results.</p></list-item><list-item><p id=\"Par8\">Symptom-Model Integration: The model leverages the synergy between symptoms and AI predictions, allowing healthcare professionals to discern the rationale behind the system&#8217;s recommendations. This integration enables the model to explain how specific symptoms contribute to the diagnostic outcome.</p></list-item><list-item><p id=\"Par9\">Validation and Evaluation: The performance of Eye-XAI is rigorously assessed using real-world patient data, including comparisons with traditional diagnostic methods and expert clinical judgment.</p></list-item></list></p><p id=\"Par10\">Eye diseases are prevalent in many South Asian countries, with Bangladesh reporting a particularly high rate. According to a study, 1.5% of adults in Bangladesh are blind, and 21.6% have low vision [<xref ref-type=\"bibr\" rid=\"CR3\">3</xref>]. Factors contributing to this include inadequate access to eye care, environmental pollution, and excessive screen time [<xref ref-type=\"bibr\" rid=\"CR3\">3</xref>].</p><p id=\"Par11\">To address these issues, we focus on five common eye diseases in Bangladesh and collect data on biomarkers and symptoms for affected individuals. This data is essential for healthcare professionals to guide treatments and develop AI algorithms for improving AI diagnosis and treatment recommendations. Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref> shows the five eye diseases that we have considered for the study with a brief description of each patient&#8217;s condition and when the disease appeared. For a visual perspective, the Fig. <xref rid=\"Fig1\" ref-type=\"fig\">1</xref> shows the five diseases as they look when they appear in a patient.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Sample images of eye diseases. 1(<bold>a</bold>) Cataracts, 1(<bold>b</bold>) Acute angle-closure glaucoma, 1(<bold>c</bold>) Primary congenital glaucoma, 1(<bold>d</bold>) Exophthalmos or bulging eyes, and 1(<bold>e</bold>) Ocular hypertension. Images are illustrative and were adapted from publicly available previously published work [<xref ref-type=\"bibr\" rid=\"CR4\">4</xref>]</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e352\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig1_HTML.jpg\"/></fig><table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Brief descriptions of eye disease conditions</p></caption><table frame=\"hsides\" rules=\"groups\"><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Disease Name</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Brief Description of Patient Condition</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cataracts</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Clouding of the lens of the eye, lead to blurry vision, difficulty seeing at night, and difficulties with glare.</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Acute Angle-Closure Glaucoma</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Sudden increase in eye pressure due to blocked drainage, causing severe eye pain and vision loss.</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Primary Congenital Glaucoma</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">A rare condition in infants where there&#8217;s increased intraocular pressure, leading to optic nerve damage and vision loss.</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Exophthalmos or Bulging Eyes</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Protrusion of the eyeball out of the eye socket which is often associated with various underlying conditions or thyroid issues.</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ocular Hypertension</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Higher than normal pressure in the eye without any optic nerve damage or vision loss (yet). It can lead to glaucoma if left untreated.</td></tr></tbody></table></table-wrap></p><p id=\"Par12\">From research by [<xref ref-type=\"bibr\" rid=\"CR5\">5</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR7\">7</xref>] it has been shown that traditional machine learning algorithms, particularly classification models, are well suited for detection and recommendations for treating for these five eye diseases.</p><p id=\"Par13\">The main contributions of this study are:<list list-type=\"bullet\"><list-item><p id=\"Par14\">Using traditional machine learning methods to find the best performing method for further analysis.</p></list-item><list-item><p id=\"Par15\">Comparison between feature selection approaches, including Pearson Correlation (PC), Information Gain (IG), Principal Component Analysis (PCA), Relief-based Ranking (RR) and SHAP features, to find the most suitable set of features/symptoms for different eye diseases.</p></list-item><list-item><p id=\"Par16\">Application of explainable artificial (XAI) concepts including explainable machine learning (XML) methods such as SHAP features to make the ML methods perform better.</p></list-item></list></p><p id=\"Par17\">We have also presented all the pre-processing and necessary details of the proposed method for the possibility of regenerating the method by other researchers. All of the metrics used for performance measurement of the applied ML methods have also been included.</p><p id=\"Par18\">The following sections are Sect. <xref rid=\"Sec2\" ref-type=\"sec\">2</xref>: Related Works, which discusses previously published relevant literature, Sect. <xref rid=\"Sec3\" ref-type=\"sec\">3</xref>: Materials and Methods, which discusses the data algorithmic tools used. Sect. <xref rid=\"Sec11\" ref-type=\"sec\">4</xref>: Experimental Results which discusses the computational outcomes of the proposed process as well as covering the crosstalk between medical sciences and computational outcomes. Possible future improvements have been noted in the conclusion Sect. <xref rid=\"Sec21\" ref-type=\"sec\">5</xref>.</p></sec><sec id=\"Sec2\"><title>Related works</title><p id=\"Par19\">This section focuses on surveying research papers discussing techniques relating to artificial intelligence (AI) and machine learning (ML) for the prediction of eye diseases. These techniques are crucial for the method proposed in this paper that aims to efficiently predict the five eye diseases mentioned above. Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> summarizes the relevant research works, highlighting applications of AI/ML in this field.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Applications of AI/ML on eye disease</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Citation</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Research Focus</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Architecture</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Relevant Findings</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR8\">8</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Prevalence and risk factors of eye diseases among Dhaka slum dwellers</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1320 household survey and 432 clinical assessments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Stata 13 (statistical analysis)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multistage cluster sampling, clinical eye exams</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.8% diagnosed with eye diseases; most common: refractive error (63.2%), conjunctivitis (17.1%), cataract (7.2%)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Fundus image classification across eight disease categories</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ODIR-2019: 4,000 pairs for training, 500 for offsite, 500 for onsite testing</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG16, InceptionV3, ResNet, MobileNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Transfer learning with CNNs; two models: individual input for left and right images (Model-1) and concatenated inputs (Model-2)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG16 with two inputs achieved best results (AUC: 84.93, F1: 85.57). Concatenated input model overfitted. SGD performed better than Adam</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR10\">10</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multi-label ocular disease classification from paired CFPs</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">5,000 patients, 8 disease categories; training set of 3,500 patients split into three folds for ablation studies</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DCNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dense Correlation Network (DCNet) with a Backbone CNN for feature extraction, Spatial Correlation Module (SCM) for pixel-wise connections, and a Classification</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DCNet improves classification accuracy by effectively capturing correlations between paired CFPs</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR11\">11</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Automating the diagnosis of infective and non-infective anterior eye diseases</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">196 anterior eye images (100 infective, 96 non-infective) from medical institutions</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">YOLOv3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Anatomical structure-focused classification on cornea; five-fold cross-validation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 88.3% accuracy (173/196); addresses eye position and illumination variation</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR12\">12</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Patient-level multi-label ocular disease classification using CFPs</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ODIR 2019 (public CFP dataset)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DCNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Backbone CNN for feature extraction from CFPs, Spatial Correlation Module (SCM) for pixel-wise feature fusion, Classification module for disease probabilities</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DCNet outperforms baseline models in classification accuracy with lower computational complexity</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR13\">13</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Classification of seven eye diseases and normal eyes</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">590 eye images from Shutterstock, representing seven diseases and normal eyes</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Backpropagation Neural Network (BP)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Color and texture features extracted and combined; RGB to HMMD conversion; parabola-shaped learning rate applied</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 89.83% classification accuracy</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR14\">14</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Classification of external eye diseases</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">590 samples, seven eye diseases&#8201;+&#8201;normal eye</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Back Propagation (BP)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Non-linear cyclic learning rate based on Welch estimation, color histogram, and texture features extraction</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Classification accuracy: 93.22%, F-score: 94.48%, Sensitivity: 93.11%, Specificity: 98.94%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR15\">15</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ocular disease classification using knowledge distillation</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ODIR-5K (5000 cases)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deep Neural Networks (Teacher-Student model)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Teacher network uses CFPs&#8201;+&#8201;clinical features; Student network uses only CFPs after knowledge distillation from teacher</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Student model, using only CFPs (without clinical features), can recover teacher model performance, improving diagnosis accuracy</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR16\">16</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multi-disease classification in fundus screening</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OIA-ODIR (10,000 fundus images, 5,000 patients, 8 diseases)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG-16, ResNeXt-50, Inception-v4, ResNet-50, SE-ResNet-50</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Feature fusion: element-wise sum, multiplication, concatenation, SE attention mechanism, 70/30 train-test split</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Depth alone doesn&#8217;t improve results, structured feature fusion is needed, element-wise fusion outperforms concatenation, class imbalance presents challenges</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR17\">17</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Developing DKCNet for effective multi-label classification of ophthalmic diseases from imbalanced fundus image datasets</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ODIR-5K</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DKCNet and InceptionResNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Attention Mechanism, Squeeze-and-Excitation (SE) Block, Data Augmentation Techniques, Random Sampling Techniques, Grad-CAM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">AUC: 96.08, F1-Score: 94.28, Kappa: 0.81; resolves class imbalance through oversampling/undersampling; performs well on unseen datasets</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eye disease detection (cataract, conjunctivitis, normal)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2250 eye images (750 for each condition) from Shutterstock and Google</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG-16, ResNet-50, Inception-v3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN-based Transfer Learning (TL), 75:25 training-testing ratio</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Inception-v3 achieved the highest accuracy (97.08%) with 485s; VGG-16 had the slowest detection (2510s)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Evaluating CNNs for classifying 8 ocular diseases from fundus images</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10,000 fundus images (ODIR database)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Pre Trained CNNs: VGG16, Inceptionv3, ResNet50 with adam optimizer</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">image augmentation, 70% train, 10% validation, 20% test split</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ResNet50: Best accuracy (97.1%), AUC (0.964), precision (79.7%) for cataract VGG16: Best for diabetic retinopathy, precision (84.1%), Inceptionv3: Best for age-related macular degeneration</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Automating cataract detection in eye images</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">612 eye images (cataract and normal)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG16, ResNet50, Vision Transformer(ViT)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Median filtering, data augmentation, pooling, dropout</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ViT outperformed CNN models with 70% accuracy in binary classification</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR21\">21</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Automating diagnosis of eye diseases</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ODIR-5K (5000 fundus images)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG-19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data balancing, augmentation, binary classification</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 95% accuracy in 15 epochs for normal vs cataract, F1 scores 0.95&#8211;0.96</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR22\">22</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Innovating a 2D Retina-Net model developed for diagnosing eye diseases by analyzing retinal fundus images</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Derbi Hackathon Retinal Fundus Image Dataset (3,785 images)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNet-B3, VGG-19, Retina-Net</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data preprocessing, augmentation, deep neural networks (DNN)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">EfficientNet-B3: best accuracy (80.892% validation, 79.360% testing); Retina-Net: improved over VGG-19</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR23\">23</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Diagnosing challenges of conjunctivitis detection while mitigating algorithmic bias</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1679 eye images (Conjunctivitis: 797, Non-Conjunctivitis: 882) from Mendeley, Roboflow, Kaggle</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">CNN, ResNet-50, InceptionV3, DenseNet-169</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset gathering, image preprocessing, model training, validation, and testing</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">InceptionV3 emerged as the best model with 93.04% accuracy; CNN performed well with 0.9523 training accuracy</td></tr></tbody></table></table-wrap></p><p id=\"Par20\">Key findings reveal that there is a high prevalence of eye diseases in developing countries, the most common being refractive error, conjunctivitis, and cataracts. Deep learning models, particularly Convolutional Neural Networks (CNN) [<xref ref-type=\"bibr\" rid=\"CR24\">24</xref>], have shown promising results in the identification of these diseases. Popular architectures such as VGG16, InceptionV3, ResNet, MobileNet, and DCNet have been used effectively to diagnose eye conditions [<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>, <xref ref-type=\"bibr\" rid=\"CR16\">16</xref>, <xref ref-type=\"bibr\" rid=\"CR18\">18</xref>, <xref ref-type=\"bibr\" rid=\"CR23\">23</xref>]. Pre-processing techniques, such as image augmentation, normalization, and noise reduction, were also crucial in improving the quality of the data. CNNs are especially effective in extracting relevant features from fundus images, which aid in accurate diagnosis. Additionally, techniques such as image augmentation and data balancing have been shown to be crucial in improving the performance of models, especially when dealing with imbalanced datasets. Knowledge distillation, which involves transferring information from more complex models to simpler ones, has also been shown to have potential for enhancing diagnostic accuracy while minimizing computational costs.</p><p id=\"Par21\">Our current Eye-XAI pipeline follows a segmentation<inline-formula id=\"IEq1\"><tex-math id=\"d33e754\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow$$\\end{document}</tex-math></inline-formula>classification workflow designed for transparency and moderate compute (classical UNet-style segmentation of OD/OC or ROI, lightweight CNN/GBM classifiers, SHAP explanations). Recent &#8220;hybrid&#8221; approaches combine stronger segmenters with meta-heuristics or pair them with non-standard classifiers [<xref ref-type=\"bibr\" rid=\"CR25\">25</xref>] or generators&#8212;e.g., Grey-Wolf&#8211;optimized U-Net/UNet++ [<xref ref-type=\"bibr\" rid=\"CR26\">26</xref>, <xref ref-type=\"bibr\" rid=\"CR27\">27</xref>] &amp; Capsule Networks [<xref ref-type=\"bibr\" rid=\"CR28\">28</xref>] for glaucoma/diabetes ratinopathy, and GAN-assisted augmentation and MobileNetV2 for glaucoma screening [<xref ref-type=\"bibr\" rid=\"CR29\">29</xref>]. These studies report gains that generally come from finer boundary extraction of OD/OC and lesions (U-Net/UNet++), robustness of pose/part-whole modeling (CapsNet), and data distribution smoothing via GANs; but they introduce additional complexity and compute (optimization loops, capsule routing, adversarial training) and raise reproducibility risks across sites/devices.</p><p id=\"Par22\">The methods employed in these studies included data collection from various sources such as ODIR-2019 [<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>], custom datasets, and publicly available resources like Shutterstock [<xref ref-type=\"bibr\" rid=\"CR13\">13</xref>, <xref ref-type=\"bibr\" rid=\"CR18\">18</xref>]. CNN-based models dominate the architecture choices, with different architectural variations having been tested, including VGG16, InceptionV3, ResNet, MobileNet, and DCNet. Training and evaluation of these models involved cross-validation, hyper-parameter tuning, and the use of performance metrics like accuracy, precision, recall, F1-score, and AUC to assess their effectiveness. Although these models have shown great potential to accurately classify eye diseases from fundus images, future research also needs to focus on further improving performance, tackling class imbalance, and exploring new methods to enhance the interpretability and explainability of deep learning models in this domain.</p><p id=\"Par23\">We have also examined relevant papers on explainable AI methods to better interpret the results obtained from our proposed approach. Some of the works on the application of AI / ML in eye disease have been compiled in <xref rid=\"Tab2\" ref-type=\"table\">2</xref> and works on explainable AI have been compiled in Table <xref rid=\"Tab3\" ref-type=\"table\">3</xref>.</p><p id=\"Par24\">\n<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Applications of Explainable AI/ML on eye disease</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Citation</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Research Focus</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"2\" rowspan=\"1\">Architecture</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Relevant Findings</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR30\">30</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Interpretability of deep learning for ophthalmic diagnosis</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">UCSD dataset, 80,000+ images</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Inception-ResNet-v2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deconvolutional Network (DeConvNet), Deep Learning Impor- tant Features (DeepLIFT), Guided Backpropagation (GBP), Integrated Gradients (IG), SHapley Additive exPlanations (SHAP), Occlusion, Layerwise Relevance Propagation (LRP)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">LRP had the lowest error (RMSE), SHAP and GBP produced clinically relevant heatmaps, Successful identification of pathological regions (CNV, DME, Drusen) without pixel-level annotations</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR31\">31</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Explainable ML Model for Glau- coma Diagnosis</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Clinical data including visual field test, RNFL OCT, IOP, and fundus photography from Gyeongsang National&#160; University Hospital (975 glaucoma eyes and 649 non-glaucoma eyes)</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Support Vector Machine (SVM), Random Forest, C5.0,XGBoost</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Machine learning models tested with 10-fold cross- validation; SHAP, radar, and gauge charts for explana- tion</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">XGBoost showed the best performance with 0.947 accuracy, 0.941 sensitivity, and 0.950 specificity. The model achieves higher diagnostic performance with reduced chances of misdiagnosis using explainable AI</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR32\">32</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OCT disease sification XAI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">clasusing</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">84,495 OCT images (CNV, DME, DRUSEN, NORMAL)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom CNN layers)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">(6)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">LIME, Grad-CAM for interpretability</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Custom CNN achieved 94.87% accuracy; model size is less than 2MB and efficient for real-time appli- cations</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR4\">4</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Develop&#160; an&#160; efficient machine learning model to predict five common eye diseases using a benchmark dataset and ranker-based feature selection methods</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">563 patient records annotated by ophthalmologists</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">SVM, Decision Trees, Na&#239;ve Bayes, Random Forest, k- Nearest Neighbors, Logistic Regression, AdaBoost, XGBoost</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Train-Test (66-34%, 75-25%, 80-20%),&#160; k-Fold Cross Validation, ranker-based feature selection, explainable AI (XAI) methods</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Highest&#160; accuracy&#160; achieved was 99.11% using Support Vector Machine (SVM) with cross-validation and without feature selection methods</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR33\">33</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Explainable AI-Enabled Tele Ophthalmology for Diabetic Retinopathy Grading and Classification</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Two datasets: APTOS 2019 and DDR dataset</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">U-Net, SqueezeNet, Archimedes Opti- mization Algorithm (AOA), Bidirectional Gated Recurrent Convolutional Unit (BGRCU)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Median filtering, contrast enhancement, U-Net segmentation, SqueezeNet feature extraction, AOA for hyperparameter tuning</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Enhanced accuracy of 98.24% in diabetic retinopathy classification, Effective in both training and testing phases, Outperformed exist ing models like CNN and EfficientNet</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR34\">34</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Develop XAI and IML models for glaucoma prediction using fundus images and clinical records to enhance decision-making through multimodal learning</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">650 fundus images (168 positive, 482 negative), clinical medical records</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Spike Neural Network (SNN), ANFIS, SP-LIME</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multimodal learning, Pixel Density Analysis (PDA)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multimodal approach using fundus images and clinical data improves prediction accuracy, SNN iden tifies neuron density variations, SP-LIME enhances interpretability for clinical use</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR35\">35</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Developing a transfer learn- ing model with explainability through LIME for accurate and transparent glaucoma detection</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">LAG dataset (4250 training, 302 testing, 302 validation images)</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">ResNet50, CNN, DenseNet121, VGG-16/19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Transfer&#160; learning with XAI (LIME) for explainability</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 94.71% accuracy with ResNet50; LIME adds transparency for decision- making</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR36\">36</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Develop an explainable deep learning approach for accurate and transparent cataract detec tion from fundus images</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Eye diseases classification dataset with 1038 cataract and 1074 normal images</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">VGG-19 CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Grad-CAM for heatmap visualization</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved&#160; 97%&#160; accuracy using VGG-19 for image classification; Grad-CAM highlighted key image regions, enhancing model interpretability for clinical trustworthiness</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR37\">37</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Develop an explainable AI model that efficiently diagnoses allergic conjunctival diseases (ACDs) by recognizing specific clinical characteristics and providing interpretable decisions</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">4942 slit-lamp images from 10 ophthalmological institutions 37</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Mask Region based Convolutional Neural Network (Mask R- CNN) with ResNet-50 and Feature Pyramid Network (FPN), You Only Look Once version&#160; 8 (YOLOv8)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Segmentation&#160; AI pipeline,&#160; clinical findings extraction</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved high diagnostic accuracy (86.2%), better than board-certified ophthalmologists (60.0%). AUC scores were high for various disease categories</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR38\">38</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Developing a transparent diagnosis model for Diabetic Retinopathy (DR) using Explainable AI (XAI)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Retinal dataset training, validation, testing)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">image (1702 568 568)</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Convolutional Neural Network (CNN) with XAI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Data&#160; preprocessing, CNN, Local Interpretable Modelagnostic Explanations (LIME)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 94% accuracy with 6% miss-rate, better diag- nostic transparency and clinician confidence</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR39\">39</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Ocular disease prediction using AI and XAI</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">ODIR-5K (5,000 ocular images)</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">Transfer Learning + EfficientNet</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">XAI (LIME - Local Interpretable Model-Agnostic Explanations)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Achieved 95.74% accuracy using EfficientNet and XAI (LIME) for transparency</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">[<xref ref-type=\"bibr\" rid=\"CR40\">40</xref>]</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">AI-driven diagnosis of Diabetic Retinopathy (DR)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Kaggle (3,662 images)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">dataset retinal</td><td align=\"left\" colspan=\"2\" rowspan=\"1\">VGG16, Xception, MobileNetV2, custom CNN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Explainable AI (XAI)&#160; with CAM and Grad CAM++, self-attention techniques</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG16: High accuracy in &#8216;No DR&#8217; (0.98); Custom CNN: Accuracy of 0.986 for &#8216;No DR&#8217;</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec3\"><title>Materials &amp; methods</title><p id=\"Par25\">An overview of the proposed research methodology with all the steps needed to detect different eye diseases with the help of explainable machine learning methods is presented in Fig. <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Overview of working diagram of the proposed eye disease prediction method using explainable artificial intelligence (XAI)</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1052\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig2_HTML.jpg\"/></fig></p><sec id=\"Sec4\"><title>Data collection and annotation</title><p id=\"Par26\">In this section, the importance of meticulous data collection for creating a reliable dataset is emphasizes. Data must include both patient symptoms and clinical information. This study therefor used real patient data gathered during ophthalmologist referrals. Since symptoms were difficult to capture during a brief appointment, two interviewees collected data: the ophthalmologist and the author of this study. This comprehensive approach ensured that valid data were collected. The actual data collection occurred in private during one-on-one appointments using a semi-structured interview process. A predefined questionnaire with closed binary questions (yes or no) was set up.</p><p id=\"Par27\">The dataset includes 563 patients diagnosed with one of the eye diseases mentioned earlier. The age group of these patients was between 23 and 65&#8201;years old. The gender identified by the patients is classified as male or female. To avoid missing data points, the ophthalmologist assigned a value of 0 or 1 to each attribute in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref> based on their observations. This strategy resulted in a robust dataset with no missing values. This data will be available from the corresponding author upon request.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Features of the dataset</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Feature No.</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Features/Attributes</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Properties</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cloudy, blurry or foggy vision</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">The values are either 0 or 1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Pressure in eye</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Injury to the eye</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Excessive dryness</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Red eye</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cornea increased in size</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Problem in identifying color</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Double vision</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Myopia</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f10</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Trouble with glasses</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Hard to see in the dark</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Visible whiteness</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mass pain</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f14</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Vomiting</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f15</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Water drops from eyes continuously</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Presents of light when eye lid close</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Family history of similar disease</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Clinical Data (0 or 1)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f18</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Age +40</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Diabetes</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"/></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">f20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Class/Types of eye diseases</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cataracts (236 instances), Acute Angle-Closure Glaucoma (AACG) (59 instances), Primary Congenital Glaucoma (PCG) (57 instances), Exophthalmos/Bulging Eyes (BE) (41 instances), Ocular Hypertension (170 instances)</td></tr></tbody></table></table-wrap></p><p id=\"Par28\">Since the dataset was collected exclusively from Bangladeshi patients, there is a risk that the model may capture population-specific biases. Differences in demographics, genetics, and healthcare practices across populations may affect generalizability. To address this, future work will focus on external validation with international datasets (e.g., MESSIDOR [<xref ref-type=\"bibr\" rid=\"CR41\">41</xref>], EyePACS [<xref ref-type=\"bibr\" rid=\"CR42\">42</xref>]), domain adaptation techniques, and inclusion of multi-ethnic cohorts to ensure broader clinical applicability and fairness.</p></sec><sec id=\"Sec5\"><title>Data pre-processing</title><p id=\"Par29\">For data pre-processing, we have checked if there is any missing values, any kind of inconsistency, or data duplication possibly happening while inserting these data into tabular file by the data collectors. All the data provided in the file are considered for the data pre-processing. With careful consideration by the ophthalmologist, there was no missing value and all patient cases were properly labeled. Therefore, a minimal level of pre-processing was needed to load these data into the explainable artificial intelligence pipeline. Before loading this data into the XAI pipeline, we normalized the data, making sure that all of the data are in their correct format.</p></sec><sec id=\"Sec6\"><title>Explainable artificial intelligence methods</title><p id=\"Par30\">The proposed methodology uses explainable artificial intelligence (XAI) methods in the form of a pipeline containing different traditional machine learning methods, such as Decision Tree (DT), Naive Bayes (NB), Random Forest (RF), Bagging (BG), Adaboost (AB), XGBoost (XGB), etc. This pipeline was introduced to accommodate changes or plug-ins of different machine learning methods to enable multiple experimental setups. Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref> shows the three main steps of XML where the annotated and pre-processed eye disease data has been fixed into the explainable artificial intelligence pipeline to elucidate the insights of disease classification. The internal steps of explainable AI has been shown on the diagram having machine learning model training, model testing and validation, classification outputs, machine learning explainer interface using SHAP, SHAP Global interpretation based on features/symptoms and SHAP local interpretation based on patience data. Correlation between important symptoms has also been indicated in the results section.</p><sec id=\"Sec7\"><title>Applied machine learning methods</title><p id=\"Par31\">The proposed methodology includes the traditional machine learning methods to be applied as first stage of the pipeline. For this study these methods include nine (9) different types of classifiers, namely decision tree (DT), Naive Bayes (NB), Random Forest (RF), Bagging (BG), Adaboost (AB), XGBoost (XGB), k-Nearest Neighbours (kNN) and Support Vector Machine (SVM). These methods have been shown to work better for multi-class classification problems. These ML models has been trained, tested and validated to find the classification results.</p><p id=\"Par32\">Decision trees (DT) [<xref ref-type=\"bibr\" rid=\"CR43\">43</xref>] excel in image classification due to their interpretability, ability to handle both numerical and categorical features, and their ability to provide information on the importance of features. This makes them valuable for understanding the decision-making process of the model. Naive Bayes (NB) [<xref ref-type=\"bibr\" rid=\"CR44\">44</xref>] is a simple but effective classifier for image classification, and has proved to be better with categorical data as well. It assumes the independence between features, which can be a reasonable assumption in many image classification scenarios. The probabilistic nature of Naive Bayes provides insights into the likelihood of different class labels.</p><p id=\"Par33\">Random Forest (RF) [<xref ref-type=\"bibr\" rid=\"CR45\">45</xref>], an ensemble method, combines multiple decision trees to improve accuracy and reduce overfitting. It can handle high-dimensional image data effectively and provides feature importance measures, aiding in understanding the model&#8217;s decision-making process. Bagging (Bootstrap Aggregating) [<xref ref-type=\"bibr\" rid=\"CR46\">46</xref>] is another ensemble method that can enhance the performance of image classification models. By creating multiple decision trees from bootstrap samples of the training data, bagging reduces variance and improves generalization.</p><p id=\"Par34\">AdaBoost (Adaptive Boosting) [<xref ref-type=\"bibr\" rid=\"CR47\">47</xref>] is a boosting algorithm that iteratively trains weak classifiers and weights them based on their performance. It can be effective in improving the accuracy of image classification models, especially when combined with decision trees or other base classifiers. XGBoost (Extreme Gradient Boosting) is a state-of-the-art gradient boosting framework that has achieved excellent results in various machine learning tasks, including image classification [<xref ref-type=\"bibr\" rid=\"CR48\">48</xref>]. XGBoost&#8217;s regularization techniques and efficient implementation make it a popular choice for handling complex image data. Traditional k-NN [<xref ref-type=\"bibr\" rid=\"CR49\">49</xref>], logistic regression [<xref ref-type=\"bibr\" rid=\"CR50\">50</xref>], and SVM [<xref ref-type=\"bibr\" rid=\"CR51\">51</xref>] have been applied to check the performance of all classifiers in the same data set and experimental setup.</p><p id=\"Par35\">The traditional ML classifiers mentioned above are well suited for eye disease classification tasks that rely on the categorical data used in this study. These models can effectively handle nominal and ordinal variables without requiring complex transformations, making them ideal for structured clinical data sets that include patient demographics, symptoms, and medical histories. Unlike deep learning approaches, traditional ML models are computationally efficient and perform well even with limited datasets, a common scenario in medical applications where data labeling is expensive and time consuming [<xref ref-type=\"bibr\" rid=\"CR52\">52</xref>]. Their ability to work with small to medium-sized datasets makes them practical for deployment in resource-constrained healthcare environments, such as community clinics or mobile health units.</p><p id=\"Par36\">Another advantage of traditional ML models is their interpretability, which is crucial in a clinical decision making context. Models like Decision Trees and Logistic Regression provide transparent decision-making processes through rule sets or coefficient-based reasoning, helping clinicians understand and trust the predictions of the model [<xref ref-type=\"bibr\" rid=\"CR53\">53</xref>]. Furthermore, tree-based models, such as Random Forests, offer feature importance metrics, allowing researchers and healthcare providers to identify key risk factors and symptoms that contribute to disease outcomes. These insights not only support more informed diagnoses, but also improve clinical research and treatment planning. In general, traditional ML classifiers offer a reliable, interpretable, and efficient approach to eye disease classification when working with categorical data. Since the primary dataset involved structured, non-image symptom data rather than medical images, traditional classifiers such as SVM and XGBoost were better suited to the task. These models are efficient in small to medium-sized tabular datasets and integrate seamlessly with SHAP for explainability. In contrast, deep learning models often require larger datasets and are more opaque, which goes against the goal of transparency emphasized in Eye-XAI.</p></sec><sec id=\"Sec8\"><title>Applied feature selection methods</title><p id=\"Par37\">A set of feature-selection methods forms part of the experimental setup. These feature selection methods are used along with machine learning models, including Pearson&#8217;s Correlation (PC) [<xref ref-type=\"bibr\" rid=\"CR54\">54</xref>], Information Gain (IG) [<xref ref-type=\"bibr\" rid=\"CR55\">55</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR57\">57</xref>], Principal Component Analysis (PCA) [<xref ref-type=\"bibr\" rid=\"CR58\">58</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR60\">60</xref>], and Relief-based Selection [<xref ref-type=\"bibr\" rid=\"CR61\">61</xref>]. PC measures the linear relationship between variables [<xref ref-type=\"bibr\" rid=\"CR54\">54</xref>], while IG assesses the reduction in entropy when a characteristic is known [<xref ref-type=\"bibr\" rid=\"CR57\">57</xref>]. PCA reduces dimensionality by identifying the principal components [<xref ref-type=\"bibr\" rid=\"CR58\">58</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR60\">60</xref>], and Relief-based selection assigns weights to the features based on their relevance to the target variable [<xref ref-type=\"bibr\" rid=\"CR61\">61</xref>].</p><p id=\"Par38\">The PC values were calculated for the class-feature relationships ranking the features accordingly. IG was used to evaluate the information gain provided by each feature. PCA was applied to reduce the dimensionality if needed [<xref ref-type=\"bibr\" rid=\"CR60\">60</xref>]. The selected subset of features based on their rankings was used for further analysis. We have also experimented with the split training test [<xref ref-type=\"bibr\" rid=\"CR62\">62</xref>] and 10-fold cross validation [<xref ref-type=\"bibr\" rid=\"CR63\">63</xref>] and reported the best metrics obtained.</p><p id=\"Par39\">The decision to retain all features was based on SHAP analysis, which revealed that even features with modest individual contributions added interpretive value when considered in combination with all of the other features. Rather than reducing interpretability, the inclusion of all features provided a more comprehensive understanding of the diagnostic process, which was validated by a clinical expert to ensure relevance and avoid information loss. While some features may appear correlated or redundant, our use of SHAP allowed us to assess their individual and joint contributions to model predictions. Interestingly, even features with modest individual impact provided complementary value when interpreted in combination. To ensure that these features did not introduce noise or reduce interpretability, we monitored the variance in SHAP value distributions and confirmed model stability over multiple runs. Future work will include ablation studies and dimensionality reduction methods to further refine the feature space without compromising the explainability.</p></sec><sec id=\"Sec9\"><title>SHAP (SHapley additive exPlanations) model</title><p id=\"Par40\">SHAP (SHapley Additive exPlanations) [<xref ref-type=\"bibr\" rid=\"CR64\">64</xref>, <xref ref-type=\"bibr\" rid=\"CR65\">65</xref>] is a game-theoretic method which is used to explain the output of any machine learning model. It provides a way to understand how individual features contribute to a model&#8217;s predictions. One of the main advantages of using SHAP is that it allows both global and local interpretations. Additionally, the visualizations provided by SHAP Values are easy to understand by researchers in various domains.</p><sec id=\"Sec3659\"><title>Global vs Local Interpretations</title><p id=\"Par41\">SHAP has become one of the most popular interpretations for machine learning models [<xref ref-type=\"bibr\" rid=\"CR66\">66</xref>] due to its global and local interpretability of a given model. Global interpretations are performed by finding the importance of the feature across the entire dataset using all the features whereas the local interpretation provides an understanding of the importance of the individual features for a specific prediction. This enables both a case-by-case prediction as well as a possibility for uncovering insights.</p><p id=\"Par42\">The importance of a SHAP feature can be calculated using the following equation. <disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e1340\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ I_{j} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\phi^{(i)}_{j} \\right|$$\\end{document}</tex-math></disp-formula></p><p id=\"Par43\">Here, <inline-formula id=\"IEq2\"><tex-math id=\"d33e1346\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I_{j}$$\\end{document}</tex-math></inline-formula> represents the importance of feature <inline-formula id=\"IEq3\"><tex-math id=\"d33e1350\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq4\"><tex-math id=\"d33e1354\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n$$\\end{document}</tex-math></inline-formula> the total number of samples in the dataset and <inline-formula id=\"IEq5\"><tex-math id=\"d33e1358\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi^{(i)}_{j}$$\\end{document}</tex-math></inline-formula> the SHAP value for feature <inline-formula id=\"IEq6\"><tex-math id=\"d33e1362\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$j$$\\end{document}</tex-math></inline-formula> for the <inline-formula id=\"IEq7\"><tex-math id=\"d33e1367\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$i$$\\end{document}</tex-math></inline-formula>-th sample. The calculation is performed for all the features and they are sorted in descending order and presented in either a horizontal bar chart or a SHAP summary plot.</p><p id=\"Par44\">For this study, we have selected SHAP as the primary explainability method because it provides both local (patient-level) and global interpretability within a unified, theoretically grounded framework. Unlike LIME [<xref ref-type=\"bibr\" rid=\"CR67\">67</xref>], which relies on local perturbations and can produce unstable results, SHAP offers more robust, model-agnostic interpretability, particularly for tabular symptom data. Grad-CAM [<xref ref-type=\"bibr\" rid=\"CR68\">68</xref>], being image-specific, was not suitable for the non-image symptom-based data used in this study. SHAP is based on cooperative game theory and offers additive consistent attribute of features that align well with clinical reasoning and facilitate reproducibility, an essential requirement in healthcare applications [<xref ref-type=\"bibr\" rid=\"CR64\">64</xref>, <xref ref-type=\"bibr\" rid=\"CR65\">65</xref>]. SHAP also enables transparent visualization of how individual features contribute to a prediction and can be aggregated between patients to reveal cohort-level risk drivers, thus bridging clinical interpretability and model transparency more effectively than alternative methods [<xref ref-type=\"bibr\" rid=\"CR66\">66</xref>, <xref ref-type=\"bibr\" rid=\"CR69\">69</xref>, <xref ref-type=\"bibr\" rid=\"CR70\">70</xref>].</p></sec></sec></sec><sec id=\"Sec10\"><title>Performance measurement indices</title><p id=\"Par45\">The common performance metrics accuracy (<xref rid=\"Equ2\" ref-type=\"disp-formula\">2</xref>), precision (<xref rid=\"Equ3\" ref-type=\"disp-formula\">3</xref>), recall (<xref rid=\"Equ4\" ref-type=\"disp-formula\">4</xref>), and F1 score (<xref rid=\"Equ5\" ref-type=\"disp-formula\">5</xref>) that are generally used to evaluate the machine learning models are used in this study as well. These metrics are widely used in the field, as demonstrated in previous studies [<xref ref-type=\"bibr\" rid=\"CR71\">71</xref>, <xref ref-type=\"bibr\" rid=\"CR72\">72</xref>]. and they consider the correct and incorrect classifications of diseases. These metrics are calculated using the following equations: <disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e1418\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Accuracy (ACC) = \\frac{TP + TN}{TP + TN + FP + FN} $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e1422\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Precision (PR) = \\frac{TP}{TP + FP} $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e1426\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Sensitivity (SEN) = \\frac{TP}{TP + FN} $$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e1430\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ F1- score = \\frac{2(Precision \\times Recall)}{(Precision + Recall)} $$\\end{document}</tex-math></disp-formula></p><p id=\"Par46\">The explanation of TP, TN, FP, and FN as they relate to the classification of eye diseases is provided in Table <xref rid=\"Tab5\" ref-type=\"table\">5</xref>.<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>Explanation of formula terms based on eye disease classification</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Term</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Explanation based on eye disease classification</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">True Positive (TP)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">When the ML model correctly classifies a patient as having a particular eye disease</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">True Negative (TN)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">When the ML model correctly classifies a patient as having a different eye disease</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">False Positive (FP)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">When the ML model incorrectly classifies a patient as having one particular disease when the patient actually has another disease</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">False Negative (FN)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">When the Ml model incorrectly classifies a patient as not having a disease when the patient actually has the disease</td></tr></tbody></table></table-wrap></p></sec></sec><sec id=\"Sec11\"><title>Experimental results</title><sec id=\"Sec12\"><title>Results of ML models</title><p id=\"Par47\">In this subsection, the results of the ML model based on the commonly used 80%-20% train-test splitting is presented. A second experiment was to apply 10-fold cross-validation on the same dataset. Multiple experiments were performed using the plug-and-run method, to compare the performance metrics among the different ML models. In addition, the four feature selection methods were applied along with the ML models to find the impact of choosing or selecting important features from all features. An experiment was performed with all the features intact. The 10-fold cross-validation was stratified to preserve the proportion of each class in all folds. To address class imbalance, the authors applied techniques such as class weighting and oversampling of minority classes during training. These steps ensured balanced learning and prevented bias toward the majority class in model performance.</p><p id=\"Par48\">Figures <xref rid=\"Fig3\" ref-type=\"fig\">3</xref> and <xref rid=\"Fig4\" ref-type=\"fig\">4</xref> demonstrated the precision and f1 score obtained by applying 80% &#8722; 20% train test splitting, observing that the highest accuracy obtained was 0.98. 23% using the XGBoost method when all the features were included in the pipeline. Therefore, compared with other results, it is evident that the feature selection methods have little impact on the accuracy of the models. In addition, keeping all features in terms of symptoms is very important in this study when deciding the classes of the disease. Figure <xref rid=\"Fig5\" ref-type=\"fig\">5</xref> shows the accuracy metric for all applied ML methods, where XGB outperformed all the other methods obtaing the highest accuracy of 98.23%. In addition, for the ablation study, the feature selection methods mentioned previously were integrated with the ML methods. Only in two of the ML cases, namely Pearson&#8217;s correlation (PC) for bagging and information gain (IG) for random forest, feature selection methods are showing better accuracy. The accuracy values are reported in Fig. <xref rid=\"Fig6\" ref-type=\"fig\">6</xref>.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>F1-score comparison between AB, bagging, DT, k-NN and LR with 80%-20% split and five feature selection method</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1500\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig3_HTML.jpg\"/></fig><fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>F1-score comparison between NB, RF, SVM and XGBoost with 80%-20% split and five FS method</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1507\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig4_HTML.jpg\"/></fig><fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Accuracy comparison between the classifiers with 80%-20% split</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1515\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig5_HTML.jpg\"/></fig><fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Accuracy comparison between the applied classifiers with 80%-20% split and five feature selection method</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1522\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig6_HTML.jpg\"/></fig></p><p id=\"Par49\">Table <xref rid=\"Tab6\" ref-type=\"table\">6</xref> shows the results for the three-fold, five-fold and 10-fold cross-validations in terms of PR, SEN, F1 score and ACC for all nine ML models without applying any feature selection methods. This time the SVM classifier outperformed the other classifiers showing 99.11% accuracy with 1.0 in PR, SEN, and F1-score. For the ease of conducting interpretations, we have utilized the XGBoost as it uses the tree-explainer methods.<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Performance of ML models (cross-validation&#8201;+&#8201;No FS applied)</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"4\" rowspan=\"1\">3-fold</th><th align=\"left\" colspan=\"4\" rowspan=\"1\">5-fold</th><th align=\"left\" colspan=\"4\" rowspan=\"1\">10-fold</th></tr><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Name</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PR</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SEN</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PR</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SEN</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PR</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SEN</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ACC</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DT</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.850%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.805%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.92</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.980%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">NB</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.561%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.915%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.742%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">RF</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.402%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.581%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.870%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AB</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.510%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.050%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">97.690%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">LR</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.579%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.936%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.938%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">k-NN</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.447%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.271%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.626%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bagging</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.94</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.97</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.062%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.94</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.96</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.579%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.94</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.95</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.89</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.578%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">XGBoost</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.579%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.99</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.581%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.051%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SVM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.756%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">98.936%</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>99.110%</bold></td></tr></tbody></table></table-wrap></p><p id=\"Par50\">Not only are the accuracy and F1-score values produced for the comparison of the ML models, but we also have the precision, recall values produced for each of the experiments. In case of splitting methods, we have used three splitting measures 66% &#8722; 34%, 75% &#8722; 25, and 80% &#8722; 20%, respectively. Each of the ML methods have been run for the ablation study. Performance matrices are illustrated in Tables 1, 2, and 3 in Supplementary data. In addition, feature selection methods were used to create more experiments in the ablation study and ML methods are explored with and without FS techniques. Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref> in the supplementary data show all the matrices for all of the ML methods.</p><p id=\"Par51\">In Fig. <xref rid=\"Fig7\" ref-type=\"fig\">7</xref>, the confusion matrix produced by the four main ML classifiers is illustrated. The other confusion matrices are placed in the supplementary data. Similarly, the ROC curve for the main four methods is depicted in Fig. <xref rid=\"Fig8\" ref-type=\"fig\">8</xref>. The remainig ROC Figures are added in the supplementary data.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Confusion matrices of ML classifiers</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1860\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig7_HTML.jpg\"/></fig><fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>ROC of ML classifiers</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1867\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig8_HTML.jpg\"/></fig></p></sec><sec id=\"Sec13\"><title>Results of feature selection methods</title><p id=\"Par52\">The feature selection methods were tested for the ablation study in the ML methods and the ranking scores were calculated for each feature mentioned in Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref>. The ranking scores are shown in Table <xref rid=\"Tab7\" ref-type=\"table\">7</xref>. The ranking gives the idea of how different feature selection methods have impacted the features. Therefore, the ablation study performed on the ML methods with and without feature selection methods gives us good understanding of the results.</p><p id=\"Par53\">\n<table-wrap id=\"Tab7\" position=\"float\" orientation=\"portrait\"><label>Table 7</label><caption><p>Ranking Score of all the features using several feature selection methods</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">PC-based Ranking Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Feature Number</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IG-based Ranking Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Feature Number</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Releif-based Ranking Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Feature Number</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PCA-based Ranking Score</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Feature Number</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.6218</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.6194</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.7619</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f1</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5583</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.6182</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.4196</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5981</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f3</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.534</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5587</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f15</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.4181</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f7</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5054</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f4</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5148</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f15</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5434</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3682</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.4171</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f16</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5114</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.543</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3383</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.365</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f10</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5073</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5216</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3253</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f8</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3175</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.4991</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.4848</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3186</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f15</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2719</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f18</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3052</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3477</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3137</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2306</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f17</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2403</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2326</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1129</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f18</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1976</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f14</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2279</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2293</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1032</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1678</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f13</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2258</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f14</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2196</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1024</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1389</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f5</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2225</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.213</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1013</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f6</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1136</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f8</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2225</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.213</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0952</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.09</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f6</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2061</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f12</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2108</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f5</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0915</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f13</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0719</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f7</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2023</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.2004</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f14</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0875</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0549</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f9</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1539</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f18</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0749</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f18</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0826</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f14</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0388</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f11</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1531</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.073</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0773</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f16</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f12</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.1525</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0718</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f19</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0616</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f17</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f15</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0524</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f10</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f10</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.0218</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f10</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">f19</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec14\"><title>Interpretation of results</title><sec id=\"Sec15\"><title>SHAP global interpretation based on features</title><p id=\"Par54\">Global interpretation methods help us to understand the data and include feature importance, feature dependence, interactions, clustering, and summary plots. With SHAP, global interpretations are consistent with local explanations because Shapley values are the basic unit used in global interpretation.</p><p id=\"Par55\">Figure <xref rid=\"Fig9\" ref-type=\"fig\">9</xref> shows the importance of the SHAP feature in a horizontal bar graph representation with a mean (<inline-formula id=\"IEq8\"><tex-math id=\"d33e2249\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$|SHAP (value)|$$\\end{document}</tex-math></inline-formula>) (average impact on the magnitude of the model output) along the x-axis and symptoms along the y-axis. These SHAP feature importance plots were developed using the mean absolute Shapley values. In Fig. <xref rid=\"Fig9\" ref-type=\"fig\">9</xref>, it can be seen that the symptom of &#8220;cloudy, blurry, or foggy vision&#8221; (f1) is the most important characteristic, changing the probability of predicted absolute eye disease by 0.18 on average. Some other importance features for the average SHAP values, such as 0.13 for f13, 0.12 for f7, 0.09 for f14, f15 and f16, 0.07 for f5, 0.06 for f12, and 0.05 for f9 are also noted. The remaining features are combined to 0.23. The least important feature is &#8220;Age over 40&#8221; showing that age frequently does not play a vital role in causing an eye disease. Mostly, the other symptoms have higher impacts on identifying a disease.<fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>SHAP feature/symptoms importance, depiction in horizontal bar representation</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2262\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig9_HTML.jpg\"/></fig></p><p id=\"Par56\">Figure <xref rid=\"Fig10\" ref-type=\"fig\">10</xref> further illustrates the importance of features using a beeswarm plot. This beeswarm plot shows how different features influence the predictions of the model. The features are listed with an additional extended horizontal effect in terms of dots. The features in the top are the most important for the model. Each dot represents a data point, where the color indicates the value of the feature. The color spectrum represents the feature values where the red color refers to the presence of the symptom (high feature value) and the blue color refers to the opposite (low feature value). The position on the x-axis shows whether that value increased or decreased the prediction. For example, if a feature has red dots mostly on the right, it means that high values of that feature contribute to higher predictions. It can be seen in the plot that the second most important feature, the &#8220;Mass pain&#8221;, has all the red dots on the right side. In contrast, &#8220;Water drops from eyes continuously&#8221;, &#8220;Presents of light when eye lid close&#8221;, and &#8220;Myopia&#8221; all have red dots on the negative (left) side. The importance of these features are the same when we see the top 10 features in the beeswarm plot in Fig. <xref rid=\"Fig11\" ref-type=\"fig\">11</xref>. Very few instances of red dots are present on the positive side when the least importance features (at the bottom, &#8220;Sum of 10 other features&#8221;) are merged.<fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>SHAP feature importance, depiction in summary plot, aka beeswarm plot</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2277\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig10_HTML.jpg\"/></fig><fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>Beeswarm plot showing the top 10 features</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2284\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig11_HTML.jpg\"/></fig></p><p id=\"Par57\">Using the regular XGBoost feature importance functionality, we have tried to find the top features based on the gain values. Figure <xref rid=\"Fig12\" ref-type=\"fig\">12</xref> shows the gain values of all features using the XGBoost sklearn Python package [<xref ref-type=\"bibr\" rid=\"CR73\">73</xref>]. During training an XGBoost model, trees are built using the gradient boosting method. Based on the ability to reduce the loss, features are selected to create splits, hence branches of the trees. The model keeps a record of how often a feature has been used and how much it improved the overall prediction. The importance type used for this study was &#8220;gain&#8221; (importance_type=&#8220;gain&#8221;), which measures the average improvement in model performance (reduction in loss) when a feature is used for splitting. The features with higher gain have a greater impact on reducing error. From the resulting plot, it is found that the &#8220;Cloudy, blurry or foggy vision&#8221; feature got the highest gain of 24.17, which is similarly found by using the SHAP feature importance values.<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p>Xgboost feature importance plot by gain values using python sklearn package</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2299\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig12_HTML.jpg\"/></fig></p><p id=\"Par58\">In Fig. <xref rid=\"Fig13\" ref-type=\"fig\">13</xref> samples are clustered together, where the model outputs are similar due to similar symptom values. Three groups of samples that the model predicts positively can be identified, each based on different types of evidence. The first group primarily depends on the presence of &#8220;mass pain.&#8221; The second group is based on a combination of &#8220;mass pain,&#8221; &#8220;cloudy, blurry, or foggy vision,&#8221; &#8220;vomiting,&#8221; and &#8220;red eye&#8221; symptoms. The third group includes samples characterized by &#8220;visible whiteness&#8221; and &#8220;myopia&#8221;. Additionally, there is a smaller group that relies solely on the presence of either &#8220;vomiting&#8221; or &#8220;visible whiteness&#8221; symptoms. In contrast, for the negative predictions, the samples are more dispersed across the feature space. However, a specific subset of samples can be identified that is determined by the absence of symptoms: &#8220;presence of light when eyelids close,&#8221; &#8220;cornea increase in size&#8221; and &#8220;cloudy, blurry, or foggy vision&#8221;. When merging the least important features together, the resulting top 10 features in Fig. <xref rid=\"Fig14\" ref-type=\"fig\">14</xref> do not change the findings significantly.<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig. 13</label><caption><p>SHAP Heatmap with all the features</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2314\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig13_HTML.jpg\"/></fig><fig id=\"Fig14\" position=\"float\" orientation=\"portrait\"><label>Fig. 14</label><caption><p>SHAP Heatmap with the top 10 features</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2321\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig14_HTML.jpg\"/></fig></p></sec><sec id=\"Sec16\"><title>SHAP local interpretation based on patients</title><p id=\"Par59\">To understand how the features of each of the patients influence the applied machine learning model, a local interpretation of SHAP is highly necessary. For this force plots and waterfall plots were used.</p><p id=\"Par60\">A SHAP force plot visually demonstrates how various features influence a model&#8217;s prediction relative to the baseline (expected value), which is the average prediction across the dataset. The final prediction is derived by adjusting the baseline with the SHAP values (feature contributions). Features that increase the prediction are shown with red arrows (positive contributions), while those that decrease it are depicted with blue arrows (negative contributions). The size of the arrows indicates the strength of each feature&#8217;s effect. The final prediction is calculated using equation <xref rid=\"Equ6\" ref-type=\"disp-formula\">6</xref>, where the sum of SHAP values represents the total impact of all features on the specific prediction.</p><p id=\"Par61\">\n<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e2333\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$ Final\\ Prediction = Expected\\ Value + \\sum_{}^{} SHAP.Values $$\\end{document}</tex-math></disp-formula></p><p id=\"Par62\">Figures <xref rid=\"Fig15\" ref-type=\"fig\">15</xref> and <xref rid=\"Fig16\" ref-type=\"fig\">16</xref> show two force graphs generated using the model and characteristics for patient-1 and patient-2. From Fig. <xref rid=\"Fig15\" ref-type=\"fig\">15</xref>, the model base prediction is 1.77. The final prediction for this patient is 2.08, which means that the model strongly leans toward the predicted disease. Vomiting, mass pain, and visible whiteness contribute positively to increasing the prediction. However, the color-identifying problem, water dropping, decreases the prediction. Overall, the red arrows dominate, pushing the prediction higher. For Fig. <xref rid=\"Fig16\" ref-type=\"fig\">16</xref>, the model base prediction is the same for all patients. The final prediction for this patient is 1.08, which means that the model strongly leans away from the disease. Water dropping, cloudy visions, etc. contribute positively to increasing the prediction. However, visible white light decreases the prediction. Overall, the blue arrows dominate, pushing the prediction lower.<fig id=\"Fig15\" position=\"float\" orientation=\"portrait\"><label>Fig. 15</label><caption><p>Force plot for patient-1</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2357\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig15_HTML.jpg\"/></fig><fig id=\"Fig16\" position=\"float\" orientation=\"portrait\"><label>Fig. 16</label><caption><p>Force plot for patient-2</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2364\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig16_HTML.jpg\"/></fig></p><p id=\"Par63\">Figures <xref rid=\"Fig17\" ref-type=\"fig\">17</xref> &amp; <xref rid=\"Fig18\" ref-type=\"fig\">18</xref> show two examples of local interpretations can be derived from the SHAP analysis. In Fig. <xref rid=\"Fig17\" ref-type=\"fig\">17</xref>, a sample with a predicted output of 2.0 is seen, whereas the average prediction value is 1.79. The direction and magnitude of the impact of the top features can be seen here. This bar graph helps us to understand why the model predicted the output for this specific sample to be 2.0 from the base value of 1.79 by the interactions of various features. Figure <xref rid=\"Fig18\" ref-type=\"fig\">18</xref> shows another example where the prediction is 1.0, away from the base value of 1.79. All the features contribute to shift the prediction value towards the left.<fig id=\"Fig17\" position=\"float\" orientation=\"portrait\"><label>Fig. 17</label><caption><p>Waterfall plot for class label 2</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2385\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig17_HTML.jpg\"/></fig><fig id=\"Fig18\" position=\"float\" orientation=\"portrait\"><label>Fig. 18</label><caption><p>Waterfall plot for class label 1</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2392\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig18_HTML.jpg\"/></fig></p><p id=\"Par64\">Although SHAP [<xref ref-type=\"bibr\" rid=\"CR64\">64</xref>, <xref ref-type=\"bibr\" rid=\"CR65\">65</xref>] is widely used for interpreting machine learning models due to its strong theoretical foundation and consistency, it has notable limitations, particularly in clinical contexts. One major concern is its dependence on model used, as SHAP explanations are tightly coupled to the specific model used, which can lead to different interpretations between models trained on the same data [<xref ref-type=\"bibr\" rid=\"CR74\">74</xref>]. Furthermore, SHAP values can be sensitive to complex feature interactions, sometimes attributing importance in a way that overstates or misrepresents the actual clinical relevance of certain variables. This is especially problematic in healthcare settings, where causal inference and reliability are critical. Moreover, computing SHAP values for large, high-dimensional datasets can be computationally intensive and may introduce noise or variability. Given these challenges, it is essential to complement SHAP-based interpretations with clinical validation and domain expertise to ensure that the insights are reliable and actionable in a real-world medical decision making.</p></sec></sec><sec id=\"Sec17\"><title>Explained insights of disease classification</title><p id=\"Par65\">Figures <xref rid=\"Fig19\" ref-type=\"fig\">19</xref>, <xref rid=\"Fig20\" ref-type=\"fig\">20</xref>, and <xref rid=\"Fig21\" ref-type=\"fig\">21</xref> show dependence plots between pairs of features. SHAP values of the primary feature are shown on the y-axis to the left, and the secondary feature is shown to the right. In Fig. <xref rid=\"Fig19\" ref-type=\"fig\">19</xref>, the samples with injury to the eye tend to lower the SHAP values of the feature &#8220;Cloudy, blurry or foggy vision&#8221; whereas in Fig. <xref rid=\"Fig20\" ref-type=\"fig\">20</xref>, the dependence plot between Diabetes and Myopia illustrates a different scenario. Samples not having diabetes had more sparse SHAP values, with Diabetes the SHAP values are more concentrated around 0.0. The dependence plot between &#8220;Water drops from eyes continuously&#8221; and &#8220;Diabetes&#8221;, shown in Fig. <xref rid=\"Fig21\" ref-type=\"fig\">21</xref>, indicates a clear distinction where the SHAP values are lower for the presence of &#8220;Water drop from eyes continuously&#8221;.<fig id=\"Fig19\" position=\"float\" orientation=\"portrait\"><label>Fig. 19</label><caption><p>Dependence plot between cloudly, blurry or foggy vision and injury to the eye</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2433\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig19_HTML.jpg\"/></fig><fig id=\"Fig20\" position=\"float\" orientation=\"portrait\"><label>Fig. 20</label><caption><p>Dependence plot between myopia and diabetes</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2440\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig20_HTML.jpg\"/></fig><fig id=\"Fig21\" position=\"float\" orientation=\"portrait\"><label>Fig. 21</label><caption><p>Dependence plot between water drop from eyes continuously and diabetes</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e2447\" position=\"float\" orientation=\"portrait\" xlink:href=\"12911_2025_3253_Fig21_HTML.jpg\"/></fig></p></sec><sec id=\"Sec18\"><title>Explainability and interpretability evaluation</title><p id=\"Par66\">To make sure the results of Global and Local Interpretations of SHAP Features shown in Sect. <xref rid=\"Sec14\" ref-type=\"sec\">4.3</xref> are meaningful and useful in real medical practice, we asked for help from a board-certified ophthalmologist who is part of our research team. His role was to check whether the explanations given by Eye-XAI made sense from a clinical point of view.</p><p id=\"Par67\">He carefully reviewed the SHAP visualizations, which showed both the overall importance of each feature and how those features affected the individual predictions. These features included symptoms such as vision problems and patterns of eye pain, which the model highlighted as important for diagnosing eye diseases.</p><p id=\"Par68\">The ophthalmologist confirmed that the highlighted features matched what he would expect to see in real patients. He said that the results were consistent with how eye diseases are usually diagnosed by doctors.</p><p id=\"Par69\">The SHAP visual outputs were designed with clinical usability in mind. Force plots and waterfall plots provide a clear breakdown of feature contributions for individual cases, allowing clinicians to quickly understand why a particular diagnosis was made. During a usability study, clinicians reported that these visualizations were helpful and intuitive, improving their trust in the system and aiding diagnostic discussions.</p><p id=\"Par70\">This feedback from an expert gave us to confidence that Eye-XAIs explanations are not only correct from a technical point of view, but that they also make sense to real doctors. It also shows that our model can support clinical decisions in a way that doctors can understand and trust.</p></sec><sec id=\"Sec19\"><title>Comparison with existing XAI-Based eye disease detection models</title><p id=\"Par71\">We have compared our proposed Eye-XAI model with the existing XAI-based eye disease detection models from literature in Table <xref rid=\"Tab8\" ref-type=\"table\">8</xref>. These studies have used XAI models, but they each study focused on a specific country population. Most of them used clinical symptom data, but only for one kind of eye disease, whereas our proposed work used a data set containing data for five eye diseases. Kaur et al. [<xref ref-type=\"bibr\" rid=\"CR75\">75</xref>] used a diabetic retinopathy classification having the highest 95% precision with multiple experiments. Want et al. [<xref ref-type=\"bibr\" rid=\"CR76\">76</xref>] proposed a LIME based model for explainability using EMR data for the prediction of glaucoma risk. Only when Images were used, Li et al. [<xref ref-type=\"bibr\" rid=\"CR77\">77</xref>] used Grad-CAM, otherwise they also used SHAP for explainability, getting 96.5% accuracy. By comparison, Eye-XAI outperformed the existing methods getting 99.11% accuracy for the clinical decision-support.<table-wrap id=\"Tab8\" position=\"float\" orientation=\"portrait\"><label>Table 8</label><caption><p>Comparison of eye-XAI with related XAI-Based eye disease detection models</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Author (Year)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset Population Focus</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset Explanation</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Explainability Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Kaur et al. (2021) [<xref ref-type=\"bibr\" rid=\"CR75\">75</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">India</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Clinical symptom data for diabetic retinopathy classification</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">SHAP</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">95.0%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Wang et al. (2020) [<xref ref-type=\"bibr\" rid=\"CR76\">76</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">USA</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Patient symptoms&#8201;+&#8201;EMR data for glaucoma risk prediction</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">LIME</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92.0%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Li et al. (2022) [<xref ref-type=\"bibr\" rid=\"CR77\">77</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">China</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multi-modal data (symptoms&#8201;+&#8201;imaging) for cataract diagnosis</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Grad-CAM (images), SHAP (symptoms)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.5%</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Eye-XAI (Our work)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bangladesh</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Symptom data from Bangladeshi patients with various eye diseases</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">SHAP</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.11%</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec20\"><title>Experimental setup</title><p id=\"Par72\">To ensure reproducibility of the results of the proposed model, we have detailed the machine learning parameters and hyperparameters used in our approach in Table <xref rid=\"Tab9\" ref-type=\"table\">9</xref>. All models were implemented in Python using either the scikit-learn or Keras libraries. While many default settings and hyperparameters can be directly utilized from these packages, we have explicitly listed the specific hyperparameters employed in our experiments for clarity and transparency.<table-wrap id=\"Tab9\" position=\"float\" orientation=\"portrait\"><label>Table 9</label><caption><p>Machine learning models, packages used, and hyperparameters</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">ML Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Python Package Used</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Information of Hyperparameters</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Naive Bayes (NB)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">var_smoothing&#8201;=&#8201;1e-9 (GaussianNB)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Random Forest (RF)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">n_estimators&#8201;=&#8201;100, max_depth&#8201;=&#8201;None, random_state&#8201;=&#8201;42</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Support Vector Machine (SVM)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">C&#8201;=&#8201;1.0, kernel=&#8217;rbf&#8217;, gamma=&#8217;scale&#8217;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">XGBoost</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">xgboost</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">n_estimators&#8201;=&#8201;100, learning_rate&#8201;=&#8201;0.1, max_depth&#8201;=&#8201;6, subsample&#8201;=&#8201;1.0</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">AdaBoost (AB)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">n_estimators&#8201;=&#8201;50, learning_rate&#8201;=&#8201;1.0,base_estimator&#8201;=&#8201;DecisionTreeClassifier(max_depth&#8201;=&#8201;1)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Bagging</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">n_estimators&#8201;=&#8201;10,base_estimator&#8201;=&#8201;DecisionTreeClassifier(),random_state&#8201;=&#8201;42</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Decision Tree (DT)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">criterion=&#8217;gini&#8217;,max_depth&#8201;=&#8201;None,random_state&#8201;=&#8201;42</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">k-Nearest Neighbors (k-NN)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">n_neighbors&#8201;=&#8201;5,weights=&#8217;uniform&#8217;,metric=&#8217;minkowski&#8217;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Logistic Regression (LR)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">scikit-learn</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">C&#8201;=&#8201;1.0, solver=&#8217;lbfgs&#8217;, max_iter&#8201;=&#8201;1000, random_state&#8201;=&#8201;42</td></tr></tbody></table></table-wrap></p></sec></sec><sec id=\"Sec21\"><title>Discussion</title><p id=\"Par73\">In this section, we have discussed the challenges faced while conducting this study and the possible practical challenges that we may have to face to ensure the integration of AI tools into clinical ophthalmology workflows.</p><sec id=\"Sec22\"><title>Practical challenges in AI integration into clinical ophthalmology workflows</title><p id=\"Par74\">Eye-XAI has been developed with clinical applicability at its core. To address the practical challenges of integration, our framework incorporates explainable AI (XAI) techniques, specifically SHAP (SHapley Additive exPlanations), which allow clinicians to visualize and understand the rationale behind model predictions. By aligning these visual and textual explanations with known clinical symptoms and ophthalmic features, Eye-XAI bridges the gap between AI predictions and clinical reasoning.</p><p id=\"Par75\">Eye-XAI addresses the common criticism of AI as a being &#8220;black box&#8221; tool by integrating explainable AI techniques with symptom-level analyses. Rather than only providing a diagnosis, Eye-XAI offers clear and interpretable justifications for its predictions, highlighting which symptoms or image characteristics contributed the most to a decision. This transparency allows clinicians to trace the diagnostic logic of the model, making it easier to trust and validate the system in clinical practice.</p><p id=\"Par76\">By providing clear and understandable diagnostic insights, it helps ophthalmologists understand the reasoning behind AI-generated recommendations. This transparency allows clinicians to critically evaluate the system&#8217;s suggestions, combine them with their own medical experience, and make more informed decisions. As a result, Eye-XAI builds trust among clinicians and encourages a smooth integration into daily clinical practice.</p><p id=\"Par77\">To further support real-world use, Eye-XAI is designed for easy integration into hospital and clinic systems, such as electronic health records (EHR) , using standard communication tools such as APIs and HL7 FHIR [<xref ref-type=\"bibr\" rid=\"CR78\">78</xref>]. This allows Eye-XAI to deliver decision support directly within existing workflows, without requiring doctors to switch between platforms.</p><p id=\"Par78\">Additionally, Eye-XAI has a flexible architecture. It can be installed locally on hospital computers in settings with limited internet access (on-premise use) or run in the cloud with user-friendly dashboards that explain the model&#8217;s decisions, especially helpful for remote consultations. As a next step, we plan to conduct pilot studies in clinical ophthalmology centers to evaluate the real-world effectiveness and usability of Eye-XAI in everyday medical environments.</p></sec><sec id=\"Sec23\"><title>Future prospects of user feedback and evaluation of interpretability</title><p id=\"Par79\">To evaluate the interpretability and reliability of Eye-XAI, we plan to conduct a preliminary usability study involving 8&#8211;12 ophthalmologists and residents of ophthalmology. Participants will review 5&#8211;7 anonymized patient cases using the Eye-XAI interface, which provides prediction outputs along with visual explanations such as SHAP feature attributions and saliency maps. After a brief tutorial, clinicians will assess how explanations align with or influence their diagnoses. Feedback will be collected through the System Usability Scale (SUS) [<xref ref-type=\"bibr\" rid=\"CR79\">79</xref>] and a custom survey that measures interpretability and trust. In addition, structured interviews will explore participants&#8217; views on the usefulness of explanations for clinical decision making and patient communication. Quantitative data from surveys will be analyzed to measure overall usability and trust levels, while qualitative analysis of interview transcripts will identify key themes regarding the clinical relevance and potential impact of the system. The aim of this study is therefore to demonstrate the effectiveness of Eye-XAI in supporting clinicians, identify areas for improvement, and provide a foundation for future pilot deployments in real clinical settings.</p><p id=\"Par80\">We recognize that SHAP explanations depend on the underlying model and may propagate spurious associations. To mitigate this, we used cross-validation, regularization, and alignment with clinical knowledge, and emphasize that SHAP serves as a decision-support tool rather than stand-alone evidence [<xref ref-type=\"bibr\" rid=\"CR69\">69</xref>, <xref ref-type=\"bibr\" rid=\"CR70\">70</xref>].</p></sec><sec id=\"Sec24\"><title>Plan for external validation</title><p id=\"Par81\">In this study, we have concentrated on geographically localized patients (patients are only from Bangladesh) and it may not fully capture inter-population variations in ocular health indicators. For the experiments presented in this study, both train-test split and 10-fold cross validation techniques has been tested for each of the ML methods. Eventually, external validation may improve the acceptability of the models by clinicians. Therefore, we intend to evaluate Eye-XAI in publicly available ophthalmic data sets from other regions such as the MESSIDOR data sets (France) [<xref ref-type=\"bibr\" rid=\"CR41\">41</xref>] and EyePACS (USA) [<xref ref-type=\"bibr\" rid=\"CR42\">42</xref>]. This cross-population validation will help assess the robustness, fairness, and generalizability of the model between ethnic and demographic groups. To assess the robustness and generalizability of Eye-XAI beyond symptom-based data, we propose external validation using the MESSIDOR and EyePACS fundus image datasets. This adaptation introduces challenges related to the change in data modality (from structured symptoms to raw images), the interpretability of learned visual features, the heterogeneity of the data set and the alignment of labeling systems. By addressing these challenges, Eye-XAI can demonstrate its applicability in both structured clinical data and high-dimensional imaging data, thus ensuring broader clinical relevance and trustworthiness. Furthermore, we will explore domain adaptation techniques to minimize potential performance biases introduced by distributional changes.</p><p id=\"Par82\">Our current Eye-XAI pipeline follows a segmentation<inline-formula id=\"IEq9\"><tex-math id=\"d33e2693\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\rightarrow$$\\end{document}</tex-math></inline-formula>classification workflow designed for transparency and moderate compute (classical UNet-style segmentation of OD/OC or ROI, lightweight CNN/GBM classifiers, SHAP explanations). Recent &#8220;hybrid&#8221; approaches combine stronger segmenters with meta-heuristics or pair them with non-standard classifiers or generators&#8212;e.g., Grey-Wolf&#8211;optimized U-Net/UNet++&#8201;+&#8201;Capsule Networks for glaucoma/DR, and GAN-assisted augmentation&#8201;+&#8201;MobileNetV2 for glaucoma screening. These studies report gains that generally come from (i) finer boundary extraction of OD/OC and lesions (U-Net/UNet++), (ii) robustness of pose/part-whole modeling (CapsNet), and (iii) data distribution smoothing via GANs; but they introduce extra complexity and compute (optimization loops, capsule routing, adversarial training) and raise reproducibility risks across sites/devices.</p></sec><sec id=\"Sec25\"><title>Limitations of the study</title><p id=\"Par83\">Our cohort contains 563 patients. Although this sample is adequate for exploratory benchmarking, it is modest for training and reliably comparing nine models, especially as the outcome/classes are imbalanced; the feature space is high-dimensional, and hyperparameters are extensively tuned. In such settings, estimates can have high variance and model selection can be optimistically biased (the &#8220;winner&#8217;s curse&#8221; [<xref ref-type=\"bibr\" rid=\"CR80\">80</xref>]). There is no universal sample-size rule for ML; practical considerations may apply.</p><p id=\"Par84\">A larger and demographically broader cohort would help stabilize estimates by narrowing confidence intervals, reducing fold-to-fold variability, and lowering the risk of overfitting or model-ranking reversals. It would also improve generalization by exposing the model to a wider range of covariate changes such as age, sex, comorbidities, care pathways, and device differences, thus improving performance in external datasets and mitigating domain change. With more data, stronger and more expressive models could be trained, allowing richer feature engineering and complex interactions without overfitting. In addition, a larger cohort would improve the calibration and clinical utility, leading to better probability calibration (e.g., lower Brier score, expected calibration error), more reliable decision-curve analyses, and subgroup-specific threshold policies. This expansion would also support fairness auditing by allowing well-equipped subgroup analyses across demographic categories and facilitating meaningful bias mitigation. Finally, it would permit true holdout validation through untouched external test sets or multi-site evaluations, thereby minimizing model-selection inflation and strengthening confidence in clinical applicability.</p><p id=\"Par85\">Although retaining all features improved interpretability in this study, it may also introduce risks of redundancy, overfitting, and potential misinterpretation in real-world clinical practice [<xref ref-type=\"bibr\" rid=\"CR81\">81</xref>, <xref ref-type=\"bibr\" rid=\"CR82\">82</xref>]. Redundant variables can amplify noise and make models overly complex, limiting generalization to external datasets [<xref ref-type=\"bibr\" rid=\"CR83\">83</xref>]. To minimize this, we employed rigorous cross-validation and regularized classifiers, but we recognize that a balance must be struck between interpretability and parsimony [<xref ref-type=\"bibr\" rid=\"CR84\">84</xref>]. Future extensions of Eye-XAI will explore stability-driven or clinically guided feature selection strategies, ensuring that the retained features are not only interpretable but also robust, non-redundant, and clinically meaningful [<xref ref-type=\"bibr\" rid=\"CR85\">85</xref>].</p></sec></sec><sec id=\"Sec26\"><title>Conclusions</title><p id=\"Par86\">For any patient, proper detection of a eye disease is a very important and challenging task that informs the appropriate cure and care. Therefore, doctors rely mostly on the symptoms they find on the patients on their first visit. Based on a symptom analysis, it is evident that the most important symptoms that trigger diseases can be found. This paper relies on a data set collected from physicians offices that is used for explainable machine learning methods to find the ranks and adoption of the most important symptoms. With a highly accurate machine learning model (99. 11%), an additional explainability pipeline has been adopted for the study. For future directions, the surveillance of each patient is necessary to understand the outcomes and prognosis of the disease.</p><p id=\"Par87\">One of the limitations of this study that can be highlighted is potential data biases arising from the fact that our dataset was collected primarily from a specific regional population (Bangladesh). We acknowledge that this may limit the model&#8217;s generalizability to other demographic or geographic populations, and we suggest future work involving external validation using datasets from diverse settings. In addition, a potential deployment roadmap for Eye-XAI, highlighting how the model can be integrated into existing ophthalmology clinic workflows was provided. This includes embedding the model within clinical decision support systems, where it can assist ophthalmologists in early diagnosis and feature-based reasoning, while ensuring that its use aligns with clinical guidelines (i.e., Food and Drug Administration (FDA) [<xref ref-type=\"bibr\" rid=\"CR86\">86</xref>], Conformit&#233; Europ&#233;enne (CE) Marking and European MDR [<xref ref-type=\"bibr\" rid=\"CR87\">87</xref>]) and patient data privacy protocols (i.e., Health Insurance Portability and Accountability Act (HIPAA) [<xref ref-type=\"bibr\" rid=\"CR88\">88</xref>], General Data Protection Regulation (GDPR) [<xref ref-type=\"bibr\" rid=\"CR89\">89</xref>]).</p></sec><sec id=\"Sec3644\" sec-type=\"supplementary-material\"><title>Electronic supplementary material</title><p>Below is the link to the electronic supplementary material.</p><p>\n<supplementary-material content-type=\"local-data\" id=\"MOESM1\" position=\"float\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"12911_2025_3253_MOESM1_ESM.pdf\" position=\"float\" orientation=\"portrait\"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>\n</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research has been partially supported by the Information and Communication Technology (ICT) division of the Peoples Republic of Bangladesh. This includes technical and logistics support provided as a form of fellowship to the doctoral student, Ahmed Al Marouf.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>AAM, MMM, JR, RA, and SSR contributed to conceptualization, methodology, writing - original draft and visualization. JR and RA contributed to supervision and project administration. OJ contributed to data curation and validation. AAM contributed to formal analysis.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>No external funding was acquired for this study.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The datasets used and/or analyzed during the current study are available from the corresponding author.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\"><title>Ethics approval and consent to participate</title><p id=\"Par88\">The present study complied with the ethical guidelines of the Declarations of Helsinki of 1975. The informed consent requirement was waived by the Research Ethics Committee of the Faculty of Science and Information Technology and was approved the ethics (Application No: REC-FSIT-20220816001, 08/16/2022).</p></notes><notes id=\"FPar2\"><title>Consent for publication</title><p id=\"Par89\">Patients have provided their written consent to publish the data without any conservation. The anonymity of the data was ensured by the ethics board. None of the participants information, including name, age, photo or any identifiable components has been used or presented in this study.</p></notes><notes id=\"FPar3\" notes-type=\"COI-statement\"><title>Conflict of interest</title><p id=\"Par90\">The authors declare no conflict of interest.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><mixed-citation publication-type=\"other\">Ribeiro MT, Singh S, Guestrin C. Why should i trust you? explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016, pp. 1135&#8211;44.</mixed-citation></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saraswat</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Bhattacharya</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Verma</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Prasad</surname><given-names>VK</given-names></name><name name-style=\"western\"><surname>Tanwar</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Sharma</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Bokoro</surname><given-names>PN</given-names></name><name name-style=\"western\"><surname>Sharma</surname><given-names>R</given-names></name></person-group><article-title>Explainable ai for healthcare 5.0: opportunities and challenges</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>84486</fpage><lpage>517</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2022.3197671</pub-id><pub-id pub-id-type=\"pmcid\">PMC9423030</pub-id><pub-id pub-id-type=\"pmid\">36345376</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Saraswat D, Bhattacharya P, Verma A, Prasad VK, Tanwar S, Sharma G, Bokoro PN, Sharma R. Explainable ai for healthcare 5.0: opportunities and challenges. IEEE Access. 2022;10:84486&#8211;517.</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><mixed-citation publication-type=\"other\">Sutradhar I, Gayen P, Hasan M, Gupta RD, Roy T, Sarker M. Eye diseases: the neglected health condition among urban slum population of dhaka, Bangladesh. BMC Ophthalmol. 2019;19(38). 10.1186/s12886-019-1043-z.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s12886-019-1043-z</pub-id><pub-id pub-id-type=\"pmcid\">PMC6357461</pub-id><pub-id pub-id-type=\"pmid\">30704423</pub-id></mixed-citation></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Marouf</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mottalib</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Alhajj</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Rokne</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Jafarullah</surname><given-names>O</given-names></name></person-group><article-title>An efficient approach to predict eye diseases from symptoms using machine learning and ranker-based feature selection methods</article-title><source>Bioengineering</source><year>2022</year><volume>10</volume><fpage>25</fpage><pub-id pub-id-type=\"doi\">10.3390/bioengineering10010025</pub-id><pub-id pub-id-type=\"pmid\">36671598</pub-id><pub-id pub-id-type=\"pmcid\">PMC9854513</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Marouf A, Mottalib M, Alhajj R, Rokne J, Jafarullah O. An efficient approach to predict eye diseases from symptoms using machine learning and ranker-based feature selection methods. Bioengineering. 2022;10:25.<pub-id pub-id-type=\"pmid\">36671598</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/bioengineering10010025</pub-id><pub-id pub-id-type=\"pmcid\">PMC9854513</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ayodele</surname><given-names>TO</given-names></name></person-group><article-title>Types of machine learning algorithms</article-title><source>New Adv Mach Learn</source><year>2010</year><volume>3</volume><fpage>19</fpage><lpage>48</lpage></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Ayodele TO. Types of machine learning algorithms. New Adv Mach Learn. 2010;3:19&#8211;48.</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mair</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Kadoda</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Lefley</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Phalp</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Schofield</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Shepperd</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Webster</surname><given-names>S</given-names></name></person-group><article-title>An investigation of machine learning based prediction systems</article-title><source>J Syst Softw</source><year>2000</year><volume>53</volume><fpage>23</fpage><lpage>29</lpage><pub-id pub-id-type=\"doi\">10.1016/S0164-1212(00)00005-4</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">Mair C, Kadoda G, Lefley M, Phalp K, Schofield C, Shepperd M, Webster S. An investigation of machine learning based prediction systems. J Syst Softw. 2000;53:23&#8211;29.</mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><citation-alternatives><element-citation id=\"ec-CR7\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mackenzie</surname><given-names>A</given-names></name></person-group><article-title>The production of prediction: what does machine learning want?</article-title><source>Eur J Cult Stud</source><year>2015</year><volume>18</volume><fpage>429</fpage><lpage>45</lpage><pub-id pub-id-type=\"doi\">10.1177/1367549415577384</pub-id></element-citation><mixed-citation id=\"mc-CR7\" publication-type=\"journal\">Mackenzie A. The production of prediction: what does machine learning want? Eur J Cult Stud. 2015;18:429&#8211;45.</mixed-citation></citation-alternatives></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abbas</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Qaisar</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Farooq</surname><given-names>MS</given-names></name><name name-style=\"western\"><surname>Saleem</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Ahmad</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>MA</given-names></name></person-group><article-title>Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence</article-title><source>Sensors</source><year>2024</year><volume>24</volume><issue>20</issue><fpage>6618</fpage><pub-id pub-id-type=\"doi\">10.3390/s24206618</pub-id><pub-id pub-id-type=\"pmid\">39460097</pub-id><pub-id pub-id-type=\"pmcid\">PMC11510864</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Abbas S, Qaisar A, Farooq MS, Saleem M, Ahmad M, Khan MA. Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence. Sensors. 2024;24(20):6618.<pub-id pub-id-type=\"pmid\">39460097</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s24206618</pub-id><pub-id pub-id-type=\"pmcid\">PMC11510864</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gour</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Khanna</surname><given-names>P</given-names></name></person-group><article-title>Multi-class multi-label ophthalmological disease detection using transfer learning based convolutional neural network</article-title><source>Biomed Signal Process Control</source><year>2021</year><volume>66</volume><fpage>102329</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2020.102329</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Gour N, Khanna P. Multi-class multi-label ophthalmological disease detection using transfer learning based convolutional neural network. Biomed Signal Process Control. 2021;66:102329.</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Li C, Ye J, He J, Wang S, Qiao Y, Gu L. Dense correlation network for automated multi-label ocular disease detection with paired color fundus photographs. 2020 IEEE 17th International Symposium On Biomedical Imaging (ISBI). 2020, pp. 1&#8211;4.</mixed-citation></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oda</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Yamaguchi</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Fukuoka</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Ueno</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Mori</surname><given-names>K</given-names></name></person-group><article-title>Automated eye disease classification method from anterior eye image using anatomical structure focused image classification technique</article-title><source>Med Imag 2020: Comput-Aided Diagnosis</source><year>2020</year><volume>11314</volume><fpage>991</fpage><lpage>96</lpage></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Oda M, Yamaguchi T, Fukuoka H, Ueno Y, Mori K. Automated eye disease classification method from anterior eye image using anatomical structure focused image classification technique. Med Imag 2020: Comput-Aided Diagnosis. 2020;11314:991&#8211;96.</mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Qiao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>L</given-names></name></person-group><article-title>Multi-label ocular disease classification with a dense correlation deep neural network</article-title><source>Biomed Signal Process Control</source><year>2021</year><volume>63</volume><fpage>102167</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2020.102167</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">He J, Li C, Ye J, Qiao Y, Gu L. Multi-label ocular disease classification with a dense correlation deep neural network. Biomed Signal Process Control. 2021;63:102167.</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hameed</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Ahmed</surname><given-names>H</given-names></name></person-group><article-title>Eye diseases classification using back propagation with parabola learning rate</article-title><source>Al-Qadisiyah J Pure Sci</source><year>2021</year><volume>26</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type=\"doi\">10.29350/qjps.2021.26.1.1220</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Hameed S, Ahmed H. Eye diseases classification using back propagation with parabola learning rate. Al-Qadisiyah J Pure Sci. 2021;26:1&#8211;9.</mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ahmed</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Hameed</surname><given-names>S</given-names></name></person-group><article-title>Eye diseases classification using back propagation with welch estimation based-learning rate</article-title><source>Al-Qadisiyah J Pure Sci</source><year>2021</year><volume>26</volume><fpage>22</fpage><lpage>30</lpage></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Ahmed H, Hameed S. Eye diseases classification using back propagation with welch estimation based-learning rate. Al-Qadisiyah J Pure Sci. 2021;26:22&#8211;30.</mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Li N, Li T, Hu C, Wang K, Kang H. A benchmark of ocular disease intelligent recognition: one shot for multi-disease detection. Benchmarking, Measuring, and Optimizing: Third BenchCouncil International Symposium, Bench 2020, Virtual Event, November 15&#8211;16, 2020, Revised Selected Papers 3. 2021, pp. 177&#8211;93.</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Qiao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>L</given-names></name></person-group><article-title>Self-speculation of clinical features based on knowledge distillation for accurate ocular disease classification</article-title><source>Biomed Signal Process Control</source><year>2021</year><volume>67</volume><fpage>102491</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2021.102491</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">He J, Li C, Ye J, Qiao Y, Gu L. Self-speculation of clinical features based on knowledge distillation for accurate ocular disease classification. Biomed Signal Process Control. 2021;67:102491.</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bhati</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Gour</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Khanna</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Ojha</surname><given-names>A</given-names></name></person-group><article-title>Discriminative kernel convolution network for multi-label ophthalmic disease detection on imbalanced fundus image dataset</article-title><source>Comput Biol Med</source><year>2023</year><volume>153</volume><fpage>106519</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2022.106519</pub-id><pub-id pub-id-type=\"pmid\">36608462</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Bhati A, Gour N, Khanna P, Ojha A. Discriminative kernel convolution network for multi-label ophthalmic disease detection on imbalanced fundus image dataset. Comput Biol Med. 2023;153:106519.<pub-id pub-id-type=\"pmid\">36608462</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2022.106519</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bitto</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mahmud</surname><given-names>I</given-names></name></person-group><article-title>Multi categorical of common eye disease detect using convolutional neural network: a transfer learning approach</article-title><source>Bull Electr Eng Inf</source><year>2022</year><volume>11</volume><fpage>2378</fpage><lpage>87</lpage><pub-id pub-id-type=\"doi\">10.11591/eei.v11i4.3834</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Bitto A, Mahmud I. Multi categorical of common eye disease detect using convolutional neural network: a transfer learning approach. Bull Electr Eng Inf. 2022;11:2378&#8211;87.</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Emir</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Colak</surname><given-names>E</given-names></name></person-group><article-title>Performance analysis of pretrained convolutional neural network models for ophthalmological disease classification</article-title><source>Arq Bras Oftalmol</source><year>2023</year><volume>87</volume><fpage>2022</fpage><lpage>124</lpage><pub-id pub-id-type=\"doi\">10.5935/0004-2749.2022-0124</pub-id><pub-id pub-id-type=\"pmcid\">PMC11623917</pub-id><pub-id pub-id-type=\"pmid\">39298728</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Emir B, Colak E. Performance analysis of pretrained convolutional neural network models for ophthalmological disease classification. Arq Bras Oftalmol. 2023;87:2022&#8211;124.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.5935/0004-2749.2022-0124</pub-id><pub-id pub-id-type=\"pmcid\">PMC11623917</pub-id><pub-id pub-id-type=\"pmid\">39298728</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><mixed-citation publication-type=\"other\">Kumar D, Bakariya B, Verma C, Illes Z. Cataract disease identification using transformer and convolution neural network: a novel framework. 2023 3rd International Conference On Technological Advancements In Computational Sciences (ICTACS). 2023, pp. 1230&#8211;35.</mixed-citation></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mahmood</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Chaabouni</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Fakhfakh</surname><given-names>A</given-names></name></person-group><article-title>A new technique for cataract eye disease diagnosis in deep learning</article-title><source>Periodicals Eng Nat Sci</source><year>2023</year><volume>11</volume><fpage>14</fpage><lpage>26</lpage><pub-id pub-id-type=\"doi\">10.21533/pen.v11.i6.191</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Mahmood S, Chaabouni S, Fakhfakh A. A new technique for cataract eye disease diagnosis in deep learning. Periodicals Eng Nat Sci. 2023;11:14&#8211;26.</mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">Qaddour M, Touimi Y, Minaoui K. Classification of retinal fundus images using convolution neural network (cnn). 2023 IEEE International Conference on Advances in Data-Driven Analytics and Intelligent Systems (ADACIS). 2023, pp. 1&#8211;7.</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Vengurlekar M, Nadaf M, Fernandes N, Kumar K. Conjunctivitis eye detection using deep learning. 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC). 2024, pp. 1591&#8211;97.</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Jmour N, Zayen S, Abdelkrim A. Convolutional neural networks for image classification. 2018 International Conference on Advanced Systems and Electric Technologies (IC ASET). IEEE; 2018, pp. 397&#8211;402.</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Govindharaj</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Deva Priya</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Soujanya</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Senthilkumar</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Shantha Shalini</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Ravichandran</surname><given-names>S</given-names></name></person-group><article-title>Advanced glaucoma disease segmentation and classification with grey wolf optimized u- net++ and capsule networks</article-title><source>Int Ophthalmol</source><year>2025</year><volume>45</volume><issue>1</issue><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type=\"doi\">10.1007/s10792-025-03602-6</pub-id><pub-id pub-id-type=\"pmid\">40576831</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Govindharaj I, Deva Priya W, Soujanya K, Senthilkumar K, Shantha Shalini K, Ravichandran S. Advanced glaucoma disease segmentation and classification with grey wolf optimized u- net++ and capsule networks. Int Ophthalmol. 2025;45(1):1&#8211;24.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s10792-025-03602-6</pub-id><pub-id pub-id-type=\"pmid\">40576831</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><label>26.</label><citation-alternatives><element-citation id=\"ec-CR26\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Govindharaj</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Ramesh</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Poongodai</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Udayasankaran</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Ravichandran</surname><given-names>S</given-names></name><etal/></person-group><article-title>Grey wolf optimization technique with u-shaped and capsule networks-a novel framework for glaucoma diagnosis</article-title><source>MethodsX</source><year>2025</year><volume>14</volume><fpage>103285</fpage><pub-id pub-id-type=\"doi\">10.1016/j.mex.2025.103285</pub-id><pub-id pub-id-type=\"pmid\">40236793</pub-id><pub-id pub-id-type=\"pmcid\">PMC11999292</pub-id></element-citation><mixed-citation id=\"mc-CR26\" publication-type=\"journal\">Govindharaj I, Ramesh T, Poongodai A, Udayasankaran P, Ravichandran S, et al. Grey wolf optimization technique with u-shaped and capsule networks-a novel framework for glaucoma diagnosis. MethodsX. 2025;14:103285.<pub-id pub-id-type=\"pmid\">40236793</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.mex.2025.103285</pub-id><pub-id pub-id-type=\"pmcid\">PMC11999292</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Govindharaj</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Poongodai</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Santhakumar</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Ravichandran</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Vijaya Prabhu</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Udayakumar</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Yazhinian</surname><given-names>S</given-names></name><etal/></person-group><article-title>Enhanced diabetic retinopathy detection using u-shaped network and capsule network-driven deep learning</article-title><source>MethodsX</source><year>2025</year><volume>14</volume><fpage>103052</fpage><pub-id pub-id-type=\"doi\">10.1016/j.mex.2024.103052</pub-id><pub-id pub-id-type=\"pmid\">39802427</pub-id><pub-id pub-id-type=\"pmcid\">PMC11719411</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Govindharaj I, Poongodai A, Santhakumar D, Ravichandran S, Vijaya Prabhu R, Udayakumar K, Yazhinian S, et al. Enhanced diabetic retinopathy detection using u-shaped network and capsule network-driven deep learning. MethodsX. 2025;14:103052.<pub-id pub-id-type=\"pmid\">39802427</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.mex.2024.103052</pub-id><pub-id pub-id-type=\"pmcid\">PMC11719411</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Govindharaj</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Rampriya</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Michael</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Yazhinian</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Dinesh Kumar</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Anandh</surname><given-names>R</given-names></name></person-group><article-title>Capsule network-based deep learning for early and accurate diabetic retinopathy detection</article-title><source>Int Ophthalmol</source><year>2025</year><volume>45</volume><issue>1</issue><fpage>78</fpage><pub-id pub-id-type=\"doi\">10.1007/s10792-024-03391-4</pub-id><pub-id pub-id-type=\"pmid\">39966199</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Govindharaj I, Rampriya R, Michael G, Yazhinian S, Dinesh Kumar K, Anandh R. Capsule network-based deep learning for early and accurate diabetic retinopathy detection. Int Ophthalmol. 2025;45(1):78.<pub-id pub-id-type=\"pmid\">39966199</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s10792-024-03391-4</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Govindharaj</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Santhakumar</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Pugazharasi</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Ravichandran</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Prabhu</surname><given-names>RV</given-names></name><name name-style=\"western\"><surname>Raja</surname><given-names>J</given-names></name></person-group><article-title>Enhancing glaucoma diagnosis: generative adversarial networks in synthesized imagery and classification with pretrained mobilenetv2</article-title><source>MethodsX</source><year>2025</year><volume>14</volume><fpage>103116</fpage><pub-id pub-id-type=\"doi\">10.1016/j.mex.2024.103116</pub-id><pub-id pub-id-type=\"pmid\">39811622</pub-id><pub-id pub-id-type=\"pmcid\">PMC11732477</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Govindharaj I, Santhakumar D, Pugazharasi K, Ravichandran S, Prabhu RV, Raja J. Enhancing glaucoma diagnosis: generative adversarial networks in synthesized imagery and classification with pretrained mobilenetv2. MethodsX. 2025;14:103116.<pub-id pub-id-type=\"pmid\">39811622</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.mex.2024.103116</pub-id><pub-id pub-id-type=\"pmcid\">PMC11732477</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Singh A, Mohammed A, Zelek J, Lakshminarayanan V. Interpretation of deep learning using attributions: application to ophthalmic diagnosis. Appl Mach Learn. 2020;11511:39&#8211;49.</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oh</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Cho</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>S</given-names></name></person-group><article-title>Explainable machine learning model for glaucoma diagnosis and its interpretation</article-title><source>Diagnostics</source><year>2021</year><volume>11</volume><fpage>510</fpage><pub-id pub-id-type=\"doi\">10.3390/diagnostics11030510</pub-id><pub-id pub-id-type=\"pmid\">33805685</pub-id><pub-id pub-id-type=\"pmcid\">PMC8001225</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Oh S, Park Y, Cho K, Kim S. Explainable machine learning model for glaucoma diagnosis and its interpretation. Diagnostics. 2021;11:510.<pub-id pub-id-type=\"pmid\">33805685</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/diagnostics11030510</pub-id><pub-id pub-id-type=\"pmcid\">PMC8001225</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><mixed-citation publication-type=\"other\">Apon T, Hasan M, Islam A, Alam M. Demystifying deep learning models for retinal oct disease classification using explainable ai. 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). 2021, pp. 1&#8211;6.</mixed-citation></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Obayya</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Nemri</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Nour</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Duhayyim</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Mohsen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Rizwanullah</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Zamani</surname><given-names>AS</given-names></name><name name-style=\"western\"><surname>Motwakel</surname><given-names>A</given-names></name></person-group><article-title>Explainable artificial intelligence enabled teleophthalmology for diabetic retinopathy grading and classification</article-title><source>Appl Sci</source><year>2022</year><volume>12</volume><fpage>8749</fpage><pub-id pub-id-type=\"doi\">10.3390/app12178749</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Obayya M, Nemri N, Nour M, Duhayyim MA, Mohsen H, Rizwanullah M, Zamani AS, Motwakel A. Explainable artificial intelligence enabled teleophthalmology for diabetic retinopathy grading and classification. Appl Sci. 2022;12:8749.</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kamal</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Dey</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Chowdhury</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Hasan</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Santosh</surname><given-names>K</given-names></name></person-group><article-title>Explainable ai for glaucoma prediction analysis to understand risk factors in treatment planning</article-title><source>IEEE Trans Instrum Meas</source><year>2022</year><volume>71</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type=\"doi\">10.1109/TIM.2022.3171613</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Kamal M, Dey N, Chowdhury L, Hasan S, Santosh K. Explainable ai for glaucoma prediction analysis to understand risk factors in treatment planning. IEEE Trans Instrum Meas. 2022;71:1&#8211;9.</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Chayan T, Islam A, Rahman E, Reza M, Apon T, Alam M. Explainable ai based glaucoma detection using transfer learning and lime. 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). 2022, pp. 1&#8211;6.</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><mixed-citation publication-type=\"other\">Shah H, Patel R, Hegde S, Dalvi H. Xai meets ophthalmology: an explainable approach to cataract detection using vgg-19 and grad-cam. 2023 IEEE Pune Section International Conference (PuneCon). 2023, pp. 1&#8211;8.</mixed-citation></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Yonehara M, Nakagawa Y, Ayatsuka Y, Hara Y, Shoji J, Ebihara N, Inomata T, Huang T, Nagino K, Fukuda K. Others: use of explainable ai on slit-lamp images of anterior surface of eyes to diagnose allergic conjunctival diseases. Allergology Int. 2024.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.alit.2024.07.004</pub-id><pub-id pub-id-type=\"pmid\">39155213</pub-id></mixed-citation></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Shahzad T, Saleem M, Farooq M, Abbas S, Khan M, Ouahada K. Developing a transparent diagnosis model for diabetic retinopathy using explainable ai. IEEE Access. 2024.</mixed-citation></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abbas</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Qaisar</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Farooq</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Saleem</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Ahmad</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>MA</given-names></name></person-group><article-title>Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence</article-title><source>Sensors</source><year>2024</year><volume>24</volume><fpage>6618</fpage><pub-id pub-id-type=\"doi\">10.3390/s24206618</pub-id><pub-id pub-id-type=\"pmid\">39460097</pub-id><pub-id pub-id-type=\"pmcid\">PMC11510864</pub-id></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Abbas S, Qaisar A, Farooq M, Saleem M, Ahmad M, Khan MA. Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence. Sensors. 2024;24:6618.<pub-id pub-id-type=\"pmid\">39460097</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s24206618</pub-id><pub-id pub-id-type=\"pmcid\">PMC11510864</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR40\"><label>40.</label><mixed-citation publication-type=\"other\">Mridha K, Wang M, Zhang L. Ai-driven diagnostics in ophthalmology: tailored deep learning models for diabetic retinopathy with xai insights. Proceedings of the 16th International Conference on, vol. 101, 2024; p. 73&#8211;82.</mixed-citation></ref><ref id=\"CR41\"><label>41.</label><citation-alternatives><element-citation id=\"ec-CR41\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Decenciere</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Cazuguel</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Lay</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Cochener</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Trone</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Klein</surname><given-names>J-C</given-names></name></person-group><article-title>Feedback on a publicly distributed image database: the messidor database</article-title><source>Image Anal Stereol</source><year>2014</year><volume>33</volume><issue>3</issue><fpage>231</fpage><lpage>34</lpage><pub-id pub-id-type=\"doi\">10.5566/ias.1155</pub-id></element-citation><mixed-citation id=\"mc-CR41\" publication-type=\"journal\">Decenciere E, Zhang X, Cazuguel G, Lay B, Cochener B, Trone C, Klein J-C. Feedback on a publicly distributed image database: the messidor database. Image Anal Stereol. 2014;33(3):231&#8211;34. 10.5566/ias.1155.</mixed-citation></citation-alternatives></ref><ref id=\"CR42\"><label>42.</label><citation-alternatives><element-citation id=\"ec-CR42\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gulshan</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Coram</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Stumpe</surname><given-names>MC</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Narayanaswamy</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Venugopalan</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Widner</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Madams</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Cuadros</surname><given-names>J</given-names></name><etal/></person-group><article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title><source>JAMA</source><year>2016</year><volume>316</volume><issue>22</issue><fpage>2402</fpage><lpage>10</lpage><pub-id pub-id-type=\"doi\">10.1001/jama.2016.17216</pub-id><pub-id pub-id-type=\"pmid\">27898976</pub-id></element-citation><mixed-citation id=\"mc-CR42\" publication-type=\"journal\">Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, Venugopalan S, Widner K, Madams T, Cuadros J, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA. 2016;316(22):2402&#8211;10. 10.1001/jama.2016.17216.<pub-id pub-id-type=\"pmid\">27898976</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1001/jama.2016.17216</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Myles</surname><given-names>AJ</given-names></name><name name-style=\"western\"><surname>Feudale</surname><given-names>RN</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Woody</surname><given-names>NA</given-names></name><name name-style=\"western\"><surname>Brown</surname><given-names>SD</given-names></name></person-group><article-title>An introduction to decision tree modeling</article-title><source>J Educ Chang Chemom: A J Chemom Soc</source><year>2004</year><volume>18</volume><issue>6</issue><fpage>275</fpage><lpage>85</lpage></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Myles AJ, Feudale RN, Liu Y, Woody NA, Brown SD. An introduction to decision tree modeling. J Educ Chang Chemom: A J Chemom Soc. 2004;18(6):275&#8211;85.</mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Rish I. An empirical study of the naive bayes classifier. IJCAI 2001Workshop on Empirical Methods in Artificial Intelligence. 2001, pp. 41&#8211;46, vol. 3.</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><citation-alternatives><element-citation id=\"ec-CR45\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>Mach Learn</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type=\"doi\">10.1023/A:1010933404324</pub-id></element-citation><mixed-citation id=\"mc-CR45\" publication-type=\"journal\">Breiman L. Random forests. Mach Learn. 2001;45:5&#8211;32.</mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><label>46.</label><citation-alternatives><element-citation id=\"ec-CR46\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kurichina</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Duin</surname><given-names>RPW</given-names></name></person-group><article-title>Bagging for linear classifiers</article-title><source>Pattern Recognit</source><year>1998</year><volume>31</volume><issue>7</issue><fpage>909</fpage><lpage>30</lpage><pub-id pub-id-type=\"doi\">10.1016/S0031-3203(97)00110-6</pub-id></element-citation><mixed-citation id=\"mc-CR46\" publication-type=\"journal\">Kurichina M, Duin RPW. Bagging for linear classifiers. Pattern Recognit. 1998;31(7):909&#8211;30.</mixed-citation></citation-alternatives></ref><ref id=\"CR47\"><label>47.</label><mixed-citation publication-type=\"other\">An T-K, Kim M-H. A new diverse adaboost classifier. 2010 International Conference on Artificial Intelligence and Computational Intelligence, vol. 1. IEEE. 2010; p. 359&#8211;63.</mixed-citation></ref><ref id=\"CR48\"><label>48.</label><mixed-citation publication-type=\"other\">Chen T, Guestrin C. Xgboost: a scalable tree boosting system. Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining. 2016; p. 785&#8211;94.</mixed-citation></ref><ref id=\"CR49\"><label>49.</label><citation-alternatives><element-citation id=\"ec-CR49\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zong</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>R</given-names></name></person-group><article-title>Efficient knn classification with different numbers of nearest neighbors</article-title><source>IEEE Trans Neural Networks Learn Syst</source><year>2017</year><volume>29</volume><issue>5</issue><fpage>1774</fpage><lpage>85</lpage><pub-id pub-id-type=\"doi\">10.1109/TNNLS.2017.2673241</pub-id><pub-id pub-id-type=\"pmid\">28422666</pub-id></element-citation><mixed-citation id=\"mc-CR49\" publication-type=\"journal\">Zhang S, Li X, Zong M, Zhu X, Wang R. Efficient knn classification with different numbers of nearest neighbors. IEEE Trans Neural Networks Learn Syst. 2017;29(5):1774&#8211;85.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNNLS.2017.2673241</pub-id><pub-id pub-id-type=\"pmid\">28422666</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR50\"><label>50.</label><citation-alternatives><element-citation id=\"ec-CR50\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>LaValley</surname><given-names>MP</given-names></name></person-group><article-title>Logistic regression</article-title><source>Circulation</source><year>2008</year><volume>117</volume><issue>18</issue><fpage>2395</fpage><lpage>99</lpage><pub-id pub-id-type=\"doi\">10.1161/CIRCULATIONAHA.106.682658</pub-id><pub-id pub-id-type=\"pmid\">18458181</pub-id></element-citation><mixed-citation id=\"mc-CR50\" publication-type=\"journal\">LaValley MP. Logistic regression. Circulation. 2008;117(18):2395&#8211;99.<pub-id pub-id-type=\"pmid\">18458181</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1161/CIRCULATIONAHA.106.682658</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR51\"><label>51.</label><mixed-citation publication-type=\"other\">Cortes C. Support-vector networks. Mach Learn. 1995.</mixed-citation></ref><ref id=\"CR52\"><label>52.</label><citation-alternatives><element-citation id=\"ec-CR52\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kourou</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Exarchos</surname><given-names>TP</given-names></name><name name-style=\"western\"><surname>Exarchos</surname><given-names>KP</given-names></name><name name-style=\"western\"><surname>Karamouzis</surname><given-names>MV</given-names></name><name name-style=\"western\"><surname>Fotiadis</surname><given-names>DI</given-names></name></person-group><article-title>Machine learning applications in cancer prognosis and prediction</article-title><source>Comput Struct Biotechnol J</source><year>2015</year><volume>13</volume><fpage>8</fpage><lpage>17</lpage><pub-id pub-id-type=\"doi\">10.1016/j.csbj.2014.11.005</pub-id><pub-id pub-id-type=\"pmid\">25750696</pub-id><pub-id pub-id-type=\"pmcid\">PMC4348437</pub-id></element-citation><mixed-citation id=\"mc-CR52\" publication-type=\"journal\">Kourou K, Exarchos TP, Exarchos KP, Karamouzis MV, Fotiadis DI. Machine learning applications in cancer prognosis and prediction. Comput Struct Biotechnol J. 2015;13:8&#8211;17.<pub-id pub-id-type=\"pmid\">25750696</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.csbj.2014.11.005</pub-id><pub-id pub-id-type=\"pmcid\">PMC4348437</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR53\"><label>53.</label><mixed-citation publication-type=\"other\">Ahmad MA, Eckert C, Teredesai A. Interpretable machine learning in healthcare. Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. 2018; p. 559&#8211;60.</mixed-citation></ref><ref id=\"CR54\"><label>54.</label><citation-alternatives><element-citation id=\"ec-CR54\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pearson</surname><given-names>K</given-names></name></person-group><article-title>On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</article-title><source>Philos Mag</source><year>1900</year><volume>5</volume><fpage>157</fpage><lpage>75</lpage><pub-id pub-id-type=\"doi\">10.1080/14786440009463897</pub-id></element-citation><mixed-citation id=\"mc-CR54\" publication-type=\"journal\">Pearson K. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philos Mag. 1900;5:157&#8211;75. 10.1080/14786440009463897</mixed-citation></citation-alternatives></ref><ref id=\"CR55\"><label>55.</label><citation-alternatives><element-citation id=\"ec-CR55\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Forman</surname><given-names>G</given-names></name></person-group><article-title>An extensive empirical study of feature selection metrics for text classification</article-title><source>J Mach Learn Res</source><year>2003</year><volume>3</volume><fpage>1289</fpage><lpage>305</lpage></element-citation><mixed-citation id=\"mc-CR55\" publication-type=\"journal\">Forman G. An extensive empirical study of feature selection metrics for text classification. J Mach Learn Res. 2003;3:1289&#8211;305.</mixed-citation></citation-alternatives></ref><ref id=\"CR56\"><label>56.</label><mixed-citation publication-type=\"other\">Gao Z, Xu Y, Meng F, Qi F, Lin L. Improved information gain-based feature selection for text categorization. 2014 4th International Conference on Wireless Communications, Vehicular Technology, Information Theory and Aerospace &amp; Electronic Systems (VITAE). 2014, pp. 11&#8211;14.</mixed-citation></ref><ref id=\"CR57\"><label>57.</label><mixed-citation publication-type=\"other\">Yu L, Liu H. Feature selection for high-dimensional data: a fast correlationbased filter solution. Proceedings of the 20th International Conference on Machine Learning (ICML-03). 2003; p. 856&#8211;63.</mixed-citation></ref><ref id=\"CR58\"><label>58.</label><citation-alternatives><element-citation id=\"ec-CR58\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pearson</surname><given-names>K</given-names></name></person-group><article-title>Liii. On lines and planes of closest fit to systems of points in space</article-title><source>Lond Edinb Dublin Philos Mag J Sci</source><year>1901</year><volume>2</volume><fpage>559</fpage><lpage>72</lpage><pub-id pub-id-type=\"doi\">10.1080/14786440109462720</pub-id></element-citation><mixed-citation id=\"mc-CR58\" publication-type=\"journal\">Pearson K. Liii. On lines and planes of closest fit to systems of points in space. Lond Edinb Dublin Philos Mag J Sci. 1901;2:559&#8211;72.</mixed-citation></citation-alternatives></ref><ref id=\"CR59\"><label>59.</label><citation-alternatives><element-citation id=\"ec-CR59\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abdi</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Williams</surname><given-names>LJ</given-names></name></person-group><article-title>Principal component analysis</article-title><source>Wiley Interdiscip Rev Comput Stat</source><year>2010</year><volume>2</volume><fpage>433</fpage><lpage>59</lpage><pub-id pub-id-type=\"doi\">10.1002/wics.101</pub-id></element-citation><mixed-citation id=\"mc-CR59\" publication-type=\"journal\">Abdi H, Williams LJ. Principal component analysis. Wiley Interdiscip Rev Comput Stat. 2010;2:433&#8211;59.</mixed-citation></citation-alternatives></ref><ref id=\"CR60\"><label>60.</label><mixed-citation publication-type=\"other\">Song F, Guo Z, Mei D. Feature selection using principal component analysis. 2010 International Conference on System Science, Engineering Design and Manufacturing Informatization. 2010; p. 27&#8211;30.</mixed-citation></ref><ref id=\"CR61\"><label>61.</label><citation-alternatives><element-citation id=\"ec-CR61\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kira</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Rendell</surname><given-names>LA</given-names></name></person-group><article-title>The feature selection problem: traditional methods and a new algorithm</article-title><source>AAAI</source><year>1992</year><volume>2</volume><fpage>129</fpage><lpage>34</lpage></element-citation><mixed-citation id=\"mc-CR61\" publication-type=\"journal\">Kira K, Rendell LA. The feature selection problem: traditional methods and a new algorithm. AAAI. 1992;2:129&#8211;34.</mixed-citation></citation-alternatives></ref><ref id=\"CR62\"><label>62.</label><citation-alternatives><element-citation id=\"ec-CR62\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abraham</surname><given-names>MT</given-names></name><name name-style=\"western\"><surname>Satyam</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Lokesh</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Pradhan</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Alamri</surname><given-names>A</given-names></name></person-group><article-title>Factors affecting landslide susceptibility mapping: assessing the influence of different machine learning approaches, sampling strategies and data splitting</article-title><source>Land</source><year>2021</year><volume>10</volume><fpage>989</fpage><pub-id pub-id-type=\"doi\">10.3390/land10090989</pub-id></element-citation><mixed-citation id=\"mc-CR62\" publication-type=\"journal\">Abraham MT, Satyam N, Lokesh R, Pradhan B, Alamri A. Factors affecting landslide susceptibility mapping: assessing the influence of different machine learning approaches, sampling strategies and data splitting. Land. 2021;10:989.</mixed-citation></citation-alternatives></ref><ref id=\"CR63\"><label>63.</label><citation-alternatives><element-citation id=\"ec-CR63\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Refaeilzadeh</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name></person-group><article-title>Cross-validation</article-title><source>Encycl Database Syst</source><year>2009</year><volume>5</volume><fpage>532</fpage><lpage>38</lpage><pub-id pub-id-type=\"doi\">10.1007/978-0-387-39940-9_565</pub-id></element-citation><mixed-citation id=\"mc-CR63\" publication-type=\"journal\">Refaeilzadeh P, Tang L, Liu H. Cross-validation. Encycl Database Syst. 2009;5:532&#8211;38.</mixed-citation></citation-alternatives></ref><ref id=\"CR64\"><label>64.</label><mixed-citation publication-type=\"other\">Lundberg S. Shap (shapley additive explanations). Computer software. Github Repository. 2019. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/slundberg/shap\">https://github.com/slundberg/shap</ext-link>.</mixed-citation></ref><ref id=\"CR65\"><label>65.</label><mixed-citation publication-type=\"other\">Lundberg S. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874. 2017.</mixed-citation></ref><ref id=\"CR66\"><label>66.</label><mixed-citation publication-type=\"other\">Molnar C. Interpretable machine learning: a guide for making black box models explainable. 2nd edn. Leanpub; 2020. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://doi.org/christophm.github.io/interpretable-ml-book/.\">https://doi.org/christophm.github.io/interpretable-ml-book/.</ext-link></mixed-citation></ref><ref id=\"CR67\"><label>67.</label><mixed-citation publication-type=\"other\">Ribeiro MT, Singh S, Guestrin C. &#8220;Why should i trust you?&#8221;: explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM; 2016, pp. 1135&#8211;44.</mixed-citation></ref><ref id=\"CR68\"><label>68.</label><mixed-citation publication-type=\"other\">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: visual explanations from deep networks via gradient-based localization. Proceedings of the IEEE International Conference on Computer Vision (ICCV). 2017, pp. 618&#8211;26.</mixed-citation></ref><ref id=\"CR69\"><label>69.</label><citation-alternatives><element-citation id=\"ec-CR69\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Carvalho</surname><given-names>DV</given-names></name><name name-style=\"western\"><surname>Pereira</surname><given-names>EM</given-names></name><name name-style=\"western\"><surname>Cardoso</surname><given-names>JS</given-names></name></person-group><article-title>Machine learning interpretability: a survey on methods and metrics</article-title><source>Electronics</source><year>2019</year><volume>8</volume><issue>8</issue><fpage>832</fpage><pub-id pub-id-type=\"doi\">10.3390/electronics8080832</pub-id></element-citation><mixed-citation id=\"mc-CR69\" publication-type=\"journal\">Carvalho DV, Pereira EM, Cardoso JS. Machine learning interpretability: a survey on methods and metrics. Electronics. 2019;8(8):832.</mixed-citation></citation-alternatives></ref><ref id=\"CR70\"><label>70.</label><citation-alternatives><element-citation id=\"ec-CR70\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rasheed</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Qayyum</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Qadir</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Sivathamboo</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Kwan</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Kuhlmann</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>O&#8217;Brien</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Razi</surname><given-names>A</given-names></name></person-group><article-title>Explainable artificial intelligence for healthcare: from black box to interpretable models</article-title><source>Neural Comput Appl</source><year>2022</year><volume>34</volume><issue>16</issue><fpage>13371</fpage><lpage>406</lpage></element-citation><mixed-citation id=\"mc-CR70\" publication-type=\"journal\">Rasheed F, Qayyum A, Qadir J, Sivathamboo S, Kwan P, Kuhlmann L, O&#8217;Brien T, Razi A. Explainable artificial intelligence for healthcare: from black box to interpretable models. Neural Comput Appl. 2022;34(16):13371&#8211;406.</mixed-citation></citation-alternatives></ref><ref id=\"CR71\"><label>71.</label><citation-alternatives><element-citation id=\"ec-CR71\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Marouf</surname><given-names>AA</given-names></name><name name-style=\"western\"><surname>Hasan</surname><given-names>MK</given-names></name><name name-style=\"western\"><surname>Mahmud</surname><given-names>H</given-names></name></person-group><article-title>Comparative analysis of feature selection algorithms for computational personality prediction from social media</article-title><source>IEEE Trans Comput Soc Syst</source><year>2020</year><volume>7</volume><fpage>587</fpage><lpage>99</lpage><pub-id pub-id-type=\"doi\">10.1109/TCSS.2020.2966910</pub-id></element-citation><mixed-citation id=\"mc-CR71\" publication-type=\"journal\">Marouf AA, Hasan MK, Mahmud H. Comparative analysis of feature selection algorithms for computational personality prediction from social media. IEEE Trans Comput Soc Syst. 2020;7:587&#8211;99. 10.1109/TCSS.2020.2966910.</mixed-citation></citation-alternatives></ref><ref id=\"CR72\"><label>72.</label><citation-alternatives><element-citation id=\"ec-CR72\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ghosh</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Azam</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Jonkman</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Karim</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Shamrat</surname><given-names>FJM</given-names></name><name name-style=\"western\"><surname>Ignatious</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Shultana</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Beeravolu</surname><given-names>AR</given-names></name><name name-style=\"western\"><surname>Boer</surname><given-names>FD</given-names></name></person-group><article-title>Efficient prediction of cardiovascular disease using machine learning algorithms with relief and lasso feature selection techniques</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>19304</fpage><lpage>26</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2021.3053759</pub-id></element-citation><mixed-citation id=\"mc-CR72\" publication-type=\"journal\">Ghosh P, Azam S, Jonkman M, Karim A, Shamrat FJM, Ignatious E, Shultana S, Beeravolu AR, Boer FD. Efficient prediction of cardiovascular disease using machine learning algorithms with relief and lasso feature selection techniques. IEEE Access. 2021;9:19304&#8211;26. 10.1109/ACCESS.2021.3053759.</mixed-citation></citation-alternatives></ref><ref id=\"CR73\"><label>73.</label><mixed-citation publication-type=\"other\">Chen T, Contributors X. Xgboost documentation. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://xgboost.readthedocs.io/en/stable/python/\">https://xgboost.readthedocs.io/en/stable/python/</ext-link>. Accessed on 2 February 2024.</mixed-citation></ref><ref id=\"CR74\"><label>74.</label><citation-alternatives><element-citation id=\"ec-CR74\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Marques-Silva</surname><given-names>J</given-names></name></person-group><article-title>On the failings of shapley values for explainability</article-title><source>Int J Approximate Reasoning</source><year>2024</year><volume>171</volume><fpage>109112</fpage><pub-id pub-id-type=\"doi\">10.1016/j.ijar.2023.109112</pub-id></element-citation><mixed-citation id=\"mc-CR74\" publication-type=\"journal\">Huang X, Marques-Silva J. On the failings of shapley values for explainability. Int J Approximate Reasoning. 2024;171:109112.</mixed-citation></citation-alternatives></ref><ref id=\"CR75\"><label>75.</label><citation-alternatives><element-citation id=\"ec-CR75\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kaur</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Singh</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Sharma</surname><given-names>P</given-names></name></person-group><article-title>Explainable ai for diabetic retinopathy detection using clinical symptoms</article-title><source>J Med Imag Health Inf</source><year>2021</year><volume>11</volume><issue>3</issue><fpage>543</fpage><lpage>51</lpage></element-citation><mixed-citation id=\"mc-CR75\" publication-type=\"journal\">Kaur S, Singh R, Sharma P. Explainable ai for diabetic retinopathy detection using clinical symptoms. J Med Imag Health Inf. 2021;11(3):543&#8211;51.</mixed-citation></citation-alternatives></ref><ref id=\"CR76\"><label>76.</label><mixed-citation publication-type=\"other\">Wang Y, Chen L, Zhang R. Interpretable glaucoma risk prediction using electronic medical records and patient symptoms. Proceedings of the IEEE International Conference on Healthcare Informatics. IEEE; 2020, pp. 212&#8211;18.</mixed-citation></ref><ref id=\"CR77\"><label>77.</label><citation-alternatives><element-citation id=\"ec-CR77\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>W</given-names></name></person-group><article-title>A multi-modal explainable ai framework for cataract diagnosis combining imaging and symptom data</article-title><source>Comput Biol Med</source><year>2022</year><volume>140</volume><fpage>105123</fpage></element-citation><mixed-citation id=\"mc-CR77\" publication-type=\"journal\">Li X, Zhou M, Huang W. A multi-modal explainable ai framework for cataract diagnosis combining imaging and symptom data. Comput Biol Med. 2022;140:105123.</mixed-citation></citation-alternatives></ref><ref id=\"CR78\"><label>78.</label><mixed-citation publication-type=\"other\">Bender D, Sartipi K. Hl7 fhir: an agile and restful approach to healthcare information exchange. Proceedings of the 26th IEEE International Symposium on Computer-based Medical Systems. IEEE; 2013, pp. 326&#8211;31.</mixed-citation></ref><ref id=\"CR79\"><label>79.</label><citation-alternatives><element-citation id=\"ec-CR79\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brooke</surname><given-names>J</given-names></name></person-group><article-title>Sus: a &#8220;quick and dirty&#8221; usability scale</article-title><source>Usability Evaluation Ind</source><year>1996</year><volume>189</volume><issue>194</issue><fpage>4</fpage><lpage>7</lpage></element-citation><mixed-citation id=\"mc-CR79\" publication-type=\"journal\">Brooke J. Sus: a &#8220;quick and dirty&#8221; usability scale. Usability Evaluation Ind. 1996;189(194):4&#8211;7.</mixed-citation></citation-alternatives></ref><ref id=\"CR80\"><label>80.</label><citation-alternatives><element-citation id=\"ec-CR80\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sculley</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Snoek</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Wiltschko</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Rahimi</surname><given-names>A</given-names></name></person-group><article-title>Winner&#8217;s curse</article-title><source>On Pace, Prog Empir Rigor</source><year>2018</year><volume>1</volume><fpage>1285</fpage><lpage>98</lpage></element-citation><mixed-citation id=\"mc-CR80\" publication-type=\"journal\">Sculley D, Snoek J, Wiltschko A, Rahimi A. Winner&#8217;s curse. On Pace, Prog Empir Rigor. 2018;1:1285&#8211;98.</mixed-citation></citation-alternatives></ref><ref id=\"CR81\"><label>81.</label><citation-alternatives><element-citation id=\"ec-CR81\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guyon</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Elisseeff</surname><given-names>A</given-names></name></person-group><article-title>An introduction to variable and feature selection</article-title><source>J Mach Learn Res</source><year>2003</year><volume>3</volume><fpage>1157</fpage><lpage>82</lpage></element-citation><mixed-citation id=\"mc-CR81\" publication-type=\"journal\">Guyon I, Elisseeff A. An introduction to variable and feature selection. J Mach Learn Res. 2003;3:1157&#8211;82.</mixed-citation></citation-alternatives></ref><ref id=\"CR82\"><label>82.</label><citation-alternatives><element-citation id=\"ec-CR82\" publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kuhn</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Johnson</surname><given-names>K</given-names></name></person-group><source>Applied predictive modeling</source><year>2013</year><publisher-name>Springer</publisher-name></element-citation><mixed-citation id=\"mc-CR82\" publication-type=\"book\">Kuhn M, Johnson K. Applied predictive modeling. Springer; 2013.</mixed-citation></citation-alternatives></ref><ref id=\"CR83\"><label>83.</label><citation-alternatives><element-citation id=\"ec-CR83\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saeys</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Inza</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Larra&#241;aga</surname><given-names>P:</given-names></name></person-group><article-title>A review of feature selection techniques in bioinformatics</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>19</issue><fpage>2507</fpage><lpage>17</lpage><pub-id pub-id-type=\"doi\">10.1093/bioinformatics/btm344</pub-id><pub-id pub-id-type=\"pmid\">17720704</pub-id></element-citation><mixed-citation id=\"mc-CR83\" publication-type=\"journal\">Saeys Y, Inza I, Larra&#241;aga P. A review of feature selection techniques in bioinformatics. Bioinformatics. 2007;23(19):2507&#8211;17.<pub-id pub-id-type=\"pmid\">17720704</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/bioinformatics/btm344</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR84\"><label>84.</label><citation-alternatives><element-citation id=\"ec-CR84\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rudin</surname><given-names>C</given-names></name></person-group><article-title>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</article-title><source>Nat Mach Intell</source><year>2019</year><volume>1</volume><issue>5</issue><fpage>206</fpage><lpage>15</lpage><pub-id pub-id-type=\"doi\">10.1038/s42256-019-0048-x</pub-id><pub-id pub-id-type=\"pmid\">35603010</pub-id><pub-id pub-id-type=\"pmcid\">PMC9122117</pub-id></element-citation><mixed-citation id=\"mc-CR84\" publication-type=\"journal\">Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell. 2019;1(5):206&#8211;15.<pub-id pub-id-type=\"pmid\">35603010</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s42256-019-0048-x</pub-id><pub-id pub-id-type=\"pmcid\">PMC9122117</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR85\"><label>85.</label><citation-alternatives><element-citation id=\"ec-CR85\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Meinshausen</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>B&#252;hlmann</surname><given-names>P</given-names></name></person-group><article-title>Stability selection</article-title><source>J Retailing R Stat Soc: Ser B (Stat Methodol)</source><year>2010</year><volume>72</volume><issue>4</issue><fpage>417</fpage><lpage>73</lpage><pub-id pub-id-type=\"doi\">10.1111/j.1467-9868.2010.00740.x</pub-id></element-citation><mixed-citation id=\"mc-CR85\" publication-type=\"journal\">Meinshausen N, B&#252;hlmann P. Stability selection. J Retailing R Stat Soc: Ser B (Stat Methodol). 2010;72(4):417&#8211;73.</mixed-citation></citation-alternatives></ref><ref id=\"CR86\"><label>86.</label><mixed-citation publication-type=\"other\">Food U. Drug administration (fda). proposed regulatory framework for modifications to artificial intelligence. Machine learning (AI/ML)-based software as a medical device (SaMD)&#8212;discussion paper and request for feedback (US food &amp; drug administration (FDA), 2019). 2021.</mixed-citation></ref><ref id=\"CR87\"><label>87.</label><mixed-citation publication-type=\"other\">Vila&#231;a H. Regulation eu 2017/745 on medical devices&#8211;implementation analysis in Portugal by the distributors. Eur J Criminol Public Health. 2020;30(Supplement 5):166&#8211;1214.</mixed-citation></ref><ref id=\"CR88\"><label>88.</label><mixed-citation publication-type=\"other\">Rule RC. Us department of health &amp; human services. 2022. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.hhs.gov/ohrp/regulations-and-policy/regulations/finalized-revisions-commonrule/index.html\">https://www.hhs.gov/ohrp/regulations-and-policy/regulations/finalized-revisions-commonrule/index.html</ext-link>. Accessed 04 Oct 2022.</mixed-citation></ref><ref id=\"CR89\"><label>89.</label><citation-alternatives><element-citation id=\"ec-CR89\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Regulation</surname><given-names>P</given-names></name></person-group><article-title>General data protection regulation</article-title><source>Intouch</source><year>2018</year><volume>25</volume><fpage>1</fpage><lpage>5</lpage></element-citation><mixed-citation id=\"mc-CR89\" publication-type=\"journal\">Regulation P. General data protection regulation. Intouch. 2018;25:1&#8211;5.</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc BMC Med Inform Decis Mak BMC Med Inform Decis Mak 42 bmcmidm BMC Medical Informatics and Decision Making 1472-6947 BMC PMC12659348 PMC12659348.1 12659348 12659348 41310584 10.1186/s12911-025-03253-8 3253 1 Research Eye-XAI: an explainable artificial intelligence approach for eye disease detection using symptom analysis Marouf Ahmed Al ahmedal.marouf@ucalgary.ca 1 Mottalib Md Mozaharul 2 Ridi Sadia Sobhana 3 Jafarullah Omar 4 Rokne Jon 1 Alhajj Reda alhajj@ucalgary.ca 1 5 6 1 https://ror.org/03yjb2x39 grid.22072.35 0000 0004 1936 7697 Department of Computer Science, University of Calgary, 2500, University Drive, Calgary, Alberta T2N 1N4 Canada 2 https://ror.org/01sbq1a82 grid.33489.35 0000 0001 0454 4791 Department of Computer and Information Sciences, University of Delaware, 18 Amstel Ave, Newark, Delaware DE 19716 USA 3 https://ror.org/00sge8677 grid.52681.38 0000 0001 0746 8691 Department of Computer Science and Engineering, Brac University, 224 Bir Uttam Rafiqul Islam Ave, Dhaka, 1212 Bangladesh 4 https://ror.org/003346y56 Ispahani Islamia Eye Institute and Hospital, 116/C/2, Monipuripara, Farmgate, Dhaka, 1215 Bangladesh 5 https://ror.org/037jwzz50 grid.411781.a 0000 0004 0471 9346 Department of Computer Engineering, Istanbul Medipol University, Kavac&#305;k Mah. Ekinciler, Istanbul, 34810 Turkey 6 https://ror.org/03yrrjy16 grid.10825.3e 0000 0001 0728 0170 Department of Health Informatics, University of Southern Denmark, 55, Campusvej, Odense, 5230 Denmark 27 11 2025 2025 25 478029 433 11 2 2025 15 10 2025 27 11 2025 28 11 2025 29 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . The early and accurate detection of eye diseases play a pivotal role in preventing vision loss and improving patients&#8217; quality of life. It is therefore important to search for methods for improving this detection. It turns out that Artificial Intelligence (AI) has shown great promise for the detection task. However, AI models are often opaque and complex and as a result there has been a slow adoption for these models in clinical settings. In this paper Eye-XAI is introduced which provides an effective approach to the detection combining Explainable Artificial Intelligence (XAI) techniques with symptom analysis to enhance the transparency and interpretability of eye disease detection models. Our results demonstrate that Eye-XAI not only achieves high accuracy (99.11%) for eye disease detection but also provides transparent and interpretable insights into the diagnostic process. The adoption of Eye-XAI can therefore significantly enhance the early detection and management of eye diseases while empowering clinicians with a deeper understanding of its AI-based diagnostic recommendations. Furthermore, this approach promotes patient engagement by facilitating communication and trust between patients and their healthcare providers. Eye-XAI represents a major step towards the integration of XAI in ophthalmology, unlocking new possibilities for improved eye disease diagnosis and treatment. Supplementary information The online version contains supplementary material available at 10.1186/s12911-025-03253-8. Keywords Eye disease Explainable artificial intelligence Machine learning Symptom analysis XGBoost SHAP pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; BioMed Central Ltd., part of Springer Nature 2025 Introduction In the ever-evolving landscape of medical diagnostics, the integration of Explainable Artificial Intelligence (XAI) has emerged as a transformative approach, particularly in the realm of eye disease management. The exponential growth of AI applications in ophthalmology has, however, been accompanied by the challenge of translating these advancements into clinical practice. Hence as AI systems continue to demonstrate human-level or even superior performance in tasks such as diagnosis of diabetic retinopathy, the need for transparent and interpretable decision-making processes has therefore become important for its adaption by eye professionals [ 1 ]. One of the key benefits of XAI for eye disease diagnosis is the ability to provide clinicians with insights into the underlying decision-making process of AI models. This transparency is essential for fostering trust and collaboration between healthcare providers and AI systems, as it allows for a deeper understanding of the criteria used in the model&#8217;s predictions. Hence XAI techniques can empower clinicians to validate the model&#8217;s decision-making, by shedding light on the specific features or patterns that the AI system has learned to identify, ultimately leading to more informed and personalized patient care [ 2 ]. Eye-XAI uses a multi-faceted strategy that brings together the following key components: Comprehensive Symptom Analysis: The system incorporates a diverse range of symptoms and clinical data associated with various eye diseases, allowing for a holistic assessment of a patient&#8217;s condition. Data Pre-processing and Feature Engineering: Raw data is carefully curated and transformed to ensure data quality and consistency, while meaningful features are further engineered to capture subtle patterns and associations. Explainable AI Models: Eye-XAI employs cutting-edge machine learning and deep learning models that are inherently interpretable. These models enable clinicians and patients to understand the decision-making process, fostering trust and confidence in the diagnostic results. Symptom-Model Integration: The model leverages the synergy between symptoms and AI predictions, allowing healthcare professionals to discern the rationale behind the system&#8217;s recommendations. This integration enables the model to explain how specific symptoms contribute to the diagnostic outcome. Validation and Evaluation: The performance of Eye-XAI is rigorously assessed using real-world patient data, including comparisons with traditional diagnostic methods and expert clinical judgment. Eye diseases are prevalent in many South Asian countries, with Bangladesh reporting a particularly high rate. According to a study, 1.5% of adults in Bangladesh are blind, and 21.6% have low vision [ 3 ]. Factors contributing to this include inadequate access to eye care, environmental pollution, and excessive screen time [ 3 ]. To address these issues, we focus on five common eye diseases in Bangladesh and collect data on biomarkers and symptoms for affected individuals. This data is essential for healthcare professionals to guide treatments and develop AI algorithms for improving AI diagnosis and treatment recommendations. Table 1 shows the five eye diseases that we have considered for the study with a brief description of each patient&#8217;s condition and when the disease appeared. For a visual perspective, the Fig. 1 shows the five diseases as they look when they appear in a patient. Fig. 1 Sample images of eye diseases. 1( a ) Cataracts, 1( b ) Acute angle-closure glaucoma, 1( c ) Primary congenital glaucoma, 1( d ) Exophthalmos or bulging eyes, and 1( e ) Ocular hypertension. Images are illustrative and were adapted from publicly available previously published work [ 4 ] Table 1 Brief descriptions of eye disease conditions Disease Name Brief Description of Patient Condition Cataracts Clouding of the lens of the eye, lead to blurry vision, difficulty seeing at night, and difficulties with glare. Acute Angle-Closure Glaucoma Sudden increase in eye pressure due to blocked drainage, causing severe eye pain and vision loss. Primary Congenital Glaucoma A rare condition in infants where there&#8217;s increased intraocular pressure, leading to optic nerve damage and vision loss. Exophthalmos or Bulging Eyes Protrusion of the eyeball out of the eye socket which is often associated with various underlying conditions or thyroid issues. Ocular Hypertension Higher than normal pressure in the eye without any optic nerve damage or vision loss (yet). It can lead to glaucoma if left untreated. From research by [ 5 &#8211; 7 ] it has been shown that traditional machine learning algorithms, particularly classification models, are well suited for detection and recommendations for treating for these five eye diseases. The main contributions of this study are: Using traditional machine learning methods to find the best performing method for further analysis. Comparison between feature selection approaches, including Pearson Correlation (PC), Information Gain (IG), Principal Component Analysis (PCA), Relief-based Ranking (RR) and SHAP features, to find the most suitable set of features/symptoms for different eye diseases. Application of explainable artificial (XAI) concepts including explainable machine learning (XML) methods such as SHAP features to make the ML methods perform better. We have also presented all the pre-processing and necessary details of the proposed method for the possibility of regenerating the method by other researchers. All of the metrics used for performance measurement of the applied ML methods have also been included. The following sections are Sect. 2 : Related Works, which discusses previously published relevant literature, Sect. 3 : Materials and Methods, which discusses the data algorithmic tools used. Sect. 4 : Experimental Results which discusses the computational outcomes of the proposed process as well as covering the crosstalk between medical sciences and computational outcomes. Possible future improvements have been noted in the conclusion Sect. 5 . Related works This section focuses on surveying research papers discussing techniques relating to artificial intelligence (AI) and machine learning (ML) for the prediction of eye diseases. These techniques are crucial for the method proposed in this paper that aims to efficiently predict the five eye diseases mentioned above. Table 2 summarizes the relevant research works, highlighting applications of AI/ML in this field. Table 2 Applications of AI/ML on eye disease Citation Research Focus Dataset Architecture Method Relevant Findings [ 8 ] Prevalence and risk factors of eye diseases among Dhaka slum dwellers 1320 household survey and 432 clinical assessments Stata 13 (statistical analysis) Multistage cluster sampling, clinical eye exams 92.8% diagnosed with eye diseases; most common: refractive error (63.2%), conjunctivitis (17.1%), cataract (7.2%) [ 9 ] Fundus image classification across eight disease categories ODIR-2019: 4,000 pairs for training, 500 for offsite, 500 for onsite testing VGG16, InceptionV3, ResNet, MobileNet Transfer learning with CNNs; two models: individual input for left and right images (Model-1) and concatenated inputs (Model-2) VGG16 with two inputs achieved best results (AUC: 84.93, F1: 85.57). Concatenated input model overfitted. SGD performed better than Adam [ 10 ] Multi-label ocular disease classification from paired CFPs 5,000 patients, 8 disease categories; training set of 3,500 patients split into three folds for ablation studies DCNet Dense Correlation Network (DCNet) with a Backbone CNN for feature extraction, Spatial Correlation Module (SCM) for pixel-wise connections, and a Classification DCNet improves classification accuracy by effectively capturing correlations between paired CFPs [ 11 ] Automating the diagnosis of infective and non-infective anterior eye diseases 196 anterior eye images (100 infective, 96 non-infective) from medical institutions YOLOv3 Anatomical structure-focused classification on cornea; five-fold cross-validation Achieved 88.3% accuracy (173/196); addresses eye position and illumination variation [ 12 ] Patient-level multi-label ocular disease classification using CFPs ODIR 2019 (public CFP dataset) DCNet Backbone CNN for feature extraction from CFPs, Spatial Correlation Module (SCM) for pixel-wise feature fusion, Classification module for disease probabilities DCNet outperforms baseline models in classification accuracy with lower computational complexity [ 13 ] Classification of seven eye diseases and normal eyes 590 eye images from Shutterstock, representing seven diseases and normal eyes Backpropagation Neural Network (BP) Color and texture features extracted and combined; RGB to HMMD conversion; parabola-shaped learning rate applied Achieved 89.83% classification accuracy [ 14 ] Classification of external eye diseases 590 samples, seven eye diseases&#8201;+&#8201;normal eye Back Propagation (BP) Non-linear cyclic learning rate based on Welch estimation, color histogram, and texture features extraction Classification accuracy: 93.22%, F-score: 94.48%, Sensitivity: 93.11%, Specificity: 98.94% [ 15 ] Ocular disease classification using knowledge distillation ODIR-5K (5000 cases) Deep Neural Networks (Teacher-Student model) Teacher network uses CFPs&#8201;+&#8201;clinical features; Student network uses only CFPs after knowledge distillation from teacher Student model, using only CFPs (without clinical features), can recover teacher model performance, improving diagnosis accuracy [ 16 ] Multi-disease classification in fundus screening OIA-ODIR (10,000 fundus images, 5,000 patients, 8 diseases) VGG-16, ResNeXt-50, Inception-v4, ResNet-50, SE-ResNet-50 Feature fusion: element-wise sum, multiplication, concatenation, SE attention mechanism, 70/30 train-test split Depth alone doesn&#8217;t improve results, structured feature fusion is needed, element-wise fusion outperforms concatenation, class imbalance presents challenges [ 17 ] Developing DKCNet for effective multi-label classification of ophthalmic diseases from imbalanced fundus image datasets ODIR-5K DKCNet and InceptionResNet Attention Mechanism, Squeeze-and-Excitation (SE) Block, Data Augmentation Techniques, Random Sampling Techniques, Grad-CAM AUC: 96.08, F1-Score: 94.28, Kappa: 0.81; resolves class imbalance through oversampling/undersampling; performs well on unseen datasets [ 18 ] Eye disease detection (cataract, conjunctivitis, normal) 2250 eye images (750 for each condition) from Shutterstock and Google VGG-16, ResNet-50, Inception-v3 CNN-based Transfer Learning (TL), 75:25 training-testing ratio Inception-v3 achieved the highest accuracy (97.08%) with 485s; VGG-16 had the slowest detection (2510s) [ 19 ] Evaluating CNNs for classifying 8 ocular diseases from fundus images 10,000 fundus images (ODIR database) Pre Trained CNNs: VGG16, Inceptionv3, ResNet50 with adam optimizer image augmentation, 70% train, 10% validation, 20% test split ResNet50: Best accuracy (97.1%), AUC (0.964), precision (79.7%) for cataract VGG16: Best for diabetic retinopathy, precision (84.1%), Inceptionv3: Best for age-related macular degeneration [ 20 ] Automating cataract detection in eye images 612 eye images (cataract and normal) VGG16, ResNet50, Vision Transformer(ViT) Median filtering, data augmentation, pooling, dropout ViT outperformed CNN models with 70% accuracy in binary classification [ 21 ] Automating diagnosis of eye diseases ODIR-5K (5000 fundus images) VGG-19 Data balancing, augmentation, binary classification Achieved 95% accuracy in 15 epochs for normal vs cataract, F1 scores 0.95&#8211;0.96 [ 22 ] Innovating a 2D Retina-Net model developed for diagnosing eye diseases by analyzing retinal fundus images Derbi Hackathon Retinal Fundus Image Dataset (3,785 images) EfficientNet-B3, VGG-19, Retina-Net Data preprocessing, augmentation, deep neural networks (DNN) EfficientNet-B3: best accuracy (80.892% validation, 79.360% testing); Retina-Net: improved over VGG-19 [ 23 ] Diagnosing challenges of conjunctivitis detection while mitigating algorithmic bias 1679 eye images (Conjunctivitis: 797, Non-Conjunctivitis: 882) from Mendeley, Roboflow, Kaggle CNN, ResNet-50, InceptionV3, DenseNet-169 Dataset gathering, image preprocessing, model training, validation, and testing InceptionV3 emerged as the best model with 93.04% accuracy; CNN performed well with 0.9523 training accuracy Key findings reveal that there is a high prevalence of eye diseases in developing countries, the most common being refractive error, conjunctivitis, and cataracts. Deep learning models, particularly Convolutional Neural Networks (CNN) [ 24 ], have shown promising results in the identification of these diseases. Popular architectures such as VGG16, InceptionV3, ResNet, MobileNet, and DCNet have been used effectively to diagnose eye conditions [ 9 , 16 , 18 , 23 ]. Pre-processing techniques, such as image augmentation, normalization, and noise reduction, were also crucial in improving the quality of the data. CNNs are especially effective in extracting relevant features from fundus images, which aid in accurate diagnosis. Additionally, techniques such as image augmentation and data balancing have been shown to be crucial in improving the performance of models, especially when dealing with imbalanced datasets. Knowledge distillation, which involves transferring information from more complex models to simpler ones, has also been shown to have potential for enhancing diagnostic accuracy while minimizing computational costs. Our current Eye-XAI pipeline follows a segmentation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\rightarrow$$\\end{document} classification workflow designed for transparency and moderate compute (classical UNet-style segmentation of OD/OC or ROI, lightweight CNN/GBM classifiers, SHAP explanations). Recent &#8220;hybrid&#8221; approaches combine stronger segmenters with meta-heuristics or pair them with non-standard classifiers [ 25 ] or generators&#8212;e.g., Grey-Wolf&#8211;optimized U-Net/UNet++ [ 26 , 27 ] &amp; Capsule Networks [ 28 ] for glaucoma/diabetes ratinopathy, and GAN-assisted augmentation and MobileNetV2 for glaucoma screening [ 29 ]. These studies report gains that generally come from finer boundary extraction of OD/OC and lesions (U-Net/UNet++), robustness of pose/part-whole modeling (CapsNet), and data distribution smoothing via GANs; but they introduce additional complexity and compute (optimization loops, capsule routing, adversarial training) and raise reproducibility risks across sites/devices. The methods employed in these studies included data collection from various sources such as ODIR-2019 [ 9 ], custom datasets, and publicly available resources like Shutterstock [ 13 , 18 ]. CNN-based models dominate the architecture choices, with different architectural variations having been tested, including VGG16, InceptionV3, ResNet, MobileNet, and DCNet. Training and evaluation of these models involved cross-validation, hyper-parameter tuning, and the use of performance metrics like accuracy, precision, recall, F1-score, and AUC to assess their effectiveness. Although these models have shown great potential to accurately classify eye diseases from fundus images, future research also needs to focus on further improving performance, tackling class imbalance, and exploring new methods to enhance the interpretability and explainability of deep learning models in this domain. We have also examined relevant papers on explainable AI methods to better interpret the results obtained from our proposed approach. Some of the works on the application of AI / ML in eye disease have been compiled in 2 and works on explainable AI have been compiled in Table 3 . Table 3 Applications of Explainable AI/ML on eye disease Citation Research Focus Dataset Architecture Method Relevant Findings [ 30 ] Interpretability of deep learning for ophthalmic diagnosis UCSD dataset, 80,000+ images Inception-ResNet-v2 Deconvolutional Network (DeConvNet), Deep Learning Impor- tant Features (DeepLIFT), Guided Backpropagation (GBP), Integrated Gradients (IG), SHapley Additive exPlanations (SHAP), Occlusion, Layerwise Relevance Propagation (LRP) LRP had the lowest error (RMSE), SHAP and GBP produced clinically relevant heatmaps, Successful identification of pathological regions (CNV, DME, Drusen) without pixel-level annotations [ 31 ] Explainable ML Model for Glau- coma Diagnosis Clinical data including visual field test, RNFL OCT, IOP, and fundus photography from Gyeongsang National&#160; University Hospital (975 glaucoma eyes and 649 non-glaucoma eyes) Support Vector Machine (SVM), Random Forest, C5.0,XGBoost Machine learning models tested with 10-fold cross- validation; SHAP, radar, and gauge charts for explana- tion XGBoost showed the best performance with 0.947 accuracy, 0.941 sensitivity, and 0.950 specificity. The model achieves higher diagnostic performance with reduced chances of misdiagnosis using explainable AI [ 32 ] OCT disease sification XAI clasusing 84,495 OCT images (CNV, DME, DRUSEN, NORMAL) Custom CNN layers) (6) LIME, Grad-CAM for interpretability Custom CNN achieved 94.87% accuracy; model size is less than 2MB and efficient for real-time appli- cations [ 4 ] Develop&#160; an&#160; efficient machine learning model to predict five common eye diseases using a benchmark dataset and ranker-based feature selection methods 563 patient records annotated by ophthalmologists SVM, Decision Trees, Na&#239;ve Bayes, Random Forest, k- Nearest Neighbors, Logistic Regression, AdaBoost, XGBoost Train-Test (66-34%, 75-25%, 80-20%),&#160; k-Fold Cross Validation, ranker-based feature selection, explainable AI (XAI) methods Highest&#160; accuracy&#160; achieved was 99.11% using Support Vector Machine (SVM) with cross-validation and without feature selection methods [ 33 ] Explainable AI-Enabled Tele Ophthalmology for Diabetic Retinopathy Grading and Classification Two datasets: APTOS 2019 and DDR dataset U-Net, SqueezeNet, Archimedes Opti- mization Algorithm (AOA), Bidirectional Gated Recurrent Convolutional Unit (BGRCU) Median filtering, contrast enhancement, U-Net segmentation, SqueezeNet feature extraction, AOA for hyperparameter tuning Enhanced accuracy of 98.24% in diabetic retinopathy classification, Effective in both training and testing phases, Outperformed exist ing models like CNN and EfficientNet [ 34 ] Develop XAI and IML models for glaucoma prediction using fundus images and clinical records to enhance decision-making through multimodal learning 650 fundus images (168 positive, 482 negative), clinical medical records Spike Neural Network (SNN), ANFIS, SP-LIME Multimodal learning, Pixel Density Analysis (PDA) Multimodal approach using fundus images and clinical data improves prediction accuracy, SNN iden tifies neuron density variations, SP-LIME enhances interpretability for clinical use [ 35 ] Developing a transfer learn- ing model with explainability through LIME for accurate and transparent glaucoma detection LAG dataset (4250 training, 302 testing, 302 validation images) ResNet50, CNN, DenseNet121, VGG-16/19 Transfer&#160; learning with XAI (LIME) for explainability Achieved 94.71% accuracy with ResNet50; LIME adds transparency for decision- making [ 36 ] Develop an explainable deep learning approach for accurate and transparent cataract detec tion from fundus images Eye diseases classification dataset with 1038 cataract and 1074 normal images VGG-19 CNN Grad-CAM for heatmap visualization Achieved&#160; 97%&#160; accuracy using VGG-19 for image classification; Grad-CAM highlighted key image regions, enhancing model interpretability for clinical trustworthiness [ 37 ] Develop an explainable AI model that efficiently diagnoses allergic conjunctival diseases (ACDs) by recognizing specific clinical characteristics and providing interpretable decisions 4942 slit-lamp images from 10 ophthalmological institutions 37 Mask Region based Convolutional Neural Network (Mask R- CNN) with ResNet-50 and Feature Pyramid Network (FPN), You Only Look Once version&#160; 8 (YOLOv8) Segmentation&#160; AI pipeline,&#160; clinical findings extraction Achieved high diagnostic accuracy (86.2%), better than board-certified ophthalmologists (60.0%). AUC scores were high for various disease categories [ 38 ] Developing a transparent diagnosis model for Diabetic Retinopathy (DR) using Explainable AI (XAI) Retinal dataset training, validation, testing) image (1702 568 568) Convolutional Neural Network (CNN) with XAI Data&#160; preprocessing, CNN, Local Interpretable Modelagnostic Explanations (LIME) Achieved 94% accuracy with 6% miss-rate, better diag- nostic transparency and clinician confidence [ 39 ] Ocular disease prediction using AI and XAI ODIR-5K (5,000 ocular images) Transfer Learning + EfficientNet XAI (LIME - Local Interpretable Model-Agnostic Explanations) Achieved 95.74% accuracy using EfficientNet and XAI (LIME) for transparency [ 40 ] AI-driven diagnosis of Diabetic Retinopathy (DR) Kaggle (3,662 images) dataset retinal VGG16, Xception, MobileNetV2, custom CNN Explainable AI (XAI)&#160; with CAM and Grad CAM++, self-attention techniques VGG16: High accuracy in &#8216;No DR&#8217; (0.98); Custom CNN: Accuracy of 0.986 for &#8216;No DR&#8217; Materials &amp; methods An overview of the proposed research methodology with all the steps needed to detect different eye diseases with the help of explainable machine learning methods is presented in Fig. 2 . Fig. 2 Overview of working diagram of the proposed eye disease prediction method using explainable artificial intelligence (XAI) Data collection and annotation In this section, the importance of meticulous data collection for creating a reliable dataset is emphasizes. Data must include both patient symptoms and clinical information. This study therefor used real patient data gathered during ophthalmologist referrals. Since symptoms were difficult to capture during a brief appointment, two interviewees collected data: the ophthalmologist and the author of this study. This comprehensive approach ensured that valid data were collected. The actual data collection occurred in private during one-on-one appointments using a semi-structured interview process. A predefined questionnaire with closed binary questions (yes or no) was set up. The dataset includes 563 patients diagnosed with one of the eye diseases mentioned earlier. The age group of these patients was between 23 and 65&#8201;years old. The gender identified by the patients is classified as male or female. To avoid missing data points, the ophthalmologist assigned a value of 0 or 1 to each attribute in Table 4 based on their observations. This strategy resulted in a robust dataset with no missing values. This data will be available from the corresponding author upon request. Table 4 Features of the dataset Feature No. Features/Attributes Properties f1 Cloudy, blurry or foggy vision The values are either 0 or 1 f2 Pressure in eye f3 Injury to the eye f4 Excessive dryness f5 Red eye f6 Cornea increased in size f7 Problem in identifying color f8 Double vision f9 Myopia f10 Trouble with glasses f11 Hard to see in the dark f12 Visible whiteness f13 Mass pain f14 Vomiting f15 Water drops from eyes continuously f16 Presents of light when eye lid close f17 Family history of similar disease Clinical Data (0 or 1) f18 Age +40 f19 Diabetes f20 Class/Types of eye diseases Cataracts (236 instances), Acute Angle-Closure Glaucoma (AACG) (59 instances), Primary Congenital Glaucoma (PCG) (57 instances), Exophthalmos/Bulging Eyes (BE) (41 instances), Ocular Hypertension (170 instances) Since the dataset was collected exclusively from Bangladeshi patients, there is a risk that the model may capture population-specific biases. Differences in demographics, genetics, and healthcare practices across populations may affect generalizability. To address this, future work will focus on external validation with international datasets (e.g., MESSIDOR [ 41 ], EyePACS [ 42 ]), domain adaptation techniques, and inclusion of multi-ethnic cohorts to ensure broader clinical applicability and fairness. Data pre-processing For data pre-processing, we have checked if there is any missing values, any kind of inconsistency, or data duplication possibly happening while inserting these data into tabular file by the data collectors. All the data provided in the file are considered for the data pre-processing. With careful consideration by the ophthalmologist, there was no missing value and all patient cases were properly labeled. Therefore, a minimal level of pre-processing was needed to load these data into the explainable artificial intelligence pipeline. Before loading this data into the XAI pipeline, we normalized the data, making sure that all of the data are in their correct format. Explainable artificial intelligence methods The proposed methodology uses explainable artificial intelligence (XAI) methods in the form of a pipeline containing different traditional machine learning methods, such as Decision Tree (DT), Naive Bayes (NB), Random Forest (RF), Bagging (BG), Adaboost (AB), XGBoost (XGB), etc. This pipeline was introduced to accommodate changes or plug-ins of different machine learning methods to enable multiple experimental setups. Figure 2 shows the three main steps of XML where the annotated and pre-processed eye disease data has been fixed into the explainable artificial intelligence pipeline to elucidate the insights of disease classification. The internal steps of explainable AI has been shown on the diagram having machine learning model training, model testing and validation, classification outputs, machine learning explainer interface using SHAP, SHAP Global interpretation based on features/symptoms and SHAP local interpretation based on patience data. Correlation between important symptoms has also been indicated in the results section. Applied machine learning methods The proposed methodology includes the traditional machine learning methods to be applied as first stage of the pipeline. For this study these methods include nine (9) different types of classifiers, namely decision tree (DT), Naive Bayes (NB), Random Forest (RF), Bagging (BG), Adaboost (AB), XGBoost (XGB), k-Nearest Neighbours (kNN) and Support Vector Machine (SVM). These methods have been shown to work better for multi-class classification problems. These ML models has been trained, tested and validated to find the classification results. Decision trees (DT) [ 43 ] excel in image classification due to their interpretability, ability to handle both numerical and categorical features, and their ability to provide information on the importance of features. This makes them valuable for understanding the decision-making process of the model. Naive Bayes (NB) [ 44 ] is a simple but effective classifier for image classification, and has proved to be better with categorical data as well. It assumes the independence between features, which can be a reasonable assumption in many image classification scenarios. The probabilistic nature of Naive Bayes provides insights into the likelihood of different class labels. Random Forest (RF) [ 45 ], an ensemble method, combines multiple decision trees to improve accuracy and reduce overfitting. It can handle high-dimensional image data effectively and provides feature importance measures, aiding in understanding the model&#8217;s decision-making process. Bagging (Bootstrap Aggregating) [ 46 ] is another ensemble method that can enhance the performance of image classification models. By creating multiple decision trees from bootstrap samples of the training data, bagging reduces variance and improves generalization. AdaBoost (Adaptive Boosting) [ 47 ] is a boosting algorithm that iteratively trains weak classifiers and weights them based on their performance. It can be effective in improving the accuracy of image classification models, especially when combined with decision trees or other base classifiers. XGBoost (Extreme Gradient Boosting) is a state-of-the-art gradient boosting framework that has achieved excellent results in various machine learning tasks, including image classification [ 48 ]. XGBoost&#8217;s regularization techniques and efficient implementation make it a popular choice for handling complex image data. Traditional k-NN [ 49 ], logistic regression [ 50 ], and SVM [ 51 ] have been applied to check the performance of all classifiers in the same data set and experimental setup. The traditional ML classifiers mentioned above are well suited for eye disease classification tasks that rely on the categorical data used in this study. These models can effectively handle nominal and ordinal variables without requiring complex transformations, making them ideal for structured clinical data sets that include patient demographics, symptoms, and medical histories. Unlike deep learning approaches, traditional ML models are computationally efficient and perform well even with limited datasets, a common scenario in medical applications where data labeling is expensive and time consuming [ 52 ]. Their ability to work with small to medium-sized datasets makes them practical for deployment in resource-constrained healthcare environments, such as community clinics or mobile health units. Another advantage of traditional ML models is their interpretability, which is crucial in a clinical decision making context. Models like Decision Trees and Logistic Regression provide transparent decision-making processes through rule sets or coefficient-based reasoning, helping clinicians understand and trust the predictions of the model [ 53 ]. Furthermore, tree-based models, such as Random Forests, offer feature importance metrics, allowing researchers and healthcare providers to identify key risk factors and symptoms that contribute to disease outcomes. These insights not only support more informed diagnoses, but also improve clinical research and treatment planning. In general, traditional ML classifiers offer a reliable, interpretable, and efficient approach to eye disease classification when working with categorical data. Since the primary dataset involved structured, non-image symptom data rather than medical images, traditional classifiers such as SVM and XGBoost were better suited to the task. These models are efficient in small to medium-sized tabular datasets and integrate seamlessly with SHAP for explainability. In contrast, deep learning models often require larger datasets and are more opaque, which goes against the goal of transparency emphasized in Eye-XAI. Applied feature selection methods A set of feature-selection methods forms part of the experimental setup. These feature selection methods are used along with machine learning models, including Pearson&#8217;s Correlation (PC) [ 54 ], Information Gain (IG) [ 55 &#8211; 57 ], Principal Component Analysis (PCA) [ 58 &#8211; 60 ], and Relief-based Selection [ 61 ]. PC measures the linear relationship between variables [ 54 ], while IG assesses the reduction in entropy when a characteristic is known [ 57 ]. PCA reduces dimensionality by identifying the principal components [ 58 &#8211; 60 ], and Relief-based selection assigns weights to the features based on their relevance to the target variable [ 61 ]. The PC values were calculated for the class-feature relationships ranking the features accordingly. IG was used to evaluate the information gain provided by each feature. PCA was applied to reduce the dimensionality if needed [ 60 ]. The selected subset of features based on their rankings was used for further analysis. We have also experimented with the split training test [ 62 ] and 10-fold cross validation [ 63 ] and reported the best metrics obtained. The decision to retain all features was based on SHAP analysis, which revealed that even features with modest individual contributions added interpretive value when considered in combination with all of the other features. Rather than reducing interpretability, the inclusion of all features provided a more comprehensive understanding of the diagnostic process, which was validated by a clinical expert to ensure relevance and avoid information loss. While some features may appear correlated or redundant, our use of SHAP allowed us to assess their individual and joint contributions to model predictions. Interestingly, even features with modest individual impact provided complementary value when interpreted in combination. To ensure that these features did not introduce noise or reduce interpretability, we monitored the variance in SHAP value distributions and confirmed model stability over multiple runs. Future work will include ablation studies and dimensionality reduction methods to further refine the feature space without compromising the explainability. SHAP (SHapley additive exPlanations) model SHAP (SHapley Additive exPlanations) [ 64 , 65 ] is a game-theoretic method which is used to explain the output of any machine learning model. It provides a way to understand how individual features contribute to a model&#8217;s predictions. One of the main advantages of using SHAP is that it allows both global and local interpretations. Additionally, the visualizations provided by SHAP Values are easy to understand by researchers in various domains. Global vs Local Interpretations SHAP has become one of the most popular interpretations for machine learning models [ 66 ] due to its global and local interpretability of a given model. Global interpretations are performed by finding the importance of the feature across the entire dataset using all the features whereas the local interpretation provides an understanding of the importance of the individual features for a specific prediction. This enables both a case-by-case prediction as well as a possibility for uncovering insights. The importance of a SHAP feature can be calculated using the following equation. 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ I_{j} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\phi^{(i)}_{j} \\right|$$\\end{document} Here, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$I_{j}$$\\end{document} represents the importance of feature \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$j$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n$$\\end{document} the total number of samples in the dataset and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi^{(i)}_{j}$$\\end{document} the SHAP value for feature \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$j$$\\end{document} for the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$i$$\\end{document} -th sample. The calculation is performed for all the features and they are sorted in descending order and presented in either a horizontal bar chart or a SHAP summary plot. For this study, we have selected SHAP as the primary explainability method because it provides both local (patient-level) and global interpretability within a unified, theoretically grounded framework. Unlike LIME [ 67 ], which relies on local perturbations and can produce unstable results, SHAP offers more robust, model-agnostic interpretability, particularly for tabular symptom data. Grad-CAM [ 68 ], being image-specific, was not suitable for the non-image symptom-based data used in this study. SHAP is based on cooperative game theory and offers additive consistent attribute of features that align well with clinical reasoning and facilitate reproducibility, an essential requirement in healthcare applications [ 64 , 65 ]. SHAP also enables transparent visualization of how individual features contribute to a prediction and can be aggregated between patients to reveal cohort-level risk drivers, thus bridging clinical interpretability and model transparency more effectively than alternative methods [ 66 , 69 , 70 ]. Performance measurement indices The common performance metrics accuracy ( 2 ), precision ( 3 ), recall ( 4 ), and F1 score ( 5 ) that are generally used to evaluate the machine learning models are used in this study as well. These metrics are widely used in the field, as demonstrated in previous studies [ 71 , 72 ]. and they consider the correct and incorrect classifications of diseases. These metrics are calculated using the following equations: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Accuracy (ACC) = \\frac{TP + TN}{TP + TN + FP + FN} $$\\end{document} 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Precision (PR) = \\frac{TP}{TP + FP} $$\\end{document} 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Sensitivity (SEN) = \\frac{TP}{TP + FN} $$\\end{document} 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ F1- score = \\frac{2(Precision \\times Recall)}{(Precision + Recall)} $$\\end{document} The explanation of TP, TN, FP, and FN as they relate to the classification of eye diseases is provided in Table 5 . Table 5 Explanation of formula terms based on eye disease classification Term Explanation based on eye disease classification True Positive (TP) When the ML model correctly classifies a patient as having a particular eye disease True Negative (TN) When the ML model correctly classifies a patient as having a different eye disease False Positive (FP) When the ML model incorrectly classifies a patient as having one particular disease when the patient actually has another disease False Negative (FN) When the Ml model incorrectly classifies a patient as not having a disease when the patient actually has the disease Experimental results Results of ML models In this subsection, the results of the ML model based on the commonly used 80%-20% train-test splitting is presented. A second experiment was to apply 10-fold cross-validation on the same dataset. Multiple experiments were performed using the plug-and-run method, to compare the performance metrics among the different ML models. In addition, the four feature selection methods were applied along with the ML models to find the impact of choosing or selecting important features from all features. An experiment was performed with all the features intact. The 10-fold cross-validation was stratified to preserve the proportion of each class in all folds. To address class imbalance, the authors applied techniques such as class weighting and oversampling of minority classes during training. These steps ensured balanced learning and prevented bias toward the majority class in model performance. Figures 3 and 4 demonstrated the precision and f1 score obtained by applying 80% &#8722; 20% train test splitting, observing that the highest accuracy obtained was 0.98. 23% using the XGBoost method when all the features were included in the pipeline. Therefore, compared with other results, it is evident that the feature selection methods have little impact on the accuracy of the models. In addition, keeping all features in terms of symptoms is very important in this study when deciding the classes of the disease. Figure 5 shows the accuracy metric for all applied ML methods, where XGB outperformed all the other methods obtaing the highest accuracy of 98.23%. In addition, for the ablation study, the feature selection methods mentioned previously were integrated with the ML methods. Only in two of the ML cases, namely Pearson&#8217;s correlation (PC) for bagging and information gain (IG) for random forest, feature selection methods are showing better accuracy. The accuracy values are reported in Fig. 6 . Fig. 3 F1-score comparison between AB, bagging, DT, k-NN and LR with 80%-20% split and five feature selection method Fig. 4 F1-score comparison between NB, RF, SVM and XGBoost with 80%-20% split and five FS method Fig. 5 Accuracy comparison between the classifiers with 80%-20% split Fig. 6 Accuracy comparison between the applied classifiers with 80%-20% split and five feature selection method Table 6 shows the results for the three-fold, five-fold and 10-fold cross-validations in terms of PR, SEN, F1 score and ACC for all nine ML models without applying any feature selection methods. This time the SVM classifier outperformed the other classifiers showing 99.11% accuracy with 1.0 in PR, SEN, and F1-score. For the ease of conducting interpretations, we have utilized the XGBoost as it uses the tree-explainer methods. Table 6 Performance of ML models (cross-validation&#8201;+&#8201;No FS applied) Model 3-fold 5-fold 10-fold Name PR SEN F1 ACC PR SEN F1 ACC PR SEN F1 ACC DT 0.96 0.95 0.95 94.850% 0.96 0.96 0.96 96.805% 0.93 0.93 0.92 96.980% NB 0.96 0.96 0.96 95.561% 0.96 0.96 0.95 95.915% 0.95 0.93 0.93 95.742% RF 0.99 0.99 0.99 98.402% 1.00 1.00 1.00 98.581% 1.00 1.00 1.00 97.870% AB 0.98 0.98 0.98 97.510% 0.98 0.98 0.98 98.050% 0.98 0.98 0.98 97.690% LR 0.99 0.98 0.95 98.579% 1.00 1.00 1.00 98.936% 1.00 1.00 1.00 98.938% k-NN 0.96 0.96 0.95 96.447% 0.98 0.97 0.97 96.271% 1.00 1.00 1.00 96.626% Bagging 0.93 0.94 0.97 95.062% 0.94 0.96 0.95 95.579% 0.94 0.95 0.89 95.578% XGBoost 0.98 0.98 0.98 98.579% 0.99 0.99 0.99 98.581% 0.98 0.98 0.98 98.051% SVM 0.98 0.98 0.98 98.756% 1.00 1.00 1.00 98.936% 1.00 1.00 1.00 99.110% Not only are the accuracy and F1-score values produced for the comparison of the ML models, but we also have the precision, recall values produced for each of the experiments. In case of splitting methods, we have used three splitting measures 66% &#8722; 34%, 75% &#8722; 25, and 80% &#8722; 20%, respectively. Each of the ML methods have been run for the ablation study. Performance matrices are illustrated in Tables 1, 2, and 3 in Supplementary data. In addition, feature selection methods were used to create more experiments in the ablation study and ML methods are explored with and without FS techniques. Table 4 in the supplementary data show all the matrices for all of the ML methods. In Fig. 7 , the confusion matrix produced by the four main ML classifiers is illustrated. The other confusion matrices are placed in the supplementary data. Similarly, the ROC curve for the main four methods is depicted in Fig. 8 . The remainig ROC Figures are added in the supplementary data. Fig. 7 Confusion matrices of ML classifiers Fig. 8 ROC of ML classifiers Results of feature selection methods The feature selection methods were tested for the ablation study in the ML methods and the ranking scores were calculated for each feature mentioned in Table 4 . The ranking scores are shown in Table 7 . The ranking gives the idea of how different feature selection methods have impacted the features. Therefore, the ablation study performed on the ML methods with and without feature selection methods gives us good understanding of the results. Table 7 Ranking Score of all the features using several feature selection methods PC-based Ranking Score Feature Number IG-based Ranking Score Feature Number Releif-based Ranking Score Feature Number PCA-based Ranking Score Feature Number 0.6218 f1 0.8 f1 0.6194 f1 0.7619 f1 0.5583 f7 0.6182 f7 0.4196 f2 0.5981 f3 0.534 f8 0.5587 f15 0.4181 f7 0.5054 f4 0.5148 f15 0.5434 f8 0.3682 f3 0.4171 f16 0.5114 f2 0.543 f2 0.3383 f9 0.365 f10 0.5073 f11 0.5216 f9 0.3253 f8 0.3175 f2 0.4991 f9 0.4848 f11 0.3186 f15 0.2719 f18 0.3052 f3 0.3477 f3 0.3137 f11 0.2306 f17 0.2403 f13 0.2326 f13 0.1129 f18 0.1976 f14 0.2279 f5 0.2293 f12 0.1032 f12 0.1678 f13 0.2258 f14 0.2196 f4 0.1024 f5 0.1389 f5 0.2225 f6 0.213 f6 0.1013 f6 0.1136 f8 0.2225 f16 0.213 f16 0.0952 f19 0.09 f6 0.2061 f12 0.2108 f5 0.0915 f13 0.0719 f7 0.2023 f4 0.2004 f14 0.0875 f4 0.0549 f9 0.1539 f18 0.0749 f18 0.0826 f14 0.0388 f11 0.1531 f17 0.073 f17 0.0773 f16 0 f12 0.1525 f19 0.0718 f19 0.0616 f17 0 f15 0.0524 f10 0 f10 0.0218 f10 0 f19 Interpretation of results SHAP global interpretation based on features Global interpretation methods help us to understand the data and include feature importance, feature dependence, interactions, clustering, and summary plots. With SHAP, global interpretations are consistent with local explanations because Shapley values are the basic unit used in global interpretation. Figure 9 shows the importance of the SHAP feature in a horizontal bar graph representation with a mean ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$|SHAP (value)|$$\\end{document} ) (average impact on the magnitude of the model output) along the x-axis and symptoms along the y-axis. These SHAP feature importance plots were developed using the mean absolute Shapley values. In Fig. 9 , it can be seen that the symptom of &#8220;cloudy, blurry, or foggy vision&#8221; (f1) is the most important characteristic, changing the probability of predicted absolute eye disease by 0.18 on average. Some other importance features for the average SHAP values, such as 0.13 for f13, 0.12 for f7, 0.09 for f14, f15 and f16, 0.07 for f5, 0.06 for f12, and 0.05 for f9 are also noted. The remaining features are combined to 0.23. The least important feature is &#8220;Age over 40&#8221; showing that age frequently does not play a vital role in causing an eye disease. Mostly, the other symptoms have higher impacts on identifying a disease. Fig. 9 SHAP feature/symptoms importance, depiction in horizontal bar representation Figure 10 further illustrates the importance of features using a beeswarm plot. This beeswarm plot shows how different features influence the predictions of the model. The features are listed with an additional extended horizontal effect in terms of dots. The features in the top are the most important for the model. Each dot represents a data point, where the color indicates the value of the feature. The color spectrum represents the feature values where the red color refers to the presence of the symptom (high feature value) and the blue color refers to the opposite (low feature value). The position on the x-axis shows whether that value increased or decreased the prediction. For example, if a feature has red dots mostly on the right, it means that high values of that feature contribute to higher predictions. It can be seen in the plot that the second most important feature, the &#8220;Mass pain&#8221;, has all the red dots on the right side. In contrast, &#8220;Water drops from eyes continuously&#8221;, &#8220;Presents of light when eye lid close&#8221;, and &#8220;Myopia&#8221; all have red dots on the negative (left) side. The importance of these features are the same when we see the top 10 features in the beeswarm plot in Fig. 11 . Very few instances of red dots are present on the positive side when the least importance features (at the bottom, &#8220;Sum of 10 other features&#8221;) are merged. Fig. 10 SHAP feature importance, depiction in summary plot, aka beeswarm plot Fig. 11 Beeswarm plot showing the top 10 features Using the regular XGBoost feature importance functionality, we have tried to find the top features based on the gain values. Figure 12 shows the gain values of all features using the XGBoost sklearn Python package [ 73 ]. During training an XGBoost model, trees are built using the gradient boosting method. Based on the ability to reduce the loss, features are selected to create splits, hence branches of the trees. The model keeps a record of how often a feature has been used and how much it improved the overall prediction. The importance type used for this study was &#8220;gain&#8221; (importance_type=&#8220;gain&#8221;), which measures the average improvement in model performance (reduction in loss) when a feature is used for splitting. The features with higher gain have a greater impact on reducing error. From the resulting plot, it is found that the &#8220;Cloudy, blurry or foggy vision&#8221; feature got the highest gain of 24.17, which is similarly found by using the SHAP feature importance values. Fig. 12 Xgboost feature importance plot by gain values using python sklearn package In Fig. 13 samples are clustered together, where the model outputs are similar due to similar symptom values. Three groups of samples that the model predicts positively can be identified, each based on different types of evidence. The first group primarily depends on the presence of &#8220;mass pain.&#8221; The second group is based on a combination of &#8220;mass pain,&#8221; &#8220;cloudy, blurry, or foggy vision,&#8221; &#8220;vomiting,&#8221; and &#8220;red eye&#8221; symptoms. The third group includes samples characterized by &#8220;visible whiteness&#8221; and &#8220;myopia&#8221;. Additionally, there is a smaller group that relies solely on the presence of either &#8220;vomiting&#8221; or &#8220;visible whiteness&#8221; symptoms. In contrast, for the negative predictions, the samples are more dispersed across the feature space. However, a specific subset of samples can be identified that is determined by the absence of symptoms: &#8220;presence of light when eyelids close,&#8221; &#8220;cornea increase in size&#8221; and &#8220;cloudy, blurry, or foggy vision&#8221;. When merging the least important features together, the resulting top 10 features in Fig. 14 do not change the findings significantly. Fig. 13 SHAP Heatmap with all the features Fig. 14 SHAP Heatmap with the top 10 features SHAP local interpretation based on patients To understand how the features of each of the patients influence the applied machine learning model, a local interpretation of SHAP is highly necessary. For this force plots and waterfall plots were used. A SHAP force plot visually demonstrates how various features influence a model&#8217;s prediction relative to the baseline (expected value), which is the average prediction across the dataset. The final prediction is derived by adjusting the baseline with the SHAP values (feature contributions). Features that increase the prediction are shown with red arrows (positive contributions), while those that decrease it are depicted with blue arrows (negative contributions). The size of the arrows indicates the strength of each feature&#8217;s effect. The final prediction is calculated using equation 6 , where the sum of SHAP values represents the total impact of all features on the specific prediction. 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$ Final\\ Prediction = Expected\\ Value + \\sum_{}^{} SHAP.Values $$\\end{document} Figures 15 and 16 show two force graphs generated using the model and characteristics for patient-1 and patient-2. From Fig. 15 , the model base prediction is 1.77. The final prediction for this patient is 2.08, which means that the model strongly leans toward the predicted disease. Vomiting, mass pain, and visible whiteness contribute positively to increasing the prediction. However, the color-identifying problem, water dropping, decreases the prediction. Overall, the red arrows dominate, pushing the prediction higher. For Fig. 16 , the model base prediction is the same for all patients. The final prediction for this patient is 1.08, which means that the model strongly leans away from the disease. Water dropping, cloudy visions, etc. contribute positively to increasing the prediction. However, visible white light decreases the prediction. Overall, the blue arrows dominate, pushing the prediction lower. Fig. 15 Force plot for patient-1 Fig. 16 Force plot for patient-2 Figures 17 &amp; 18 show two examples of local interpretations can be derived from the SHAP analysis. In Fig. 17 , a sample with a predicted output of 2.0 is seen, whereas the average prediction value is 1.79. The direction and magnitude of the impact of the top features can be seen here. This bar graph helps us to understand why the model predicted the output for this specific sample to be 2.0 from the base value of 1.79 by the interactions of various features. Figure 18 shows another example where the prediction is 1.0, away from the base value of 1.79. All the features contribute to shift the prediction value towards the left. Fig. 17 Waterfall plot for class label 2 Fig. 18 Waterfall plot for class label 1 Although SHAP [ 64 , 65 ] is widely used for interpreting machine learning models due to its strong theoretical foundation and consistency, it has notable limitations, particularly in clinical contexts. One major concern is its dependence on model used, as SHAP explanations are tightly coupled to the specific model used, which can lead to different interpretations between models trained on the same data [ 74 ]. Furthermore, SHAP values can be sensitive to complex feature interactions, sometimes attributing importance in a way that overstates or misrepresents the actual clinical relevance of certain variables. This is especially problematic in healthcare settings, where causal inference and reliability are critical. Moreover, computing SHAP values for large, high-dimensional datasets can be computationally intensive and may introduce noise or variability. Given these challenges, it is essential to complement SHAP-based interpretations with clinical validation and domain expertise to ensure that the insights are reliable and actionable in a real-world medical decision making. Explained insights of disease classification Figures 19 , 20 , and 21 show dependence plots between pairs of features. SHAP values of the primary feature are shown on the y-axis to the left, and the secondary feature is shown to the right. In Fig. 19 , the samples with injury to the eye tend to lower the SHAP values of the feature &#8220;Cloudy, blurry or foggy vision&#8221; whereas in Fig. 20 , the dependence plot between Diabetes and Myopia illustrates a different scenario. Samples not having diabetes had more sparse SHAP values, with Diabetes the SHAP values are more concentrated around 0.0. The dependence plot between &#8220;Water drops from eyes continuously&#8221; and &#8220;Diabetes&#8221;, shown in Fig. 21 , indicates a clear distinction where the SHAP values are lower for the presence of &#8220;Water drop from eyes continuously&#8221;. Fig. 19 Dependence plot between cloudly, blurry or foggy vision and injury to the eye Fig. 20 Dependence plot between myopia and diabetes Fig. 21 Dependence plot between water drop from eyes continuously and diabetes Explainability and interpretability evaluation To make sure the results of Global and Local Interpretations of SHAP Features shown in Sect. 4.3 are meaningful and useful in real medical practice, we asked for help from a board-certified ophthalmologist who is part of our research team. His role was to check whether the explanations given by Eye-XAI made sense from a clinical point of view. He carefully reviewed the SHAP visualizations, which showed both the overall importance of each feature and how those features affected the individual predictions. These features included symptoms such as vision problems and patterns of eye pain, which the model highlighted as important for diagnosing eye diseases. The ophthalmologist confirmed that the highlighted features matched what he would expect to see in real patients. He said that the results were consistent with how eye diseases are usually diagnosed by doctors. The SHAP visual outputs were designed with clinical usability in mind. Force plots and waterfall plots provide a clear breakdown of feature contributions for individual cases, allowing clinicians to quickly understand why a particular diagnosis was made. During a usability study, clinicians reported that these visualizations were helpful and intuitive, improving their trust in the system and aiding diagnostic discussions. This feedback from an expert gave us to confidence that Eye-XAIs explanations are not only correct from a technical point of view, but that they also make sense to real doctors. It also shows that our model can support clinical decisions in a way that doctors can understand and trust. Comparison with existing XAI-Based eye disease detection models We have compared our proposed Eye-XAI model with the existing XAI-based eye disease detection models from literature in Table 8 . These studies have used XAI models, but they each study focused on a specific country population. Most of them used clinical symptom data, but only for one kind of eye disease, whereas our proposed work used a data set containing data for five eye diseases. Kaur et al. [ 75 ] used a diabetic retinopathy classification having the highest 95% precision with multiple experiments. Want et al. [ 76 ] proposed a LIME based model for explainability using EMR data for the prediction of glaucoma risk. Only when Images were used, Li et al. [ 77 ] used Grad-CAM, otherwise they also used SHAP for explainability, getting 96.5% accuracy. By comparison, Eye-XAI outperformed the existing methods getting 99.11% accuracy for the clinical decision-support. Table 8 Comparison of eye-XAI with related XAI-Based eye disease detection models Author (Year) Dataset Population Focus Dataset Explanation Explainability Method Accuracy (%) Kaur et al. (2021) [ 75 ] India Clinical symptom data for diabetic retinopathy classification SHAP 95.0% Wang et al. (2020) [ 76 ] USA Patient symptoms&#8201;+&#8201;EMR data for glaucoma risk prediction LIME 92.0% Li et al. (2022) [ 77 ] China Multi-modal data (symptoms&#8201;+&#8201;imaging) for cataract diagnosis Grad-CAM (images), SHAP (symptoms) 96.5% Eye-XAI (Our work) Bangladesh Symptom data from Bangladeshi patients with various eye diseases SHAP 99.11% Experimental setup To ensure reproducibility of the results of the proposed model, we have detailed the machine learning parameters and hyperparameters used in our approach in Table 9 . All models were implemented in Python using either the scikit-learn or Keras libraries. While many default settings and hyperparameters can be directly utilized from these packages, we have explicitly listed the specific hyperparameters employed in our experiments for clarity and transparency. Table 9 Machine learning models, packages used, and hyperparameters ML Method Python Package Used Information of Hyperparameters Naive Bayes (NB) scikit-learn var_smoothing&#8201;=&#8201;1e-9 (GaussianNB) Random Forest (RF) scikit-learn n_estimators&#8201;=&#8201;100, max_depth&#8201;=&#8201;None, random_state&#8201;=&#8201;42 Support Vector Machine (SVM) scikit-learn C&#8201;=&#8201;1.0, kernel=&#8217;rbf&#8217;, gamma=&#8217;scale&#8217; XGBoost xgboost n_estimators&#8201;=&#8201;100, learning_rate&#8201;=&#8201;0.1, max_depth&#8201;=&#8201;6, subsample&#8201;=&#8201;1.0 AdaBoost (AB) scikit-learn n_estimators&#8201;=&#8201;50, learning_rate&#8201;=&#8201;1.0,base_estimator&#8201;=&#8201;DecisionTreeClassifier(max_depth&#8201;=&#8201;1) Bagging scikit-learn n_estimators&#8201;=&#8201;10,base_estimator&#8201;=&#8201;DecisionTreeClassifier(),random_state&#8201;=&#8201;42 Decision Tree (DT) scikit-learn criterion=&#8217;gini&#8217;,max_depth&#8201;=&#8201;None,random_state&#8201;=&#8201;42 k-Nearest Neighbors (k-NN) scikit-learn n_neighbors&#8201;=&#8201;5,weights=&#8217;uniform&#8217;,metric=&#8217;minkowski&#8217; Logistic Regression (LR) scikit-learn C&#8201;=&#8201;1.0, solver=&#8217;lbfgs&#8217;, max_iter&#8201;=&#8201;1000, random_state&#8201;=&#8201;42 Discussion In this section, we have discussed the challenges faced while conducting this study and the possible practical challenges that we may have to face to ensure the integration of AI tools into clinical ophthalmology workflows. Practical challenges in AI integration into clinical ophthalmology workflows Eye-XAI has been developed with clinical applicability at its core. To address the practical challenges of integration, our framework incorporates explainable AI (XAI) techniques, specifically SHAP (SHapley Additive exPlanations), which allow clinicians to visualize and understand the rationale behind model predictions. By aligning these visual and textual explanations with known clinical symptoms and ophthalmic features, Eye-XAI bridges the gap between AI predictions and clinical reasoning. Eye-XAI addresses the common criticism of AI as a being &#8220;black box&#8221; tool by integrating explainable AI techniques with symptom-level analyses. Rather than only providing a diagnosis, Eye-XAI offers clear and interpretable justifications for its predictions, highlighting which symptoms or image characteristics contributed the most to a decision. This transparency allows clinicians to trace the diagnostic logic of the model, making it easier to trust and validate the system in clinical practice. By providing clear and understandable diagnostic insights, it helps ophthalmologists understand the reasoning behind AI-generated recommendations. This transparency allows clinicians to critically evaluate the system&#8217;s suggestions, combine them with their own medical experience, and make more informed decisions. As a result, Eye-XAI builds trust among clinicians and encourages a smooth integration into daily clinical practice. To further support real-world use, Eye-XAI is designed for easy integration into hospital and clinic systems, such as electronic health records (EHR) , using standard communication tools such as APIs and HL7 FHIR [ 78 ]. This allows Eye-XAI to deliver decision support directly within existing workflows, without requiring doctors to switch between platforms. Additionally, Eye-XAI has a flexible architecture. It can be installed locally on hospital computers in settings with limited internet access (on-premise use) or run in the cloud with user-friendly dashboards that explain the model&#8217;s decisions, especially helpful for remote consultations. As a next step, we plan to conduct pilot studies in clinical ophthalmology centers to evaluate the real-world effectiveness and usability of Eye-XAI in everyday medical environments. Future prospects of user feedback and evaluation of interpretability To evaluate the interpretability and reliability of Eye-XAI, we plan to conduct a preliminary usability study involving 8&#8211;12 ophthalmologists and residents of ophthalmology. Participants will review 5&#8211;7 anonymized patient cases using the Eye-XAI interface, which provides prediction outputs along with visual explanations such as SHAP feature attributions and saliency maps. After a brief tutorial, clinicians will assess how explanations align with or influence their diagnoses. Feedback will be collected through the System Usability Scale (SUS) [ 79 ] and a custom survey that measures interpretability and trust. In addition, structured interviews will explore participants&#8217; views on the usefulness of explanations for clinical decision making and patient communication. Quantitative data from surveys will be analyzed to measure overall usability and trust levels, while qualitative analysis of interview transcripts will identify key themes regarding the clinical relevance and potential impact of the system. The aim of this study is therefore to demonstrate the effectiveness of Eye-XAI in supporting clinicians, identify areas for improvement, and provide a foundation for future pilot deployments in real clinical settings. We recognize that SHAP explanations depend on the underlying model and may propagate spurious associations. To mitigate this, we used cross-validation, regularization, and alignment with clinical knowledge, and emphasize that SHAP serves as a decision-support tool rather than stand-alone evidence [ 69 , 70 ]. Plan for external validation In this study, we have concentrated on geographically localized patients (patients are only from Bangladesh) and it may not fully capture inter-population variations in ocular health indicators. For the experiments presented in this study, both train-test split and 10-fold cross validation techniques has been tested for each of the ML methods. Eventually, external validation may improve the acceptability of the models by clinicians. Therefore, we intend to evaluate Eye-XAI in publicly available ophthalmic data sets from other regions such as the MESSIDOR data sets (France) [ 41 ] and EyePACS (USA) [ 42 ]. This cross-population validation will help assess the robustness, fairness, and generalizability of the model between ethnic and demographic groups. To assess the robustness and generalizability of Eye-XAI beyond symptom-based data, we propose external validation using the MESSIDOR and EyePACS fundus image datasets. This adaptation introduces challenges related to the change in data modality (from structured symptoms to raw images), the interpretability of learned visual features, the heterogeneity of the data set and the alignment of labeling systems. By addressing these challenges, Eye-XAI can demonstrate its applicability in both structured clinical data and high-dimensional imaging data, thus ensuring broader clinical relevance and trustworthiness. Furthermore, we will explore domain adaptation techniques to minimize potential performance biases introduced by distributional changes. Our current Eye-XAI pipeline follows a segmentation \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\rightarrow$$\\end{document} classification workflow designed for transparency and moderate compute (classical UNet-style segmentation of OD/OC or ROI, lightweight CNN/GBM classifiers, SHAP explanations). Recent &#8220;hybrid&#8221; approaches combine stronger segmenters with meta-heuristics or pair them with non-standard classifiers or generators&#8212;e.g., Grey-Wolf&#8211;optimized U-Net/UNet++&#8201;+&#8201;Capsule Networks for glaucoma/DR, and GAN-assisted augmentation&#8201;+&#8201;MobileNetV2 for glaucoma screening. These studies report gains that generally come from (i) finer boundary extraction of OD/OC and lesions (U-Net/UNet++), (ii) robustness of pose/part-whole modeling (CapsNet), and (iii) data distribution smoothing via GANs; but they introduce extra complexity and compute (optimization loops, capsule routing, adversarial training) and raise reproducibility risks across sites/devices. Limitations of the study Our cohort contains 563 patients. Although this sample is adequate for exploratory benchmarking, it is modest for training and reliably comparing nine models, especially as the outcome/classes are imbalanced; the feature space is high-dimensional, and hyperparameters are extensively tuned. In such settings, estimates can have high variance and model selection can be optimistically biased (the &#8220;winner&#8217;s curse&#8221; [ 80 ]). There is no universal sample-size rule for ML; practical considerations may apply. A larger and demographically broader cohort would help stabilize estimates by narrowing confidence intervals, reducing fold-to-fold variability, and lowering the risk of overfitting or model-ranking reversals. It would also improve generalization by exposing the model to a wider range of covariate changes such as age, sex, comorbidities, care pathways, and device differences, thus improving performance in external datasets and mitigating domain change. With more data, stronger and more expressive models could be trained, allowing richer feature engineering and complex interactions without overfitting. In addition, a larger cohort would improve the calibration and clinical utility, leading to better probability calibration (e.g., lower Brier score, expected calibration error), more reliable decision-curve analyses, and subgroup-specific threshold policies. This expansion would also support fairness auditing by allowing well-equipped subgroup analyses across demographic categories and facilitating meaningful bias mitigation. Finally, it would permit true holdout validation through untouched external test sets or multi-site evaluations, thereby minimizing model-selection inflation and strengthening confidence in clinical applicability. Although retaining all features improved interpretability in this study, it may also introduce risks of redundancy, overfitting, and potential misinterpretation in real-world clinical practice [ 81 , 82 ]. Redundant variables can amplify noise and make models overly complex, limiting generalization to external datasets [ 83 ]. To minimize this, we employed rigorous cross-validation and regularized classifiers, but we recognize that a balance must be struck between interpretability and parsimony [ 84 ]. Future extensions of Eye-XAI will explore stability-driven or clinically guided feature selection strategies, ensuring that the retained features are not only interpretable but also robust, non-redundant, and clinically meaningful [ 85 ]. Conclusions For any patient, proper detection of a eye disease is a very important and challenging task that informs the appropriate cure and care. Therefore, doctors rely mostly on the symptoms they find on the patients on their first visit. Based on a symptom analysis, it is evident that the most important symptoms that trigger diseases can be found. This paper relies on a data set collected from physicians offices that is used for explainable machine learning methods to find the ranks and adoption of the most important symptoms. With a highly accurate machine learning model (99. 11%), an additional explainability pipeline has been adopted for the study. For future directions, the surveillance of each patient is necessary to understand the outcomes and prognosis of the disease. One of the limitations of this study that can be highlighted is potential data biases arising from the fact that our dataset was collected primarily from a specific regional population (Bangladesh). We acknowledge that this may limit the model&#8217;s generalizability to other demographic or geographic populations, and we suggest future work involving external validation using datasets from diverse settings. In addition, a potential deployment roadmap for Eye-XAI, highlighting how the model can be integrated into existing ophthalmology clinic workflows was provided. This includes embedding the model within clinical decision support systems, where it can assist ophthalmologists in early diagnosis and feature-based reasoning, while ensuring that its use aligns with clinical guidelines (i.e., Food and Drug Administration (FDA) [ 86 ], Conformit&#233; Europ&#233;enne (CE) Marking and European MDR [ 87 ]) and patient data privacy protocols (i.e., Health Insurance Portability and Accountability Act (HIPAA) [ 88 ], General Data Protection Regulation (GDPR) [ 89 ]). Electronic supplementary material Below is the link to the electronic supplementary material. Supplementary Material 1 Publisher&#8217;s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements This research has been partially supported by the Information and Communication Technology (ICT) division of the Peoples Republic of Bangladesh. This includes technical and logistics support provided as a form of fellowship to the doctoral student, Ahmed Al Marouf. Author contributions AAM, MMM, JR, RA, and SSR contributed to conceptualization, methodology, writing - original draft and visualization. JR and RA contributed to supervision and project administration. OJ contributed to data curation and validation. AAM contributed to formal analysis. Funding No external funding was acquired for this study. Data availability The datasets used and/or analyzed during the current study are available from the corresponding author. Declarations Ethics approval and consent to participate The present study complied with the ethical guidelines of the Declarations of Helsinki of 1975. The informed consent requirement was waived by the Research Ethics Committee of the Faculty of Science and Information Technology and was approved the ethics (Application No: REC-FSIT-20220816001, 08/16/2022). Consent for publication Patients have provided their written consent to publish the data without any conservation. The anonymity of the data was ensured by the ethics board. None of the participants information, including name, age, photo or any identifiable components has been used or presented in this study. Conflict of interest The authors declare no conflict of interest. References 1. Ribeiro MT, Singh S, Guestrin C. Why should i trust you? explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2016, pp. 1135&#8211;44. 2. Saraswat D Bhattacharya P Verma A Prasad VK Tanwar S Sharma G Bokoro PN Sharma R Explainable ai for healthcare 5.0: opportunities and challenges IEEE Access 2022 10 84486 517 10.1109/ACCESS.2022.3197671 PMC9423030 36345376 Saraswat D, Bhattacharya P, Verma A, Prasad VK, Tanwar S, Sharma G, Bokoro PN, Sharma R. Explainable ai for healthcare 5.0: opportunities and challenges. IEEE Access. 2022;10:84486&#8211;517. 3. Sutradhar I, Gayen P, Hasan M, Gupta RD, Roy T, Sarker M. Eye diseases: the neglected health condition among urban slum population of dhaka, Bangladesh. BMC Ophthalmol. 2019;19(38). 10.1186/s12886-019-1043-z. 10.1186/s12886-019-1043-z PMC6357461 30704423 4. Marouf A Mottalib M Alhajj R Rokne J Jafarullah O An efficient approach to predict eye diseases from symptoms using machine learning and ranker-based feature selection methods Bioengineering 2022 10 25 10.3390/bioengineering10010025 36671598 PMC9854513 Marouf A, Mottalib M, Alhajj R, Rokne J, Jafarullah O. An efficient approach to predict eye diseases from symptoms using machine learning and ranker-based feature selection methods. Bioengineering. 2022;10:25. 36671598 10.3390/bioengineering10010025 PMC9854513 5. Ayodele TO Types of machine learning algorithms New Adv Mach Learn 2010 3 19 48 Ayodele TO. Types of machine learning algorithms. New Adv Mach Learn. 2010;3:19&#8211;48. 6. Mair C Kadoda G Lefley M Phalp K Schofield C Shepperd M Webster S An investigation of machine learning based prediction systems J Syst Softw 2000 53 23 29 10.1016/S0164-1212(00)00005-4 Mair C, Kadoda G, Lefley M, Phalp K, Schofield C, Shepperd M, Webster S. An investigation of machine learning based prediction systems. J Syst Softw. 2000;53:23&#8211;29. 7. Mackenzie A The production of prediction: what does machine learning want? Eur J Cult Stud 2015 18 429 45 10.1177/1367549415577384 Mackenzie A. The production of prediction: what does machine learning want? Eur J Cult Stud. 2015;18:429&#8211;45. 8. Abbas S Qaisar A Farooq MS Saleem M Ahmad M Khan MA Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence Sensors 2024 24 20 6618 10.3390/s24206618 39460097 PMC11510864 Abbas S, Qaisar A, Farooq MS, Saleem M, Ahmad M, Khan MA. Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence. Sensors. 2024;24(20):6618. 39460097 10.3390/s24206618 PMC11510864 9. Gour N Khanna P Multi-class multi-label ophthalmological disease detection using transfer learning based convolutional neural network Biomed Signal Process Control 2021 66 102329 10.1016/j.bspc.2020.102329 Gour N, Khanna P. Multi-class multi-label ophthalmological disease detection using transfer learning based convolutional neural network. Biomed Signal Process Control. 2021;66:102329. 10. Li C, Ye J, He J, Wang S, Qiao Y, Gu L. Dense correlation network for automated multi-label ocular disease detection with paired color fundus photographs. 2020 IEEE 17th International Symposium On Biomedical Imaging (ISBI). 2020, pp. 1&#8211;4. 11. Oda M Yamaguchi T Fukuoka H Ueno Y Mori K Automated eye disease classification method from anterior eye image using anatomical structure focused image classification technique Med Imag 2020: Comput-Aided Diagnosis 2020 11314 991 96 Oda M, Yamaguchi T, Fukuoka H, Ueno Y, Mori K. Automated eye disease classification method from anterior eye image using anatomical structure focused image classification technique. Med Imag 2020: Comput-Aided Diagnosis. 2020;11314:991&#8211;96. 12. He J Li C Ye J Qiao Y Gu L Multi-label ocular disease classification with a dense correlation deep neural network Biomed Signal Process Control 2021 63 102167 10.1016/j.bspc.2020.102167 He J, Li C, Ye J, Qiao Y, Gu L. Multi-label ocular disease classification with a dense correlation deep neural network. Biomed Signal Process Control. 2021;63:102167. 13. Hameed S Ahmed H Eye diseases classification using back propagation with parabola learning rate Al-Qadisiyah J Pure Sci 2021 26 1 9 10.29350/qjps.2021.26.1.1220 Hameed S, Ahmed H. Eye diseases classification using back propagation with parabola learning rate. Al-Qadisiyah J Pure Sci. 2021;26:1&#8211;9. 14. Ahmed H Hameed S Eye diseases classification using back propagation with welch estimation based-learning rate Al-Qadisiyah J Pure Sci 2021 26 22 30 Ahmed H, Hameed S. Eye diseases classification using back propagation with welch estimation based-learning rate. Al-Qadisiyah J Pure Sci. 2021;26:22&#8211;30. 15. Li N, Li T, Hu C, Wang K, Kang H. A benchmark of ocular disease intelligent recognition: one shot for multi-disease detection. Benchmarking, Measuring, and Optimizing: Third BenchCouncil International Symposium, Bench 2020, Virtual Event, November 15&#8211;16, 2020, Revised Selected Papers 3. 2021, pp. 177&#8211;93. 16. He J Li C Ye J Qiao Y Gu L Self-speculation of clinical features based on knowledge distillation for accurate ocular disease classification Biomed Signal Process Control 2021 67 102491 10.1016/j.bspc.2021.102491 He J, Li C, Ye J, Qiao Y, Gu L. Self-speculation of clinical features based on knowledge distillation for accurate ocular disease classification. Biomed Signal Process Control. 2021;67:102491. 17. Bhati A Gour N Khanna P Ojha A Discriminative kernel convolution network for multi-label ophthalmic disease detection on imbalanced fundus image dataset Comput Biol Med 2023 153 106519 10.1016/j.compbiomed.2022.106519 36608462 Bhati A, Gour N, Khanna P, Ojha A. Discriminative kernel convolution network for multi-label ophthalmic disease detection on imbalanced fundus image dataset. Comput Biol Med. 2023;153:106519. 36608462 10.1016/j.compbiomed.2022.106519 18. Bitto A Mahmud I Multi categorical of common eye disease detect using convolutional neural network: a transfer learning approach Bull Electr Eng Inf 2022 11 2378 87 10.11591/eei.v11i4.3834 Bitto A, Mahmud I. Multi categorical of common eye disease detect using convolutional neural network: a transfer learning approach. Bull Electr Eng Inf. 2022;11:2378&#8211;87. 19. Emir B Colak E Performance analysis of pretrained convolutional neural network models for ophthalmological disease classification Arq Bras Oftalmol 2023 87 2022 124 10.5935/0004-2749.2022-0124 PMC11623917 39298728 Emir B, Colak E. Performance analysis of pretrained convolutional neural network models for ophthalmological disease classification. Arq Bras Oftalmol. 2023;87:2022&#8211;124. 10.5935/0004-2749.2022-0124 PMC11623917 39298728 20. Kumar D, Bakariya B, Verma C, Illes Z. Cataract disease identification using transformer and convolution neural network: a novel framework. 2023 3rd International Conference On Technological Advancements In Computational Sciences (ICTACS). 2023, pp. 1230&#8211;35. 21. Mahmood S Chaabouni S Fakhfakh A A new technique for cataract eye disease diagnosis in deep learning Periodicals Eng Nat Sci 2023 11 14 26 10.21533/pen.v11.i6.191 Mahmood S, Chaabouni S, Fakhfakh A. A new technique for cataract eye disease diagnosis in deep learning. Periodicals Eng Nat Sci. 2023;11:14&#8211;26. 22. Qaddour M, Touimi Y, Minaoui K. Classification of retinal fundus images using convolution neural network (cnn). 2023 IEEE International Conference on Advances in Data-Driven Analytics and Intelligent Systems (ADACIS). 2023, pp. 1&#8211;7. 23. Vengurlekar M, Nadaf M, Fernandes N, Kumar K. Conjunctivitis eye detection using deep learning. 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC). 2024, pp. 1591&#8211;97. 24. Jmour N, Zayen S, Abdelkrim A. Convolutional neural networks for image classification. 2018 International Conference on Advanced Systems and Electric Technologies (IC ASET). IEEE; 2018, pp. 397&#8211;402. 25. Govindharaj I Deva Priya W Soujanya K Senthilkumar K Shantha Shalini K Ravichandran S Advanced glaucoma disease segmentation and classification with grey wolf optimized u- net++ and capsule networks Int Ophthalmol 2025 45 1 1 24 10.1007/s10792-025-03602-6 40576831 Govindharaj I, Deva Priya W, Soujanya K, Senthilkumar K, Shantha Shalini K, Ravichandran S. Advanced glaucoma disease segmentation and classification with grey wolf optimized u- net++ and capsule networks. Int Ophthalmol. 2025;45(1):1&#8211;24. 10.1007/s10792-025-03602-6 40576831 26. Govindharaj I Ramesh T Poongodai A Udayasankaran P Ravichandran S Grey wolf optimization technique with u-shaped and capsule networks-a novel framework for glaucoma diagnosis MethodsX 2025 14 103285 10.1016/j.mex.2025.103285 40236793 PMC11999292 Govindharaj I, Ramesh T, Poongodai A, Udayasankaran P, Ravichandran S, et al. Grey wolf optimization technique with u-shaped and capsule networks-a novel framework for glaucoma diagnosis. MethodsX. 2025;14:103285. 40236793 10.1016/j.mex.2025.103285 PMC11999292 27. Govindharaj I Poongodai A Santhakumar D Ravichandran S Vijaya Prabhu R Udayakumar K Yazhinian S Enhanced diabetic retinopathy detection using u-shaped network and capsule network-driven deep learning MethodsX 2025 14 103052 10.1016/j.mex.2024.103052 39802427 PMC11719411 Govindharaj I, Poongodai A, Santhakumar D, Ravichandran S, Vijaya Prabhu R, Udayakumar K, Yazhinian S, et al. Enhanced diabetic retinopathy detection using u-shaped network and capsule network-driven deep learning. MethodsX. 2025;14:103052. 39802427 10.1016/j.mex.2024.103052 PMC11719411 28. Govindharaj I Rampriya R Michael G Yazhinian S Dinesh Kumar K Anandh R Capsule network-based deep learning for early and accurate diabetic retinopathy detection Int Ophthalmol 2025 45 1 78 10.1007/s10792-024-03391-4 39966199 Govindharaj I, Rampriya R, Michael G, Yazhinian S, Dinesh Kumar K, Anandh R. Capsule network-based deep learning for early and accurate diabetic retinopathy detection. Int Ophthalmol. 2025;45(1):78. 39966199 10.1007/s10792-024-03391-4 29. Govindharaj I Santhakumar D Pugazharasi K Ravichandran S Prabhu RV Raja J Enhancing glaucoma diagnosis: generative adversarial networks in synthesized imagery and classification with pretrained mobilenetv2 MethodsX 2025 14 103116 10.1016/j.mex.2024.103116 39811622 PMC11732477 Govindharaj I, Santhakumar D, Pugazharasi K, Ravichandran S, Prabhu RV, Raja J. Enhancing glaucoma diagnosis: generative adversarial networks in synthesized imagery and classification with pretrained mobilenetv2. MethodsX. 2025;14:103116. 39811622 10.1016/j.mex.2024.103116 PMC11732477 30. Singh A, Mohammed A, Zelek J, Lakshminarayanan V. Interpretation of deep learning using attributions: application to ophthalmic diagnosis. Appl Mach Learn. 2020;11511:39&#8211;49. 31. Oh S Park Y Cho K Kim S Explainable machine learning model for glaucoma diagnosis and its interpretation Diagnostics 2021 11 510 10.3390/diagnostics11030510 33805685 PMC8001225 Oh S, Park Y, Cho K, Kim S. Explainable machine learning model for glaucoma diagnosis and its interpretation. Diagnostics. 2021;11:510. 33805685 10.3390/diagnostics11030510 PMC8001225 32. Apon T, Hasan M, Islam A, Alam M. Demystifying deep learning models for retinal oct disease classification using explainable ai. 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). 2021, pp. 1&#8211;6. 33. Obayya M Nemri N Nour M Duhayyim MA Mohsen H Rizwanullah M Zamani AS Motwakel A Explainable artificial intelligence enabled teleophthalmology for diabetic retinopathy grading and classification Appl Sci 2022 12 8749 10.3390/app12178749 Obayya M, Nemri N, Nour M, Duhayyim MA, Mohsen H, Rizwanullah M, Zamani AS, Motwakel A. Explainable artificial intelligence enabled teleophthalmology for diabetic retinopathy grading and classification. Appl Sci. 2022;12:8749. 34. Kamal M Dey N Chowdhury L Hasan S Santosh K Explainable ai for glaucoma prediction analysis to understand risk factors in treatment planning IEEE Trans Instrum Meas 2022 71 1 9 10.1109/TIM.2022.3171613 Kamal M, Dey N, Chowdhury L, Hasan S, Santosh K. Explainable ai for glaucoma prediction analysis to understand risk factors in treatment planning. IEEE Trans Instrum Meas. 2022;71:1&#8211;9. 35. Chayan T, Islam A, Rahman E, Reza M, Apon T, Alam M. Explainable ai based glaucoma detection using transfer learning and lime. 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE). 2022, pp. 1&#8211;6. 36. Shah H, Patel R, Hegde S, Dalvi H. Xai meets ophthalmology: an explainable approach to cataract detection using vgg-19 and grad-cam. 2023 IEEE Pune Section International Conference (PuneCon). 2023, pp. 1&#8211;8. 37. Yonehara M, Nakagawa Y, Ayatsuka Y, Hara Y, Shoji J, Ebihara N, Inomata T, Huang T, Nagino K, Fukuda K. Others: use of explainable ai on slit-lamp images of anterior surface of eyes to diagnose allergic conjunctival diseases. Allergology Int. 2024. 10.1016/j.alit.2024.07.004 39155213 38. Shahzad T, Saleem M, Farooq M, Abbas S, Khan M, Ouahada K. Developing a transparent diagnosis model for diabetic retinopathy using explainable ai. IEEE Access. 2024. 39. Abbas S Qaisar A Farooq M Saleem M Ahmad M Khan MA Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence Sensors 2024 24 6618 10.3390/s24206618 39460097 PMC11510864 Abbas S, Qaisar A, Farooq M, Saleem M, Ahmad M, Khan MA. Smart vision transparency: efficient ocular disease prediction model using explainable artificial intelligence. Sensors. 2024;24:6618. 39460097 10.3390/s24206618 PMC11510864 40. Mridha K, Wang M, Zhang L. Ai-driven diagnostics in ophthalmology: tailored deep learning models for diabetic retinopathy with xai insights. Proceedings of the 16th International Conference on, vol. 101, 2024; p. 73&#8211;82. 41. Decenciere E Zhang X Cazuguel G Lay B Cochener B Trone C Klein J-C Feedback on a publicly distributed image database: the messidor database Image Anal Stereol 2014 33 3 231 34 10.5566/ias.1155 Decenciere E, Zhang X, Cazuguel G, Lay B, Cochener B, Trone C, Klein J-C. Feedback on a publicly distributed image database: the messidor database. Image Anal Stereol. 2014;33(3):231&#8211;34. 10.5566/ias.1155. 42. Gulshan V Peng L Coram M Stumpe MC Wu D Narayanaswamy A Venugopalan S Widner K Madams T Cuadros J Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs JAMA 2016 316 22 2402 10 10.1001/jama.2016.17216 27898976 Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, Venugopalan S, Widner K, Madams T, Cuadros J, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA. 2016;316(22):2402&#8211;10. 10.1001/jama.2016.17216. 27898976 10.1001/jama.2016.17216 43. Myles AJ Feudale RN Liu Y Woody NA Brown SD An introduction to decision tree modeling J Educ Chang Chemom: A J Chemom Soc 2004 18 6 275 85 Myles AJ, Feudale RN, Liu Y, Woody NA, Brown SD. An introduction to decision tree modeling. J Educ Chang Chemom: A J Chemom Soc. 2004;18(6):275&#8211;85. 44. Rish I. An empirical study of the naive bayes classifier. IJCAI 2001Workshop on Empirical Methods in Artificial Intelligence. 2001, pp. 41&#8211;46, vol. 3. 45. Breiman L Random forests Mach Learn 2001 45 5 32 10.1023/A:1010933404324 Breiman L. Random forests. Mach Learn. 2001;45:5&#8211;32. 46. Kurichina M Duin RPW Bagging for linear classifiers Pattern Recognit 1998 31 7 909 30 10.1016/S0031-3203(97)00110-6 Kurichina M, Duin RPW. Bagging for linear classifiers. Pattern Recognit. 1998;31(7):909&#8211;30. 47. An T-K, Kim M-H. A new diverse adaboost classifier. 2010 International Conference on Artificial Intelligence and Computational Intelligence, vol. 1. IEEE. 2010; p. 359&#8211;63. 48. Chen T, Guestrin C. Xgboost: a scalable tree boosting system. Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining. 2016; p. 785&#8211;94. 49. Zhang S Li X Zong M Zhu X Wang R Efficient knn classification with different numbers of nearest neighbors IEEE Trans Neural Networks Learn Syst 2017 29 5 1774 85 10.1109/TNNLS.2017.2673241 28422666 Zhang S, Li X, Zong M, Zhu X, Wang R. Efficient knn classification with different numbers of nearest neighbors. IEEE Trans Neural Networks Learn Syst. 2017;29(5):1774&#8211;85. 10.1109/TNNLS.2017.2673241 28422666 50. LaValley MP Logistic regression Circulation 2008 117 18 2395 99 10.1161/CIRCULATIONAHA.106.682658 18458181 LaValley MP. Logistic regression. Circulation. 2008;117(18):2395&#8211;99. 18458181 10.1161/CIRCULATIONAHA.106.682658 51. Cortes C. Support-vector networks. Mach Learn. 1995. 52. Kourou K Exarchos TP Exarchos KP Karamouzis MV Fotiadis DI Machine learning applications in cancer prognosis and prediction Comput Struct Biotechnol J 2015 13 8 17 10.1016/j.csbj.2014.11.005 25750696 PMC4348437 Kourou K, Exarchos TP, Exarchos KP, Karamouzis MV, Fotiadis DI. Machine learning applications in cancer prognosis and prediction. Comput Struct Biotechnol J. 2015;13:8&#8211;17. 25750696 10.1016/j.csbj.2014.11.005 PMC4348437 53. Ahmad MA, Eckert C, Teredesai A. Interpretable machine learning in healthcare. Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. 2018; p. 559&#8211;60. 54. Pearson K On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling Philos Mag 1900 5 157 75 10.1080/14786440009463897 Pearson K. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philos Mag. 1900;5:157&#8211;75. 10.1080/14786440009463897 55. Forman G An extensive empirical study of feature selection metrics for text classification J Mach Learn Res 2003 3 1289 305 Forman G. An extensive empirical study of feature selection metrics for text classification. J Mach Learn Res. 2003;3:1289&#8211;305. 56. Gao Z, Xu Y, Meng F, Qi F, Lin L. Improved information gain-based feature selection for text categorization. 2014 4th International Conference on Wireless Communications, Vehicular Technology, Information Theory and Aerospace &amp; Electronic Systems (VITAE). 2014, pp. 11&#8211;14. 57. Yu L, Liu H. Feature selection for high-dimensional data: a fast correlationbased filter solution. Proceedings of the 20th International Conference on Machine Learning (ICML-03). 2003; p. 856&#8211;63. 58. Pearson K Liii. On lines and planes of closest fit to systems of points in space Lond Edinb Dublin Philos Mag J Sci 1901 2 559 72 10.1080/14786440109462720 Pearson K. Liii. On lines and planes of closest fit to systems of points in space. Lond Edinb Dublin Philos Mag J Sci. 1901;2:559&#8211;72. 59. Abdi H Williams LJ Principal component analysis Wiley Interdiscip Rev Comput Stat 2010 2 433 59 10.1002/wics.101 Abdi H, Williams LJ. Principal component analysis. Wiley Interdiscip Rev Comput Stat. 2010;2:433&#8211;59. 60. Song F, Guo Z, Mei D. Feature selection using principal component analysis. 2010 International Conference on System Science, Engineering Design and Manufacturing Informatization. 2010; p. 27&#8211;30. 61. Kira K Rendell LA The feature selection problem: traditional methods and a new algorithm AAAI 1992 2 129 34 Kira K, Rendell LA. The feature selection problem: traditional methods and a new algorithm. AAAI. 1992;2:129&#8211;34. 62. Abraham MT Satyam N Lokesh R Pradhan B Alamri A Factors affecting landslide susceptibility mapping: assessing the influence of different machine learning approaches, sampling strategies and data splitting Land 2021 10 989 10.3390/land10090989 Abraham MT, Satyam N, Lokesh R, Pradhan B, Alamri A. Factors affecting landslide susceptibility mapping: assessing the influence of different machine learning approaches, sampling strategies and data splitting. Land. 2021;10:989. 63. Refaeilzadeh P Tang L Liu H Cross-validation Encycl Database Syst 2009 5 532 38 10.1007/978-0-387-39940-9_565 Refaeilzadeh P, Tang L, Liu H. Cross-validation. Encycl Database Syst. 2009;5:532&#8211;38. 64. Lundberg S. Shap (shapley additive explanations). Computer software. Github Repository. 2019. https://github.com/slundberg/shap . 65. Lundberg S. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874. 2017. 66. Molnar C. Interpretable machine learning: a guide for making black box models explainable. 2nd edn. Leanpub; 2020. https://doi.org/christophm.github.io/interpretable-ml-book/. 67. Ribeiro MT, Singh S, Guestrin C. &#8220;Why should i trust you?&#8221;: explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM; 2016, pp. 1135&#8211;44. 68. Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: visual explanations from deep networks via gradient-based localization. Proceedings of the IEEE International Conference on Computer Vision (ICCV). 2017, pp. 618&#8211;26. 69. Carvalho DV Pereira EM Cardoso JS Machine learning interpretability: a survey on methods and metrics Electronics 2019 8 8 832 10.3390/electronics8080832 Carvalho DV, Pereira EM, Cardoso JS. Machine learning interpretability: a survey on methods and metrics. Electronics. 2019;8(8):832. 70. Rasheed F Qayyum A Qadir J Sivathamboo S Kwan P Kuhlmann L O&#8217;Brien T Razi A Explainable artificial intelligence for healthcare: from black box to interpretable models Neural Comput Appl 2022 34 16 13371 406 Rasheed F, Qayyum A, Qadir J, Sivathamboo S, Kwan P, Kuhlmann L, O&#8217;Brien T, Razi A. Explainable artificial intelligence for healthcare: from black box to interpretable models. Neural Comput Appl. 2022;34(16):13371&#8211;406. 71. Marouf AA Hasan MK Mahmud H Comparative analysis of feature selection algorithms for computational personality prediction from social media IEEE Trans Comput Soc Syst 2020 7 587 99 10.1109/TCSS.2020.2966910 Marouf AA, Hasan MK, Mahmud H. Comparative analysis of feature selection algorithms for computational personality prediction from social media. IEEE Trans Comput Soc Syst. 2020;7:587&#8211;99. 10.1109/TCSS.2020.2966910. 72. Ghosh P Azam S Jonkman M Karim A Shamrat FJM Ignatious E Shultana S Beeravolu AR Boer FD Efficient prediction of cardiovascular disease using machine learning algorithms with relief and lasso feature selection techniques IEEE Access 2021 9 19304 26 10.1109/ACCESS.2021.3053759 Ghosh P, Azam S, Jonkman M, Karim A, Shamrat FJM, Ignatious E, Shultana S, Beeravolu AR, Boer FD. Efficient prediction of cardiovascular disease using machine learning algorithms with relief and lasso feature selection techniques. IEEE Access. 2021;9:19304&#8211;26. 10.1109/ACCESS.2021.3053759. 73. Chen T, Contributors X. Xgboost documentation. https://xgboost.readthedocs.io/en/stable/python/ . Accessed on 2 February 2024. 74. Huang X Marques-Silva J On the failings of shapley values for explainability Int J Approximate Reasoning 2024 171 109112 10.1016/j.ijar.2023.109112 Huang X, Marques-Silva J. On the failings of shapley values for explainability. Int J Approximate Reasoning. 2024;171:109112. 75. Kaur S Singh R Sharma P Explainable ai for diabetic retinopathy detection using clinical symptoms J Med Imag Health Inf 2021 11 3 543 51 Kaur S, Singh R, Sharma P. Explainable ai for diabetic retinopathy detection using clinical symptoms. J Med Imag Health Inf. 2021;11(3):543&#8211;51. 76. Wang Y, Chen L, Zhang R. Interpretable glaucoma risk prediction using electronic medical records and patient symptoms. Proceedings of the IEEE International Conference on Healthcare Informatics. IEEE; 2020, pp. 212&#8211;18. 77. Li X Zhou M Huang W A multi-modal explainable ai framework for cataract diagnosis combining imaging and symptom data Comput Biol Med 2022 140 105123 Li X, Zhou M, Huang W. A multi-modal explainable ai framework for cataract diagnosis combining imaging and symptom data. Comput Biol Med. 2022;140:105123. 78. Bender D, Sartipi K. Hl7 fhir: an agile and restful approach to healthcare information exchange. Proceedings of the 26th IEEE International Symposium on Computer-based Medical Systems. IEEE; 2013, pp. 326&#8211;31. 79. Brooke J Sus: a &#8220;quick and dirty&#8221; usability scale Usability Evaluation Ind 1996 189 194 4 7 Brooke J. Sus: a &#8220;quick and dirty&#8221; usability scale. Usability Evaluation Ind. 1996;189(194):4&#8211;7. 80. Sculley D Snoek J Wiltschko A Rahimi A Winner&#8217;s curse On Pace, Prog Empir Rigor 2018 1 1285 98 Sculley D, Snoek J, Wiltschko A, Rahimi A. Winner&#8217;s curse. On Pace, Prog Empir Rigor. 2018;1:1285&#8211;98. 81. Guyon I Elisseeff A An introduction to variable and feature selection J Mach Learn Res 2003 3 1157 82 Guyon I, Elisseeff A. An introduction to variable and feature selection. J Mach Learn Res. 2003;3:1157&#8211;82. 82. Kuhn M Johnson K Applied predictive modeling 2013 Springer Kuhn M, Johnson K. Applied predictive modeling. Springer; 2013. 83. Saeys Y Inza I Larra&#241;aga P: A review of feature selection techniques in bioinformatics Bioinformatics 2007 23 19 2507 17 10.1093/bioinformatics/btm344 17720704 Saeys Y, Inza I, Larra&#241;aga P. A review of feature selection techniques in bioinformatics. Bioinformatics. 2007;23(19):2507&#8211;17. 17720704 10.1093/bioinformatics/btm344 84. Rudin C Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead Nat Mach Intell 2019 1 5 206 15 10.1038/s42256-019-0048-x 35603010 PMC9122117 Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell. 2019;1(5):206&#8211;15. 35603010 10.1038/s42256-019-0048-x PMC9122117 85. Meinshausen N B&#252;hlmann P Stability selection J Retailing R Stat Soc: Ser B (Stat Methodol) 2010 72 4 417 73 10.1111/j.1467-9868.2010.00740.x Meinshausen N, B&#252;hlmann P. Stability selection. J Retailing R Stat Soc: Ser B (Stat Methodol). 2010;72(4):417&#8211;73. 86. Food U. Drug administration (fda). proposed regulatory framework for modifications to artificial intelligence. Machine learning (AI/ML)-based software as a medical device (SaMD)&#8212;discussion paper and request for feedback (US food &amp; drug administration (FDA), 2019). 2021. 87. Vila&#231;a H. Regulation eu 2017/745 on medical devices&#8211;implementation analysis in Portugal by the distributors. Eur J Criminol Public Health. 2020;30(Supplement 5):166&#8211;1214. 88. Rule RC. Us department of health &amp; human services. 2022. https://www.hhs.gov/ohrp/regulations-and-policy/regulations/finalized-revisions-commonrule/index.html . Accessed 04 Oct 2022. 89. Regulation P General data protection regulation Intouch 2018 25 1 5 Regulation P. General data protection regulation. Intouch. 2018;25:1&#8211;5."
}