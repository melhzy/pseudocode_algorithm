{
  "pmcid": "PMC12682651",
  "source": "PMC",
  "download_date": "2025-12-09T16:06:40.947449",
  "metadata": {
    "journal_title": "Frontiers in Oncology",
    "journal_nlm_ta": "Front Oncol",
    "journal_iso_abbrev": "Front Oncol",
    "journal": "Frontiers in Oncology",
    "pmcid": "PMC12682651",
    "doi": "10.3389/fonc.2025.1672274",
    "title": "Research on breast tumor segmentation based on the Mamba architecture",
    "authors": [
      "Wei Weihao",
      "Wu Jiacheng",
      "Shao Guangming"
    ],
    "abstract": "Medical image segmentation is fundamental for disease diagnosis, particularly in the context of breast cancer, a prevalent malignancy affecting women. The accuracy of lesion localization and preservation of image details are essential for ensuring the integrity of lesion segmentation. However, the low resolution of breast tumor B-mode ultrasound images poses challenges in precisely identifying lesion sites. To address this issue, this study introduces the Mamba architecture model, which combines three foundational models with the long-sequence processing model Mamba to develop a novel segmentation model for breast tumor ultrasound images. The selective mechanism and hardware-aware algorithm of the Mamba model enable longer sequence inputs and faster computing speeds. Moreover, integrating a complete chain of VMamba blocks into the basic model enhances segmentation accuracy and image detail processing capabilities. Experimental segmentation was performed on two benchmark ultrasound datasets (BUSI and BUS-BRA) using both the baseline and improved models. The results were compared using metrics such as Dice and IoU, with additional evaluations conducted under small-sample training conditions. This study is intended to provide guidance for the future development of medical image segmentation. Moreover, the experimental results demonstrate that the model incorporating the Mamba architecture achieves superior performance on breast ultrasound images.",
    "keywords": [
      "breast tumors",
      "medical image segmentation",
      "Mamba",
      "selective mechanism",
      "hardware-aware algorithm"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Front Oncol</journal-id><journal-id journal-id-type=\"iso-abbrev\">Front Oncol</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1755</journal-id><journal-id journal-id-type=\"pmc-domain\">frontonco</journal-id><journal-id journal-id-type=\"publisher-id\">Front. Oncol.</journal-id><journal-title-group><journal-title>Frontiers in Oncology</journal-title></journal-title-group><issn pub-type=\"epub\">2234-943X</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682651</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682651.1</article-id><article-id pub-id-type=\"pmcaid\">12682651</article-id><article-id pub-id-type=\"pmcaiid\">12682651</article-id><article-id pub-id-type=\"doi\">10.3389/fonc.2025.1672274</article-id><article-version-alternatives><article-version article-version-type=\"pmc-version\">1</article-version><article-version article-version-type=\"Version of Record\" vocab=\"NISO-RP-8-2008\"/></article-version-alternatives><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Research on breast tumor segmentation based on the Mamba architecture</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wei</surname><given-names initials=\"W\">Weihao</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3191976/overview\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"J\">Jiacheng</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Shao</surname><given-names initials=\"G\">Guangming</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref><xref rid=\"c001\" ref-type=\"corresp\">\n<sup>*</sup>\n</xref><uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3146063/overview\"/><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role></contrib></contrib-group><aff id=\"aff1\"><label>1</label><institution>Anhui University of Chinese Medicine</institution>, <city>Hefei</city>,&#160;<country country=\"cn\">China</country></aff><aff id=\"aff2\"><label>2</label><institution>College of Medicine and Biological Information Engineering, Northeastern University</institution>, <city>Shenyang</city>,&#160;<country country=\"cn\">China</country></aff><author-notes><corresp id=\"c001\"><label>*</label>Correspondence: Jiacheng Wu, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:wjc_@163.com\">wjc_@163.com</email>; Guangming Shao, <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"mailto:guangmingshao@163.com\">guangmingshao@163.com</email></corresp></author-notes><pub-date publication-format=\"electronic\" date-type=\"pub\" iso-8601-date=\"2025-11-24\"><day>24</day><month>11</month><year>2025</year></pub-date><pub-date publication-format=\"electronic\" date-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">480898</issue-id><elocation-id>1672274</elocation-id><history><date date-type=\"received\"><day>25</day><month>7</month><year>2025</year></date><date date-type=\"accepted\"><day>23</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>24</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 00:25:14.317\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Wei, Wu and Shao.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Wei, Wu and Shao</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\" start_date=\"2025-11-24\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution License (CC BY)</ext-link>. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"fonc-15-1672274.pdf\"/><abstract><p>Medical image segmentation is fundamental for disease diagnosis, particularly in the context of breast cancer, a prevalent malignancy affecting women. The accuracy of lesion localization and preservation of image details are essential for ensuring the integrity of lesion segmentation. However, the low resolution of breast tumor B-mode ultrasound images poses challenges in precisely identifying lesion sites. To address this issue, this study introduces the Mamba architecture model, which combines three foundational models with the long-sequence processing model Mamba to develop a novel segmentation model for breast tumor ultrasound images. The selective mechanism and hardware-aware algorithm of the Mamba model enable longer sequence inputs and faster computing speeds. Moreover, integrating a complete chain of VMamba blocks into the basic model enhances segmentation accuracy and image detail processing capabilities. Experimental segmentation was performed on two benchmark ultrasound datasets (BUSI and BUS-BRA) using both the baseline and improved models. The results were compared using metrics such as Dice and IoU, with additional evaluations conducted under small-sample training conditions. This study is intended to provide guidance for the future development of medical image segmentation. Moreover, the experimental results demonstrate that the model incorporating the Mamba architecture achieves superior performance on breast ultrasound images.</p></abstract><kwd-group><kwd>breast tumors</kwd><kwd>medical image segmentation</kwd><kwd>Mamba</kwd><kwd>selective mechanism</kwd><kwd>hardware-aware algorithm</kwd></kwd-group><funding-group><award-group id=\"gs1\"><funding-source id=\"sp1\"><institution-wrap><institution>Anhui Provincial Department of Education</institution><institution-id institution-id-type=\"doi\" vocab=\"open-funder-registry\" vocab-identifier=\"10.13039/open_funder_registry\">10.13039/501100010814</institution-id></institution-wrap></funding-source><award-id rid=\"sp1\">KJ2019A0438, 2024AH050970, SK2020A0255</award-id></award-group><funding-statement>The author(s) declare financial support was received for the research and/or publication of this article. This work was partially supported by the Scientific Research Foundation of the Anhui Provincial Education Department (Grant No. KJ2019A0438, 2024AH050970, SK2020A0255).</funding-statement></funding-group><counts><fig-count count=\"11\"/><table-count count=\"4\"/><equation-count count=\"11\"/><ref-count count=\"37\"/><page-count count=\"14\"/><word-count count=\"6660\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Cancer Imaging and Image-directed Interventions</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"s1\"><label>1</label><title>Introduction</title><p>Tumors, which are caused by the aggregation of mutated cells into masses or growths, can be categorized into benign tumors that do not spread and malignant tumors that are uncontrollably cancerous (<xref rid=\"B1\" ref-type=\"bibr\">1</xref>). Breast cancer, one of the most commonly malignant tumors among women, is also one of the leading causes of cancer death in females. In the early stages, treatment is carried out through lumpectomy, with the goal of completely removing the tumor while preserving as much healthy tissue as possible. Therefore, the precision of tumor excision is a significant challenge in this surgery, and for patients with unclear margins, there is a high probability of requiring a second excision, which may cause patients to miss the best treatment time and increase their psychological burden. For the high incidence of positive cancer margins after breast tumor excision, accurate tumor localization is key to overcoming this challenge. Ultrasound detection is considered the best method for examining breast tumors due to its non-radiation and non-invasive medical imaging approach (<xref rid=\"B2\" ref-type=\"bibr\">2</xref>). However, the frequency of the ultrasound equipment and probe directly affects image quality and lesion display, thereby influencing the diagnostician&#8217;s judgment (<xref rid=\"B3\" ref-type=\"bibr\">3</xref>), leading to missed diagnoses and misdiagnoses, which highlights the importance of early precise detection for successful treatment.</p><p>Conventional diagnostic methods relying on subjective judgments have limitations and risks of misdiagnosis (<xref rid=\"B4\" ref-type=\"bibr\">4</xref>). Medical image segmentation is a crucial technology in medical image processing (<xref rid=\"B5\" ref-type=\"bibr\">5</xref>, <xref rid=\"B6\" ref-type=\"bibr\">6</xref>), essential for disease diagnosis, treatment planning, and evaluating treatment outcomes. Accurate segmentation delineates diseased and normal tissue boundaries, providing precise anatomical and pathological information for clinical decision-making (<xref rid=\"B5\" ref-type=\"bibr\">5</xref>). However, due to the inherent limitations of ultrasound imaging, such as poor contrast and the variability in the appearance of tumors, the development of reliable and effective segmentation algorithms still faces significant challenges. Deep convolutional neural networks (DCNNs) (<xref rid=\"B7\" ref-type=\"bibr\">7</xref>) have revolutionized this field by automatically extracting key visual features relevant to disease diagnosis from extensive medical image datasets (<xref rid=\"B8\" ref-type=\"bibr\">8</xref>, <xref rid=\"B9\" ref-type=\"bibr\">9</xref>). Recent advancements in medical image segmentation, notably the UNet deep-learning network, have shown remarkable potential in segmenting and classifying breast tumor images (<xref rid=\"B10\" ref-type=\"bibr\">10</xref>). UNet&#8217;s exceptional performance and adaptable network structure have made it a focal point in research (<xref rid=\"B11\" ref-type=\"bibr\">11</xref>).</p><p>To further enhance segmentation models, researchers are exploring novel network architectures like dense connections (<xref rid=\"B12\" ref-type=\"bibr\">12</xref>), residual blocks (<xref rid=\"B13\" ref-type=\"bibr\">13</xref>), and attention mechanisms (<xref rid=\"B14\" ref-type=\"bibr\">14</xref>). Kumari et&#160;al. (<xref rid=\"B15\" ref-type=\"bibr\">15</xref>) utilized a neural network with a dense connection known as Densely Connected Convolutional Network (DCCN) to identify deep liver irregularities; (<xref rid=\"B16\" ref-type=\"bibr\">16</xref>) introduced a deep learning architecture (MRFB-Net) that leverages an attention-based pooling decoder module to enhance the segmentation of uterine fibroids in preoperative ultrasound images. However, common CNN models face limitations in their ability to model long-range interactions, and Transformers are constrained by their quadratic computational complexity, making them less than satisfactory for processing breast ultrasound images. This has led to the emergence of State Space Models (SSM) (<xref rid=\"B17\" ref-type=\"bibr\">17</xref>, <xref rid=\"B18\" ref-type=\"bibr\">18</xref>), represented by Mamba, as a promising solution. The Mamba model excels not only in modeling long-range interactions but also in maintaining linear computational complexity. It specifically improves the S4 state space model through selective mechanisms and hardware-aware algorithms, excelling in processing long-sequence data with its unique features. By integrating the cross-scan module (CSM) into the visual state space model (VMamba), Mamba enhances its applicability to computer vision tasks by spatially traversing the domain (<xref rid=\"B17\" ref-type=\"bibr\">17</xref>). (<xref rid=\"B19\" ref-type=\"bibr\">19</xref>) proposed the Shuffle-Reshuffle Gradient Mamba (SRGM) tailored for MMIF, and designed the Local and Global Gradient Mamba (LGGM) to extract modality-specific features while retaining rich spatial details. (<xref rid=\"B20\" ref-type=\"bibr\">20</xref>) introduced Semi-Mamba-UNet, which integrates a pure vision-based Mamba-based U-shaped encoder-decoder architecture with the traditional CNN-based UNet into a semi-supervised learning (SSL) framework and tested it on the ACDC and PROMISE12 medical imaging datasets. (<xref rid=\"B21\" ref-type=\"bibr\">21</xref>) introduce Edge-Mix enhanced Mamba (EM-Mamba) for kidney segmentation, which is designed to capture global and local information from multi-scales. EM-Mamba leverages SegMamba as its backbone, utilizing Mamba&#8217;s efficiency in extracting long-range dependencies. Although Transformer models excel at global modeling, their self-attention mechanism requires a computational complexity that is quadratic with respect to the image size (<xref rid=\"B22\" ref-type=\"bibr\">22</xref>), which becomes particularly evident in the task of medical image segmentation that demands dense predictions. Building on these advancements, our goal is to enhance long-sequence data processing by integrating Mamba into foundational models like UNet++ (<xref rid=\"B23\" ref-type=\"bibr\">23</xref>) and DeepLabv3+ (<xref rid=\"B24\" ref-type=\"bibr\">24</xref>), aiming to improve breast ultrasound image segmentation.</p><p>Integrating the VMamba block (VSS) (<xref rid=\"B25\" ref-type=\"bibr\">25</xref>) from the Mamba model into other networks enhances the model&#8217;s medical image segmentation performance. The VSS features a unique selective mechanism and hardware-aware algorithm, offering significant advantages in processing long-sequence data. By adaptively selecting crucial information for processing, the Mamba model avoids redundant computations, thereby enhancing computational efficiency. Additionally, its hardware-aware algorithm enables seamless adaptation to diverse hardware platforms, further expediting the model&#8217;s inference process. Our research focuses on demonstrating the notable benefits of incorporating the Mamba structure into an image segmentation model for breast tumor image segmentation and classification tasks. This integration enables precise differentiation between tumor tissues and normal breast tissues, resulting in high-precision image segmentation. Specifically, we integrated the VMamba module into the encoder of the model, thereby effectively capturing the multi-scale spatial features and global contextual cues of breast ultrasound images. We conducted extensive experiments on the BUSI and BUS-BRA datasets using various metrics, and the results demonstrated that the models incorporating the VSS block achieved higher segmentation accuracy for breast ultrasound images compared to the original models. This enhancement enables precise segmentation of diverse breast tumors and their complex boundary structures. Such accuracy provides valuable support for clinicians, advancing the clinical application and scientific exploration of artificial intelligence technology in medical image processing, particularly in addressing challenges related to breast tumor image processing.</p></sec><sec id=\"s2\"><label>2</label><title>Mamba model structure</title><p>Mamba, a state space model (SSM), shares the capability of transformers in extracting global features from lengthy sequences. However, Mamba distinguishes itself through its selective mechanism and hardware-aware algorithm, resulting in an inference speed five times faster than that of Transformers. Notably, Mamba&#8217;s computational complexity and memory usage scale linearly with input sequence length, allowing it to process sequences of millions in length. In contrast, Transformers exhibit a time and space complexity of <italic toggle=\"yes\">O</italic>(<italic toggle=\"yes\">n</italic><sup>2</sup>), highlighting Mamba&#8217;s ability to markedly alleviate GPU memory and computing resource demands during the training of long-sequence text models (<xref rid=\"B17\" ref-type=\"bibr\">17</xref>). Mamba integrates the SSM architecture with the multi-layer perceptron (MLP) block within the Transformer framework. SSM serves to characterize state representations and forecast their subsequent states given specific inputs. The structured state space sequence model operates on the following principle:</p><disp-formula id=\"eq1\"><label>(1)</label><mml:math id=\"M1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"eq2\"><label>(2)</label><mml:math id=\"M2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>In this context, <italic toggle=\"yes\">h</italic>(<italic toggle=\"yes\">t</italic>) denotes the current state variable, <italic toggle=\"yes\">A</italic> signifies the state transition matrix, <italic toggle=\"yes\">x</italic>(<italic toggle=\"yes\">t</italic>) represents the input control variable, and <italic toggle=\"yes\">B</italic> indicates the impact of the control variable on the state variable (<xref rid=\"B26\" ref-type=\"bibr\">26</xref>). Furthermore, <italic toggle=\"yes\">y</italic>(<italic toggle=\"yes\">t</italic>) denotes the system output, while <italic toggle=\"yes\">C</italic> signifies the influence of the current state variable on the output. The state and output equations imply that the state at time step <italic toggle=\"yes\">t</italic> is predicted from the preceding state. By incorporating past information in the sequence and the input from the prior state, the system&#8217;s future states can be anticipated. The <italic toggle=\"yes\">A</italic> state transition matrix plays a crucial role in updating the sequence state by incorporating skip connections. These connections directly combine the previous input with the output sequence, thereby improving feature extraction. To tackle the challenge of context sequence dependencies, SSM utilizes Hierarchical Positional Pointers (HiPPO) for long-range dependencies. By employing function approximation, SSM achieves the optimal solution (<xref rid=\"B27\" ref-type=\"bibr\">27</xref>) of the matrix <italic toggle=\"yes\">A</italic>, enabling the retention of a more extensive historical record.</p><p>Selection Mechanism: The conventional SSM model excels in processing structured input data. In contrast, Mamba introduces a selective mechanism that parameterize the SSM input. This mechanism selectively compresses historical data, filters out extraneous 18 information, and preserves essential long-term memory. Consequently, Mamba addresses the challenge faced by traditional models in managing fluctuations or disorder in input sequences, thereby ensuring that parameters influencing sequence interactions adapt to the input dynamics. Specifically, a new learnable parameter step size <inline-formula>\n<mml:math id=\"im1\" display=\"inline\" overflow=\"scroll\"><mml:mtext>&#916;</mml:mtext></mml:math></inline-formula> represents the stage resolution, sampling the continuous input signal over time to obtain discrete output, which is realized by solving the ordinary differential <xref rid=\"eq3\" ref-type=\"disp-formula\">Equation 2</xref> and performing a direct discretization operation. Then, by sampling with step size <inline-formula>\n<mml:math id=\"im2\" display=\"inline\" overflow=\"scroll\"><mml:mtext>&#916;</mml:mtext></mml:math></inline-formula> (i.e., <inline-formula>\n<mml:math id=\"im3\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>d</mml:mi><mml:mi>&#964;</mml:mi><mml:msubsup><mml:mo>&#10072;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>), <inline-formula>\n<mml:math id=\"im4\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be discretized by <xref rid=\"eq4\" ref-type=\"disp-formula\">Equation 4</xref>.</p><disp-formula id=\"eq3\"><label>(3)</label><mml:math id=\"M3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mstyle displaystyle=\"true\"><mml:mrow><mml:msubsup><mml:mo>&#8747;</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>&#964;</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>&#964;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mtext>&#160;</mml:mtext><mml:mi>d</mml:mi><mml:mi>&#964;</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id=\"eq4\"><label>(4)</label><mml:math id=\"M4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle=\"true\"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mi>a</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:msub><mml:mtext>&#916;</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><p>In addition, performing zero-order hold processing on parameters <inline-formula>\n<mml:math id=\"im5\" display=\"inline\" overflow=\"scroll\"><mml:mi>A</mml:mi></mml:math></inline-formula>, <inline-formula>\n<mml:math id=\"im6\" display=\"inline\" overflow=\"scroll\"><mml:mi>B</mml:mi></mml:math></inline-formula> to obtain <inline-formula>\n<mml:math id=\"im7\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mtext>exp&#160;</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mtext>&#916;</mml:mtext><mml:mi>A</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula>\n<mml:math id=\"im8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mtext>&#916;</mml:mtext><mml:mi>A</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mtext>exp&#160;</mml:mtext><mml:mo stretchy=\"false\">(</mml:mo><mml:mtext>&#916;</mml:mtext><mml:mi>A</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mtext>&#916;</mml:mtext><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, ultimately converting the continuous SSM to a discrete SSM, thus updating <xref rid=\"eq3\" ref-type=\"disp-formula\">Equations 1</xref> and <xref rid=\"eq2\" ref-type=\"disp-formula\">2</xref> to <xref rid=\"eq3\" ref-type=\"disp-formula\">3</xref> and <xref rid=\"eq4\" ref-type=\"disp-formula\">4</xref>.</p><disp-formula id=\"eq5\"><label>(5)</label><mml:math id=\"M5\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id=\"eq6\"><label>(6)</label><mml:math id=\"M6\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><p>In contrast to the fixed spacing between input and output elements in conventional copy tasks, selective copying involves adjusting token positions based on content-specific reasoning to eliminate extraneous information. As illustrated in <xref rid=\"eq7\" ref-type=\"disp-formula\">Equation 7</xref>, this process incorporates an additional linear layer in each matrix computation to selectively filter input control and state variables, thereby enhancing reasoning efficiency and augmenting data throughput. Enhancement of the <italic toggle=\"yes\">B</italic> matrix affecting input, the <italic toggle=\"yes\">C</italic> matrix influencing state, and the &#916; time-size parameter enables the model to discern the content of individual tokens, which represent the smallest meaningful units understood and generated by the model. The dimensions of <italic toggle=\"yes\">B, C</italic> and &#916; can be extended by incorporating functions <italic toggle=\"yes\">s<sub>B</sub></italic>(<italic toggle=\"yes\">x</italic>), <italic toggle=\"yes\">s<sub>C</sub></italic>(<italic toggle=\"yes\">x</italic>) and <italic toggle=\"yes\">s</italic><sub>&#916;</sub>(<italic toggle=\"yes\">x</italic>). The introduction of the selection mechanism addresses the limitation of SSM in screening signals across time.</p><disp-formula id=\"eq7\"><label>(7)</label><mml:math id=\"M7\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mtext>&#916;</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>&#964;</mml:mi><mml:mtext>&#916;</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><sec id=\"s2_1\"><title>Hardware-optimized algorithm</title><p>The Mamba algorithm utilizes a multi-threaded parallel scanning approach that leverages the associative law for executing out-of-order computations and aggregating outcomes. In this method, each sequence involves updating the state <inline-formula>\n<mml:math id=\"im9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> according to <xref rid=\"eq8\" ref-type=\"disp-formula\">Equation 8</xref>, where it is computed by multiplying the previous state with a matrix <inline-formula>\n<mml:math id=\"im10\" display=\"inline\" overflow=\"scroll\"><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:math></inline-formula> and adding the current input <inline-formula>\n<mml:math id=\"im11\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> multipied by <inline-formula>\n<mml:math id=\"im12\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. The parallel scanning process integrates segmental sequence computation and iteration to achieve its objectives.</p><disp-formula id=\"eq8\"><label>(8)</label><mml:math id=\"M8\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><p>Notably, the cyclic convolution mode enables bypassing the initial fixed state (<italic toggle=\"yes\">B, L, D, N</italic>), leading to the utilization of a more efficient 3<italic toggle=\"yes\">a</italic> convolution kernel (<italic toggle=\"yes\">B, L, D</italic>) and significantly enhancing computational performance.</p><p>The state <inline-formula>\n<mml:math id=\"im13\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is exclusively operational within the memory hierarchy. To mitigate memory bandwidth constraints, the kernel fusion technique is employed to diminish GPU memory occupancy, thereby substantially enhancing training velocity. The utilization of Flash Attention technology alters the computation outcomes sequentially inscribed in DROM to batch writing from DRAM, thereby curtailing the frequency of redundant read and write operations (<xref rid=\"B28\" ref-type=\"bibr\">28</xref>). Consequently, is substitutedfor the initial (<inline-formula>\n<mml:math id=\"im14\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mi>A</mml:mi><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent=\"true\"><mml:mi>B</mml:mi><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) with a scale of <inline-formula>\n<mml:math id=\"im15\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and fed into the high-speed Static Random-Access Memory (SRAM). To mitigate the need for storing intermediate states during backpropagation, the utilization of recomputation technology is imperative. This approach aims to minimize memory usage by recalculating intermediate states during the backward pass, rather than storing them when loading input from High Bandwidth Memory (HBM) to SRAM. By implementing this technique, the selective scanning layer can achieve a level of memory efficiency akin to that of the high-speed attention Transformer.</p></sec></sec><sec id=\"s3\"><label>3</label><title>Data processing</title><p>The study leveraged data from two publicly available datasets: BUSI (<xref rid=\"B29\" ref-type=\"bibr\">29</xref>) and BUS-BRA (<xref rid=\"B30\" ref-type=\"bibr\">30</xref>). The BUSI dataset comprises breast ultrasound images and their corresponding label images, collected from 600 women aged 25 to 75 in 2018. Each original image is paired with a tumor image (mask), with benign and malignant samples typically featuring one or two lesions. As a result, the labels outlining the lesion areas may require overlapping to consolidate multiple lesions into a single label. The BUS-BRA dataset includes 1875 anonymized breast ultrasound images from 1064 patients, with 722 benign and 342 malignant tumors. It provides BI-RADS assessments, manual segmentations, and 5- and 10-fold cross-validation partitions for standardized evaluation of CAD systems. The detailed information of the dataset is shown in <xref rid=\"T1\" ref-type=\"table\"><bold>Table&#160;1</bold></xref>.</p><table-wrap position=\"float\" id=\"T1\" orientation=\"portrait\"><label>Table&#160;1</label><caption><p>Introduction of dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Case</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">BUSI</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">BUS-BRA</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Number of Images</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">780</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">1875</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Benign</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">437</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">1268</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Malignant</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">210</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">607</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Normal</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">133</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">&#8211;</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Annotation Information</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Masks</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Masks and BI-RADS classification</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Dataset Characteristics</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Smaller</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Larger, suited for extensive training</td></tr></tbody></table></table-wrap><p>Adequate data is essential for effectively training deep learning networks to prevent underfitting and subpar classification performance (<xref rid=\"B31\" ref-type=\"bibr\">31</xref>). To bolster model robustness, a substantial volume of high-quality datasets is necessary (<xref rid=\"B32\" ref-type=\"bibr\">32</xref>). However, obtaining medical image data is intricate, necessitating the expansion of existing public datasets through data augmentation techniques. In our approach, we employ online augmentation, randomly rotating and mirror-flipping each image and its corresponding label in the dataset to enhance the model&#8217;s generalization capabilities. Furthermore, we enhance image quality by applying linear transformations to address the indistinct edges characteristic of ultrasound images in the dataset in function (9), thus suppressing the Hausdorff dimension inflation caused by ultrasonic speckle noise. Because the scanning position varies across breasts, the collected ultrasound images exhibit inconsistent sharpness and brightness. We therefore perform dynamic contrast normalization as defined in <xref rid=\"eq9\" ref-type=\"disp-formula\">Equation 9</xref>: the gray-level histogram of each image is first computed, its intensity bins are used to derive an adaptive weight, and the image contrast is adjusted accordingly, yielding a standardized dataset.</p><disp-formula id=\"eq9\"><label>(9)</label><mml:math id=\"M9\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>*</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mo>&#160;</mml:mo><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></disp-formula><p>Here, <italic toggle=\"yes\">H</italic> and <italic toggle=\"yes\">W</italic> denote the height and width of the input image, <italic toggle=\"yes\">I</italic>(<italic toggle=\"yes\">i,j</italic>) represents a pixel point in the input image, <italic toggle=\"yes\">O</italic>(<italic toggle=\"yes\">i,j</italic>) for the output image; by adjusting the size of parameters <italic toggle=\"yes\">a,b</italic> to achieve transformation of the image grayscale range, thereby adjusting the image contrast.</p></sec><sec id=\"s4\"><label>4</label><title>Research on ultrasound breast tumor image segmentation based on mamba architecture</title><p>This study integrates the Mamba model with different segmentation network architectures to enhance the performance of medical image segmentation. By incorporating the VSS block featuring the Mamba model into diverse segmentation networks, improvements in segmentation accuracy are achieved. Evaluation on a dataset and comparison of segmentation outcomes of the fused models demonstrate that the integration of the Mamba structure accelerates computation while preserving long-term data information.</p><sec id=\"s4_1\"><label>4.1</label><title>Analysis of the VMamba block</title><p><xref rid=\"f1\" ref-type=\"fig\"><bold>Figure&#160;1</bold></xref> illustrates the architecture of the VMamba block, comprising an H3 block and a gated MLP. The H3 block embodies a selective SSM (independent sequence transformation) state-space model. Simplifying the H3 structure involves amalgamating linear attention and MLP blocks, stacking them uniformly, and enabling controlled expansion of the model dimension. The Mamba architecture is constructed by iteratively replicating this block, incorporating residual connections and standard normalization interchangeably. To mitigate gradient vanishing, a residual term is introduced in conjunction with the gated MLP. The VMamba block is limited to extracting features from semantic data like text and cannot handle image data. To address this limitation, Yue et&#160;al. (<xref rid=\"B33\" ref-type=\"bibr\">33</xref>) substituted the S6 module in the VMamba block with the SS2D module, which is designed to process image data using the VSS block. This modification resulted in the creation of the VSS block.</p><fig position=\"float\" id=\"f1\" orientation=\"portrait\"><label>Figure&#160;1</label><caption><p>Structures of the Mamba block (left), the Vanilla VSS block (middle), and the VMamba (VSS) block.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g001.jpg\"><alt-text content-type=\"machine-generated\">Three block diagrams labeled Mamba Block, The Vanilla VSS Block, and VSS Block show different neural network architectures. Each block includes components like Linear, Silu, DWConv, and normalization layers. Mamba Block has a unique S6 layer, The Vanilla VSS Block features SS2D, and VSS Block includes SS2DBlock and FFN. Arrows indicate data flow.</alt-text></graphic></fig><p>Following layer normalization, the VSS block comprises two branches. One branch employs a 3&#215;3 depthwise convolutional layer for feature extraction. Initially, the input undergoes processing in a linear layer, a depthwise separable convolution, and an activation function before entering the two-dimensional selective scanning (SS2D) module for further feature extraction. Subsequently, feature normalization is applied, followed by element-wise multiplication with the output from the alternate branch to merge the pathways. A linear layer is then utilized to blend the features, which are combined with a residual connection to yield the VSS module output. The second branch includes a linear mapping layer followed by a SiLU activation layer to compute the multiplicative gating signal. Notably, the key distinction from the standard VSS block lies in replacing the S6 module with the SS2D module, enabling adaptive selective scanning for 2D visual data. This design choice opts for a more compact structure without the fully connected phase, resulting in denser stack blocks within the same depth constraints.</p></sec><sec id=\"s4_2\"><label>4.2</label><title>Construction of the VM-UNet++ model</title><p><xref rid=\"f2\" ref-type=\"fig\"><bold>Figure&#160;2</bold></xref> illustrates the architecture of VM-UNet++. This design integrates the U-Net framework with the VSS block to construct the encoder and decoder components. The U-Net features a symmetrical U-shaped configuration comprising an encoder for feature extraction, a decoder for feature fusion, and skip connections to mitigate gradient vanishing (<xref rid=\"B15\" ref-type=\"bibr\">15</xref>). Within the decoder&#8217;s upsampling phase, skip connections are employed post each convolution to link with the downsampled encoder features at the corresponding level and lower-level features, thereby diminishing gradient vanishing and preserving more spatial detail features. The VM-UNet++ configuration encompasses a patch embedding layer, an encoder, a decoder, a final projection layer, and skip connections. Initially, the input image is transformed into a one-dimensional sequence of H/4&#215;W/4&#215;C via the patch and linear embedding layers. The encoder incorporates multiple VSS blocks and patch merging layers to extract token features, diminish height and width, and augment dimensionality (<xref rid=\"B34\" ref-type=\"bibr\">34</xref>). The decoder mirrors the encoder&#8217;s structure, with the patch merging layer substituted by a patch expansion layer to enhance height and width while reducing dimensionality, thereby generating outputs with consistent feature sizes. Ultimately, the linear projection layer restores the channel count to align with the input resolution. A densely connected network is introduced during the upsampling phase, where the convolution output of the preceding layer is added to each subsequent layer, forming local Unet networks within each segment. This approach fuses low-resolution features from upsampling with high-resolution features from downsampling to retain both spatial detail features and global information. The densely connected structure of the VM-UNet++ model facilitates straightforward network depth augmentation to bolster learning capacity during construction. Moreover, it permits a moderate depth reduction through network pruning strategies without compromising the original network architecture.</p><fig position=\"float\" id=\"f2\" orientation=\"portrait\"><label>Figure&#160;2</label><caption><p>Structure diagram of VM-UNet++.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g002.jpg\"><alt-text content-type=\"machine-generated\">Flowchart showing an encoder-decoder architecture for image processing. The encoder consists of multiple VSS block modules interspersed with patch partitions, reducing dimensions sequentially from height H by width W to smaller ones. Skip connections link corresponding blocks in encoder and decoder. The decoder reconstructs the image with similar VSS blocks and patch partitions, ending with linear embedding to produce the final segmented output labeled as &#8220;Seg&#8221;. Lines depict connections and data flow, with dimensions labeled at each stage.</alt-text></graphic></fig></sec><sec id=\"s4_3\"><label>4.3</label><title>Construction of Deep-VMamba</title><p>DeepLabV3+ comprises an encoder and a decoder, incorporating Atrous convolution, depthwise separable convolution, Atrous Spatial Pyramid Pooling (ASPP), and fully convolutional networks (<xref rid=\"B31\" ref-type=\"bibr\">31</xref>). As illustrated in <xref rid=\"f3\" ref-type=\"fig\"><bold>Figure&#160;3</bold></xref>, this study integrates the Mamba structure into the DeepLabV3+ architecture. The VMamba block, fused with the encoder output of DeepLabV3+, enhances the delineation of tumor lesions in ultrasound breast images by capturing finer details and edge information. The incorporation of the VMamba block supplements global information to the original DeepLabV3+ segmentation, thereby expanding the network&#8217;s receptive field without compromising feature retention, facilitating more comprehensive malignant tumor segmentation. Additionally, the encoder segment of DeepLabV3+ encompasses a complete feature extraction and sampling branch, preserving all feature extraction capabilities while augmenting the model&#8217;s proficiency in feature extraction from images and processing extended sequences, without compromising its original functionality.</p><fig position=\"float\" id=\"f3\" orientation=\"portrait\"><label>Figure&#160;3</label><caption><p>Structure of the Deep-VMamba model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g003.jpg\"><alt-text content-type=\"machine-generated\">Diagram illustrating a neural network architecture with three sections: VMamba, Encoder, and Decoder. The VMamba section includes Patch Partition, FFN, LN, and other blocks. The Encoder features a convolution pyramid for image pooling and a DCNN with Atrous Convolution. The Decoder processes low-level features, includes upsampling, and uses concatenation to output an image. An inset shows an ultrasound image as input.</alt-text></graphic></fig></sec><sec id=\"s4_4\"><label>4.4</label><title>Construction of the SAM-VMamba model</title><p>The Segment Anything Model (SAM) model is tailored for a novel image segmentation assignment, trained on a dataset of 11 million images with over one billion masks. Moreover, SAM can segment images based on various prompts such as points, boxes, and text, without the need for retraining on specific datasets. Its efficient design and training facilitate zero shot transfer to new image distributions and tasks, which has garnered widespread attention. For instance, Ma and Wang et&#160;al. (<xref rid=\"B35\" ref-type=\"bibr\">35</xref>) proposed MedSAM for general medical image segmentation. This model, trained on a meticulously constructed dataset, is capable of achieving desirable performance. However, the limited scale of the assembled dataset and the modality imbalance issue restrict MedSAM&#8217;s performance on ultrasound images. The MSA method proposed by Wu et&#160;al. (<xref rid=\"B36\" ref-type=\"bibr\">36</xref>) significantly enhances image segmentation performance by freezing the pre-trained parameters of SAM and inserting adapter modules at specific locations. As shown in <xref rid=\"f4\" ref-type=\"fig\"><bold>Figure&#160;4</bold></xref>, the SAM model comprises an image encoder, a prompt encoder, and a mask decoder, the SAM model employs a prompting approach to segment user-specified points. Users can provide prompt information through user-defined points, bounding boxes, and randomly circled regions. Furthermore, free-form text prompts are utilized to present initial results. Notably, the prompt encoder of the SAM model can effectively segment desired objects based on user prompts, thereby enabling targeted area segmentation. For the segmentation of breast ultrasound images, Tu et&#160;al. (<xref rid=\"B37\" ref-type=\"bibr\">37</xref>) proposed an innovative SAM adapter (BUSSAM), which migrates the SAM framework to the field of breast ultrasound image segmentation through adaptation techniques, and validated its feasibility and effectiveness.</p><fig position=\"float\" id=\"f4\" orientation=\"portrait\"><label>Figure&#160;4</label><caption><p>Structure of the SAM model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g004.jpg\"><alt-text content-type=\"machine-generated\">Flowchart showing an ultrasound image of the left breast being processed by an image encoder. The outputs are fed into a convolutional layer, then to mask and prompt decoders, resulting in two processed ultrasound images with highlighted areas.</alt-text></graphic></fig><p>Although the SAM demonstrates evident effectiveness for the segmentation of the vast majority of natural images, it faces challenges when dealing with fine medical images due to the inherent low resolution and complexity of medical imaging, leading to suboptimal performance in zero-shot segmentation scenarios. Therefore, few-shot training becomes crucial for achieving superior performance in practical applications. Moreover, considering the limitations of SAM in global attention, our study incorporates the VMamba block into the SAM model framework to enhance its capabilities. As shown in <xref rid=\"f5\" ref-type=\"fig\"><bold>Figure&#160;5</bold></xref>, the VMamba block, situated alongside the ViT within the SAM image encoder, facilitates the processing of extended input sequence data for a more comprehensive contextual understanding. Illustrated in <xref rid=\"f5\" ref-type=\"fig\"><bold>Figure&#160;5</bold></xref>, the model comprises an image encoder, a prompt encoder, and a mask decoder. Following the Patch Embedding step in the image encoder, the VMamba block operates in parallel to convert the upper layer&#8217;s output tokens into a linear vector with long-range memory. This vector is then fused with the Transformer block output and subjected to two convolutions (Neck layer) to generate the Image Embedding, which subsequently serves as input for the mask decoder.</p><fig position=\"float\" id=\"f5\" orientation=\"portrait\"><label>Figure&#160;5</label><caption><p>Structure of the SAM-VMamba model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g005.jpg\"><alt-text content-type=\"machine-generated\">Flowchart depicting the VMamba model architecture. It shows processes like patch embedding, positional embedding, and image encoding using window transformer and attention mechanisms. Outputs include a mask decoder and prompt encoder for image analysis, illustrated with ultrasound scans, and highlighted regions.</alt-text></graphic></fig><p>The enhanced comprehension of extended sequences by the Mamba model enables SAM-VMamba to establish improved global connections and achieve enhanced segmentation performance. Additionally, the integration of a selective mechanism and hardware-aware algorithm in SAM-VMamba expedites model training and implementation without incurring additional time costs, thereby substantially decreasing the time and resources needed for training deep segmentation models. Moreover, by integrating the generalization and pre-training capabilities of the SAM model, the SAM-VMamba model is able to achieve accurate segmentation effects with only a small number of breast ultrasound image training samples.</p></sec></sec><sec id=\"s5\"><label>5</label><title>Experimental results and analysis</title><sec id=\"s5_1\"><label>5.1</label><title>Experimental environment</title><p>The model proposed in our study is deployed and trained on the RTX A6000 GPU, with all experiments conducted using the same hardware device. The experiments utilize the PyTorch 2.2.0 deep learning framework and Python 3.11.5 programming language, with GPU computation supported by the CUDA 12.2 architecture. The batch size is set to 24, with a maximum of 100 training epochs, and the AdamW optimizer is employed in conjunction with the CosineAnnealingLR learning rate scheduling strategy for model optimization. Additionally, the proposed SAM-VMamba network is initialized using the pre-trained weights of SAM&#8217;s ViT-B.</p></sec><sec id=\"s5_2\"><label>5.2</label><title>Evaluation indicators</title><p>The Dice Coefficient (DICE) metric assesses model performance in segmentation tasks by quantifying the overlap between predicted and ground-truth regions. In medical image analysis, the DICE coefficient is commonly employed to evaluate neural network models in tasks like lesion detection and tissue segmentation. The formula for calculating the DICE coefficient is defined by <xref rid=\"eq10\" ref-type=\"disp-formula\"><bold>Equation 10</bold></xref>.</p><disp-formula id=\"eq10\"><label>(10)</label><mml:math id=\"M10\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><p>The MIoU metric assesses the correspondence between predicted outcomes and true labels in semantic segmentation tasks. It is computed as the mean of the Intersection over Union (IoU) for individual categories. IoU represents the ratio of the intersection to the union of predicted and actual values, reflecting the degree of overlap. A higher IoU signifies improved segmentation accuracy, indicating a greater overlap between areas. The calculation is given by <xref rid=\"eq11\" ref-type=\"disp-formula\"><bold>Equation 11</bold></xref>.</p><disp-formula id=\"eq11\"><label>(11)</label><mml:math id=\"M11\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><p>Precision is defined as the proportion of pixels that are correctly identified as the true lesion area. It represents the ratio of true positive pixels to the sum of true positive and false positive pixels, expressed as: <inline-formula>\n<mml:math id=\"im16\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Accuracy represents the proportion of correctly identified image pixels, that is, the ratio of breast tumor and non-breast tumor areas to the total number of pixels (mask), expressed as: <inline-formula>\n<mml:math id=\"im17\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Recall, also known as the sensitivity, is the proportion of the actual lesion area that is identified in the image. It represents the size of the true positive cases relative to the entire lesion area, expressed as: <inline-formula>\n<mml:math id=\"im18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></sec><sec id=\"s5_3\"><label>5.3</label><title>Experimental results</title><p><xref rid=\"T2\" ref-type=\"table\"><bold>Table&#160;2</bold></xref> summarizes the training hyper-parameters and computational performance of each model. Where, throughput denotes the maximum number of training samples the model can process per second, while total multiply-adds signify the computational burden of the model during a single forward propagation. The exceptional long-sequence processing capabilities of Mamba are confirmed through an assessment of the computational efficiency of output images. This evaluation involves comparing parameters, processes, and throughput during both training and inference to gauge generalization performance. Results indicate that models incorporating Mamba exhibit consistent performance across various input image sizes. For instance, at an input resolution of 512 &#215; 512, Unet++ achieves the highest throughput among baseline models, while DeepLabV3+ demonstrates the highest throughput per epoch during training. Despite higher computational load compared to baseline models at the same input size, the integration of the Mamba structure allows for increased throughput capacity, enabling the retention and processing of longer data sequences, thereby enhancing comprehensive image data processing. Moreover, while the integrated models maintain relatively high inference speeds (higher throughput than baseline models) at a resolution of 512&#215;512, their computational load escalates significantly, surpassing that of baseline models and indicating limited generalization capability. In terms of computational efficiency, current SSM-based vision models typically exhibit superior throughput only with large-scale inputs and high resolutions.</p><table-wrap position=\"float\" id=\"T2\" orientation=\"portrait\"><label>Table&#160;2</label><caption><p>Parameter table on the 512<sup>2</sup> image.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">#param</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">FLOPs</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Throughput</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Train throughput</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Total mult-adds</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Unet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">24.4M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">31.3G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.38</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">57.69</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">31.26</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Unet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">26.1M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.7G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">71.12</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">33.72</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.53</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">DeepLabV3+</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">22.4M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">31.7G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.71</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">58.44</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">31.54</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">VM-UNet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.4M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">16.4G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">48.87</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">25.46</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">310.02</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">SAM-Med2D</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">221.9M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">303.5G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">21.28</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">24.10</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">259.31</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">VM-UNet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.4M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.2G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">47.03</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">30.27</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">443.97</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">Deep-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.8M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">121.2G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">64.15</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">46.90</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">43.28</td></tr><tr><td valign=\"middle\" align=\"left\" rowspan=\"1\" colspan=\"1\">SAM-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">236.5M</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">259.8G</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">27.73</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.35</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">303.26</td></tr></tbody></table></table-wrap><p>Based on <xref rid=\"T3\" ref-type=\"table\"><bold>Tables&#160;3</bold></xref> and <xref rid=\"T4\" ref-type=\"table\"><bold>4</bold></xref>, it is evident that traditional models do not achieve highly accurate segmentation of ultrasonic breast tumor images. The Unet model, for instance, exhibits relatively low performance with Dice coefficients of 81.92% and 82.10%, and IoU values of 69.53% and 73.52% across the two datasets. In contrast, models incorporating the VMamba block demonstrate a significant improvement in segmentation metrics compared to their original counterparts. Notably, the SAM-VMamba model, which integrates the VMamba block, achieves the highest performance with Dice scores of 90.62% and 90.25%, and IoU values of 82.55% and 82.54%. This improvement can be attributed to the inherent challenges posed by breast ultrasound images, characterized by low clarity and predominantly dark tones, leading to a diminished signal-to-noise ratio. Given that breast tumors occupy a small portion of the image, there is a risk of lesion oversight and misjudgment. Furthermore, the indistinct boundary between breast tumors and normal tissues, coupled with blurred lesion edges lacking distinctive features against the background, contributes to reduced segmentation accuracy. Moreover, the uniform grayscale distribution in the images results in minimal variations in pixel intensities, thereby compromising texture and detail resolution. However, the integration of Mamba facilitates the capture of prolonged sequential information, enabling more comprehensive breast tumor segmentation and enhanced edge delineation.</p><table-wrap position=\"float\" id=\"T3\" orientation=\"portrait\"><label>Table&#160;3</label><caption><p>Presents a comparative evaluation of the segmentation outcomes of the models on BUSI.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">DICE(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">IoU(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Precision(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Accuray(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Recall(%)</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Unet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">81.92</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">69.53</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.33</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.78</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">78.53</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Unet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.76</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">70.81</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.98</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.81</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.32</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">DeepLabV3+</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">83.61</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">71.97</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.05</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.92</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.74</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">VM-UNet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">83.56</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">74.13</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.58</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.60</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.43</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">SAM-Med2D</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.13</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">81.16</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">89.45</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.56</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.43</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">VM-UNet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.89</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">75.59</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.57</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.99</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.61</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Deep-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.24</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">71.24</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.26</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.84</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.21</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">SAM-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.62</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.55</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.44</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.08</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">92.05</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"T4\" orientation=\"portrait\"><label>Table&#160;4</label><caption><p>Presents a comparative evaluation of the segmentation outcomes of the models on BUS-BRA.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Model</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">DICE (%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">IoU(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Precision(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Accuray(%)</th><th valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Recall(%)</th></tr></thead><tbody><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Unet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.10</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">73.52</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.30</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.51</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.27</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Unet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">84.88</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">76.28</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.02</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.43</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.71</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">DeepLabV3+</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.43</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.98</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.50</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.74</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.24</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">VM-UNet</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.69</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">77.19</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.95</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.59</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.48</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">SAM-Med2D</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.68</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">80.05</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.49</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.35</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">81.76</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">VM-UNet++</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.35</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">78.01</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.95</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.63</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">86.61</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">Deep-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.16</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">79.01</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">87.82</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">97.82</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">88.99</td></tr><tr><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">SAM-VMamba</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">90.25</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">82.54</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">96.19</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">98.58</td><td valign=\"middle\" align=\"center\" rowspan=\"1\" colspan=\"1\">85.43</td></tr></tbody></table></table-wrap><p><xref rid=\"f6\" ref-type=\"fig\"><bold>Figures&#160;6</bold></xref> and <xref rid=\"f7\" ref-type=\"fig\"><bold>7</bold></xref> visualize the actual segmentation results of each model on breast tumors. Traditional models exhibit poor performance in segmentation due to the limitations of their structure. The pooling layers and downsampling operations used in network training result in the loss of partial information as the network depth increases and size decreases. In contrast, during upsampling, only a basic addition operation is conducted on high-resolution images from the downsampling layer, leading to the loss of crucial &#8220;deep-layer&#8221; feature information. However, Mamba, characterized by its capacity for ultra-long sequence processing and memory within the integrated model, preserves more spatial details, thereby yielding superior segmentation outcomes. SAM-VMamba demonstrates superior performance with small-sample data due to its integration of the SAM segmentation model. Furthermore, as illustrated in <xref rid=\"f8\" ref-type=\"fig\"><bold>Figure&#160;8</bold></xref>, the distinctive prompt encoder of SAM enables precise regional segmentation of images, enhancing its practical utility by eliminating the need to process redundant image components.</p><fig position=\"float\" id=\"f6\" orientation=\"portrait\"><label>Figure&#160;6</label><caption><p>Comparison of image segmentation effects on the BUSI dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g006.jpg\"><alt-text content-type=\"machine-generated\">Comparison of ultrasound images and segmentation masks using various methods. Rows represent malignant, benign, and normal cases. Columns include original images, masks, and results from Unet, Unet++, VM-UNet, VM-UNet++, DeepLabV3+, Deep-VMamba, SAM-Med2D, and SAM-VMamba. Segmentation accuracy varies across different methods.</alt-text></graphic></fig><fig position=\"float\" id=\"f7\" orientation=\"portrait\"><label>Figure&#160;7</label><caption><p>Comparison of image segmentation effects on the BUS-BRA dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g007.jpg\"><alt-text content-type=\"machine-generated\">Three rows of images compare breast ultrasound segmentation results from different models. Each row includes an original ultrasound image, its mask, and segmentation outputs from Unet, Unet++, VM-UNet, VM-UNet++, DeepLabV3+, Deep-VMamba, SAM-Med2D, and SAM-VMamba. The segmentations vary in shape and accuracy.</alt-text></graphic></fig><fig position=\"float\" id=\"f8\" orientation=\"portrait\"><label>Figure&#160;8</label><caption><p>Segmentation results of SAM-VMamba.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g008.jpg\"><alt-text content-type=\"machine-generated\">A grid of ultrasound images showing analysis of malignant and benign areas. The columns display the original image, mask, and segmentations using different prompts: 1 point, 2 point, local box, and global box. The malignant and benign rows illustrate variations in shading and shape with blue overlays and green boxes indicating segmented areas.</alt-text></graphic></fig><p><xref rid=\"f9\" ref-type=\"fig\"><bold>Figures 9a, b</bold></xref> depict the cumulative distribution of prediction effects for each model based on 300 segmentation predictions of breast ultrasound images. The segmentation outcomes of conventional models such as U-Net predominantly cluster around 0.8 for Dice and 0.7 for IoU. The integration of the Mamba model notably enhances the overall segmentation performance, yielding higher accuracy metrics compared to the baseline models. Particularly noteworthy is the superior segmentation efficacy of the SAM model surpassing that of its counterparts. The SAM model demonstrates heightened segmentation accuracy and data concentration, indicative of its robust stability. The primary reason lies in the prompt encoder mechanism of the SAM model and its pretraining on large-scale datasets, which enable superior adaptation and handling of out-of-domain datasets. In contrast, baseline models suffer from a substantial loss distance between their initialized weights and the optimal solution, requiring large sample sizes and extensive iterations to reduce this gap, thereby leading to unstable extrapolation in prediction distributions. Moreover, models augmented with the Mamba structure further enhance the multi-scale spatial decomposition of breast tumor images and the modeling of intra-scale feature dependencies, thereby facilitating the extraction of tumors with varying shapes and types.</p><fig position=\"float\" id=\"f9\" orientation=\"portrait\"><label>Figure&#160;9</label><caption><p>Cumulative distributions of dice and IoU under 300 predictions. <bold>(a)</bold> Dice, <bold>(b)</bold> IoU.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g009.jpg\"><alt-text content-type=\"machine-generated\">Two bar charts compare the distribution of predictions from different segmentation models. Chart (a) shows the Dice coefficient distribution, while chart (b) shows the Intersection over Union (IoU) distribution. Models compared include U-Net, U-Net++, DeepLabV3+, and others, each represented by different colors. Both charts display the number of predictions on the y-axis and threshold values on the x-axis, with noticeable peaks around higher threshold values, indicating varying performance across models.</alt-text></graphic></fig><p>The <xref rid=\"f10\" ref-type=\"fig\"><bold>Figures&#160;10a, b</bold></xref> illustrates notable enhancement in the model&#8217;s performance with the integration of the Mamba structure compared to the original model. Specifically, the incorporation of this structure significantly improves the segmentation performance of the model on breast ultrasound images with long sequences. With an increase in the number of training iterations, conventional models like Unet exhibit some degree of enhancement. However, the integrated Mamba model surpasses the performance of individual traditional models in overall improvement. Particularly noteworthy is the superior segmentation performance of the SAM model compared to other models, attributed to its pre-training on a large dataset of millions of images. This model requires only a limited number of training epochs to achieve a stable and optimal performance level.</p><fig position=\"float\" id=\"f10\" orientation=\"portrait\"><label>Figure&#160;10</label><caption><p>Comparison of segmentation accuracies among different models trained with a low number of iterations. <bold>(a)</bold> IoU, <bold>(b)</bold> Dice.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g010.jpg\"><alt-text content-type=\"machine-generated\">Two line graphs compare different models across epochs. Graph (a) displays IoU scores, and graph (b) shows Dice scores. Models include Unet, Unet++, DeepLabV3+, Deep-VMamba, VM-UNet, VM-UNet++, SAM-Med2D, and SAM-VMamba. SAM-VMamba and SAM-Med2D show consistently high performance in both graphs. Each line represents a model, demonstrating their improvement over 70 epochs.</alt-text></graphic></fig><p><xref rid=\"f11\" ref-type=\"fig\"><bold>Figure&#160;11</bold></xref> presents the performance of all models under few-sample testing conditions. Findings indicate that when trained on a small dataset, the model incorporating VMamba blocks generally outperforms the baseline model in segmentation accuracy. The primary reason is that breast tumors exhibit a relatively low signal-to-noise ratio and indistinct features, which forces baseline models to rely on large sample averaging to suppress noise. In contrast, Mamba compresses two-dimensional spatial sequences into fixed-dimensional state vectors, inherently embedding a Gaussian&#8211;Markov smoothing mechanism that provides natural denoising. These advantages collectively enable the Mamba-enhanced model to achieve an average improvement of 9.12% in the IoU metric. Remarkably, despite being pretrained on extensive data, the SAM model exhibits sustained high segmentation performance in the context of limited-sample training, maintaining an IoU value of approximately 80%.</p><fig position=\"float\" id=\"f11\" orientation=\"portrait\"><label>Figure&#160;11</label><caption><p>Comparison of segmentation accuracy (IoU) of each model on small samples.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"fonc-15-1672274-g011.jpg\"><alt-text content-type=\"machine-generated\">Line graph comparing multiple models' Intersection over Union (IoU) performance based on small sample sizes. The x-axis represents small sample sizes ranging from 30 to 120. The y-axes display IoU scores: left (36 to 75) and right (67 to 84). Different lines, both solid and dashed, represent models like Unet, Unet++, DeepLabV3+, and SAM variants, showing varying performance trends. A legend identifies each model with a specific line style and color.</alt-text></graphic></fig></sec></sec><sec sec-type=\"conclusions\" id=\"s6\"><label>6</label><title>Conclusions</title><p>In this investigation, we enhanced the image segmentation performance of the original model for breast tumor ultrasound images by integrating the Mamba structure. Comparative analysis with conventional models like Unet, DeepLabV3+, and Unet++ revealed the superior performance of the model incorporating the VMamba block across various evaluation metrics, including the DICE coefficient, MIoU, Precision, and Recall. Benefiting from the global attention capability of Mamba, the enhanced model is able to simultaneously capture multi-scale global dependencies and better focus on the details of breast tumor segmentation. Experimental results show that incorporating Mamba into the model yields average improvements of 3.07% and 5.11% in Dice and IoU on the BUSI dataset, and 2.89% and 3.26% on the BUS-BRA dataset. Notably, the SAM-VMamba achieved the highest segmentation accuracy and quality, with Dice scores of 90.25% and 90.62% on the BUSI and BUS-BRA datasets. These outcomes signify the model&#8217;s success in accurately localizing and distinguishing boundaries of breast tumors.</p></sec></body><back><fn-group><fn id=\"n1\" fn-type=\"edited-by\"><p>Edited by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/190136\" ext-link-type=\"uri\">Pierre Boulanger</ext-link>, University of Alberta, Canada</p></fn><fn id=\"n2\" fn-type=\"reviewed-by\"><p>Reviewed by: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/441437\" ext-link-type=\"uri\">Yu Zhang</ext-link>, Zhejiang Lab, China</p><p><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://loop.frontiersin.org/people/3203688&gt;\" ext-link-type=\"uri\">Arshpreet Kaur</ext-link>, Amity University, India</p></fn></fn-group><sec sec-type=\"data-availability\" id=\"s7\"><title>Data availability statement</title><p>The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding author.</p></sec><sec sec-type=\"author-contributions\" id=\"s8\"><title>Author contributions</title><p>WW: Writing &#8211; original draft, Writing &#8211; review &amp; editing. JW: Writing &#8211; original draft, Writing &#8211; review &amp; editing. GS: Writing &#8211; original draft, Writing &#8211; review &amp; editing.</p></sec><sec sec-type=\"COI-statement\" id=\"s10\"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type=\"ai-statement\" id=\"s11\"><title>Generative AI statement</title><p>The author(s) declare that no Generative AI was used in the creation of this manuscript.</p><p>Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us.</p></sec><sec sec-type=\"disclaimer\" id=\"s12\"><title>Publisher&#8217;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id=\"B1\"><label>1</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roy</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Ghosh</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Mukherjee</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sain</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Pathak</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Chaudhuri</surname><given-names>SS</given-names></name><etal/></person-group>. &#8220;\n<article-title>Breast tumor segmentation using image segmentation algorithms</article-title>,&#8221; <conf-name>2019 International Conference on Opto-Electronics and Applied Optics (Optronix)</conf-name>, <conf-loc>Kolkata, India</conf-loc> (<year>2019</year>), pp. <fpage>1</fpage>&#8211;<lpage>5</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/OPTRONIX.2019.8862339</pub-id></mixed-citation></ref><ref id=\"B2\"><label>2</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Benaouali</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Bentoumi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Touati</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Ahmed</surname><given-names>AT</given-names></name><name name-style=\"western\"><surname>Mimi</surname><given-names>M</given-names></name></person-group>. &#8220;\n<article-title>Segmentation and classification of benign and Malignant breast tumors via texture characterization from ultrasound images</article-title>,&#8221; <conf-name>2022 7th International Conference on Image and Signal Processing and their Applications (ISPA)</conf-name>, <conf-loc>Mostaganem, Algeria</conf-loc> (<year>2022</year>), pp. <fpage>1</fpage>&#8211;<lpage>4</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ISPA54004.2022.9786350</pub-id></mixed-citation></ref><ref id=\"B3\"><label>3</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>El-Azizy</surname><given-names>ARM</given-names></name><name name-style=\"western\"><surname>Salaheldien</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Rushdi</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Gewefel</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Mahmoud</surname><given-names>AM</given-names></name></person-group>. &#8220;\n<article-title>Morphological characterization of breast tumors using conventional b-mode ultrasound images</article-title>,&#8221; <conf-name>2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name>, <conf-loc>Berlin, Germany</conf-loc> (<year>2019</year>), pp. <page-range>6620&#8211;3</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/EMBC.2019.8857438</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">31947359</pub-id></mixed-citation></ref><ref id=\"B4\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Elmore</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Armstrong</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Lehman</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Fletcher</surname><given-names>S</given-names></name></person-group>. \n<article-title>Screening for breast cancer</article-title>. <source>Jama</source>. (<year>2005</year>) <volume>293</volume>:<page-range>1245&#8211;56</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1001/jama.293.10.1245</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">15755947</pub-id><pub-id pub-id-type=\"pmcid\">PMC3149836</pub-id></mixed-citation></ref><ref id=\"B5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Patil</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Deore</surname><given-names>S</given-names></name></person-group>. \n<article-title>Medical image segmentation: a review</article-title>. <source>Int J Comput Sci Mobile Computing</source>. (<year>2013</year>) <volume>2</volume>:<page-range>22&#8211;7</page-range>.\n</mixed-citation></ref><ref id=\"B6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Belsare</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mushrif</surname><given-names>M</given-names></name></person-group>. \n<article-title>Histopathological image analysis using image processing techniques: An overview</article-title>. <source>Signal Image Process</source>. (<year>2012</year>) <volume>3</volume>:<fpage>23</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.5121/sipij.2012.3403</pub-id></mixed-citation></ref><ref id=\"B7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Krizhevsky</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sutskever</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Hinton</surname><given-names>G</given-names></name></person-group>. \n<article-title>Imagenet classification with deep convolutional neural networks</article-title>. In <source>Advances in Neural Information Processing Systems</source> (<year>2012</year>) Vol. <volume>25</volume>, pp. <page-range>1097&#8211;1105</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1145/3065386</pub-id></mixed-citation></ref><ref id=\"B8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Olimov</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Koh</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>J</given-names></name></person-group>. \n<article-title>Aedcn-net: Accurate and efficient deep convolutional neural network model for medical image segmentation</article-title>. <source>IEEE Access</source>. (<year>2021</year>) <volume>9</volume>:<page-range>154194&#8211;203</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ACCESS.2021.3128607</pub-id></mixed-citation></ref><ref id=\"B9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tulbure</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Tulbure</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Dulf</surname><given-names>E</given-names></name></person-group>. \n<article-title>A review on modern defect detection models using dcnns&#8211;deep convolutional neural networks</article-title>. <source>J Advanced Res</source>. (<year>2022</year>) <volume>35</volume>:<fpage>33</fpage>&#8211;<lpage>48</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.jare.2021.03.015</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">35024194</pub-id><pub-id pub-id-type=\"pmcid\">PMC8721352</pub-id></mixed-citation></ref><ref id=\"B10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vianna</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Farias</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>de Albuquerque Pereira</surname><given-names>W</given-names></name></person-group>. \n<article-title>U-net and segnet performances on lesion segmentation of breast ultrasonography images</article-title>. <source>Res Biomed Eng</source>. (<year>2021</year>) <volume>37</volume>:<page-range>171&#8211;9</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/s42600-021-00137-4</pub-id></mixed-citation></ref><ref id=\"B11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Dou</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Heng</surname><given-names>P</given-names></name></person-group>. \n<article-title>H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</article-title>. <source>IEEE Trans Med Imaging</source>. (<year>2018</year>) <volume>37</volume>:<page-range>2663&#8211;74</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/TMI.2018.2845918</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">29994201</pub-id></mixed-citation></ref><ref id=\"B12\"><label>12</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>van der Maaten</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Weinberger</surname><given-names>K</given-names></name></person-group>. &#8220;\n<article-title>Densely connected convolutional networks</article-title>,&#8221; <conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, <conf-loc>Honolulu, HI, USA</conf-loc> (<year>2017</year>), pp. <page-range>2261&#8211;9</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/CVPR.2017.243</pub-id></mixed-citation></ref><ref id=\"B13\"><label>13</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J</given-names></name></person-group>. &#8220;\n<article-title>Deep residual learning for image recognition</article-title>,&#8221; <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, <conf-loc>Las Vegas, NV, USA</conf-loc> (<year>2016</year>), pp. <page-range>770&#8211;8</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref><ref id=\"B14\"><label>14</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Woo</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Kweon</surname><given-names>I</given-names></name></person-group>. \n<article-title>CBAM: Convolutional Block Attention Module</article-title>. In: \n<person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Ferrari</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Hebert</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Sminchisescu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Weiss</surname><given-names>Y</given-names></name></person-group> (eds) <source>Computer Vision - ECCV 2018. ECCV 2018. Lecture Notes in Computer Science()</source>, vol <volume>11211</volume>. \n<publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc>. (<year>2016</year>). doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/978-3-030-01234-2_1</pub-id></mixed-citation></ref><ref id=\"B15\"><label>15</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Q</given-names></name></person-group>. &#8220;\n<article-title>Surface Water Quality Monitoring System Based on Autonomous Underwater Vehicles</article-title>,&#8221; <conf-name>2023 3rd International Conference on Electrical Engineering and Control Science (IC2ECS)</conf-name>, <conf-loc>Hangzhou, China</conf-loc>, (<year>2023</year>), pp. <page-range>1264&#8211;1269</page-range>, doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/IC2ECS60824.2023.10493254</pub-id></mixed-citation></ref><ref id=\"B16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>H</given-names></name></person-group>. \n<article-title>Mrfb-net: A novel attention pooling network with modified receptive field block for uterine fibroid segmentation</article-title>. <source>IEEE Access</source>. (<year>2025</year>) <volume>13</volume>:<page-range>134601&#8211;14</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ACCESS.2025.3593364</pub-id></mixed-citation></ref><ref id=\"B17\"><label>17</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A</given-names></name></person-group>. <source>Modeling Sequences with Structured State Spaces</source>. Dissertation, \n<publisher-name>Stanford University</publisher-name> (<year>2023</year>).\n</mixed-citation></ref><ref id=\"B18\"><label>18</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Omar</surname><given-names>M</given-names></name><etal/></person-group>. &#8220;\n<article-title>Selective structured state-spaces for long-form video understanding</article-title>,&#8221; <conf-name>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>, <conf-loc>Vancouver, BC, Canada</conf-loc> (<year>2023</year>), pp. <page-range>6387&#8211;97</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/CVPR52729.2023.00618</pub-id></mixed-citation></ref><ref id=\"B19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Duan</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Zhuang</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Tu</surname><given-names>X</given-names></name><etal/></person-group>. \n<article-title>Shuffle-reshuffle gradient mamba for multimodal medical image fusion</article-title>. <source>Neurocomputing</source>. (<year>2025</year>) <volume>654</volume>:<elocation-id>131316</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.neucom.2025.131316</pub-id></mixed-citation></ref><ref id=\"B20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name></person-group>. \n<article-title>Semi-mamba-unet: Pixel-level contrastive and cross supervised visual mamba-based unet for semi-supervised medical image segmentation</article-title>. <source>Knowledge-Based Syst</source>. (<year>2024</year>) <volume>300</volume>:<elocation-id>112203</elocation-id>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.knosys.2024.112203</pub-id></mixed-citation></ref><ref id=\"B21\"><label>21</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Feng</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>C</given-names></name></person-group>. \n<article-title>Em-mamba: An edge-mix enhanced long-range sequential modeling mamba for kidney segmentation in ct scans</article-title>. In <conf-name>2024 IEEE Smart World Congress (SWC)</conf-name>. (<year>2024</year>), <page-range>729&#8211;35</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/SWC62898.2024.00128</pub-id></mixed-citation></ref><ref id=\"B22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vaswani</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Shazeer</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Parmar</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Uszkoreit</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Jones</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Gomez</surname><given-names>A</given-names></name><etal/></person-group>. &#8220;\n<article-title>Attention is all you need</article-title>.&#8221; <source>Advances in Neural Information Processing Systems</source>. vol. <volume>30</volume>, (<year>2017</year>), pp. <page-range>5998&#8211;6008</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.48550/arXiv.1706.03762</pub-id></mixed-citation></ref><ref id=\"B23\"><label>23</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Rahman Siddiquee</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>J</given-names></name></person-group>. \n<article-title>Unet++: A nested U-Net architecture for medical image segmentation</article-title>. In <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source>. \n<publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc> (<year>2018</year>), pp. <fpage>3</fpage>&#8211;<lpage>11</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1007/978-3-030-00889-5_1</pub-id>, PMID: \n<pub-id pub-id-type=\"pmcid\">PMC7329239</pub-id><pub-id pub-id-type=\"pmid\">32613207</pub-id></mixed-citation></ref><ref id=\"B24\"><label>24</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Shao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>Z</given-names></name><etal/></person-group>. \n<article-title>Semantic segmentation of litchi branches using deeplabv3+ model</article-title>. <source>IEEE Access</source>. (<year>2020</year>) <volume>8</volume>:<page-range>164546&#8211;1645</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1109/ACCESS.2020.3021739</pub-id></mixed-citation></ref><ref id=\"B25\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group>. &#8220;\n<article-title>VMamba: Visual State Space Model</article-title>.&#8221; <source>Advances in Neural Information Processing Systems</source>, vol. <volume>37</volume>, (<year>2024</year>). [arXiv:2401.10166].\n</mixed-citation></ref><ref id=\"B26\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ruan</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>S</given-names></name></person-group>. \n<article-title>Vm-unet: Vision mamba unet for medical image segmentation</article-title>. <source>arXiv preprint</source>. (<year>2024</year>). doi:&#160;<pub-id pub-id-type=\"doi\">10.1145/3767748</pub-id></mixed-citation></ref><ref id=\"B27\"><label>27</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>C</given-names></name></person-group>. \n<article-title>Semi-mamba-unet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-supervised medical image segmentation</article-title>. <source>arXiv preprint arXiv:2404.07459</source>. (<year>2024</year>).\n</mixed-citation></ref><ref id=\"B28\"><label>28</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xing</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>L</given-names></name></person-group>. \n<article-title>Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation</article-title>. <source>arXiv preprint arXiv:2404.09533</source>. (<year>2024</year>)., PMID: \n<pub-id pub-id-type=\"pmid\">40679879</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TMI.2025.3589797</pub-id></mixed-citation></ref><ref id=\"B29\"><label>29</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Al-Dhabyani</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Gomaa</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Khaled</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Fahmy</surname><given-names>A</given-names></name></person-group>. \n<article-title>Dataset of breast ultrasound images</article-title>. <source>Data Brief</source>. (<year>2020</year>) <volume>28</volume>:<fpage>104863</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1016/j.dib.2019.104863</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">31867417</pub-id><pub-id pub-id-type=\"pmcid\">PMC6906728</pub-id></mixed-citation></ref><ref id=\"B30\"><label>30</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>G&#243;mez-Flores</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Gregorio-Calas</surname><given-names>MJ</given-names></name><name name-style=\"western\"><surname>de Albuquerque Pereira</surname><given-names>WC</given-names></name></person-group>. \n<article-title>Bus-bra: A breast ultrasound dataset for assessing computer-aided diagnosis systems</article-title>. <source>Med Phys</source>. (<year>2024</year>) <volume>51</volume>:<page-range>3110&#8211;23</page-range>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1002/mp.16812</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">37937827</pub-id></mixed-citation></ref><ref id=\"B31\"><label>31</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Rahman Siddiquee</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>J</given-names></name></person-group>. \n<article-title>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</article-title>. In: \n<person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Stoyanov</surname><given-names>D</given-names></name><etal/></person-group>. <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. DLMIA ML-CDS 2018 2018. Lecture Notes in Computer Science()</source>, vol <volume>11045</volume>. \n<publisher-name>Springer</publisher-name>, <publisher-loc>Cham</publisher-loc>. (<year>2018</year>)., PMID: \n<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-030-00889-5_1</pub-id><pub-id pub-id-type=\"pmcid\">PMC7329239</pub-id><pub-id pub-id-type=\"pmid\">32613207</pub-id></mixed-citation></ref><ref id=\"B32\"><label>32</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shorten</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Khoshgoftaar</surname><given-names>T</given-names></name></person-group>. \n<article-title>A survey on image data augmentation for deep learning</article-title>. <source>J big Data</source>. (<year>2019</year>) <volume>6</volume>:<fpage>1</fpage>&#8211;<lpage>48</lpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1186/s40537-019-0197-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC8287113</pub-id><pub-id pub-id-type=\"pmid\">34306963</pub-id></mixed-citation></ref><ref id=\"B33\"><label>33</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yue</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name></person-group>. \n<article-title>MedMamba: Vision Mamba for Medical Image Classification</article-title>. <source>arXiv preprint arXiv:2403.03849</source>. (<year>2024</year>).\n</mixed-citation></ref><ref id=\"B34\"><label>34</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name></person-group>. \n<article-title>Vision mamba: Efficient visual representation learning with bidirectional state space model</article-title>. <source>arXiv preprint arXiv:2401.09417</source>. (<year>2024</year>).\n</mixed-citation></ref><ref id=\"B35\"><label>35</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>You</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B</given-names></name></person-group>. \n<article-title>Segment anything in medical images</article-title>. <source>Nat Commun</source>. (<year>2024</year>) <volume>15</volume>:<fpage>654</fpage>. doi:&#160;<pub-id pub-id-type=\"doi\">10.1038/s41467-024-44824-z</pub-id>, PMID: \n<pub-id pub-id-type=\"pmid\">38253604</pub-id><pub-id pub-id-type=\"pmcid\">PMC10803759</pub-id></mixed-citation></ref><ref id=\"B36\"><label>36</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group>. \n<article-title>Medical sam adapter: Adapting segment anything model for medical image segmentation</article-title>. <source>arXiv preprint arXiv:2304.12620</source>. (<year>2023</year>).\n<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.media.2025.103547</pub-id><pub-id pub-id-type=\"pmid\">40121809</pub-id></mixed-citation></ref><ref id=\"B37\"><label>37</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>B</given-names></name></person-group>. \n<article-title>Ultrasound sam adapter: Adapting sam for breast lesion segmentation in ultrasound images</article-title>. <source>arXiv preprint arXiv:2404.07793</source>. (<year>2024</year>).\n</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Front Oncol Front Oncol 1755 frontonco Front. Oncol. Frontiers in Oncology 2234-943X Frontiers Media SA PMC12682651 PMC12682651.1 12682651 12682651 10.3389/fonc.2025.1672274 1 Original Research Research on breast tumor segmentation based on the Mamba architecture Wei Weihao 1 2 Writing &#8211; original draft Writing &#8211; review &amp; editing Wu Jiacheng 1 * Writing &#8211; original draft Writing &#8211; review &amp; editing Shao Guangming 1 * Writing &#8211; original draft Writing &#8211; review &amp; editing 1 Anhui University of Chinese Medicine , Hefei ,&#160; China 2 College of Medicine and Biological Information Engineering, Northeastern University , Shenyang ,&#160; China * Correspondence: Jiacheng Wu, wjc_@163.com ; Guangming Shao, guangmingshao@163.com 24 11 2025 2025 15 480898 1672274 25 7 2025 23 10 2025 24 11 2025 09 12 2025 09 12 2025 Copyright &#169; 2025 Wei, Wu and Shao. 2025 Wei, Wu and Shao https://creativecommons.org/licenses/by/4.0/ This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY) . The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Medical image segmentation is fundamental for disease diagnosis, particularly in the context of breast cancer, a prevalent malignancy affecting women. The accuracy of lesion localization and preservation of image details are essential for ensuring the integrity of lesion segmentation. However, the low resolution of breast tumor B-mode ultrasound images poses challenges in precisely identifying lesion sites. To address this issue, this study introduces the Mamba architecture model, which combines three foundational models with the long-sequence processing model Mamba to develop a novel segmentation model for breast tumor ultrasound images. The selective mechanism and hardware-aware algorithm of the Mamba model enable longer sequence inputs and faster computing speeds. Moreover, integrating a complete chain of VMamba blocks into the basic model enhances segmentation accuracy and image detail processing capabilities. Experimental segmentation was performed on two benchmark ultrasound datasets (BUSI and BUS-BRA) using both the baseline and improved models. The results were compared using metrics such as Dice and IoU, with additional evaluations conducted under small-sample training conditions. This study is intended to provide guidance for the future development of medical image segmentation. Moreover, the experimental results demonstrate that the model incorporating the Mamba architecture achieves superior performance on breast ultrasound images. breast tumors medical image segmentation Mamba selective mechanism hardware-aware algorithm Anhui Provincial Department of Education 10.13039/501100010814 KJ2019A0438, 2024AH050970, SK2020A0255 The author(s) declare financial support was received for the research and/or publication of this article. This work was partially supported by the Scientific Research Foundation of the Anhui Provincial Education Department (Grant No. KJ2019A0438, 2024AH050970, SK2020A0255). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes section-at-acceptance Cancer Imaging and Image-directed Interventions 1 Introduction Tumors, which are caused by the aggregation of mutated cells into masses or growths, can be categorized into benign tumors that do not spread and malignant tumors that are uncontrollably cancerous ( 1 ). Breast cancer, one of the most commonly malignant tumors among women, is also one of the leading causes of cancer death in females. In the early stages, treatment is carried out through lumpectomy, with the goal of completely removing the tumor while preserving as much healthy tissue as possible. Therefore, the precision of tumor excision is a significant challenge in this surgery, and for patients with unclear margins, there is a high probability of requiring a second excision, which may cause patients to miss the best treatment time and increase their psychological burden. For the high incidence of positive cancer margins after breast tumor excision, accurate tumor localization is key to overcoming this challenge. Ultrasound detection is considered the best method for examining breast tumors due to its non-radiation and non-invasive medical imaging approach ( 2 ). However, the frequency of the ultrasound equipment and probe directly affects image quality and lesion display, thereby influencing the diagnostician&#8217;s judgment ( 3 ), leading to missed diagnoses and misdiagnoses, which highlights the importance of early precise detection for successful treatment. Conventional diagnostic methods relying on subjective judgments have limitations and risks of misdiagnosis ( 4 ). Medical image segmentation is a crucial technology in medical image processing ( 5 , 6 ), essential for disease diagnosis, treatment planning, and evaluating treatment outcomes. Accurate segmentation delineates diseased and normal tissue boundaries, providing precise anatomical and pathological information for clinical decision-making ( 5 ). However, due to the inherent limitations of ultrasound imaging, such as poor contrast and the variability in the appearance of tumors, the development of reliable and effective segmentation algorithms still faces significant challenges. Deep convolutional neural networks (DCNNs) ( 7 ) have revolutionized this field by automatically extracting key visual features relevant to disease diagnosis from extensive medical image datasets ( 8 , 9 ). Recent advancements in medical image segmentation, notably the UNet deep-learning network, have shown remarkable potential in segmenting and classifying breast tumor images ( 10 ). UNet&#8217;s exceptional performance and adaptable network structure have made it a focal point in research ( 11 ). To further enhance segmentation models, researchers are exploring novel network architectures like dense connections ( 12 ), residual blocks ( 13 ), and attention mechanisms ( 14 ). Kumari et&#160;al. ( 15 ) utilized a neural network with a dense connection known as Densely Connected Convolutional Network (DCCN) to identify deep liver irregularities; ( 16 ) introduced a deep learning architecture (MRFB-Net) that leverages an attention-based pooling decoder module to enhance the segmentation of uterine fibroids in preoperative ultrasound images. However, common CNN models face limitations in their ability to model long-range interactions, and Transformers are constrained by their quadratic computational complexity, making them less than satisfactory for processing breast ultrasound images. This has led to the emergence of State Space Models (SSM) ( 17 , 18 ), represented by Mamba, as a promising solution. The Mamba model excels not only in modeling long-range interactions but also in maintaining linear computational complexity. It specifically improves the S4 state space model through selective mechanisms and hardware-aware algorithms, excelling in processing long-sequence data with its unique features. By integrating the cross-scan module (CSM) into the visual state space model (VMamba), Mamba enhances its applicability to computer vision tasks by spatially traversing the domain ( 17 ). ( 19 ) proposed the Shuffle-Reshuffle Gradient Mamba (SRGM) tailored for MMIF, and designed the Local and Global Gradient Mamba (LGGM) to extract modality-specific features while retaining rich spatial details. ( 20 ) introduced Semi-Mamba-UNet, which integrates a pure vision-based Mamba-based U-shaped encoder-decoder architecture with the traditional CNN-based UNet into a semi-supervised learning (SSL) framework and tested it on the ACDC and PROMISE12 medical imaging datasets. ( 21 ) introduce Edge-Mix enhanced Mamba (EM-Mamba) for kidney segmentation, which is designed to capture global and local information from multi-scales. EM-Mamba leverages SegMamba as its backbone, utilizing Mamba&#8217;s efficiency in extracting long-range dependencies. Although Transformer models excel at global modeling, their self-attention mechanism requires a computational complexity that is quadratic with respect to the image size ( 22 ), which becomes particularly evident in the task of medical image segmentation that demands dense predictions. Building on these advancements, our goal is to enhance long-sequence data processing by integrating Mamba into foundational models like UNet++ ( 23 ) and DeepLabv3+ ( 24 ), aiming to improve breast ultrasound image segmentation. Integrating the VMamba block (VSS) ( 25 ) from the Mamba model into other networks enhances the model&#8217;s medical image segmentation performance. The VSS features a unique selective mechanism and hardware-aware algorithm, offering significant advantages in processing long-sequence data. By adaptively selecting crucial information for processing, the Mamba model avoids redundant computations, thereby enhancing computational efficiency. Additionally, its hardware-aware algorithm enables seamless adaptation to diverse hardware platforms, further expediting the model&#8217;s inference process. Our research focuses on demonstrating the notable benefits of incorporating the Mamba structure into an image segmentation model for breast tumor image segmentation and classification tasks. This integration enables precise differentiation between tumor tissues and normal breast tissues, resulting in high-precision image segmentation. Specifically, we integrated the VMamba module into the encoder of the model, thereby effectively capturing the multi-scale spatial features and global contextual cues of breast ultrasound images. We conducted extensive experiments on the BUSI and BUS-BRA datasets using various metrics, and the results demonstrated that the models incorporating the VSS block achieved higher segmentation accuracy for breast ultrasound images compared to the original models. This enhancement enables precise segmentation of diverse breast tumors and their complex boundary structures. Such accuracy provides valuable support for clinicians, advancing the clinical application and scientific exploration of artificial intelligence technology in medical image processing, particularly in addressing challenges related to breast tumor image processing. 2 Mamba model structure Mamba, a state space model (SSM), shares the capability of transformers in extracting global features from lengthy sequences. However, Mamba distinguishes itself through its selective mechanism and hardware-aware algorithm, resulting in an inference speed five times faster than that of Transformers. Notably, Mamba&#8217;s computational complexity and memory usage scale linearly with input sequence length, allowing it to process sequences of millions in length. In contrast, Transformers exhibit a time and space complexity of O ( n 2 ), highlighting Mamba&#8217;s ability to markedly alleviate GPU memory and computing resource demands during the training of long-sequence text models ( 17 ). Mamba integrates the SSM architecture with the multi-layer perceptron (MLP) block within the Transformer framework. SSM serves to characterize state representations and forecast their subsequent states given specific inputs. The structured state space sequence model operates on the following principle: (1) h &#8242; ( t ) = A h ( t ) + B x ( t ) (2) y ( t ) = C h ( t ) + D x ( t ) In this context, h ( t ) denotes the current state variable, A signifies the state transition matrix, x ( t ) represents the input control variable, and B indicates the impact of the control variable on the state variable ( 26 ). Furthermore, y ( t ) denotes the system output, while C signifies the influence of the current state variable on the output. The state and output equations imply that the state at time step t is predicted from the preceding state. By incorporating past information in the sequence and the input from the prior state, the system&#8217;s future states can be anticipated. The A state transition matrix plays a crucial role in updating the sequence state by incorporating skip connections. These connections directly combine the previous input with the output sequence, thereby improving feature extraction. To tackle the challenge of context sequence dependencies, SSM utilizes Hierarchical Positional Pointers (HiPPO) for long-range dependencies. By employing function approximation, SSM achieves the optimal solution ( 27 ) of the matrix A , enabling the retention of a more extensive historical record. Selection Mechanism: The conventional SSM model excels in processing structured input data. In contrast, Mamba introduces a selective mechanism that parameterize the SSM input. This mechanism selectively compresses historical data, filters out extraneous 18 information, and preserves essential long-term memory. Consequently, Mamba addresses the challenge faced by traditional models in managing fluctuations or disorder in input sequences, thereby ensuring that parameters influencing sequence interactions adapt to the input dynamics. Specifically, a new learnable parameter step size &#916; represents the stage resolution, sampling the continuous input signal over time to obtain discrete output, which is realized by solving the ordinary differential Equation 2 and performing a direct discretization operation. Then, by sampling with step size &#916; (i.e., d &#964; &#10072; t i t i + 1 = &#916; i ), h ( t b ) can be discretized by Equation 4 . (3) h ( t b ) = e A ( t b &#8722; t a ) h ( t a ) + e A ( t b &#8722; t a ) &#8747; t a t b B ( &#964; ) u ( &#964; ) e &#8722; A ( &#964; &#8722; t a ) &#160; d &#964; (4) h b = e A ( &#916; a + &#8230; + &#916; b &#8722; 1 ) &#160; ( h a + &#8721; i = a b &#8722; 1 B i u i e &#8722; A ( &#916; a + &#8230; + &#916; i ) &#916; i ) In addition, performing zero-order hold processing on parameters A , B to obtain A &#175; = exp&#160; ( &#916; A ) , B &#175; = ( &#916; A ) &#8722; 1 ( exp&#160; ( &#916; A ) &#8722; I ) &#916; B , ultimately converting the continuous SSM to a discrete SSM, thus updating Equations 1 and 2 to 3 and 4 . (5) h k = A &#175; h k &#8722; 1 + B &#175; x k (6) y k = C h k In contrast to the fixed spacing between input and output elements in conventional copy tasks, selective copying involves adjusting token positions based on content-specific reasoning to eliminate extraneous information. As illustrated in Equation 7 , this process incorporates an additional linear layer in each matrix computation to selectively filter input control and state variables, thereby enhancing reasoning efficiency and augmenting data throughput. Enhancement of the B matrix affecting input, the C matrix influencing state, and the &#916; time-size parameter enables the model to discern the content of individual tokens, which represent the smallest meaningful units understood and generated by the model. The dimensions of B, C and &#916; can be extended by incorporating functions s B ( x ), s C ( x ) and s &#916; ( x ). The introduction of the selection mechanism addresses the limitation of SSM in screening signals across time. (7) s B ( x ) = L i n e a r N ( x ) , s C ( x ) = L i n e a r N ( x ) , s &#916; ( x ) = L i n e a r D ( x ) , &#964; &#916; = s o f t p l u s Hardware-optimized algorithm The Mamba algorithm utilizes a multi-threaded parallel scanning approach that leverages the associative law for executing out-of-order computations and aggregating outcomes. In this method, each sequence involves updating the state H i according to Equation 8 , where it is computed by multiplying the previous state with a matrix A &#175; and adding the current input X i multipied by B &#175; . The parallel scanning process integrates segmental sequence computation and iteration to achieve its objectives. (8) H i = A &#175; H i &#8722; 1 + B &#175; X i Notably, the cyclic convolution mode enables bypassing the initial fixed state ( B, L, D, N ), leading to the utilization of a more efficient 3 a convolution kernel ( B, L, D ) and significantly enhancing computational performance. The state H i is exclusively operational within the memory hierarchy. To mitigate memory bandwidth constraints, the kernel fusion technique is employed to diminish GPU memory occupancy, thereby substantially enhancing training velocity. The utilization of Flash Attention technology alters the computation outcomes sequentially inscribed in DROM to batch writing from DRAM, thereby curtailing the frequency of redundant read and write operations ( 28 ). Consequently, is substitutedfor the initial ( A &#175; , B &#175; ) with a scale of ( B , L , D , N ) and fed into the high-speed Static Random-Access Memory (SRAM). To mitigate the need for storing intermediate states during backpropagation, the utilization of recomputation technology is imperative. This approach aims to minimize memory usage by recalculating intermediate states during the backward pass, rather than storing them when loading input from High Bandwidth Memory (HBM) to SRAM. By implementing this technique, the selective scanning layer can achieve a level of memory efficiency akin to that of the high-speed attention Transformer. 3 Data processing The study leveraged data from two publicly available datasets: BUSI ( 29 ) and BUS-BRA ( 30 ). The BUSI dataset comprises breast ultrasound images and their corresponding label images, collected from 600 women aged 25 to 75 in 2018. Each original image is paired with a tumor image (mask), with benign and malignant samples typically featuring one or two lesions. As a result, the labels outlining the lesion areas may require overlapping to consolidate multiple lesions into a single label. The BUS-BRA dataset includes 1875 anonymized breast ultrasound images from 1064 patients, with 722 benign and 342 malignant tumors. It provides BI-RADS assessments, manual segmentations, and 5- and 10-fold cross-validation partitions for standardized evaluation of CAD systems. The detailed information of the dataset is shown in Table&#160;1 . Table&#160;1 Introduction of dataset. Case BUSI BUS-BRA Number of Images 780 1875 Benign 437 1268 Malignant 210 607 Normal 133 &#8211; Annotation Information Masks Masks and BI-RADS classification Dataset Characteristics Smaller Larger, suited for extensive training Adequate data is essential for effectively training deep learning networks to prevent underfitting and subpar classification performance ( 31 ). To bolster model robustness, a substantial volume of high-quality datasets is necessary ( 32 ). However, obtaining medical image data is intricate, necessitating the expansion of existing public datasets through data augmentation techniques. In our approach, we employ online augmentation, randomly rotating and mirror-flipping each image and its corresponding label in the dataset to enhance the model&#8217;s generalization capabilities. Furthermore, we enhance image quality by applying linear transformations to address the indistinct edges characteristic of ultrasound images in the dataset in function (9), thus suppressing the Hausdorff dimension inflation caused by ultrasonic speckle noise. Because the scanning position varies across breasts, the collected ultrasound images exhibit inconsistent sharpness and brightness. We therefore perform dynamic contrast normalization as defined in Equation 9 : the gray-level histogram of each image is first computed, its intensity bins are used to derive an adaptive weight, and the image contrast is adjusted accordingly, yielding a standardized dataset. (9) O ( i , j ) = &#945; * I ( i , j ) + b &#160; &#160; &#160; &#160; &#160; 0 &#8804; i &lt; H , 0 &#8804; j &lt; W Here, H and W denote the height and width of the input image, I ( i,j ) represents a pixel point in the input image, O ( i,j ) for the output image; by adjusting the size of parameters a,b to achieve transformation of the image grayscale range, thereby adjusting the image contrast. 4 Research on ultrasound breast tumor image segmentation based on mamba architecture This study integrates the Mamba model with different segmentation network architectures to enhance the performance of medical image segmentation. By incorporating the VSS block featuring the Mamba model into diverse segmentation networks, improvements in segmentation accuracy are achieved. Evaluation on a dataset and comparison of segmentation outcomes of the fused models demonstrate that the integration of the Mamba structure accelerates computation while preserving long-term data information. 4.1 Analysis of the VMamba block Figure&#160;1 illustrates the architecture of the VMamba block, comprising an H3 block and a gated MLP. The H3 block embodies a selective SSM (independent sequence transformation) state-space model. Simplifying the H3 structure involves amalgamating linear attention and MLP blocks, stacking them uniformly, and enabling controlled expansion of the model dimension. The Mamba architecture is constructed by iteratively replicating this block, incorporating residual connections and standard normalization interchangeably. To mitigate gradient vanishing, a residual term is introduced in conjunction with the gated MLP. The VMamba block is limited to extracting features from semantic data like text and cannot handle image data. To address this limitation, Yue et&#160;al. ( 33 ) substituted the S6 module in the VMamba block with the SS2D module, which is designed to process image data using the VSS block. This modification resulted in the creation of the VSS block. Figure&#160;1 Structures of the Mamba block (left), the Vanilla VSS block (middle), and the VMamba (VSS) block. Three block diagrams labeled Mamba Block, The Vanilla VSS Block, and VSS Block show different neural network architectures. Each block includes components like Linear, Silu, DWConv, and normalization layers. Mamba Block has a unique S6 layer, The Vanilla VSS Block features SS2D, and VSS Block includes SS2DBlock and FFN. Arrows indicate data flow. Following layer normalization, the VSS block comprises two branches. One branch employs a 3&#215;3 depthwise convolutional layer for feature extraction. Initially, the input undergoes processing in a linear layer, a depthwise separable convolution, and an activation function before entering the two-dimensional selective scanning (SS2D) module for further feature extraction. Subsequently, feature normalization is applied, followed by element-wise multiplication with the output from the alternate branch to merge the pathways. A linear layer is then utilized to blend the features, which are combined with a residual connection to yield the VSS module output. The second branch includes a linear mapping layer followed by a SiLU activation layer to compute the multiplicative gating signal. Notably, the key distinction from the standard VSS block lies in replacing the S6 module with the SS2D module, enabling adaptive selective scanning for 2D visual data. This design choice opts for a more compact structure without the fully connected phase, resulting in denser stack blocks within the same depth constraints. 4.2 Construction of the VM-UNet++ model Figure&#160;2 illustrates the architecture of VM-UNet++. This design integrates the U-Net framework with the VSS block to construct the encoder and decoder components. The U-Net features a symmetrical U-shaped configuration comprising an encoder for feature extraction, a decoder for feature fusion, and skip connections to mitigate gradient vanishing ( 15 ). Within the decoder&#8217;s upsampling phase, skip connections are employed post each convolution to link with the downsampled encoder features at the corresponding level and lower-level features, thereby diminishing gradient vanishing and preserving more spatial detail features. The VM-UNet++ configuration encompasses a patch embedding layer, an encoder, a decoder, a final projection layer, and skip connections. Initially, the input image is transformed into a one-dimensional sequence of H/4&#215;W/4&#215;C via the patch and linear embedding layers. The encoder incorporates multiple VSS blocks and patch merging layers to extract token features, diminish height and width, and augment dimensionality ( 34 ). The decoder mirrors the encoder&#8217;s structure, with the patch merging layer substituted by a patch expansion layer to enhance height and width while reducing dimensionality, thereby generating outputs with consistent feature sizes. Ultimately, the linear projection layer restores the channel count to align with the input resolution. A densely connected network is introduced during the upsampling phase, where the convolution output of the preceding layer is added to each subsequent layer, forming local Unet networks within each segment. This approach fuses low-resolution features from upsampling with high-resolution features from downsampling to retain both spatial detail features and global information. The densely connected structure of the VM-UNet++ model facilitates straightforward network depth augmentation to bolster learning capacity during construction. Moreover, it permits a moderate depth reduction through network pruning strategies without compromising the original network architecture. Figure&#160;2 Structure diagram of VM-UNet++. Flowchart showing an encoder-decoder architecture for image processing. The encoder consists of multiple VSS block modules interspersed with patch partitions, reducing dimensions sequentially from height H by width W to smaller ones. Skip connections link corresponding blocks in encoder and decoder. The decoder reconstructs the image with similar VSS blocks and patch partitions, ending with linear embedding to produce the final segmented output labeled as &#8220;Seg&#8221;. Lines depict connections and data flow, with dimensions labeled at each stage. 4.3 Construction of Deep-VMamba DeepLabV3+ comprises an encoder and a decoder, incorporating Atrous convolution, depthwise separable convolution, Atrous Spatial Pyramid Pooling (ASPP), and fully convolutional networks ( 31 ). As illustrated in Figure&#160;3 , this study integrates the Mamba structure into the DeepLabV3+ architecture. The VMamba block, fused with the encoder output of DeepLabV3+, enhances the delineation of tumor lesions in ultrasound breast images by capturing finer details and edge information. The incorporation of the VMamba block supplements global information to the original DeepLabV3+ segmentation, thereby expanding the network&#8217;s receptive field without compromising feature retention, facilitating more comprehensive malignant tumor segmentation. Additionally, the encoder segment of DeepLabV3+ encompasses a complete feature extraction and sampling branch, preserving all feature extraction capabilities while augmenting the model&#8217;s proficiency in feature extraction from images and processing extended sequences, without compromising its original functionality. Figure&#160;3 Structure of the Deep-VMamba model. Diagram illustrating a neural network architecture with three sections: VMamba, Encoder, and Decoder. The VMamba section includes Patch Partition, FFN, LN, and other blocks. The Encoder features a convolution pyramid for image pooling and a DCNN with Atrous Convolution. The Decoder processes low-level features, includes upsampling, and uses concatenation to output an image. An inset shows an ultrasound image as input. 4.4 Construction of the SAM-VMamba model The Segment Anything Model (SAM) model is tailored for a novel image segmentation assignment, trained on a dataset of 11 million images with over one billion masks. Moreover, SAM can segment images based on various prompts such as points, boxes, and text, without the need for retraining on specific datasets. Its efficient design and training facilitate zero shot transfer to new image distributions and tasks, which has garnered widespread attention. For instance, Ma and Wang et&#160;al. ( 35 ) proposed MedSAM for general medical image segmentation. This model, trained on a meticulously constructed dataset, is capable of achieving desirable performance. However, the limited scale of the assembled dataset and the modality imbalance issue restrict MedSAM&#8217;s performance on ultrasound images. The MSA method proposed by Wu et&#160;al. ( 36 ) significantly enhances image segmentation performance by freezing the pre-trained parameters of SAM and inserting adapter modules at specific locations. As shown in Figure&#160;4 , the SAM model comprises an image encoder, a prompt encoder, and a mask decoder, the SAM model employs a prompting approach to segment user-specified points. Users can provide prompt information through user-defined points, bounding boxes, and randomly circled regions. Furthermore, free-form text prompts are utilized to present initial results. Notably, the prompt encoder of the SAM model can effectively segment desired objects based on user prompts, thereby enabling targeted area segmentation. For the segmentation of breast ultrasound images, Tu et&#160;al. ( 37 ) proposed an innovative SAM adapter (BUSSAM), which migrates the SAM framework to the field of breast ultrasound image segmentation through adaptation techniques, and validated its feasibility and effectiveness. Figure&#160;4 Structure of the SAM model. Flowchart showing an ultrasound image of the left breast being processed by an image encoder. The outputs are fed into a convolutional layer, then to mask and prompt decoders, resulting in two processed ultrasound images with highlighted areas. Although the SAM demonstrates evident effectiveness for the segmentation of the vast majority of natural images, it faces challenges when dealing with fine medical images due to the inherent low resolution and complexity of medical imaging, leading to suboptimal performance in zero-shot segmentation scenarios. Therefore, few-shot training becomes crucial for achieving superior performance in practical applications. Moreover, considering the limitations of SAM in global attention, our study incorporates the VMamba block into the SAM model framework to enhance its capabilities. As shown in Figure&#160;5 , the VMamba block, situated alongside the ViT within the SAM image encoder, facilitates the processing of extended input sequence data for a more comprehensive contextual understanding. Illustrated in Figure&#160;5 , the model comprises an image encoder, a prompt encoder, and a mask decoder. Following the Patch Embedding step in the image encoder, the VMamba block operates in parallel to convert the upper layer&#8217;s output tokens into a linear vector with long-range memory. This vector is then fused with the Transformer block output and subjected to two convolutions (Neck layer) to generate the Image Embedding, which subsequently serves as input for the mask decoder. Figure&#160;5 Structure of the SAM-VMamba model. Flowchart depicting the VMamba model architecture. It shows processes like patch embedding, positional embedding, and image encoding using window transformer and attention mechanisms. Outputs include a mask decoder and prompt encoder for image analysis, illustrated with ultrasound scans, and highlighted regions. The enhanced comprehension of extended sequences by the Mamba model enables SAM-VMamba to establish improved global connections and achieve enhanced segmentation performance. Additionally, the integration of a selective mechanism and hardware-aware algorithm in SAM-VMamba expedites model training and implementation without incurring additional time costs, thereby substantially decreasing the time and resources needed for training deep segmentation models. Moreover, by integrating the generalization and pre-training capabilities of the SAM model, the SAM-VMamba model is able to achieve accurate segmentation effects with only a small number of breast ultrasound image training samples. 5 Experimental results and analysis 5.1 Experimental environment The model proposed in our study is deployed and trained on the RTX A6000 GPU, with all experiments conducted using the same hardware device. The experiments utilize the PyTorch 2.2.0 deep learning framework and Python 3.11.5 programming language, with GPU computation supported by the CUDA 12.2 architecture. The batch size is set to 24, with a maximum of 100 training epochs, and the AdamW optimizer is employed in conjunction with the CosineAnnealingLR learning rate scheduling strategy for model optimization. Additionally, the proposed SAM-VMamba network is initialized using the pre-trained weights of SAM&#8217;s ViT-B. 5.2 Evaluation indicators The Dice Coefficient (DICE) metric assesses model performance in segmentation tasks by quantifying the overlap between predicted and ground-truth regions. In medical image analysis, the DICE coefficient is commonly employed to evaluate neural network models in tasks like lesion detection and tissue segmentation. The formula for calculating the DICE coefficient is defined by Equation 10 . (10) D I C E = 2 | T P | 2 | T P | + | F P | + | F N | The MIoU metric assesses the correspondence between predicted outcomes and true labels in semantic segmentation tasks. It is computed as the mean of the Intersection over Union (IoU) for individual categories. IoU represents the ratio of the intersection to the union of predicted and actual values, reflecting the degree of overlap. A higher IoU signifies improved segmentation accuracy, indicating a greater overlap between areas. The calculation is given by Equation 11 . (11) I o U = | T P | | T P | + | F P | + | F N | Precision is defined as the proportion of pixels that are correctly identified as the true lesion area. It represents the ratio of true positive pixels to the sum of true positive and false positive pixels, expressed as: | T P | | T P | + | F P | . Accuracy represents the proportion of correctly identified image pixels, that is, the ratio of breast tumor and non-breast tumor areas to the total number of pixels (mask), expressed as: | T P | + | T N | T + P . Recall, also known as the sensitivity, is the proportion of the actual lesion area that is identified in the image. It represents the size of the true positive cases relative to the entire lesion area, expressed as: | T P | | T P | + | F N | . 5.3 Experimental results Table&#160;2 summarizes the training hyper-parameters and computational performance of each model. Where, throughput denotes the maximum number of training samples the model can process per second, while total multiply-adds signify the computational burden of the model during a single forward propagation. The exceptional long-sequence processing capabilities of Mamba are confirmed through an assessment of the computational efficiency of output images. This evaluation involves comparing parameters, processes, and throughput during both training and inference to gauge generalization performance. Results indicate that models incorporating Mamba exhibit consistent performance across various input image sizes. For instance, at an input resolution of 512 &#215; 512, Unet++ achieves the highest throughput among baseline models, while DeepLabV3+ demonstrates the highest throughput per epoch during training. Despite higher computational load compared to baseline models at the same input size, the integration of the Mamba structure allows for increased throughput capacity, enabling the retention and processing of longer data sequences, thereby enhancing comprehensive image data processing. Moreover, while the integrated models maintain relatively high inference speeds (higher throughput than baseline models) at a resolution of 512&#215;512, their computational load escalates significantly, surpassing that of baseline models and indicating limited generalization capability. In terms of computational efficiency, current SSM-based vision models typically exhibit superior throughput only with large-scale inputs and high resolutions. Table&#160;2 Parameter table on the 512 2 image. Model #param FLOPs Throughput Train throughput Total mult-adds Unet 24.4M 31.3G 75.38 57.69 31.26 Unet++ 26.1M 73.7G 71.12 33.72 73.53 DeepLabV3+ 22.4M 31.7G 73.71 58.44 31.54 VM-UNet 27.4M 16.4G 48.87 25.46 310.02 SAM-Med2D 221.9M 303.5G 21.28 24.10 259.31 VM-UNet++ 27.4M 27.2G 47.03 30.27 443.97 Deep-VMamba 27.8M 121.2G 64.15 46.90 43.28 SAM-VMamba 236.5M 259.8G 27.73 35.35 303.26 Based on Tables&#160;3 and 4 , it is evident that traditional models do not achieve highly accurate segmentation of ultrasonic breast tumor images. The Unet model, for instance, exhibits relatively low performance with Dice coefficients of 81.92% and 82.10%, and IoU values of 69.53% and 73.52% across the two datasets. In contrast, models incorporating the VMamba block demonstrate a significant improvement in segmentation metrics compared to their original counterparts. Notably, the SAM-VMamba model, which integrates the VMamba block, achieves the highest performance with Dice scores of 90.62% and 90.25%, and IoU values of 82.55% and 82.54%. This improvement can be attributed to the inherent challenges posed by breast ultrasound images, characterized by low clarity and predominantly dark tones, leading to a diminished signal-to-noise ratio. Given that breast tumors occupy a small portion of the image, there is a risk of lesion oversight and misjudgment. Furthermore, the indistinct boundary between breast tumors and normal tissues, coupled with blurred lesion edges lacking distinctive features against the background, contributes to reduced segmentation accuracy. Moreover, the uniform grayscale distribution in the images results in minimal variations in pixel intensities, thereby compromising texture and detail resolution. However, the integration of Mamba facilitates the capture of prolonged sequential information, enabling more comprehensive breast tumor segmentation and enhanced edge delineation. Table&#160;3 Presents a comparative evaluation of the segmentation outcomes of the models on BUSI. Model DICE(%) IoU(%) Precision(%) Accuray(%) Recall(%) Unet 81.92 69.53 86.33 97.78 78.53 Unet++ 82.76 70.81 86.98 97.81 79.32 DeepLabV3+ 83.61 71.97 88.05 97.92 79.74 VM-UNet 83.56 74.13 84.58 97.60 84.43 SAM-Med2D 89.13 81.16 89.45 98.56 87.43 VM-UNet++ 84.89 75.59 86.57 97.99 87.61 Deep-VMamba 84.24 71.24 88.26 97.84 79.21 SAM-VMamba 90.62 82.55 88.44 98.08 92.05 Table&#160;4 Presents a comparative evaluation of the segmentation outcomes of the models on BUS-BRA. Model DICE (%) IoU(%) Precision(%) Accuray(%) Recall(%) Unet 82.10 73.52 87.30 97.51 84.27 Unet++ 84.88 76.28 87.02 97.43 85.71 DeepLabV3+ 86.43 77.98 87.50 97.74 88.24 VM-UNet 85.69 77.19 87.95 97.59 86.48 SAM-Med2D 88.68 80.05 97.49 98.35 81.76 VM-UNet++ 86.35 78.01 88.95 97.63 86.61 Deep-VMamba 87.16 79.01 87.82 97.82 88.99 SAM-VMamba 90.25 82.54 96.19 98.58 85.43 Figures&#160;6 and 7 visualize the actual segmentation results of each model on breast tumors. Traditional models exhibit poor performance in segmentation due to the limitations of their structure. The pooling layers and downsampling operations used in network training result in the loss of partial information as the network depth increases and size decreases. In contrast, during upsampling, only a basic addition operation is conducted on high-resolution images from the downsampling layer, leading to the loss of crucial &#8220;deep-layer&#8221; feature information. However, Mamba, characterized by its capacity for ultra-long sequence processing and memory within the integrated model, preserves more spatial details, thereby yielding superior segmentation outcomes. SAM-VMamba demonstrates superior performance with small-sample data due to its integration of the SAM segmentation model. Furthermore, as illustrated in Figure&#160;8 , the distinctive prompt encoder of SAM enables precise regional segmentation of images, enhancing its practical utility by eliminating the need to process redundant image components. Figure&#160;6 Comparison of image segmentation effects on the BUSI dataset. Comparison of ultrasound images and segmentation masks using various methods. Rows represent malignant, benign, and normal cases. Columns include original images, masks, and results from Unet, Unet++, VM-UNet, VM-UNet++, DeepLabV3+, Deep-VMamba, SAM-Med2D, and SAM-VMamba. Segmentation accuracy varies across different methods. Figure&#160;7 Comparison of image segmentation effects on the BUS-BRA dataset. Three rows of images compare breast ultrasound segmentation results from different models. Each row includes an original ultrasound image, its mask, and segmentation outputs from Unet, Unet++, VM-UNet, VM-UNet++, DeepLabV3+, Deep-VMamba, SAM-Med2D, and SAM-VMamba. The segmentations vary in shape and accuracy. Figure&#160;8 Segmentation results of SAM-VMamba. A grid of ultrasound images showing analysis of malignant and benign areas. The columns display the original image, mask, and segmentations using different prompts: 1 point, 2 point, local box, and global box. The malignant and benign rows illustrate variations in shading and shape with blue overlays and green boxes indicating segmented areas. Figures 9a, b depict the cumulative distribution of prediction effects for each model based on 300 segmentation predictions of breast ultrasound images. The segmentation outcomes of conventional models such as U-Net predominantly cluster around 0.8 for Dice and 0.7 for IoU. The integration of the Mamba model notably enhances the overall segmentation performance, yielding higher accuracy metrics compared to the baseline models. Particularly noteworthy is the superior segmentation efficacy of the SAM model surpassing that of its counterparts. The SAM model demonstrates heightened segmentation accuracy and data concentration, indicative of its robust stability. The primary reason lies in the prompt encoder mechanism of the SAM model and its pretraining on large-scale datasets, which enable superior adaptation and handling of out-of-domain datasets. In contrast, baseline models suffer from a substantial loss distance between their initialized weights and the optimal solution, requiring large sample sizes and extensive iterations to reduce this gap, thereby leading to unstable extrapolation in prediction distributions. Moreover, models augmented with the Mamba structure further enhance the multi-scale spatial decomposition of breast tumor images and the modeling of intra-scale feature dependencies, thereby facilitating the extraction of tumors with varying shapes and types. Figure&#160;9 Cumulative distributions of dice and IoU under 300 predictions. (a) Dice, (b) IoU. Two bar charts compare the distribution of predictions from different segmentation models. Chart (a) shows the Dice coefficient distribution, while chart (b) shows the Intersection over Union (IoU) distribution. Models compared include U-Net, U-Net++, DeepLabV3+, and others, each represented by different colors. Both charts display the number of predictions on the y-axis and threshold values on the x-axis, with noticeable peaks around higher threshold values, indicating varying performance across models. The Figures&#160;10a, b illustrates notable enhancement in the model&#8217;s performance with the integration of the Mamba structure compared to the original model. Specifically, the incorporation of this structure significantly improves the segmentation performance of the model on breast ultrasound images with long sequences. With an increase in the number of training iterations, conventional models like Unet exhibit some degree of enhancement. However, the integrated Mamba model surpasses the performance of individual traditional models in overall improvement. Particularly noteworthy is the superior segmentation performance of the SAM model compared to other models, attributed to its pre-training on a large dataset of millions of images. This model requires only a limited number of training epochs to achieve a stable and optimal performance level. Figure&#160;10 Comparison of segmentation accuracies among different models trained with a low number of iterations. (a) IoU, (b) Dice. Two line graphs compare different models across epochs. Graph (a) displays IoU scores, and graph (b) shows Dice scores. Models include Unet, Unet++, DeepLabV3+, Deep-VMamba, VM-UNet, VM-UNet++, SAM-Med2D, and SAM-VMamba. SAM-VMamba and SAM-Med2D show consistently high performance in both graphs. Each line represents a model, demonstrating their improvement over 70 epochs. Figure&#160;11 presents the performance of all models under few-sample testing conditions. Findings indicate that when trained on a small dataset, the model incorporating VMamba blocks generally outperforms the baseline model in segmentation accuracy. The primary reason is that breast tumors exhibit a relatively low signal-to-noise ratio and indistinct features, which forces baseline models to rely on large sample averaging to suppress noise. In contrast, Mamba compresses two-dimensional spatial sequences into fixed-dimensional state vectors, inherently embedding a Gaussian&#8211;Markov smoothing mechanism that provides natural denoising. These advantages collectively enable the Mamba-enhanced model to achieve an average improvement of 9.12% in the IoU metric. Remarkably, despite being pretrained on extensive data, the SAM model exhibits sustained high segmentation performance in the context of limited-sample training, maintaining an IoU value of approximately 80%. Figure&#160;11 Comparison of segmentation accuracy (IoU) of each model on small samples. Line graph comparing multiple models' Intersection over Union (IoU) performance based on small sample sizes. The x-axis represents small sample sizes ranging from 30 to 120. The y-axes display IoU scores: left (36 to 75) and right (67 to 84). Different lines, both solid and dashed, represent models like Unet, Unet++, DeepLabV3+, and SAM variants, showing varying performance trends. A legend identifies each model with a specific line style and color. 6 Conclusions In this investigation, we enhanced the image segmentation performance of the original model for breast tumor ultrasound images by integrating the Mamba structure. Comparative analysis with conventional models like Unet, DeepLabV3+, and Unet++ revealed the superior performance of the model incorporating the VMamba block across various evaluation metrics, including the DICE coefficient, MIoU, Precision, and Recall. Benefiting from the global attention capability of Mamba, the enhanced model is able to simultaneously capture multi-scale global dependencies and better focus on the details of breast tumor segmentation. Experimental results show that incorporating Mamba into the model yields average improvements of 3.07% and 5.11% in Dice and IoU on the BUSI dataset, and 2.89% and 3.26% on the BUS-BRA dataset. Notably, the SAM-VMamba achieved the highest segmentation accuracy and quality, with Dice scores of 90.25% and 90.62% on the BUSI and BUS-BRA datasets. These outcomes signify the model&#8217;s success in accurately localizing and distinguishing boundaries of breast tumors. Edited by: Pierre Boulanger , University of Alberta, Canada Reviewed by: Yu Zhang , Zhejiang Lab, China Arshpreet Kaur , Amity University, India Data availability statement The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding author. Author contributions WW: Writing &#8211; original draft, Writing &#8211; review &amp; editing. JW: Writing &#8211; original draft, Writing &#8211; review &amp; editing. GS: Writing &#8211; original draft, Writing &#8211; review &amp; editing. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Generative AI statement The author(s) declare that no Generative AI was used in the creation of this manuscript. Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible. If you identify any issues, please contact us. Publisher&#8217;s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. References 1 Roy K Ghosh S Mukherjee A Sain S Pathak S Chaudhuri SS . &#8220; Breast tumor segmentation using image segmentation algorithms ,&#8221; 2019 International Conference on Opto-Electronics and Applied Optics (Optronix) , Kolkata, India ( 2019 ), pp. 1 &#8211; 5 . doi:&#160; 10.1109/OPTRONIX.2019.8862339 2 Benaouali M Bentoumi M Touati M Ahmed AT Mimi M . &#8220; Segmentation and classification of benign and Malignant breast tumors via texture characterization from ultrasound images ,&#8221; 2022 7th International Conference on Image and Signal Processing and their Applications (ISPA) , Mostaganem, Algeria ( 2022 ), pp. 1 &#8211; 4 . doi:&#160; 10.1109/ISPA54004.2022.9786350 3 El-Azizy ARM Salaheldien M Rushdi MA Gewefel H Mahmoud AM . &#8220; Morphological characterization of breast tumors using conventional b-mode ultrasound images ,&#8221; 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) , Berlin, Germany ( 2019 ), pp. 6620&#8211;3 . doi:&#160; 10.1109/EMBC.2019.8857438 , PMID: 31947359 4 Elmore J Armstrong K Lehman C Fletcher S . Screening for breast cancer . Jama . ( 2005 ) 293 : 1245&#8211;56 . doi:&#160; 10.1001/jama.293.10.1245 , PMID: 15755947 PMC3149836 5 Patil D Deore S . Medical image segmentation: a review . Int J Comput Sci Mobile Computing . ( 2013 ) 2 : 22&#8211;7 . 6 Belsare A Mushrif M . Histopathological image analysis using image processing techniques: An overview . Signal Image Process . ( 2012 ) 3 : 23 . doi:&#160; 10.5121/sipij.2012.3403 7 Krizhevsky A Sutskever I Hinton G . Imagenet classification with deep convolutional neural networks . In Advances in Neural Information Processing Systems ( 2012 ) Vol. 25 , pp. 1097&#8211;1105 . doi:&#160; 10.1145/3065386 8 Olimov B Koh S Kim J . Aedcn-net: Accurate and efficient deep convolutional neural network model for medical image segmentation . IEEE Access . ( 2021 ) 9 : 154194&#8211;203 . doi:&#160; 10.1109/ACCESS.2021.3128607 9 Tulbure A Tulbure A Dulf E . A review on modern defect detection models using dcnns&#8211;deep convolutional neural networks . J Advanced Res . ( 2022 ) 35 : 33 &#8211; 48 . doi:&#160; 10.1016/j.jare.2021.03.015 , PMID: 35024194 PMC8721352 10 Vianna P Farias R de Albuquerque Pereira W . U-net and segnet performances on lesion segmentation of breast ultrasonography images . Res Biomed Eng . ( 2021 ) 37 : 171&#8211;9 . doi:&#160; 10.1007/s42600-021-00137-4 11 Li X Chen H Qi X Dou Q Fu C Heng P . H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes . IEEE Trans Med Imaging . ( 2018 ) 37 : 2663&#8211;74 . doi:&#160; 10.1109/TMI.2018.2845918 , PMID: 29994201 12 Huang G Liu Z van der Maaten L Weinberger K . &#8220; Densely connected convolutional networks ,&#8221; 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Honolulu, HI, USA ( 2017 ), pp. 2261&#8211;9 . doi:&#160; 10.1109/CVPR.2017.243 13 He K Zhang X Ren S Sun J . &#8220; Deep residual learning for image recognition ,&#8221; 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Las Vegas, NV, USA ( 2016 ), pp. 770&#8211;8 . doi:&#160; 10.1109/CVPR.2016.90 14 Woo S Park J Lee J Kweon I . CBAM: Convolutional Block Attention Module . In: Ferrari V Hebert M Sminchisescu C Weiss Y (eds) Computer Vision - ECCV 2018. ECCV 2018. Lecture Notes in Computer Science() , vol 11211 . Springer , Cham . ( 2016 ). doi:&#160; 10.1007/978-3-030-01234-2_1 15 Zhang B Sun S Su Y Huang Q . &#8220; Surface Water Quality Monitoring System Based on Autonomous Underwater Vehicles ,&#8221; 2023 3rd International Conference on Electrical Engineering and Control Science (IC2ECS) , Hangzhou, China , ( 2023 ), pp. 1264&#8211;1269 , doi:&#160; 10.1109/IC2ECS60824.2023.10493254 16 Jiang Y Ding X Zhou H . Mrfb-net: A novel attention pooling network with modified receptive field block for uterine fibroid segmentation . IEEE Access . ( 2025 ) 13 : 134601&#8211;14 . doi:&#160; 10.1109/ACCESS.2025.3593364 17 Gu A . Modeling Sequences with Structured State Spaces . Dissertation, Stanford University ( 2023 ). 18 Wang J Zhu W Wang P Yu X Liu L Omar M . &#8220; Selective structured state-spaces for long-form video understanding ,&#8221; 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Vancouver, BC, Canada ( 2023 ), pp. 6387&#8211;97 . doi:&#160; 10.1109/CVPR52729.2023.00618 19 Lin L Duan Y Wang Y Huang J Zhuang R Tu X . Shuffle-reshuffle gradient mamba for multimodal medical image fusion . Neurocomputing . ( 2025 ) 654 : 131316 . doi:&#160; 10.1016/j.neucom.2025.131316 20 Ma C Wang Z . Semi-mamba-unet: Pixel-level contrastive and cross supervised visual mamba-based unet for semi-supervised medical image segmentation . Knowledge-Based Syst . ( 2024 ) 300 : 112203 . doi:&#160; 10.1016/j.knosys.2024.112203 21 Feng S Li Z Zheng M Yang Y Wang X Guo C . Em-mamba: An edge-mix enhanced long-range sequential modeling mamba for kidney segmentation in ct scans . In 2024 IEEE Smart World Congress (SWC) . ( 2024 ), 729&#8211;35 . doi:&#160; 10.1109/SWC62898.2024.00128 22 Vaswani A Shazeer N Parmar N Uszkoreit J Jones L Gomez A . &#8220; Attention is all you need .&#8221; Advances in Neural Information Processing Systems . vol. 30 , ( 2017 ), pp. 5998&#8211;6008 . doi:&#160; 10.48550/arXiv.1706.03762 23 Zhou Z Rahman Siddiquee M Tajbakhsh N Liang J . Unet++: A nested U-Net architecture for medical image segmentation . In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support . Springer , Cham ( 2018 ), pp. 3 &#8211; 11 . doi:&#160; 10.1007/978-3-030-00889-5_1 , PMID: PMC7329239 32613207 24 Peng H Xue C Shao Y Chen K Xiong J Xie Z . Semantic segmentation of litchi branches using deeplabv3+ model . IEEE Access . ( 2020 ) 8 : 164546&#8211;1645 . doi:&#160; 10.1109/ACCESS.2020.3021739 25 Liu Y Tian Y Zhao Y Yu H Xie L Wang Y . &#8220; VMamba: Visual State Space Model .&#8221; Advances in Neural Information Processing Systems , vol. 37 , ( 2024 ). [arXiv:2401.10166]. 26 Ruan J Xiang S . Vm-unet: Vision mamba unet for medical image segmentation . arXiv preprint . ( 2024 ). doi:&#160; 10.1145/3767748 27 Wang Z Ma C . Semi-mamba-unet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-supervised medical image segmentation . arXiv preprint arXiv:2404.07459 . ( 2024 ). 28 Xing Z Ye T Yang Y Liu G Zhu L . Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation . arXiv preprint arXiv:2404.09533 . ( 2024 )., PMID: 40679879 10.1109/TMI.2025.3589797 29 Al-Dhabyani W Gomaa M Khaled H Fahmy A . Dataset of breast ultrasound images . Data Brief . ( 2020 ) 28 : 104863 . doi:&#160; 10.1016/j.dib.2019.104863 , PMID: 31867417 PMC6906728 30 G&#243;mez-Flores W Gregorio-Calas MJ de Albuquerque Pereira WC . Bus-bra: A breast ultrasound dataset for assessing computer-aided diagnosis systems . Med Phys . ( 2024 ) 51 : 3110&#8211;23 . doi:&#160; 10.1002/mp.16812 , PMID: 37937827 31 Zhou Z Rahman Siddiquee M Tajbakhsh N Liang J . UNet++: A Nested U-Net Architecture for Medical Image Segmentation . In: Stoyanov D . Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. DLMIA ML-CDS 2018 2018. Lecture Notes in Computer Science() , vol 11045 . Springer , Cham . ( 2018 )., PMID: 10.1007/978-3-030-00889-5_1 PMC7329239 32613207 32 Shorten C Khoshgoftaar T . A survey on image data augmentation for deep learning . J big Data . ( 2019 ) 6 : 1 &#8211; 48 . doi:&#160; 10.1186/s40537-019-0197-0 PMC8287113 34306963 33 Yue Y Li Z . MedMamba: Vision Mamba for Medical Image Classification . arXiv preprint arXiv:2403.03849 . ( 2024 ). 34 Zhu L Liao B Zhang Q Wang X Liu W Wang X . Vision mamba: Efficient visual representation learning with bidirectional state space model . arXiv preprint arXiv:2401.09417 . ( 2024 ). 35 Ma J He Y Li F Han L You C Wang B . Segment anything in medical images . Nat Commun . ( 2024 ) 15 : 654 . doi:&#160; 10.1038/s41467-024-44824-z , PMID: 38253604 PMC10803759 36 Wu J Fu R Fang H Liu Y Wang Z Xu Y . Medical sam adapter: Adapting segment anything model for medical image segmentation . arXiv preprint arXiv:2304.12620 . ( 2023 ). 10.1016/j.media.2025.103547 40121809 37 Tu Z Gu L Wang X Jiang B . Ultrasound sam adapter: Adapting sam for breast lesion segmentation in ultrasound images . arXiv preprint arXiv:2404.07793 . ( 2024 )."
}