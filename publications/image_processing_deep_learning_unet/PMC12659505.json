{
  "pmcid": "PMC12659505",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:29.612982",
  "metadata": {
    "journal_title": "Plant Methods",
    "journal_nlm_ta": "Plant Methods",
    "journal_iso_abbrev": "Plant Methods",
    "journal": "Plant Methods",
    "pmcid": "PMC12659505",
    "pmid": "41299589",
    "doi": "10.1186/s13007-025-01476-4",
    "title": "A conditional segmentation-guided network for pomegranate image completion under occlusion",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Zhang Duokuo",
      "Hou Ruizhe",
      "Guo Jingjing",
      "Zhao Mingfu",
      "Wang Qi",
      "Luo Zhen",
      "Xu Kun"
    ],
    "abstract": "In agricultural images acquired under natural conditions, pomegranate fruits are often partially occluded by leaves and branches, resulting in missing structural information that compromises the accuracy of yield estimation and automated harvesting. To overcome the challenges of recovering structural integrity in occluded agricultural imagery, we propose the Conditional Segmentation-guided Diffusion Network (CSD-Net). CSD-Net is a lightweight, unified framework, representing the first conditional diffusion model specifically designed for the joint tasks of pomegranate image completion and segmentation. CSD-Net aims to address the structural fidelity limitations of traditional completion methods. It utilizes a shared encoder, a segmentation branch, and an RGB diffusion branch. Crucially, the network leverages the segmentation mask as a key structural prior condition to guide the diffusion generation process. This innovative conditional guidance mechanism ensures high-fidelity reconstruction of fruit structures while maintaining spatial and textural consistency. Experimental results demonstrate that CSD-Net substantially outperforms conventional methods across metrics, achieving 30.37 dB in PSNR and 0.9490 in SSIM. Furthermore, its model size is only 117 MB, striking an effective balance between high completion quality and inference efficiency. This study offers a novel and highly effective solution for mitigating occlusion issues in agricultural visual perception tasks. Upon acceptance of this paper, the source code will be made publicly available at  https://github.com/zdkd/PCSN .",
    "keywords": [
      "Pomegranate image completion",
      "Conditional diffusion model",
      "Image segmentation",
      "Multi-scale conditional fusion",
      "Agricultural computer vision"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Plant Methods</journal-id><journal-id journal-id-type=\"iso-abbrev\">Plant Methods</journal-id><journal-id journal-id-type=\"pmc-domain-id\">354</journal-id><journal-id journal-id-type=\"pmc-domain\">plantmeth</journal-id><journal-title-group><journal-title>Plant Methods</journal-title></journal-title-group><issn pub-type=\"epub\">1746-4811</issn><publisher><publisher-name>BMC</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12659505</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12659505.1</article-id><article-id pub-id-type=\"pmcaid\">12659505</article-id><article-id pub-id-type=\"pmcaiid\">12659505</article-id><article-id pub-id-type=\"pmid\">41299589</article-id><article-id pub-id-type=\"doi\">10.1186/s13007-025-01476-4</article-id><article-id pub-id-type=\"publisher-id\">1476</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Research</subject></subj-group></article-categories><title-group><article-title>A conditional segmentation-guided network for pomegranate image completion under occlusion</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"D\">Duokuo</given-names></name><address><email>zhangduokuo@stu.hist.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Hou</surname><given-names initials=\"R\">Ruizhe</given-names></name><xref ref-type=\"aff\" rid=\"Aff5\">5</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names initials=\"J\">Jingjing</given-names></name><xref ref-type=\"aff\" rid=\"Aff4\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names initials=\"M\">Mingfu</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"Q\">Qi</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Luo</surname><given-names initials=\"Z\">Zhen</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"K\">Kun</given-names></name><address><email>xk.xukun@163.com</email></address><xref ref-type=\"aff\" rid=\"Aff2\">2</xref><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/0578f1k82</institution-id><institution-id institution-id-type=\"GRID\">grid.503006.0</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1761 7808</institution-id><institution>School of Information Engineering, </institution><institution>Henan Institute of Science and Technology, </institution></institution-wrap>Hongqi, Xinxiang, 453003 Henan China </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00jjkh886</institution-id><institution-id institution-id-type=\"GRID\">grid.460173.7</institution-id><institution-id institution-id-type=\"ISNI\">0000 0000 9940 7302</institution-id><institution>School of artificial Intelligence, </institution><institution>Zhoukou Normal University, </institution></institution-wrap>Chuanhui, Zkoukou, 466001 Henan China </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/00jjkh886</institution-id><institution-id institution-id-type=\"GRID\">grid.460173.7</institution-id><institution-id institution-id-type=\"ISNI\">0000 0000 9940 7302</institution-id><institution>Henan International Joint Laboratory of Smart Agriculture Information Processing, </institution><institution>Zhoukou Normal University, </institution></institution-wrap>Chuanhui, Zkoukou, 466001 Henan China </aff><aff id=\"Aff4\"><label>4</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/01hg31662</institution-id><institution-id institution-id-type=\"GRID\">grid.411618.b</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 2214 9197</institution-id><institution>Beijing Key Laboratory of Information Service Engineering, </institution><institution>Beijing Union University, </institution></institution-wrap>Chaoyang, Beijing, 100101 China </aff><aff id=\"Aff5\"><label>5</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/0530pts50</institution-id><institution-id institution-id-type=\"GRID\">grid.79703.3a</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1764 3838</institution-id><institution>School of Automation Science and Engineering, </institution><institution>South China University of Technology, </institution></institution-wrap>Tianhe, Guangzhou, 510641 Guangdong China </aff></contrib-group><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>21</volume><issue-id pub-id-type=\"pmc-issue-id\">478388</issue-id><elocation-id>153</elocation-id><history><date date-type=\"received\"><day>14</day><month>8</month><year>2025</year></date><date date-type=\"accepted\"><day>18</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 21:25:12.083\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"13007_2025_Article_1476.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">In agricultural images acquired under natural conditions, pomegranate fruits are often partially occluded by leaves and branches, resulting in missing structural information that compromises the accuracy of yield estimation and automated harvesting. To overcome the challenges of recovering structural integrity in occluded agricultural imagery, we propose the Conditional Segmentation-guided Diffusion Network (CSD-Net). CSD-Net is a lightweight, unified framework, representing the first conditional diffusion model specifically designed for the joint tasks of pomegranate image completion and segmentation. CSD-Net aims to address the structural fidelity limitations of traditional completion methods. It utilizes a shared encoder, a segmentation branch, and an RGB diffusion branch. Crucially, the network leverages the segmentation mask as a key structural prior condition to guide the diffusion generation process. This innovative conditional guidance mechanism ensures high-fidelity reconstruction of fruit structures while maintaining spatial and textural consistency. Experimental results demonstrate that CSD-Net substantially outperforms conventional methods across metrics, achieving 30.37 dB in PSNR and 0.9490 in SSIM. Furthermore, its model size is only 117 MB, striking an effective balance between high completion quality and inference efficiency. This study offers a novel and highly effective solution for mitigating occlusion issues in agricultural visual perception tasks. Upon acceptance of this paper, the source code will be made publicly available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://github.com/zdkd/PCSN\">https://github.com/zdkd/PCSN</ext-link>.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Pomegranate image completion</kwd><kwd>Conditional diffusion model</kwd><kwd>Image segmentation</kwd><kwd>Multi-scale conditional fusion</kwd><kwd>Agricultural computer vision</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type=\"FundRef\">https://doi.org/10.13039/501100009101</institution-id><institution>Education Department of Henan Province</institution></institution-wrap></funding-source><award-id>252102210135</award-id><award-id>252102210231</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; BioMed Central Ltd., part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">Pomegranate (<italic toggle=\"yes\">Punica granatum L.</italic>) is a diploid fruit crop [<xref ref-type=\"bibr\" rid=\"CR1\">1</xref>] widely cultivated in tropical and subtropical regions around the world [<xref ref-type=\"bibr\" rid=\"CR2\">2</xref>]. As an economically important species with edible, medicinal, and ornamental value [<xref ref-type=\"bibr\" rid=\"CR3\">3</xref>], pomegranate has been extensively promoted due to its strong adaptability and ease of cultivation and management. Its fruits are rich in polyphenolic antioxidant compounds and exhibit notable anti-inflammatory, antiproliferative, and antitumor activities, showing great potential in the prevention and adjuvant treatment of various cancers [<xref ref-type=\"bibr\" rid=\"CR4\">4</xref>]. With the continuous growth of the global population and increasing demand for high-quality agricultural products, the precise production and intelligent management of pomegranate have become critical topics in modern agriculture.</p><p id=\"Par3\">In this context, computer vision techniques have been widely applied in tasks such as fruit harvesting [<xref ref-type=\"bibr\" rid=\"CR5\">5</xref>], yield estimation [<xref ref-type=\"bibr\" rid=\"CR6\">6</xref>], and growth monitoring. For instance, [<xref ref-type=\"bibr\" rid=\"CR7\">7</xref>] designed a deep learning&#8211;based visual recognition system for fruit-picking robots; [<xref ref-type=\"bibr\" rid=\"CR8\">8</xref>] proposed an early yield prediction approach integrating artificial neural networks (ANN), image analysis, and canopy features; [<xref ref-type=\"bibr\" rid=\"CR9\">9</xref>] introduced a high-resolution object detection model, HR-YOLOv8, based on a self-attention mechanism to enhance small-object detection performance under complex conditions.</p><p id=\"Par4\">However, in real-world agricultural environments, pomegranate images are often affected by leaf and branch occlusions, fruit overlap, rain and fog interference, and uneven illumination, which severely compromise the reliability of vision systems. To address these challenges, researchers have explored multiple directions. On one hand, efforts have focused on improving feature extraction networks to enhance model robustness. For example, [<xref ref-type=\"bibr\" rid=\"CR10\">10</xref>] combined YOLOv5 with UNet-MobileNetV2 to improve pomegranate segmentation accuracy, while [<xref ref-type=\"bibr\" rid=\"CR11\">11</xref>] employed a multi-stage transfer learning strategy to fine-tune pretrained models using fruit images collected under controlled conditions to better adapt to complex field scenarios. On the other hand, some studies have targeted image degradation problems. [<xref ref-type=\"bibr\" rid=\"CR12\">12</xref>] proposed a generative adversarial network (GAN) with hybrid attention for remote sensing image dehazing, and [<xref ref-type=\"bibr\" rid=\"CR13\">13</xref>] utilized a cycle-consistent GAN for unpaired dehazing of agricultural phenotyping images.</p><p id=\"Par5\">Although these methods have achieved noticeable progress in improving image quality or segmentation accuracy, most of them focus solely on detection or segmentation tasks and fail to directly address the structural information loss of pomegranate fruits caused by occlusion. In practice, structural loss can significantly impair downstream applications: in fruit counting, occlusion often leads to missed or double detections [<xref ref-type=\"bibr\" rid=\"CR14\">14</xref>], in robotic harvesting, incomplete contours may result in motion planning failure [<xref ref-type=\"bibr\" rid=\"CR15\">15</xref>], and in yield estimation, occlusions hinder accurate measurement of fruit volume. Therefore, restoring structurally complete, visually realistic, and seamlessly integrated pomegranate foregrounds from partially occluded images has become a key challenge for enhancing the robustness and automation of agricultural vision systems.</p><p id=\"Par6\">Currently, mainstream paradigms for fruit image completion typically adopt a multi-stage &#8220;segmentation&#8211;completion&#8211;fusion&#8221; pipeline. For example, [<xref ref-type=\"bibr\" rid=\"CR16\">16</xref>]. proposed a high-precision reconstruction method for occluded fruits following this workflow. However, such cascaded architectures are often cumbersome and prone to cumulative errors, which significantly degrade overall system efficiency and real-time performance.</p><p id=\"Par7\">Inspired by the joint modeling paradigm proposed by [<xref ref-type=\"bibr\" rid=\"CR17\">17</xref>], this study introduces an end-to-end Pomegranate Completion and Segmentation Network based on a conditional diffusion model. The proposed framework abandons traditional multi-stage pipelines and unifies instance segmentation and guided diffusion completion within a single network. Specifically, the model first performs accurate segmentation of visible fruit regions from occluded images. Then, the dynamically generated segmentation mask is used as a structural prior to conditionally guide the diffusion process, enabling high-quality reconstruction of occluded parts while preserving the original background. The final results are structurally consistent, texturally realistic, and exhibit natural boundaries, thereby achieving seamless pomegranate foreground completion. The main contributions of this study are as follows: <list list-type=\"order\"><list-item><p id=\"Par8\">This study introduce the first conditional diffusion-based framework for pomegranate image completion under occlusion, featuring a multi-branch collaborative design to improve structural consistency and detail restoration.</p></list-item><list-item><p id=\"Par9\">We design a synergistic architecture combining segmentation and completion, using structural masks to guide the generation process and enhance geometric fidelity.</p></list-item><list-item><p id=\"Par10\">We integrate multi-scale conditional fusion with adaptive diffusion strategies, achieving enabling a unified model that jointly learns structure, semantics, and high-fidelity completion.</p></list-item></list>The structure of this paper is as follows:Sect.&#160;''<xref rid=\"Sec2\" ref-type=\"sec\">Related work</xref>'' reviews related work on image segmentation and image completion technologies. Section&#160;''<xref rid=\"Sec5\" ref-type=\"sec\">MaterialsandMethods</xref>'' introduces the materials, data preprocessing, and the proposed Conditional Segmentation-guided Diffusion Network (CSD-Net) architecture in detail. Section&#160;''<xref rid=\"Sec18\" ref-type=\"sec\">Results</xref>'' presents experimental results and comparisons with state-of-the-art methods, followed by an in-depth discussion in Sect.&#160;''<xref rid=\"Sec21\" ref-type=\"sec\">Discussion</xref>''. Finally, Sect.&#160;''<xref rid=\"Sec22\" ref-type=\"sec\">Conclusions</xref>'' concludes the paper.</p></sec><sec id=\"Sec2\"><title>Related work</title><sec id=\"Sec3\"><title>Image segmentation</title><p id=\"Par11\">In the developmental trajectory of segmentation methodologies, early approaches primarily relied on conventional image processing techniques [<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>], such as k-means clustering [<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>] and region growing [<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>]. However, these traditional methods depend heavily on handcrafted features and predefined rules. When confronted with complex image scenes or highly variable targets, their performance tends to degrade significantly.</p><p id=\"Par12\">With the advent and rapid progress of machine learning technologies, hybrid approaches combining machine learning with image processing have been gradually introduced into segmentation tasks. Representative examples include support vector machines (SVM) [<xref ref-type=\"bibr\" rid=\"CR21\">21</xref>] and early neural network models. These methods learn image features and segmentation rules through model training, achieving certain improvements over traditional approaches, particularly in handling complex visual environments.</p><p id=\"Par13\">Nevertheless, the true revolution in image segmentation has been driven by the emergence of deep learning. In particular, the introduction and advancement of convolutional neural networks (CNNs) [<xref ref-type=\"bibr\" rid=\"CR22\">22</xref>] have led to a significant leap forward in segmentation techniques. Deep learning&#8211;based methods automatically acquire image feature representations and segmentation rules through integrated learning processes, eliminating the need for manual feature engineering while demonstrating enhanced generalization ability and adaptability.</p><p id=\"Par14\">In contemporary computer vision research, architectures such as Fully Convolutional Networks (FCN) [<xref ref-type=\"bibr\" rid=\"CR23\">23</xref>], U-Net [<xref ref-type=\"bibr\" rid=\"CR24\">24</xref>], and Mask R-CNN [<xref ref-type=\"bibr\" rid=\"CR25\">25</xref>] have become the dominant frameworks for segmentation tasks. These architectures consistently deliver outstanding performance across diverse datasets and challenging segmentation scenarios.</p><p id=\"Par15\">Building upon these advancements, the present study does not treat segmentation as an independent preprocessing step. Instead, segmentation is deeply integrated into the image restoration process, forming a joint learning framework that achieves collaborative optimization of segmentation and completion tasks.</p></sec><sec id=\"Sec4\"><title>Image generation and completion technology</title><p id=\"Par16\">Image inpainting, also known as image restoration, aims to reconstruct missing or occluded regions of an image in a visually plausible manner. Traditional methods typically search for patches in the intact regions of an image that closely match the missing areas and use them for completion. Representative examples include the PatchMatch algorithm proposed by [<xref ref-type=\"bibr\" rid=\"CR26\">26</xref>], the Markov Random Field (MRF) approach by [<xref ref-type=\"bibr\" rid=\"CR27\">27</xref>], and the exemplar-based texture synthesis method introduced by [<xref ref-type=\"bibr\" rid=\"CR28\">28</xref>]. While these methods perform well on images with repetitive textures, they often fail when dealing with non-repetitive or semantically complex structures.</p><p id=\"Par17\">Since the emergence of convolutional neural networks, deep learning has progressively become the dominant paradigm in image restoration research [<xref ref-type=\"bibr\" rid=\"CR29\">29</xref>, <xref ref-type=\"bibr\" rid=\"CR30\">30</xref>] introduced the Variational Autoencoder (VAE), which combines the principles of autoencoders and probabilistic graphical models by mapping input images into a latent probabilistic space and decoding new samples from it. This probabilistic framework enables the generation of diverse image reconstructions while effectively capturing underlying structural and semantic features. Subsequently, [<xref ref-type=\"bibr\" rid=\"CR31\">31</xref>] proposed the Generative Adversarial Network (GAN), in which a generator and discriminator are trained in an adversarial manner. GANs are capable of rapidly synthesizing realistic images, making them a popular choice for image inpainting tasks.</p><p id=\"Par18\">In recent years, diffusion models have emerged as a breakthrough in the field of image restoration. Their progressive denoising process allows images to be reconstructed step by step, while the incorporation of temporal embeddings and conditional controls enables fine-grained regulation of the generative process. This leads to outstanding detail fidelity and high visual quality. Consequently, diffusion-based inpainting-where missing regions are filled according to a static occlusion mask and then seamlessly fused with the original image-has become the mainstream paradigm.</p><p id=\"Par19\">Unlike existing image inpainting approaches, the model proposed in this study adopts an end-to-end joint learning framework that dynamically predicts visible region masks of pomegranates and incorporates them as structural priors into the conditional diffusion generation process in a multi-scale manner. This enables high-fidelity completion with structurally consistent, texturally natural, and background-coherent results, while preserving the integrity of the original image background.</p></sec></sec><sec id=\"Sec5\"><title>Materials and methods</title><sec id=\"Sec6\"><title>Data sources and acquisition</title><p id=\"Par20\">The pomegranate image dataset used in this study consists of two parts: field-captured images and publicly available online data. The publicly available portion primarily includes the dataset of pomegranates by Kumar et al. [<xref ref-type=\"bibr\" rid=\"CR32\">32</xref>, <xref ref-type=\"bibr\" rid=\"CR33\">33</xref>] (1080 images) and the pomegranate disease dataset by Pakruddin et al. [<xref ref-type=\"bibr\" rid=\"CR34\">34</xref>] (1440 images). The remaining 692 images were collected by our research team through on-site photography in the orchards of Henan Institute of Science and Technology, China, during the period from May to September each year. The field acquisition comprehensively covered five key phenological stages of pomegranate growth: bud stage, flowering stage, fruit set, mid-growth stage, and maturity stage. For the purpose of this experiment, we primarily selected images from the mid-growth and maturity stages to ensure the dataset contained morphologically complete fruits.</p><p id=\"Par21\">All field-captured images were taken under natural lighting conditions between 9:00 AM and 5:00 PM, covering diverse weather scenarios such as sunny and cloudy days, thereby enhancing the real-world variability of the dataset. The original images were manually captured using the rear camera of an iPhone XR in HEIC format with a resolution of 3024 <inline-formula id=\"IEq1\"><tex-math id=\"d33e437\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 4032 pixels. Subsequently, all images were uniformly converted to JPG format at a resolution of 640 <inline-formula id=\"IEq2\"><tex-math id=\"d33e441\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 480 pixels to ensure data consistency and computational efficiency. It is worth noting that all collected images clearly display intact pomegranates, providing a solid foundation for generating simulated occlusion images in subsequent experiments.</p></sec><sec id=\"Sec7\"><title>Data preprocessing</title><p id=\"Par22\">To construct a training set with diverse occlusion patterns, we implemented a carefully designed data preprocessing pipeline. First, 300 individual samples of leaves and branches were manually cropped using Meitu Xiuxiu (version 7.7.1.6). These samples were then augmented through rotation, scaling, and other transformations, resulting in a leaf sample library containing 600 images.</p><p id=\"Par23\">Subsequently, our team&#8217;s self-developed deep neural network model, YOLO-Granada [<xref ref-type=\"bibr\" rid=\"CR35\">35</xref>], was employed to detect the positions of pomegranates in the original images. Based on the detection results, images from the leaf sample library were randomly pasted over the detected pomegranate regions to simulate occlusion scenarios. To ensure dataset quality, we conducted meticulous manual inspections and corrected any images where occlusion synthesis failed due to inaccurate pomegranate localization.</p><p id=\"Par24\">Finally, all generated images were manually annotated using the LabelMe tool to produce the corresponding masks of unobstructed pomegranate regions. Through this series of steps, we constructed a comprehensive dataset containing 3212 images. The detailed distribution of the dataset is shown in Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref>, and the overall data preprocessing workflow is illustrated in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>.The final processed image is shown in the Figure <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Data preprocessing workflow. The original images are processed to generate occluded images and corresponding masks</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig1_HTML.jpg\"/></fig><fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>Visualization images of the dataset</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig2_HTML.jpg\"/></fig></p><p id=\"Par25\">\n<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Number of images in the dataset</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Number/type of images</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">original image</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Leaf Sample Bank</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Training</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Validation</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">test</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Number of images</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3212</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">600</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2570</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">321</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">321</td></tr></tbody></table></table-wrap>\n</p></sec><sec id=\"Sec8\"><title>Overview of model architecture</title><p id=\"Par26\">In this study, we propose an end-to-end unified framework for pomegranate image completion and segmentation. The overall workflow is illustrated in Fig&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>. First, the partially occluded pomegranate image is fed into a shared encoder to extract multi-scale features. These features are then processed by the segmentation branch to predict a visible-region mask of the pomegranate. Guided by the segmentation results, the RGB diffusion branch employs a conditional diffusion model to generate a complete pomegranate image, thereby filling in the occluded regions. To further enhance the seamless integration between the completed regions and the visible areas, the framework incorporates two key innovations: a multi-scale conditional fusion module and an adaptive timestep embedding module. The segmentation mask is employed as crucial structural prior information and introduced into the denoising process of the diffusion model in a conditional form. It provides precise boundary and shape information for the image. The multi-scale feature fusion module receives features from different levels of a shared encoder, ensuring that during the diffusion process, the network not only focuses on the global structure but also leverages low-level detailed features to finely reconstruct the surface texture, illumination, and color gradients of the fruit. The segmentation mask ensures structural integrity, while multi-scale fusion guarantees texture fidelity. The final outputs consist of the completed pomegranate image and the corresponding visible-region mask. All modules operate in a coordinated manner to achieve high-quality completion and segmentation of occluded pomegranates. The following sections detail the structure and implementation of each module.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Overview of the proposed CSD-Net framework <bold>a</bold> The overall structure of the framework is presented, including a shared encoder, a split branch, an RGB diffusion branch, a multi-scale conditional fusion module, and an adaptive time-step embedding module. The segmentation branch predicts a visible region mask, which guides the RGB diffusion branch to generate a complete pomegranate image. <bold>b</bold> shows the basic architecture of the diffusion model, and <bold>c</bold> shows the image of our network denoising process</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig3_HTML.jpg\"/></fig></p></sec><sec id=\"Sec9\"><title>Diffusion models</title><p id=\"Par27\">In this study, we adopt the Denoising Diffusion Probabilistic Model (DDPM) [<xref ref-type=\"bibr\" rid=\"CR36\">36</xref>] as the core generative mechanism for completing missing regions in occluded images. DDPM is a generative framework based on a Markov chain, which simulates a forward diffusion process that gradually adds Gaussian noise to an image, and learns a reverse denoising process to reconstruct the original data. The key idea is to progressively perturb a data sample into a standard normal distribution by adding Gaussian noise, and then iteratively recover a clean image through the learned reverse process.</p><p id=\"Par28\">Formally, the forward diffusion process can be defined as:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e555\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} q(x_t \\mid x_{t-1})&amp;= {\\mathcal {N}} \\big ( x_t; \\sqrt{1-\\beta _t} \\, x_{t-1} \\\\&amp;\\quad \\beta _t {\\textbf{I}} \\big ), \\quad t = 1, \\dots , T \\end{aligned} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq3\"><tex-math id=\"d33e560\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta _t$$\\end{document}</tex-math></inline-formula> is a variance schedule controlling the noise level at each timestep <inline-formula id=\"IEq4\"><tex-math id=\"d33e564\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t$$\\end{document}</tex-math></inline-formula>, and <italic toggle=\"yes\">T</italic> is the total number of diffusion steps. This process gradually transforms the original image <inline-formula id=\"IEq5\"><tex-math id=\"d33e571\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_0$$\\end{document}</tex-math></inline-formula> into an isotropic Gaussian distribution.</p><p id=\"Par29\">The reverse denoising process is parameterized by a neural network as:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e577\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} p_\\theta (x_{t-1} \\mid x_t) = {\\mathcal {N}} \\left( x_{t-1}; \\mu _\\theta (x_t, t), \\Sigma _\\theta (x_t, t) \\right) \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq6\"><tex-math id=\"d33e582\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _\\theta$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq7\"><tex-math id=\"d33e586\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Sigma _\\theta$$\\end{document}</tex-math></inline-formula> are predicted by the network, conditioned on the noisy input <inline-formula id=\"IEq8\"><tex-math id=\"d33e590\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document}</tex-math></inline-formula> and the timestep <inline-formula id=\"IEq9\"><tex-math id=\"d33e594\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t$$\\end{document}</tex-math></inline-formula>.</p><p id=\"Par30\">In our framework, Gaussian noise is added to the complete pomegranate foreground image <inline-formula id=\"IEq10\"><tex-math id=\"d33e600\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_0$$\\end{document}</tex-math></inline-formula> during the forward process to obtain noisy images <inline-formula id=\"IEq11\"><tex-math id=\"d33e604\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x_t$$\\end{document}</tex-math></inline-formula>. During the reverse diffusion process, the noisy image is gradually denoised by using the CSD-net network we designed, and then a complete pomegranate foreground image is generated. This conditional formulation enables progressive reconstruction of the pomegranate foreground, effectively modeling complex structural and textural features, and producing natural, coherent completion results.</p></sec><sec id=\"Sec10\"><title>Shared encoder network</title><p id=\"Par31\">In the proposed image completion and segmentation framework, the shared encoder serves as the foundation of the entire network, responsible for extracting multi-scale, high-level semantic features from the input occluded image. The core design principle is that, regardless of whether the subsequent task is visible-region segmentation or occluded-region completion via diffusion, a deep understanding of the input image is essential. A powerful shared feature extraction module can effectively enhance overall performance and facilitate information synergy between different tasks.</p><p id=\"Par32\">We adopt a U-Net encoder architecture due to its widespread success in agricultural vision tasks (e.g., crop segmentation, disease detection), where preserving spatial resolution through skip connections is critical for fine-grained object recovery. Unlike pure CNN backbones (e.g., ResNet), U-Net explicitly maintains multi-scale feature maps, which are indispensable for both accurate segmentation and context-aware diffusion guidance in occluded pomegranate images.The shared encoder progressively extracting features through a series of downsampling operations. It consists of an initial convolutional layer, multiple downsampling blocks, and a bottleneck layer. The initial convolutional layer applies a <inline-formula id=\"IEq12\"><tex-math id=\"d33e614\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3 \\times 3$$\\end{document}</tex-math></inline-formula> convolution to perform preliminary feature extraction, expanding the channel dimension from 3 to a base value of 64. This stage primarily captures low-level visual features such as edges and textures.</p><p id=\"Par33\">The encoder is designed with three downsampling stages. Each stage comprises two residual blocks (ResBlocks) [<xref ref-type=\"bibr\" rid=\"CR37\">37</xref>] and one downsampling convolutional layer. Skip connections are incorporated to alleviate the vanishing gradient problem in deep networks and enable the learning of more complex feature mappings. The downsampling convolutional layers reduce the spatial resolution of the feature maps by half while doubling the number of channels, thereby allowing the network to capture features with larger receptive fields at deeper levels. Three sets of skip-connection features are obtained through these stages.</p><p id=\"Par34\">After the three downsampling stages, a bottleneck layer is introduced. This layer includes two residual blocks and one self-attention block (AttentionBlock). The self-attention mechanism captures long-range dependencies between any positions in the feature map, which is crucial for understanding the global context of the image, particularly when handling occluded regions.</p><p id=\"Par35\">The shared encoder ultimately outputs two components: the bottleneck features, which are the highest-level and most abstract representations obtained after multiple downsampling and feature extraction steps, serving as shared inputs for both the segmentation and diffusion branches; and a list of skip-connection features from different downsampling stages, which are fed into the subsequent decoders to provide multi-scale information for recovering spatial details. The architecture of the shared encoder is illustrated in Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref>.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Architecture of the shared encoder network. The encoder consists of an initial convolutional layer, three downsampling stages, and a bottleneck layer with self-attention. It outputs multi-scale features for subsequent segmentation and diffusion tasks</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig4_HTML.jpg\"/></fig></p></sec><sec id=\"Sec11\"><title>Segmentation branch</title><p id=\"Par36\">The segmentation branch constitutes the second core component of the proposed framework, aiming to accurately predict the visible pomegranate regions from the features extracted by the shared encoder. The generated visible mask is subsequently used to guide the conditional diffusion process. Architecturally, this branch adopts a U-Net-style decoder, This architecture can fully utilize the hierarchical characteristics of the shared encoder. This symmetrical design ensures pixel-level alignment between the encoder and decoder features, which is crucial for generating precise visible region masks - and this is a prerequisite for reliable diffusion conditions.U-Net decoder progressively restoring spatial resolution through a sequence of upsampling operations and skip connections, thereby producing high-resolution segmentation masks.</p><p id=\"Par37\">Let the bottleneck feature map from the shared encoder be F3 &#8712; R<sup>B&#215;8C0&#215;H/16&#215;W/16</sup> and the three skip connection features be F2 &#8712; R<sup>B&#215;4C0&#215;H/8&#215;W/8</sup>, F1 &#8712;R<sup>B&#215;2C0&#215;H/4&#215;W/4</sup>,F0 &#8712;R<sup>B&#215;C0&#215;H/2&#215;W/2</sup>, where <inline-formula id=\"IEq17\"><tex-math id=\"d33e651\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$B$$\\end{document}</tex-math></inline-formula> denotes the batch size, <inline-formula id=\"IEq18\"><tex-math id=\"d33e656\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_0$$\\end{document}</tex-math></inline-formula> the base channel number, and <inline-formula id=\"IEq19\"><tex-math id=\"d33e660\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H/W$$\\end{document}</tex-math></inline-formula> the spatial resolution of the input image.</p><p id=\"Par38\">The decoding process begins by spatially upsampling <inline-formula id=\"IEq20\"><tex-math id=\"d33e666\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_3$$\\end{document}</tex-math></inline-formula> using a transposed convolution <inline-formula id=\"IEq21\"><tex-math id=\"d33e670\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$D_{2\\times }(\\cdot ; \\theta )$$\\end{document}</tex-math></inline-formula> to double its resolution:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e674\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} U_3&amp;= D_{2\\times }(F_3; \\theta _3) = W_3 * \\textrm{Pad}(F_3) \\\\ \\quad&amp;U_3 \\in {\\mathbb {R}}^{B \\times 4C_0 \\times H/8 \\times W/8} \\end{aligned} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq22\"><tex-math id=\"d33e679\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_3$$\\end{document}</tex-math></inline-formula> is the transposed convolution kernel, <inline-formula id=\"IEq23\"><tex-math id=\"d33e683\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document}</tex-math></inline-formula> denotes the convolution operation, and <inline-formula id=\"IEq24\"><tex-math id=\"d33e688\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{Pad}(\\cdot )$$\\end{document}</tex-math></inline-formula> represents zero-padding to ensure dimensional consistency. The upsampled feature <inline-formula id=\"IEq25\"><tex-math id=\"d33e692\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$U_3$$\\end{document}</tex-math></inline-formula> is concatenated with the mid-level feature <inline-formula id=\"IEq26\"><tex-math id=\"d33e696\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_2$$\\end{document}</tex-math></inline-formula> along the channel dimension:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e700\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} X_3 = \\textrm{Concat}(U_3, F_2) \\in {\\mathbb {R}}^{B \\times 8C_0 \\times H/8 \\times W/8} \\end{aligned}$$\\end{document}</tex-math></disp-formula>and refined by two consecutive residual blocks<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e705\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} H_3 = \\textrm{ResBlock}(\\textrm{ResBlock}(X_3)) \\end{aligned}$$\\end{document}</tex-math></disp-formula>The same &#8220;upsample&#8211;concatenate&#8211;residual refinement&#8221; operation is repeated for <inline-formula id=\"IEq27\"><tex-math id=\"d33e710\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_3$$\\end{document}</tex-math></inline-formula> with <inline-formula id=\"IEq28\"><tex-math id=\"d33e715\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_1$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq29\"><tex-math id=\"d33e719\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_0$$\\end{document}</tex-math></inline-formula>, producing the full-resolution feature map <inline-formula id=\"IEq30\"><tex-math id=\"d33e723\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$H_0$$\\end{document}</tex-math></inline-formula>. Finally, a <inline-formula id=\"IEq31\"><tex-math id=\"d33e727\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1\\times 1$$\\end{document}</tex-math></inline-formula> convolution <inline-formula id=\"IEq32\"><tex-math id=\"d33e731\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_{1\\times 1}(\\cdot ; \\omega )$$\\end{document}</tex-math></inline-formula> reduces the channels to one, followed by a Sigmoid activation to generate the probability mask of the visible regions:<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e735\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M = \\sigma (C_{1\\times 1}(H_0; \\omega )), \\quad M \\in [0,1], \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq33\"><tex-math id=\"d33e741\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma (x) = 1 / (1 + e^{-x})$$\\end{document}</tex-math></inline-formula>.This hierarchical feature fusion combined with residual learning effectively mitigates gradient vanishing while preserving both spatial details and global semantic information, thereby enhancing the accuracy and robustness of visible region prediction.</p></sec><sec id=\"Sec12\"><title>Diffusion branch</title><p id=\"Par39\">The diffusion branch serves as the core generative module of the proposed framework, aiming to perform high-fidelity inpainting of occluded regions in pomegranate images and to produce complete, visually coherent foreground reconstructions. This branch innovatively integrates a multi-scale conditional fusion mechanism with an adaptive timestep embedding mechanism, significantly improving structural consistency and perceptual realism in the generated results.</p><sec id=\"Sec13\"><title>Multi-scale conditional fusion module</title><p id=\"Par40\">The diffusion process relies on various conditional inputs, including the initial noisy image, the visible region segmentation mask, and multi-scale features extracted by the shared encoder. To exploit their complementary properties, we design a fusion-based decoder architecture that explicitly interacts with these conditions at multiple scales.</p><p id=\"Par41\">Given the original-resolution visible mask<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e755\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} M_{vis} \\in {\\mathbb {R}}^{1 \\times H \\times W} \\end{aligned}$$\\end{document}</tex-math></disp-formula>it is resized via bilinear interpolation to match the spatial dimensions of the feature maps at each downsampling stage:<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e760\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} M_{vis}^{(s)}&amp;= Interpolate(M_{vis}, \\\\&amp;size =F_{feat}^{(s)}.shape) \\end{aligned} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq34\"><tex-math id=\"d33e765\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{\\textrm{feat}}^{(s)}$$\\end{document}</tex-math></inline-formula> denotes the feature map at scale <inline-formula id=\"IEq35\"><tex-math id=\"d33e769\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$s$$\\end{document}</tex-math></inline-formula> (e.g., <inline-formula id=\"IEq36\"><tex-math id=\"d33e773\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$8\\times$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq37\"><tex-math id=\"d33e778\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$4\\times$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq38\"><tex-math id=\"d33e782\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$2\\times$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq39\"><tex-math id=\"d33e786\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1\\times$$\\end{document}</tex-math></inline-formula> scales). At each scale, the resized mask is concatenated with the corresponding feature map along the channel dimension:<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e790\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F_{concat}^{(s)} = \\textrm{Concat}(F_{\\textrm{feat}}^{(s)}, M_{vis}^{(s)}) \\end{aligned}$$\\end{document}</tex-math></disp-formula>and then processed by a convolution layer, group normalization, and a non-linear activation to produce condition-aware features:<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e795\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} F ^{(s)} = \\delta (GN(W^{(s)}_{cond}*F_{concat}^{(s)})) \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq40\"><tex-math id=\"d33e800\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$W_{\\textrm{cond}}^{(s)}$$\\end{document}</tex-math></inline-formula> denotes the convolution kernel, <inline-formula id=\"IEq41\"><tex-math id=\"d33e805\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$*$$\\end{document}</tex-math></inline-formula> indicates convolution, and <inline-formula id=\"IEq42\"><tex-math id=\"d33e809\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\delta (\\cdot )$$\\end{document}</tex-math></inline-formula> is the activation function. Independent fusion paths are defined for each scale, balancing global structural consistency and local detail preservation.</p></sec><sec id=\"Sec14\"><title>Adaptive timestep embedding module</title><p id=\"Par42\">To enable the model to explicitly perceive the current diffusion time step <inline-formula id=\"IEq43\"><tex-math id=\"d33e817\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t$$\\end{document}</tex-math></inline-formula>, we adopted a position encoding-based temporal embedding module that maps the scalar time step <inline-formula id=\"IEq44\"><tex-math id=\"d33e821\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t$$\\end{document}</tex-math></inline-formula> into a high-dimensional vector and performs feature enhancement through a Multi-Layer Perceptron (MLP). First, the integer t is encoded into a set of periodic feature vectors.<disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e825\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\gamma (t) = [\\sin (\\frac{t}{10000^{2i/d}}),\\cos (\\frac{t}{10000^{2i/d}})]_{i=1}^{d/2} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq45\"><tex-math id=\"d33e830\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$d$$\\end{document}</tex-math></inline-formula> is the embedding dimension</p><p id=\"Par43\">Then, two fully connected layers are used to uplift the temporal embedding to a dimension that matches the image features.<disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e836\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} e_t&amp;= MLP(\\gamma (t)) \\\\&amp;= FC_2(GeLU(FC_1(\\gamma (t)))) \\end{aligned} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq46\"><tex-math id=\"d33e841\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FC_1$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq47\"><tex-math id=\"d33e845\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$FC_2$$\\end{document}</tex-math></inline-formula> are fully connected layers</p><p id=\"Par44\">The time-embedding <inline-formula id=\"IEq48\"><tex-math id=\"d33e851\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$e_t$$\\end{document}</tex-math></inline-formula> with subscript is injected into multiple ResBlock in the network, affecting feature propagation through channel-by-channel addition, enabling the network to have different prediction capabilities at different time steps.</p></sec></sec><sec id=\"Sec15\"><title> Experimental configuration</title><p id=\"Par45\">All experiments in this study were conducted in a high-performance computing environment. The operating system platform was Ubuntu 20.04.6 LTS (64-bit), and computational tasks were supported by a powerful 13th Gen Intel<inline-formula id=\"IEq49\"><tex-math id=\"d33e859\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\circledR$$\\end{document}</tex-math></inline-formula>\n<inline-formula id=\"IEq50\"><tex-math id=\"d33e863\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text {Core}^{\\texttt {TM}}$$\\end{document}</tex-math></inline-formula> i9&#8211;13900K processor with 32 logical cores. To accelerate the training and inference of deep learning models, the system was equipped with two NVIDIA GeForce RTX 4090 GPUs, running driver version 535.161.07 with CUDA 12.2 support. The machine was provisioned with 128 GB of RAM to ensure sufficient memory for large-scale experiments.</p><p id=\"Par46\">On the software side, all implementations were developed and executed using the PyTorch 2.3.1 deep learning framework, in conjunction with TorchVision 0.18.1 and TorchAudio 2.3.1, along with other relevant libraries. This configuration ensured both computational efficiency and reproducibility of the experimental results.</p></sec><sec id=\"Sec16\"><title>Training and validation</title><p id=\"Par47\">The training process of the proposed model followed a rigorously structured data organization and processing protocol. The complete image dataset was logically split into training, validation, and testing sets at a ratio of 8:1:1. To standardize the input and optimize computational efficiency, all images and corresponding masks were resized to <inline-formula id=\"IEq51\"><tex-math id=\"d33e873\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$256 \\times 256$$\\end{document}</tex-math></inline-formula> pixels before being fed into the network. For pixel value processing, RGB images were normalized to the range <inline-formula id=\"IEq52\"><tex-math id=\"d33e877\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$[-1, 1]$$\\end{document}</tex-math></inline-formula>, while masks were binarized to meet the network&#8217;s input requirements.</p><p id=\"Par48\">The batch size during training was set to 8, and the AdamW optimizer was adopted due to its robust performance, with an initial learning rate of <inline-formula id=\"IEq53\"><tex-math id=\"d33e883\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1 \\times 10^{-4}$$\\end{document}</tex-math></inline-formula>. The maximum number of training epochs was set to 300. During the training of the diffusion model, a random timestep ttt was sampled, and Gaussian noise was added to the complete image to simulate the diffusion process.</p><p id=\"Par49\">To fully utilize the available hardware resources, the model was trained in a multi-GPU parallel setting using torch.nn.DataParallel. At the end of each epoch, model weights were saved, and an additional &#8220;best model&#8221; checkpoint was recorded whenever the validation loss reached its minimum. For training monitoring and intuitive visualization of results, after each epoch, random samples from the test set were selected for completion visualization, and composite images containing multiple intermediate outputs were saved.</p><p id=\"Par50\">Throughout the training and validation phases, we continuously computed and recorded key evaluation metrics - PSNR for image reconstruction quality-to monitor model performance in real time.</p></sec><sec id=\"Sec17\"><title>Evaluation metrics</title><p id=\"Par51\">To comprehensively evaluate the performance of the proposed pomegranate completion and segmentation network, three objective evaluation metrics were employed: Structural Similarity Index (SSIM) [<xref ref-type=\"bibr\" rid=\"CR38\">38</xref>], and Peak Signal-to-Noise Ratio (PSNR) [<xref ref-type=\"bibr\" rid=\"CR39\">39</xref>]. These metrics quantify the model&#8217;s performance from two perspectives: segmentation accuracy and image reconstruction quality.</p><p id=\"Par52\">SSIM is a perceptual metric that measures the similarity between two images from the perspective of human visual perception. Unlike pixel-wise error metrics, SSIM considers luminance, contrast, and structural information simultaneously, making it particularly suitable for evaluating the perceptual quality of reconstructed images. The SSIM between a generated image <inline-formula id=\"IEq54\"><tex-math id=\"d33e903\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X$$\\end{document}</tex-math></inline-formula> and a reference image <inline-formula id=\"IEq55\"><tex-math id=\"d33e907\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Y$$\\end{document}</tex-math></inline-formula> is computed as:<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e911\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} \\begin{aligned} SSIM(X,Y) =&amp;\\frac{\\left( 2\\mu _X\\mu _Y+C_1\\right) }{\\left( \\mu _X^2+\\mu _Y^2+C_1\\right) }\\\\&amp;\\cdot \\frac{\\left( 2\\sigma _{XY}+C_2\\right) }{\\left( \\sigma _x^2+\\sigma _Y^2+C_2\\right) } \\end{aligned} \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq56\"><tex-math id=\"d33e916\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _X$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq57\"><tex-math id=\"d33e920\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu _Y$$\\end{document}</tex-math></inline-formula> are the mean intensities of <inline-formula id=\"IEq58\"><tex-math id=\"d33e925\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$X$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq59\"><tex-math id=\"d33e929\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Y$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq60\"><tex-math id=\"d33e933\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _X^2$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq61\"><tex-math id=\"d33e937\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _Y^2$$\\end{document}</tex-math></inline-formula> are their variances, <inline-formula id=\"IEq62\"><tex-math id=\"d33e941\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\sigma _{XY}$$\\end{document}</tex-math></inline-formula> is their covariance, and <inline-formula id=\"IEq63\"><tex-math id=\"d33e945\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_1$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq64\"><tex-math id=\"d33e950\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_2$$\\end{document}</tex-math></inline-formula> are small constants to stabilize the division when denominators are close to zero. SSIM values range from <inline-formula id=\"IEq65\"><tex-math id=\"d33e954\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$-1$$\\end{document}</tex-math></inline-formula> to <inline-formula id=\"IEq66\"><tex-math id=\"d33e958\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$1$$\\end{document}</tex-math></inline-formula>, with higher values indicating greater structural similarity.</p><p id=\"Par53\">PSNR is a standard metric for assessing the quality of reconstructed images, particularly suitable for evaluating tasks such as image inpainting or denoising, where pixel-level fidelity to the original image is critical. PSNR is computed based on the Mean Squared Error (MSE) between the generated image and the original complete image. Higher PSNR values generally indicate smaller pixel-wise differences, implying better reconstruction quality. The computation is as follows:</p><p id=\"Par54\">First, the MSE is calculated:<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e966\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} MSE=\\frac{1}{MN}\\sum _{i=0}^{M-1}\\sum _{j=0}^{N-1}\\left[ I\\left( i,j\\right) -K\\left( i,j\\right) \\right] ^2 \\end{aligned}$$\\end{document}</tex-math></disp-formula>Then, PSNR is derived as:<disp-formula id=\"Equ15\"><label>15</label><tex-math id=\"d33e971\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} PSNR=10\\cdot log_{10}\\left( \\frac{MAX_I^2}{MSE}\\right) \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq67\"><tex-math id=\"d33e976\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$M$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq68\"><tex-math id=\"d33e980\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$N$$\\end{document}</tex-math></inline-formula> denote the image width and height, <inline-formula id=\"IEq69\"><tex-math id=\"d33e984\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I(i,j)$$\\end{document}</tex-math></inline-formula> represents the pixel value at position <inline-formula id=\"IEq70\"><tex-math id=\"d33e989\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$(i,j)$$\\end{document}</tex-math></inline-formula> in the ground truth complete image, <inline-formula id=\"IEq71\"><tex-math id=\"d33e993\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$K(i,j)$$\\end{document}</tex-math></inline-formula> denotes the corresponding pixel value in the model-generated completed image, and <inline-formula id=\"IEq72\"><tex-math id=\"d33e997\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\textrm{MAX}_I$$\\end{document}</tex-math></inline-formula> is the maximum possible pixel value in the image.</p></sec></sec><sec id=\"Sec18\"><title>Results</title><p id=\"Par55\">In this section, we present both quantitative and qualitative results to evaluate the performance of the proposed CSD-Net for pomegranate image completion under occlusion. We first conduct ablation studies to assess the contribution of each key component in our framework. Then, we compare CSD-Net against several state-of-the-art image completion methods, including DDPM, Pix2Pix, and Seg-Guide. The evaluation is based on two widely used image quality metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), which respectively measure pixel-level fidelity and perceptual-structural consistency between the completed images and ground-truth references.</p><sec id=\"Sec19\"><title>Ablation studies</title><p id=\"Par56\">To gain deeper insights into the contribution of each module in the pomegranate occlusion completion task, we designed four ablation experiments. In each experiment, specific modules were progressively removed to evaluate their impact on the overall network performance. Specifically, we assessed the effects of the Segmentation Branch, the Multi-scale Conditional Fusion module, and the Adaptive Time Embedding module. PSNR was adopted as the primary evaluation metric.</p><p id=\"Par57\">Table<xref rid=\"Tab2\" ref-type=\"table\">2</xref> summarizes the configurations of the CSD-Net model with different combinations of improvement strategies, namely the Segmentation Branch(SegB), the Multi-scale Conditional Fusion module(MsCF), and the Adaptive Time Embedding(ATE) module. Each experiment in the table explicitly indicates which enhancement strategies were applied. A simplified diffusion model was used as the baseline, and its results served as a reference for all subsequent experiments. By comparing the outcomes of four independent experimental settings, we can clearly identify the performance contributions of each individual module.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Ablation experiments</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"/><th align=\"left\" colspan=\"1\" rowspan=\"1\">SegB</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">MsCF</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">ATE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSNR</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Weight Size(MB)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IoU</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Experiment1</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">-</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">23.4460</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">55.9</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Experiment2</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq73\"><tex-math id=\"d33e1063\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30.2720</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">99.0</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.9325</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Experiment3</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq74\"><tex-math id=\"d33e1081\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq75\"><tex-math id=\"d33e1086\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8211;</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30.3250</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">115</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.9321</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Experiment4</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq76\"><tex-math id=\"d33e1102\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq77\"><tex-math id=\"d33e1107\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq78\"><tex-math id=\"d33e1112\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\checkmark$$\\end{document}</tex-math></inline-formula></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30.3689</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">117</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.9341</td></tr></tbody></table></table-wrap></p><p id=\"Par58\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Visual comparison of the four ablation experiments</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig5_HTML.jpg\"/></fig>\n</p><p id=\"Par59\">From the baseline results in Experiment 1, we observe that the simplified diffusion model, without incorporating any task-specific enhancements for occlusion completion, achieves a PSNR of 23.4460 dB. As illustrated in Fig.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, this baseline model exhibits clear deficiencies in leaf removal under complex backgrounds and challenging illumination conditions. This indicates that, although the base diffusion model possesses a certain degree of image generation capability, its performance in the pomegranate completion task under complex occlusions still has substantial room for improvement.In Experiment 2, the integration of the Segmentation Branch into the baseline model yields a remarkable improvement, with the PSNR increasing from 23.4460 dB to 30.2720 dB. This substantial gain strongly demonstrates the critical importance of explicitly identifying visible regions in image completion tasks. The Segmentation Branch provides the diffusion model with precise spatial conditioning information, enabling it to effectively differentiate between known and missing regions. Consequently, it avoids unnecessary modifications to already visible areas while more accurately guiding the diffusion process to fill in the occluded parts.Building on Experiment 2, Experiment 3 further incorporates the Multi-scale Conditional Fusion module. Here, the PSNR shows a slight improvement from 30.2720 dB to 30.3250 dB. While the numerical gain is modest, the convergence curves in Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref> clearly reveal that Experiment 3 achieves faster convergence compared to Experiment 2.Finally, in Experiment 4, the addition of the Adaptive Time Embedding module to Experiment 3 yields limited improvement in PSNR. However, Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref> (a) also shows that the convergence curve of Experiment 4 is noticeably smoother than that of Experiment 3. This suggests that these additional modules, while not significantly boosting final PSNR scores, contribute positively to the training efficiency and stability of the model.</p><p id=\"Par60\">In the experiment of adding segmentation branches, we also evaluated the performance of the segmentation branches. Since this study did not make any special optimizations to the segmentation branches but only carried out multi-task collaboration, the IoU values of the added segmentation branches did not differ much.Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref> shows some specific values of our experimental indicators.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Visualization of the results from ablation experiments. <bold>a</bold> PSNR convergence curves for the four ablation experiments. <bold>b</bold> IoU values of the segmentation branch across different experiments. <bold>c</bold> Diffusion branch loss curves for the four ablation experiments. <bold>d</bold> Tatol loss curves for the four ablation experiments</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig6_HTML.jpg\"/></fig></p></sec><sec id=\"Sec20\"><title>Comparative experiments</title><p id=\"Par61\">To further validate the effectiveness of the proposed CSD-Net, we conducted comparative experiments against several representative segmentation-based image completion approaches. All models were trained and evaluated on the same dataset under identical experimental settings to ensure fairness. The comparison included DDPM,Pix2Pix [<xref ref-type=\"bibr\" rid=\"CR40\">40</xref>], Seg-Guide [<xref ref-type=\"bibr\" rid=\"CR41\">41</xref>].<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Comparative test results</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"/><th align=\"left\" colspan=\"1\" rowspan=\"1\">Weight Size(MB)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSNR</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SSIM</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">NIQE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">BRISQUE</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">INFERENCE TIME(ms)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DDPM</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">433</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">17.22</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.6857</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">11.0252</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">32.5852</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">17.584</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Pix2pix</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">207</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22.73</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.7955</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">9.7201</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">14.5019</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3.39</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Seg-Guided</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">433</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">12.11</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.3732</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">11.4948</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29.8655</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">17.426</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ours</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">117</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">30.3689</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.9490</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10.6881</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">25.6386</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">15.866</td></tr></tbody></table></table-wrap></p><p id=\"Par62\">From the results in the table <xref rid=\"Tab3\" ref-type=\"table\">3</xref>, it is evident that CSD-net outperforms all competing methods across both full-reference and no-reference image quality metrics. Specifically, CSD-Net achieves a PSNR of 30.37 dB, exceeding the second-best method, Pix2Pix, by 7.64 dB, and attains an SSIM of 0.9490, representing a substantial improvement in structural similarity. These results demonstrate that CSD-Net excels in both reconstruction fidelity and structural consistency.Moreover, in terms of blind quality assessment, CSD-net yields a BRISQUE score of 25.64, significantly lower than DDPM (32.59) and Seg-Guide (29.87), indicating fewer perceptual distortions; its NIQE score (10.69) is also competitive, reflecting naturalness in texture and contrast that closely resembles real pomegranate images. In terms of model size, CSD-Net is only 117 MB, which is not only significantly smaller than DDPM (433 MB) and Seg-Guide (433 MB) but also smaller than Pix2Pix (207 MB), indicating a higher parameter efficiency in its architectural design.While its inference speed is slower than Pix2Pix, which utilizes a GAN architecture, it outperforms Seg-Guide and DDPM, which also use a diffusion model architecture.As shown in Fig.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref>, although DDPM, as a diffusion-based model, possesses strong generative capabilities, its lack of task-specific conditional guidance limits its ability to accurately restore local structures and textures, and it exhibits noticeable deficiencies in foreground color reproduction. Pix2Pix achieves competitive performance in conditional image generation; however, its ability to recover fine details in heavily occluded regions is limited, often producing artifacts along object boundaries. Seg-Guide performs relatively well in restoring object shapes and contours, successfully reconstructing the overall fruit geometry. Nevertheless, the generated backgrounds often lack consistency with the original input image, leading to spatial misalignments or style discrepancies that significantly degrade overall similarity metrics.By integrating the segmentation branch, multi-scale conditional fusion module, and adaptive time embedding module, CSD-Net not only preserves the structural integrity of the fruit but also generates high-quality texture details. This synergy enables the model to achieve superior completion quality while maintaining a lightweight architecture.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Visual comparison of the proposed CSD-Net with other methods</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"13007_2025_1476_Fig7_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec21\"><title>Discussion</title><p id=\"Par63\">The experimental results presented in Sect.&#160;''<xref rid=\"Sec18\" ref-type=\"sec\">Results</xref>'' consistently demonstrate that the proposed CSD-Net framework achieves state-of-the-art performance in completing occluded pomegranate images, both quantitatively (PSNR = 30.37 dB, SSIM = 0.9490) and qualitatively. This superiority stems not from simply stacking a segmentation network and a diffusion model, but from a task-aware co-design that tightly couples structure perception with content generation. Unlike conventional inpainting approaches that rely on static, pre-defined occlusion masks and adopt a &#8220;complete-then-composite&#8221; pipeline, CSD-Net dynamically predicts the visible-region mask and uses it as an adaptive structural prior to guide the diffusion process at multiple scales. This ensures that known regions remain untouched while missing parts are reconstructed with high geometric fidelity.</p><p id=\"Par64\">Moreover, the lightweight architecture (117 MB) highlights an important design philosophy for agricultural vision systems: high performance need not come at the cost of computational overhead. By sharing a U-Net encoder between the segmentation and diffusion branches and eliminating redundant feature extraction, CSD-Net achieves a favorable trade-off between accuracy and efficiency.</p><p id=\"Par65\">The ablation studies further confirm that the segmentation branch is the most critical component, contributing a PSNR gain of nearly 6.8 dB over the baseline diffusion model. This underscores a key insight: explicit modeling of visible structure is essential for high-fidelity fruit completion in complex natural scenes. The multi-scale conditional fusion and adaptive timestep embedding modules, while offering modest improvements in final metrics, significantly enhance training stability and convergence speed, which is valuable for practical model development.</p><p id=\"Par66\">Nevertheless, the current framework has limitations.Although branching is effective, in the case where the fruits are mutually occluded, this study cannot determine which fruit corresponds to which mask. Therefore, this method is ineffective in this situation.Under extreme occlusion conditions, the results of this study will be unstable (the dataset partially lacks images in this regard).</p><p id=\"Par67\">To address these challenges, future work will focus on three directions: (1) extending PCSN to multi-instance settings by integrating instance-aware segmentation and depth cues to handle fruit-to-fruit occlusion; (2) improving segmentation robustness through self-supervised or weakly supervised learning to reduce reliance on pixel-level annotations; and (3) Design more natural datasets and add images of extreme cases.</p><p id=\"Par68\">In summary, CSD-Net represents a shift from generic image restoration toward structure-preserving, application-specific completion tailored for agricultural perception-a direction that aligns with the growing need for reliable, interpretable, and efficient vision systems in smart farming.</p></sec><sec id=\"Sec22\"><title>Conclusions</title><p id=\"Par69\">In this work, we proposed CSD-Net, a lightweight, end-to-end conditional diffusion framework that jointly learns segmentation and image completion for occluded pomegranates. By dynamically predicting visible-region masks and fusing them into the diffusion process at multiple scales, CSD-Net achieves state-of-the-art performance in both structural fidelity and background consistency. The model&#8217;s compact size (117 MB) and high reconstruction quality make it a promising solution for real-world agricultural vision systems.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors would like to express many thanks to all the anonymous reviewers.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>D.Z.: Conceptualization, Data Curation, Data Analysis, Implementation, Model Development, Formal Analysis, Methodology, Writing-Original Draft, Visualization; R.H.: Data Preprocessing, Writing-Review and Editing, Resources.; J.G. and Z.L.: Investigation, Data Curation.; M.Z., Q.W. and K.X.: Methodology, Project Supervisor, Writing-Review and Editing. All authors read and approved the final manuscript.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>This work was supported in part by Department of Education of Henan Province under Grant 252,102,210,135 and 252102210231.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>This study mainly uses the Pomegranate Fruit Dataset: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/kumararun37/pomegranate-fruit-dataset\">https://www.kaggle.com/datasets/kumararun37/pomegranate-fruit-dataset</ext-link>, Pomegranate Fruit Diseases [Image] the Dataset: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.kaggle.com/datasets/sujaykapadnis/pomegranate-fruit-diseases-dataset\">https://www.kaggle.com/datasets/sujaykapadnis/pomegranate-fruit-diseases-dataset</ext-link> and Pomegranate Images the Dataset: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://data.mendeley.com/datasets/kgwsthf2w6/5\">https://data.mendeley.com/datasets/kgwsthf2w6/5</ext-link></p></notes><notes><title>Declarations</title><notes id=\"FPar1\"><title>Ethics approval and consent to participate</title><p id=\"Par70\">All authors agreed to publish this manuscript.</p></notes><notes id=\"FPar2\"><title>Consent for publication</title><p id=\"Par71\">Consent and approval for publication were obtained from all authors.</p></notes><notes id=\"FPar3\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par72\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Al Jaberi</surname><given-names>SM</given-names></name><name name-style=\"western\"><surname>Patel</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>AL-Masri</surname><given-names>AN</given-names></name></person-group><article-title>Object tracking and detection techniques under GANN threats: a systemic review</article-title><source>Appl Soft Comput.</source><year>2023</year><volume>139</volume><fpage>110224</fpage><pub-id pub-id-type=\"doi\">10.1016/j.asoc.2023.110224</pub-id></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Al Jaberi SM, Patel A, AL-Masri AN. Object tracking and detection techniques under GANN threats: a systemic review. Appl Soft Comput. 2023;139:110224.</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chandra</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Babu</surname><given-names>KD</given-names></name><name name-style=\"western\"><surname>Jadhav</surname><given-names>VT</given-names></name></person-group><article-title>Origin, history and domestication of pomegranate</article-title><source>Global Sci Books</source><year>2010</year><volume>2</volume><fpage>12</fpage></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Chandra R, Babu KD, Jadhav VT. Origin, history and domestication of pomegranate. Global Sci Books. 2010;2:12.</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Teixeira da Silva</surname><given-names>JA</given-names></name><name name-style=\"western\"><surname>Rana</surname><given-names>TS</given-names></name><name name-style=\"western\"><surname>Narzary</surname><given-names>D</given-names></name><etal/></person-group><article-title>Pomegranate biology and biotechnology: a review</article-title><source>Sci Hortic</source><year>2013</year><volume>160</volume><fpage>85</fpage><lpage>107</lpage><pub-id pub-id-type=\"doi\">10.1016/j.scienta.2013.05.017</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Teixeira da Silva JA, Rana TS, Narzary D, et al. Pomegranate biology and biotechnology: a review. Sci Hortic. 2013;160:85&#8211;107. 10.1016/j.scienta.2013.05.017.</mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Viuda-Martos M, Fern&#225;ndez-L&#243;pez J, P&#233;rez-&#193;lvarez JA. Pomegranate and its Many Functional Components as Related to Human Health: A Review. Comprehensive reviews in food science and food safety 2010;9(6). 10.1111/j.1541-4337.2010.00131.x, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://pubmed.ncbi.nlm.nih.gov/33467822/\">https://pubmed.ncbi.nlm.nih.gov/33467822/</ext-link>, publisher: Compr Rev Food Sci Food Saf<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1111/j.1541-4337.2010.00131.x</pub-id><pub-id pub-id-type=\"pmid\">33467822</pub-id></mixed-citation></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hua</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zeng</surname><given-names>J</given-names></name><etal/></person-group><article-title>A review of target recognition technology for fruit picking robots: from digital image processing to deep learning</article-title><source>Appl Sci</source><year>2023</year><volume>13</volume><issue>7</issue><fpage>4160</fpage><pub-id pub-id-type=\"doi\">10.3390/app13074160</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Hua X, Li H, Zeng J, et al. A review of target recognition technology for fruit picking robots: from digital image processing to deep learning. Appl Sci. 2023;13(7):4160. 10.3390/app13074160.</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Lu W, Du R, Niu P, et al. Soybean Yield Preharvest Prediction Based on Bean Pods and Leaves Image Recognition Using Deep Learning Neural Network Combined With GRNN. Front Plant Sci. 2022;12. 10.3389/fpls.2021.791256<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fpls.2021.791256</pub-id><pub-id pub-id-type=\"pmcid\">PMC8792930</pub-id><pub-id pub-id-type=\"pmid\">35095964</pub-id></mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Ma R, Liu Y, Wu Q, et&#160;al. Deep Learning-based Visual Recognition System for Fruit-Picking Robots. In: 2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM), 2024;567&#8211;572, 10.1109/CIS-RAM61939.2024.10673388, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://ieeexplore.ieee.org/document/10673388\">https://ieeexplore.ieee.org/document/10673388</ext-link></mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cheng</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Damerow</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Early yield prediction using image analysis of apple fruit and tree canopy features with neural networks</article-title><source>J Imaging</source><year>2017</year><volume>3</volume><issue>1</issue><fpage>6</fpage><pub-id pub-id-type=\"doi\">10.3390/jimaging3010006</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Cheng H, Damerow L, Sun Y, et al. Early yield prediction using image analysis of apple fruit and tree canopy features with neural networks. J Imaging. 2017;3(1):6. 10.3390/jimaging3010006.</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><mixed-citation publication-type=\"other\">Zhang J, Yang W, Lu Z, et&#160;al. Hr-yolov8: A crop growth status object detection method based on yolov8. Electronics 2024;13(9). 10.3390/electronics13091620, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.mdpi.com/2079-9292/13/9/1620\">https://www.mdpi.com/2079-9292/13/9/1620</ext-link></mixed-citation></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Mane S, Bartakke P, Bastewad T. DetSSeg: A Selective On-Field Pomegranate Segmentation Approach. In: 2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI), 2023;1&#8211;6, 10.1109/CVMI59935.2023.10464563, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://ieeexplore.ieee.org/abstract/document/10464563\">https://ieeexplore.ieee.org/abstract/document/10464563</ext-link></mixed-citation></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Devanna</surname><given-names>RP</given-names></name><name name-style=\"western\"><surname>Milella</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Marani</surname><given-names>R</given-names></name><etal/></person-group><article-title>In-field automatic identification of pomegranates using a farmer robot</article-title><source>Sensors</source><year>2022</year><volume>22</volume><issue>15</issue><fpage>5821</fpage><pub-id pub-id-type=\"doi\">10.3390/s22155821</pub-id><pub-id pub-id-type=\"pmid\">35957377</pub-id><pub-id pub-id-type=\"pmcid\">PMC9370860</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Devanna RP, Milella A, Marani R, et al. In-field automatic identification of pomegranates using a farmer robot. Sensors. 2022;22(15):5821. 10.3390/s22155821.<pub-id pub-id-type=\"pmid\">35957377</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/s22155821</pub-id><pub-id pub-id-type=\"pmcid\">PMC9370860</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>MaO</surname><given-names>Kebiao</given-names></name><name name-style=\"western\"><surname>Zhonghua</surname><given-names>GUO</given-names></name></person-group><article-title>Defogging remote sensing images method based on a hybrid attention-based generative adversarial network</article-title><source>Smart Agric</source><year>2025</year><volume>7</volume><issue>2</issue><fpage>172</fpage><pub-id pub-id-type=\"doi\">10.12133/j.smartag.SA202410011</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Liu MA, MaO Kebiao, Zhonghua GUO. Defogging remote sensing images method based on a hybrid attention-based generative adversarial network. Smart Agric. 2025;7(2):172. 10.12133/j.smartag.SA202410011.</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>JT</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>YY</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>M</given-names></name><etal/></person-group><article-title>AgriGAN: unpaired image dehazing via a cycle-consistent generative adversarial network for the agricultural plant phenotype</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>14994</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-65540-0</pub-id><pub-id pub-id-type=\"pmid\">38951207</pub-id><pub-id pub-id-type=\"pmcid\">PMC11217274</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Ding JT, Peng YY, Huang M, et al. AgriGAN: unpaired image dehazing via a cycle-consistent generative adversarial network for the agricultural plant phenotype. Sci Rep. 2024;14(1):14994. 10.1038/s41598-024-65540-0.<pub-id pub-id-type=\"pmid\">38951207</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-65540-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC11217274</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Qian</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>H</given-names></name><etal/></person-group><article-title>A survey of deep learning-based object detection methods in crop counting</article-title><source>Comput Electron Agric</source><year>2023</year><volume>215</volume><fpage>108425</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2023.108425</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Huang Y, Qian Y, Wei H, et al. A survey of deep learning-based object detection methods in crop counting. Comput Electron Agric. 2023;215:108425. 10.1016/j.compag.2023.108425.</mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Efficient occlusion avoidance based on active deep sensing for harvesting robots</article-title><source>Comput Electron Agric</source><year>2024</year><volume>225</volume><fpage>109360</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2024.109360</pub-id></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Sun T, Zhang W, Gao X, et al. Efficient occlusion avoidance based on active deep sensing for harvesting robots. Comput Electron Agric. 2024;225:109360. 10.1016/j.compag.2024.109360.</mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gong</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>T</given-names></name><etal/></person-group><article-title>Robotic harvesting of the occluded fruits with a precise shape and position reconstruction approach</article-title><source>J Field Robotics</source><year>2022</year><volume>39</volume><issue>1</issue><fpage>69</fpage><lpage>84</lpage><pub-id pub-id-type=\"doi\">10.1002/rob.22041</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Gong L, Wang W, Wang T, et al. Robotic harvesting of the occluded fruits with a precise shape and position reconstruction approach. J Field Robotics. 2022;39(1):69&#8211;84. 10.1002/rob.22041.</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Umirzakova</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Muksimova</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Shavkatovich Buriboev</surname><given-names>A</given-names></name><etal/></person-group><article-title>A unified transformer model for simultaneous cotton boll detection, pest damage segmentation, and phenological stage classification from UAV imagery</article-title><source>Drones</source><year>2025</year><volume>9</volume><issue>8</issue><fpage>555</fpage><pub-id pub-id-type=\"doi\">10.3390/drones9080555</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Umirzakova S, Muksimova S, Shavkatovich Buriboev A, et al. A unified transformer model for simultaneous cotton boll detection, pest damage segmentation, and phenological stage classification from UAV imagery. Drones. 2025;9(8):555. 10.3390/drones9080555.</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Otsu</surname><given-names>N</given-names></name></person-group><article-title>A threshold selection method from gray-level histograms</article-title><source>IEEE Trans Syst Man Cybernet</source><year>1979</year><volume>9</volume><issue>1</issue><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type=\"doi\">10.1109/TSMC.1979.4310076</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Otsu N. A threshold selection method from gray-level histograms. IEEE Trans Syst Man Cybernet. 1979;9(1):62&#8211;6. 10.1109/TSMC.1979.4310076.</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dhanachandra</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Manglem</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Chanu</surname><given-names>YJ</given-names></name></person-group><article-title>Image segmentation using K -means clustering algorithm and subtractive clustering algorithm</article-title><source>Proc Computer Sci</source><year>2015</year><volume>54</volume><fpage>764</fpage><lpage>771</lpage><pub-id pub-id-type=\"doi\">10.1016/j.procs.2015.06.090</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Dhanachandra N, Manglem K, Chanu YJ. Image segmentation using K -means clustering algorithm and subtractive clustering algorithm. Proc Computer Sci. 2015;54:764&#8211;71. 10.1016/j.procs.2015.06.090.</mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nock</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Nielsen</surname><given-names>F</given-names></name></person-group><article-title>Statistical region merging</article-title><source>IEEE Trans Pattern Anal Machine Intell</source><year>2004</year><volume>26</volume><issue>11</issue><fpage>1452</fpage><lpage>1458</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2004.110</pub-id><pub-id pub-id-type=\"pmid\">15521493</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Nock R, Nielsen F. Statistical region merging. IEEE Trans Pattern Anal Machine Intell. 2004;26(11):1452&#8211;8. 10.1109/TPAMI.2004.110.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2004.110</pub-id><pub-id pub-id-type=\"pmid\">15521493</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cortes</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Mach Learn</source><year>1995</year><volume>20</volume><issue>3</issue><fpage>273</fpage><lpage>297</lpage><pub-id pub-id-type=\"doi\">10.1007/BF00994018</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Cortes C, Vapnik V. Support-vector networks. Mach Learn. 1995;20(3):273&#8211;97. 10.1007/BF00994018.</mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">LeCun Y, Boser B, Denker J, et&#160;al (1989) Handwritten Digit Recognition with a Back-Propagation Network. In: Touretzky D (ed) Advances in Neural Information Processing Systems, vol&#160;2. Morgan-Kaufmann, https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Long J, Shelhamer E, Darrell T (2015) Fully Convolutional Networks for Semantic Segmentation. 10.48550/arXiv.1411.4038, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1411.4038\">http://arxiv.org/abs/1411.4038</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1411.4038\">arXiv:1411.4038</ext-link><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2016.2572683</pub-id><pub-id pub-id-type=\"pmid\">27244717</pub-id></mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. 2015 10.48550/arXiv.1505.04597, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1505.04597\">http://arxiv.org/abs/1505.04597</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1505.04597\">arXiv:1505.04597</ext-link> [cs]</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">He K, Gkioxari G, Doll r P, et&#160;al. Mask R-CNN. 2018 10.48550/arXiv.1703.06870, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1703.06870\">http://arxiv.org/abs/1703.06870</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1703.06870\">arXiv:1703.06870</ext-link></mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Barnes C, Shechtman E, Finkelstein A, et al. PatchMatch. ACM Transactions on Graphics (TOG). 2009. 10.1145/1531326.1531330.</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Efros</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Leung</surname><given-names>T</given-names></name></person-group><article-title>Texture synthesis by non-parametric sampling</article-title><source>Proc Seventh IEEE Int Conf Computer Vision</source><year>1999</year><volume>2</volume><fpage>1033</fpage><lpage>1038</lpage><pub-id pub-id-type=\"doi\">10.1109/ICCV.1999.790383</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Efros A, Leung T. Texture synthesis by non-parametric sampling. Proc Seventh IEEE Int Conf Computer Vision. 1999;2:1033&#8211;8. 10.1109/ICCV.1999.790383.</mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Criminisi</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Perez</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Toyama</surname><given-names>K</given-names></name></person-group><article-title>Region filling and object removal by exemplar-based image inpainting</article-title><source>IEEE Trans Image Process</source><year>2004</year><volume>13</volume><issue>9</issue><fpage>1200</fpage><lpage>1212</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2004.833105</pub-id><pub-id pub-id-type=\"pmid\">15449582</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Criminisi A, Perez P, Toyama K. Region filling and object removal by exemplar-based image inpainting. IEEE Trans Image Process. 2004;13(9):1200&#8211;12. 10.1109/TIP.2004.833105.<pub-id pub-id-type=\"pmid\">15449582</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/tip.2004.833105</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Kingma DP, Welling M. Auto-Encoding Variational Bayes. 2013 10.48550/arXiv.1312.6114, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1312.6114\">http://arxiv.org/abs/1312.6114</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1312.6114\">arXiv:1312.6114</ext-link> [stat]</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><citation-alternatives><element-citation id=\"ec-CR30\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>T</given-names></name><etal/></person-group><article-title>Image inpainting based on deep learning: a review</article-title><source>Inform Fusion</source><year>2023</year><volume>90</volume><fpage>74</fpage><lpage>94</lpage><pub-id pub-id-type=\"doi\">10.1016/j.inffus.2022.08.033</pub-id></element-citation><mixed-citation id=\"mc-CR30\" publication-type=\"journal\">Zhang X, Zhai D, Li T, et al. Image inpainting based on deep learning: a review. Inform Fusion. 2023;90:74&#8211;94. 10.1016/j.inffus.2022.08.033.</mixed-citation></citation-alternatives></ref><ref id=\"CR31\"><label>31.</label><mixed-citation publication-type=\"other\">Goodfellow IJ, Pouget-Abadie J, Mirza M, et&#160;al. Generative Adversarial Networks. 2014 10.48550/arXiv.1406.2661, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1406.2661\">http://arxiv.org/abs/1406.2661</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1406.2661\">arXiv:1406.2661</ext-link></mixed-citation></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kumar</surname><given-names>RA</given-names></name><name name-style=\"western\"><surname>Rajpurohit</surname><given-names>VS</given-names></name><name name-style=\"western\"><surname>Jirage</surname><given-names>BJ</given-names></name></person-group><article-title>Pomegranate fruit quality assessment using machine intelligence and wavelet features</article-title><source>J Horticul Res</source><year>2018</year><volume>26</volume><issue>1</issue><fpage>53</fpage><lpage>60</lpage><pub-id pub-id-type=\"doi\">10.2478/johr-2018-0006</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Kumar RA, Rajpurohit VS, Jirage BJ. Pomegranate fruit quality assessment using machine intelligence and wavelet features. J Horticul Res. 2018;26(1):53&#8211;60. 10.2478/johr-2018-0006.</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kumar</surname><given-names>RA</given-names></name><name name-style=\"western\"><surname>Rajpurohit</surname><given-names>VS</given-names></name><name name-style=\"western\"><surname>Bidari</surname><given-names>KY</given-names></name></person-group><article-title>Multi class grading and quality assessment of pomegranate fruits based on physical and visual parameters</article-title><source>Int J Fruit Sci</source><year>2019</year><volume>19</volume><issue>4</issue><fpage>372</fpage><lpage>396</lpage><pub-id pub-id-type=\"doi\">10.1080/15538362.2018.1552230</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Kumar RA, Rajpurohit VS, Bidari KY. Multi class grading and quality assessment of pomegranate fruits based on physical and visual parameters. Int J Fruit Sci. 2019;19(4):372&#8211;96. 10.1080/15538362.2018.1552230.</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pakruddin</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Hemavathy</surname><given-names>R</given-names></name></person-group><article-title>A comprehensive standardized dataset of numerous pomegranate fruit diseases for deep learning</article-title><source>Data Brief.</source><year>2024</year><volume>54</volume><fpage>110284</fpage><pub-id pub-id-type=\"doi\">10.1016/j.dib.2024.110284</pub-id><pub-id pub-id-type=\"pmid\">38962206</pub-id><pub-id pub-id-type=\"pmcid\">PMC11220843</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Pakruddin B, Hemavathy R. A comprehensive standardized dataset of numerous pomegranate fruit diseases for deep learning. Data Brief. 2024;54:110284. 10.1016/j.dib.2024.110284.<pub-id pub-id-type=\"pmid\">38962206</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.dib.2024.110284</pub-id><pub-id pub-id-type=\"pmcid\">PMC11220843</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><citation-alternatives><element-citation id=\"ec-CR35\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title>YOLO-Granada: a lightweight attentioned Yolo for pomegranates fruit detection</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>16848</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-67526-4</pub-id><pub-id pub-id-type=\"pmid\">39039263</pub-id><pub-id pub-id-type=\"pmcid\">PMC11263582</pub-id></element-citation><mixed-citation id=\"mc-CR35\" publication-type=\"journal\">Zhao J, Du C, Li Y, et al. YOLO-Granada: a lightweight attentioned Yolo for pomegranates fruit detection. Sci Rep. 2024;14(1):16848. 10.1038/s41598-024-67526-4.<pub-id pub-id-type=\"pmid\">39039263</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-67526-4</pub-id><pub-id pub-id-type=\"pmcid\">PMC11263582</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR36\"><label>36.</label><mixed-citation publication-type=\"other\">Ho J, Jain A, Abbeel P. Denoising Diffusion Probabilistic Models. 2020 10.48550/arXiv.2006.11239, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2006.11239\">http://arxiv.org/abs/2006.11239</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2006.11239\">arXiv:2006.11239</ext-link> [cs]</mixed-citation></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Abdi M, Nahavandi S. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. 2017 10.48550/arXiv.1609.05672, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1609.05672\">arXiv:1609.05672</ext-link></mixed-citation></ref><ref id=\"CR38\"><label>38.</label><citation-alternatives><element-citation id=\"ec-CR38\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Bovik</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Sheikh</surname><given-names>H</given-names></name><etal/></person-group><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Trans Image Process</source><year>2004</year><volume>13</volume><issue>4</issue><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type=\"pmid\">15376593</pub-id></element-citation><mixed-citation id=\"mc-CR38\" publication-type=\"journal\">Wang Z, Bovik A, Sheikh H, et al. Image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process. 2004;13(4):600&#8211;12. 10.1109/TIP.2003.819861.<pub-id pub-id-type=\"pmid\">15376593</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/tip.2003.819861</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huynh-Thu</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Ghanbari</surname><given-names>M</given-names></name></person-group><article-title>Scope of validity of PSNR in image/video quality assessment</article-title><source>Electron Lett</source><year>2008</year><volume>44</volume><issue>13</issue><fpage>800</fpage><lpage>801</lpage><pub-id pub-id-type=\"doi\">10.1049/el:20080522</pub-id></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Huynh-Thu Q, Ghanbari M. Scope of validity of PSNR in image/video quality assessment. Electron Lett. 2008;44(13):800&#8211;1. 10.1049/el:20080522.</mixed-citation></citation-alternatives></ref><ref id=\"CR40\"><label>40.</label><mixed-citation publication-type=\"other\">Isola P, Zhu JY, Zhou T, et&#160;al. Image-to-Image Translation with Conditional Adversarial Networks. 2018 10.48550/arXiv.1611.07004, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1611.07004\">http://arxiv.org/abs/1611.07004</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/1611.07004\">arXiv:1611.07004</ext-link> [cs]</mixed-citation></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Konz N, Chen Y, Dong H, et&#160;al. Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models. 2024 10.48550/arXiv.2402.05210, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2402.05210\">http://arxiv.org/abs/2402.05210</ext-link>, <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2402.05210\">arXiv:2402.05210</ext-link> [eess]</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Plant Methods Plant Methods 354 plantmeth Plant Methods 1746-4811 BMC PMC12659505 PMC12659505.1 12659505 12659505 41299589 10.1186/s13007-025-01476-4 1476 1 Research A conditional segmentation-guided network for pomegranate image completion under occlusion Zhang Duokuo zhangduokuo@stu.hist.edu.cn 1 2 Hou Ruizhe 5 Guo Jingjing 4 Zhao Mingfu 1 Wang Qi 2 Luo Zhen 2 Xu Kun xk.xukun@163.com 2 3 1 https://ror.org/0578f1k82 grid.503006.0 0000 0004 1761 7808 School of Information Engineering, Henan Institute of Science and Technology, Hongqi, Xinxiang, 453003 Henan China 2 https://ror.org/00jjkh886 grid.460173.7 0000 0000 9940 7302 School of artificial Intelligence, Zhoukou Normal University, Chuanhui, Zkoukou, 466001 Henan China 3 https://ror.org/00jjkh886 grid.460173.7 0000 0000 9940 7302 Henan International Joint Laboratory of Smart Agriculture Information Processing, Zhoukou Normal University, Chuanhui, Zkoukou, 466001 Henan China 4 https://ror.org/01hg31662 grid.411618.b 0000 0001 2214 9197 Beijing Key Laboratory of Information Service Engineering, Beijing Union University, Chaoyang, Beijing, 100101 China 5 https://ror.org/0530pts50 grid.79703.3a 0000 0004 1764 3838 School of Automation Science and Engineering, South China University of Technology, Tianhe, Guangzhou, 510641 Guangdong China 27 11 2025 2025 21 478388 153 14 8 2025 18 11 2025 27 11 2025 28 11 2025 28 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . In agricultural images acquired under natural conditions, pomegranate fruits are often partially occluded by leaves and branches, resulting in missing structural information that compromises the accuracy of yield estimation and automated harvesting. To overcome the challenges of recovering structural integrity in occluded agricultural imagery, we propose the Conditional Segmentation-guided Diffusion Network (CSD-Net). CSD-Net is a lightweight, unified framework, representing the first conditional diffusion model specifically designed for the joint tasks of pomegranate image completion and segmentation. CSD-Net aims to address the structural fidelity limitations of traditional completion methods. It utilizes a shared encoder, a segmentation branch, and an RGB diffusion branch. Crucially, the network leverages the segmentation mask as a key structural prior condition to guide the diffusion generation process. This innovative conditional guidance mechanism ensures high-fidelity reconstruction of fruit structures while maintaining spatial and textural consistency. Experimental results demonstrate that CSD-Net substantially outperforms conventional methods across metrics, achieving 30.37 dB in PSNR and 0.9490 in SSIM. Furthermore, its model size is only 117 MB, striking an effective balance between high completion quality and inference efficiency. This study offers a novel and highly effective solution for mitigating occlusion issues in agricultural visual perception tasks. Upon acceptance of this paper, the source code will be made publicly available at https://github.com/zdkd/PCSN . Keywords Pomegranate image completion Conditional diffusion model Image segmentation Multi-scale conditional fusion Agricultural computer vision https://doi.org/10.13039/501100009101 Education Department of Henan Province 252102210135 252102210231 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; BioMed Central Ltd., part of Springer Nature 2025 Introduction Pomegranate ( Punica granatum L. ) is a diploid fruit crop [ 1 ] widely cultivated in tropical and subtropical regions around the world [ 2 ]. As an economically important species with edible, medicinal, and ornamental value [ 3 ], pomegranate has been extensively promoted due to its strong adaptability and ease of cultivation and management. Its fruits are rich in polyphenolic antioxidant compounds and exhibit notable anti-inflammatory, antiproliferative, and antitumor activities, showing great potential in the prevention and adjuvant treatment of various cancers [ 4 ]. With the continuous growth of the global population and increasing demand for high-quality agricultural products, the precise production and intelligent management of pomegranate have become critical topics in modern agriculture. In this context, computer vision techniques have been widely applied in tasks such as fruit harvesting [ 5 ], yield estimation [ 6 ], and growth monitoring. For instance, [ 7 ] designed a deep learning&#8211;based visual recognition system for fruit-picking robots; [ 8 ] proposed an early yield prediction approach integrating artificial neural networks (ANN), image analysis, and canopy features; [ 9 ] introduced a high-resolution object detection model, HR-YOLOv8, based on a self-attention mechanism to enhance small-object detection performance under complex conditions. However, in real-world agricultural environments, pomegranate images are often affected by leaf and branch occlusions, fruit overlap, rain and fog interference, and uneven illumination, which severely compromise the reliability of vision systems. To address these challenges, researchers have explored multiple directions. On one hand, efforts have focused on improving feature extraction networks to enhance model robustness. For example, [ 10 ] combined YOLOv5 with UNet-MobileNetV2 to improve pomegranate segmentation accuracy, while [ 11 ] employed a multi-stage transfer learning strategy to fine-tune pretrained models using fruit images collected under controlled conditions to better adapt to complex field scenarios. On the other hand, some studies have targeted image degradation problems. [ 12 ] proposed a generative adversarial network (GAN) with hybrid attention for remote sensing image dehazing, and [ 13 ] utilized a cycle-consistent GAN for unpaired dehazing of agricultural phenotyping images. Although these methods have achieved noticeable progress in improving image quality or segmentation accuracy, most of them focus solely on detection or segmentation tasks and fail to directly address the structural information loss of pomegranate fruits caused by occlusion. In practice, structural loss can significantly impair downstream applications: in fruit counting, occlusion often leads to missed or double detections [ 14 ], in robotic harvesting, incomplete contours may result in motion planning failure [ 15 ], and in yield estimation, occlusions hinder accurate measurement of fruit volume. Therefore, restoring structurally complete, visually realistic, and seamlessly integrated pomegranate foregrounds from partially occluded images has become a key challenge for enhancing the robustness and automation of agricultural vision systems. Currently, mainstream paradigms for fruit image completion typically adopt a multi-stage &#8220;segmentation&#8211;completion&#8211;fusion&#8221; pipeline. For example, [ 16 ]. proposed a high-precision reconstruction method for occluded fruits following this workflow. However, such cascaded architectures are often cumbersome and prone to cumulative errors, which significantly degrade overall system efficiency and real-time performance. Inspired by the joint modeling paradigm proposed by [ 17 ], this study introduces an end-to-end Pomegranate Completion and Segmentation Network based on a conditional diffusion model. The proposed framework abandons traditional multi-stage pipelines and unifies instance segmentation and guided diffusion completion within a single network. Specifically, the model first performs accurate segmentation of visible fruit regions from occluded images. Then, the dynamically generated segmentation mask is used as a structural prior to conditionally guide the diffusion process, enabling high-quality reconstruction of occluded parts while preserving the original background. The final results are structurally consistent, texturally realistic, and exhibit natural boundaries, thereby achieving seamless pomegranate foreground completion. The main contributions of this study are as follows: This study introduce the first conditional diffusion-based framework for pomegranate image completion under occlusion, featuring a multi-branch collaborative design to improve structural consistency and detail restoration. We design a synergistic architecture combining segmentation and completion, using structural masks to guide the generation process and enhance geometric fidelity. We integrate multi-scale conditional fusion with adaptive diffusion strategies, achieving enabling a unified model that jointly learns structure, semantics, and high-fidelity completion. The structure of this paper is as follows:Sect.&#160;'' Related work '' reviews related work on image segmentation and image completion technologies. Section&#160;'' MaterialsandMethods '' introduces the materials, data preprocessing, and the proposed Conditional Segmentation-guided Diffusion Network (CSD-Net) architecture in detail. Section&#160;'' Results '' presents experimental results and comparisons with state-of-the-art methods, followed by an in-depth discussion in Sect.&#160;'' Discussion ''. Finally, Sect.&#160;'' Conclusions '' concludes the paper. Related work Image segmentation In the developmental trajectory of segmentation methodologies, early approaches primarily relied on conventional image processing techniques [ 18 ], such as k-means clustering [ 19 ] and region growing [ 20 ]. However, these traditional methods depend heavily on handcrafted features and predefined rules. When confronted with complex image scenes or highly variable targets, their performance tends to degrade significantly. With the advent and rapid progress of machine learning technologies, hybrid approaches combining machine learning with image processing have been gradually introduced into segmentation tasks. Representative examples include support vector machines (SVM) [ 21 ] and early neural network models. These methods learn image features and segmentation rules through model training, achieving certain improvements over traditional approaches, particularly in handling complex visual environments. Nevertheless, the true revolution in image segmentation has been driven by the emergence of deep learning. In particular, the introduction and advancement of convolutional neural networks (CNNs) [ 22 ] have led to a significant leap forward in segmentation techniques. Deep learning&#8211;based methods automatically acquire image feature representations and segmentation rules through integrated learning processes, eliminating the need for manual feature engineering while demonstrating enhanced generalization ability and adaptability. In contemporary computer vision research, architectures such as Fully Convolutional Networks (FCN) [ 23 ], U-Net [ 24 ], and Mask R-CNN [ 25 ] have become the dominant frameworks for segmentation tasks. These architectures consistently deliver outstanding performance across diverse datasets and challenging segmentation scenarios. Building upon these advancements, the present study does not treat segmentation as an independent preprocessing step. Instead, segmentation is deeply integrated into the image restoration process, forming a joint learning framework that achieves collaborative optimization of segmentation and completion tasks. Image generation and completion technology Image inpainting, also known as image restoration, aims to reconstruct missing or occluded regions of an image in a visually plausible manner. Traditional methods typically search for patches in the intact regions of an image that closely match the missing areas and use them for completion. Representative examples include the PatchMatch algorithm proposed by [ 26 ], the Markov Random Field (MRF) approach by [ 27 ], and the exemplar-based texture synthesis method introduced by [ 28 ]. While these methods perform well on images with repetitive textures, they often fail when dealing with non-repetitive or semantically complex structures. Since the emergence of convolutional neural networks, deep learning has progressively become the dominant paradigm in image restoration research [ 29 , 30 ] introduced the Variational Autoencoder (VAE), which combines the principles of autoencoders and probabilistic graphical models by mapping input images into a latent probabilistic space and decoding new samples from it. This probabilistic framework enables the generation of diverse image reconstructions while effectively capturing underlying structural and semantic features. Subsequently, [ 31 ] proposed the Generative Adversarial Network (GAN), in which a generator and discriminator are trained in an adversarial manner. GANs are capable of rapidly synthesizing realistic images, making them a popular choice for image inpainting tasks. In recent years, diffusion models have emerged as a breakthrough in the field of image restoration. Their progressive denoising process allows images to be reconstructed step by step, while the incorporation of temporal embeddings and conditional controls enables fine-grained regulation of the generative process. This leads to outstanding detail fidelity and high visual quality. Consequently, diffusion-based inpainting-where missing regions are filled according to a static occlusion mask and then seamlessly fused with the original image-has become the mainstream paradigm. Unlike existing image inpainting approaches, the model proposed in this study adopts an end-to-end joint learning framework that dynamically predicts visible region masks of pomegranates and incorporates them as structural priors into the conditional diffusion generation process in a multi-scale manner. This enables high-fidelity completion with structurally consistent, texturally natural, and background-coherent results, while preserving the integrity of the original image background. Materials and methods Data sources and acquisition The pomegranate image dataset used in this study consists of two parts: field-captured images and publicly available online data. The publicly available portion primarily includes the dataset of pomegranates by Kumar et al. [ 32 , 33 ] (1080 images) and the pomegranate disease dataset by Pakruddin et al. [ 34 ] (1440 images). The remaining 692 images were collected by our research team through on-site photography in the orchards of Henan Institute of Science and Technology, China, during the period from May to September each year. The field acquisition comprehensively covered five key phenological stages of pomegranate growth: bud stage, flowering stage, fruit set, mid-growth stage, and maturity stage. For the purpose of this experiment, we primarily selected images from the mid-growth and maturity stages to ensure the dataset contained morphologically complete fruits. All field-captured images were taken under natural lighting conditions between 9:00 AM and 5:00 PM, covering diverse weather scenarios such as sunny and cloudy days, thereby enhancing the real-world variability of the dataset. The original images were manually captured using the rear camera of an iPhone XR in HEIC format with a resolution of 3024 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 4032 pixels. Subsequently, all images were uniformly converted to JPG format at a resolution of 640 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 480 pixels to ensure data consistency and computational efficiency. It is worth noting that all collected images clearly display intact pomegranates, providing a solid foundation for generating simulated occlusion images in subsequent experiments. Data preprocessing To construct a training set with diverse occlusion patterns, we implemented a carefully designed data preprocessing pipeline. First, 300 individual samples of leaves and branches were manually cropped using Meitu Xiuxiu (version 7.7.1.6). These samples were then augmented through rotation, scaling, and other transformations, resulting in a leaf sample library containing 600 images. Subsequently, our team&#8217;s self-developed deep neural network model, YOLO-Granada [ 35 ], was employed to detect the positions of pomegranates in the original images. Based on the detection results, images from the leaf sample library were randomly pasted over the detected pomegranate regions to simulate occlusion scenarios. To ensure dataset quality, we conducted meticulous manual inspections and corrected any images where occlusion synthesis failed due to inaccurate pomegranate localization. Finally, all generated images were manually annotated using the LabelMe tool to produce the corresponding masks of unobstructed pomegranate regions. Through this series of steps, we constructed a comprehensive dataset containing 3212 images. The detailed distribution of the dataset is shown in Table 1 , and the overall data preprocessing workflow is illustrated in Fig.&#160; 1 .The final processed image is shown in the Figure 2 . Fig. 1 Data preprocessing workflow. The original images are processed to generate occluded images and corresponding masks Fig. 2 Visualization images of the dataset Table 1 Number of images in the dataset Number/type of images original image Leaf Sample Bank Training Validation test Number of images 3212 600 2570 321 321 Overview of model architecture In this study, we propose an end-to-end unified framework for pomegranate image completion and segmentation. The overall workflow is illustrated in Fig&#160; 3 . First, the partially occluded pomegranate image is fed into a shared encoder to extract multi-scale features. These features are then processed by the segmentation branch to predict a visible-region mask of the pomegranate. Guided by the segmentation results, the RGB diffusion branch employs a conditional diffusion model to generate a complete pomegranate image, thereby filling in the occluded regions. To further enhance the seamless integration between the completed regions and the visible areas, the framework incorporates two key innovations: a multi-scale conditional fusion module and an adaptive timestep embedding module. The segmentation mask is employed as crucial structural prior information and introduced into the denoising process of the diffusion model in a conditional form. It provides precise boundary and shape information for the image. The multi-scale feature fusion module receives features from different levels of a shared encoder, ensuring that during the diffusion process, the network not only focuses on the global structure but also leverages low-level detailed features to finely reconstruct the surface texture, illumination, and color gradients of the fruit. The segmentation mask ensures structural integrity, while multi-scale fusion guarantees texture fidelity. The final outputs consist of the completed pomegranate image and the corresponding visible-region mask. All modules operate in a coordinated manner to achieve high-quality completion and segmentation of occluded pomegranates. The following sections detail the structure and implementation of each module. Fig. 3 Overview of the proposed CSD-Net framework a The overall structure of the framework is presented, including a shared encoder, a split branch, an RGB diffusion branch, a multi-scale conditional fusion module, and an adaptive time-step embedding module. The segmentation branch predicts a visible region mask, which guides the RGB diffusion branch to generate a complete pomegranate image. b shows the basic architecture of the diffusion model, and c shows the image of our network denoising process Diffusion models In this study, we adopt the Denoising Diffusion Probabilistic Model (DDPM) [ 36 ] as the core generative mechanism for completing missing regions in occluded images. DDPM is a generative framework based on a Markov chain, which simulates a forward diffusion process that gradually adds Gaussian noise to an image, and learns a reverse denoising process to reconstruct the original data. The key idea is to progressively perturb a data sample into a standard normal distribution by adding Gaussian noise, and then iteratively recover a clean image through the learned reverse process. Formally, the forward diffusion process can be defined as: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\begin{aligned} q(x_t \\mid x_{t-1})&amp;= {\\mathcal {N}} \\big ( x_t; \\sqrt{1-\\beta _t} \\, x_{t-1} \\\\&amp;\\quad \\beta _t {\\textbf{I}} \\big ), \\quad t = 1, \\dots , T \\end{aligned} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta _t$$\\end{document} is a variance schedule controlling the noise level at each timestep \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$t$$\\end{document} , and T is the total number of diffusion steps. This process gradually transforms the original image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x_0$$\\end{document} into an isotropic Gaussian distribution. The reverse denoising process is parameterized by a neural network as: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} p_\\theta (x_{t-1} \\mid x_t) = {\\mathcal {N}} \\left( x_{t-1}; \\mu _\\theta (x_t, t), \\Sigma _\\theta (x_t, t) \\right) \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu _\\theta$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\Sigma _\\theta$$\\end{document} are predicted by the network, conditioned on the noisy input \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x_t$$\\end{document} and the timestep \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$t$$\\end{document} . In our framework, Gaussian noise is added to the complete pomegranate foreground image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x_0$$\\end{document} during the forward process to obtain noisy images \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x_t$$\\end{document} . During the reverse diffusion process, the noisy image is gradually denoised by using the CSD-net network we designed, and then a complete pomegranate foreground image is generated. This conditional formulation enables progressive reconstruction of the pomegranate foreground, effectively modeling complex structural and textural features, and producing natural, coherent completion results. Shared encoder network In the proposed image completion and segmentation framework, the shared encoder serves as the foundation of the entire network, responsible for extracting multi-scale, high-level semantic features from the input occluded image. The core design principle is that, regardless of whether the subsequent task is visible-region segmentation or occluded-region completion via diffusion, a deep understanding of the input image is essential. A powerful shared feature extraction module can effectively enhance overall performance and facilitate information synergy between different tasks. We adopt a U-Net encoder architecture due to its widespread success in agricultural vision tasks (e.g., crop segmentation, disease detection), where preserving spatial resolution through skip connections is critical for fine-grained object recovery. Unlike pure CNN backbones (e.g., ResNet), U-Net explicitly maintains multi-scale feature maps, which are indispensable for both accurate segmentation and context-aware diffusion guidance in occluded pomegranate images.The shared encoder progressively extracting features through a series of downsampling operations. It consists of an initial convolutional layer, multiple downsampling blocks, and a bottleneck layer. The initial convolutional layer applies a \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$3 \\times 3$$\\end{document} convolution to perform preliminary feature extraction, expanding the channel dimension from 3 to a base value of 64. This stage primarily captures low-level visual features such as edges and textures. The encoder is designed with three downsampling stages. Each stage comprises two residual blocks (ResBlocks) [ 37 ] and one downsampling convolutional layer. Skip connections are incorporated to alleviate the vanishing gradient problem in deep networks and enable the learning of more complex feature mappings. The downsampling convolutional layers reduce the spatial resolution of the feature maps by half while doubling the number of channels, thereby allowing the network to capture features with larger receptive fields at deeper levels. Three sets of skip-connection features are obtained through these stages. After the three downsampling stages, a bottleneck layer is introduced. This layer includes two residual blocks and one self-attention block (AttentionBlock). The self-attention mechanism captures long-range dependencies between any positions in the feature map, which is crucial for understanding the global context of the image, particularly when handling occluded regions. The shared encoder ultimately outputs two components: the bottleneck features, which are the highest-level and most abstract representations obtained after multiple downsampling and feature extraction steps, serving as shared inputs for both the segmentation and diffusion branches; and a list of skip-connection features from different downsampling stages, which are fed into the subsequent decoders to provide multi-scale information for recovering spatial details. The architecture of the shared encoder is illustrated in Fig.&#160; 4 . Fig. 4 Architecture of the shared encoder network. The encoder consists of an initial convolutional layer, three downsampling stages, and a bottleneck layer with self-attention. It outputs multi-scale features for subsequent segmentation and diffusion tasks Segmentation branch The segmentation branch constitutes the second core component of the proposed framework, aiming to accurately predict the visible pomegranate regions from the features extracted by the shared encoder. The generated visible mask is subsequently used to guide the conditional diffusion process. Architecturally, this branch adopts a U-Net-style decoder, This architecture can fully utilize the hierarchical characteristics of the shared encoder. This symmetrical design ensures pixel-level alignment between the encoder and decoder features, which is crucial for generating precise visible region masks - and this is a prerequisite for reliable diffusion conditions.U-Net decoder progressively restoring spatial resolution through a sequence of upsampling operations and skip connections, thereby producing high-resolution segmentation masks. Let the bottleneck feature map from the shared encoder be F3 &#8712; R B&#215;8C0&#215;H/16&#215;W/16 and the three skip connection features be F2 &#8712; R B&#215;4C0&#215;H/8&#215;W/8 , F1 &#8712;R B&#215;2C0&#215;H/4&#215;W/4 ,F0 &#8712;R B&#215;C0&#215;H/2&#215;W/2 , where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$B$$\\end{document} denotes the batch size, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_0$$\\end{document} the base channel number, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H/W$$\\end{document} the spatial resolution of the input image. The decoding process begins by spatially upsampling \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_3$$\\end{document} using a transposed convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$D_{2\\times }(\\cdot ; \\theta )$$\\end{document} to double its resolution: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\begin{aligned} U_3&amp;= D_{2\\times }(F_3; \\theta _3) = W_3 * \\textrm{Pad}(F_3) \\\\ \\quad&amp;U_3 \\in {\\mathbb {R}}^{B \\times 4C_0 \\times H/8 \\times W/8} \\end{aligned} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$W_3$$\\end{document} is the transposed convolution kernel, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$*$$\\end{document} denotes the convolution operation, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{Pad}(\\cdot )$$\\end{document} represents zero-padding to ensure dimensional consistency. The upsampled feature \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$U_3$$\\end{document} is concatenated with the mid-level feature \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_2$$\\end{document} along the channel dimension: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} X_3 = \\textrm{Concat}(U_3, F_2) \\in {\\mathbb {R}}^{B \\times 8C_0 \\times H/8 \\times W/8} \\end{aligned}$$\\end{document} and refined by two consecutive residual blocks 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} H_3 = \\textrm{ResBlock}(\\textrm{ResBlock}(X_3)) \\end{aligned}$$\\end{document} The same &#8220;upsample&#8211;concatenate&#8211;residual refinement&#8221; operation is repeated for \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_3$$\\end{document} with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_1$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_0$$\\end{document} , producing the full-resolution feature map \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$H_0$$\\end{document} . Finally, a \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1\\times 1$$\\end{document} convolution \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_{1\\times 1}(\\cdot ; \\omega )$$\\end{document} reduces the channels to one, followed by a Sigmoid activation to generate the probability mask of the visible regions: 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} M = \\sigma (C_{1\\times 1}(H_0; \\omega )), \\quad M \\in [0,1], \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma (x) = 1 / (1 + e^{-x})$$\\end{document} .This hierarchical feature fusion combined with residual learning effectively mitigates gradient vanishing while preserving both spatial details and global semantic information, thereby enhancing the accuracy and robustness of visible region prediction. Diffusion branch The diffusion branch serves as the core generative module of the proposed framework, aiming to perform high-fidelity inpainting of occluded regions in pomegranate images and to produce complete, visually coherent foreground reconstructions. This branch innovatively integrates a multi-scale conditional fusion mechanism with an adaptive timestep embedding mechanism, significantly improving structural consistency and perceptual realism in the generated results. Multi-scale conditional fusion module The diffusion process relies on various conditional inputs, including the initial noisy image, the visible region segmentation mask, and multi-scale features extracted by the shared encoder. To exploit their complementary properties, we design a fusion-based decoder architecture that explicitly interacts with these conditions at multiple scales. Given the original-resolution visible mask 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} M_{vis} \\in {\\mathbb {R}}^{1 \\times H \\times W} \\end{aligned}$$\\end{document} it is resized via bilinear interpolation to match the spatial dimensions of the feature maps at each downsampling stage: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\begin{aligned} M_{vis}^{(s)}&amp;= Interpolate(M_{vis}, \\\\&amp;size =F_{feat}^{(s)}.shape) \\end{aligned} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_{\\textrm{feat}}^{(s)}$$\\end{document} denotes the feature map at scale \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$s$$\\end{document} (e.g., \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$8\\times$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$4\\times$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2\\times$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1\\times$$\\end{document} scales). At each scale, the resized mask is concatenated with the corresponding feature map along the channel dimension: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F_{concat}^{(s)} = \\textrm{Concat}(F_{\\textrm{feat}}^{(s)}, M_{vis}^{(s)}) \\end{aligned}$$\\end{document} and then processed by a convolution layer, group normalization, and a non-linear activation to produce condition-aware features: 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} F ^{(s)} = \\delta (GN(W^{(s)}_{cond}*F_{concat}^{(s)})) \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$W_{\\textrm{cond}}^{(s)}$$\\end{document} denotes the convolution kernel, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$*$$\\end{document} indicates convolution, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\delta (\\cdot )$$\\end{document} is the activation function. Independent fusion paths are defined for each scale, balancing global structural consistency and local detail preservation. Adaptive timestep embedding module To enable the model to explicitly perceive the current diffusion time step \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$t$$\\end{document} , we adopted a position encoding-based temporal embedding module that maps the scalar time step \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$t$$\\end{document} into a high-dimensional vector and performs feature enhancement through a Multi-Layer Perceptron (MLP). First, the integer t is encoded into a set of periodic feature vectors. 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\gamma (t) = [\\sin (\\frac{t}{10000^{2i/d}}),\\cos (\\frac{t}{10000^{2i/d}})]_{i=1}^{d/2} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$d$$\\end{document} is the embedding dimension Then, two fully connected layers are used to uplift the temporal embedding to a dimension that matches the image features. 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\begin{aligned} e_t&amp;= MLP(\\gamma (t)) \\\\&amp;= FC_2(GeLU(FC_1(\\gamma (t)))) \\end{aligned} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FC_1$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$FC_2$$\\end{document} are fully connected layers The time-embedding \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$e_t$$\\end{document} with subscript is injected into multiple ResBlock in the network, affecting feature propagation through channel-by-channel addition, enabling the network to have different prediction capabilities at different time steps. Experimental configuration All experiments in this study were conducted in a high-performance computing environment. The operating system platform was Ubuntu 20.04.6 LTS (64-bit), and computational tasks were supported by a powerful 13th Gen Intel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\circledR$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text {Core}^{\\texttt {TM}}$$\\end{document} i9&#8211;13900K processor with 32 logical cores. To accelerate the training and inference of deep learning models, the system was equipped with two NVIDIA GeForce RTX 4090 GPUs, running driver version 535.161.07 with CUDA 12.2 support. The machine was provisioned with 128 GB of RAM to ensure sufficient memory for large-scale experiments. On the software side, all implementations were developed and executed using the PyTorch 2.3.1 deep learning framework, in conjunction with TorchVision 0.18.1 and TorchAudio 2.3.1, along with other relevant libraries. This configuration ensured both computational efficiency and reproducibility of the experimental results. Training and validation The training process of the proposed model followed a rigorously structured data organization and processing protocol. The complete image dataset was logically split into training, validation, and testing sets at a ratio of 8:1:1. To standardize the input and optimize computational efficiency, all images and corresponding masks were resized to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$256 \\times 256$$\\end{document} pixels before being fed into the network. For pixel value processing, RGB images were normalized to the range \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[-1, 1]$$\\end{document} , while masks were binarized to meet the network&#8217;s input requirements. The batch size during training was set to 8, and the AdamW optimizer was adopted due to its robust performance, with an initial learning rate of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1 \\times 10^{-4}$$\\end{document} . The maximum number of training epochs was set to 300. During the training of the diffusion model, a random timestep ttt was sampled, and Gaussian noise was added to the complete image to simulate the diffusion process. To fully utilize the available hardware resources, the model was trained in a multi-GPU parallel setting using torch.nn.DataParallel. At the end of each epoch, model weights were saved, and an additional &#8220;best model&#8221; checkpoint was recorded whenever the validation loss reached its minimum. For training monitoring and intuitive visualization of results, after each epoch, random samples from the test set were selected for completion visualization, and composite images containing multiple intermediate outputs were saved. Throughout the training and validation phases, we continuously computed and recorded key evaluation metrics - PSNR for image reconstruction quality-to monitor model performance in real time. Evaluation metrics To comprehensively evaluate the performance of the proposed pomegranate completion and segmentation network, three objective evaluation metrics were employed: Structural Similarity Index (SSIM) [ 38 ], and Peak Signal-to-Noise Ratio (PSNR) [ 39 ]. These metrics quantify the model&#8217;s performance from two perspectives: segmentation accuracy and image reconstruction quality. SSIM is a perceptual metric that measures the similarity between two images from the perspective of human visual perception. Unlike pixel-wise error metrics, SSIM considers luminance, contrast, and structural information simultaneously, making it particularly suitable for evaluating the perceptual quality of reconstructed images. The SSIM between a generated image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$X$$\\end{document} and a reference image \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Y$$\\end{document} is computed as: 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} \\begin{aligned} SSIM(X,Y) =&amp;\\frac{\\left( 2\\mu _X\\mu _Y+C_1\\right) }{\\left( \\mu _X^2+\\mu _Y^2+C_1\\right) }\\\\&amp;\\cdot \\frac{\\left( 2\\sigma _{XY}+C_2\\right) }{\\left( \\sigma _x^2+\\sigma _Y^2+C_2\\right) } \\end{aligned} \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu _X$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu _Y$$\\end{document} are the mean intensities of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$X$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Y$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma _X^2$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma _Y^2$$\\end{document} are their variances, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\sigma _{XY}$$\\end{document} is their covariance, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_1$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_2$$\\end{document} are small constants to stabilize the division when denominators are close to zero. SSIM values range from \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$-1$$\\end{document} to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1$$\\end{document} , with higher values indicating greater structural similarity. PSNR is a standard metric for assessing the quality of reconstructed images, particularly suitable for evaluating tasks such as image inpainting or denoising, where pixel-level fidelity to the original image is critical. PSNR is computed based on the Mean Squared Error (MSE) between the generated image and the original complete image. Higher PSNR values generally indicate smaller pixel-wise differences, implying better reconstruction quality. The computation is as follows: First, the MSE is calculated: 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} MSE=\\frac{1}{MN}\\sum _{i=0}^{M-1}\\sum _{j=0}^{N-1}\\left[ I\\left( i,j\\right) -K\\left( i,j\\right) \\right] ^2 \\end{aligned}$$\\end{document} Then, PSNR is derived as: 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} PSNR=10\\cdot log_{10}\\left( \\frac{MAX_I^2}{MSE}\\right) \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$N$$\\end{document} denote the image width and height, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$I(i,j)$$\\end{document} represents the pixel value at position \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$(i,j)$$\\end{document} in the ground truth complete image, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$K(i,j)$$\\end{document} denotes the corresponding pixel value in the model-generated completed image, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textrm{MAX}_I$$\\end{document} is the maximum possible pixel value in the image. Results In this section, we present both quantitative and qualitative results to evaluate the performance of the proposed CSD-Net for pomegranate image completion under occlusion. We first conduct ablation studies to assess the contribution of each key component in our framework. Then, we compare CSD-Net against several state-of-the-art image completion methods, including DDPM, Pix2Pix, and Seg-Guide. The evaluation is based on two widely used image quality metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), which respectively measure pixel-level fidelity and perceptual-structural consistency between the completed images and ground-truth references. Ablation studies To gain deeper insights into the contribution of each module in the pomegranate occlusion completion task, we designed four ablation experiments. In each experiment, specific modules were progressively removed to evaluate their impact on the overall network performance. Specifically, we assessed the effects of the Segmentation Branch, the Multi-scale Conditional Fusion module, and the Adaptive Time Embedding module. PSNR was adopted as the primary evaluation metric. Table 2 summarizes the configurations of the CSD-Net model with different combinations of improvement strategies, namely the Segmentation Branch(SegB), the Multi-scale Conditional Fusion module(MsCF), and the Adaptive Time Embedding(ATE) module. Each experiment in the table explicitly indicates which enhancement strategies were applied. A simplified diffusion model was used as the baseline, and its results served as a reference for all subsequent experiments. By comparing the outcomes of four independent experimental settings, we can clearly identify the performance contributions of each individual module. Table 2 Ablation experiments SegB MsCF ATE PSNR Weight Size(MB) IoU Experiment1 - &#8211; &#8211; 23.4460 55.9 &#8211; Experiment2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} &#8211; &#8211; 30.2720 99.0 0.9325 Experiment3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} &#8211; 30.3250 115 0.9321 Experiment4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\checkmark$$\\end{document} 30.3689 117 0.9341 Fig. 5 Visual comparison of the four ablation experiments From the baseline results in Experiment 1, we observe that the simplified diffusion model, without incorporating any task-specific enhancements for occlusion completion, achieves a PSNR of 23.4460 dB. As illustrated in Fig.&#160; 5 , this baseline model exhibits clear deficiencies in leaf removal under complex backgrounds and challenging illumination conditions. This indicates that, although the base diffusion model possesses a certain degree of image generation capability, its performance in the pomegranate completion task under complex occlusions still has substantial room for improvement.In Experiment 2, the integration of the Segmentation Branch into the baseline model yields a remarkable improvement, with the PSNR increasing from 23.4460 dB to 30.2720 dB. This substantial gain strongly demonstrates the critical importance of explicitly identifying visible regions in image completion tasks. The Segmentation Branch provides the diffusion model with precise spatial conditioning information, enabling it to effectively differentiate between known and missing regions. Consequently, it avoids unnecessary modifications to already visible areas while more accurately guiding the diffusion process to fill in the occluded parts.Building on Experiment 2, Experiment 3 further incorporates the Multi-scale Conditional Fusion module. Here, the PSNR shows a slight improvement from 30.2720 dB to 30.3250 dB. While the numerical gain is modest, the convergence curves in Fig.&#160; 6 clearly reveal that Experiment 3 achieves faster convergence compared to Experiment 2.Finally, in Experiment 4, the addition of the Adaptive Time Embedding module to Experiment 3 yields limited improvement in PSNR. However, Fig.&#160; 6 (a) also shows that the convergence curve of Experiment 4 is noticeably smoother than that of Experiment 3. This suggests that these additional modules, while not significantly boosting final PSNR scores, contribute positively to the training efficiency and stability of the model. In the experiment of adding segmentation branches, we also evaluated the performance of the segmentation branches. Since this study did not make any special optimizations to the segmentation branches but only carried out multi-task collaboration, the IoU values of the added segmentation branches did not differ much.Fig.&#160; 6 shows some specific values of our experimental indicators. Fig. 6 Visualization of the results from ablation experiments. a PSNR convergence curves for the four ablation experiments. b IoU values of the segmentation branch across different experiments. c Diffusion branch loss curves for the four ablation experiments. d Tatol loss curves for the four ablation experiments Comparative experiments To further validate the effectiveness of the proposed CSD-Net, we conducted comparative experiments against several representative segmentation-based image completion approaches. All models were trained and evaluated on the same dataset under identical experimental settings to ensure fairness. The comparison included DDPM,Pix2Pix [ 40 ], Seg-Guide [ 41 ]. Table 3 Comparative test results Weight Size(MB) PSNR SSIM NIQE BRISQUE INFERENCE TIME(ms) DDPM 433 17.22 0.6857 11.0252 32.5852 17.584 Pix2pix 207 22.73 0.7955 9.7201 14.5019 3.39 Seg-Guided 433 12.11 0.3732 11.4948 29.8655 17.426 Ours 117 30.3689 0.9490 10.6881 25.6386 15.866 From the results in the table 3 , it is evident that CSD-net outperforms all competing methods across both full-reference and no-reference image quality metrics. Specifically, CSD-Net achieves a PSNR of 30.37 dB, exceeding the second-best method, Pix2Pix, by 7.64 dB, and attains an SSIM of 0.9490, representing a substantial improvement in structural similarity. These results demonstrate that CSD-Net excels in both reconstruction fidelity and structural consistency.Moreover, in terms of blind quality assessment, CSD-net yields a BRISQUE score of 25.64, significantly lower than DDPM (32.59) and Seg-Guide (29.87), indicating fewer perceptual distortions; its NIQE score (10.69) is also competitive, reflecting naturalness in texture and contrast that closely resembles real pomegranate images. In terms of model size, CSD-Net is only 117 MB, which is not only significantly smaller than DDPM (433 MB) and Seg-Guide (433 MB) but also smaller than Pix2Pix (207 MB), indicating a higher parameter efficiency in its architectural design.While its inference speed is slower than Pix2Pix, which utilizes a GAN architecture, it outperforms Seg-Guide and DDPM, which also use a diffusion model architecture.As shown in Fig.&#160; 7 , although DDPM, as a diffusion-based model, possesses strong generative capabilities, its lack of task-specific conditional guidance limits its ability to accurately restore local structures and textures, and it exhibits noticeable deficiencies in foreground color reproduction. Pix2Pix achieves competitive performance in conditional image generation; however, its ability to recover fine details in heavily occluded regions is limited, often producing artifacts along object boundaries. Seg-Guide performs relatively well in restoring object shapes and contours, successfully reconstructing the overall fruit geometry. Nevertheless, the generated backgrounds often lack consistency with the original input image, leading to spatial misalignments or style discrepancies that significantly degrade overall similarity metrics.By integrating the segmentation branch, multi-scale conditional fusion module, and adaptive time embedding module, CSD-Net not only preserves the structural integrity of the fruit but also generates high-quality texture details. This synergy enables the model to achieve superior completion quality while maintaining a lightweight architecture. Fig. 7 Visual comparison of the proposed CSD-Net with other methods Discussion The experimental results presented in Sect.&#160;'' Results '' consistently demonstrate that the proposed CSD-Net framework achieves state-of-the-art performance in completing occluded pomegranate images, both quantitatively (PSNR = 30.37 dB, SSIM = 0.9490) and qualitatively. This superiority stems not from simply stacking a segmentation network and a diffusion model, but from a task-aware co-design that tightly couples structure perception with content generation. Unlike conventional inpainting approaches that rely on static, pre-defined occlusion masks and adopt a &#8220;complete-then-composite&#8221; pipeline, CSD-Net dynamically predicts the visible-region mask and uses it as an adaptive structural prior to guide the diffusion process at multiple scales. This ensures that known regions remain untouched while missing parts are reconstructed with high geometric fidelity. Moreover, the lightweight architecture (117 MB) highlights an important design philosophy for agricultural vision systems: high performance need not come at the cost of computational overhead. By sharing a U-Net encoder between the segmentation and diffusion branches and eliminating redundant feature extraction, CSD-Net achieves a favorable trade-off between accuracy and efficiency. The ablation studies further confirm that the segmentation branch is the most critical component, contributing a PSNR gain of nearly 6.8 dB over the baseline diffusion model. This underscores a key insight: explicit modeling of visible structure is essential for high-fidelity fruit completion in complex natural scenes. The multi-scale conditional fusion and adaptive timestep embedding modules, while offering modest improvements in final metrics, significantly enhance training stability and convergence speed, which is valuable for practical model development. Nevertheless, the current framework has limitations.Although branching is effective, in the case where the fruits are mutually occluded, this study cannot determine which fruit corresponds to which mask. Therefore, this method is ineffective in this situation.Under extreme occlusion conditions, the results of this study will be unstable (the dataset partially lacks images in this regard). To address these challenges, future work will focus on three directions: (1) extending PCSN to multi-instance settings by integrating instance-aware segmentation and depth cues to handle fruit-to-fruit occlusion; (2) improving segmentation robustness through self-supervised or weakly supervised learning to reduce reliance on pixel-level annotations; and (3) Design more natural datasets and add images of extreme cases. In summary, CSD-Net represents a shift from generic image restoration toward structure-preserving, application-specific completion tailored for agricultural perception-a direction that aligns with the growing need for reliable, interpretable, and efficient vision systems in smart farming. Conclusions In this work, we proposed CSD-Net, a lightweight, end-to-end conditional diffusion framework that jointly learns segmentation and image completion for occluded pomegranates. By dynamically predicting visible-region masks and fusing them into the diffusion process at multiple scales, CSD-Net achieves state-of-the-art performance in both structural fidelity and background consistency. The model&#8217;s compact size (117 MB) and high reconstruction quality make it a promising solution for real-world agricultural vision systems. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements The authors would like to express many thanks to all the anonymous reviewers. Author contributions D.Z.: Conceptualization, Data Curation, Data Analysis, Implementation, Model Development, Formal Analysis, Methodology, Writing-Original Draft, Visualization; R.H.: Data Preprocessing, Writing-Review and Editing, Resources.; J.G. and Z.L.: Investigation, Data Curation.; M.Z., Q.W. and K.X.: Methodology, Project Supervisor, Writing-Review and Editing. All authors read and approved the final manuscript. Funding This work was supported in part by Department of Education of Henan Province under Grant 252,102,210,135 and 252102210231. Data availability This study mainly uses the Pomegranate Fruit Dataset: https://www.kaggle.com/datasets/kumararun37/pomegranate-fruit-dataset , Pomegranate Fruit Diseases [Image] the Dataset: https://www.kaggle.com/datasets/sujaykapadnis/pomegranate-fruit-diseases-dataset and Pomegranate Images the Dataset: https://data.mendeley.com/datasets/kgwsthf2w6/5 Declarations Ethics approval and consent to participate All authors agreed to publish this manuscript. Consent for publication Consent and approval for publication were obtained from all authors. Competing interests The authors declare no competing interests. References 1. Al Jaberi SM Patel A AL-Masri AN Object tracking and detection techniques under GANN threats: a systemic review Appl Soft Comput. 2023 139 110224 10.1016/j.asoc.2023.110224 Al Jaberi SM, Patel A, AL-Masri AN. Object tracking and detection techniques under GANN threats: a systemic review. Appl Soft Comput. 2023;139:110224. 2. Chandra R Babu KD Jadhav VT Origin, history and domestication of pomegranate Global Sci Books 2010 2 12 Chandra R, Babu KD, Jadhav VT. Origin, history and domestication of pomegranate. Global Sci Books. 2010;2:12. 3. Teixeira da Silva JA Rana TS Narzary D Pomegranate biology and biotechnology: a review Sci Hortic 2013 160 85 107 10.1016/j.scienta.2013.05.017 Teixeira da Silva JA, Rana TS, Narzary D, et al. Pomegranate biology and biotechnology: a review. Sci Hortic. 2013;160:85&#8211;107. 10.1016/j.scienta.2013.05.017. 4. Viuda-Martos M, Fern&#225;ndez-L&#243;pez J, P&#233;rez-&#193;lvarez JA. Pomegranate and its Many Functional Components as Related to Human Health: A Review. Comprehensive reviews in food science and food safety 2010;9(6). 10.1111/j.1541-4337.2010.00131.x, https://pubmed.ncbi.nlm.nih.gov/33467822/ , publisher: Compr Rev Food Sci Food Saf 10.1111/j.1541-4337.2010.00131.x 33467822 5. Hua X Li H Zeng J A review of target recognition technology for fruit picking robots: from digital image processing to deep learning Appl Sci 2023 13 7 4160 10.3390/app13074160 Hua X, Li H, Zeng J, et al. A review of target recognition technology for fruit picking robots: from digital image processing to deep learning. Appl Sci. 2023;13(7):4160. 10.3390/app13074160. 6. Lu W, Du R, Niu P, et al. Soybean Yield Preharvest Prediction Based on Bean Pods and Leaves Image Recognition Using Deep Learning Neural Network Combined With GRNN. Front Plant Sci. 2022;12. 10.3389/fpls.2021.791256 10.3389/fpls.2021.791256 PMC8792930 35095964 7. Ma R, Liu Y, Wu Q, et&#160;al. Deep Learning-based Visual Recognition System for Fruit-Picking Robots. In: 2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE International Conference on Robotics, Automation and Mechatronics (RAM), 2024;567&#8211;572, 10.1109/CIS-RAM61939.2024.10673388, https://ieeexplore.ieee.org/document/10673388 8. Cheng H Damerow L Sun Y Early yield prediction using image analysis of apple fruit and tree canopy features with neural networks J Imaging 2017 3 1 6 10.3390/jimaging3010006 Cheng H, Damerow L, Sun Y, et al. Early yield prediction using image analysis of apple fruit and tree canopy features with neural networks. J Imaging. 2017;3(1):6. 10.3390/jimaging3010006. 9. Zhang J, Yang W, Lu Z, et&#160;al. Hr-yolov8: A crop growth status object detection method based on yolov8. Electronics 2024;13(9). 10.3390/electronics13091620, https://www.mdpi.com/2079-9292/13/9/1620 10. Mane S, Bartakke P, Bastewad T. DetSSeg: A Selective On-Field Pomegranate Segmentation Approach. In: 2023 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI), 2023;1&#8211;6, 10.1109/CVMI59935.2023.10464563, https://ieeexplore.ieee.org/abstract/document/10464563 11. Devanna RP Milella A Marani R In-field automatic identification of pomegranates using a farmer robot Sensors 2022 22 15 5821 10.3390/s22155821 35957377 PMC9370860 Devanna RP, Milella A, Marani R, et al. In-field automatic identification of pomegranates using a farmer robot. Sensors. 2022;22(15):5821. 10.3390/s22155821. 35957377 10.3390/s22155821 PMC9370860 12. Liu MA MaO Kebiao Zhonghua GUO Defogging remote sensing images method based on a hybrid attention-based generative adversarial network Smart Agric 2025 7 2 172 10.12133/j.smartag.SA202410011 Liu MA, MaO Kebiao, Zhonghua GUO. Defogging remote sensing images method based on a hybrid attention-based generative adversarial network. Smart Agric. 2025;7(2):172. 10.12133/j.smartag.SA202410011. 13. Ding JT Peng YY Huang M AgriGAN: unpaired image dehazing via a cycle-consistent generative adversarial network for the agricultural plant phenotype Sci Rep 2024 14 1 14994 10.1038/s41598-024-65540-0 38951207 PMC11217274 Ding JT, Peng YY, Huang M, et al. AgriGAN: unpaired image dehazing via a cycle-consistent generative adversarial network for the agricultural plant phenotype. Sci Rep. 2024;14(1):14994. 10.1038/s41598-024-65540-0. 38951207 10.1038/s41598-024-65540-0 PMC11217274 14. Huang Y Qian Y Wei H A survey of deep learning-based object detection methods in crop counting Comput Electron Agric 2023 215 108425 10.1016/j.compag.2023.108425 Huang Y, Qian Y, Wei H, et al. A survey of deep learning-based object detection methods in crop counting. Comput Electron Agric. 2023;215:108425. 10.1016/j.compag.2023.108425. 15. Sun T Zhang W Gao X Efficient occlusion avoidance based on active deep sensing for harvesting robots Comput Electron Agric 2024 225 109360 10.1016/j.compag.2024.109360 Sun T, Zhang W, Gao X, et al. Efficient occlusion avoidance based on active deep sensing for harvesting robots. Comput Electron Agric. 2024;225:109360. 10.1016/j.compag.2024.109360. 16. Gong L Wang W Wang T Robotic harvesting of the occluded fruits with a precise shape and position reconstruction approach J Field Robotics 2022 39 1 69 84 10.1002/rob.22041 Gong L, Wang W, Wang T, et al. Robotic harvesting of the occluded fruits with a precise shape and position reconstruction approach. J Field Robotics. 2022;39(1):69&#8211;84. 10.1002/rob.22041. 17. Umirzakova S Muksimova S Shavkatovich Buriboev A A unified transformer model for simultaneous cotton boll detection, pest damage segmentation, and phenological stage classification from UAV imagery Drones 2025 9 8 555 10.3390/drones9080555 Umirzakova S, Muksimova S, Shavkatovich Buriboev A, et al. A unified transformer model for simultaneous cotton boll detection, pest damage segmentation, and phenological stage classification from UAV imagery. Drones. 2025;9(8):555. 10.3390/drones9080555. 18. Otsu N A threshold selection method from gray-level histograms IEEE Trans Syst Man Cybernet 1979 9 1 62 66 10.1109/TSMC.1979.4310076 Otsu N. A threshold selection method from gray-level histograms. IEEE Trans Syst Man Cybernet. 1979;9(1):62&#8211;6. 10.1109/TSMC.1979.4310076. 19. Dhanachandra N Manglem K Chanu YJ Image segmentation using K -means clustering algorithm and subtractive clustering algorithm Proc Computer Sci 2015 54 764 771 10.1016/j.procs.2015.06.090 Dhanachandra N, Manglem K, Chanu YJ. Image segmentation using K -means clustering algorithm and subtractive clustering algorithm. Proc Computer Sci. 2015;54:764&#8211;71. 10.1016/j.procs.2015.06.090. 20. Nock R Nielsen F Statistical region merging IEEE Trans Pattern Anal Machine Intell 2004 26 11 1452 1458 10.1109/TPAMI.2004.110 15521493 Nock R, Nielsen F. Statistical region merging. IEEE Trans Pattern Anal Machine Intell. 2004;26(11):1452&#8211;8. 10.1109/TPAMI.2004.110. 10.1109/TPAMI.2004.110 15521493 21. Cortes C Vapnik V Support-vector networks Mach Learn 1995 20 3 273 297 10.1007/BF00994018 Cortes C, Vapnik V. Support-vector networks. Mach Learn. 1995;20(3):273&#8211;97. 10.1007/BF00994018. 22. LeCun Y, Boser B, Denker J, et&#160;al (1989) Handwritten Digit Recognition with a Back-Propagation Network. In: Touretzky D (ed) Advances in Neural Information Processing Systems, vol&#160;2. Morgan-Kaufmann, https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf 23. Long J, Shelhamer E, Darrell T (2015) Fully Convolutional Networks for Semantic Segmentation. 10.48550/arXiv.1411.4038, http://arxiv.org/abs/1411.4038 , arXiv:1411.4038 10.1109/TPAMI.2016.2572683 27244717 24. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. 2015 10.48550/arXiv.1505.04597, http://arxiv.org/abs/1505.04597 , arXiv:1505.04597 [cs] 25. He K, Gkioxari G, Doll r P, et&#160;al. Mask R-CNN. 2018 10.48550/arXiv.1703.06870, http://arxiv.org/abs/1703.06870 , arXiv:1703.06870 26. Barnes C, Shechtman E, Finkelstein A, et al. PatchMatch. ACM Transactions on Graphics (TOG). 2009. 10.1145/1531326.1531330. 27. Efros A Leung T Texture synthesis by non-parametric sampling Proc Seventh IEEE Int Conf Computer Vision 1999 2 1033 1038 10.1109/ICCV.1999.790383 Efros A, Leung T. Texture synthesis by non-parametric sampling. Proc Seventh IEEE Int Conf Computer Vision. 1999;2:1033&#8211;8. 10.1109/ICCV.1999.790383. 28. Criminisi A Perez P Toyama K Region filling and object removal by exemplar-based image inpainting IEEE Trans Image Process 2004 13 9 1200 1212 10.1109/TIP.2004.833105 15449582 Criminisi A, Perez P, Toyama K. Region filling and object removal by exemplar-based image inpainting. IEEE Trans Image Process. 2004;13(9):1200&#8211;12. 10.1109/TIP.2004.833105. 15449582 10.1109/tip.2004.833105 29. Kingma DP, Welling M. Auto-Encoding Variational Bayes. 2013 10.48550/arXiv.1312.6114, http://arxiv.org/abs/1312.6114 , arXiv:1312.6114 [stat] 30. Zhang X Zhai D Li T Image inpainting based on deep learning: a review Inform Fusion 2023 90 74 94 10.1016/j.inffus.2022.08.033 Zhang X, Zhai D, Li T, et al. Image inpainting based on deep learning: a review. Inform Fusion. 2023;90:74&#8211;94. 10.1016/j.inffus.2022.08.033. 31. Goodfellow IJ, Pouget-Abadie J, Mirza M, et&#160;al. Generative Adversarial Networks. 2014 10.48550/arXiv.1406.2661, http://arxiv.org/abs/1406.2661 , arXiv:1406.2661 32. Kumar RA Rajpurohit VS Jirage BJ Pomegranate fruit quality assessment using machine intelligence and wavelet features J Horticul Res 2018 26 1 53 60 10.2478/johr-2018-0006 Kumar RA, Rajpurohit VS, Jirage BJ. Pomegranate fruit quality assessment using machine intelligence and wavelet features. J Horticul Res. 2018;26(1):53&#8211;60. 10.2478/johr-2018-0006. 33. Kumar RA Rajpurohit VS Bidari KY Multi class grading and quality assessment of pomegranate fruits based on physical and visual parameters Int J Fruit Sci 2019 19 4 372 396 10.1080/15538362.2018.1552230 Kumar RA, Rajpurohit VS, Bidari KY. Multi class grading and quality assessment of pomegranate fruits based on physical and visual parameters. Int J Fruit Sci. 2019;19(4):372&#8211;96. 10.1080/15538362.2018.1552230. 34. Pakruddin B Hemavathy R A comprehensive standardized dataset of numerous pomegranate fruit diseases for deep learning Data Brief. 2024 54 110284 10.1016/j.dib.2024.110284 38962206 PMC11220843 Pakruddin B, Hemavathy R. A comprehensive standardized dataset of numerous pomegranate fruit diseases for deep learning. Data Brief. 2024;54:110284. 10.1016/j.dib.2024.110284. 38962206 10.1016/j.dib.2024.110284 PMC11220843 35. Zhao J Du C Li Y YOLO-Granada: a lightweight attentioned Yolo for pomegranates fruit detection Sci Rep 2024 14 1 16848 10.1038/s41598-024-67526-4 39039263 PMC11263582 Zhao J, Du C, Li Y, et al. YOLO-Granada: a lightweight attentioned Yolo for pomegranates fruit detection. Sci Rep. 2024;14(1):16848. 10.1038/s41598-024-67526-4. 39039263 10.1038/s41598-024-67526-4 PMC11263582 36. Ho J, Jain A, Abbeel P. Denoising Diffusion Probabilistic Models. 2020 10.48550/arXiv.2006.11239, http://arxiv.org/abs/2006.11239 , arXiv:2006.11239 [cs] 37. Abdi M, Nahavandi S. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. 2017 10.48550/arXiv.1609.05672, arXiv:1609.05672 38. Wang Z Bovik A Sheikh H Image quality assessment: from error visibility to structural similarity IEEE Trans Image Process 2004 13 4 600 612 10.1109/TIP.2003.819861 15376593 Wang Z, Bovik A, Sheikh H, et al. Image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process. 2004;13(4):600&#8211;12. 10.1109/TIP.2003.819861. 15376593 10.1109/tip.2003.819861 39. Huynh-Thu Q Ghanbari M Scope of validity of PSNR in image/video quality assessment Electron Lett 2008 44 13 800 801 10.1049/el:20080522 Huynh-Thu Q, Ghanbari M. Scope of validity of PSNR in image/video quality assessment. Electron Lett. 2008;44(13):800&#8211;1. 10.1049/el:20080522. 40. Isola P, Zhu JY, Zhou T, et&#160;al. Image-to-Image Translation with Conditional Adversarial Networks. 2018 10.48550/arXiv.1611.07004, http://arxiv.org/abs/1611.07004 , arXiv:1611.07004 [cs] 41. Konz N, Chen Y, Dong H, et&#160;al. Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models. 2024 10.48550/arXiv.2402.05210, http://arxiv.org/abs/2402.05210 , arXiv:2402.05210 [eess]"
}