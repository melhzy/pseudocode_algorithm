{
  "pmcid": "PMC12660927",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:29.157262",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12660927",
    "pmid": "41309855",
    "doi": "10.1038/s41598-025-26480-5",
    "title": "DDNet: disaster damage detection for buildings based on dual-temporal joint attention network",
    "year": "2025",
    "month": "11",
    "day": "27",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "27"
    },
    "authors": [
      "Ge Xiaosan",
      "Zhou Lin",
      "Meng Di"
    ],
    "abstract": "Rapid and accurate assessment of building damage is crucial for effective post-disaster emergency response. The use of pre-disaster and post-disaster satellite imagery is a common approach for detecting building damage. This task involves two essential subtasks: building localization and damage classification. In building localization, the imbalance between buildings and background, along with low recall rates, often leads to boundary deviations, which negatively impact the accuracy of subsequent damage classification. In damage classification, features from both pre-disaster and post-disaster images, combined with localization results, are used; however, variations in imaging modalities and insufficient feature extraction from temporal images can introduce interference and reduce classification performance. To address these challenges, we propose a novel two-stage network, referred to as DDNet. In the first stage, the building localization network utilizes differential upsampling connections to enhance detailed feature acquisition and employs a unified focal loss to mitigate class imbalance between buildings and background, thereby balancing precision and recall. In the second stage, a joint attention module is introduced to effectively mine features from pre-disaster and post-disaster images, leading to improved classification accuracy. Finally, a connected component analysis algorithm is applied to convert pixel-level detection results into building-level damage outputs. On the xBD dataset, the proposed framework achieves a total F1 score of 79.56%, an F1 localization score of 86.38%, and an F1 damage classification score of 76.64%.",
    "keywords": [
      "Damage detection",
      "Dual-temporal satellite imagery",
      "Joint attention mechanism",
      "Imbalanced data classification",
      "Natural hazards",
      "Information technology"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12660927</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12660927.1</article-id><article-id pub-id-type=\"pmcaid\">12660927</article-id><article-id pub-id-type=\"pmcaiid\">12660927</article-id><article-id pub-id-type=\"pmid\">41309855</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-26480-5</article-id><article-id pub-id-type=\"publisher-id\">26480</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>DDNet: disaster damage detection for buildings based on dual-temporal joint attention network</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Ge</surname><given-names initials=\"X\">Xiaosan</given-names></name><address><email>gxs@hpu.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names initials=\"L\">Lin</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Meng</surname><given-names initials=\"D\">Di</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/05vr1c885</institution-id><institution-id institution-id-type=\"GRID\">grid.412097.9</institution-id><institution-id institution-id-type=\"ISNI\">0000 0000 8645 6375</institution-id><institution>School of Surveying and Land Information Engineering, </institution><institution>Henan Polytechnic University, </institution></institution-wrap>Jiaozuo, China </aff><aff id=\"Aff2\"><label>2</label>Hebi Polytechnic, Hebi, China </aff></contrib-group><pub-date pub-type=\"epub\"><day>27</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>42513</elocation-id><history><date date-type=\"received\"><day>12</day><month>3</month><year>2025</year></date><date date-type=\"accepted\"><day>29</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>29</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-01 11:25:13.003\"><day>01</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_26480.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Rapid and accurate assessment of building damage is crucial for effective post-disaster emergency response. The use of pre-disaster and post-disaster satellite imagery is a common approach for detecting building damage. This task involves two essential subtasks: building localization and damage classification. In building localization, the imbalance between buildings and background, along with low recall rates, often leads to boundary deviations, which negatively impact the accuracy of subsequent damage classification. In damage classification, features from both pre-disaster and post-disaster images, combined with localization results, are used; however, variations in imaging modalities and insufficient feature extraction from temporal images can introduce interference and reduce classification performance. To address these challenges, we propose a novel two-stage network, referred to as DDNet. In the first stage, the building localization network utilizes differential upsampling connections to enhance detailed feature acquisition and employs a unified focal loss to mitigate class imbalance between buildings and background, thereby balancing precision and recall. In the second stage, a joint attention module is introduced to effectively mine features from pre-disaster and post-disaster images, leading to improved classification accuracy. Finally, a connected component analysis algorithm is applied to convert pixel-level detection results into building-level damage outputs. On the xBD dataset, the proposed framework achieves a total F1 score of 79.56%, an F1 localization score of 86.38%, and an F1 damage classification score of 76.64%.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Damage detection</kwd><kwd>Dual-temporal satellite imagery</kwd><kwd>Joint attention mechanism</kwd><kwd>Imbalanced data classification</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Natural hazards</kwd><kwd>Information technology</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">Natural disasters with high destructive potential, including earthquakes, hurricanes, floods, and tsunamis, can result in significant loss of life and extensive infrastructure damage<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref>,<xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. In such contexts, the effectiveness of post-disaster emergency response becomes critically important. Rapid and accurate assessment of building damage is essential for emergency responders to capture the spatiotemporal characteristics of disaster impacts<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>.</p><p id=\"Par3\">Traditional on-site building damage surveys provide highly reliable information but are time-consuming and labor-intensive when applied over large areas. In contrast, remote sensing technology offers rapid, accurate, and large-scale observation capabilities<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup>. With the continuous advancement of remote sensing techniques, numerous methods for building damage detection have been developed<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR9\">9</xref></sup> Methods based on single post-disaster images face inherent limitations: without complete building outlines for comparison, these approaches often fail to accurately localize building areas, resulting in inevitable detection errors<sup><xref ref-type=\"bibr\" rid=\"CR10\">10</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>. Approaches that leverage both pre-disaster and post-disaster remote sensing images utilize pre-disaster imagery to identify changes in building areas and types across the image pairs. However, manually designed feature extraction methods for pre-disaster and post-disaster image changes often rely on empirical heuristics and struggle to capture the most discriminative and effective features.</p><p id=\"Par4\">Deep learning eliminates the reliance on hand-crafted image features, enabling automatic learning of representative features from low to high levels. Xu et al.<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref></sup> compared the performance of four different CNN architectures for detecting building damage caused by the Haitian earthquake. Presa-Reyes et al.<sup><xref ref-type=\"bibr\" rid=\"CR14\">14</xref></sup> employed a dual-branch CNN to establish correspondences between pre-disaster and post-disaster image features, categorizing building damage into four levels. Weber et al.<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup> treated building damage detection as a semantic segmentation task, in which a dual-branch CNN connected pre-disaster and post-disaster features in its final layer for building damage segmentation. UNet, a widely used CNN-based semantic segmentation network, introduces skip connections to the encoder-decoder structure, effectively integrating low- and high-level features and mitigating information loss during sampling. Due to these advantages, UNet has been extensively applied in remote sensing image processing tasks, including building extraction<sup><xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, road extraction<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>, change detection, and damage assessment<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup>. Despite these strengths, CNNs generally lack a global understanding of images and have limited ability to model non-local relationships between pixels. While UNet enhances feature representations through skip connections, it often introduces redundant low-level features. Hao et al.<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> incorporated a self-attention module into each branch of a dual-branch network to improve damage-level classification by considering information from the entire input image. Wu et al.<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> evaluated various backbone networks with different attention mechanisms and introduced attention gates to high-resolution feature maps connected via skip connections. These self-attention-based enhancement methods are effective at capturing global information; however, merely using attention to re-weight fused features across channels remains insufficient for modeling long-range dependencies. YOLOv8<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, designed to properly detect buildings and evaluate building damage levels in satellite pictures. YOLO, renowned for its real-time object detection capability, was employed in experiments using YOLOv8 Medium and YOLOv8 Nano on a customized annotated building dataset. The evaluation results are as follows: Faster R-CNN (with RESNET-50) achieved a validation accuracy of 82%, while YOLOv8 Medium and YOLOv8 Nano achieved validation accuracies of 54% and 52%, respectively<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup>. Sofia Tilon et al.<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup> utilized the xBD dataset, which contains pre-disaster and post-disaster satellite imagery across multiple disaster types, and proposed the use of a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN). The results indicate that, in the European region, the best-performing model achieved a recall of 0.59, a precision of 0.97, and an F1-score of 0.74.</p><p id=\"Par5\">The transformer architecture, introduced in 2017, has demonstrated outstanding performance in natural language processing. Equipped with a global self-attention mechanism, transformers possess strong capabilities for modeling long-range dependencies and global context. In numerous computer vision tasks, including image classification<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>, object detection, semantic segmentation<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup> and super-resolution<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup>, transformers achieve results comparable to or surpassing those of state-of-the-art CNNs. The remarkable performance of transformers, especially when combined with CNNs, has facilitated their successful application in change detection<sup><xref ref-type=\"bibr\" rid=\"CR31\">31</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR33\">33</xref></sup> and building damage assessment<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup>.</p><p id=\"Par6\">The objective of change detection is to assign binary labels&#8212;change or no change&#8212;to each pixel within a region by comparing co-registered images of the same area captured at different times<sup><xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>. In contrast, building damage detection typically involves comparing pre-disaster and post-disaster images to identify building locations and classify the severity of damage sustained. Damage detection must address two key tasks: building localization and building damage classification. Building localization aims to delineate building areas, whereas building damage classification focuses on categorizing the extent of damage based on the building locations. Change detection methods generally utilize both pre-disaster and post-disaster images for learning. However, post-disaster imagery can introduce interference, leading to suboptimal building localization. While change detection primarily identifies changed and unchanged regions, applying it directly to damage detection presents limitations. Pseudo-damage results derived from changed regions may include non-building objects such as fallen trees or damaged roads, while unchanged regions typically contain intact buildings. Therefore, directly employing a change detection network is insufficient to fully meet the requirements of building damage assessment.</p><p id=\"Par7\">Building damage detection can be divided into two subtasks: building localization and building damage classification. Building localization is performed to obtain accurate building boundary information. Cao et al.<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref></sup> proposed a method that combines remote sensing imagery with previously observed geographic features, transforming the damage detection task into a multiclass classification problem for known building locations. However, methods relying on known building positions face significant limitations in detecting building damage. Alternatively, unknown building localization methods utilize the complete architectural characteristics present in pre-disaster images to differentiate buildings from the background. This approach often results in an imbalance between positive and negative samples, which can introduce classification biases during model inference. Such biases manifest as both false positives (FPs) and false negatives (FNs). An FP occurs when background pixels are incorrectly classified as building pixels, leading to an overestimation of the building boundary. Conversely, an FN occurs when building pixels are misclassified as background, resulting in an underestimation of the building boundary.</p><p id=\"Par8\">To address the imbalance between positive and negative samples, Chen et al.<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup> introduced the Dice loss, which emphasizes the consistency between labels and predictions. The Dice loss generally outperforms the cross-entropy loss in scenarios with imbalanced positive and negative samples, as the latter treats all pixels equally. However, the Dice loss can be unstable, often resulting in incomplete training boundaries. Xie et al.<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup> proposed incorporating a contrastive loss along with a boundary loss to improve building localization. Nevertheless, the boundary loss suffers from similar instability as the Dice loss, and the benefits of the contrastive loss are limited. More recently, the unified focal loss<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>, was proposed, combining the strengths of the focal loss and the focal Tversky loss<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>. This approach balances recall and precision through the focal Tversky component while addressing class imbalance via both the focal loss and focal Tversky loss. Additionally, an offset parameter is employed to enhance positive sample learning and suppress background classification, thereby improving the recall rate of positive examples.</p><p id=\"Par9\">For building damage classification, the effective utilization of pre-disaster and post-disaster image information is a critical concern. Siam-U-Net-Attn-diff<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref></sup> performs subtractive fusion on dual-branch features, which can alleviate interference caused by irrelevant changes. However, simple subtraction reduces the distinctions between visually similar damage categories, such as no damage and slight damage, thereby increasing the difficulty of classification. Wu et al.<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> did not fully account for the correlations between dual-temporal images in their dual-branch feature design, which limits the network&#8217;s ability to focus on the true regions of interest. Deng and Wang<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup> introduced an attention mechanism to perform weighted fusion of pre-disaster and post-disaster images, but their approach did not consider interference from uncorrelated changes, such as meteorological variations, changes in illumination, or differences in imaging angles. Therefore, effective building damage classification requires not only exploiting the correlations between pre-disaster and post-disaster images but also suppressing interference from unrelated changes.</p><p id=\"Par10\">Building damage classification is derived from pixel-level damage detection. The damage experienced by each building can vary, with a single building potentially containing multiple damaged areas, each exhibiting different levels of severity. The primary goal of building damage detection is to obtain information regarding the extent of damage sustained by individual buildings, making it essential to optimize the classification results. The connected component analysis method functions as a voting algorithm, assigning unique labels to all pixels within each connected component (i.e., each object). This enables labeling of individual buildings based on pre-disaster images, analysis of the damaged areas within each building, and the assignment of a single damage level to each building.</p><p id=\"Par11\">In summary, existing methods for disaster damage detection still face several critical challenges in practical applications. First, building localization often suffers from insufficient accuracy, particularly for densely distributed or small-scale structures, where low recall rates and boundary inaccuracies are common. Second, variations between pre-disaster and post-disaster images&#8212;such as differences in illumination, season, weather, and imaging angles&#8212;introduce substantial non-disaster-related interference, thereby reducing the robustness of damage classification. Third, the distribution of damage categories exhibits a pronounced long-tail effect, leading to suppressed recognition performance for both minor and severe damage classes.</p><p id=\"Par12\">To address the aforementioned challenges, this paper proposes a two-stage building damage detection method that combines a CNN and a transformer, referred to as DDNet. First, for building localization, the TransUNet architecture is employed. A D-Cat module is incorporated into the skip connections to differentially enhance shallow and deep feature maps, reducing redundant information between layers while increasing the amount of detailed information acquired. The focal Dice loss and focal loss are combined to form a unified focal loss, and an offset parameter is applied to both building and background classes to obtain more accurate building boundaries and effective coverage areas. Second, for damage classification, a dual-branch skip network structure is utilized. A joint attention module is integrated into the feature interaction process to mine correlation features between pre-disaster and post-disaster images, thereby suppressing interference caused by irrelevant changes during imaging. Finally, connected component analysis is applied to improve the interpretability and readability of the building damage detection results.</p></sec><sec id=\"Sec2\"><title>Datasets and preprocessing</title><sec id=\"Sec3\"><title>xBD dataset</title><p id=\"Par13\">The training model in this study utilizes the xBD dataset<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup>, a large-scale benchmark dataset for building damage assessment. The dataset comprises 22,068 pairs of pre-disaster and post-disaster images collected from nineteen natural disaster events worldwide via the Maxar/Digital Globe Open program. To enhance the robustness and generalization capacity of the model, data augmentation techniques were employed. Specifically, random geometric transformations, including translation, rotation, and cropping, were applied to both pre-disaster and post-disaster images and their corresponding labels during training, thereby altering the position, orientation, and size of image pairs and label pairs. Additionally, random adjustments to brightness, contrast, saturation, and hue were performed on the attributes of pre-disaster and post-disaster images, accompanied by the random introduction of Gaussian noise. All images have undergone geometric correction and precise registration, ensuring high positional accuracy. Each image has a resolution of 1024&#8201;&#215;&#8201;1024 pixels with a ground sampling distance of 0.8&#160;m. The dataset spans a total area of 45,361.79 square kilometers and contains 850,736 annotated buildings. Damage annotations are categorized into four levels, defined as follows:<list list-type=\"bullet\"><list-item><p id=\"Par14\">No damage: The building remains undisturbed, with no signs of water, structural or roof damage, or burn marks.</p></list-item><list-item><p id=\"Par15\">Minor Damage: Partial roof or structural damage, nearby water or volcanic flow, missing roof elements, or visible cracks.</p></list-item><list-item><p id=\"Par16\">Major Damage: Partial collapse of walls or roof, encroaching volcanic flow, or surrounded by water/mud.</p></list-item><list-item><p id=\"Par17\">Destroyed: Scorched, completely collapsed, partially or fully submerged in water/mud, or otherwise no longer present.</p></list-item></list></p><p id=\"Par18\">All annotations were reviewed and corrected by the California Air National Guard, NASA, and the Federal Emergency Management Agency. Masks were generated based on the building annotations extracted from the xBD dataset JSON files. Figure&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref> presents a set of representative examples from the xBD dataset.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Examples of images from the xBD dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e359\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig1_HTML.jpg\"/></fig></p></sec><sec id=\"Sec4\"><title>Methods</title><sec id=\"Sec5\"><title>DDNet framework</title><p id=\"Par19\">Figure&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> shows that DDNet is a two-stage network model including building localization and damage classification subnetworks. The basic architecture contains an encoder-decoder structure, the skip connection between the encoder and the decoder retains details, and the encoder is equipped with a combined CNN-transformer structure.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>DDNet framework.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e375\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig2_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec6\"><title>Stage 1: Building localization</title><p id=\"Par20\">As illustrated in Fig.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref>a, pre-disaster buildings possess complete and well-defined outlines. Therefore, pre-disaster images are used as inputs to train the building localization network. At this stage, the encoder&#8217;s CNN layers extract features, which are further processed by 12 transformer encoder layers to obtain a deep feature map. The decoder progressively upsamples the features to the original image resolution, while the upsampled deep features and shallow features are differentially enhanced using the D-Cat module to retain detailed information. The final output of this network is a binary building segmentation map, representing the spatial distribution of buildings in the pre-disaster image. The formulation is given as follows:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e383\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{P}}_{\\text{B}}={\\text{argmax}}({\\text{P}}_{\\text{b}})$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq1\"><tex-math id=\"d33e388\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$P_{B} \\varepsilon \\{ 0,1\\}^{1 \\times H \\times W}$$\\end{document}</tex-math></inline-formula>, with 1 representing buildings and 0 representing the background, <italic toggle=\"yes\">P</italic><sub>b</sub>&#8201;&#8712;&#8201;R<sup>2&#215;H&#215;W</sup>, <inline-formula id=\"IEq2\"><tex-math id=\"d33e399\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${P}_{b}$$\\end{document}</tex-math></inline-formula> denotes the probability distribution indicating whether a pixel belongs to the *building* or *background* category.</p><sec id=\"Sec7\"><title>Loss function</title><p id=\"Par21\">Building localization provides spatial guidance for damage classification tasks, thereby improving the accuracy of damage detection. Consequently, building localization must achieve precise delineation of building boundaries and accurately determine effective area coverage. This task is formulated as a binary classification problem, where pixels are classified as either building or background. Since building pixels are far fewer than background pixels, an imbalance between positive and negative samples arises. Addressing this imbalance helps reduce the occurrence of false positives and false negatives, enabling more accurate boundary delineation.</p><p id=\"Par22\">The effective area coverage, defined as the proportion of buildings correctly recognized by the model, can be improved by increasing the recall rate. The unified focal loss<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup> integrates the characteristics of the focal loss and the focal Tversky loss<sup><xref ref-type=\"bibr\" rid=\"CR41\">41</xref></sup>, balancing recall and precision through the focal Tversky component while mitigating class imbalance via both the focal loss and the focal Tversky loss.</p><p id=\"Par23\">The focal loss incorporates two hyperparameters, &#945; and &#947;, into the binary cross-entropy loss. These parameters, referred to as focal parameters, control the class weighting and the degree of down-weighting for easily classified pixels, respectively. Specifically, &#945; adjusts the weight of the negative (background) class, while &#947; modulates the reduction of loss contribution from pixels that are easy to classify, as defined by the following formulation:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e419\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${L}_{FL}\\left(\\alpha ,\\gamma \\right)=\\alpha [{\\left(1-p\\right)}^{\\gamma }log\\left(p\\right)+{\\left(p\\right)}^{\\gamma }log\\left(1-p\\right)]$$\\end{document}</tex-math></disp-formula>where p denotes the predicted probability of a positive sample.</p><p id=\"Par24\">The focal Tversky loss modifies the standard Tversky loss by introducing the parameter &#947; , as follows:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e426\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${L}_{FTL}\\left(\\beta \\right)={{\\sum }_{C=1}^{C}(1-DSC)}^{\\frac{1}{\\gamma }}$$\\end{document}</tex-math></disp-formula>where DSC represents the F<sub>1</sub> score.</p><p id=\"Par25\">A simple weighted combination of the focal loss and focal Tversky loss introduces six hyperparameters, which is excessive. The enhancement or suppression effects of the focal parameters in both the focal loss and focal Tversky loss are applied uniformly across all classes, potentially affecting model convergence during the final stages of training. To address this, we suppress and enhance the effects of the focal loss and focal Tversky loss by combining functionally equivalent hyperparameters and introducing asymmetry to adjust the focal parameters. The focal loss is formally defined as follows:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e435\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${L}_{\\text{CFL}}=\\left[-\\frac{\\beta }{N}ylog\\left(p\\right)-\\frac{1-\\beta }{N}{\\left(1-y\\right)}^{\\gamma }log\\left(1-p\\right)\\right]$$\\end{document}</tex-math></disp-formula></p><p id=\"Par26\">Here, the weights of the building and background classes are controlled by &#946;, while &#947; suppresses the background class and enhances the building class in the focal loss. p represents the predicted class probability, and <italic toggle=\"yes\">y</italic> denotes the ground truth label.<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e444\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$L_{{{\\text{CFTL}}}} = \\sum _{{C = 1}}^{C} \\left( {1 - \\frac{{\\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{0i}} }}{{\\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{0i}} + \\beta \\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{1i}} + (1 - \\beta )\\sum\\limits_{{i = 1}}^{N} {p_{{1i}} } g_{{0i}} }}} \\right)^{{\\frac{1}{\\gamma }}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par27\">Here, &#945; balances recall and precision while enhancing the representation of the building class in the focal Tversky loss. We set &#945;&#8201;=&#8201;0.9 and &#946;&#8201;=&#8201;0.3. In this formulation, p denotes the predicted class probability, and g represents the ground truth.</p><p id=\"Par28\">The modified focal loss and the focal Tversky loss are integrated into a unified focal loss, which is defined as follows, where <inline-formula id=\"IEq3\"><tex-math id=\"d33e452\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mu$$\\end{document}</tex-math></inline-formula> is set to 0.5.<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e456\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{L}}_{\\text{loc}}\\left(\\uplambda ,\\upbeta ,\\upgamma \\right)=\\uplambda {\\text{L}}_{\\text{CFL}}+\\left(1-\\uplambda \\right){\\text{L}}_{\\text{CFTL}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par29\">In this way, we introduce the unified focal loss. The offset parameter is designed to enhance the building class while suppressing the background class, thereby improving the recall rate of building extraction through the adjustment of <inline-formula id=\"IEq4\"><tex-math id=\"d33e462\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\beta$$\\end{document}</tex-math></inline-formula>.</p></sec><sec id=\"Sec8\"><title>Transformer encoder</title><p id=\"Par30\">The transformer architecture serves as a global self-attention modeling framework for sequential data and provides an effective network structure for capturing long-range contextual dependencies. Originally developed for natural language processing (NLP), it has since been widely adopted in computer vision (CV). The architecture typically consists of a patch embedding layer followed by multiple encoder layers. Each encoder layer incorporates a multi-head self-attention (MHSA) mechanism, a multilayer perceptron (MLP) module, a normalization layer, and a residual connection. Prior to entering the encoder layers, the input image undergoes patch embedding, in which the image is partitioned into patches of a specified size, and the information within each patch is flattened into a vector sequence. To preserve spatial information, positional encodings are subsequently added to the sequence<sup><xref ref-type=\"bibr\" rid=\"CR42\">42</xref></sup>.</p><p id=\"Par31\">The resulting sequence of embedding vectors, as illustrated in Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>, is sequentially processed within the encoder layer, where it is first updated by the MHSA weights and subsequently transformed through dimensional linearization by the MLP. The final output is a feature representation map. MHSA constitutes the core of the transformer architecture and is composed of multiple independent attention heads. In single-head attention, a sequence of vectors is multiplied by initialized weight matrices to obtain the query (Q), key (K), and value (V) vectors. These vectors are computed as follows:<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Transformer encoder.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e485\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig3_HTML.jpg\"/></fig><disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e486\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{Attention}(\\text{Q},\\text{K},\\text{V})=\\text{softmax}(\\frac{{\\text{QK}}^{\\text{T}}}{\\sqrt{{\\text{d}}_{\\text{k}}}})\\text{V}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par32\">Let d<sub>k</sub> represent the dimensionality of the sequence k. The product of the query Q and key K is scaled by dividing it by the square root of d<sub>k</sub>, after which the resulting values are normalized using the softmax function. The normalized weights are then multiplied by the value V to produce the final output.</p><p id=\"Par33\">The Multi-Head Self-Attention (MHSA) mechanism is constructed by concatenating the outputs of multiple self-attention heads and applying a linear transformation, enabling the model to capture information from diverse representation subspaces:<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e498\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$MultiHead(Q,K,V)=Concat(hea{d}_{1}\\cdots hea{d}_{n}){W}^{O}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec9\"><title>Differential concatenation structure (D-Cat)</title><p id=\"Par34\">In the original skip connection structure, the deep feature map is upsampled and concatenated with the shallow feature map for fusion. This approach helps mitigate the loss of feature information caused by upsampling but often introduces considerable redundancy between the upsampled deep features and the shallow features. To address this issue, this paper proposes an upsampling-based differential splicing structure, illustrated in Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref>. In this structure, the upsampled deep feature map is subtracted from the corresponding shallow feature map to generate a differential feature map. This operation effectively removes redundant information between the two feature layers while supplementing missing information in the deep feature map, thereby enabling the network to more efficiently leverage the boundary information present in the shallow feature map. Subsequently, the upsampled deep feature map is concatenated with the differential feature map for further processing. Taking a single feature layer as an example, the feature map of this layer is upsampled to match the original image resolution corresponding to that layer.<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>The structure of D-Cat.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e515\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig4_HTML.jpg\"/></fig></p><p id=\"Par35\">This operation can be formally expressed by the following equation:<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e518\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM{\\prime}}_{i-1}=Upsample(Conv\\left({FM}_{i-1}\\right))$$\\end{document}</tex-math></disp-formula>where Upsample is a bilinear interpolation upsampling operation. Then, the feature maps of layers <inline-formula id=\"IEq5\"><tex-math id=\"d33e523\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM}_{i}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq6\"><tex-math id=\"d33e527\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM{\\prime}}_{i-1}$$\\end{document}</tex-math></inline-formula> are used to obtain the missing <inline-formula id=\"IEq7\"><tex-math id=\"d33e531\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM{\\prime}}_{i-1}$$\\end{document}</tex-math></inline-formula> information by subtraction. Finally, a concatenation operation is used to obtain the output feature <inline-formula id=\"IEq8\"><tex-math id=\"d33e535\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM}_{i-1}$$\\end{document}</tex-math></inline-formula>:<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e540\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${FM}_{iout}=Cat({FM{\\prime}}_{i-1},\\left({FM{\\prime}}_{i-1}-{FM}_{i}\\right))$$\\end{document}</tex-math></disp-formula></p></sec></sec><sec id=\"Sec10\"><title>Stage 2: Damage classification</title><p id=\"Par36\">Building damage classification aims to assess the extent of damage by comparing the changes in a building&#8217;s state before and after a disaster. As illustrated in Fig.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref>b, pre-disaster and post-disaster images are fed into a dual-branch network, from which multilevel feature maps are extracted via a CNN. To enhance the network&#8217;s focus on damage-relevant features and suppress interference caused by varying imaging conditions, dual-temporal joint attention is applied to the last three layers of the feature maps, modulating the interactions between pre-disaster and post-disaster features. Prior to producing the final outputs of the dual-branch network, the building localization segmentation results are incorporated as follows:<disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e551\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{P}}_{\\text{D}}=\\text{argmax}({\\text{P}}_{\\text{B}}\\cdot {\\text{P}}_{\\text{d}})$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq9\"><tex-math id=\"d33e556\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${P}_{d}\\in {R}^{C\\times H\\times W}$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq10\"><tex-math id=\"d33e560\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C$$\\end{document}</tex-math></inline-formula> denotes the number of damage levels, and <inline-formula id=\"IEq11\"><tex-math id=\"d33e564\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${P}_{B}\\in {R}^{H\\times W}$$\\end{document}</tex-math></inline-formula> is the result of building localization.</p><sec id=\"Sec11\"><title>Loss function</title><p id=\"Par37\">Building damage classification is inherently a long-tailed data detection problem. The distribution of annotations across the categories of no damage, minor damage, major damage, and destroyed buildings is presented in Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref>. Notably, the number of annotations for undamaged buildings is 3.18 times greater than that for the combined minor, major, and destroyed categories. For these tail categories, the quality of the classification fit significantly impacts the overall network accuracy. To address this challenge, this study employs a combination of the Combo loss<sup><xref ref-type=\"bibr\" rid=\"CR43\">43</xref></sup> and the Seesaw loss<sup><xref ref-type=\"bibr\" rid=\"CR44\">44</xref></sup>, incorporating single-label weighting to effectively handle the underrepresented classes:<disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e583\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{L}}_{{{\\text{dam}}}} \\left( {\\lambda_{{\\text{i}}} ,\\alpha } \\right) = \\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} \\lambda_{{\\text{i}}} {\\text{L}}_{{{\\text{CL}}}} + 2{\\text{L}}_{{{\\text{SL}}}} = \\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} \\lambda_{{\\text{i}}} \\left[ {\\left( {1 - {\\text{DSC}}} \\right) + \\left( {1 - {\\text{P}}_{{\\text{t}}} } \\right)^{\\alpha } {\\text{L}}_{{{\\text{BCE}}}} } \\right] + 2 - \\left[ {\\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} {\\text{w}}_{{{\\text{y}}_{{\\text{i}}} }} {\\text{L}}_{{{\\text{BCE}}}} \\left( {{\\text{p}}_{{\\text{i}}} ,{\\text{y}}_{{\\text{i}}} } \\right)} \\right]$$\\end{document}</tex-math></disp-formula>where the Combo loss and Seesaw loss are combined with a weighting ratio of 1:2. The single-label weighted Combo loss comprises Dice loss and focal loss, and is calculated separately for each label to enable the network to better address tail categories and mitigate competitive relationships among classes. Specifically, the weights for minor, major, and destroyed damage are set to 0.4, the major damage weight is set to 0.3, and the destroyed weight is set to 0.1, so <inline-formula id=\"IEq12\"><tex-math id=\"d33e588\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\upmu }_{\\text{i}}=[\\text{0.1,0.1,0.4,0.3},.0.1]$$\\end{document}</tex-math></inline-formula>. <inline-formula id=\"IEq13\"><tex-math id=\"d33e593\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\text{DSC}$$\\end{document}</tex-math></inline-formula> is the <inline-formula id=\"IEq14\"><tex-math id=\"d33e597\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1}$$\\end{document}</tex-math></inline-formula> score, <inline-formula id=\"IEq15\"><tex-math id=\"d33e601\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{p}}_{\\text{t}}$$\\end{document}</tex-math></inline-formula> is the intersection percentage between the model output and the label computation, and <inline-formula id=\"IEq16\"><tex-math id=\"d33e605\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\alpha$$\\end{document}</tex-math></inline-formula> is set to 2. <inline-formula id=\"IEq17\"><tex-math id=\"d33e609\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{L}}_{\\text{BCE}}$$\\end{document}</tex-math></inline-formula> is the cross-entropy loss function. The Seesaw loss dynamically adjusts the category weights by the number of examples. <inline-formula id=\"IEq18\"><tex-math id=\"d33e613\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{W}}_{({\\text{y}}_{\\text{i}})}$$\\end{document}</tex-math></inline-formula> denotes the weight adaptively updated based on both the sample frequency and the global sample mean.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>The distribution of the damaged buildings in the xBD dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Damage level</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">No damage</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Minor damage</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Major damage</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Destroyed</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Polygons</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">313,003</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">36,860</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">29,904</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">31,560</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ratio</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">8.47</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.00</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.93</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1.03</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec12\"><title>Dual-Temporal Joint Attention Module (DJA)</title><p id=\"Par38\">The acquisition of dual-temporal images is often subject to substantial interference arising from varying seasonal and meteorological conditions, as well as diverse imaging angles. Effective suppression of such interference is necessary prior to exploring correlations between features extracted from pre-disaster and post-disaster images. Given that bitemporal images contain a large amount of task-irrelevant information, it is crucial for the attention mechanism to accurately identify features indicative of actual building damage. In this study, the dual-temporal joint attention (DJA) module is implemented as a combination of self-attention and cross-temporal cross-attention. The self-attention operation is defined in Eq.&#160;<xref rid=\"Equ12\" ref-type=\"disp-formula\">12</xref>, while the cross-temporal cross-attention incorporates partial feature representations from the alternate branch and is computed as follows:<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e671\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$CrossAtt\\left( {Q_{a} ,K_{b} ,V_{b} } \\right) = Soft\\max \\left( {Q_{a} ,K_{b}^{T} c} \\right) \\cdot V_{b}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq19\"><tex-math id=\"d33e676\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Q}_{a}$$\\end{document}</tex-math></inline-formula>&#12289;<inline-formula id=\"IEq20\"><tex-math id=\"d33e680\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${K}_{a}$$\\end{document}</tex-math></inline-formula>&#12289;<inline-formula id=\"IEq21\"><tex-math id=\"d33e684\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${V}_{a}$$\\end{document}</tex-math></inline-formula>&#12289;<inline-formula id=\"IEq22\"><tex-math id=\"d33e689\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Q}_{b}$$\\end{document}</tex-math></inline-formula>&#12289;<inline-formula id=\"IEq23\"><tex-math id=\"d33e693\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${K}_{b}$$\\end{document}</tex-math></inline-formula>&#12289;<inline-formula id=\"IEq24\"><tex-math id=\"d33e697\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${V}_{b}$$\\end{document}</tex-math></inline-formula> are obtained by applying convolution operations to the pre-disaster and post-disaster images, respectively.</p><p id=\"Par39\">The distinction between cross-temporal cross-attention and the dual-temporal joint attention (DJA) module is as follows. Taking images A and B as examples, cross-temporal cross-attention uses the query vector Q of each image to learn the key vector K and value vector V from the same image. In contrast, the DJA module concatenates the Q vectors of both images while learning the K and V of the corresponding image. By integrating both the global self-attention of each image and the cross-attention between dual-temporal images, the proposed approach effectively mitigates interference arising from seasonal and meteorological variations as well as differing imaging angles. As a result, the network is able to focus on the features representing changes between pre-disaster and post-disaster conditions, thereby enabling more precise analysis. As illustrated in Fig.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, the matrix generation process of the pre-disaster branch is used as an example.The feature maps of the pre-disaster and post-disaster images are concatenated with Q through K and V, which are obtained via 1&#8201;&#215;&#8201;1 convolution. <inline-formula id=\"IEq25\"><tex-math id=\"d33e706\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${JA}_{1}$$\\end{document}</tex-math></inline-formula> is obtained by multiplying <inline-formula id=\"IEq26\"><tex-math id=\"d33e710\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Q}_{cat}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq27\"><tex-math id=\"d33e714\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${K}_{a}$$\\end{document}</tex-math></inline-formula> and then executing the softmax function. The DJA is calculated as follows:<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e718\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_{a} = DJA\\left( {\\left\\{ {Q_{a} ,Q_{b} } \\right\\},\\left\\{ {K_{a} ,V_{a} } \\right\\}} \\right) + input_{a} { = }Soft\\max \\left( {Concat\\left( {Q_{a} ,Q_{b} } \\right) \\bullet K_{a}^{T} } \\right) \\bullet V_{a} + input_{a}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq28\"><tex-math id=\"d33e724\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Q}_{a}$$\\end{document}</tex-math></inline-formula> ,<inline-formula id=\"IEq29\"><tex-math id=\"d33e728\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${K}_{a}$$\\end{document}</tex-math></inline-formula> ,<inline-formula id=\"IEq30\"><tex-math id=\"d33e732\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${V}_{a}$$\\end{document}</tex-math></inline-formula> ,<inline-formula id=\"IEq31\"><tex-math id=\"d33e736\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${Q}_{b}$$\\end{document}</tex-math></inline-formula> ,<inline-formula id=\"IEq32\"><tex-math id=\"d33e740\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${K}_{b}$$\\end{document}</tex-math></inline-formula> ,<inline-formula id=\"IEq33\"><tex-math id=\"d33e744\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${V}_{b}$$\\end{document}</tex-math></inline-formula> are the queries, keys, and values of images a and b obtained after the convolution. <inline-formula id=\"IEq34\"><tex-math id=\"d33e749\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${input}_{a}$$\\end{document}</tex-math></inline-formula> is the raw input.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>The structure of the DJA module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e759\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig5_HTML.jpg\"/></fig></p><p id=\"Par40\">The proposed Dual-Temporal Joint Attention (DJA) mechanism, while conceptually related to dual-branch attention fusion structures such as those in Siamese networks, differs substantially in implementation and objectives. Conventional cross-attention typically enables unidirectional branch-wise interactions, where the Query of one branch attends to the Key-Value of the other. In contrast, DJA integrates self-attention and cross-temporal cross-attention, concatenating Queries from pre- and post-disaster images for joint modeling. This design preserves temporal contextual consistency and more effectively captures cross-temporal variations.</p><p id=\"Par41\">Unlike general Siamese or cross-attention methods aimed at detecting the presence of change, DJA is tailored for post-disaster building damage detection. It not only distinguishes changed from unchanged regions but also discriminates among damage levels, suppressing pseudo-changes induced by illumination, weather, or viewpoint variations while highlighting structural features of real damage.</p><p id=\"Par42\">Furthermore, DJA is embedded only in the final three layers of the damage classification encoder, where high-level semantic features are both more sensitive to imaging discrepancies and more relevant to structural damage patterns. This strategy enhances robustness and task specificity while ensuring computational efficiency.</p></sec><sec id=\"Sec13\"><title>The connected component analysis algorithm</title><p id=\"Par43\">The semantic information within the spatial domain of each building must be unified, as inconsistencies in the representation of damaged buildings can hinder the effective use of the network&#8217;s output, despite the capability of deep learning architectures to detect building damage. Connected domain analysis is a widely used method for regional delineation of image pixels, in which regions composed of pixels with the same value and their neighbors are labeled to form a connected domain, enabling the analysis of the geometric semantics of each region. As illustrated in Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>, the process begins by calculating the number of pixels within the building damage area based on a connected domain analysis of the building location map. Subsequently, the same connected domain analysis is applied to the damage detection image to obtain the number of pixels corresponding to damage changes. Finally, by comparing the damage change area with the building area, the algorithm outputs the damage assessment result for each individual building.<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Illustration of the connected component analysis algorithm.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e779\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig6_HTML.jpg\"/></fig></p></sec></sec></sec><sec id=\"Sec14\"><title>Experiment and results</title><sec id=\"Sec15\"><title>Training details</title><p id=\"Par44\">The proposed two-stage model is implemented in Python 3.7 with PyTorch 1.10 and trained and tested on an NVIDIA Tesla V100 GPU. Due to GPU memory constraints, both the training and testing images were seamlessly cropped into 73,464 and 7464 image patches, respectively, each with a resolution of 512&#8201;&#215;&#8201;512 pixels. To enhance the model&#8217;s robustness and generalization, data augmentation techniques were applied, including random translations and rotations of pre-disaster and post-disaster images during training, as well as stochastic adjustments to brightness, contrast, saturation, hue, and Gaussian noise.The dataset was randomly split into training (80%), validation (10%), and test (10%) subsets to ensure model training, parameter tuning, and performance evaluation. Both the training and testing processes were initialized with pretrained weights. In the building localization phase, the model was trained with a learning rate of 0.01, a batch size of 8, and 100 epochs, whereas in the building classification phase, a learning rate of 0.01, a batch size of 12, and 100 epochs were used.</p></sec><sec id=\"Sec16\"><title>Metrics</title><p id=\"Par45\">Since the damage classification network relies on the outputs of the localization network to detect building damage in this experiment, the performance of the localization network is comprehensively evaluated using Precision, Recall, F1 score, Overall Accuracy (OA), and Intersection over Union (IoU). The calculation methods for these metrics are defined as follows:<disp-formula id=\"Equ15\"><label>15</label><tex-math id=\"d33e790\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Precision=\\frac{TP}{TP+FP}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ16\"><label>16</label><tex-math id=\"d33e794\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Recall=\\frac{TP}{TP+FN}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ17\"><label>17</label><tex-math id=\"d33e798\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$OA=\\frac{TP+TN}{TP+TN+FP+FN}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ18\"><label>18</label><tex-math id=\"d33e802\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$IoU=\\frac{TP}{TP+FP+FN}$$\\end{document}</tex-math></disp-formula><disp-formula id=\"Equ19\"><label>19</label><tex-math id=\"d33e806\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${F}_{1loc}=2\\frac{Precision*Recall}{Precision+Recall}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par46\">The damage classification network categorizes image pixels into four damage levels: no damage, minor damage, major damage, and destroyed. Accordingly, a corresponding metric value is computed for each category. The <inline-formula id=\"IEq35\"><tex-math id=\"d33e812\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document}</tex-math></inline-formula> metric across the four damage levels is computed using the harmonic mean3, which is defined as follows:<disp-formula id=\"Equ20\"><label>20</label><tex-math id=\"d33e816\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F_{1cls} = \\frac{4}{{\\frac{1}{{F_{1no dam} }} + \\frac{1}{{F_{1minor dam} }} + \\frac{1}{{F_{1major dam} }} + \\frac{1}{{F_{1destroyed} }}}}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par47\">The overall evaluation metric used in this study is the <inline-formula id=\"IEq36\"><tex-math id=\"d33e822\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{overall}}$$\\end{document}</tex-math></inline-formula> , which is calculated as the weighted average of the localization <inline-formula id=\"IEq37\"><tex-math id=\"d33e826\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{loc}}$$\\end{document}</tex-math></inline-formula> and the damage <inline-formula id=\"IEq38\"><tex-math id=\"d33e830\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document}</tex-math></inline-formula>. This metric is widely adopted for evaluating performance on the xBD dataset.<disp-formula id=\"Equ21\"><label>21</label><tex-math id=\"d33e834\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${F}_{1overall}{=0.3F}_{1loc}{+0.7F}_{1cls}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec17\"><title>Results</title><p id=\"Par48\">In the building localization stage, since most previous studies do not report specific evaluation metrics for building localization, we compare the performance of the trained classical semantic segmentation models with that of DDNet. Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> presents the comparative results. Our model achieves an F1-score of 0.8638. Compared with SCSE-ResNeXtUNet (SERes-UNet), DeepLabV3&#8201;+&#8201;<sup><xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup>, and U-Net, DDNet exhibits improvements of 0.0384, 0.0483, and 0.0644, respectively. Its overall accuracy is 0.0398 higher than that of the lowest-performing model, DeepLabV3&#8201;+&#8201;. Additionally, DDNet achieves recall improvements of 0.0384, 0.0560, and 0.0945, respectively, demonstrating that it reliably maintains high recall in building localization compared with other models.<table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Evaluation metrics of the building localization results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1loc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">IOU</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">OA</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">U-Net</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7994</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8617</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7455</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6658</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9803</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DeepLabV3&#8201;+&#8201;</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8155</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8371</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7952</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6886</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9810</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">SCSE-ResNext-UNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8254</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8384</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8128</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7027</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9819</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">TransUNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8576</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8741</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8417</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7507</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9853</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DDNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8638</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8769</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8512</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7603</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9858</td></tr></tbody></table></table-wrap></p><p id=\"Par49\">Figure&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref> presents a comparison of the extraction results for dense buildings, elongated buildings, and small buildings across the evaluated methods. As shown, DDNet is capable of capturing detailed representations of building rooftops and accurately delineating the boundaries between adjacent buildings. The proposed method demonstrates superior performance in accurately extracting small buildings and achieves the highest levels of continuity and smoothness when delineating elongated buildings.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Visualization of building localization results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e952\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig7_HTML.jpg\"/></fig></p><p id=\"Par50\">The subsequent damage detection subtask involves classifying the severity of damage sustained by individual buildings. The corresponding data comparison is presented in Table <xref rid=\"Tab3\" ref-type=\"table\">3</xref>, with evaluation primarily based on the <inline-formula id=\"IEq39\"><tex-math id=\"d33e958\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq40\"><tex-math id=\"d33e962\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document}</tex-math></inline-formula> metrics. Weber<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup> performs damage detection by integrating features through a change detection&#8211;based approach. Hao<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> integrated the U-Net model with a dual-branch CNN architecture, employing the U-Net to perform semantic segmentation of buildings, while the dual-branch network was utilized for damaged building classification. A self-attention module was introduced to enhance performance by incorporating long-range contextual information from the entire image. RescueNet<sup><xref ref-type=\"bibr\" rid=\"CR46\">46</xref></sup> adopts a dual-branch learning strategy based on pre-disaster and post-disaster images, where deep features are concatenated across multiple scales to enhance the information exchange between pre-disaster and post-disaster representations. ChangeOS<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup> employs the pre-disaster image for building localization and the post-disaster image for damage classification, incorporating post-processing to ensure consistency of damage detection results within building areas. DamFormer<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup> enhances building localization and damage classification by introducing cross-layer fusion within a combined CNN&#8211;transformer architecture. BSSNet<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup> incorporates contrastive loss and boundary loss into the building localization process, thereby improving the accuracy of building segmentation results.<table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table 3</label><caption><p>Overall building damage detection results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1overall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1loc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1cls</th><th align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq45\"><tex-math id=\"d33e1017\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{No damage}}$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq46\"><tex-math id=\"d33e1022\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Minor}}$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq47\"><tex-math id=\"d33e1027\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Major}}$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq48\"><tex-math id=\"d33e1032\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Destroyed}}$$\\end{document}</tex-math></inline-formula></th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Ethan Weber<sup><xref ref-type=\"bibr\" rid=\"CR15\">15</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7410</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8350</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7000</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9060</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.4930</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7220</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8370</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Hao<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7190</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7300</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7140</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9230</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.3980</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6750</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8120</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">RescueNet<sup><xref ref-type=\"bibr\" rid=\"CR46\">46</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7700</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8400</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7350</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8850</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5630</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7710</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8080</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">ChangeOS<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7857</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8541</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7564</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9266</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6014</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7418</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8345</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DAMFormer<sup><xref ref-type=\"bibr\" rid=\"CR34\">34</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7702</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8686</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7281</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8986</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5678</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7256</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8079</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">BSSNet<sup><xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7720</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8630</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7330</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9040</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5330</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7720</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8460</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">DDNet</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7956</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8638</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7664</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9314</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6052</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7542</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8567</td></tr></tbody></table></table-wrap></p><p id=\"Par51\">Weber exhibits lower performance in building damage classification, which can be attributed to its reliance on a change detection&#8211;based approach. Our method achieves the highest building damage classification performance, attaining the best <inline-formula id=\"IEq41\"><tex-math id=\"d33e1181\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq42\"><tex-math id=\"d33e1185\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document}</tex-math></inline-formula> among all compared methods. Specifically, the <inline-formula id=\"IEq43\"><tex-math id=\"d33e1189\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document}</tex-math></inline-formula> of our approach is 0.0100 higher than that of the second-best method, and its <inline-formula id=\"IEq44\"><tex-math id=\"d33e1193\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document}</tex-math></inline-formula> is 0.0099 higher than that of the second-best method. Nevertheless, our method has certain limitations. As shown in Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref>, major damage is frequently misclassified as either minor damage or destroyed, indicating that the model still struggles to fully address the imbalanced data distribution.</p><p id=\"Par52\">Figure&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref> presents the visualization of damage detection results generated by several models. DDNet produces clearer segmentation boundaries and reduces the number of misclassified damaged buildings. In the first image of Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>, the upper portion of the white building in the lower-left corner is significantly displaced, and the lower portion is partially damaged; ChangeOS<sup><xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup> classifies it as minor damage, whereas DDNet correctly identifies the damage level. In the third image, the building at the top is slightly damaged; ChangeOS classifies it as major damage, while DDNet correctly classifies it. Overall, DDNet demonstrates improved accuracy and effectiveness in building damage classification.<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Classifcation of building damage in xBD dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1218\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_26480_Fig8_HTML.jpg\"/></fig></p></sec><sec id=\"Sec18\"><title>Ablation study</title><sec id=\"Sec19\"><title>Differential concatenate (D-Cat)</title><p id=\"Par53\">Table <xref rid=\"Tab4\" ref-type=\"table\">4</xref> presents the performance improvements achieved by incorporating D-Cat into the localization network. The inclusion of D-Cat enhances both the accuracy and recall metrics, resulting in a 0.0032 increase in the F<sub>1loc</sub> score. When both D-Cat and O-UFL are integrated into the localization network, the F<sub>1loc</sub> score improves by 0.0062 compared with the baseline network without these enhancements.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table 4</label><caption><p>Ablation study on D-Cat and O-UFL.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1loc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Baseline</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8576</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8741</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8417</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Cat</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8608</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8781</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8442</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">O-UFL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8616</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8732</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8504</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">D-Cat&#8201;+&#8201;O-UFL</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8638</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8769</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8512</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec20\"><title>The offsetting focal parameters of the unified focal loss (O-UFL)</title><p id=\"Par54\">O-UFL incorporates three hyperparameters in total. Specifically, &#947; is empirically set to 0.3 to balance positive and negative examples, while &#956; is fixed at 0.5 following the formulation of Combo loss. Increasing &#946; leads to an improvement in recall; however, this gain is accompanied by a corresponding reduction in accuracy. In this study, three distinct values of &#946;&#8212;0.5, 0.7, and 0.9&#8212;are evaluated to investigate their influence on the performance of disaster-damaged building detection. The comparative results, summarized in Table <xref rid=\"Tab5\" ref-type=\"table\">5</xref>, demonstrate that setting &#946; to 0.9 effectively extends the acquisition range for buildings and enhances the overall performance of the network.<table-wrap id=\"Tab5\" position=\"float\" orientation=\"portrait\"><label>Table 5</label><caption><p>The impact of the hyperparameter <inline-formula id=\"IEq49\"><tex-math id=\"d33e1302\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\upbeta$$\\end{document}</tex-math></inline-formula>.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\"><inline-formula id=\"IEq50\"><tex-math id=\"d33e1316\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\upbeta$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1loc</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1cls</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.5</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8671</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8880</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8472</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7227</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.7</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8647</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8859</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8445</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7434</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.9</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8638</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8769</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8512</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7664</td></tr></tbody></table></table-wrap></p></sec><sec id=\"Sec21\"><title>DJA and the connected-component analysis algorithm</title><p id=\"Par55\">To evaluate the effectiveness of incorporating the dual-temporal joint attention (DJA) module for capturing correlations and extracting change features from pre-disaster and post-disaster images, this study compares the base two-branch model, the self-attention (SA) module, and the DJA module. The base structure refers to the unenhanced two-branch model. Table <xref rid=\"Tab6\" ref-type=\"table\">6</xref> presents a comparative analysis of the results. The inclusion of the DJA module improves accuracy across all damage detection categories, indicating that the joint attention mechanism effectively guides the network to learn informative features from both pre-disaster and post-disaster images. When both the DJA module and the connected component analysis algorithm are integrated into the classification network, the F<sub>1cls</sub> index increases by 1.17% relative to the base model.<table-wrap id=\"Tab6\" position=\"float\" orientation=\"portrait\"><label>Table 6</label><caption><p>Analysis of the DJA module and the algorithm for connected-component analysis.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Damage classification network</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1cls</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">No damage</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Minor</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Major</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Destroyed</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">base</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7547</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9238</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5988</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7416</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8338</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">base&#8201;+&#8201;SA</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7563</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9270</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.5983</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7443</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8369</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">base&#8201;+&#8201;DJA</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7620</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9306</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6072</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7470</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8408</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">base&#8201;+&#8201;DJA&#8201;+&#8201;connected-component</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7664</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.9314</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.6052</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.7542</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.8567</td></tr></tbody></table></table-wrap></p></sec></sec></sec><sec id=\"Sec22\"><title>Discussion</title><sec id=\"Sec23\"><title>Summary</title><p id=\"Par56\">In post-disaster scenarios, buildings are of critical concern, and the rapid and accurate extraction of damage-related building information from remote sensing images is essential for effective emergency response. This study employs deep learning techniques and utilizes high-resolution imagery from both pre-disaster and post-disaster phases to perform multi-task change detection, with a focus on identifying disaster-affected structures. To address challenges such as inaccurate boundary delineation, interference suppression, and the effective integration of features from pre-disaster and post-disaster data, this research emphasizes the tasks of building localization and damage classification. A two-stage DDNet model is proposed to tackle these challenges. The main findings and conclusions of this study are summarized as follows:</p><p id=\"Par57\">A novel two-stage DDNet model was proposed to address the inherent challenges in building localization and damaged building classification. By integrating CNN and Transformer architectures, the building localization phase overcomes the limitations of TransUNet through the use of a Unified Focal Loss function with offset focal parameters, thereby expanding the effective feature extraction region. In addition, a differential connection module was incorporated to enhance boundary delineation. In the damage classification phase, dual-temporal joint attention modules were employed to encode relevant features from both pre-disaster and post-disaster images while mitigating interference caused by variations in imaging conditions. Subsequently, buildings were segmented to identify damage zones, and a connected component analysis algorithm was applied to generate distinct damage assessment results. Compared with alternative methods, DDNet demonstrated superior performance on the xBD dataset, achieving an overall F1 score of 79.56%, with F1 scores of 86.38% for building localization and 76.64% for damaged building classification.</p></sec><sec id=\"Sec24\"><title>Challenges and future directions</title><p id=\"Par58\">The proposed models effectively address the challenge of extracting disaster-damaged buildings. Experimental results demonstrate that the DDNet model significantly improves extraction accuracy. Nevertheless, certain limitations persist, which warrant further investigation and discussion.</p><sec id=\"Sec25\"><title>Model improvement</title><p id=\"Par59\">Model performance often deteriorates in real-world applications. Although incremental and transfer learning approaches can improve accuracy through additional training, they incur substantial time costs. Moreover, the limited sample sizes of minor and severe damage classes, combined with the constraints of loss function&#8211;based methods, pose significant challenges. Future research should focus on enhancing model generalization and addressing the difficulties associated with these challenging damage categories.</p></sec><sec id=\"Sec26\"><title>Integration of multi-modal data</title><p id=\"Par60\">This study utilizes high-resolution RGB optical imagery, which captures rooftop details but lacks information regarding building facades and sides. As a result, the damage assessment is limited and potentially imprecise, restricting its ability to support a comprehensive evaluation. Future research could integrate multi-modal data sources, including SAR imagery, multi-view point clouds, and geographic information, to enable more detailed and holistic damage assessments. Furthermore, disasters often involve secondary events influenced by both natural and social factors, rendering static damage assessments insufficient. Subsequent investigations should focus on developing deep learning models for multi-temporal imagery to facilitate dynamic monitoring of disaster progression.</p></sec><sec id=\"Sec27\"><title>Limitations of two-dimensional imagery and the necessity of multimodal integration</title><p id=\"Par61\">It is important to note that buildings are inherently three-dimensional structures, and their damage states often involve multi-level structural features. The aerial and satellite optical imagery employed in this study primarily provides a two-dimensional perspective of rooftops, lacking critical fa&#231;ade and depth information. Such limitations in data naturally constrain the accuracy of multi-level damage detection and the granularity of classification, particularly for fine-grained or subtle damage patterns. Consequently, analyses relying solely on 2D imagery may not fully capture the complete damage state of buildings.</p><p id=\"Par62\">To partially mitigate these limitations, the proposed approach leverages cross-temporal feature modeling and regional consistency optimization. Specifically, the Dual-temporal Joint Attention (DJA) mechanism emphasizes semantic differences in texture, shape, and contour between pre- and post-disaster images, thereby capturing structural variations at the rooftop level and partially compensating for the absence of fa&#231;ade information. Additionally, connected-component analysis is applied during the classification stage to enforce regional consistency across entire building areas, which helps reduce prediction instability arising from missing local features or incomplete structural information. These strategies allow our 2D-based method to achieve improved localization and classification performance despite the intrinsic limitations of the input data.</p><p id=\"Par63\">Nevertheless, the fundamental constraints of two-dimensional optical imagery remain. Without integrating three-dimensional data sources&#8212;such as LiDAR point clouds, oblique aerial images, stereo image pairs, or photogrammetric reconstructions&#8212;the method cannot fully characterize the volumetric structure and nuanced damage patterns of buildings. Future research will focus on the fusion of multimodal and 3D spatial data with 2D cross-temporal feature modeling. By combining the complementary strengths of 2D and 3D information, we expect to enhance both the accuracy and granularity of post-disaster building damage detection, and to increase the overall reliability and interpretability of the results, particularly for complex or multi-level structural damages.</p></sec></sec></sec><sec id=\"Sec28\"><title>Conclusions</title><p id=\"Par64\">In this study, we propose DDNet for detecting buildings damaged by disasters. The localization network employs a hybrid architecture that integrates CNN and Transformer components, utilizing a Unified Focal Loss with offset focal parameters to expand the effective acquisition range. Additionally, a differential concatenation module is incorporated to enhance the extraction of building boundary information. To capture correlation features between pre-disaster and post-disaster images while mitigating interference caused by varying imaging conditions, the damage classification network is equipped with a dual-temporal joint attention module. Subsequently, buildings of interest are segmented to identify damaged regions, and a connected component analysis algorithm is applied to generate distinct damage assessment results. Compared with existing methods, DDNet demonstrates superior performance on the xBD dataset.</p><p id=\"Par65\">Future research will focus on further improving the model&#8217;s generalization capabilities and enhancing its detection accuracy based on the current findings. In addition, we plan to collect post-disaster visible-light aerial imagery and conduct building damage detection experiments using end-to-end models. This will further validate the applicability and performance differences of detection-based methods across different data modalities, and help establish a multi-modal disaster analysis framework spanning from visible-light detection to remote sensing segmentation.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>Conceptualization, L.Z. and XS.G.; methodology, L.Z.; writing&#8212;original draft preparation, L.Z.; writing&#8212;review and editing, XS.G.; visualization, L.Z.; data curation, D.M.; supervision, XS.G.; funding acquisition, XS.G. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>This research was funded by the Natural Science Foundation project of Henan Province, Grant Number 222300420450; National Natural Science Foundation of China, Grant Number 41572341.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The datasets used and/or analysed during the current study available from the corresponding author on reasonable request.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par66\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><citation-alternatives><element-citation id=\"ec-CR1\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Building damage assessment from post-hurricane imageries using unsupervised domain adaptation with enhanced feature discrimination</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>10</lpage></element-citation><mixed-citation id=\"mc-CR1\" publication-type=\"journal\">Lin, C. et al. Building damage assessment from post-hurricane imageries using unsupervised domain adaptation with enhanced feature discrimination. <italic toggle=\"yes\">IEEE Trans. Geosci. Remote Sens.</italic><bold>60</bold>, 1&#8211;10 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Deniz</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Arneson</surname><given-names>EE</given-names></name><name name-style=\"western\"><surname>Liel</surname><given-names>AB</given-names></name><etal/></person-group><article-title>Flood loss models for residential buildings, based on the 2013 Colorado floods</article-title><source>Nat. Hazards</source><year>2017</year><volume>85</volume><issue>2</issue><fpage>977</fpage><lpage>1003</lpage><pub-id pub-id-type=\"doi\">10.1007/s11069-016-2615-3</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Deniz, D. et al. Flood loss models for residential buildings, based on the 2013 Colorado floods. <italic toggle=\"yes\">Nat. Hazards</italic><bold>85</bold>(2), 977&#8211;1003 (2017).</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><mixed-citation publication-type=\"other\">Gupta, R,. Hosfelt, R., Sajeev, S., et al. xBD: A dataset for assessing building damage from satellite imagery. arXiv, 2019.</mixed-citation></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yusuf</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Matsuoka</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Yamazaki</surname><given-names>F</given-names></name></person-group><article-title>Damage assessment after 2001 Gujarat earthquake using Landsat-7 satellite images</article-title><source>J. Indian Soc. Remote Sens.</source><year>2001</year><volume>29</volume><issue>1&#8211;2</issue><fpage>17</fpage><lpage>22</lpage><pub-id pub-id-type=\"doi\">10.1007/BF02989909</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Yusuf, Y., Matsuoka, M. &amp; Yamazaki, F. Damage assessment after 2001 Gujarat earthquake using Landsat-7 satellite images. <italic toggle=\"yes\">J. Indian Soc. Remote Sens.</italic><bold>29</bold>(1&#8211;2), 17&#8211;22 (2001).</mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yamazaki</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Yano</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Matsuoka</surname><given-names>M</given-names></name></person-group><article-title>Visual damage interpretation of buildings in bam city using quick bird images following the 2003 Bam, Iran Earthquake</article-title><source>Earthq. Spectra</source><year>2005</year><volume>21</volume><fpage>S328</fpage><lpage>S336</lpage><pub-id pub-id-type=\"doi\">10.1193/1.2101807</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Yamazaki, F., Yano, Y. &amp; Matsuoka, M. Visual damage interpretation of buildings in bam city using quick bird images following the 2003 Bam, Iran Earthquake. <italic toggle=\"yes\">Earthq. Spectra</italic><bold>21</bold>, S328&#8211;S336 (2005).</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Tomowski, D., Ehlers, M., Klonus, S. Colour and texture based change detection for urban disaster analysis. In <italic toggle=\"yes\">2011 Joint Urban Remote Sensing Event. Munich, Germany: IEEE</italic>, 2011: 329-332</mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Xie, Z., Wang, M., Han, Y., et al. Hierarchical decision tree for change detection using high resolution remote sensing images. In Xie, Y., Zhang, A., Liu, H., et al. <italic toggle=\"yes\">Geo-informatics in Sustainable Ecosystem and Society</italic>: Vol. 980. Singapore: Springer Singapore, 2019: 176&#8211;184.</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wessels</surname><given-names>KJ</given-names></name><name name-style=\"western\"><surname>Van den Bergh</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Roy</surname><given-names>DP</given-names></name><etal/></person-group><article-title>Rapid land cover map updates using change detection and robust random forest classifiers</article-title><source>Remote Sens.</source><year>2016</year><volume>8</volume><issue>11</issue><fpage>888</fpage><pub-id pub-id-type=\"doi\">10.3390/rs8110888</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Wessels, K. J. et al. Rapid land cover map updates using change detection and robust random forest classifiers. <italic toggle=\"yes\">Remote Sens.</italic><bold>8</bold>(11), 888 (2016).</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mansouri</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Hamednia</surname><given-names>Y</given-names></name></person-group><article-title>A soft computing method for damage mapping using VHR optical satellite imagery</article-title><source>IEEE J. Selected Topics Appl. Earth Observ. Remote Sens.</source><year>2015</year><volume>8</volume><issue>10</issue><fpage>4935</fpage><lpage>4941</lpage><pub-id pub-id-type=\"doi\">10.1109/JSTARS.2015.2493342</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Mansouri, B. &amp; Hamednia, Y. A soft computing method for damage mapping using VHR optical satellite imagery. <italic toggle=\"yes\">IEEE J. Selected Topics Appl. Earth Observ. Remote Sens.</italic><bold>8</bold>(10), 4935&#8211;4941 (2015).</mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><citation-alternatives><element-citation id=\"ec-CR10\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nex</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Duarte</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Tonolo</surname><given-names>FG</given-names></name><etal/></person-group><article-title>Structural building damage detection with deep learning: Assessment of a state-of-the-Art CNN in operational conditions</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><issue>23</issue><fpage>2765</fpage><pub-id pub-id-type=\"doi\">10.3390/rs11232765</pub-id></element-citation><mixed-citation id=\"mc-CR10\" publication-type=\"journal\">Nex, F. et al. Structural building damage detection with deep learning: Assessment of a state-of-the-Art CNN in operational conditions. <italic toggle=\"yes\">Remote Sens.</italic><bold>11</bold>(23), 2765 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abdi</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Jabari</surname><given-names>S</given-names></name></person-group><article-title>A multi-feature fusion using deep transfer learning for earthquake building damage detection</article-title><source>Can. J. Remote. Sens.</source><year>2021</year><volume>47</volume><issue>2</issue><fpage>337</fpage><lpage>352</lpage><pub-id pub-id-type=\"doi\">10.1080/07038992.2021.1925530</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Abdi, G. &amp; Jabari, S. A multi-feature fusion using deep transfer learning for earthquake building damage detection. <italic toggle=\"yes\">Can. J. Remote. Sens.</italic><bold>47</bold>(2), 337&#8211;352 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>H</given-names></name><etal/></person-group><article-title>Building damage detection from post-event aerial imagery using single shot multibox detector</article-title><source>Appl. Sci.</source><year>2019</year><volume>9</volume><issue>6</issue><fpage>1128</fpage><pub-id pub-id-type=\"doi\">10.3390/app9061128</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Li, Y. et al. Building damage detection from post-event aerial imagery using single shot multibox detector. <italic toggle=\"yes\">Appl. Sci.</italic><bold>9</bold>(6), 1128 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><mixed-citation publication-type=\"other\">Xu, J. Z., Lu, W., Li, Z., et al. Building damage detection in satellite imagery using convolutional neural networks. arXiv, 2019.</mixed-citation></ref><ref id=\"CR14\"><label>14.</label><mixed-citation publication-type=\"other\">Presa-Reyes, M., Chen, S. C. Assessing building damage by learning the deep feature correspondence of before and after aerial images. In <italic toggle=\"yes\">2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)</italic>. Shenzhen, Guangdong, China: IEEE, 2020: 43&#8211;48.</mixed-citation></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Weber, E,. Kan&#233;, H. Building disaster damage assessment in satellite imagery with multi-temporal fusion. arXiv, 2020.</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Building extraction based on U-Net with an attention block and multiple losses</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><issue>9</issue><fpage>1400</fpage><pub-id pub-id-type=\"doi\">10.3390/rs12091400</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Guo, M. et al. Building extraction based on U-Net with an attention block and multiple losses. <italic toggle=\"yes\">Remote Sens.</italic><bold>12</bold>(9), 1400 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nie</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>An</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X</given-names></name><etal/></person-group><article-title>An improved U-Net network for sandy road extraction from remote sensing imagery</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><issue>20</issue><fpage>4899</fpage><pub-id pub-id-type=\"doi\">10.3390/rs15204899</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Nie, Y. et al. An improved U-Net network for sandy road extraction from remote sensing imagery. <italic toggle=\"yes\">Remote Sens.</italic><bold>15</bold>(20), 4899 (2023).</mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xiaosan</surname><given-names>GE</given-names></name><name name-style=\"western\"><surname>Xi</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wenzhi</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Detection of damaged buildings based on generative adversarial networks</article-title><source>Acta Geodaetica et artographica Sinica</source><year>2022</year><volume>51</volume><issue>2</issue><fpage>238</fpage><lpage>247</lpage></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Xiaosan, G. E. et al. Detection of damaged buildings based on generative adversarial networks. <italic toggle=\"yes\">Acta Geodaetica et artographica Sinica</italic><bold>51</bold>(2), 238&#8211;247 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>J</given-names></name><etal/></person-group><article-title>Building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><issue>5</issue><fpage>905</fpage><pub-id pub-id-type=\"doi\">10.3390/rs13050905</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Wu, C. et al. Building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets. <italic toggle=\"yes\">Remote Sens.</italic><bold>13</bold>(5), 905 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Deng</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>Post-disaster building damage assessment based on improved U-Net</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><issue>1</issue><fpage>15862</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-022-20114-w</pub-id><pub-id pub-id-type=\"pmid\">36151272</pub-id><pub-id pub-id-type=\"pmcid\">PMC9508235</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Deng, L. &amp; Wang, Y. Post-disaster building damage assessment based on improved U-Net. <italic toggle=\"yes\">Sci. Rep.</italic><bold>12</bold>(1), 15862 (2022).<pub-id pub-id-type=\"pmid\">36151272</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-022-20114-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC9508235</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Hao, H., Baireddy, S., Bartusiak, E. R., et al. An attention-based system for damage assessment using satellite imagery. In <italic toggle=\"yes\">2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</italic>. Brussels, Belgium: IEEE, 2021: 4396-4399</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><mixed-citation publication-type=\"other\">Yap, Y. L., Lim, S. L., Jatmiko, W., et al. Building detection from satellite images using deep learning. In <italic toggle=\"yes\">2024 Multimedia University Engineering Conference (MECON)</italic>. 2024: 1&#8211;7.</mixed-citation></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Pimpalkar, S., Madhavi, D. S., Rao, N. K. Performance analysis of a deep learning-based object detection approach for post-disaster building damage level assessment using YOLO and faster R-CNN. In Kalam, A., Mekhilef, S., Williamson S S. <italic toggle=\"yes\">Innovations in Electrical and Electronics Engineering</italic>. Singapore: Springer Nature, 2025: 167&#8211;183.</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tilon</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Nex</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Kerle</surname><given-names>N</given-names></name><etal/></person-group><article-title>Post-disaster building damage detection from earth observation imagery using unsupervised and transferable anomaly detecting generative adversarial networks</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><issue>24</issue><fpage>4193</fpage><pub-id pub-id-type=\"doi\">10.3390/rs12244193</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Tilon, S. et al. Post-disaster building damage detection from earth observation imagery using unsupervised and transferable anomaly detecting generative adversarial networks. <italic toggle=\"yes\">Remote Sens.</italic><bold>12</bold>(24), 4193 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\">Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is all you need. In Advances in Neural Information Processing Systems: Vol. 30. Curran Associates, Inc., 2017.</mixed-citation></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Zheng, S., Lu, J., Zhao, H., et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 2021: 6881&#8211;6890.</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><mixed-citation publication-type=\"other\">Carion, N., Massa, F., Synnaeve, G., et al. End-to-End object detection with transformers. In Vedaldi A, Bischof H, Brox T, et al. Computer vision&#8212;ECCV 2020. Cham: Springer International Publishing, 2020: 213&#8211;229.</mixed-citation></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Zheng, C., Cham, T. J., Cai, J., et al. Bridging global context interactions for high-fidelity image completion. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 2022: 11512&#8211;11522.</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><mixed-citation publication-type=\"other\">Liu Z, Lin Y, Cao Y, et al. Swin transformer: hierarchical vision transformer using shifted windows. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>. 2021: 10012&#8211;10022.</mixed-citation></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Chen, J., Lu, Y., Yu, Q., et al. TransUNet: Transformers make strong encoders for medical image segmentation. arXiv, 2021.</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>Remote sensing image change detection with transformers</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>14</lpage></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Chen, H., Qi, Z. &amp; Shi, Z. Remote sensing image change detection with transformers. <italic toggle=\"yes\">IEEE Trans. Geosci. Remote Sens.</italic><bold>60</bold>, 1&#8211;14 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Feng</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>J</given-names></name><etal/></person-group><article-title>ICIF-Net: Intra-scale cross-interaction and inter-scale feature fusion network for bitemporal remote sensing images change detection</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2022</year><volume>60</volume><fpage>1</fpage><lpage>13</lpage></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Feng, Y. et al. ICIF-Net: Intra-scale cross-interaction and inter-scale feature fusion network for bitemporal remote sensing images change detection. <italic toggle=\"yes\">IEEE Trans. Geosci. Remote Sens.</italic><bold>60</bold>, 1&#8211;13 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Lei</surname><given-names>T</given-names></name><etal/></person-group><article-title>MSTDSNet-CD: Multiscale swin transformer and deeply supervised network for change detection of the fast-growing urban regions</article-title><source>IEEE Geosci. Remote Sens. Lett.</source><year>2022</year><volume>19</volume><fpage>1</fpage><lpage>5</lpage></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Song, F. et al. MSTDSNet-CD: Multiscale swin transformer and deeply supervised network for change detection of the fast-growing urban regions. <italic toggle=\"yes\">IEEE Geosci. Remote Sens. Lett.</italic><bold>19</bold>, 1&#8211;5 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><mixed-citation publication-type=\"other\">Chen, H., Nemni, E., Vallecorsa, S., et al. Dual-tasks siamese transformer framework for building damage assessment. In <italic toggle=\"yes\">IGARSS 2022&#8211;2022 IEEE International Geoscience and Remote Sensing Symposium</italic>. 2022: 1600&#8211;1603.</mixed-citation></ref><ref id=\"CR35\"><label>35.</label><mixed-citation publication-type=\"other\">Ban, Y., Yousif, O,. Change detection techniques: A review. In Ban Y. Multitemporal remote sensing: methods and applications. Cham: Springer International Publishing, 2016: 19&#8211;43.</mixed-citation></ref><ref id=\"CR36\"><label>36.</label><citation-alternatives><element-citation id=\"ec-CR36\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cao</surname><given-names>QD</given-names></name><name name-style=\"western\"><surname>Choe</surname><given-names>Y</given-names></name></person-group><article-title>Posthurricane damage assessment using satellite imagery and geolocation features</article-title><source>Risk Anal.</source><year>2024</year><volume>44</volume><issue>5</issue><fpage>1103</fpage><lpage>1113</lpage><pub-id pub-id-type=\"doi\">10.1111/risa.14244</pub-id><pub-id pub-id-type=\"pmid\">37897045</pub-id></element-citation><mixed-citation id=\"mc-CR36\" publication-type=\"journal\">Cao, Q. D. &amp; Choe, Y. Posthurricane damage assessment using satellite imagery and geolocation features. <italic toggle=\"yes\">Risk Anal.</italic><bold>44</bold>(5), 1103&#8211;1113 (2024).<pub-id pub-id-type=\"pmid\">37897045</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1111/risa.14244</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR37\"><label>37.</label><citation-alternatives><element-citation id=\"ec-CR37\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>H</given-names></name><etal/></person-group><article-title>BSSNet: building subclass segmentation from satellite images using boundary guidance and contrastive learning</article-title><source>IEEE J. Selected Topics Appl. Earth Observ. Remote Sens.</source><year>2022</year><volume>15</volume><fpage>7700</fpage><lpage>7711</lpage><pub-id pub-id-type=\"doi\">10.1109/JSTARS.2022.3202524</pub-id></element-citation><mixed-citation id=\"mc-CR37\" publication-type=\"journal\">Xie, H. et al. BSSNet: building subclass segmentation from satellite images using boundary guidance and contrastive learning. <italic toggle=\"yes\">IEEE J. Selected Topics Appl. Earth Observ. Remote Sens.</italic><bold>15</bold>, 7700&#8211;7711 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR38\"><label>38.</label><citation-alternatives><element-citation id=\"ec-CR38\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yeung</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Sala</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Sch&#246;nlieb</surname><given-names>CB</given-names></name><etal/></person-group><article-title>Unified focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation</article-title><source>Comput. Med. Imaging Graph.</source><year>2022</year><volume>95</volume><fpage>102026</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compmedimag.2021.102026</pub-id><pub-id pub-id-type=\"pmid\">34953431</pub-id><pub-id pub-id-type=\"pmcid\">PMC8785124</pub-id></element-citation><mixed-citation id=\"mc-CR38\" publication-type=\"journal\">Yeung, M. et al. Unified focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. <italic toggle=\"yes\">Comput. Med. Imaging Graph.</italic><bold>95</bold>, 102026 (2022).<pub-id pub-id-type=\"pmid\">34953431</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compmedimag.2021.102026</pub-id><pub-id pub-id-type=\"pmcid\">PMC8785124</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR39\"><label>39.</label><mixed-citation publication-type=\"other\">Abraham, N., Khan, N. M. A novel focal tversky loss function with improved attention U-Net for lesion segmentation. In <italic toggle=\"yes\">2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</italic>. 2019: 683&#8211;687.</mixed-citation></ref><ref id=\"CR40\"><label>40.</label><mixed-citation publication-type=\"other\">Hao, H., Baireddy, S., Bartusiak, E. R., et al. An attention-based system for damage assessment using satellite imagery. In: 2021 IEEE international geoscience and remote sensing symposium IGARSS. 2021: 4396&#8211;4399.</mixed-citation></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Salehi, S. S. M., Erdogmus, D., Gholipour, A. Tversky loss function for image segmentation using 3D fully convolutional deep networks. In Wang Q, Shi Y, Suk H I, et al. Machine learning in medical imaging: Vol. 10541. Cham: Springer International Publishing, 2017: 379&#8211;387.</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><mixed-citation publication-type=\"other\">Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. An image is worth 16 &#215; 16 words: transformers for image recognition at scale. arXiv, 2021.</mixed-citation></ref><ref id=\"CR43\"><label>43.</label><citation-alternatives><element-citation id=\"ec-CR43\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Taghanaki</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>Zheng</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Kevin Zhou</surname><given-names>S</given-names></name><etal/></person-group><article-title>Combo loss: Handling input and output imbalance in multi-organ segmentation</article-title><source>Comput. Med. Imaging Graph.</source><year>2019</year><volume>75</volume><fpage>24</fpage><lpage>33</lpage><pub-id pub-id-type=\"doi\">10.1016/j.compmedimag.2019.04.005</pub-id><pub-id pub-id-type=\"pmid\">31129477</pub-id></element-citation><mixed-citation id=\"mc-CR43\" publication-type=\"journal\">Taghanaki, S. A. et al. Combo loss: Handling input and output imbalance in multi-organ segmentation. <italic toggle=\"yes\">Comput. Med. Imaging Graph.</italic><bold>75</bold>, 24&#8211;33 (2019).<pub-id pub-id-type=\"pmid\">31129477</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compmedimag.2019.04.005</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Wang, J., Zhang, W., Zang, Y., et al. Seesaw loss for long-tailed instance segmentation. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 2021: 9695&#8211;9704.</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><citation-alternatives><element-citation id=\"ec-CR45\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>H</given-names></name><etal/></person-group><article-title>A cucumber leaf disease severity classification method based on the fusion of DeepLabV3+ and U-Net</article-title><source>Comput. Electron. Agric.</source><year>2021</year><volume>189</volume><fpage>106373</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compag.2021.106373</pub-id></element-citation><mixed-citation id=\"mc-CR45\" publication-type=\"journal\">Wang, C. et al. A cucumber leaf disease severity classification method based on the fusion of DeepLabV3+ and U-Net. <italic toggle=\"yes\">Comput. Electron. Agric.</italic><bold>189</bold>, 106373 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR46\"><label>46.</label><mixed-citation publication-type=\"other\">Gupta, R., Shah, M. RescueNet: Joint building segmentation and damage assessment from satellite imagery. In <italic toggle=\"yes\">2020 25th International Conference on Pattern Recognition (ICPR)</italic>. 2021: 4405&#8211;4411.</mixed-citation></ref><ref id=\"CR47\"><label>47.</label><citation-alternatives><element-citation id=\"ec-CR47\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zheng</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhong</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters</article-title><source>Remote Sens. Environ.</source><year>2021</year><volume>265</volume><fpage>112636</fpage><pub-id pub-id-type=\"doi\">10.1016/j.rse.2021.112636</pub-id></element-citation><mixed-citation id=\"mc-CR47\" publication-type=\"journal\">Zheng, Z. et al. Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters. <italic toggle=\"yes\">Remote Sens. Environ.</italic><bold>265</bold>, 112636 (2021).</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12660927 PMC12660927.1 12660927 12660927 41309855 10.1038/s41598-025-26480-5 26480 1 Article DDNet: disaster damage detection for buildings based on dual-temporal joint attention network Ge Xiaosan gxs@hpu.edu.cn 1 Zhou Lin 1 2 Meng Di 1 1 https://ror.org/05vr1c885 grid.412097.9 0000 0000 8645 6375 School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China 2 Hebi Polytechnic, Hebi, China 27 11 2025 2025 15 478255 42513 12 3 2025 29 10 2025 27 11 2025 29 11 2025 01 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Rapid and accurate assessment of building damage is crucial for effective post-disaster emergency response. The use of pre-disaster and post-disaster satellite imagery is a common approach for detecting building damage. This task involves two essential subtasks: building localization and damage classification. In building localization, the imbalance between buildings and background, along with low recall rates, often leads to boundary deviations, which negatively impact the accuracy of subsequent damage classification. In damage classification, features from both pre-disaster and post-disaster images, combined with localization results, are used; however, variations in imaging modalities and insufficient feature extraction from temporal images can introduce interference and reduce classification performance. To address these challenges, we propose a novel two-stage network, referred to as DDNet. In the first stage, the building localization network utilizes differential upsampling connections to enhance detailed feature acquisition and employs a unified focal loss to mitigate class imbalance between buildings and background, thereby balancing precision and recall. In the second stage, a joint attention module is introduced to effectively mine features from pre-disaster and post-disaster images, leading to improved classification accuracy. Finally, a connected component analysis algorithm is applied to convert pixel-level detection results into building-level damage outputs. On the xBD dataset, the proposed framework achieves a total F1 score of 79.56%, an F1 localization score of 86.38%, and an F1 damage classification score of 76.64%. Keywords Damage detection Dual-temporal satellite imagery Joint attention mechanism Imbalanced data classification Subject terms Natural hazards Information technology pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Natural disasters with high destructive potential, including earthquakes, hurricanes, floods, and tsunamis, can result in significant loss of life and extensive infrastructure damage 1 , 2 . In such contexts, the effectiveness of post-disaster emergency response becomes critically important. Rapid and accurate assessment of building damage is essential for emergency responders to capture the spatiotemporal characteristics of disaster impacts 3 . Traditional on-site building damage surveys provide highly reliable information but are time-consuming and labor-intensive when applied over large areas. In contrast, remote sensing technology offers rapid, accurate, and large-scale observation capabilities 4 &#8211; 6 . With the continuous advancement of remote sensing techniques, numerous methods for building damage detection have been developed 7 &#8211; 9 Methods based on single post-disaster images face inherent limitations: without complete building outlines for comparison, these approaches often fail to accurately localize building areas, resulting in inevitable detection errors 10 &#8211; 12 . Approaches that leverage both pre-disaster and post-disaster remote sensing images utilize pre-disaster imagery to identify changes in building areas and types across the image pairs. However, manually designed feature extraction methods for pre-disaster and post-disaster image changes often rely on empirical heuristics and struggle to capture the most discriminative and effective features. Deep learning eliminates the reliance on hand-crafted image features, enabling automatic learning of representative features from low to high levels. Xu et al. 13 compared the performance of four different CNN architectures for detecting building damage caused by the Haitian earthquake. Presa-Reyes et al. 14 employed a dual-branch CNN to establish correspondences between pre-disaster and post-disaster image features, categorizing building damage into four levels. Weber et al. 15 treated building damage detection as a semantic segmentation task, in which a dual-branch CNN connected pre-disaster and post-disaster features in its final layer for building damage segmentation. UNet, a widely used CNN-based semantic segmentation network, introduces skip connections to the encoder-decoder structure, effectively integrating low- and high-level features and mitigating information loss during sampling. Due to these advantages, UNet has been extensively applied in remote sensing image processing tasks, including building extraction 16 , road extraction 17 , change detection, and damage assessment 18 &#8211; 20 . Despite these strengths, CNNs generally lack a global understanding of images and have limited ability to model non-local relationships between pixels. While UNet enhances feature representations through skip connections, it often introduces redundant low-level features. Hao et al. 21 incorporated a self-attention module into each branch of a dual-branch network to improve damage-level classification by considering information from the entire input image. Wu et al. 19 evaluated various backbone networks with different attention mechanisms and introduced attention gates to high-resolution feature maps connected via skip connections. These self-attention-based enhancement methods are effective at capturing global information; however, merely using attention to re-weight fused features across channels remains insufficient for modeling long-range dependencies. YOLOv8 22 , designed to properly detect buildings and evaluate building damage levels in satellite pictures. YOLO, renowned for its real-time object detection capability, was employed in experiments using YOLOv8 Medium and YOLOv8 Nano on a customized annotated building dataset. The evaluation results are as follows: Faster R-CNN (with RESNET-50) achieved a validation accuracy of 82%, while YOLOv8 Medium and YOLOv8 Nano achieved validation accuracies of 54% and 52%, respectively 23 . Sofia Tilon et al. 24 utilized the xBD dataset, which contains pre-disaster and post-disaster satellite imagery across multiple disaster types, and proposed the use of a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN). The results indicate that, in the European region, the best-performing model achieved a recall of 0.59, a precision of 0.97, and an F1-score of 0.74. The transformer architecture, introduced in 2017, has demonstrated outstanding performance in natural language processing. Equipped with a global self-attention mechanism, transformers possess strong capabilities for modeling long-range dependencies and global context. In numerous computer vision tasks, including image classification 25 , object detection, semantic segmentation 26 &#8211; 29 and super-resolution 30 , transformers achieve results comparable to or surpassing those of state-of-the-art CNNs. The remarkable performance of transformers, especially when combined with CNNs, has facilitated their successful application in change detection 31 &#8211; 33 and building damage assessment 34 . The objective of change detection is to assign binary labels&#8212;change or no change&#8212;to each pixel within a region by comparing co-registered images of the same area captured at different times 35 . In contrast, building damage detection typically involves comparing pre-disaster and post-disaster images to identify building locations and classify the severity of damage sustained. Damage detection must address two key tasks: building localization and building damage classification. Building localization aims to delineate building areas, whereas building damage classification focuses on categorizing the extent of damage based on the building locations. Change detection methods generally utilize both pre-disaster and post-disaster images for learning. However, post-disaster imagery can introduce interference, leading to suboptimal building localization. While change detection primarily identifies changed and unchanged regions, applying it directly to damage detection presents limitations. Pseudo-damage results derived from changed regions may include non-building objects such as fallen trees or damaged roads, while unchanged regions typically contain intact buildings. Therefore, directly employing a change detection network is insufficient to fully meet the requirements of building damage assessment. Building damage detection can be divided into two subtasks: building localization and building damage classification. Building localization is performed to obtain accurate building boundary information. Cao et al. 36 proposed a method that combines remote sensing imagery with previously observed geographic features, transforming the damage detection task into a multiclass classification problem for known building locations. However, methods relying on known building positions face significant limitations in detecting building damage. Alternatively, unknown building localization methods utilize the complete architectural characteristics present in pre-disaster images to differentiate buildings from the background. This approach often results in an imbalance between positive and negative samples, which can introduce classification biases during model inference. Such biases manifest as both false positives (FPs) and false negatives (FNs). An FP occurs when background pixels are incorrectly classified as building pixels, leading to an overestimation of the building boundary. Conversely, an FN occurs when building pixels are misclassified as background, resulting in an underestimation of the building boundary. To address the imbalance between positive and negative samples, Chen et al. 34 introduced the Dice loss, which emphasizes the consistency between labels and predictions. The Dice loss generally outperforms the cross-entropy loss in scenarios with imbalanced positive and negative samples, as the latter treats all pixels equally. However, the Dice loss can be unstable, often resulting in incomplete training boundaries. Xie et al. 37 proposed incorporating a contrastive loss along with a boundary loss to improve building localization. Nevertheless, the boundary loss suffers from similar instability as the Dice loss, and the benefits of the contrastive loss are limited. More recently, the unified focal loss 38 , was proposed, combining the strengths of the focal loss and the focal Tversky loss 39 . This approach balances recall and precision through the focal Tversky component while addressing class imbalance via both the focal loss and focal Tversky loss. Additionally, an offset parameter is employed to enhance positive sample learning and suppress background classification, thereby improving the recall rate of positive examples. For building damage classification, the effective utilization of pre-disaster and post-disaster image information is a critical concern. Siam-U-Net-Attn-diff 40 performs subtractive fusion on dual-branch features, which can alleviate interference caused by irrelevant changes. However, simple subtraction reduces the distinctions between visually similar damage categories, such as no damage and slight damage, thereby increasing the difficulty of classification. Wu et al. 19 did not fully account for the correlations between dual-temporal images in their dual-branch feature design, which limits the network&#8217;s ability to focus on the true regions of interest. Deng and Wang 20 introduced an attention mechanism to perform weighted fusion of pre-disaster and post-disaster images, but their approach did not consider interference from uncorrelated changes, such as meteorological variations, changes in illumination, or differences in imaging angles. Therefore, effective building damage classification requires not only exploiting the correlations between pre-disaster and post-disaster images but also suppressing interference from unrelated changes. Building damage classification is derived from pixel-level damage detection. The damage experienced by each building can vary, with a single building potentially containing multiple damaged areas, each exhibiting different levels of severity. The primary goal of building damage detection is to obtain information regarding the extent of damage sustained by individual buildings, making it essential to optimize the classification results. The connected component analysis method functions as a voting algorithm, assigning unique labels to all pixels within each connected component (i.e., each object). This enables labeling of individual buildings based on pre-disaster images, analysis of the damaged areas within each building, and the assignment of a single damage level to each building. In summary, existing methods for disaster damage detection still face several critical challenges in practical applications. First, building localization often suffers from insufficient accuracy, particularly for densely distributed or small-scale structures, where low recall rates and boundary inaccuracies are common. Second, variations between pre-disaster and post-disaster images&#8212;such as differences in illumination, season, weather, and imaging angles&#8212;introduce substantial non-disaster-related interference, thereby reducing the robustness of damage classification. Third, the distribution of damage categories exhibits a pronounced long-tail effect, leading to suppressed recognition performance for both minor and severe damage classes. To address the aforementioned challenges, this paper proposes a two-stage building damage detection method that combines a CNN and a transformer, referred to as DDNet. First, for building localization, the TransUNet architecture is employed. A D-Cat module is incorporated into the skip connections to differentially enhance shallow and deep feature maps, reducing redundant information between layers while increasing the amount of detailed information acquired. The focal Dice loss and focal loss are combined to form a unified focal loss, and an offset parameter is applied to both building and background classes to obtain more accurate building boundaries and effective coverage areas. Second, for damage classification, a dual-branch skip network structure is utilized. A joint attention module is integrated into the feature interaction process to mine correlation features between pre-disaster and post-disaster images, thereby suppressing interference caused by irrelevant changes during imaging. Finally, connected component analysis is applied to improve the interpretability and readability of the building damage detection results. Datasets and preprocessing xBD dataset The training model in this study utilizes the xBD dataset 3 , a large-scale benchmark dataset for building damage assessment. The dataset comprises 22,068 pairs of pre-disaster and post-disaster images collected from nineteen natural disaster events worldwide via the Maxar/Digital Globe Open program. To enhance the robustness and generalization capacity of the model, data augmentation techniques were employed. Specifically, random geometric transformations, including translation, rotation, and cropping, were applied to both pre-disaster and post-disaster images and their corresponding labels during training, thereby altering the position, orientation, and size of image pairs and label pairs. Additionally, random adjustments to brightness, contrast, saturation, and hue were performed on the attributes of pre-disaster and post-disaster images, accompanied by the random introduction of Gaussian noise. All images have undergone geometric correction and precise registration, ensuring high positional accuracy. Each image has a resolution of 1024&#8201;&#215;&#8201;1024 pixels with a ground sampling distance of 0.8&#160;m. The dataset spans a total area of 45,361.79 square kilometers and contains 850,736 annotated buildings. Damage annotations are categorized into four levels, defined as follows: No damage: The building remains undisturbed, with no signs of water, structural or roof damage, or burn marks. Minor Damage: Partial roof or structural damage, nearby water or volcanic flow, missing roof elements, or visible cracks. Major Damage: Partial collapse of walls or roof, encroaching volcanic flow, or surrounded by water/mud. Destroyed: Scorched, completely collapsed, partially or fully submerged in water/mud, or otherwise no longer present. All annotations were reviewed and corrected by the California Air National Guard, NASA, and the Federal Emergency Management Agency. Masks were generated based on the building annotations extracted from the xBD dataset JSON files. Figure&#160; 1 presents a set of representative examples from the xBD dataset. Fig. 1 Examples of images from the xBD dataset. Methods DDNet framework Figure&#160; 2 shows that DDNet is a two-stage network model including building localization and damage classification subnetworks. The basic architecture contains an encoder-decoder structure, the skip connection between the encoder and the decoder retains details, and the encoder is equipped with a combined CNN-transformer structure. Fig. 2 DDNet framework. Stage 1: Building localization As illustrated in Fig.&#160; 2 a, pre-disaster buildings possess complete and well-defined outlines. Therefore, pre-disaster images are used as inputs to train the building localization network. At this stage, the encoder&#8217;s CNN layers extract features, which are further processed by 12 transformer encoder layers to obtain a deep feature map. The decoder progressively upsamples the features to the original image resolution, while the upsampled deep features and shallow features are differentially enhanced using the D-Cat module to retain detailed information. The final output of this network is a binary building segmentation map, representing the spatial distribution of buildings in the pre-disaster image. The formulation is given as follows: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{P}}_{\\text{B}}={\\text{argmax}}({\\text{P}}_{\\text{b}})$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$P_{B} \\varepsilon \\{ 0,1\\}^{1 \\times H \\times W}$$\\end{document} , with 1 representing buildings and 0 representing the background, P b &#8201;&#8712;&#8201;R 2&#215;H&#215;W , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${P}_{b}$$\\end{document} denotes the probability distribution indicating whether a pixel belongs to the *building* or *background* category. Loss function Building localization provides spatial guidance for damage classification tasks, thereby improving the accuracy of damage detection. Consequently, building localization must achieve precise delineation of building boundaries and accurately determine effective area coverage. This task is formulated as a binary classification problem, where pixels are classified as either building or background. Since building pixels are far fewer than background pixels, an imbalance between positive and negative samples arises. Addressing this imbalance helps reduce the occurrence of false positives and false negatives, enabling more accurate boundary delineation. The effective area coverage, defined as the proportion of buildings correctly recognized by the model, can be improved by increasing the recall rate. The unified focal loss 38 integrates the characteristics of the focal loss and the focal Tversky loss 41 , balancing recall and precision through the focal Tversky component while mitigating class imbalance via both the focal loss and the focal Tversky loss. The focal loss incorporates two hyperparameters, &#945; and &#947;, into the binary cross-entropy loss. These parameters, referred to as focal parameters, control the class weighting and the degree of down-weighting for easily classified pixels, respectively. Specifically, &#945; adjusts the weight of the negative (background) class, while &#947; modulates the reduction of loss contribution from pixels that are easy to classify, as defined by the following formulation: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${L}_{FL}\\left(\\alpha ,\\gamma \\right)=\\alpha [{\\left(1-p\\right)}^{\\gamma }log\\left(p\\right)+{\\left(p\\right)}^{\\gamma }log\\left(1-p\\right)]$$\\end{document} where p denotes the predicted probability of a positive sample. The focal Tversky loss modifies the standard Tversky loss by introducing the parameter &#947; , as follows: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${L}_{FTL}\\left(\\beta \\right)={{\\sum }_{C=1}^{C}(1-DSC)}^{\\frac{1}{\\gamma }}$$\\end{document} where DSC represents the F 1 score. A simple weighted combination of the focal loss and focal Tversky loss introduces six hyperparameters, which is excessive. The enhancement or suppression effects of the focal parameters in both the focal loss and focal Tversky loss are applied uniformly across all classes, potentially affecting model convergence during the final stages of training. To address this, we suppress and enhance the effects of the focal loss and focal Tversky loss by combining functionally equivalent hyperparameters and introducing asymmetry to adjust the focal parameters. The focal loss is formally defined as follows: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${L}_{\\text{CFL}}=\\left[-\\frac{\\beta }{N}ylog\\left(p\\right)-\\frac{1-\\beta }{N}{\\left(1-y\\right)}^{\\gamma }log\\left(1-p\\right)\\right]$$\\end{document} Here, the weights of the building and background classes are controlled by &#946;, while &#947; suppresses the background class and enhances the building class in the focal loss. p represents the predicted class probability, and y denotes the ground truth label. 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L_{{{\\text{CFTL}}}} = \\sum _{{C = 1}}^{C} \\left( {1 - \\frac{{\\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{0i}} }}{{\\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{0i}} + \\beta \\sum\\limits_{{i = 1}}^{N} {p_{{0i}} } g_{{1i}} + (1 - \\beta )\\sum\\limits_{{i = 1}}^{N} {p_{{1i}} } g_{{0i}} }}} \\right)^{{\\frac{1}{\\gamma }}}$$\\end{document} Here, &#945; balances recall and precision while enhancing the representation of the building class in the focal Tversky loss. We set &#945;&#8201;=&#8201;0.9 and &#946;&#8201;=&#8201;0.3. In this formulation, p denotes the predicted class probability, and g represents the ground truth. The modified focal loss and the focal Tversky loss are integrated into a unified focal loss, which is defined as follows, where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mu$$\\end{document} is set to 0.5. 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{L}}_{\\text{loc}}\\left(\\uplambda ,\\upbeta ,\\upgamma \\right)=\\uplambda {\\text{L}}_{\\text{CFL}}+\\left(1-\\uplambda \\right){\\text{L}}_{\\text{CFTL}}$$\\end{document} In this way, we introduce the unified focal loss. The offset parameter is designed to enhance the building class while suppressing the background class, thereby improving the recall rate of building extraction through the adjustment of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta$$\\end{document} . Transformer encoder The transformer architecture serves as a global self-attention modeling framework for sequential data and provides an effective network structure for capturing long-range contextual dependencies. Originally developed for natural language processing (NLP), it has since been widely adopted in computer vision (CV). The architecture typically consists of a patch embedding layer followed by multiple encoder layers. Each encoder layer incorporates a multi-head self-attention (MHSA) mechanism, a multilayer perceptron (MLP) module, a normalization layer, and a residual connection. Prior to entering the encoder layers, the input image undergoes patch embedding, in which the image is partitioned into patches of a specified size, and the information within each patch is flattened into a vector sequence. To preserve spatial information, positional encodings are subsequently added to the sequence 42 . The resulting sequence of embedding vectors, as illustrated in Fig.&#160; 3 , is sequentially processed within the encoder layer, where it is first updated by the MHSA weights and subsequently transformed through dimensional linearization by the MLP. The final output is a feature representation map. MHSA constitutes the core of the transformer architecture and is composed of multiple independent attention heads. In single-head attention, a sequence of vectors is multiplied by initialized weight matrices to obtain the query (Q), key (K), and value (V) vectors. These vectors are computed as follows: Fig. 3 Transformer encoder. 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{Attention}(\\text{Q},\\text{K},\\text{V})=\\text{softmax}(\\frac{{\\text{QK}}^{\\text{T}}}{\\sqrt{{\\text{d}}_{\\text{k}}}})\\text{V}$$\\end{document} Let d k represent the dimensionality of the sequence k. The product of the query Q and key K is scaled by dividing it by the square root of d k , after which the resulting values are normalized using the softmax function. The normalized weights are then multiplied by the value V to produce the final output. The Multi-Head Self-Attention (MHSA) mechanism is constructed by concatenating the outputs of multiple self-attention heads and applying a linear transformation, enabling the model to capture information from diverse representation subspaces: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$MultiHead(Q,K,V)=Concat(hea{d}_{1}\\cdots hea{d}_{n}){W}^{O}$$\\end{document} Differential concatenation structure (D-Cat) In the original skip connection structure, the deep feature map is upsampled and concatenated with the shallow feature map for fusion. This approach helps mitigate the loss of feature information caused by upsampling but often introduces considerable redundancy between the upsampled deep features and the shallow features. To address this issue, this paper proposes an upsampling-based differential splicing structure, illustrated in Fig.&#160; 4 . In this structure, the upsampled deep feature map is subtracted from the corresponding shallow feature map to generate a differential feature map. This operation effectively removes redundant information between the two feature layers while supplementing missing information in the deep feature map, thereby enabling the network to more efficiently leverage the boundary information present in the shallow feature map. Subsequently, the upsampled deep feature map is concatenated with the differential feature map for further processing. Taking a single feature layer as an example, the feature map of this layer is upsampled to match the original image resolution corresponding to that layer. Fig. 4 The structure of D-Cat. This operation can be formally expressed by the following equation: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM{\\prime}}_{i-1}=Upsample(Conv\\left({FM}_{i-1}\\right))$$\\end{document} where Upsample is a bilinear interpolation upsampling operation. Then, the feature maps of layers \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM}_{i}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM{\\prime}}_{i-1}$$\\end{document} are used to obtain the missing \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM{\\prime}}_{i-1}$$\\end{document} information by subtraction. Finally, a concatenation operation is used to obtain the output feature \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM}_{i-1}$$\\end{document} : 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${FM}_{iout}=Cat({FM{\\prime}}_{i-1},\\left({FM{\\prime}}_{i-1}-{FM}_{i}\\right))$$\\end{document} Stage 2: Damage classification Building damage classification aims to assess the extent of damage by comparing the changes in a building&#8217;s state before and after a disaster. As illustrated in Fig.&#160; 2 b, pre-disaster and post-disaster images are fed into a dual-branch network, from which multilevel feature maps are extracted via a CNN. To enhance the network&#8217;s focus on damage-relevant features and suppress interference caused by varying imaging conditions, dual-temporal joint attention is applied to the last three layers of the feature maps, modulating the interactions between pre-disaster and post-disaster features. Prior to producing the final outputs of the dual-branch network, the building localization segmentation results are incorporated as follows: 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{P}}_{\\text{D}}=\\text{argmax}({\\text{P}}_{\\text{B}}\\cdot {\\text{P}}_{\\text{d}})$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${P}_{d}\\in {R}^{C\\times H\\times W}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C$$\\end{document} denotes the number of damage levels, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${P}_{B}\\in {R}^{H\\times W}$$\\end{document} is the result of building localization. Loss function Building damage classification is inherently a long-tailed data detection problem. The distribution of annotations across the categories of no damage, minor damage, major damage, and destroyed buildings is presented in Table 1 . Notably, the number of annotations for undamaged buildings is 3.18 times greater than that for the combined minor, major, and destroyed categories. For these tail categories, the quality of the classification fit significantly impacts the overall network accuracy. To address this challenge, this study employs a combination of the Combo loss 43 and the Seesaw loss 44 , incorporating single-label weighting to effectively handle the underrepresented classes: 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{L}}_{{{\\text{dam}}}} \\left( {\\lambda_{{\\text{i}}} ,\\alpha } \\right) = \\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} \\lambda_{{\\text{i}}} {\\text{L}}_{{{\\text{CL}}}} + 2{\\text{L}}_{{{\\text{SL}}}} = \\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} \\lambda_{{\\text{i}}} \\left[ {\\left( {1 - {\\text{DSC}}} \\right) + \\left( {1 - {\\text{P}}_{{\\text{t}}} } \\right)^{\\alpha } {\\text{L}}_{{{\\text{BCE}}}} } \\right] + 2 - \\left[ {\\mathop \\sum \\limits_{{{\\text{i}} = 0}}^{4} {\\text{w}}_{{{\\text{y}}_{{\\text{i}}} }} {\\text{L}}_{{{\\text{BCE}}}} \\left( {{\\text{p}}_{{\\text{i}}} ,{\\text{y}}_{{\\text{i}}} } \\right)} \\right]$$\\end{document} where the Combo loss and Seesaw loss are combined with a weighting ratio of 1:2. The single-label weighted Combo loss comprises Dice loss and focal loss, and is calculated separately for each label to enable the network to better address tail categories and mitigate competitive relationships among classes. Specifically, the weights for minor, major, and destroyed damage are set to 0.4, the major damage weight is set to 0.3, and the destroyed weight is set to 0.1, so \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\upmu }_{\\text{i}}=[\\text{0.1,0.1,0.4,0.3},.0.1]$$\\end{document} . \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text{DSC}$$\\end{document} is the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1}$$\\end{document} score, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{p}}_{\\text{t}}$$\\end{document} is the intersection percentage between the model output and the label computation, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\alpha$$\\end{document} is set to 2. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{L}}_{\\text{BCE}}$$\\end{document} is the cross-entropy loss function. The Seesaw loss dynamically adjusts the category weights by the number of examples. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{W}}_{({\\text{y}}_{\\text{i}})}$$\\end{document} denotes the weight adaptively updated based on both the sample frequency and the global sample mean. Table 1 The distribution of the damaged buildings in the xBD dataset. Damage level No damage Minor damage Major damage Destroyed Polygons 313,003 36,860 29,904 31,560 Ratio 8.47 1.00 0.93 1.03 Dual-Temporal Joint Attention Module (DJA) The acquisition of dual-temporal images is often subject to substantial interference arising from varying seasonal and meteorological conditions, as well as diverse imaging angles. Effective suppression of such interference is necessary prior to exploring correlations between features extracted from pre-disaster and post-disaster images. Given that bitemporal images contain a large amount of task-irrelevant information, it is crucial for the attention mechanism to accurately identify features indicative of actual building damage. In this study, the dual-temporal joint attention (DJA) module is implemented as a combination of self-attention and cross-temporal cross-attention. The self-attention operation is defined in Eq.&#160; 12 , while the cross-temporal cross-attention incorporates partial feature representations from the alternate branch and is computed as follows: 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$CrossAtt\\left( {Q_{a} ,K_{b} ,V_{b} } \\right) = Soft\\max \\left( {Q_{a} ,K_{b}^{T} c} \\right) \\cdot V_{b}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${Q}_{a}$$\\end{document} &#12289; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${K}_{a}$$\\end{document} &#12289; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${V}_{a}$$\\end{document} &#12289; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${Q}_{b}$$\\end{document} &#12289; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${K}_{b}$$\\end{document} &#12289; \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${V}_{b}$$\\end{document} are obtained by applying convolution operations to the pre-disaster and post-disaster images, respectively. The distinction between cross-temporal cross-attention and the dual-temporal joint attention (DJA) module is as follows. Taking images A and B as examples, cross-temporal cross-attention uses the query vector Q of each image to learn the key vector K and value vector V from the same image. In contrast, the DJA module concatenates the Q vectors of both images while learning the K and V of the corresponding image. By integrating both the global self-attention of each image and the cross-attention between dual-temporal images, the proposed approach effectively mitigates interference arising from seasonal and meteorological variations as well as differing imaging angles. As a result, the network is able to focus on the features representing changes between pre-disaster and post-disaster conditions, thereby enabling more precise analysis. As illustrated in Fig.&#160; 5 , the matrix generation process of the pre-disaster branch is used as an example.The feature maps of the pre-disaster and post-disaster images are concatenated with Q through K and V, which are obtained via 1&#8201;&#215;&#8201;1 convolution. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${JA}_{1}$$\\end{document} is obtained by multiplying \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${Q}_{cat}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${K}_{a}$$\\end{document} and then executing the softmax function. The DJA is calculated as follows: 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_{a} = DJA\\left( {\\left\\{ {Q_{a} ,Q_{b} } \\right\\},\\left\\{ {K_{a} ,V_{a} } \\right\\}} \\right) + input_{a} { = }Soft\\max \\left( {Concat\\left( {Q_{a} ,Q_{b} } \\right) \\bullet K_{a}^{T} } \\right) \\bullet V_{a} + input_{a}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${Q}_{a}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${K}_{a}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${V}_{a}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${Q}_{b}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${K}_{b}$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${V}_{b}$$\\end{document} are the queries, keys, and values of images a and b obtained after the convolution. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${input}_{a}$$\\end{document} is the raw input. Fig. 5 The structure of the DJA module. The proposed Dual-Temporal Joint Attention (DJA) mechanism, while conceptually related to dual-branch attention fusion structures such as those in Siamese networks, differs substantially in implementation and objectives. Conventional cross-attention typically enables unidirectional branch-wise interactions, where the Query of one branch attends to the Key-Value of the other. In contrast, DJA integrates self-attention and cross-temporal cross-attention, concatenating Queries from pre- and post-disaster images for joint modeling. This design preserves temporal contextual consistency and more effectively captures cross-temporal variations. Unlike general Siamese or cross-attention methods aimed at detecting the presence of change, DJA is tailored for post-disaster building damage detection. It not only distinguishes changed from unchanged regions but also discriminates among damage levels, suppressing pseudo-changes induced by illumination, weather, or viewpoint variations while highlighting structural features of real damage. Furthermore, DJA is embedded only in the final three layers of the damage classification encoder, where high-level semantic features are both more sensitive to imaging discrepancies and more relevant to structural damage patterns. This strategy enhances robustness and task specificity while ensuring computational efficiency. The connected component analysis algorithm The semantic information within the spatial domain of each building must be unified, as inconsistencies in the representation of damaged buildings can hinder the effective use of the network&#8217;s output, despite the capability of deep learning architectures to detect building damage. Connected domain analysis is a widely used method for regional delineation of image pixels, in which regions composed of pixels with the same value and their neighbors are labeled to form a connected domain, enabling the analysis of the geometric semantics of each region. As illustrated in Fig.&#160; 6 , the process begins by calculating the number of pixels within the building damage area based on a connected domain analysis of the building location map. Subsequently, the same connected domain analysis is applied to the damage detection image to obtain the number of pixels corresponding to damage changes. Finally, by comparing the damage change area with the building area, the algorithm outputs the damage assessment result for each individual building. Fig. 6 Illustration of the connected component analysis algorithm. Experiment and results Training details The proposed two-stage model is implemented in Python 3.7 with PyTorch 1.10 and trained and tested on an NVIDIA Tesla V100 GPU. Due to GPU memory constraints, both the training and testing images were seamlessly cropped into 73,464 and 7464 image patches, respectively, each with a resolution of 512&#8201;&#215;&#8201;512 pixels. To enhance the model&#8217;s robustness and generalization, data augmentation techniques were applied, including random translations and rotations of pre-disaster and post-disaster images during training, as well as stochastic adjustments to brightness, contrast, saturation, hue, and Gaussian noise.The dataset was randomly split into training (80%), validation (10%), and test (10%) subsets to ensure model training, parameter tuning, and performance evaluation. Both the training and testing processes were initialized with pretrained weights. In the building localization phase, the model was trained with a learning rate of 0.01, a batch size of 8, and 100 epochs, whereas in the building classification phase, a learning rate of 0.01, a batch size of 12, and 100 epochs were used. Metrics Since the damage classification network relies on the outputs of the localization network to detect building damage in this experiment, the performance of the localization network is comprehensively evaluated using Precision, Recall, F1 score, Overall Accuracy (OA), and Intersection over Union (IoU). The calculation methods for these metrics are defined as follows: 15 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Precision=\\frac{TP}{TP+FP}$$\\end{document} 16 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Recall=\\frac{TP}{TP+FN}$$\\end{document} 17 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$OA=\\frac{TP+TN}{TP+TN+FP+FN}$$\\end{document} 18 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$IoU=\\frac{TP}{TP+FP+FN}$$\\end{document} 19 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${F}_{1loc}=2\\frac{Precision*Recall}{Precision+Recall}$$\\end{document} The damage classification network categorizes image pixels into four damage levels: no damage, minor damage, major damage, and destroyed. Accordingly, a corresponding metric value is computed for each category. The \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document} metric across the four damage levels is computed using the harmonic mean3, which is defined as follows: 20 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F_{1cls} = \\frac{4}{{\\frac{1}{{F_{1no dam} }} + \\frac{1}{{F_{1minor dam} }} + \\frac{1}{{F_{1major dam} }} + \\frac{1}{{F_{1destroyed} }}}}$$\\end{document} The overall evaluation metric used in this study is the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{overall}}$$\\end{document} , which is calculated as the weighted average of the localization \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{loc}}$$\\end{document} and the damage \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document} . This metric is widely adopted for evaluating performance on the xBD dataset. 21 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${F}_{1overall}{=0.3F}_{1loc}{+0.7F}_{1cls}$$\\end{document} Results In the building localization stage, since most previous studies do not report specific evaluation metrics for building localization, we compare the performance of the trained classical semantic segmentation models with that of DDNet. Table 2 presents the comparative results. Our model achieves an F1-score of 0.8638. Compared with SCSE-ResNeXtUNet (SERes-UNet), DeepLabV3&#8201;+&#8201; 45 , and U-Net, DDNet exhibits improvements of 0.0384, 0.0483, and 0.0644, respectively. Its overall accuracy is 0.0398 higher than that of the lowest-performing model, DeepLabV3&#8201;+&#8201;. Additionally, DDNet achieves recall improvements of 0.0384, 0.0560, and 0.0945, respectively, demonstrating that it reliably maintains high recall in building localization compared with other models. Table 2 Evaluation metrics of the building localization results. Model F1loc Accuracy Recall IOU OA U-Net 0.7994 0.8617 0.7455 0.6658 0.9803 DeepLabV3&#8201;+&#8201; 0.8155 0.8371 0.7952 0.6886 0.9810 SCSE-ResNext-UNet 0.8254 0.8384 0.8128 0.7027 0.9819 TransUNet 0.8576 0.8741 0.8417 0.7507 0.9853 DDNet 0.8638 0.8769 0.8512 0.7603 0.9858 Figure&#160; 7 presents a comparison of the extraction results for dense buildings, elongated buildings, and small buildings across the evaluated methods. As shown, DDNet is capable of capturing detailed representations of building rooftops and accurately delineating the boundaries between adjacent buildings. The proposed method demonstrates superior performance in accurately extracting small buildings and achieves the highest levels of continuity and smoothness when delineating elongated buildings. Fig. 7 Visualization of building localization results. The subsequent damage detection subtask involves classifying the severity of damage sustained by individual buildings. The corresponding data comparison is presented in Table 3 , with evaluation primarily based on the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document} metrics. Weber 15 performs damage detection by integrating features through a change detection&#8211;based approach. Hao 21 integrated the U-Net model with a dual-branch CNN architecture, employing the U-Net to perform semantic segmentation of buildings, while the dual-branch network was utilized for damaged building classification. A self-attention module was introduced to enhance performance by incorporating long-range contextual information from the entire image. RescueNet 46 adopts a dual-branch learning strategy based on pre-disaster and post-disaster images, where deep features are concatenated across multiple scales to enhance the information exchange between pre-disaster and post-disaster representations. ChangeOS 47 employs the pre-disaster image for building localization and the post-disaster image for damage classification, incorporating post-processing to ensure consistency of damage detection results within building areas. DamFormer 34 enhances building localization and damage classification by introducing cross-layer fusion within a combined CNN&#8211;transformer architecture. BSSNet 37 incorporates contrastive loss and boundary loss into the building localization process, thereby improving the accuracy of building segmentation results. Table 3 Overall building damage detection results. Method F1overall F1loc F1cls \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{No damage}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Minor}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Major}}$$\\end{document} \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}^{\\text{Destroyed}}$$\\end{document} Ethan Weber 15 0.7410 0.8350 0.7000 0.9060 0.4930 0.7220 0.8370 Hao 21 0.7190 0.7300 0.7140 0.9230 0.3980 0.6750 0.8120 RescueNet 46 0.7700 0.8400 0.7350 0.8850 0.5630 0.7710 0.8080 ChangeOS 47 0.7857 0.8541 0.7564 0.9266 0.6014 0.7418 0.8345 DAMFormer 34 0.7702 0.8686 0.7281 0.8986 0.5678 0.7256 0.8079 BSSNet 37 0.7720 0.8630 0.7330 0.9040 0.5330 0.7720 0.8460 DDNet 0.7956 0.8638 0.7664 0.9314 0.6052 0.7542 0.8567 Weber exhibits lower performance in building damage classification, which can be attributed to its reliance on a change detection&#8211;based approach. Our method achieves the highest building damage classification performance, attaining the best \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document} among all compared methods. Specifically, the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{1\\text{cls}}$$\\end{document} of our approach is 0.0100 higher than that of the second-best method, and its \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\text{F}}_{\\text{1overall}}$$\\end{document} is 0.0099 higher than that of the second-best method. Nevertheless, our method has certain limitations. As shown in Table 1 , major damage is frequently misclassified as either minor damage or destroyed, indicating that the model still struggles to fully address the imbalanced data distribution. Figure&#160; 8 presents the visualization of damage detection results generated by several models. DDNet produces clearer segmentation boundaries and reduces the number of misclassified damaged buildings. In the first image of Fig.&#160; 8 , the upper portion of the white building in the lower-left corner is significantly displaced, and the lower portion is partially damaged; ChangeOS 47 classifies it as minor damage, whereas DDNet correctly identifies the damage level. In the third image, the building at the top is slightly damaged; ChangeOS classifies it as major damage, while DDNet correctly classifies it. Overall, DDNet demonstrates improved accuracy and effectiveness in building damage classification. Fig. 8 Classifcation of building damage in xBD dataset. Ablation study Differential concatenate (D-Cat) Table 4 presents the performance improvements achieved by incorporating D-Cat into the localization network. The inclusion of D-Cat enhances both the accuracy and recall metrics, resulting in a 0.0032 increase in the F 1loc score. When both D-Cat and O-UFL are integrated into the localization network, the F 1loc score improves by 0.0062 compared with the baseline network without these enhancements. Table 4 Ablation study on D-Cat and O-UFL. Model F1loc Accuracy Recall Baseline 0.8576 0.8741 0.8417 Cat 0.8608 0.8781 0.8442 O-UFL 0.8616 0.8732 0.8504 D-Cat&#8201;+&#8201;O-UFL 0.8638 0.8769 0.8512 The offsetting focal parameters of the unified focal loss (O-UFL) O-UFL incorporates three hyperparameters in total. Specifically, &#947; is empirically set to 0.3 to balance positive and negative examples, while &#956; is fixed at 0.5 following the formulation of Combo loss. Increasing &#946; leads to an improvement in recall; however, this gain is accompanied by a corresponding reduction in accuracy. In this study, three distinct values of &#946;&#8212;0.5, 0.7, and 0.9&#8212;are evaluated to investigate their influence on the performance of disaster-damaged building detection. The comparative results, summarized in Table 5 , demonstrate that setting &#946; to 0.9 effectively extends the acquisition range for buildings and enhances the overall performance of the network. Table 5 The impact of the hyperparameter \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\upbeta$$\\end{document} . \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\upbeta$$\\end{document} F1loc Accuracy Recall F1cls 0.5 0.8671 0.8880 0.8472 0.7227 0.7 0.8647 0.8859 0.8445 0.7434 0.9 0.8638 0.8769 0.8512 0.7664 DJA and the connected-component analysis algorithm To evaluate the effectiveness of incorporating the dual-temporal joint attention (DJA) module for capturing correlations and extracting change features from pre-disaster and post-disaster images, this study compares the base two-branch model, the self-attention (SA) module, and the DJA module. The base structure refers to the unenhanced two-branch model. Table 6 presents a comparative analysis of the results. The inclusion of the DJA module improves accuracy across all damage detection categories, indicating that the joint attention mechanism effectively guides the network to learn informative features from both pre-disaster and post-disaster images. When both the DJA module and the connected component analysis algorithm are integrated into the classification network, the F 1cls index increases by 1.17% relative to the base model. Table 6 Analysis of the DJA module and the algorithm for connected-component analysis. Damage classification network F1cls No damage Minor Major Destroyed base 0.7547 0.9238 0.5988 0.7416 0.8338 base&#8201;+&#8201;SA 0.7563 0.9270 0.5983 0.7443 0.8369 base&#8201;+&#8201;DJA 0.7620 0.9306 0.6072 0.7470 0.8408 base&#8201;+&#8201;DJA&#8201;+&#8201;connected-component 0.7664 0.9314 0.6052 0.7542 0.8567 Discussion Summary In post-disaster scenarios, buildings are of critical concern, and the rapid and accurate extraction of damage-related building information from remote sensing images is essential for effective emergency response. This study employs deep learning techniques and utilizes high-resolution imagery from both pre-disaster and post-disaster phases to perform multi-task change detection, with a focus on identifying disaster-affected structures. To address challenges such as inaccurate boundary delineation, interference suppression, and the effective integration of features from pre-disaster and post-disaster data, this research emphasizes the tasks of building localization and damage classification. A two-stage DDNet model is proposed to tackle these challenges. The main findings and conclusions of this study are summarized as follows: A novel two-stage DDNet model was proposed to address the inherent challenges in building localization and damaged building classification. By integrating CNN and Transformer architectures, the building localization phase overcomes the limitations of TransUNet through the use of a Unified Focal Loss function with offset focal parameters, thereby expanding the effective feature extraction region. In addition, a differential connection module was incorporated to enhance boundary delineation. In the damage classification phase, dual-temporal joint attention modules were employed to encode relevant features from both pre-disaster and post-disaster images while mitigating interference caused by variations in imaging conditions. Subsequently, buildings were segmented to identify damage zones, and a connected component analysis algorithm was applied to generate distinct damage assessment results. Compared with alternative methods, DDNet demonstrated superior performance on the xBD dataset, achieving an overall F1 score of 79.56%, with F1 scores of 86.38% for building localization and 76.64% for damaged building classification. Challenges and future directions The proposed models effectively address the challenge of extracting disaster-damaged buildings. Experimental results demonstrate that the DDNet model significantly improves extraction accuracy. Nevertheless, certain limitations persist, which warrant further investigation and discussion. Model improvement Model performance often deteriorates in real-world applications. Although incremental and transfer learning approaches can improve accuracy through additional training, they incur substantial time costs. Moreover, the limited sample sizes of minor and severe damage classes, combined with the constraints of loss function&#8211;based methods, pose significant challenges. Future research should focus on enhancing model generalization and addressing the difficulties associated with these challenging damage categories. Integration of multi-modal data This study utilizes high-resolution RGB optical imagery, which captures rooftop details but lacks information regarding building facades and sides. As a result, the damage assessment is limited and potentially imprecise, restricting its ability to support a comprehensive evaluation. Future research could integrate multi-modal data sources, including SAR imagery, multi-view point clouds, and geographic information, to enable more detailed and holistic damage assessments. Furthermore, disasters often involve secondary events influenced by both natural and social factors, rendering static damage assessments insufficient. Subsequent investigations should focus on developing deep learning models for multi-temporal imagery to facilitate dynamic monitoring of disaster progression. Limitations of two-dimensional imagery and the necessity of multimodal integration It is important to note that buildings are inherently three-dimensional structures, and their damage states often involve multi-level structural features. The aerial and satellite optical imagery employed in this study primarily provides a two-dimensional perspective of rooftops, lacking critical fa&#231;ade and depth information. Such limitations in data naturally constrain the accuracy of multi-level damage detection and the granularity of classification, particularly for fine-grained or subtle damage patterns. Consequently, analyses relying solely on 2D imagery may not fully capture the complete damage state of buildings. To partially mitigate these limitations, the proposed approach leverages cross-temporal feature modeling and regional consistency optimization. Specifically, the Dual-temporal Joint Attention (DJA) mechanism emphasizes semantic differences in texture, shape, and contour between pre- and post-disaster images, thereby capturing structural variations at the rooftop level and partially compensating for the absence of fa&#231;ade information. Additionally, connected-component analysis is applied during the classification stage to enforce regional consistency across entire building areas, which helps reduce prediction instability arising from missing local features or incomplete structural information. These strategies allow our 2D-based method to achieve improved localization and classification performance despite the intrinsic limitations of the input data. Nevertheless, the fundamental constraints of two-dimensional optical imagery remain. Without integrating three-dimensional data sources&#8212;such as LiDAR point clouds, oblique aerial images, stereo image pairs, or photogrammetric reconstructions&#8212;the method cannot fully characterize the volumetric structure and nuanced damage patterns of buildings. Future research will focus on the fusion of multimodal and 3D spatial data with 2D cross-temporal feature modeling. By combining the complementary strengths of 2D and 3D information, we expect to enhance both the accuracy and granularity of post-disaster building damage detection, and to increase the overall reliability and interpretability of the results, particularly for complex or multi-level structural damages. Conclusions In this study, we propose DDNet for detecting buildings damaged by disasters. The localization network employs a hybrid architecture that integrates CNN and Transformer components, utilizing a Unified Focal Loss with offset focal parameters to expand the effective acquisition range. Additionally, a differential concatenation module is incorporated to enhance the extraction of building boundary information. To capture correlation features between pre-disaster and post-disaster images while mitigating interference caused by varying imaging conditions, the damage classification network is equipped with a dual-temporal joint attention module. Subsequently, buildings of interest are segmented to identify damaged regions, and a connected component analysis algorithm is applied to generate distinct damage assessment results. Compared with existing methods, DDNet demonstrates superior performance on the xBD dataset. Future research will focus on further improving the model&#8217;s generalization capabilities and enhancing its detection accuracy based on the current findings. In addition, we plan to collect post-disaster visible-light aerial imagery and conduct building damage detection experiments using end-to-end models. This will further validate the applicability and performance differences of detection-based methods across different data modalities, and help establish a multi-modal disaster analysis framework spanning from visible-light detection to remote sensing segmentation. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Author contributions Conceptualization, L.Z. and XS.G.; methodology, L.Z.; writing&#8212;original draft preparation, L.Z.; writing&#8212;review and editing, XS.G.; visualization, L.Z.; data curation, D.M.; supervision, XS.G.; funding acquisition, XS.G. All authors have read and agreed to the published version of the manuscript. Funding This research was funded by the Natural Science Foundation project of Henan Province, Grant Number 222300420450; National Natural Science Foundation of China, Grant Number 41572341. Data availability The datasets used and/or analysed during the current study available from the corresponding author on reasonable request. Declarations Competing interests The authors declare no competing interests. References 1. Lin C Li Y Liu Y Building damage assessment from post-hurricane imageries using unsupervised domain adaptation with enhanced feature discrimination IEEE Trans. Geosci. Remote Sens. 2022 60 1 10 Lin, C. et al. Building damage assessment from post-hurricane imageries using unsupervised domain adaptation with enhanced feature discrimination. IEEE Trans. Geosci. Remote Sens. 60 , 1&#8211;10 (2022). 2. Deniz D Arneson EE Liel AB Flood loss models for residential buildings, based on the 2013 Colorado floods Nat. Hazards 2017 85 2 977 1003 10.1007/s11069-016-2615-3 Deniz, D. et al. Flood loss models for residential buildings, based on the 2013 Colorado floods. Nat. Hazards 85 (2), 977&#8211;1003 (2017). 3. Gupta, R,. Hosfelt, R., Sajeev, S., et al. xBD: A dataset for assessing building damage from satellite imagery. arXiv, 2019. 4. Yusuf Y Matsuoka M Yamazaki F Damage assessment after 2001 Gujarat earthquake using Landsat-7 satellite images J. Indian Soc. Remote Sens. 2001 29 1&#8211;2 17 22 10.1007/BF02989909 Yusuf, Y., Matsuoka, M. &amp; Yamazaki, F. Damage assessment after 2001 Gujarat earthquake using Landsat-7 satellite images. J. Indian Soc. Remote Sens. 29 (1&#8211;2), 17&#8211;22 (2001). 5. Yamazaki F Yano Y Matsuoka M Visual damage interpretation of buildings in bam city using quick bird images following the 2003 Bam, Iran Earthquake Earthq. Spectra 2005 21 S328 S336 10.1193/1.2101807 Yamazaki, F., Yano, Y. &amp; Matsuoka, M. Visual damage interpretation of buildings in bam city using quick bird images following the 2003 Bam, Iran Earthquake. Earthq. Spectra 21 , S328&#8211;S336 (2005). 6. Tomowski, D., Ehlers, M., Klonus, S. Colour and texture based change detection for urban disaster analysis. In 2011 Joint Urban Remote Sensing Event. Munich, Germany: IEEE , 2011: 329-332 7. Xie, Z., Wang, M., Han, Y., et al. Hierarchical decision tree for change detection using high resolution remote sensing images. In Xie, Y., Zhang, A., Liu, H., et al. Geo-informatics in Sustainable Ecosystem and Society : Vol. 980. Singapore: Springer Singapore, 2019: 176&#8211;184. 8. Wessels KJ Van den Bergh F Roy DP Rapid land cover map updates using change detection and robust random forest classifiers Remote Sens. 2016 8 11 888 10.3390/rs8110888 Wessels, K. J. et al. Rapid land cover map updates using change detection and robust random forest classifiers. Remote Sens. 8 (11), 888 (2016). 9. Mansouri B Hamednia Y A soft computing method for damage mapping using VHR optical satellite imagery IEEE J. Selected Topics Appl. Earth Observ. Remote Sens. 2015 8 10 4935 4941 10.1109/JSTARS.2015.2493342 Mansouri, B. &amp; Hamednia, Y. A soft computing method for damage mapping using VHR optical satellite imagery. IEEE J. Selected Topics Appl. Earth Observ. Remote Sens. 8 (10), 4935&#8211;4941 (2015). 10. Nex F Duarte D Tonolo FG Structural building damage detection with deep learning: Assessment of a state-of-the-Art CNN in operational conditions Remote Sens. 2019 11 23 2765 10.3390/rs11232765 Nex, F. et al. Structural building damage detection with deep learning: Assessment of a state-of-the-Art CNN in operational conditions. Remote Sens. 11 (23), 2765 (2019). 11. Abdi G Jabari S A multi-feature fusion using deep transfer learning for earthquake building damage detection Can. J. Remote. Sens. 2021 47 2 337 352 10.1080/07038992.2021.1925530 Abdi, G. &amp; Jabari, S. A multi-feature fusion using deep transfer learning for earthquake building damage detection. Can. J. Remote. Sens. 47 (2), 337&#8211;352 (2021). 12. Li Y Hu W Dong H Building damage detection from post-event aerial imagery using single shot multibox detector Appl. Sci. 2019 9 6 1128 10.3390/app9061128 Li, Y. et al. Building damage detection from post-event aerial imagery using single shot multibox detector. Appl. Sci. 9 (6), 1128 (2019). 13. Xu, J. Z., Lu, W., Li, Z., et al. Building damage detection in satellite imagery using convolutional neural networks. arXiv, 2019. 14. Presa-Reyes, M., Chen, S. C. Assessing building damage by learning the deep feature correspondence of before and after aerial images. In 2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR) . Shenzhen, Guangdong, China: IEEE, 2020: 43&#8211;48. 15. Weber, E,. Kan&#233;, H. Building disaster damage assessment in satellite imagery with multi-temporal fusion. arXiv, 2020. 16. Guo M Liu H Xu Y Building extraction based on U-Net with an attention block and multiple losses Remote Sens. 2020 12 9 1400 10.3390/rs12091400 Guo, M. et al. Building extraction based on U-Net with an attention block and multiple losses. Remote Sens. 12 (9), 1400 (2020). 17. Nie Y An K Chen X An improved U-Net network for sandy road extraction from remote sensing imagery Remote Sens. 2023 15 20 4899 10.3390/rs15204899 Nie, Y. et al. An improved U-Net network for sandy road extraction from remote sensing imagery. Remote Sens. 15 (20), 4899 (2023). 18. Xiaosan GE Xi C Wenzhi Z Detection of damaged buildings based on generative adversarial networks Acta Geodaetica et artographica Sinica 2022 51 2 238 247 Xiaosan, G. E. et al. Detection of damaged buildings based on generative adversarial networks. Acta Geodaetica et artographica Sinica 51 (2), 238&#8211;247 (2022). 19. Wu C Zhang F Xia J Building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets Remote Sens. 2021 13 5 905 10.3390/rs13050905 Wu, C. et al. Building damage detection using u-net with attention mechanism from pre- and post-disaster remote sensing datasets. Remote Sens. 13 (5), 905 (2021). 20. Deng L Wang Y Post-disaster building damage assessment based on improved U-Net Sci. Rep. 2022 12 1 15862 10.1038/s41598-022-20114-w 36151272 PMC9508235 Deng, L. &amp; Wang, Y. Post-disaster building damage assessment based on improved U-Net. Sci. Rep. 12 (1), 15862 (2022). 36151272 10.1038/s41598-022-20114-w PMC9508235 21. Hao, H., Baireddy, S., Bartusiak, E. R., et al. An attention-based system for damage assessment using satellite imagery. In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS . Brussels, Belgium: IEEE, 2021: 4396-4399 22. Yap, Y. L., Lim, S. L., Jatmiko, W., et al. Building detection from satellite images using deep learning. In 2024 Multimedia University Engineering Conference (MECON) . 2024: 1&#8211;7. 23. Pimpalkar, S., Madhavi, D. S., Rao, N. K. Performance analysis of a deep learning-based object detection approach for post-disaster building damage level assessment using YOLO and faster R-CNN. In Kalam, A., Mekhilef, S., Williamson S S. Innovations in Electrical and Electronics Engineering . Singapore: Springer Nature, 2025: 167&#8211;183. 24. Tilon S Nex F Kerle N Post-disaster building damage detection from earth observation imagery using unsupervised and transferable anomaly detecting generative adversarial networks Remote Sens. 2020 12 24 4193 10.3390/rs12244193 Tilon, S. et al. Post-disaster building damage detection from earth observation imagery using unsupervised and transferable anomaly detecting generative adversarial networks. Remote Sens. 12 (24), 4193 (2020). 25. Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is all you need. In Advances in Neural Information Processing Systems: Vol. 30. Curran Associates, Inc., 2017. 26. Zheng, S., Lu, J., Zhao, H., et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2021: 6881&#8211;6890. 27. Carion, N., Massa, F., Synnaeve, G., et al. End-to-End object detection with transformers. In Vedaldi A, Bischof H, Brox T, et al. Computer vision&#8212;ECCV 2020. Cham: Springer International Publishing, 2020: 213&#8211;229. 28. Zheng, C., Cham, T. J., Cai, J., et al. Bridging global context interactions for high-fidelity image completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2022: 11512&#8211;11522. 29. Liu Z, Lin Y, Cao Y, et al. Swin transformer: hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 2021: 10012&#8211;10022. 30. Chen, J., Lu, Y., Yu, Q., et al. TransUNet: Transformers make strong encoders for medical image segmentation. arXiv, 2021. 31. Chen H Qi Z Shi Z Remote sensing image change detection with transformers IEEE Trans. Geosci. Remote Sens. 2022 60 1 14 Chen, H., Qi, Z. &amp; Shi, Z. Remote sensing image change detection with transformers. IEEE Trans. Geosci. Remote Sens. 60 , 1&#8211;14 (2022). 32. Feng Y Xu H Jiang J ICIF-Net: Intra-scale cross-interaction and inter-scale feature fusion network for bitemporal remote sensing images change detection IEEE Trans. Geosci. Remote Sens. 2022 60 1 13 Feng, Y. et al. ICIF-Net: Intra-scale cross-interaction and inter-scale feature fusion network for bitemporal remote sensing images change detection. IEEE Trans. Geosci. Remote Sens. 60 , 1&#8211;13 (2022). 33. Song F Zhang S Lei T MSTDSNet-CD: Multiscale swin transformer and deeply supervised network for change detection of the fast-growing urban regions IEEE Geosci. Remote Sens. Lett. 2022 19 1 5 Song, F. et al. MSTDSNet-CD: Multiscale swin transformer and deeply supervised network for change detection of the fast-growing urban regions. IEEE Geosci. Remote Sens. Lett. 19 , 1&#8211;5 (2022). 34. Chen, H., Nemni, E., Vallecorsa, S., et al. Dual-tasks siamese transformer framework for building damage assessment. In IGARSS 2022&#8211;2022 IEEE International Geoscience and Remote Sensing Symposium . 2022: 1600&#8211;1603. 35. Ban, Y., Yousif, O,. Change detection techniques: A review. In Ban Y. Multitemporal remote sensing: methods and applications. Cham: Springer International Publishing, 2016: 19&#8211;43. 36. Cao QD Choe Y Posthurricane damage assessment using satellite imagery and geolocation features Risk Anal. 2024 44 5 1103 1113 10.1111/risa.14244 37897045 Cao, Q. D. &amp; Choe, Y. Posthurricane damage assessment using satellite imagery and geolocation features. Risk Anal. 44 (5), 1103&#8211;1113 (2024). 37897045 10.1111/risa.14244 37. Xie H Hu X Jiang H BSSNet: building subclass segmentation from satellite images using boundary guidance and contrastive learning IEEE J. Selected Topics Appl. Earth Observ. Remote Sens. 2022 15 7700 7711 10.1109/JSTARS.2022.3202524 Xie, H. et al. BSSNet: building subclass segmentation from satellite images using boundary guidance and contrastive learning. IEEE J. Selected Topics Appl. Earth Observ. Remote Sens. 15 , 7700&#8211;7711 (2022). 38. Yeung M Sala E Sch&#246;nlieb CB Unified focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation Comput. Med. Imaging Graph. 2022 95 102026 10.1016/j.compmedimag.2021.102026 34953431 PMC8785124 Yeung, M. et al. Unified focal loss: Generalising Dice and cross entropy-based losses to handle class imbalanced medical image segmentation. Comput. Med. Imaging Graph. 95 , 102026 (2022). 34953431 10.1016/j.compmedimag.2021.102026 PMC8785124 39. Abraham, N., Khan, N. M. A novel focal tversky loss function with improved attention U-Net for lesion segmentation. In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019) . 2019: 683&#8211;687. 40. Hao, H., Baireddy, S., Bartusiak, E. R., et al. An attention-based system for damage assessment using satellite imagery. In: 2021 IEEE international geoscience and remote sensing symposium IGARSS. 2021: 4396&#8211;4399. 41. Salehi, S. S. M., Erdogmus, D., Gholipour, A. Tversky loss function for image segmentation using 3D fully convolutional deep networks. In Wang Q, Shi Y, Suk H I, et al. Machine learning in medical imaging: Vol. 10541. Cham: Springer International Publishing, 2017: 379&#8211;387. 42. Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. An image is worth 16 &#215; 16 words: transformers for image recognition at scale. arXiv, 2021. 43. Taghanaki SA Zheng Y Kevin Zhou S Combo loss: Handling input and output imbalance in multi-organ segmentation Comput. Med. Imaging Graph. 2019 75 24 33 10.1016/j.compmedimag.2019.04.005 31129477 Taghanaki, S. A. et al. Combo loss: Handling input and output imbalance in multi-organ segmentation. Comput. Med. Imaging Graph. 75 , 24&#8211;33 (2019). 31129477 10.1016/j.compmedimag.2019.04.005 44. Wang, J., Zhang, W., Zang, Y., et al. Seesaw loss for long-tailed instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2021: 9695&#8211;9704. 45. Wang C Du P Wu H A cucumber leaf disease severity classification method based on the fusion of DeepLabV3+ and U-Net Comput. Electron. Agric. 2021 189 106373 10.1016/j.compag.2021.106373 Wang, C. et al. A cucumber leaf disease severity classification method based on the fusion of DeepLabV3+ and U-Net. Comput. Electron. Agric. 189 , 106373 (2021). 46. Gupta, R., Shah, M. RescueNet: Joint building segmentation and damage assessment from satellite imagery. In 2020 25th International Conference on Pattern Recognition (ICPR) . 2021: 4405&#8211;4411. 47. Zheng Z Zhong Y Wang J Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters Remote Sens. Environ. 2021 265 112636 10.1016/j.rse.2021.112636 Zheng, Z. et al. Building damage assessment for rapid disaster response with a deep object-based semantic change detection framework: From natural disasters to man-made disasters. Remote Sens. Environ. 265 , 112636 (2021)."
}