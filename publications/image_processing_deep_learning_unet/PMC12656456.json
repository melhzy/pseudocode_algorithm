{
  "pmcid": "PMC12656456",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.953134",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656456",
    "pmid": "41305024",
    "doi": "10.3390/s25226815",
    "title": "Enhanced Deep Neural Network for Prostate Segmentation in Micro-Ultrasound Images",
    "year": "2025",
    "month": "11",
    "day": "07",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "07"
    },
    "authors": [
      "AL-Qurri Ahmed",
      "Thaher Asem",
      "Almekkawy Mohamed Khaled"
    ],
    "abstract": "Prostate cancer is a global health concern, and early diagnosis plays a vital role in improving the survival rate. Accurate segmentation is a key step in the automated diagnosis of prostate cancer; however, manual segmentation remains time-consuming and challenging. Micro-Ultrasound (US) is particularly well-suited for prostate cancer detection, offering real-time imaging with a resolution comparable to that of MRI. This enables improved spatial resolution and detailed visualization of small anatomical structures. With recent advances in deep learning for medical image segmentation, precise prostate segmentation has become critical for biopsy guidance, disease diagnosis, and follow-up. However, segmentation of the prostate in micro-US images remains challenging due to indistinct boundaries between the prostate and surrounding tissue. In this work, we propose a model for precise micro-ultrasound image segmentation. The model employs a dual-encoder architecture that integrates Convolutional Neural Networks (CNN) and Transformer-based encoders in parallel, combined with a fusion module to capture both global dependencies and low-level spatial details. More importantly, we introduce a decoder based on Mamba v2 to enhance segmentation accuracy. A Hypergraph Neural Network (HGNN) is employed as a bridge between the dual encoders and Mamba decoder to model correlations among non-pairwise connections. Experimental results on micro-US datasets demonstrated that our model achieved superior or comparable performance to state-of-the-art methods, with a Dice score of 0.9416 and an HD95 of 1.93.",
    "keywords": [
      "UNet",
      "UNet++",
      "Transformer",
      "CNN",
      "attention",
      "medical imaging",
      "Micro-Ultrasound",
      "Hypergraph Neural Network",
      "Mamba"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656456</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656456.1</article-id><article-id pub-id-type=\"pmcaid\">12656456</article-id><article-id pub-id-type=\"pmcaiid\">12656456</article-id><article-id pub-id-type=\"pmid\">41305024</article-id><article-id pub-id-type=\"doi\">10.3390/s25226815</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-06815</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhanced Deep Neural Network for Prostate Segmentation in Micro-Ultrasound Images</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0009-0000-9331-0187</contrib-id><name name-style=\"western\"><surname>AL-Qurri</surname><given-names initials=\"A\">Ahmed</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><xref rid=\"af1-sensors-25-06815\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Thaher</surname><given-names initials=\"A\">Asem</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><xref rid=\"af2-sensors-25-06815\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0002-9222-3003</contrib-id><name name-style=\"western\"><surname>Almekkawy</surname><given-names initials=\"MK\">Mohamed Khaled</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-sensors-25-06815\" ref-type=\"aff\">1</xref><xref rid=\"c1-sensors-25-06815\" ref-type=\"corresp\">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Ling</surname><given-names initials=\"S\">Steve</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Lyu</surname><given-names initials=\"J\">Juan</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-06815\"><label>1</label>The School of Electrical Engineering and Computer Science, Pennsylvania State University, University Park, PA 16802, USA; <email>aqa6122@psu.edu</email></aff><aff id=\"af2-sensors-25-06815\"><label>2</label>Independent Researcher, State College, PA 16803, USA</aff><author-notes><corresp id=\"c1-sensors-25-06815\"><label>*</label>Correspondence: <email>mka9@psu.edu</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>07</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>6815</elocation-id><history><date date-type=\"received\"><day>08</day><month>9</month><year>2025</year></date><date date-type=\"rev-recd\"><day>29</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>04</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>07</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-06815.pdf\"/><abstract><p>Prostate cancer is a global health concern, and early diagnosis plays a vital role in improving the survival rate. Accurate segmentation is a key step in the automated diagnosis of prostate cancer; however, manual segmentation remains time-consuming and challenging. Micro-Ultrasound (US) is particularly well-suited for prostate cancer detection, offering real-time imaging with a resolution comparable to that of MRI. This enables improved spatial resolution and detailed visualization of small anatomical structures. With recent advances in deep learning for medical image segmentation, precise prostate segmentation has become critical for biopsy guidance, disease diagnosis, and follow-up. However, segmentation of the prostate in micro-US images remains challenging due to indistinct boundaries between the prostate and surrounding tissue. In this work, we propose a model for precise micro-ultrasound image segmentation. The model employs a dual-encoder architecture that integrates Convolutional Neural Networks (CNN) and Transformer-based encoders in parallel, combined with a fusion module to capture both global dependencies and low-level spatial details. More importantly, we introduce a decoder based on Mamba v2 to enhance segmentation accuracy. A Hypergraph Neural Network (HGNN) is employed as a bridge between the dual encoders and Mamba decoder to model correlations among non-pairwise connections. Experimental results on micro-US datasets demonstrated that our model achieved superior or comparable performance to state-of-the-art methods, with a Dice score of 0.9416 and an HD95 of 1.93.</p></abstract><kwd-group><kwd>UNet</kwd><kwd>UNet++</kwd><kwd>Transformer</kwd><kwd>CNN</kwd><kwd>attention</kwd><kwd>medical imaging</kwd><kwd>Micro-Ultrasound</kwd><kwd>Hypergraph Neural Network</kwd><kwd>Mamba</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-06815\"><title>1. Introduction</title><p>Prostate cancer is the second most prevalent cancer worldwide, with over 1.2 million new cases reported in 2020 [<xref rid=\"B1-sensors-25-06815\" ref-type=\"bibr\">1</xref>]. Accurate imaging is critical for an early diagnosis. Traditionally, lesions in the prostate are identified using multiparametric MRI (mpMRI), which provides high-resolution anatomical and functional information about the lesions. However, MRI is expensive, time-consuming, and often inaccessible in many clinical settings, limiting its widespread adoption [<xref rid=\"B2-sensors-25-06815\" ref-type=\"bibr\">2</xref>]. Ultrasound has become a widely adopted imaging modality owing to its low cost and high accessibility [<xref rid=\"B3-sensors-25-06815\" ref-type=\"bibr\">3</xref>]. Micro-Ultrasound (micro-US) has recently emerged as a promising alternative, operating at substantially higher frequencies (typically 29 MHz or greater) than conventional ultrasound, thereby offering improved spatial resolution for prostate imaging [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]. Medical image segmentation plays a critical role in clinical practice by enabling the automatic localization of the prostate capsule. Although manual segmentation is feasible, it remains a time-consuming and labor-intensive task that is often inadequate for capturing the broader pathological context [<xref rid=\"B5-sensors-25-06815\" ref-type=\"bibr\">5</xref>,<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>,<xref rid=\"B7-sensors-25-06815\" ref-type=\"bibr\">7</xref>]. Recently, deep learning has been successfully applied to medical image segmentation. Nevertheless, despite advances in neural network architectures, segmentation remains a challenging task owing to noisy and scattered features, low resolution, weak boundaries, and irregular shapes of human organs [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>,<xref rid=\"B8-sensors-25-06815\" ref-type=\"bibr\">8</xref>,<xref rid=\"B9-sensors-25-06815\" ref-type=\"bibr\">9</xref>].</p><p>In deep learning architectures, the convolution operation is the fundamental building block of U-Net [<xref rid=\"B10-sensors-25-06815\" ref-type=\"bibr\">10</xref>,<xref rid=\"B11-sensors-25-06815\" ref-type=\"bibr\">11</xref>]; however, its inherently limited receptive field constrains the network&#8217;s ability to capture global context [<xref rid=\"B5-sensors-25-06815\" ref-type=\"bibr\">5</xref>]. To overcome this limitation, attention mechanisms have been introduced into deep learning. Attention allocates computational resources to regions containing the most relevant information, mimicking the functionality of the human visual system [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>,<xref rid=\"B12-sensors-25-06815\" ref-type=\"bibr\">12</xref>]. Consequently, several attention mechanisms have been proposed that can be categorized according to various criteria. One common categorization is based on the dimensions along which the attention feature map functions. For instance, channel attention exploits inter-channel dependencies, which are employed in Squeeze-and-Excitation (SE) networks [<xref rid=\"B12-sensors-25-06815\" ref-type=\"bibr\">12</xref>]. In contrast, spatial attention emphasizes regions within feature maps that contain critical information while suppressing less relevant areas, as demonstrated in Non-Local Neural Networks [<xref rid=\"B13-sensors-25-06815\" ref-type=\"bibr\">13</xref>]. Attention mechanisms can also be categorized based on the operation of the attention map [<xref rid=\"B5-sensors-25-06815\" ref-type=\"bibr\">5</xref>]. For example, global attention enhances interactions across both channels and spatial locations, as in DANet [<xref rid=\"B5-sensors-25-06815\" ref-type=\"bibr\">5</xref>,<xref rid=\"B14-sensors-25-06815\" ref-type=\"bibr\">14</xref>], whereas local attention, such as in CBAM [<xref rid=\"B15-sensors-25-06815\" ref-type=\"bibr\">15</xref>], focuses on specific spatial regions and their relationships with the channels.</p><p>Recently, Transformer-based models have gained prominence, initially introduced for sequence-to-sequence tasks in natural language processing (NLP) [<xref rid=\"B16-sensors-25-06815\" ref-type=\"bibr\">16</xref>]. Transformers have been adopted across various domains due to their powerful attention mechanisms. Unlike CNN-based models, they overcome the limitation of restricted receptive fields by employing a self-attention (SA) mechanism [<xref rid=\"B17-sensors-25-06815\" ref-type=\"bibr\">17</xref>]. The SA captures the internal correlations among all input tokens, enabling the modeling of long-range dependencies [<xref rid=\"B18-sensors-25-06815\" ref-type=\"bibr\">18</xref>]. The Transformer architecture incorporates multiple self-attention blocks, known as multi-head self-attention (MHSA), which operate in parallel to generate diverse feature representations [<xref rid=\"B17-sensors-25-06815\" ref-type=\"bibr\">17</xref>,<xref rid=\"B18-sensors-25-06815\" ref-type=\"bibr\">18</xref>]. Building on this, Dosovitskiy et al. [<xref rid=\"B19-sensors-25-06815\" ref-type=\"bibr\">19</xref>] introduced the Vision Transformer (ViT) for computer vision tasks, such as image classification, achieving state-of-the-art results on ImageNet. Moreover, Cao et al. [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>] introduced Swin-Unet, the first U-shaped segmentation network based entirely on a Transformer architecture [<xref rid=\"B21-sensors-25-06815\" ref-type=\"bibr\">21</xref>]. Swin-Unet integrates Swin Transformer blocks, where Window Multi-Head Self-Attention (W-MSA) captures fine-grained details within each window, and Shifted Window Self-Attention (SW-MSA) models cross-window interactions [<xref rid=\"B22-sensors-25-06815\" ref-type=\"bibr\">22</xref>]. Unlike U-Net, Swin-Unet replaces conventional upsampling with patch-expanding layers, eliminating the need for convolutions or interpolation [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>,<xref rid=\"B23-sensors-25-06815\" ref-type=\"bibr\">23</xref>,<xref rid=\"B24-sensors-25-06815\" ref-type=\"bibr\">24</xref>]. Nevertheless, despite its strong capability to capture the global context, the Transformer remains limited in modeling fine-grained details due to its lack of spatial inductive bias for local information. This limitation is particularly evident in medical image segmentation tasks [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>].</p><p>To leverage the strengths of both Transformers and U-Net, Chen et al. [<xref rid=\"B26-sensors-25-06815\" ref-type=\"bibr\">26</xref>] proposed the TransUNet. This hybrid architecture employs Convolutional Neural Networks (CNNs) to extract low-level features and a Transformer to capture global information [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>]. Furthermore, hybrid models, such as TransFuse, adopt parallel encoder branches. In TransFuse [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>], both a CNN-based spatial branch and a transformer-based global branch are fused using a BiFusion module [<xref rid=\"B24-sensors-25-06815\" ref-type=\"bibr\">24</xref>,<xref rid=\"B27-sensors-25-06815\" ref-type=\"bibr\">27</xref>]. In the BiFusion module, Transformer features are refined using a channel attention SE block [<xref rid=\"B12-sensors-25-06815\" ref-type=\"bibr\">12</xref>], whereas the CNN features are enhanced using a spatial attention module inspired by CBAM [<xref rid=\"B15-sensors-25-06815\" ref-type=\"bibr\">15</xref>]. The features from both branches were then combined using a Hadamard product to model their interactions. Similarly, CoTrFuse [<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>] follows a dual-branch strategy but incorporates Swin Transformer blocks for the global branch and EfficientNet blocks for the spatial branch, with feature fusion achieved through a specialized STCF module. Other researchers have focused on improving decoder performance. For instance, TransNorm [<xref rid=\"B29-sensors-25-06815\" ref-type=\"bibr\">29</xref>] employs spatial normalization outputs from the Transformer to construct a two-level attention gate, where channel attention normalizes the feature representation to emphasize more informative channels and Transformer-produced spatial coefficients to amplify the relevant areas of the feature map. Other architectures, such as MS-TransUNet++ [<xref rid=\"B30-sensors-25-06815\" ref-type=\"bibr\">30</xref>] and CoT-UNet++ [<xref rid=\"B31-sensors-25-06815\" ref-type=\"bibr\">31</xref>], enhance CNN-Transformer hybrid architectures by introducing dense skip connections between encoders and decoders at multiple levels, similar to U-Net++ [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>]. Some works have explored the frequency domain. For example, Discrete Fourier Transform (DFT) modules have been integrated to capture long-range dependencies more effectively [<xref rid=\"B32-sensors-25-06815\" ref-type=\"bibr\">32</xref>]. Another line of research utilized deep supervision, which was first introduced in Google&#8217;s Inception architecture [<xref rid=\"B5-sensors-25-06815\" ref-type=\"bibr\">5</xref>,<xref rid=\"B33-sensors-25-06815\" ref-type=\"bibr\">33</xref>]. U-Net++ adopts deep supervision to help intermediate layers learn discriminative features and mitigate vanishing gradients [<xref rid=\"B34-sensors-25-06815\" ref-type=\"bibr\">34</xref>]. Other network architectures incorporate side outputs for deep supervision. For example, BASNet [<xref rid=\"B35-sensors-25-06815\" ref-type=\"bibr\">35</xref>] employs deep supervision via side outputs and was proposed for salient-object detection. One network relevant to this research is MicroSegNet, which employs multi-scale deep supervision for prostate segmentation from micro-ultrasound images [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>].</p><p>In this paper, we propose an enhanced network architecture that utilizes a dual encoder, comprising both CNN and Transformer encoders, to effectively capture both local and global contextual information. The decoder is designed based on Mamba v2 in combination with CNN layers to improve the segmentation accuracy. Additionally, a Hypergraph Neural Network (HGNN) is integrated into the skip connections to capture non-pairwise correlations. To mitigate overfitting, we employed a tailored ultrasound-specific augmentation scheme incorporating depth attenuation, Gaussian shadows, haze artifacts, and speckle reduction.The experimental results demonstrate that the proposed architecture outperforms state-of-the-art segmentation models, achieving a superior segmentation accuracy. Our contributions can be summarized as follows:<list list-type=\"bullet\"><list-item><p>We propose a model for precise Micro-Ultrasound medical image segmentation composed of dual encoders with a fusion module designed to capture both local details and long-range dependencies. A Hypergraph Neural Network (HGNN) is integrated into the skip connections to model non-pairwise correlations.</p></list-item><list-item><p>To further enhance segmentation accuracy, a Mamba-based decoder is incorporated, utilizing VSSD blocks built upon Mamba-2 and NC-SSD.</p></list-item><list-item><p>Experimental results demonstrate that our method achieves superior performance on the Micro-Ultrasound (US) prostate medical image segmentation dataset.</p></list-item></list></p></sec><sec id=\"sec2-sensors-25-06815\"><title>2. Materials and Methods</title><sec id=\"sec2dot1-sensors-25-06815\"><title>2.1. Overall Architecture</title><p>Inspired by CoTrFuse [<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>], we designed a network comprising two parallel branches. One branch employs a Transformer (Swin blocks) to capture the global context, whereas the other branch utilizes EfficientNet blocks to extract local details. The outputs from both branches are fused using a module based on BiFusion [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>]. The overall architecture of the model is illustrated in <xref rid=\"sensors-25-06815-f001\" ref-type=\"fig\">Figure 1</xref>. On the decoder side, a combination of CNN layers and Visual State Space Duality (VSSD) blocks based on Mamba v2 was employed. The VSSD incorporates Non-Causal SSD (NC-SSD) modules [<xref rid=\"B36-sensors-25-06815\" ref-type=\"bibr\">36</xref>] to enhance feature representation. Furthermore, inspired by BASNet [<xref rid=\"B35-sensors-25-06815\" ref-type=\"bibr\">35</xref>], we integrated side outputs from the decoder for deep supervision, leveraging complementary information across intermediate prediction maps [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>]. These outputs were upsampled and incorporated into the training to provide guidance at multiple levels. This approach accelerates convergence and mitigates the vanishing gradient problems. A more detailed discussion of the network architecture is provided in the following section.</p><sec id=\"sec2dot1dot1-sensors-25-06815\"><title>2.1.1. Dual Encoder</title><p>The dual-encoder architecture employs EfficientNet to extract local features [<xref rid=\"B37-sensors-25-06815\" ref-type=\"bibr\">37</xref>]. EfficientNet was developed using a neural architecture search (NAS) to systematically scale convolutional networks, resulting in a family of highly efficient models that balance network width, depth, and resolution. Each EfficientNet model is constructed from a series of Inverted Residual Blocks, also referred to as MBConv blocks [<xref rid=\"B38-sensors-25-06815\" ref-type=\"bibr\">38</xref>]. The other branch employs Swin Transformer blocks with patch-merging layers, similar to [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>,<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>]. As mentioned in the introduction section, Cao et al. [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>] introduced Swin-Unet, a Transformer-based architecture that incorporates a hierarchical structure and a window-shifting mechanism to enhance feature representation. Swin Transformer blocks perform feature learning through a window-based multi-head self-attention (W-MSA) module, followed by a shifted window multi-head self-attention (SW-MSA) module [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>], as illustrated in <xref rid=\"sensors-25-06815-f002\" ref-type=\"fig\">Figure 2</xref>. Specifically, the W-MSA captures the local dependencies among pixels within each window, whereas the SW-MSA models the global interactions across windows, thereby enabling cross-contextual attention for feature recalibration. The mathematical formulation of the Swin Transformer blocks is shown in Equations (<xref rid=\"FD1-sensors-25-06815\" ref-type=\"disp-formula\">1</xref>)&#8211;(<xref rid=\"FD4-sensors-25-06815\" ref-type=\"disp-formula\">4</xref>):<disp-formula id=\"FD1-sensors-25-06815\"><label>(1)</label><mml:math id=\"mm1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>WMSA</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-sensors-25-06815\"><label>(2)</label><mml:math id=\"mm2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>MLP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD3-sensors-25-06815\"><label>(3)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>SWMSA</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-06815\"><label>(4)</label><mml:math id=\"mm4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>MLP</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the outputs of the WMSA/SWMSA module and the MLP module of the <inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> block.</p><p>The self-attention (SA) mechanism is the core component of the Transformer&#8217;s block, as shown in Equation (<xref rid=\"FD5-sensors-25-06815\" ref-type=\"disp-formula\">5</xref>):<disp-formula id=\"FD5-sensors-25-06815\"><label>(5)</label><mml:math id=\"mm8\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#215;</mml:mo><mml:mo>&#160;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represent the query, key, and value matrices, respectively. <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and d donate the number of patches in a window and the dimension of either the query and key, respectively. B is a relative position bias, and it is based on the bias matrix <inline-formula><mml:math id=\"mm11\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>B</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>P</mml:mi><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Similar to Swin-Unet, the first step in the Swin Transformer branch of our model is a linear embedding layer that projects the input features into an arbitrary dimension. This is followed by a patch-merging layer that performs downsampling while increasing the feature dimension before the Transformer blocks process the data. Skip connections carrying multi-scale features from the Transformer encoder are later fused with the feature maps from the CNN encoder in the fusion module, as discussed in the subsequent section. For simplicity, the linear embedding layer is omitted from <xref rid=\"sensors-25-06815-f001\" ref-type=\"fig\">Figure 1</xref>.</p><p>The two branches are merged using a fusion module as illustrated in the next section.</p></sec><sec id=\"sec2dot1dot2-sensors-25-06815\"><title>2.1.2. Fusion Module</title><p>TransFuse [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>] and CoTrFuse [<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>] both leverage dual encoders that are combined through fusion modules. Although the two fusion modules share similar concepts, they differ in their implementations. Both modules incorporate channel and spatial attention mechanisms to enhance feature representation.</p><p>For example, the BiFusion module [<xref rid=\"B25-sensors-25-06815\" ref-type=\"bibr\">25</xref>] in TransFuse uses an SE block [<xref rid=\"B12-sensors-25-06815\" ref-type=\"bibr\">12</xref>] as channel attention to emphasize global information from the Transformer branch, while a CBAM block [<xref rid=\"B15-sensors-25-06815\" ref-type=\"bibr\">15</xref>] is used as spatial attention to capture fine-grained details from the CNN branch.</p><p>The STCF fusion module in CoTrFuse adopts a similar attention-based approach. However, unlike BiFusion, where channel attention is applied to the Transformer feature map and spatial attention is applied to the CNN feature map, STCF applies both attention mechanisms to both feature maps before summing them. In our model, we adopted the STCF fusion strategy, as shown in <xref rid=\"sensors-25-06815-f003\" ref-type=\"fig\">Figure 3</xref>.</p></sec><sec id=\"sec2dot1dot3-sensors-25-06815\"><title>2.1.3. Hyper GNN</title><p>In an ordinary graph, each edge connects only two nodes, representing the pairwise correlations. In contrast, a hypergraph employs hyperedges that can connect more than two nodes, thereby enabling the modeling of non-pairwise correlations. This makes hypergraphs more effective in capturing complex relationships. Han et al. [<xref rid=\"B39-sensors-25-06815\" ref-type=\"bibr\">39</xref>] introduced a Vision Hypergraph Neural Network (ViHGNN). Subsequently, Feng et al. [<xref rid=\"B40-sensors-25-06815\" ref-type=\"bibr\">40</xref>] proposed a Hypergraph Neural Network (HGNN), which constructs a hypergraph from an image using K-Nearest Neighbors (KNN). In the context of medical image segmentation, Peng et al. [<xref rid=\"B41-sensors-25-06815\" ref-type=\"bibr\">41</xref>] integrated an HGNN into a U-Net for MRI segmentation. More recent work has focused on adaptive hypergraph construction. For example, unlike earlier methods that relied on a fixed number of neighbors per node for graph construction, Chai et al. [<xref rid=\"B42-sensors-25-06815\" ref-type=\"bibr\">42</xref>] proposed an adaptive strategy for hyperedge formation known as Adaptive Hypergraph Construction. This method utilizes the K-Nearest Neighbors (KNN) algorithm to generate a matrix used to compute the degree of each node, which in turn guides the construction of hyperedges. Readers are referred to [<xref rid=\"B42-sensors-25-06815\" ref-type=\"bibr\">42</xref>] for further details regarding the Adaptive Hypergraph Construction procedure. Their Adaptive Hypergraph Construction method models the shape attributes more accurately, particularly in medical imaging. Furthermore, Chai et al. extended the concept of convolutional sliding windows to include hypergraphs. In this approach, the sliding-window-based convolution mechanism employs fixed-size kernels to convolve the feature maps, enabling interaction with local neighboring pixels across the entire image through stride operations.</p><p>The Adaptive Hypergraph Construction is illustrated in <xref rid=\"sensors-25-06815-f004\" ref-type=\"fig\">Figure 4</xref>, with hypergraph convolution denoted as HGC.</p></sec><sec id=\"sec2dot1dot4-sensors-25-06815\"><title>2.1.4. Mamba Decoder and VSSD Block</title><p>Motivated by the success of Transformers in capturing long-range dependencies, Mamba with State Space Models (SSMs) has been proposed to capture the global context while avoiding the high computational cost of Vision Transformers (ViTs). The Mamba S6 model improves upon the S4 model by introducing a selective mechanism and hardware optimization. To adapt Mamba for computer vision, VMamba [<xref rid=\"B36-sensors-25-06815\" ref-type=\"bibr\">36</xref>] converts non-causal visual images into sequences of ordered patches using a Cross-Scan Module (CSM). The S6 block was further enhanced in Mamba2 [<xref rid=\"B43-sensors-25-06815\" ref-type=\"bibr\">43</xref>] through the introduction of State Space Duality (SSD) [<xref rid=\"B44-sensors-25-06815\" ref-type=\"bibr\">44</xref>]. Recently, Xu et al. proposed a Non-Causal SSD (NC-SSD) that eliminates the need for a causal mask and specialized scanning paths. Building on this, they introduced Visual State Space Duality (VSSD) to replace the VSS vision block [<xref rid=\"B45-sensors-25-06815\" ref-type=\"bibr\">45</xref>]. The VSSD block based on Mamba2 is shown in <xref rid=\"sensors-25-06815-f005\" ref-type=\"fig\">Figure 5</xref>.</p><p>The VSSD block exhibits a design similar to that of the Mamba block [<xref rid=\"B46-sensors-25-06815\" ref-type=\"bibr\">46</xref>]; however, it employs the NC-SSD module instead of the standard Mamba SSD. Vision Mamba models, such as Vision Mamba [<xref rid=\"B47-sensors-25-06815\" ref-type=\"bibr\">47</xref>] and VMamba [<xref rid=\"B36-sensors-25-06815\" ref-type=\"bibr\">36</xref>] typically rely on image traversal mechanisms that use different scanning routes to flatten a 2D image into a 1D sequence. In contrast, NC-SSD introduces an enhanced algorithm by leveraging the fact that the matrix A in Mamba 2 [<xref rid=\"B43-sensors-25-06815\" ref-type=\"bibr\">43</xref>] is reduced to a scalar [<xref rid=\"B45-sensors-25-06815\" ref-type=\"bibr\">45</xref>]. This design eliminates the need for a specific scanning route and removes causal mask requirements. For more detailed information on the NC-SSD, readers are referred to [<xref rid=\"B45-sensors-25-06815\" ref-type=\"bibr\">45</xref>]. As shown in <xref rid=\"sensors-25-06815-f001\" ref-type=\"fig\">Figure 1</xref>, our decoder employs four VSSD blocks, with convolution and ReLU activation layers inserted between them, each of which is repeated twice. The configuration of the VSSD blocks is as follows: channel dimensions [256, 384, 448, 480] and number of heads [2, 4, 8, 16]. At each stage, each block was applied once without repetition.</p></sec></sec><sec id=\"sec2dot2-sensors-25-06815\"><title>2.2. Dataset</title><p>For this study, we utilized a prostate segmentation dataset based on micro-ultrasound (micro-US) images from [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]. The dataset is publicly available at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://zenodo.org/records/10475293\">https://zenodo.org/records/10475293</uri> and was accessed on 2 February 2025. To the best of our knowledge, this is the only publicly available micro-US dataset. Micro-ultrasound is a 29-MHz imaging technology that provides a resolution that is 3&#8211;4 times higher than that of conventional ultrasound. A key advantage of micro-US over other modalities, such as MRI and standard ultrasound, is its real-time visualization capability, eliminating the need for image fusion, which can account for up to 50% of diagnostic errors. The dataset was collected from 75 men who underwent micro-US-guided prostate biopsy at the University of Florida [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]. Each patient underwent a prostate scan from left to right, capturing approximately 200&#8211;300 micro-US images. The images were converted from B-mode to a DICOM series with embedded pixel spacing information. Ground-truth prostate annotations were performed by two non-expert annotators and one expert annotator for all 75 patients. Additionally, for evaluation purposes, three annotators manually segmented the prostate capsule in 20 test cases. For model training, 2060 micro-US images from 55 patients were used, and 758 images from 20 patients were reserved for testing. The dataset collection study was approved by the Institutional Review Board at the University of Florida.</p></sec><sec id=\"sec2dot3-sensors-25-06815\"><title>2.3. Augmentation</title><p>Ultrasound-specific data augmentation techniques were employed to prevent overfitting. Four different augmentations have been utilized: Depth Attenuation, Gaussian Shadow, Haze Artifact, and Speckle Reduction using the USAugment (version 1.0.1) [<xref rid=\"B48-sensors-25-06815\" ref-type=\"bibr\">48</xref>] (<uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/adamtupper/ultrasound-augmentation\">https://github.com/adamtupper/ultrasound-augmentation</uri>, access date (7 May 2025)). Depth Attenuation simulates the gradual loss of ultrasound wave energy as it propagates through the tissue, causing the intensity to decrease with increasing distance from the probe. Haze Artifact models semi-static noise bands that occasionally appear in the ultrasound images. Gaussian shadows replicate the acoustic shadows caused by air or tissue obstructing wave propagation, generating two-dimensional Gaussian-shaped shadows with randomly selected parameters. Speckle noise, arising from interference among ultrasound waves, is mitigated using Speckle Reduction, which applies a bilateral filter with randomly sampled parameters to reduce speckle patterns. Empirically, we found that applying these augmentations with a probability of 0.2 yields the best result.</p></sec><sec id=\"sec2dot4-sensors-25-06815\"><title>2.4. Loss Function and Evaluation</title><p>Following [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>], the AG-BCE loss function was employed. The AG-BCE loss is based on the BCE loss but accounts for the characteristics of prostate segmentation in micro-US images by penalizing prediction errors more heavily in challenging regions, particularly along the borders between the prostate and bladder. Hence, it assigns different weights to each pixel.<disp-formula id=\"FD6-sensors-25-06815\"><label>(6)</label><mml:math id=\"mm12\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"script\">L</mml:mi><mml:mrow><mml:mi>AG</mml:mi><mml:mo>-</mml:mo><mml:mi>BCE</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced separators=\"\" open=\"[\" close=\"]\"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form=\"prefix\">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form=\"prefix\">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere</p><list list-type=\"bullet\"><list-item><p><italic toggle=\"yes\">N</italic> is the total number of pixels,</p></list-item><list-item><p><inline-formula><mml:math id=\"mm13\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the is the weight of assigned to pixel <italic toggle=\"yes\">i</italic>,</p></list-item><list-item><p><inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the ground truth label for pixel <italic toggle=\"yes\">i</italic>,</p></list-item><list-item><p><inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted probability for pixel <italic toggle=\"yes\">i</italic>.</p></list-item></list><p>The AG-BCE loss function differentiates between hard and easy regions. Hard regions are defined as areas where expert and non-expert annotations disagree, whereas easy regions are defined as those where both annotations coincide. AG-BCE assigns a weight (denoted by Equation (<xref rid=\"FD6-sensors-25-06815\" ref-type=\"disp-formula\">6</xref>) as <italic toggle=\"yes\">w</italic> of four hard regions and one easy region, following the implementation in MicroSegNet [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]. For a more detailed explanation of the AG-BCE loss formulation, readers are referred to [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>].</p><p>The average Dice Similarity Coefficient (DSC) and average Hausdorff Distance (HD95) were used as evaluation metrics. HD95 measures the Hausdorff distance by considering the 95th percentile of the distances between the boundary points of the two sets. Unlike the standard Hausdorff distance, which reports the maximum distance, HD95 reduces the influence of outliers by ignoring the extreme values [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>]. The Dice Similarity Coefficient (DSC) is defined as<disp-formula id=\"FD7-sensors-25-06815\"><label>(7)</label><mml:math id=\"mm16\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>DSC</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mspace width=\"0.166667em\"/><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>P</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <italic toggle=\"yes\">G</italic> denotes the set of ground-truth pixels, and <italic toggle=\"yes\">P</italic> denotes the set of predicted pixels. The Hausdorff distance between two sets <italic toggle=\"yes\">G</italic> and <italic toggle=\"yes\">P</italic> is defined as<disp-formula id=\"FD8-sensors-25-06815\"><label>(8)</label><mml:math id=\"mm17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo movablelimits=\"true\" form=\"prefix\">max</mml:mo><mml:mfenced separators=\"\" open=\"{\" close=\"}\"><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width=\"0.277778em\"/><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere<disp-formula id=\"FD9-sensors-25-06815\"><label>(9)</label><mml:math id=\"mm18\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits=\"true\" form=\"prefix\">max</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mspace width=\"0.277778em\"/><mml:munder><mml:mo movablelimits=\"true\" form=\"prefix\">min</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>g</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nand<disp-formula id=\"FD10-sensors-25-06815\"><label>(10)</label><mml:math id=\"mm19\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits=\"true\" form=\"prefix\">max</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:mspace width=\"0.277778em\"/><mml:munder><mml:mo movablelimits=\"true\" form=\"prefix\">min</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>g</mml:mi><mml:mo>&#8741;</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle=\"yes\">G</italic> and <italic toggle=\"yes\">P</italic> denote the ground truth and the predicted outcome, respectively, while <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#8741;</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>&#8741;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the Euclidean distance. Intuitively, <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum distance from any point in <italic toggle=\"yes\">G</italic> to its nearest neighbor in <italic toggle=\"yes\">P</italic>, and <inline-formula><mml:math id=\"mm22\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is defined analogously.</p><p>In order to account for image resizing, the HD95 value is adjusted by multiplying it with a spacing parameter, which was specified as 0.033586 in the implementation described by [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>].</p></sec><sec id=\"sec2dot5-sensors-25-06815\"><title>2.5. Implementation Details</title><p>As a preprocessing step, all images were resized to <inline-formula><mml:math id=\"mm23\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Following the setup in [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>], training was conducted using an image patch size of 16 and a batch size of eight. Stochastic Gradient Descent (SGD) optimizer and momentum, with a decaying learning rate, is used as model optimizer. The initial learning rate, momentum, and weight decay were set to 0.01, 0.9, and 0.0001, respectively. An adaptive learning rate strategy was employed, which reduced the learning rate after a certain number of iterations. The number of epochs was set to 150. Both training and testing were implemented and conducted using PyTorch (Version: 2.7.1+cu118) on an NVIDIA RTX A4500 GPU with 20 GB of memory and Python 3.12.7.</p></sec></sec><sec sec-type=\"results\" id=\"sec3-sensors-25-06815\"><title>3. Results and Discussion</title><p><xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref> presents a comparison between our proposed model and the state-of-the-art (SOTA) methods, with Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff Distance (HD95) used as evaluation metrics. The results in <xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref> represent the average performance across several runs. Our reported results are based on our own training, which was conducted for both 150 and 10 epochs to ensure a fair comparison. The only exception is the MicroSegNet scores, that is based on 10 epochs. This score is reported from [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]. As noted by H. Jiang et al. [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>], MicroSegNet training was limited to 10 epochs to avoid overfitting. In contrast, overfitting in our model was mitigated through data augmentation, as discussed earlier. However, to ensure a fair comparison under the same conditions (with augmentation), <xref rid=\"sensors-25-06815-t002\" ref-type=\"table\">Table 2</xref> presents results for our model and an augmented version of MicroSegNet. As shown in <xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref>, our model achieved its best performance when trained for 150 epochs. For comparison, we incorporated several models, some of which are recent, such as RWKV-UNet [<xref rid=\"B49-sensors-25-06815\" ref-type=\"bibr\">49</xref>] and SegU-KAN [<xref rid=\"B50-sensors-25-06815\" ref-type=\"bibr\">50</xref>]. For all evaluated models, we used AG-BCE as the loss function, similar to [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>], to focus more on challenging regions over easily segmented ones during training.</p><p>We employed pretraining for certain models, such as Swin-UNet [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>], but not all models in <xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref> were pretrained. As shown in <xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref>, our model achieved the highest performance, consistently outperforming the competing approaches. Specifically, it attained an average Dice Similarity Coefficient (DSC) of 94.16% and a Hausdorff Distance (HD95) of 1.93 mm. For example, compared with Swin-UNet, the average DSC increased from 0.932 to 0.9416, corresponding to an improvement of nearly 1%, whereas the HD95 decreased from 2.04 mm to 1.93 mm, representing a reduction of approximately 5%. These experimental results demonstrate that the proposed network architecture outperforms state-of-the-art segmentation models, achieving a significantly improved segmentation accuracy. Moreover, <xref rid=\"sensors-25-06815-t001\" ref-type=\"table\">Table 1</xref> also reports the number of FLOPs, model parameters, GPU memory usage, and inference time for each model. Notice, the number of FLOPs and the number of parameters are calculated using the <italic toggle=\"yes\">calflops</italic> library (<uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/MrYxJ/calculate-flops.pytorch\">https://github.com/MrYxJ/calculate-flops.pytorch</uri>, access date (11 March 2024)).</p><p><xref rid=\"sensors-25-06815-f006\" ref-type=\"fig\">Figure 6</xref> presents the segmentation outcomes of the various approaches, namely, SwinUNet, MicroSegNet and our model on the Micro-ultrasound dataset from three images for qualitative comparison. To evaluate the impact of different optimizers, we conducted experiments on the Micro-Ultrasound dataset. <xref rid=\"sensors-25-06815-f007\" ref-type=\"fig\">Figure 7</xref> and <xref rid=\"sensors-25-06815-f008\" ref-type=\"fig\">Figure 8</xref> illustrate the results for four optimizers: SGD [<xref rid=\"B57-sensors-25-06815\" ref-type=\"bibr\">57</xref>,<xref rid=\"B58-sensors-25-06815\" ref-type=\"bibr\">58</xref>], AdamW [<xref rid=\"B59-sensors-25-06815\" ref-type=\"bibr\">59</xref>], SAM [<xref rid=\"B60-sensors-25-06815\" ref-type=\"bibr\">60</xref>,<xref rid=\"B61-sensors-25-06815\" ref-type=\"bibr\">61</xref>], and Adan [<xref rid=\"B62-sensors-25-06815\" ref-type=\"bibr\">62</xref>]. For clarity, the training losses and validation scores are presented in separate figures. SGD, which we used for our model, updates the gradient in each iteration using a randomly selected sample rather than computing the exact gradient. The AdamW optimizer is similar to Adam, but while the Adam optimizer adaptively adjusts the learning rate for each parameter based on first and second order moment estimates of the gradient [<xref rid=\"B6-sensors-25-06815\" ref-type=\"bibr\">6</xref>] using the geometry curvature of the objective function [<xref rid=\"B62-sensors-25-06815\" ref-type=\"bibr\">62</xref>], AdamW decouples the weight decay from the gradient update.</p><p>Sharpness-Aware Minimization (SAM) attempts to find parameter neighborhoods that have uniformly low loss, making it more robust to noisy labels in the training set by perturbing the loss landscape. SAM is particularly beneficial when applied to Vision Transformers (ViTs), as these architectures are more prone to end up in local minima than CNN-based architectures [<xref rid=\"B63-sensors-25-06815\" ref-type=\"bibr\">63</xref>,<xref rid=\"B64-sensors-25-06815\" ref-type=\"bibr\">64</xref>]. However, SAM requires forward and backward passes twice for each iteration. Adaptive Nesterov Momentum (Adan) introduces a new Nesterov Momentum Estimation (NME) method based on Nesterov acceleration. Its efficiency arises from the elimination of the overhead of computing the gradient at the extrapolation point. <xref rid=\"sensors-25-06815-f007\" ref-type=\"fig\">Figure 7</xref> illustrates the behavior of each optimizer during the training. Both AdamW and Adan exhibited lower loss curves, indicating better performance than SGD and SAM. This trend is also confirmed by the evaluation curves shown in <xref rid=\"sensors-25-06815-f008\" ref-type=\"fig\">Figure 8</xref>. Unlike in <xref rid=\"sensors-25-06815-f007\" ref-type=\"fig\">Figure 7</xref>, the curves in <xref rid=\"sensors-25-06815-f008\" ref-type=\"fig\">Figure 8</xref> are intertwined; however, careful examination reveals that AdamW and Adan show better accuracy than the others.</p><p><xref rid=\"sensors-25-06815-f009\" ref-type=\"fig\">Figure 9</xref> and <xref rid=\"sensors-25-06815-f010\" ref-type=\"fig\">Figure 10</xref> depict the loss landscapes of different neural networks as 3D surface plots and contour maps, respectively. These visualizations were generated using the filter-normalization method [<xref rid=\"B65-sensors-25-06815\" ref-type=\"bibr\">65</xref>], with the 3D surface renderings in <xref rid=\"sensors-25-06815-f009\" ref-type=\"fig\">Figure 9</xref> produced using ParaView (Version 5.12) (<uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://paraview.org/\">http://paraview.org/</uri>, access date (5 March 2024)) software. The filter-normalization approach evaluates the loss landscape along filter-normalized directions, where the geometry of the landscape provides insights into model trainability and generalizability [<xref rid=\"B64-sensors-25-06815\" ref-type=\"bibr\">64</xref>]. In particular, smoother and more convex landscapes are typically correlated with lower error values, whereas irregular or chaotic landscapes tend to yield higher training errors.</p></sec><sec id=\"sec4-sensors-25-06815\"><title>4. Ablation Study</title><p>Ablation studies were performed to assess the contribution of each component in the proposed method and to support the design decisions based on the performance outcomes of the Micro-Ultrasound dataset. These results reflect the average scores computed over multiple runs. The results for the five different configurations are summarized in <xref rid=\"sensors-25-06815-t003\" ref-type=\"table\">Table 3</xref>. Notice that DS denotes deep supervision. Our analysis demonstrates the effectiveness of the proposed enhancements compared with the baseline model. As shown in the table, integrating the Mamba blocks into the decoder improved both the DSC and the 95th percentile HD95. The addition of deep supervision further enhanced the performance of both metrics. <xref rid=\"sensors-25-06815-t003\" ref-type=\"table\">Table 3</xref> also lists the number of FLOPs and parameters. It is noteworthy that the majority of components contribute minimally to the increase in FLOPs and parameters; the only exception is the integration of Mamba blocks within the decoder (VSSD), which leads to an increase in both metrics. Also, notice that adding the hyperGraph module slightly improved the DSC score, but it worsened the HD score. Hence, to verify the effectiveness of integrating HyperGraph, <xref rid=\"sensors-25-06815-t004\" ref-type=\"table\">Table 4</xref> presents a comparison of the model with and without HyperGraph. As shown, HyperGraph indeed enhances accuracy based on both metrics. Finally, the combination of Mamba, deep supervision, and ultrasound-specific data augmentation achieved the best average results for both HD95 and DSC scores.</p><p><xref rid=\"sensors-25-06815-t005\" ref-type=\"table\">Table 5</xref> presents a comparison of the model performance based on different Swin block configurations. The Tiny (Swin-T) configuration has channel dimensions of [96, 192, 384, 768] and numbers of heads [3, 6, 12, 24], with the blocks repeated [2, 2, 6, 2]. The Small (Swin-S) configuration uses the same channel dimensions and numbers of heads but repeats the blocks [2, 2, 18, 2] [<xref rid=\"B66-sensors-25-06815\" ref-type=\"bibr\">66</xref>]. Note that we did not test the larger Swin variants (Swin-B and Swin-L) due to their higher GPU memory requirements.</p></sec><sec sec-type=\"conclusions\" id=\"sec5-sensors-25-06815\"><title>5. Conclusions</title><p>This work presents an improved model for ultrasound medical image segmentation, with particular emphasis on micro-US. The proposed architecture features a dual-encoder design: a Swin Transformer branch for capturing global context and a CNN-based branch for extracting fine local details. To further enhance feature representation, a hypergraph network is incorporated to model non-pairwise correlations. The decoder leverages the Mamba 2 architecture, built upon VSSD blocks, to improve localization accuracy. Furthermore, ultrasound-specific augmentation techniques, such as depth attenuation, were employed during training to mitigate overfitting and enhance robustness. Experimental results demonstrate that the proposed method achieves superior performance across multiple evaluation metrics, underscoring its potential to advance segmentation accuracy in ultrasound medical imaging. Future research could focus on mitigating the scarcity of Micro-Ultrasound (US) image datasets using semi-supervised or weakly supervised learning. These approaches can effectively leverage large volumes of unlabeled data to reduce annotation costs while preserving the segmentation boundary accuracy. In particular, weakly supervised methods may utilize alternative supervisory cues such as image-level tags, bounding boxes, or scribbles [<xref rid=\"B67-sensors-25-06815\" ref-type=\"bibr\">67</xref>].</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.A.-Q., M.K.A.; methodology, A.A.-Q., M.K.A.; investigation, A.A.-Q., M.K.A.; writing&#8212;original draft preparation, A.A.-Q.; writing&#8212;review and editing, A.T., M.K.A.; supervision, M.K.A. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The Micro-ultrasound (micro-US) images segmentation dataset is publicly available at <uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://zenodo.org/records/10475293\">https://zenodo.org/records/10475293</uri> and was accessed on 2 February 2025.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-06815\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>H.J.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Zi</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>X.</given-names></name><etal/></person-group><article-title>A genomic and epigenomic atlas of prostate cancer in Asian populations</article-title><source>Nature</source><year>2020</year><volume>580</volume><fpage>93</fpage><lpage>99</lpage><pub-id pub-id-type=\"doi\">10.1038/s41586-020-2135-x</pub-id><pub-id pub-id-type=\"pmid\">32238934</pub-id></element-citation></ref><ref id=\"B2-sensors-25-06815\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Imran</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Pensa</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Falzarano</surname><given-names>S.M.</given-names></name><name name-style=\"western\"><surname>Sisk</surname><given-names>A.E.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>DiBianco</surname><given-names>J.M.</given-names></name><name name-style=\"western\"><surname>Su</surname><given-names>L.M.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Joseph</surname><given-names>J.P.</given-names></name><etal/></person-group><article-title>Image registration of in vivo micro-ultrasound and ex vivo pseudo-whole mount histopathology images of the prostate: A proof-of-concept study</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>96</volume><elocation-id>106657</elocation-id><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.106657</pub-id></element-citation></ref><ref id=\"B3-sensors-25-06815\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wasih</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Ahmad</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>A robust cascaded deep neural network for image reconstruction of single plane wave ultrasound RF data</article-title><source>Ultrasonics</source><year>2023</year><volume>132</volume><fpage>106981</fpage><pub-id pub-id-type=\"doi\">10.1016/j.ultras.2023.106981</pub-id><pub-id pub-id-type=\"pmid\">36913830</pub-id></element-citation></ref><ref id=\"B4-sensors-25-06815\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Imran</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Muralidharan</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Patel</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Pensa</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Benidir</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Grajo</surname><given-names>J.R.</given-names></name><name name-style=\"western\"><surname>Joseph</surname><given-names>J.P.</given-names></name><name name-style=\"western\"><surname>Terry</surname><given-names>R.</given-names></name><etal/></person-group><article-title>MicroSegNet: A deep learning approach for prostate segmentation on micro-ultrasound images</article-title><source>Comput. Med. Imaging Graph.</source><year>2024</year><volume>112</volume><fpage>102326</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compmedimag.2024.102326</pub-id><pub-id pub-id-type=\"pmid\">38211358</pub-id></element-citation></ref><ref id=\"B5-sensors-25-06815\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Dai</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Lian</surname><given-names>C.</given-names></name></person-group><article-title>MSCA-Net: Multi-scale contextual attention network for skin lesion segmentation</article-title><source>Pattern Recognit.</source><year>2023</year><volume>139</volume><fpage>109524</fpage><pub-id pub-id-type=\"doi\">10.1016/j.patcog.2023.109524</pub-id></element-citation></ref><ref id=\"B6-sensors-25-06815\"><label>6.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Al Qurri</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Improved UNet with Attention for Medical Image Segmentation</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>8589</elocation-id><pub-id pub-id-type=\"doi\">10.3390/s23208589</pub-id><pub-id pub-id-type=\"pmid\">37896682</pub-id><pub-id pub-id-type=\"pmcid\">PMC10611347</pub-id></element-citation></ref><ref id=\"B7-sensors-25-06815\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Meng</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name></person-group><article-title>Automatic polyp segmentation via image-level and surrounding-level context fusion deep neural network</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>123</volume><fpage>106168</fpage><pub-id pub-id-type=\"doi\">10.1016/j.engappai.2023.106168</pub-id></element-citation></ref><ref id=\"B8-sensors-25-06815\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Long</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>A.</given-names></name></person-group><article-title>HiFuse: Hierarchical multi-scale feature fusion network for medical image classification</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>87</volume><elocation-id>105534</elocation-id><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2023.105534</pub-id></element-citation></ref><ref id=\"B9-sensors-25-06815\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>ASU-Net++: A nested U-Net with adaptive feature extractions for liver tumor segmentation</article-title><source>Comput. Biol. Med.</source><year>2021</year><volume>136</volume><elocation-id>104688</elocation-id><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2021.104688</pub-id><pub-id pub-id-type=\"pmid\">34523421</pub-id></element-citation></ref><ref id=\"B10-sensors-25-06815\"><label>10.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ahmed</surname><given-names>A.Q.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Improved UNet++ Based on Kolmogorov-Arnold Convolutions</article-title><source>Proceedings of the 2025 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Anchorage, AK, USA</conf-loc><conf-date>14&#8211;18 September 2025</conf-date><fpage>905</fpage><lpage>910</lpage></element-citation></ref><ref id=\"B11-sensors-25-06815\"><label>11.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Al-Qurri</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Fast and Resource-Efficient Ultrasound Segmentation Using FPGAs</article-title><source>Proceedings of the 2025 IEEE International Ultrasonics Symposium (IUS)</source><conf-loc>Utrecht, The Netherlands</conf-loc><conf-date>15&#8211;18 September 2025</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id=\"B12-sensors-25-06815\"><label>12.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id=\"B13-sensors-25-06815\"><label>13.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Girshick</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>K.</given-names></name></person-group><article-title>Non-local neural networks</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>7794</fpage><lpage>7803</lpage></element-citation></ref><ref id=\"B14-sensors-25-06815\"><label>14.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Bao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>H.</given-names></name></person-group><article-title>Dual attention network for scene segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>3146</fpage><lpage>3154</lpage></element-citation></ref><ref id=\"B15-sensors-25-06815\"><label>15.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Woo</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J.Y.</given-names></name><name name-style=\"western\"><surname>Kweon</surname><given-names>I.S.</given-names></name></person-group><article-title>Cbam: Convolutional block attention module</article-title><source>Computer Vision&#8212;ECCV 2018, Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8&#8211;14 September 2018</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2018</year><fpage>3</fpage><lpage>19</lpage></element-citation></ref><ref id=\"B16-sensors-25-06815\"><label>16.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Shazeer</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jones</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style=\"western\"><surname>Kaiser</surname><given-names>&#321;.</given-names></name><name name-style=\"western\"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Advances in Neural Information Processing Systems 30 (NIPS 2017), Proceedings of the Annual Conference on Neural Information Processing Systems 2017, Long Beach, CA, USA, 4&#8211;9 December 2017</source><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Guyon</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Von Luxburg</surname><given-names>U.</given-names></name><name name-style=\"western\"><surname>Bengio</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wallach</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Fergus</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Vishwanathan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Garnett</surname><given-names>R.</given-names></name></person-group><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2017</year></element-citation></ref><ref id=\"B17-sensors-25-06815\"><label>17.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Iwamoto</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>X.H.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y.W.</given-names></name><name name-style=\"western\"><surname>Tong</surname><given-names>R.</given-names></name></person-group><article-title>Mixed transformer u-net for medical image segmentation</article-title><source>Proceedings of the ICASSP 2022&#8211;2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Singapore</conf-loc><conf-date>22&#8211;27 May 2022</conf-date><fpage>2390</fpage><lpage>2394</lpage></element-citation></ref><ref id=\"B18-sensors-25-06815\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Landman</surname><given-names>B.A.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>S.K.</given-names></name></person-group><article-title>Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives</article-title><source>Med. Image Anal.</source><year>2023</year><volume>85</volume><fpage>102762</fpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2023.102762</pub-id><pub-id pub-id-type=\"pmid\">36738650</pub-id><pub-id pub-id-type=\"pmcid\">PMC10010286</pub-id></element-citation></ref><ref id=\"B19-sensors-25-06815\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"arxiv\">2010.11929</pub-id></element-citation></ref><ref id=\"B20-sensors-25-06815\"><label>20.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>M.</given-names></name></person-group><article-title>Swin-unet: Unet-like pure transformer for medical image segmentation</article-title><source>Computer Vision&#8212;ECCV 2022 Workshops, Proceedings of the European Conference on Computer Vision, Tel Aviv, Israel, 23&#8211;27 October, 2022</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2022</year><fpage>205</fpage><lpage>218</lpage></element-citation></ref><ref id=\"B21-sensors-25-06815\"><label>21.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zuo</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Vision transformers for dense prediction: A survey</article-title><source>Knowl.-Based Syst.</source><year>2022</year><volume>253</volume><fpage>109552</fpage><pub-id pub-id-type=\"doi\">10.1016/j.knosys.2022.109552</pub-id></element-citation></ref><ref id=\"B22-sensors-25-06815\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gao</surname><given-names>B.B.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Z.</given-names></name></person-group><article-title>CSTrans: Correlation-guided Self-Activation Transformer for Counting Everything</article-title><source>Pattern Recognit.</source><year>2024</year><volume>153</volume><fpage>110556</fpage><pub-id pub-id-type=\"doi\">10.1016/j.patcog.2024.110556</pub-id></element-citation></ref><ref id=\"B23-sensors-25-06815\"><label>23.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Si</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title>Dual-branch hybrid network for lesion segmentation in gastric cancer images</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>6377</elocation-id><pub-id pub-id-type=\"doi\">10.1038/s41598-023-33462-y</pub-id><pub-id pub-id-type=\"pmid\">37076573</pub-id><pub-id pub-id-type=\"pmcid\">PMC10115814</pub-id></element-citation></ref><ref id=\"B24-sensors-25-06815\"><label>24.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Kazerouni</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Heidari</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Aghdam</surname><given-names>E.K.</given-names></name><name name-style=\"western\"><surname>Molaei</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Jose</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Roy</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><article-title>Advances in medical image analysis with vision transformers: A comprehensive review</article-title><source>Med. Image Anal.</source><year>2023</year><volume>91</volume><fpage>103000</fpage><pub-id pub-id-type=\"doi\">10.1016/j.media.2023.103000</pub-id><pub-id pub-id-type=\"pmid\">37883822</pub-id></element-citation></ref><ref id=\"B25-sensors-25-06815\"><label>25.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><article-title>Transfuse: Fusing transformers and cnns for medical image segmentation</article-title><source>Medical Image Computing and Computer Assisted Intervention&#8212;MICCAI 2021, Proceedings of the Medical Image Computing and Computer Assisted Intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, 27 September&#8211;1 October 2021</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>14</fpage><lpage>24</lpage></element-citation></ref><ref id=\"B26-sensors-25-06815\"><label>26.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Adeli</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Yuille</surname><given-names>A.L.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>Transunet: Transformers make strong encoders for medical image segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2102.04306</pub-id><pub-id pub-id-type=\"arxiv\">2102.04306</pub-id></element-citation></ref><ref id=\"B27-sensors-25-06815\"><label>27.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ahmed</surname><given-names>A.Q.</given-names></name><name name-style=\"western\"><surname>Alqarni</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Trifuse: Triplet Encoders Network for Medical Image Segmentation</article-title><source>Proceedings of the 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</source><conf-loc>Houston, TX, USA</conf-loc><conf-date>14&#8211;17 April 2025</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id=\"B28-sensors-25-06815\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Tan</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Du</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Tong</surname><given-names>T.</given-names></name></person-group><article-title>CoTrFuse: A novel framework by fusing CNN and transformer for medical image segmentation</article-title><source>Phys. Med. Biol.</source><year>2023</year><volume>68</volume><elocation-id>175027</elocation-id><pub-id pub-id-type=\"doi\">10.1088/1361-6560/acede8</pub-id><pub-id pub-id-type=\"pmid\">37605997</pub-id></element-citation></ref><ref id=\"B29-sensors-25-06815\"><label>29.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Al-Antary</surname><given-names>M.T.</given-names></name><name name-style=\"western\"><surname>Heidari</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><article-title>Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>108205</fpage><lpage>108215</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2022.3211501</pub-id></element-citation></ref><ref id=\"B30-sensors-25-06815\"><label>30.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>C.</given-names></name></person-group><article-title>Multiscale transunet++: Dense hybrid U-Net with transformer for medical image segmentation</article-title><source>Signal, Image Video Process.</source><year>2022</year><volume>16</volume><fpage>1607</fpage><lpage>1614</lpage><pub-id pub-id-type=\"doi\">10.1007/s11760-021-02115-w</pub-id></element-citation></ref><ref id=\"B31-sensors-25-06815\"><label>31.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yin</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>H.</given-names></name></person-group><article-title>CoT-UNet++: A medical image segmentation method based on contextual transformer and dense connection</article-title><source>Math. Biosci. Eng.</source><year>2023</year><volume>20</volume><fpage>8320</fpage><lpage>8336</lpage><pub-id pub-id-type=\"doi\">10.3934/mbe.2023364</pub-id><pub-id pub-id-type=\"pmid\">37161200</pub-id></element-citation></ref><ref id=\"B32-sensors-25-06815\"><label>32.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Al-Qurri</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Ultrasound Image Segmentation using a Model of Transformer and DFT</article-title><source>Proceedings of the 2024 IEEE UFFC Latin America Ultrasonics Symposium (LAUS)</source><conf-loc>Montevideo, Uruguay</conf-loc><conf-date>8&#8211;10 May 2024</conf-date><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id=\"B33-sensors-25-06815\"><label>33.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Ioffe</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Shlens</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wojna</surname><given-names>Z.</given-names></name></person-group><article-title>Rethinking the inception architecture for computer vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id=\"B34-sensors-25-06815\"><label>34.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zeng</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Y.</given-names></name></person-group><article-title>Small but Mighty: Enhancing 3D Point Clouds Semantic Segmentation with U-Next Framework</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"arxiv\">2304.00749</pub-id><pub-id pub-id-type=\"doi\">10.1016/j.jag.2024.104309</pub-id></element-citation></ref><ref id=\"B35-sensors-25-06815\"><label>35.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Dehghan</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Jagersand</surname><given-names>M.</given-names></name></person-group><article-title>Basnet: Boundary-aware salient object detection</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>7479</fpage><lpage>7489</lpage></element-citation></ref><ref id=\"B36-sensors-25-06815\"><label>36.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xie</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Vmamba: Visual state space model</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2401.10166</pub-id></element-citation></ref><ref id=\"B37-sensors-25-06815\"><label>37.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tan</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Le</surname><given-names>Q.</given-names></name></person-group><article-title>Efficientnet: Rethinking model scaling for convolutional neural networks</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>9&#8211;15 June 2019</conf-date><fpage>6105</fpage><lpage>6114</lpage></element-citation></ref><ref id=\"B38-sensors-25-06815\"><label>38.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Howard</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zhmoginov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L.C.</given-names></name></person-group><article-title>Mobilenetv2: Inverted residuals and linear bottlenecks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;23 June 2018</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id=\"B39-sensors-25-06815\"><label>39.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Kundu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Z.</given-names></name></person-group><article-title>Vision hgnn: An image is more than a graph of nodes</article-title><source>Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>1&#8211;6 October 2023</conf-date><fpage>19878</fpage><lpage>19888</lpage></element-citation></ref><ref id=\"B40-sensors-25-06815\"><label>40.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Feng</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>You</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ji</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>Y.</given-names></name></person-group><article-title>Hypergraph neural networks</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>27 January&#8211;1 February 2019</conf-date><fpage>3558</fpage><lpage>3565</lpage></element-citation></ref><ref id=\"B41-sensors-25-06815\"><label>41.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xia</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Fu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>Z.</given-names></name></person-group><article-title>Make U-Net Greater: An Easy-to-Embed Approach to Improve Segmentation Performance Using Hypergraph</article-title><source>Comput. Syst. Sci. Eng.</source><year>2022</year><volume>42</volume><fpage>319</fpage><lpage>333</lpage><pub-id pub-id-type=\"doi\">10.32604/csse.2022.022314</pub-id></element-citation></ref><ref id=\"B42-sensors-25-06815\"><label>42.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chai</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Jain</surname><given-names>R.K.</given-names></name><name name-style=\"western\"><surname>Mo</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tateyama</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y.W.</given-names></name></person-group><article-title>A Novel Adaptive Hypergraph Neural Network for Enhancing Medical Image Segmentation</article-title><source>Medical Image Computing and Computer Assisted Intervention&#8212;MICCAI 2024, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, Marrakesh, Morocco, 6&#8211;10 October 2024</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2024</year><fpage>23</fpage><lpage>33</lpage></element-citation></ref><ref id=\"B43-sensors-25-06815\"><label>43.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name></person-group><article-title>Transformers are ssms: Generalized models and efficient algorithms through structured state space duality</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2405.21060</pub-id><pub-id pub-id-type=\"arxiv\">2405.21060</pub-id></element-citation></ref><ref id=\"B44-sensors-25-06815\"><label>44.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Al-Qurri</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Enhancing Medical Image Segmentation with Mamba and UNet++</article-title><source>Proceedings of the 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</source><conf-loc>Houston, TX, USA</conf-loc><conf-date>14&#8211;17 April 2025</conf-date><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id=\"B45-sensors-25-06815\"><label>45.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>C.</given-names></name></person-group><article-title>VSSD: Vision Mamba with Non-Casual State Space Duality</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2407.18559</pub-id><pub-id pub-id-type=\"arxiv\">2407.18559</pub-id></element-citation></ref><ref id=\"B46-sensors-25-06815\"><label>46.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Dao</surname><given-names>T.</given-names></name></person-group><article-title>Mamba: Linear-time sequence modeling with selective state spaces</article-title><source>Proceedings of the First Conference on Language Modeling</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>7&#8211;9 October 2024</conf-date></element-citation></ref><ref id=\"B47-sensors-25-06815\"><label>47.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Vision mamba: Efficient visual representation learning with bidirectional state space model</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2401.09417</pub-id><pub-id pub-id-type=\"arxiv\">2401.09417</pub-id></element-citation></ref><ref id=\"B48-sensors-25-06815\"><label>48.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tupper</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Gagn&#233;</surname><given-names>C.</given-names></name></person-group><article-title>Revisiting Data Augmentation for Ultrasound Images</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2501.13193</pub-id><pub-id pub-id-type=\"arxiv\">2501.13193</pub-id></element-citation></ref><ref id=\"B49-sensors-25-06815\"><label>49.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jiang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Rwkv-unet: Improving unet with long-range cooperation for effective medical image segmentation</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type=\"arxiv\">2501.08458</pub-id></element-citation></ref><ref id=\"B50-sensors-25-06815\"><label>50.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>Y.</given-names></name></person-group><article-title>U-kan makes strong backbone for medical image segmentation and generation</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>25 February&#8211;4 March 2025</conf-date><fpage>4652</fpage><lpage>4660</lpage></element-citation></ref><ref id=\"B51-sensors-25-06815\"><label>51.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Rahman Siddiquee</surname><given-names>M.M.</given-names></name><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>J.</given-names></name></person-group><article-title>Unet++: A nested u-net architecture for medical image segmentation</article-title><source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Proceedings of the 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Granada, Spain, 20 September 2018</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2018</year><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-030-00889-5_1</pub-id><pub-id pub-id-type=\"pmcid\">PMC7329239</pub-id><pub-id pub-id-type=\"pmid\">32613207</pub-id></element-citation></ref><ref id=\"B52-sensors-25-06815\"><label>52.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Heidari</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kazerouni</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Soltany</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Aghdam</surname><given-names>E.K.</given-names></name><name name-style=\"western\"><surname>Cohen-Adad</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><article-title>Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;7 January 2023</conf-date><fpage>6202</fpage><lpage>6212</lpage></element-citation></ref><ref id=\"B53-sensors-25-06815\"><label>53.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azad</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Niggemeier</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>H&#252;ttemann</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kazerouni</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Aghdam</surname><given-names>E.K.</given-names></name><name name-style=\"western\"><surname>Velichko</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Bagci</surname><given-names>U.</given-names></name><name name-style=\"western\"><surname>Merhof</surname><given-names>D.</given-names></name></person-group><article-title>Beyond self-attention: Deformable large kernel attention for medical image segmentation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#8211;8 January 2024</conf-date><fpage>1287</fpage><lpage>1297</lpage></element-citation></ref><ref id=\"B54-sensors-25-06815\"><label>54.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>Q.</given-names></name></person-group><article-title>H-vmunet: High-order vision mamba unet for medical image segmentation</article-title><source>Neurocomputing</source><year>2025</year><volume>624</volume><fpage>129447</fpage><pub-id pub-id-type=\"doi\">10.1016/j.neucom.2025.129447</pub-id></element-citation></ref><ref id=\"B55-sensors-25-06815\"><label>55.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Gu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Tao</surname><given-names>X.</given-names></name></person-group><article-title>VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2403.09157</pub-id></element-citation></ref><ref id=\"B56-sensors-25-06815\"><label>56.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ruan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>S.</given-names></name></person-group><article-title>VM-UNet: Vision Mamba UNet for Medical Image Segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2402.02491</pub-id><pub-id pub-id-type=\"doi\">10.1145/3767748</pub-id></element-citation></ref><ref id=\"B57-sensors-25-06815\"><label>57.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sun</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>J.</given-names></name></person-group><article-title>A survey of optimization methods from a machine learning perspective</article-title><source>IEEE Trans. Cybern.</source><year>2019</year><volume>50</volume><fpage>3668</fpage><lpage>3681</lpage><pub-id pub-id-type=\"doi\">10.1109/TCYB.2019.2950779</pub-id><pub-id pub-id-type=\"pmid\">31751262</pub-id></element-citation></ref><ref id=\"B58-sensors-25-06815\"><label>58.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Shang</surname><given-names>M.</given-names></name></person-group><article-title>Adjusted stochastic gradient descent for latent factor analysis</article-title><source>Inf. Sci.</source><year>2022</year><volume>588</volume><fpage>196</fpage><lpage>213</lpage><pub-id pub-id-type=\"doi\">10.1016/j.ins.2021.12.065</pub-id></element-citation></ref><ref id=\"B59-sensors-25-06815\"><label>59.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Loshchilov</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Hutter</surname><given-names>F.</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type=\"arxiv\">1711.05101</pub-id></element-citation></ref><ref id=\"B60-sensors-25-06815\"><label>60.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Roux</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Schmidt</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Bach</surname><given-names>F.</given-names></name></person-group><article-title>A stochastic gradient method with an exponential convergence _rate for finite training sets</article-title><source>Advances in Neural Information Processing Systems 25 (NIPS 2012), Proceedings of the Annual Conference on Neural Information Processing Systems 2012, Lake Tahoe, NV, USA, 3&#8211;6 December 2012</source><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Pereira</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Burges</surname><given-names>C.J.</given-names></name><name name-style=\"western\"><surname>Bottou</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2012</year></element-citation></ref><ref id=\"B61-sensors-25-06815\"><label>61.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Foret</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Kleiner</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mobahi</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Neyshabur</surname><given-names>B.</given-names></name></person-group><article-title>Sharpness-aware minimization for efficiently improving generalization</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"arxiv\">2010.01412</pub-id></element-citation></ref><ref id=\"B62-sensors-25-06815\"><label>62.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>S.</given-names></name></person-group><article-title>Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>9508</fpage><lpage>9520</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2024.3423382</pub-id><pub-id pub-id-type=\"pmid\">38963744</pub-id></element-citation></ref><ref id=\"B63-sensors-25-06815\"><label>63.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Hsieh</surname><given-names>C.J.</given-names></name><name name-style=\"western\"><surname>Gong</surname><given-names>B.</given-names></name></person-group><article-title>When vision transformers outperform resnets without pre-training or strong data augmentations</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"arxiv\">2106.01548</pub-id></element-citation></ref><ref id=\"B64-sensors-25-06815\"><label>64.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qurri</surname><given-names>A.A.</given-names></name><name name-style=\"western\"><surname>Almekkawy</surname><given-names>M.</given-names></name></person-group><article-title>Hybrid MultiResUNet with transformers for medical image segmentation</article-title><source>Biomed. Signal Process. Control</source><year>2025</year><volume>110</volume><elocation-id>108056</elocation-id><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2025.108056</pub-id></element-citation></ref><ref id=\"B65-sensors-25-06815\"><label>65.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Taylor</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Studer</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Goldstein</surname><given-names>T.</given-names></name></person-group><article-title>Visualizing the loss landscape of neural nets</article-title><source>Advances in Neural Information Processing Systems 31 (NeurIPS 2018), Proceedings of Annual Conference on Neural Information Processing Systems 2018, Montr&#233;al, QC, Canada, 3&#8211;8 December 2018</source><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Bengio</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wallach</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Larochelle</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Grauman</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Cesa-Bianchi</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Garnett</surname><given-names>R.</given-names></name></person-group><publisher-name>Curran Associates Inc.</publisher-name><publisher-loc>Red Hook, NY, USA</publisher-loc><year>2018</year></element-citation></ref><ref id=\"B66-sensors-25-06815\"><label>66.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>11&#8211;17 October 2021</conf-date><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id=\"B67-sensors-25-06815\"><label>67.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Duan</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Grau</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>J.</given-names></name></person-group><article-title>From gaze to insight: Bridging human visual attention and vision language model explanation for weakly-supervised medical image segmentation</article-title><source>IEEE Trans. Med. Imaging</source><year>2025</year><pub-id pub-id-type=\"doi\">10.1109/TMI.2025.3616598</pub-id><pub-id pub-id-type=\"pmid\">41052168</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-06815-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Overall network architecture with dual encoders, Hyper GNN, and Mamba decoder.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Diagram of the Swin-Transformer Block based on Swin-Unet. The Swin transformer employs a window-based multi-head self-attention (W-MSA) module and a shifted window-based multi-head self-attention (SW-MSA) module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Architecture of the proposed fusion module block, adapted from the STCF fusion module [<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>]. The SE block and CBAM are used to perform channel and spatial attention, respectively.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>The Adaptive Hypergraph Construction module, implemented following the method proposed by Chai et al. [<xref rid=\"B42-sensors-25-06815\" ref-type=\"bibr\">42</xref>]. This module constructs hyper-edges adaptively based on node degree, utilizing the K-Nearest Neighbors (KNN) algorithm to form the adjacency matrix for hypergraph generation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>The VSSD block based on [<xref rid=\"B45-sensors-25-06815\" ref-type=\"bibr\">45</xref>] that employ Mamba2.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Qualitative comparison of CT slices from the Micro-US dataset, showing prostate boundaries donated as a green circle. From left to right: (<bold>a</bold>) Ground Truth, (<bold>b</bold>) SwinUNet, (<bold>c</bold>) MicroSegNet, and (<bold>d</bold>) our model&#8217;s prediction.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Dice Loss for different optimizers during training.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Dice Score During Evaluation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g008.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f009\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Loss surface visualization of different neural networks using the filter-normalization method. Top-left: UNet++; top-right: Swin-UNet; bottom-left: MicroSegNet; bottom-right: our model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g009.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06815-f010\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Contour maps of the 2D loss landscape for different neural networks using the filter-normalization method. Top left: U-Net++; top right: Swin-UNet; bottom left: MicroSegNet; bottom right: proposed model.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06815-g010.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-06815-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06815-t001_Table 1</object-id><label>Table 1</label><caption><p>The segmentation accuracy on the micro-US dataset was evaluated using the average Dice score and 95% Hausdorff distance (HD95). The best results are indicated in bold. The Dice scores marked with * are reported in [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>], and their values are provided with a precision of three decimal places. All results are based on training for 150 epochs and 10 epochs to ensure a fair comparison. FLOPs (G) denote the number of floating-point operations (in billions). #Params (M) indicates the number of parameters (in millions). GPU Mem (G) denotes GPU memory usage (in gigabytes). Inference (S) represents the inference time (in seconds).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Model</th><th colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\">150 Epochs</th><th colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\">10 Epochs</th><td align=\"left\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"left\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"left\" valign=\"middle\" style=\"border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n</td><td rowspan=\"2\" align=\"left\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" colspan=\"1\">\n<bold>Inference (S)</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>DSC&#8593;</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>HD&#8595;</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>DSC&#8593;</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>HD&#8595;</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>FLOPs (G)</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>#Params (M)</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>GPU Mem (G)</bold>\n</td></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Unet [<xref rid=\"B26-sensors-25-06815\" ref-type=\"bibr\">26</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8897</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.94</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8420</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">7.62</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">21.37</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">7.85</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.93</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>120</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UNet++ [<xref rid=\"B51-sensors-25-06815\" ref-type=\"bibr\">51</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9081</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.81 1</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8894</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4.82</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">53.1</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9.16</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">11.39</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">134</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">TransUNet [<xref rid=\"B26-sensors-25-06815\" ref-type=\"bibr\">26</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9293</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.39</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9303</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.20</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">58.49</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">105.28</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.57</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">133</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Swin-UNet [<xref rid=\"B20-sensors-25-06815\" ref-type=\"bibr\">20</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9327</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.04</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9218</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.49</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">17.4</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.38</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">19.46</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">165</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">TransNorm [<xref rid=\"B29-sensors-25-06815\" ref-type=\"bibr\">29</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9214</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.63</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9232</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.45</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.18</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">117.63</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">16.54</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">130</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HiFormer-B [<xref rid=\"B52-sensors-25-06815\" ref-type=\"bibr\">52</xref>,<xref rid=\"B53-sensors-25-06815\" ref-type=\"bibr\">53</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8967</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4.40</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8784</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.34</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8.045</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">25.51</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">11.40</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">141</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CoTrFuse [<xref rid=\"B28-sensors-25-06815\" ref-type=\"bibr\">28</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9266</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.74</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9065</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.42</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33.07</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">56.19</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">15.30</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">166</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">RWKV-UNet [<xref rid=\"B49-sensors-25-06815\" ref-type=\"bibr\">49</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8866</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4.62</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.7958</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.85</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">57.44</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">120.24</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.96</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">122</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">H-vmunet [<xref rid=\"B54-sensors-25-06815\" ref-type=\"bibr\">54</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8817</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3.34</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8746</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">4.39</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1140.6</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8.97</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>3.79</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">167</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Seg. U-KAN [<xref rid=\"B50-sensors-25-06815\" ref-type=\"bibr\">50</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8918</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.09</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8409</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">6.81</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">14.02</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>6.35</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">15.01</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">130</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">VM-UNet [<xref rid=\"B55-sensors-25-06815\" ref-type=\"bibr\">55</xref>,<xref rid=\"B56-sensors-25-06815\" ref-type=\"bibr\">56</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9042</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.89</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.8630</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.30</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>7.56</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">34.62</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">18.97</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">126</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">MicroSegNet [<xref rid=\"B4-sensors-25-06815\" ref-type=\"bibr\">4</xref>]</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9341</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.23</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"><bold>0.939</bold> *</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>2.02</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">58.49</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">105.28</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">10.96</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">131</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Ours</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>0.9416</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>1.93</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">0.9380</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>2.02</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">70.72</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">93.13</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">11.93</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">202</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06815-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06815-t002_Table 2</object-id><label>Table 2</label><caption><p>Quantitative comparison between our model and MicroSegNet when data augmentation is applied to MicroSegNet.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DSC&#8593;</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HD95&#8595;</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">MicroSegNet with Augmentation (150 Epochs)</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9321</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.29</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">MicroSegNet with Augmentation (10 Epochs)</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.9356</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.04</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Ours</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>0.9416</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>1.93</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06815-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06815-t003_Table 3</object-id><label>Table 3</label><caption><p>Contribution of each component to the overall performance on the Micro-Ultrasound dataset. FLOPs (G) denote the number of floating-point operations (in billions). #Params (M) indicates the number of parameters (in millions).</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">\n</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Mamba (VSSD)</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DS</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HyperGraph</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Augment.</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs\n(G)</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">#Params\n(M)</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DSC&#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HD&#8595;</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Baseline</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>42.4</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>66.79</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.28</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.05</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.19</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.05</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.73</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.95</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.75</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.99</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Proposed Model</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">70.72</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">93.13</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>94.16</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>1.93</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06815-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06815-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of the model with and without HyperGraph on the Micro-Ultrasound Dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DSC&#8593;</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HD95&#8595;</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Without HyperGraph</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.96</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.95</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">With HyperGraph (Overall)</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>94.16</bold>\n</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>1.93</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06815-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06815-t005_Table 5</object-id><label>Table 5</label><caption><p>Ablation study on the model scale for the Swin Blocks.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Methods</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">DSC&#8593;</th><th align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">HD95&#8595;</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Swin-T</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>94.16</bold>\n</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>1.93</bold>\n</td></tr><tr><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Swin-S</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">93.96</td><td align=\"left\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">1.95</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656456 PMC12656456.1 12656456 12656456 41305024 10.3390/s25226815 sensors-25-06815 1 Article Enhanced Deep Neural Network for Prostate Segmentation in Micro-Ultrasound Images https://orcid.org/0009-0000-9331-0187 AL-Qurri Ahmed Writing &#8211; review &amp; editing Writing &#8211; original draft Conceptualization Methodology Software Validation 1 Thaher Asem Validation 2 https://orcid.org/0000-0002-9222-3003 Almekkawy Mohamed Khaled Supervision Validation Investigation Writing &#8211; review &amp; editing 1 * Ling Steve Academic Editor Lyu Juan Academic Editor 1 The School of Electrical Engineering and Computer Science, Pennsylvania State University, University Park, PA 16802, USA; aqa6122@psu.edu 2 Independent Researcher, State College, PA 16803, USA * Correspondence: mka9@psu.edu 07 11 2025 11 2025 25 22 501335 6815 08 9 2025 29 10 2025 04 11 2025 07 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Prostate cancer is a global health concern, and early diagnosis plays a vital role in improving the survival rate. Accurate segmentation is a key step in the automated diagnosis of prostate cancer; however, manual segmentation remains time-consuming and challenging. Micro-Ultrasound (US) is particularly well-suited for prostate cancer detection, offering real-time imaging with a resolution comparable to that of MRI. This enables improved spatial resolution and detailed visualization of small anatomical structures. With recent advances in deep learning for medical image segmentation, precise prostate segmentation has become critical for biopsy guidance, disease diagnosis, and follow-up. However, segmentation of the prostate in micro-US images remains challenging due to indistinct boundaries between the prostate and surrounding tissue. In this work, we propose a model for precise micro-ultrasound image segmentation. The model employs a dual-encoder architecture that integrates Convolutional Neural Networks (CNN) and Transformer-based encoders in parallel, combined with a fusion module to capture both global dependencies and low-level spatial details. More importantly, we introduce a decoder based on Mamba v2 to enhance segmentation accuracy. A Hypergraph Neural Network (HGNN) is employed as a bridge between the dual encoders and Mamba decoder to model correlations among non-pairwise connections. Experimental results on micro-US datasets demonstrated that our model achieved superior or comparable performance to state-of-the-art methods, with a Dice score of 0.9416 and an HD95 of 1.93. UNet UNet++ Transformer CNN attention medical imaging Micro-Ultrasound Hypergraph Neural Network Mamba This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Prostate cancer is the second most prevalent cancer worldwide, with over 1.2 million new cases reported in 2020 [ 1 ]. Accurate imaging is critical for an early diagnosis. Traditionally, lesions in the prostate are identified using multiparametric MRI (mpMRI), which provides high-resolution anatomical and functional information about the lesions. However, MRI is expensive, time-consuming, and often inaccessible in many clinical settings, limiting its widespread adoption [ 2 ]. Ultrasound has become a widely adopted imaging modality owing to its low cost and high accessibility [ 3 ]. Micro-Ultrasound (micro-US) has recently emerged as a promising alternative, operating at substantially higher frequencies (typically 29 MHz or greater) than conventional ultrasound, thereby offering improved spatial resolution for prostate imaging [ 4 ]. Medical image segmentation plays a critical role in clinical practice by enabling the automatic localization of the prostate capsule. Although manual segmentation is feasible, it remains a time-consuming and labor-intensive task that is often inadequate for capturing the broader pathological context [ 5 , 6 , 7 ]. Recently, deep learning has been successfully applied to medical image segmentation. Nevertheless, despite advances in neural network architectures, segmentation remains a challenging task owing to noisy and scattered features, low resolution, weak boundaries, and irregular shapes of human organs [ 6 , 8 , 9 ]. In deep learning architectures, the convolution operation is the fundamental building block of U-Net [ 10 , 11 ]; however, its inherently limited receptive field constrains the network&#8217;s ability to capture global context [ 5 ]. To overcome this limitation, attention mechanisms have been introduced into deep learning. Attention allocates computational resources to regions containing the most relevant information, mimicking the functionality of the human visual system [ 6 , 12 ]. Consequently, several attention mechanisms have been proposed that can be categorized according to various criteria. One common categorization is based on the dimensions along which the attention feature map functions. For instance, channel attention exploits inter-channel dependencies, which are employed in Squeeze-and-Excitation (SE) networks [ 12 ]. In contrast, spatial attention emphasizes regions within feature maps that contain critical information while suppressing less relevant areas, as demonstrated in Non-Local Neural Networks [ 13 ]. Attention mechanisms can also be categorized based on the operation of the attention map [ 5 ]. For example, global attention enhances interactions across both channels and spatial locations, as in DANet [ 5 , 14 ], whereas local attention, such as in CBAM [ 15 ], focuses on specific spatial regions and their relationships with the channels. Recently, Transformer-based models have gained prominence, initially introduced for sequence-to-sequence tasks in natural language processing (NLP) [ 16 ]. Transformers have been adopted across various domains due to their powerful attention mechanisms. Unlike CNN-based models, they overcome the limitation of restricted receptive fields by employing a self-attention (SA) mechanism [ 17 ]. The SA captures the internal correlations among all input tokens, enabling the modeling of long-range dependencies [ 18 ]. The Transformer architecture incorporates multiple self-attention blocks, known as multi-head self-attention (MHSA), which operate in parallel to generate diverse feature representations [ 17 , 18 ]. Building on this, Dosovitskiy et al. [ 19 ] introduced the Vision Transformer (ViT) for computer vision tasks, such as image classification, achieving state-of-the-art results on ImageNet. Moreover, Cao et al. [ 20 ] introduced Swin-Unet, the first U-shaped segmentation network based entirely on a Transformer architecture [ 21 ]. Swin-Unet integrates Swin Transformer blocks, where Window Multi-Head Self-Attention (W-MSA) captures fine-grained details within each window, and Shifted Window Self-Attention (SW-MSA) models cross-window interactions [ 22 ]. Unlike U-Net, Swin-Unet replaces conventional upsampling with patch-expanding layers, eliminating the need for convolutions or interpolation [ 6 , 23 , 24 ]. Nevertheless, despite its strong capability to capture the global context, the Transformer remains limited in modeling fine-grained details due to its lack of spatial inductive bias for local information. This limitation is particularly evident in medical image segmentation tasks [ 25 ]. To leverage the strengths of both Transformers and U-Net, Chen et al. [ 26 ] proposed the TransUNet. This hybrid architecture employs Convolutional Neural Networks (CNNs) to extract low-level features and a Transformer to capture global information [ 25 ]. Furthermore, hybrid models, such as TransFuse, adopt parallel encoder branches. In TransFuse [ 25 ], both a CNN-based spatial branch and a transformer-based global branch are fused using a BiFusion module [ 24 , 27 ]. In the BiFusion module, Transformer features are refined using a channel attention SE block [ 12 ], whereas the CNN features are enhanced using a spatial attention module inspired by CBAM [ 15 ]. The features from both branches were then combined using a Hadamard product to model their interactions. Similarly, CoTrFuse [ 28 ] follows a dual-branch strategy but incorporates Swin Transformer blocks for the global branch and EfficientNet blocks for the spatial branch, with feature fusion achieved through a specialized STCF module. Other researchers have focused on improving decoder performance. For instance, TransNorm [ 29 ] employs spatial normalization outputs from the Transformer to construct a two-level attention gate, where channel attention normalizes the feature representation to emphasize more informative channels and Transformer-produced spatial coefficients to amplify the relevant areas of the feature map. Other architectures, such as MS-TransUNet++ [ 30 ] and CoT-UNet++ [ 31 ], enhance CNN-Transformer hybrid architectures by introducing dense skip connections between encoders and decoders at multiple levels, similar to U-Net++ [ 6 ]. Some works have explored the frequency domain. For example, Discrete Fourier Transform (DFT) modules have been integrated to capture long-range dependencies more effectively [ 32 ]. Another line of research utilized deep supervision, which was first introduced in Google&#8217;s Inception architecture [ 5 , 33 ]. U-Net++ adopts deep supervision to help intermediate layers learn discriminative features and mitigate vanishing gradients [ 34 ]. Other network architectures incorporate side outputs for deep supervision. For example, BASNet [ 35 ] employs deep supervision via side outputs and was proposed for salient-object detection. One network relevant to this research is MicroSegNet, which employs multi-scale deep supervision for prostate segmentation from micro-ultrasound images [ 4 ]. In this paper, we propose an enhanced network architecture that utilizes a dual encoder, comprising both CNN and Transformer encoders, to effectively capture both local and global contextual information. The decoder is designed based on Mamba v2 in combination with CNN layers to improve the segmentation accuracy. Additionally, a Hypergraph Neural Network (HGNN) is integrated into the skip connections to capture non-pairwise correlations. To mitigate overfitting, we employed a tailored ultrasound-specific augmentation scheme incorporating depth attenuation, Gaussian shadows, haze artifacts, and speckle reduction.The experimental results demonstrate that the proposed architecture outperforms state-of-the-art segmentation models, achieving a superior segmentation accuracy. Our contributions can be summarized as follows: We propose a model for precise Micro-Ultrasound medical image segmentation composed of dual encoders with a fusion module designed to capture both local details and long-range dependencies. A Hypergraph Neural Network (HGNN) is integrated into the skip connections to model non-pairwise correlations. To further enhance segmentation accuracy, a Mamba-based decoder is incorporated, utilizing VSSD blocks built upon Mamba-2 and NC-SSD. Experimental results demonstrate that our method achieves superior performance on the Micro-Ultrasound (US) prostate medical image segmentation dataset. 2. Materials and Methods 2.1. Overall Architecture Inspired by CoTrFuse [ 28 ], we designed a network comprising two parallel branches. One branch employs a Transformer (Swin blocks) to capture the global context, whereas the other branch utilizes EfficientNet blocks to extract local details. The outputs from both branches are fused using a module based on BiFusion [ 25 ]. The overall architecture of the model is illustrated in Figure 1 . On the decoder side, a combination of CNN layers and Visual State Space Duality (VSSD) blocks based on Mamba v2 was employed. The VSSD incorporates Non-Causal SSD (NC-SSD) modules [ 36 ] to enhance feature representation. Furthermore, inspired by BASNet [ 35 ], we integrated side outputs from the decoder for deep supervision, leveraging complementary information across intermediate prediction maps [ 6 ]. These outputs were upsampled and incorporated into the training to provide guidance at multiple levels. This approach accelerates convergence and mitigates the vanishing gradient problems. A more detailed discussion of the network architecture is provided in the following section. 2.1.1. Dual Encoder The dual-encoder architecture employs EfficientNet to extract local features [ 37 ]. EfficientNet was developed using a neural architecture search (NAS) to systematically scale convolutional networks, resulting in a family of highly efficient models that balance network width, depth, and resolution. Each EfficientNet model is constructed from a series of Inverted Residual Blocks, also referred to as MBConv blocks [ 38 ]. The other branch employs Swin Transformer blocks with patch-merging layers, similar to [ 20 , 28 ]. As mentioned in the introduction section, Cao et al. [ 20 ] introduced Swin-Unet, a Transformer-based architecture that incorporates a hierarchical structure and a window-shifting mechanism to enhance feature representation. Swin Transformer blocks perform feature learning through a window-based multi-head self-attention (W-MSA) module, followed by a shifted window multi-head self-attention (SW-MSA) module [ 20 ], as illustrated in Figure 2 . Specifically, the W-MSA captures the local dependencies among pixels within each window, whereas the SW-MSA models the global interactions across windows, thereby enabling cross-contextual attention for feature recalibration. The mathematical formulation of the Swin Transformer blocks is shown in Equations ( 1 )&#8211;( 4 ): (1) Z n &#8242; = WMSA ( L N ( Z n &#8722; 1 ) ) + Z n &#8722; 1 (2) Z n = MLP ( L N ( Z n &#8242; ) ) + Z n &#8242; (3) Z n + 1 &#8242; = SWMSA ( L N ( Z n ) ) + Z n (4) Z n + 1 = MLP ( L N ( Z n + 1 &#8242; ) ) + Z n + 1 &#8242; where Z n &#8242; and Z n represent the outputs of the WMSA/SWMSA module and the MLP module of the n t h block. The self-attention (SA) mechanism is the core component of the Transformer&#8217;s block, as shown in Equation ( 5 ): (5) A t t e n t i o n ( Q , K , V ) = s o f t m a x ( Q K T d + B ) V where Q , K , V &#8712; R P 2 &#215; &#160; d represent the query, key, and value matrices, respectively. P 2 and d donate the number of patches in a window and the dimension of either the query and key, respectively. B is a relative position bias, and it is based on the bias matrix B &#8242; &#8712; R ( 2 P &#8722; 1 ) &#215; ( 2 P + 1 ) . Similar to Swin-Unet, the first step in the Swin Transformer branch of our model is a linear embedding layer that projects the input features into an arbitrary dimension. This is followed by a patch-merging layer that performs downsampling while increasing the feature dimension before the Transformer blocks process the data. Skip connections carrying multi-scale features from the Transformer encoder are later fused with the feature maps from the CNN encoder in the fusion module, as discussed in the subsequent section. For simplicity, the linear embedding layer is omitted from Figure 1 . The two branches are merged using a fusion module as illustrated in the next section. 2.1.2. Fusion Module TransFuse [ 25 ] and CoTrFuse [ 28 ] both leverage dual encoders that are combined through fusion modules. Although the two fusion modules share similar concepts, they differ in their implementations. Both modules incorporate channel and spatial attention mechanisms to enhance feature representation. For example, the BiFusion module [ 25 ] in TransFuse uses an SE block [ 12 ] as channel attention to emphasize global information from the Transformer branch, while a CBAM block [ 15 ] is used as spatial attention to capture fine-grained details from the CNN branch. The STCF fusion module in CoTrFuse adopts a similar attention-based approach. However, unlike BiFusion, where channel attention is applied to the Transformer feature map and spatial attention is applied to the CNN feature map, STCF applies both attention mechanisms to both feature maps before summing them. In our model, we adopted the STCF fusion strategy, as shown in Figure 3 . 2.1.3. Hyper GNN In an ordinary graph, each edge connects only two nodes, representing the pairwise correlations. In contrast, a hypergraph employs hyperedges that can connect more than two nodes, thereby enabling the modeling of non-pairwise correlations. This makes hypergraphs more effective in capturing complex relationships. Han et al. [ 39 ] introduced a Vision Hypergraph Neural Network (ViHGNN). Subsequently, Feng et al. [ 40 ] proposed a Hypergraph Neural Network (HGNN), which constructs a hypergraph from an image using K-Nearest Neighbors (KNN). In the context of medical image segmentation, Peng et al. [ 41 ] integrated an HGNN into a U-Net for MRI segmentation. More recent work has focused on adaptive hypergraph construction. For example, unlike earlier methods that relied on a fixed number of neighbors per node for graph construction, Chai et al. [ 42 ] proposed an adaptive strategy for hyperedge formation known as Adaptive Hypergraph Construction. This method utilizes the K-Nearest Neighbors (KNN) algorithm to generate a matrix used to compute the degree of each node, which in turn guides the construction of hyperedges. Readers are referred to [ 42 ] for further details regarding the Adaptive Hypergraph Construction procedure. Their Adaptive Hypergraph Construction method models the shape attributes more accurately, particularly in medical imaging. Furthermore, Chai et al. extended the concept of convolutional sliding windows to include hypergraphs. In this approach, the sliding-window-based convolution mechanism employs fixed-size kernels to convolve the feature maps, enabling interaction with local neighboring pixels across the entire image through stride operations. The Adaptive Hypergraph Construction is illustrated in Figure 4 , with hypergraph convolution denoted as HGC. 2.1.4. Mamba Decoder and VSSD Block Motivated by the success of Transformers in capturing long-range dependencies, Mamba with State Space Models (SSMs) has been proposed to capture the global context while avoiding the high computational cost of Vision Transformers (ViTs). The Mamba S6 model improves upon the S4 model by introducing a selective mechanism and hardware optimization. To adapt Mamba for computer vision, VMamba [ 36 ] converts non-causal visual images into sequences of ordered patches using a Cross-Scan Module (CSM). The S6 block was further enhanced in Mamba2 [ 43 ] through the introduction of State Space Duality (SSD) [ 44 ]. Recently, Xu et al. proposed a Non-Causal SSD (NC-SSD) that eliminates the need for a causal mask and specialized scanning paths. Building on this, they introduced Visual State Space Duality (VSSD) to replace the VSS vision block [ 45 ]. The VSSD block based on Mamba2 is shown in Figure 5 . The VSSD block exhibits a design similar to that of the Mamba block [ 46 ]; however, it employs the NC-SSD module instead of the standard Mamba SSD. Vision Mamba models, such as Vision Mamba [ 47 ] and VMamba [ 36 ] typically rely on image traversal mechanisms that use different scanning routes to flatten a 2D image into a 1D sequence. In contrast, NC-SSD introduces an enhanced algorithm by leveraging the fact that the matrix A in Mamba 2 [ 43 ] is reduced to a scalar [ 45 ]. This design eliminates the need for a specific scanning route and removes causal mask requirements. For more detailed information on the NC-SSD, readers are referred to [ 45 ]. As shown in Figure 1 , our decoder employs four VSSD blocks, with convolution and ReLU activation layers inserted between them, each of which is repeated twice. The configuration of the VSSD blocks is as follows: channel dimensions [256, 384, 448, 480] and number of heads [2, 4, 8, 16]. At each stage, each block was applied once without repetition. 2.2. Dataset For this study, we utilized a prostate segmentation dataset based on micro-ultrasound (micro-US) images from [ 4 ]. The dataset is publicly available at https://zenodo.org/records/10475293 and was accessed on 2 February 2025. To the best of our knowledge, this is the only publicly available micro-US dataset. Micro-ultrasound is a 29-MHz imaging technology that provides a resolution that is 3&#8211;4 times higher than that of conventional ultrasound. A key advantage of micro-US over other modalities, such as MRI and standard ultrasound, is its real-time visualization capability, eliminating the need for image fusion, which can account for up to 50% of diagnostic errors. The dataset was collected from 75 men who underwent micro-US-guided prostate biopsy at the University of Florida [ 4 ]. Each patient underwent a prostate scan from left to right, capturing approximately 200&#8211;300 micro-US images. The images were converted from B-mode to a DICOM series with embedded pixel spacing information. Ground-truth prostate annotations were performed by two non-expert annotators and one expert annotator for all 75 patients. Additionally, for evaluation purposes, three annotators manually segmented the prostate capsule in 20 test cases. For model training, 2060 micro-US images from 55 patients were used, and 758 images from 20 patients were reserved for testing. The dataset collection study was approved by the Institutional Review Board at the University of Florida. 2.3. Augmentation Ultrasound-specific data augmentation techniques were employed to prevent overfitting. Four different augmentations have been utilized: Depth Attenuation, Gaussian Shadow, Haze Artifact, and Speckle Reduction using the USAugment (version 1.0.1) [ 48 ] ( https://github.com/adamtupper/ultrasound-augmentation , access date (7 May 2025)). Depth Attenuation simulates the gradual loss of ultrasound wave energy as it propagates through the tissue, causing the intensity to decrease with increasing distance from the probe. Haze Artifact models semi-static noise bands that occasionally appear in the ultrasound images. Gaussian shadows replicate the acoustic shadows caused by air or tissue obstructing wave propagation, generating two-dimensional Gaussian-shaped shadows with randomly selected parameters. Speckle noise, arising from interference among ultrasound waves, is mitigated using Speckle Reduction, which applies a bilateral filter with randomly sampled parameters to reduce speckle patterns. Empirically, we found that applying these augmentations with a probability of 0.2 yields the best result. 2.4. Loss Function and Evaluation Following [ 4 ], the AG-BCE loss function was employed. The AG-BCE loss is based on the BCE loss but accounts for the characteristics of prostate segmentation in micro-US images by penalizing prediction errors more heavily in challenging regions, particularly along the borders between the prostate and bladder. Hence, it assigns different weights to each pixel. (6) L AG - BCE = &#8722; 1 N &#8721; i = 1 N w i y i log ( p i ) + ( 1 &#8722; y i ) log ( 1 &#8722; p i ) where N is the total number of pixels, w i &#8712; { 0 , 1 } is the is the weight of assigned to pixel i , y i &#8712; { 0 , 1 } is the ground truth label for pixel i , p i &#8712; [ 0 , 1 ] is the predicted probability for pixel i . The AG-BCE loss function differentiates between hard and easy regions. Hard regions are defined as areas where expert and non-expert annotations disagree, whereas easy regions are defined as those where both annotations coincide. AG-BCE assigns a weight (denoted by Equation ( 6 ) as w of four hard regions and one easy region, following the implementation in MicroSegNet [ 4 ]. For a more detailed explanation of the AG-BCE loss formulation, readers are referred to [ 4 ]. The average Dice Similarity Coefficient (DSC) and average Hausdorff Distance (HD95) were used as evaluation metrics. HD95 measures the Hausdorff distance by considering the 95th percentile of the distances between the boundary points of the two sets. Unlike the standard Hausdorff distance, which reports the maximum distance, HD95 reduces the influence of outliers by ignoring the extreme values [ 6 ]. The Dice Similarity Coefficient (DSC) is defined as (7) DSC ( G , P ) = 2 | G &#8745; P | | G | + | P | , where G denotes the set of ground-truth pixels, and P denotes the set of predicted pixels. The Hausdorff distance between two sets G and P is defined as (8) d H ( G , P ) = max d h ( G , P ) , d h ( P , G ) , where (9) d h ( G , P ) = max g &#8712; G min p &#8712; P &#8741; g &#8722; p &#8741; , and (10) d h ( P , G ) = max p &#8712; P min g &#8712; G &#8741; p &#8722; g &#8741; . Here, G and P denote the ground truth and the predicted outcome, respectively, while &#8741; &#183; &#8741; represents the Euclidean distance. Intuitively, d h ( G , P ) is the maximum distance from any point in G to its nearest neighbor in P , and d h ( P , G ) is defined analogously. In order to account for image resizing, the HD95 value is adjusted by multiplying it with a spacing parameter, which was specified as 0.033586 in the implementation described by [ 4 ]. 2.5. Implementation Details As a preprocessing step, all images were resized to 224 &#215; 224 . Following the setup in [ 4 ], training was conducted using an image patch size of 16 and a batch size of eight. Stochastic Gradient Descent (SGD) optimizer and momentum, with a decaying learning rate, is used as model optimizer. The initial learning rate, momentum, and weight decay were set to 0.01, 0.9, and 0.0001, respectively. An adaptive learning rate strategy was employed, which reduced the learning rate after a certain number of iterations. The number of epochs was set to 150. Both training and testing were implemented and conducted using PyTorch (Version: 2.7.1+cu118) on an NVIDIA RTX A4500 GPU with 20 GB of memory and Python 3.12.7. 3. Results and Discussion Table 1 presents a comparison between our proposed model and the state-of-the-art (SOTA) methods, with Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff Distance (HD95) used as evaluation metrics. The results in Table 1 represent the average performance across several runs. Our reported results are based on our own training, which was conducted for both 150 and 10 epochs to ensure a fair comparison. The only exception is the MicroSegNet scores, that is based on 10 epochs. This score is reported from [ 4 ]. As noted by H. Jiang et al. [ 4 ], MicroSegNet training was limited to 10 epochs to avoid overfitting. In contrast, overfitting in our model was mitigated through data augmentation, as discussed earlier. However, to ensure a fair comparison under the same conditions (with augmentation), Table 2 presents results for our model and an augmented version of MicroSegNet. As shown in Table 1 , our model achieved its best performance when trained for 150 epochs. For comparison, we incorporated several models, some of which are recent, such as RWKV-UNet [ 49 ] and SegU-KAN [ 50 ]. For all evaluated models, we used AG-BCE as the loss function, similar to [ 4 ], to focus more on challenging regions over easily segmented ones during training. We employed pretraining for certain models, such as Swin-UNet [ 20 ], but not all models in Table 1 were pretrained. As shown in Table 1 , our model achieved the highest performance, consistently outperforming the competing approaches. Specifically, it attained an average Dice Similarity Coefficient (DSC) of 94.16% and a Hausdorff Distance (HD95) of 1.93 mm. For example, compared with Swin-UNet, the average DSC increased from 0.932 to 0.9416, corresponding to an improvement of nearly 1%, whereas the HD95 decreased from 2.04 mm to 1.93 mm, representing a reduction of approximately 5%. These experimental results demonstrate that the proposed network architecture outperforms state-of-the-art segmentation models, achieving a significantly improved segmentation accuracy. Moreover, Table 1 also reports the number of FLOPs, model parameters, GPU memory usage, and inference time for each model. Notice, the number of FLOPs and the number of parameters are calculated using the calflops library ( https://github.com/MrYxJ/calculate-flops.pytorch , access date (11 March 2024)). Figure 6 presents the segmentation outcomes of the various approaches, namely, SwinUNet, MicroSegNet and our model on the Micro-ultrasound dataset from three images for qualitative comparison. To evaluate the impact of different optimizers, we conducted experiments on the Micro-Ultrasound dataset. Figure 7 and Figure 8 illustrate the results for four optimizers: SGD [ 57 , 58 ], AdamW [ 59 ], SAM [ 60 , 61 ], and Adan [ 62 ]. For clarity, the training losses and validation scores are presented in separate figures. SGD, which we used for our model, updates the gradient in each iteration using a randomly selected sample rather than computing the exact gradient. The AdamW optimizer is similar to Adam, but while the Adam optimizer adaptively adjusts the learning rate for each parameter based on first and second order moment estimates of the gradient [ 6 ] using the geometry curvature of the objective function [ 62 ], AdamW decouples the weight decay from the gradient update. Sharpness-Aware Minimization (SAM) attempts to find parameter neighborhoods that have uniformly low loss, making it more robust to noisy labels in the training set by perturbing the loss landscape. SAM is particularly beneficial when applied to Vision Transformers (ViTs), as these architectures are more prone to end up in local minima than CNN-based architectures [ 63 , 64 ]. However, SAM requires forward and backward passes twice for each iteration. Adaptive Nesterov Momentum (Adan) introduces a new Nesterov Momentum Estimation (NME) method based on Nesterov acceleration. Its efficiency arises from the elimination of the overhead of computing the gradient at the extrapolation point. Figure 7 illustrates the behavior of each optimizer during the training. Both AdamW and Adan exhibited lower loss curves, indicating better performance than SGD and SAM. This trend is also confirmed by the evaluation curves shown in Figure 8 . Unlike in Figure 7 , the curves in Figure 8 are intertwined; however, careful examination reveals that AdamW and Adan show better accuracy than the others. Figure 9 and Figure 10 depict the loss landscapes of different neural networks as 3D surface plots and contour maps, respectively. These visualizations were generated using the filter-normalization method [ 65 ], with the 3D surface renderings in Figure 9 produced using ParaView (Version 5.12) ( http://paraview.org/ , access date (5 March 2024)) software. The filter-normalization approach evaluates the loss landscape along filter-normalized directions, where the geometry of the landscape provides insights into model trainability and generalizability [ 64 ]. In particular, smoother and more convex landscapes are typically correlated with lower error values, whereas irregular or chaotic landscapes tend to yield higher training errors. 4. Ablation Study Ablation studies were performed to assess the contribution of each component in the proposed method and to support the design decisions based on the performance outcomes of the Micro-Ultrasound dataset. These results reflect the average scores computed over multiple runs. The results for the five different configurations are summarized in Table 3 . Notice that DS denotes deep supervision. Our analysis demonstrates the effectiveness of the proposed enhancements compared with the baseline model. As shown in the table, integrating the Mamba blocks into the decoder improved both the DSC and the 95th percentile HD95. The addition of deep supervision further enhanced the performance of both metrics. Table 3 also lists the number of FLOPs and parameters. It is noteworthy that the majority of components contribute minimally to the increase in FLOPs and parameters; the only exception is the integration of Mamba blocks within the decoder (VSSD), which leads to an increase in both metrics. Also, notice that adding the hyperGraph module slightly improved the DSC score, but it worsened the HD score. Hence, to verify the effectiveness of integrating HyperGraph, Table 4 presents a comparison of the model with and without HyperGraph. As shown, HyperGraph indeed enhances accuracy based on both metrics. Finally, the combination of Mamba, deep supervision, and ultrasound-specific data augmentation achieved the best average results for both HD95 and DSC scores. Table 5 presents a comparison of the model performance based on different Swin block configurations. The Tiny (Swin-T) configuration has channel dimensions of [96, 192, 384, 768] and numbers of heads [3, 6, 12, 24], with the blocks repeated [2, 2, 6, 2]. The Small (Swin-S) configuration uses the same channel dimensions and numbers of heads but repeats the blocks [2, 2, 18, 2] [ 66 ]. Note that we did not test the larger Swin variants (Swin-B and Swin-L) due to their higher GPU memory requirements. 5. Conclusions This work presents an improved model for ultrasound medical image segmentation, with particular emphasis on micro-US. The proposed architecture features a dual-encoder design: a Swin Transformer branch for capturing global context and a CNN-based branch for extracting fine local details. To further enhance feature representation, a hypergraph network is incorporated to model non-pairwise correlations. The decoder leverages the Mamba 2 architecture, built upon VSSD blocks, to improve localization accuracy. Furthermore, ultrasound-specific augmentation techniques, such as depth attenuation, were employed during training to mitigate overfitting and enhance robustness. Experimental results demonstrate that the proposed method achieves superior performance across multiple evaluation metrics, underscoring its potential to advance segmentation accuracy in ultrasound medical imaging. Future research could focus on mitigating the scarcity of Micro-Ultrasound (US) image datasets using semi-supervised or weakly supervised learning. These approaches can effectively leverage large volumes of unlabeled data to reduce annotation costs while preserving the segmentation boundary accuracy. In particular, weakly supervised methods may utilize alternative supervisory cues such as image-level tags, bounding boxes, or scribbles [ 67 ]. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, A.A.-Q., M.K.A.; methodology, A.A.-Q., M.K.A.; investigation, A.A.-Q., M.K.A.; writing&#8212;original draft preparation, A.A.-Q.; writing&#8212;review and editing, A.T., M.K.A.; supervision, M.K.A. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement The Micro-ultrasound (micro-US) images segmentation dataset is publicly available at https://zenodo.org/records/10475293 and was accessed on 2 February 2025. Conflicts of Interest The authors declare no conflicts of interest. References 1. Li J. Xu C. Lee H.J. Ren S. Zi X. Zhang Z. Wang H. Yu Y. Yang C. Gao X. A genomic and epigenomic atlas of prostate cancer in Asian populations Nature 2020 580 93 99 10.1038/s41586-020-2135-x 32238934 2. Imran M. Nguyen B. Pensa J. Falzarano S.M. Sisk A.E. Liang M. DiBianco J.M. Su L.M. Zhou Y. Joseph J.P. Image registration of in vivo micro-ultrasound and ex vivo pseudo-whole mount histopathology images of the prostate: A proof-of-concept study Biomed. Signal Process. Control 2024 96 106657 10.1016/j.bspc.2024.106657 3. Wasih M. Ahmad S. Almekkawy M. A robust cascaded deep neural network for image reconstruction of single plane wave ultrasound RF data Ultrasonics 2023 132 106981 10.1016/j.ultras.2023.106981 36913830 4. Jiang H. Imran M. Muralidharan P. Patel A. Pensa J. Liang M. Benidir T. Grajo J.R. Joseph J.P. Terry R. MicroSegNet: A deep learning approach for prostate segmentation on micro-ultrasound images Comput. Med. Imaging Graph. 2024 112 102326 10.1016/j.compmedimag.2024.102326 38211358 5. Sun Y. Dai D. Zhang Q. Wang Y. Xu S. Lian C. MSCA-Net: Multi-scale contextual attention network for skin lesion segmentation Pattern Recognit. 2023 139 109524 10.1016/j.patcog.2023.109524 6. Al Qurri A. Almekkawy M. Improved UNet with Attention for Medical Image Segmentation Sensors 2023 23 8589 10.3390/s23208589 37896682 PMC10611347 7. Wang C. Xu R. Xu S. Meng W. Zhang X. Automatic polyp segmentation via image-level and surrounding-level context fusion deep neural network Eng. Appl. Artif. Intell. 2023 123 106168 10.1016/j.engappai.2023.106168 8. Huo X. Sun G. Tian S. Wang Y. Yu L. Long J. Zhang W. Li A. HiFuse: Hierarchical multi-scale feature fusion network for medical image classification Biomed. Signal Process. Control 2024 87 105534 10.1016/j.bspc.2023.105534 9. Gao Q. Almekkawy M. ASU-Net++: A nested U-Net with adaptive feature extractions for liver tumor segmentation Comput. Biol. Med. 2021 136 104688 10.1016/j.compbiomed.2021.104688 34523421 10. Ahmed A.Q. Almekkawy M. Improved UNet++ Based on Kolmogorov-Arnold Convolutions Proceedings of the 2025 IEEE International Conference on Image Processing (ICIP) Anchorage, AK, USA 14&#8211;18 September 2025 905 910 11. Kang J. Al-Qurri A. Almekkawy M. Fast and Resource-Efficient Ultrasound Segmentation Using FPGAs Proceedings of the 2025 IEEE International Ultrasonics Symposium (IUS) Utrecht, The Netherlands 15&#8211;18 September 2025 1 5 12. Hu J. Shen L. Sun G. Squeeze-and-excitation networks Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;23 June 2018 7132 7141 13. Wang X. Girshick R. Gupta A. He K. Non-local neural networks Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;23 June 2018 7794 7803 14. Fu J. Liu J. Tian H. Li Y. Bao Y. Fang Z. Lu H. Dual attention network for scene segmentation Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Long Beach, CA, USA 15&#8211;20 June 2019 3146 3154 15. Woo S. Park J. Lee J.Y. Kweon I.S. Cbam: Convolutional block attention module Computer Vision&#8212;ECCV 2018, Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8&#8211;14 September 2018 Springer Berlin/Heidelberg, Germany 2018 3 19 16. Vaswani A. Shazeer N. Parmar N. Uszkoreit J. Jones L. Gomez A.N. Kaiser &#321;. Polosukhin I. Attention is all you need Advances in Neural Information Processing Systems 30 (NIPS 2017), Proceedings of the Annual Conference on Neural Information Processing Systems 2017, Long Beach, CA, USA, 4&#8211;9 December 2017 Guyon I. Von Luxburg U. Bengio S. Wallach H. Fergus R. Vishwanathan S. Garnett R. Curran Associates Inc. Red Hook, NY, USA 2017 17. Wang H. Xie S. Lin L. Iwamoto Y. Han X.H. Chen Y.W. Tong R. Mixed transformer u-net for medical image segmentation Proceedings of the ICASSP 2022&#8211;2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Singapore 22&#8211;27 May 2022 2390 2394 18. Li J. Chen J. Tang Y. Wang C. Landman B.A. Zhou S.K. Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives Med. Image Anal. 2023 85 102762 10.1016/j.media.2023.102762 36738650 PMC10010286 19. Dosovitskiy A. Beyer L. Kolesnikov A. Weissenborn D. Zhai X. Unterthiner T. Dehghani M. Minderer M. Heigold G. Gelly S. An image is worth 16x16 words: Transformers for image recognition at scale arXiv 2020 2010.11929 20. Cao H. Wang Y. Chen J. Jiang D. Zhang X. Tian Q. Wang M. Swin-unet: Unet-like pure transformer for medical image segmentation Computer Vision&#8212;ECCV 2022 Workshops, Proceedings of the European Conference on Computer Vision, Tel Aviv, Israel, 23&#8211;27 October, 2022 Springer Berlin/Heidelberg, Germany 2022 205 218 21. Zuo S. Xiao Y. Chang X. Wang X. Vision transformers for dense prediction: A survey Knowl.-Based Syst. 2022 253 109552 10.1016/j.knosys.2022.109552 22. Gao B.B. Huang Z. CSTrans: Correlation-guided Self-Activation Transformer for Counting Everything Pattern Recognit. 2024 153 110556 10.1016/j.patcog.2024.110556 23. He D. Zhang Y. Huang H. Si Y. Wang Z. Li Y. Dual-branch hybrid network for lesion segmentation in gastric cancer images Sci. Rep. 2023 13 6377 10.1038/s41598-023-33462-y 37076573 PMC10115814 24. Azad R. Kazerouni A. Heidari M. Aghdam E.K. Molaei A. Jia Y. Jose A. Roy R. Merhof D. Advances in medical image analysis with vision transformers: A comprehensive review Med. Image Anal. 2023 91 103000 10.1016/j.media.2023.103000 37883822 25. Zhang Y. Liu H. Hu Q. Transfuse: Fusing transformers and cnns for medical image segmentation Medical Image Computing and Computer Assisted Intervention&#8212;MICCAI 2021, Proceedings of the Medical Image Computing and Computer Assisted Intervention&#8211;MICCAI 2021: 24th International Conference, Strasbourg, France, 27 September&#8211;1 October 2021 Springer Berlin/Heidelberg, Germany 2021 14 24 26. Chen J. Lu Y. Yu Q. Luo X. Adeli E. Wang Y. Lu L. Yuille A.L. Zhou Y. Transunet: Transformers make strong encoders for medical image segmentation arXiv 2021 10.48550/arXiv.2102.04306 2102.04306 27. Ahmed A.Q. Alqarni A. Almekkawy M. Trifuse: Triplet Encoders Network for Medical Image Segmentation Proceedings of the 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI) Houston, TX, USA 14&#8211;17 April 2025 1 5 28. Chen Y. Wang T. Tang H. Zhao L. Zhang X. Tan T. Gao Q. Du M. Tong T. CoTrFuse: A novel framework by fusing CNN and transformer for medical image segmentation Phys. Med. Biol. 2023 68 175027 10.1088/1361-6560/acede8 37605997 29. Azad R. Al-Antary M.T. Heidari M. Merhof D. Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model IEEE Access 2022 10 108205 108215 10.1109/ACCESS.2022.3211501 30. Wang B. Wang F. Dong P. Li C. Multiscale transunet++: Dense hybrid U-Net with transformer for medical image segmentation Signal, Image Video Process. 2022 16 1607 1614 10.1007/s11760-021-02115-w 31. Yin Y. Xu W. Chen L. Wu H. CoT-UNet++: A medical image segmentation method based on contextual transformer and dense connection Math. Biosci. Eng. 2023 20 8320 8336 10.3934/mbe.2023364 37161200 32. Al-Qurri A. Almekkawy M. Ultrasound Image Segmentation using a Model of Transformer and DFT Proceedings of the 2024 IEEE UFFC Latin America Ultrasonics Symposium (LAUS) Montevideo, Uruguay 8&#8211;10 May 2024 1 4 33. Szegedy C. Vanhoucke V. Ioffe S. Shlens J. Wojna Z. Rethinking the inception architecture for computer vision Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Las Vegas, NV, USA 27&#8211;30 June 2016 2818 2826 34. Zeng Z. Hu Q. Xie Z. Zhou J. Xu Y. Small but Mighty: Enhancing 3D Point Clouds Semantic Segmentation with U-Next Framework arXiv 2023 2304.00749 10.1016/j.jag.2024.104309 35. Qin X. Zhang Z. Huang C. Gao C. Dehghan M. Jagersand M. Basnet: Boundary-aware salient object detection Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Long Beach, CA, USA 15&#8211;20 June 2019 7479 7489 36. Liu Y. Tian Y. Zhao Y. Yu H. Xie L. Wang Y. Ye Q. Liu Y. Vmamba: Visual state space model arXiv 2024 2401.10166 37. Tan M. Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks Proceedings of the International Conference on Machine Learning Long Beach, CA, USA 9&#8211;15 June 2019 6105 6114 38. Sandler M. Howard A. Zhu M. Zhmoginov A. Chen L.C. Mobilenetv2: Inverted residuals and linear bottlenecks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;23 June 2018 4510 4520 39. Han Y. Wang P. Kundu S. Ding Y. Wang Z. Vision hgnn: An image is more than a graph of nodes Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV) Paris, France 1&#8211;6 October 2023 19878 19888 40. Feng Y. You H. Zhang Z. Ji R. Gao Y. Hypergraph neural networks Proceedings of the AAAI Conference on Artificial Intelligence Honolulu, HI, USA 27 January&#8211;1 February 2019 3558 3565 41. Peng J. Yang J. Xia C. Li X. Guo Y. Fu Y. Chen X. Cui Z. Make U-Net Greater: An Easy-to-Embed Approach to Improve Segmentation Performance Using Hypergraph Comput. Syst. Sci. Eng. 2022 42 319 333 10.32604/csse.2022.022314 42. Chai S. Jain R.K. Mo S. Liu J. Yang Y. Li Y. Tateyama T. Lin L. Chen Y.W. A Novel Adaptive Hypergraph Neural Network for Enhancing Medical Image Segmentation Medical Image Computing and Computer Assisted Intervention&#8212;MICCAI 2024, Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention, Marrakesh, Morocco, 6&#8211;10 October 2024 Springer Berlin/Heidelberg, Germany 2024 23 33 43. Dao T. Gu A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality arXiv 2024 10.48550/arXiv.2405.21060 2405.21060 44. Al-Qurri A. Almekkawy M. Enhancing Medical Image Segmentation with Mamba and UNet++ Proceedings of the 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI) Houston, TX, USA 14&#8211;17 April 2025 1 5 45. Shi Y. Dong M. Li M. Xu C. VSSD: Vision Mamba with Non-Casual State Space Duality arXiv 2024 10.48550/arXiv.2407.18559 2407.18559 46. Gu A. Dao T. Mamba: Linear-time sequence modeling with selective state spaces Proceedings of the First Conference on Language Modeling Philadelphia, PA, USA 7&#8211;9 October 2024 47. Zhu L. Liao B. Zhang Q. Wang X. Liu W. Wang X. Vision mamba: Efficient visual representation learning with bidirectional state space model arXiv 2024 10.48550/arXiv.2401.09417 2401.09417 48. Tupper A. Gagn&#233; C. Revisiting Data Augmentation for Ultrasound Images arXiv 2025 10.48550/arXiv.2501.13193 2501.13193 49. Jiang J. Zhang J. Liu W. Gao M. Hu X. Yan X. Huang F. Liu Y. Rwkv-unet: Improving unet with long-range cooperation for effective medical image segmentation arXiv 2025 2501.08458 50. Li C. Liu X. Li W. Wang C. Liu H. Liu Y. Chen Z. Yuan Y. U-kan makes strong backbone for medical image segmentation and generation Proceedings of the AAAI Conference on Artificial Intelligence Philadelphia, PA, USA 25 February&#8211;4 March 2025 4652 4660 51. Zhou Z. Rahman Siddiquee M.M. Tajbakhsh N. Liang J. Unet++: A nested u-net architecture for medical image segmentation Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Proceedings of the 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Granada, Spain, 20 September 2018 Springer Berlin/Heidelberg, Germany 2018 3 11 10.1007/978-3-030-00889-5_1 PMC7329239 32613207 52. Heidari M. Kazerouni A. Soltany M. Azad R. Aghdam E.K. Cohen-Adad J. Merhof D. Hiformer: Hierarchical multi-scale representations using transformers for medical image segmentation Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Waikoloa, HI, USA 3&#8211;7 January 2023 6202 6212 53. Azad R. Niggemeier L. H&#252;ttemann M. Kazerouni A. Aghdam E.K. Velichko Y. Bagci U. Merhof D. Beyond self-attention: Deformable large kernel attention for medical image segmentation Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Waikoloa, HI, USA 3&#8211;8 January 2024 1287 1297 54. Wu R. Liu Y. Liang P. Chang Q. H-vmunet: High-order vision mamba unet for medical image segmentation Neurocomputing 2025 624 129447 10.1016/j.neucom.2025.129447 55. Zhang M. Yu Y. Gu L. Lin T. Tao X. VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation arXiv 2024 2403.09157 56. Ruan J. Li J. Xiang S. VM-UNet: Vision Mamba UNet for Medical Image Segmentation arXiv 2024 2402.02491 10.1145/3767748 57. Sun S. Cao Z. Zhu H. Zhao J. A survey of optimization methods from a machine learning perspective IEEE Trans. Cybern. 2019 50 3668 3681 10.1109/TCYB.2019.2950779 31751262 58. Li Q. Xiong D. Shang M. Adjusted stochastic gradient descent for latent factor analysis Inf. Sci. 2022 588 196 213 10.1016/j.ins.2021.12.065 59. Loshchilov I. Hutter F. Decoupled weight decay regularization arXiv 2017 1711.05101 60. Roux N. Schmidt M. Bach F. A stochastic gradient method with an exponential convergence _rate for finite training sets Advances in Neural Information Processing Systems 25 (NIPS 2012), Proceedings of the Annual Conference on Neural Information Processing Systems 2012, Lake Tahoe, NV, USA, 3&#8211;6 December 2012 Pereira F. Burges C.J. Bottou L. Weinberger K.Q. Curran Associates Inc. Red Hook, NY, USA 2012 61. Foret P. Kleiner A. Mobahi H. Neyshabur B. Sharpness-aware minimization for efficiently improving generalization arXiv 2020 2010.01412 62. Xie X. Zhou P. Li H. Lin Z. Yan S. Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models IEEE Trans. Pattern Anal. Mach. Intell. 2024 46 9508 9520 10.1109/TPAMI.2024.3423382 38963744 63. Chen X. Hsieh C.J. Gong B. When vision transformers outperform resnets without pre-training or strong data augmentations arXiv 2021 2106.01548 64. Qurri A.A. Almekkawy M. Hybrid MultiResUNet with transformers for medical image segmentation Biomed. Signal Process. Control 2025 110 108056 10.1016/j.bspc.2025.108056 65. Li H. Xu Z. Taylor G. Studer C. Goldstein T. Visualizing the loss landscape of neural nets Advances in Neural Information Processing Systems 31 (NeurIPS 2018), Proceedings of Annual Conference on Neural Information Processing Systems 2018, Montr&#233;al, QC, Canada, 3&#8211;8 December 2018 Bengio S. Wallach H. Larochelle H. Grauman K. Cesa-Bianchi N. Garnett R. Curran Associates Inc. Red Hook, NY, USA 2018 66. Liu Z. Lin Y. Cao Y. Hu H. Wei Y. Zhang Z. Lin S. Guo B. Swin transformer: Hierarchical vision transformer using shifted windows Proceedings of the IEEE/CVF International Conference on Computer Vision Montreal, QC, Canada 11&#8211;17 October 2021 10012 10022 67. Chen J. Duan H. Zhang X. Gao B. Grau V. Han J. From gaze to insight: Bridging human visual attention and vision language model explanation for weakly-supervised medical image segmentation IEEE Trans. Med. Imaging 2025 10.1109/TMI.2025.3616598 41052168 Figure 1 Overall network architecture with dual encoders, Hyper GNN, and Mamba decoder. Figure 2 Diagram of the Swin-Transformer Block based on Swin-Unet. The Swin transformer employs a window-based multi-head self-attention (W-MSA) module and a shifted window-based multi-head self-attention (SW-MSA) module. Figure 3 Architecture of the proposed fusion module block, adapted from the STCF fusion module [ 28 ]. The SE block and CBAM are used to perform channel and spatial attention, respectively. Figure 4 The Adaptive Hypergraph Construction module, implemented following the method proposed by Chai et al. [ 42 ]. This module constructs hyper-edges adaptively based on node degree, utilizing the K-Nearest Neighbors (KNN) algorithm to form the adjacency matrix for hypergraph generation. Figure 5 The VSSD block based on [ 45 ] that employ Mamba2. Figure 6 Qualitative comparison of CT slices from the Micro-US dataset, showing prostate boundaries donated as a green circle. From left to right: ( a ) Ground Truth, ( b ) SwinUNet, ( c ) MicroSegNet, and ( d ) our model&#8217;s prediction. Figure 7 Dice Loss for different optimizers during training. Figure 8 Dice Score During Evaluation. Figure 9 Loss surface visualization of different neural networks using the filter-normalization method. Top-left: UNet++; top-right: Swin-UNet; bottom-left: MicroSegNet; bottom-right: our model. Figure 10 Contour maps of the 2D loss landscape for different neural networks using the filter-normalization method. Top left: U-Net++; top right: Swin-UNet; bottom left: MicroSegNet; bottom right: proposed model. sensors-25-06815-t001_Table 1 Table 1 The segmentation accuracy on the micro-US dataset was evaluated using the average Dice score and 95% Hausdorff distance (HD95). The best results are indicated in bold. The Dice scores marked with * are reported in [ 4 ], and their values are provided with a precision of three decimal places. All results are based on training for 150 epochs and 10 epochs to ensure a fair comparison. FLOPs (G) denote the number of floating-point operations (in billions). #Params (M) indicates the number of parameters (in millions). GPU Mem (G) denotes GPU memory usage (in gigabytes). Inference (S) represents the inference time (in seconds). Model 150 Epochs 10 Epochs Inference (S) DSC&#8593; HD&#8595; DSC&#8593; HD&#8595; FLOPs (G) #Params (M) GPU Mem (G) Unet [ 26 ] 0.8897 5.94 0.8420 7.62 21.37 7.85 5.93 120 UNet++ [ 51 ] 0.9081 3.81 1 0.8894 4.82 53.1 9.16 11.39 134 TransUNet [ 26 ] 0.9293 2.39 0.9303 2.20 58.49 105.28 10.57 133 Swin-UNet [ 20 ] 0.9327 2.04 0.9218 2.49 17.4 41.38 19.46 165 TransNorm [ 29 ] 0.9214 2.63 0.9232 2.45 62.18 117.63 16.54 130 HiFormer-B [ 52 , 53 ] 0.8967 4.40 0.8784 5.34 8.045 25.51 11.40 141 CoTrFuse [ 28 ] 0.9266 2.74 0.9065 3.42 33.07 56.19 15.30 166 RWKV-UNet [ 49 ] 0.8866 4.62 0.7958 5.85 57.44 120.24 10.96 122 H-vmunet [ 54 ] 0.8817 3.34 0.8746 4.39 1140.6 8.97 3.79 167 Seg. U-KAN [ 50 ] 0.8918 5.09 0.8409 6.81 14.02 6.35 15.01 130 VM-UNet [ 55 , 56 ] 0.9042 2.89 0.8630 5.30 7.56 34.62 18.97 126 MicroSegNet [ 4 ] 0.9341 2.23 0.939 * 2.02 58.49 105.28 10.96 131 Ours 0.9416 1.93 0.9380 2.02 70.72 93.13 11.93 202 sensors-25-06815-t002_Table 2 Table 2 Quantitative comparison between our model and MicroSegNet when data augmentation is applied to MicroSegNet. Methods DSC&#8593; HD95&#8595; MicroSegNet with Augmentation (150 Epochs) 0.9321 2.29 MicroSegNet with Augmentation (10 Epochs) 0.9356 2.04 Ours 0.9416 1.93 sensors-25-06815-t003_Table 3 Table 3 Contribution of each component to the overall performance on the Micro-Ultrasound dataset. FLOPs (G) denote the number of floating-point operations (in billions). #Params (M) indicates the number of parameters (in millions). Mamba (VSSD) DS HyperGraph Augment. FLOPs (G) #Params (M) DSC&#8593; HD&#8595; Baseline 42.4 66.79 92.84 2.28 &#10003; 68.5 93.05 93.03 2.19 &#10003; &#10003; 68.5 93.05 93.73 1.95 &#10003; &#10003; &#10003; 70.74 93.13 93.75 1.99 Proposed Model &#10003; &#10003; &#10003; &#10003; 70.72 93.13 94.16 1.93 sensors-25-06815-t004_Table 4 Table 4 Comparison of the model with and without HyperGraph on the Micro-Ultrasound Dataset. Methods DSC&#8593; HD95&#8595; Without HyperGraph 93.96 1.95 With HyperGraph (Overall) 94.16 1.93 sensors-25-06815-t005_Table 5 Table 5 Ablation study on the model scale for the Swin Blocks. Methods DSC&#8593; HD95&#8595; Swin-T 94.16 1.93 Swin-S 93.96 1.95"
}