{
  "pmcid": "PMC12656184",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:31.421290",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656184",
    "pmid": "41305170",
    "doi": "10.3390/s25226963",
    "title": "CAM-UNet: A Novel Water Environment Perception Method Integrating CoAtNet Structure",
    "year": "2025",
    "month": "11",
    "day": "14",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "14"
    },
    "authors": [
      "Gao Xingyi",
      "Liu Jie",
      "Liu Yanyi",
      "Wu Yin"
    ],
    "abstract": "Accurate segmentation of navigable waters and obstacles is critical for unmanned surface vessel navigation yet remains challenging in real aquatic environments characterized by complex water textures and blurred boundaries. Current models often struggle to simultaneously capture long-range contextual dependencies and fine spatial details, frequently leading to fragmented segmentation results. In order to resolve these issues, we present a novel segmentation model based on the CoAtNet architecture. Our framework employs an enhanced convolutional attention encoder, where a Fused Mobile Inverted Bottleneck Convolution (Fused-MBConv) module refines boundary features while a Convolutional Block Attention Module (CBAM) enhances feature awareness. The model incorporates a Bi-level Former (BiFormer) to enable collaborative modeling of global and local features, complemented by a Multi-scale Attention Aggregation (MSAA) module that effectively captures contextual information across different scales. The decoder, based on U-Net, restores spatial resolution gradually through skip connections and upsampling. In our experiments, the model achieves 95.15% mIoU on a self-collected dataset and 98.48% on the public MaSTr1325 dataset, outperforming DeepLabV3+, SeaFormer, and WaSRNet. These results show the modelâ€™s ability to effectively interpret complex aquatic environments for autonomous navigation.",
    "keywords": [
      "deep learning",
      "CoAtNet",
      "semantic segmentation",
      "image processing",
      "unmanned surface vessel"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656184</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656184.1</article-id><article-id pub-id-type=\"pmcaid\">12656184</article-id><article-id pub-id-type=\"pmcaiid\">12656184</article-id><article-id pub-id-type=\"pmid\">41305170</article-id><article-id pub-id-type=\"doi\">10.3390/s25226963</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-06963</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CAM-UNet: A Novel Water Environment Perception Method Integrating CoAtNet Structure</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0009-0009-3468-4125</contrib-id><name name-style=\"western\"><surname>Gao</surname><given-names initials=\"X\">Xingyi</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><xref rid=\"af1-sensors-25-06963\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"J\">Jie</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><xref rid=\"af2-sensors-25-06963\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"Y\">Yanyi</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><xref rid=\"af1-sensors-25-06963\" ref-type=\"aff\">1</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0001-8473-8990</contrib-id><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"Y\">Yin</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><xref rid=\"af1-sensors-25-06963\" ref-type=\"aff\">1</xref><xref rid=\"c1-sensors-25-06963\" ref-type=\"corresp\">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"H\">Honggang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-06963\"><label>1</label>College of Information Science and Technology &amp; Artificial Intelligence, Nanjing Forestry University, Nanjing 210037, China; <email>nfugxy@njfu.edu.cn</email> (X.G.); <email>yyliu@njfu.edu.cn</email> (Y.L.)</aff><aff id=\"af2-sensors-25-06963\"><label>2</label>Qingdao Guoshi Intelligent Equipment Technology Co., Ltd., Qingdao 266000, China; <email>jliu05328@gmail.com</email></aff><author-notes><corresp id=\"c1-sensors-25-06963\"><label>*</label>Correspondence: <email>wuyin@njfu.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>14</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>6963</elocation-id><history><date date-type=\"received\"><day>24</day><month>9</month><year>2025</year></date><date date-type=\"rev-recd\"><day>07</day><month>11</month><year>2025</year></date><date date-type=\"accepted\"><day>12</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>14</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-06963.pdf\"/><abstract><p>Accurate segmentation of navigable waters and obstacles is critical for unmanned surface vessel navigation yet remains challenging in real aquatic environments characterized by complex water textures and blurred boundaries. Current models often struggle to simultaneously capture long-range contextual dependencies and fine spatial details, frequently leading to fragmented segmentation results. In order to resolve these issues, we present a novel segmentation model based on the CoAtNet architecture. Our framework employs an enhanced convolutional attention encoder, where a Fused Mobile Inverted Bottleneck Convolution (Fused-MBConv) module refines boundary features while a Convolutional Block Attention Module (CBAM) enhances feature awareness. The model incorporates a Bi-level Former (BiFormer) to enable collaborative modeling of global and local features, complemented by a Multi-scale Attention Aggregation (MSAA) module that effectively captures contextual information across different scales. The decoder, based on U-Net, restores spatial resolution gradually through skip connections and upsampling. In our experiments, the model achieves 95.15% mIoU on a self-collected dataset and 98.48% on the public MaSTr1325 dataset, outperforming DeepLabV3+, SeaFormer, and WaSRNet. These results show the model&#8217;s ability to effectively interpret complex aquatic environments for autonomous navigation.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>CoAtNet</kwd><kwd>semantic segmentation</kwd><kwd>image processing</kwd><kwd>unmanned surface vessel</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>32171788</award-id></award-group><award-group><funding-source>Jiangsu Provincial Government Scholarship for Overseas Studies</funding-source><award-id>JS-2018-043</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China grant number 32171788 and the the Jiangsu Provincial Government Scholarship for Overseas Studies grant number JS-2018-043.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-06963\"><title>1. Introduction</title><p>In recent years, with the continuous advancement of artificial intelligence technology, unmanned systems such as drones and self-driving cars have received widespread attention. Unmanned surface vehicles (USVs) are an important branch of this field and have shown broad application prospects in the fields of ocean surveys, environmental monitoring, water rescue and resource development [<xref rid=\"B1-sensors-25-06963\" ref-type=\"bibr\">1</xref>]. In order to achieve autonomous navigation and mission planning, USVs need to have efficient environmental perception capabilities. Among them, accurately segmenting the traversable area and obstacles on the water surface is a key task in its visual perception system, which affects the safety and execution efficiency of the navigation path [<xref rid=\"B2-sensors-25-06963\" ref-type=\"bibr\">2</xref>].</p><p>The environmental perception of USVs is the basis for their autonomous navigation and mission execution. They usually collect visible light images by carrying visual sensors and perform semantic segmentation of the water surface, land and obstacles [<xref rid=\"B3-sensors-25-06963\" ref-type=\"bibr\">3</xref>]. Common sensors include monocular or binocular RGB cameras, panoramic cameras, infrared imaging equipment, LiDAR, radar and multispectral/hyperspectral sensors. Among them, visual cameras have the advantages of low cost and rich information and are the most widely used perception devices; infrared sensors can assist in identifying targets at night or in low-light environments; lidar can provide accurate three-dimensional spatial information, which is suitable for structural mapping and obstacle detection; and radar has strong penetration and can maintain stable perception under adverse weather conditions [<xref rid=\"B4-sensors-25-06963\" ref-type=\"bibr\">4</xref>]. Current image segmentation methods mainly include traditional image processing and deep learning.</p><p>Traditional image segmentation methods mainly include edge detection, threshold segmentation, active contour model and region growing, etc. [<xref rid=\"B5-sensors-25-06963\" ref-type=\"bibr\">5</xref>]. These methods have low computing power and data requirements and can run efficiently on low-power edge devices, but they are difficult to handle complex scenes. Edge detection methods (e.g., Canny) often produce fragmented boundaries, while thresholding techniques require manual parameter tuning; active contour models are highly sensitive to initialization and image intensity variations [<xref rid=\"B6-sensors-25-06963\" ref-type=\"bibr\">6</xref>]; region growing can improve local accuracy but suffers from high computational cost and sensitivity to seed selection [<xref rid=\"B7-sensors-25-06963\" ref-type=\"bibr\">7</xref>]. To address these issues, Peng et al. [<xref rid=\"B8-sensors-25-06963\" ref-type=\"bibr\">8</xref>] proposed combining the HSV color model with edge detection for shoreline extraction, though the method lacked robustness for irregular boundaries. Zheng et al. [<xref rid=\"B9-sensors-25-06963\" ref-type=\"bibr\">9</xref>] integrated Otsu thresholding with region growing to enhance accuracy, but at the cost of increased computation.</p><p>Deep learning-based methods have been extensively applied to water surface segmentation [<xref rid=\"B10-sensors-25-06963\" ref-type=\"bibr\">10</xref>,<xref rid=\"B11-sensors-25-06963\" ref-type=\"bibr\">11</xref>,<xref rid=\"B12-sensors-25-06963\" ref-type=\"bibr\">12</xref>,<xref rid=\"B13-sensors-25-06963\" ref-type=\"bibr\">13</xref>], with Convolutional Neural Networks (CNNs) demonstrating strong capability in extracting local features. The adaptive scale network SANet proposed by Cui et al. [<xref rid=\"B14-sensors-25-06963\" ref-type=\"bibr\">14</xref>] performed well in the segmentation task of the coastal area of Lianyungang, Jiangsu, but it has not been verified on data from other regions, which limits its applicability in a wider range of scenarios. Guo Yangang et al. [<xref rid=\"B15-sensors-25-06963\" ref-type=\"bibr\">15</xref>] improved PSPNet by introducing transfer learning and attention mechanism to reduce the false detection rate, but the recognition of details in the fuzzy boundary area is still limited; Xiong Rui et al. [<xref rid=\"B16-sensors-25-06963\" ref-type=\"bibr\">16</xref>] proposed DeepLabV3-CSPNet based on DeepLabV3, which improved the accuracy, but the stability is still insufficient under extreme lighting conditions. Zhang Liya et al. [<xref rid=\"B17-sensors-25-06963\" ref-type=\"bibr\">17</xref>] proposed BEMSNet, which effectively improved the boundary recognition accuracy in the semantic segmentation of unmanned surface vehicle navigation images through the innovative design of boundary abstraction module and boundary enhancement module, However, it still has limitations, such as poor adaptability to extreme weather scenarios. On the other hand, the Transformer architecture has demonstrated superior semantic understanding performance due to its excellent global modeling capabilities. Representative models such as Swin Transformer [<xref rid=\"B18-sensors-25-06963\" ref-type=\"bibr\">18</xref>] and SegFormer [<xref rid=\"B19-sensors-25-06963\" ref-type=\"bibr\">19</xref>] have achieved leading results in multiple segmentation tasks. However, the Transformer model has large parameter scale, high computational resource consumption, complex deployment, and is prone to overfitting in small sample tasks. Its application in USV visual perception still faces challenges.</p><p>Based on the above problems, this paper proposes an improved semantic segmentation network CAM-UNet based on CoAtNet. The main contributions are as follows:</p><list list-type=\"simple\"><list-item><label>(1)</label><p>We proposed CAM-UNet, an improved encoder&#8211;decoder network that combines convolutional structures and attention mechanisms to capture both local textures and global semantics. The model was specifically designed to handle water surface segmentation tasks, especially those involving complex boundaries or small targets.</p></list-item><list-item><label>(2)</label><p>We implemented a multi-stage feature extraction framework based on the CoAtNet backbone, incorporating various modules to improve both local details and global context understanding. To handle challenging conditions such as surface ripples and strong reflections, we introduced a module between the encoder and decoder to improve multi-scale feature representation.</p></list-item><list-item><label>(3)</label><p>We created and manually annotated a semantic segmentation dataset containing 4950 images covering seven types of water surfaces and six obstacle categories. The dataset included more than 10 representative scenarios and was used for both model training and performance evaluation.</p></list-item></list></sec><sec id=\"sec2-sensors-25-06963\"><title>2. Materials and Methods</title><sec id=\"sec2dot1-sensors-25-06963\"><title>2.1. Experimental Details</title><p>All experiments were conducted on a workstation equipped with an NVIDIA GeForce RTX 4070 GPU (12 GB VRAM) from ASUS (Taipei, China), an Intel Core i7-13700KF processor from Intel Corporation (Santa Clara, CA, USA), and 64 GB of memory. The model was implemented using the PyTorch 1.13.0 framework with Python 3.8 and accelerated by CUDA 11.6 and cuDNN 8.4. During training, the Adam optimizer was employed with an initial learning rate of 1 &#215; 10<sup>&#8722;4</sup> and a batch size of 8. The learning rate was gradually decayed using a cosine annealing strategy, and the model was trained for a total of 200 epochs. The experimental equipment configuration is shown in <xref rid=\"sensors-25-06963-t001\" ref-type=\"table\">Table 1</xref>.</p><p>The loss functions used in this experiment are Cross Entropy Loss and Dice Loss. Since the foreground regions&#8212;especially obstacle classes&#8212;occupy relatively small areas in the dataset, while the background covers a much larger proportion, using a conventional Cross Entropy Loss (CE Loss) alone may lead to the positive samples being overwhelmed by the negative ones. Dice Loss is more sensitive to positive samples in the early stages of training and thus helps to better capture the foreground regions. However, it may suffer from loss saturation; therefore, it is combined with Cross Entropy Loss to achieve a balanced optimization.</p></sec><sec id=\"sec2dot2-sensors-25-06963\"><title>2.2. Dataset Construction and Processing</title><p>The task of segmenting feasible water domains and obstacles involves high scene complexity and substantial visual interference, placing stringent requirements on the quality, diversity, and annotation precision of the training data. To ensure reliable performance in various environments, the dataset should cover multiple types of water bodies and scene settings. Additionally, a wide range of lighting conditions, weather patterns, and time of day should be taken into account to make the dataset content rich and diverse.</p><p>At the object level, the dataset should include various common surface structures as well as dynamic obstacles. These obstacles should vary in size, direction, material properties, and occlusion level. This diversity ensures that the dataset closely reflects the real-world challenges faced in navigable water environments. Additionally, the dataset should also contain a large number of pure water scenes, which will help the model better learn the navigable boundaries, surface textures, and shape changes, ultimately facilitating clearer classification during the segmentation process. Sampling.</p><p>Based on these requirements, this paper constructed a water surface image dataset suitable for this study through self-collection and online search. The final dataset contains a total of 4950 images, which are divided into a training set of 3465 images, a validation set of 990 images, and a test set of 495 images according to a 7:2:1 ratio.</p><p>Image acquisition was conducted in several representative aquatic environments using high-definition RGB cameras mounted on an autonomous surface platform and fixed onshore equipment. The captured images cover various times of day and incorporate perspective variations and reflection disturbances in various natural environments. The captured images are shown in <xref rid=\"sensors-25-06963-f001\" ref-type=\"fig\">Figure 1</xref>, encompassing daytime, nighttime, sunny, rainy, and foggy conditions. Labels include feasible regions and obstacles.</p><p>The original image was manually annotated frame by frame using Labelme 5.8.1 software. The polygonal outline of the target area was accurately drawn to generate the corresponding segmentation label. The annotation process is as follows. The example of annotation is shown in <xref rid=\"sensors-25-06963-f002\" ref-type=\"fig\">Figure 2</xref>.</p><p>To enhance the model&#8217;s generalization capabilities and adaptability to diverse environmental changes, this study performed data augmentation on the training set. Obstacle types and backgrounds in water scenes are complex and varied, and images may be subject to lighting variations, angle offsets, and object occlusions, all of which can affect model training. To this end, we employed common image augmentation techniques, including rotation, flipping, scaling, cropping, brightness adjustment, and salt-and-pepper noise. <xref rid=\"sensors-25-06963-f003\" ref-type=\"fig\">Figure 3</xref> shows some of these data augmentation methods.</p></sec><sec id=\"sec2dot3-sensors-25-06963\"><title>2.3. Methods</title><sec id=\"sec2dot3dot1-sensors-25-06963\"><title>2.3.1. CAM-UNet Overall Framework</title><p>CAM-UNet is an improved encoder&#8211;decoder network architecture for water surface feasible domain and obstacle segmentation tasks. The encoder is structurally optimized based on CoAtNet and employs a multi-scale attention mechanism to improve segmentation accuracy. The decoder adopts a U-Net architecture. <xref rid=\"sensors-25-06963-f004\" ref-type=\"fig\">Figure 4</xref> shows the overall framework of CAM-UNet.</p><p>In the encoder, the architecture is refined from CoAtNet and designed to perform hierarchical, progressive feature extraction. At the initial S0 stage, two successive <inline-formula><mml:math id=\"mm1\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layers capture fundamental texture patterns, supplying abundant low-level cues to support higher-level semantic reasoning. The S1 stage incorporates the Fused-MBConv module, which enhances the representation of local textures and boundary structures while keeping computational costs low.</p><p>In the S2 stage, we use an improved MBConv block adapted from MobileNetV3, where the original SE attention mechanism is replaced by the CBAM mechanism. This change enables the model to more accurately focus on key areas under conditions such as complex water surface reflections, changing lighting, and deformable obstacles. As a result, the model&#8217;s ability to distinguish different features has been significantly enhanced.</p><p>For the deeper semantic stages S3 and S4, we employed the BiFormer [<xref rid=\"B20-sensors-25-06963\" ref-type=\"bibr\">20</xref>] module, which features a bidirectional attention mechanism that can effectively capture local dependencies and long-distance context relationships. This enables the model to significantly enhance its ability to identify distant obstacles and large-scale scene layouts, while reducing the impact of background noise. The MSAA [<xref rid=\"B21-sensors-25-06963\" ref-type=\"bibr\">21</xref>] module is located between the encoder and the decoder, and it can collect multi-scale context cues, thereby further improving the segmentation accuracy.</p><p>The decoder part employs the U-Net architecture, which gradually reconstructs high-resolution feature maps through upsampling and skip connections. It has excellent symmetry and strong feature restoration capabilities, providing robust performance and improving segmentation quality. Especially in complex environments, it can handle tasks that require high-precision outputs.</p></sec><sec id=\"sec2dot3dot2-sensors-25-06963\"><title>2.3.2. CoAtNet Network Structure</title><p>Due to its local perception ability and inductive bias, CNN [<xref rid=\"B22-sensors-25-06963\" ref-type=\"bibr\">22</xref>] has outstanding generalization ability and convergence speed during training and is suitable for small sample scenarios and low-latency tasks. However, the limitation of CNN is that its receptive field is limited, and it is difficult to capture long-distance dependencies. Compared with CNN, Transformer [<xref rid=\"B23-sensors-25-06963\" ref-type=\"bibr\">23</xref>] achieves global modeling through the self-attention mechanism, which is suitable for modeling complex semantic relationships. However, it lacks local inductive priors, has poor training stability, and its computational complexity increases with the square of the input resolution, making it difficult to efficiently process high-resolution images.</p><p>CoAtNet [<xref rid=\"B24-sensors-25-06963\" ref-type=\"bibr\">24</xref>] is a combined model of Convolution and Attention. It captures local and global information of input data by introducing depthwise separable convolution (MBConv) and relative attention mechanism (Rel-attention). MBConv improves the generalization performance of the model under small sample conditions through the inductive bias of the convolutional network; Rel-attention uses relative position encoding to make up for the shortcomings of CNN and Transformer in processing position information. CoAtNet combines convolution operations with self-attention mechanisms into a basic computational unit and vertically stacks multiple computational units in an organized manner to construct a complete network architecture.</p><p>In this model, the convolution kernel operates as a fixed filter, making it well-suited for capturing translation-invariant features. In contrast, the self-attention mechanism enriches the model&#8217;s representational capacity and ability to model complex spatial dependencies through the incorporation of relative positional encoding. An effective design is achieved by integrating the convolution layer with an adaptive attention matrix&#8212;combining a static, globally applied convolution kernel with a dynamically updated attention matrix, applied before and after the softmax normalization. This hybrid approach capitalizes on the strengths of both operations. Equations (1) and (2) describe the stages prior to and following the softmax operation, respectively.<disp-formula id=\"FD1-sensors-25-06963\"><label>(1)</label><mml:math id=\"mm2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mi mathvariant=\"normal\">o</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle=\"true\"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfenced><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mi mathvariant=\"normal\">p</mml:mi></mml:mrow></mml:mstyle><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-sensors-25-06963\"><label>(2)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mi mathvariant=\"normal\">r</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle=\"true\"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:munder><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mi mathvariant=\"normal\">p</mml:mi></mml:mrow></mml:mstyle><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-</mml:mtext><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm4\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represent specific coordinate positions in the global spatial space<inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mtext>&#160;</mml:mtext><mml:mi>G</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm8\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#969;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weight matrix of position <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mtext>-</mml:mtext><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> represents the global spatial space.</p><p>After combining the advantages of convolutional layers and self-attention layers, the network constructs a complete architecture by stacking. Due to the complexity of global context calculation, directly applying the relative attention mechanism will result in slow calculation. Therefore, it is necessary to downsample the feature map to reduce the spatial size and then use the global relative attention mechanism to enhance the feature representation. CoAtNet designed four model variants: C-C-C-C, C-C-C-T, C-C-T-T, and C-T-T-T (C represents the convolutional layer and T represents the self-attention layer), and compared them with the VIT [<xref rid=\"B25-sensors-25-06963\" ref-type=\"bibr\">25</xref>] (Vision Transformer) model. The model capability and generalization ability results are shown in Equations (3) and (4).<disp-formula id=\"FD3-sensors-25-06963\"><label>(3)</label><mml:math id=\"mm11\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&#8776;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&gt;</mml:mo><mml:mi mathvariant=\"normal\">V</mml:mi><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&gt;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&gt;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-06963\"><label>(4)</label><mml:math id=\"mm12\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mo>&#8776;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&#8805;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&gt;</mml:mo><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi><mml:mo>&#8805;</mml:mo><mml:mi mathvariant=\"normal\">V</mml:mi><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mi mathvariant=\"normal\">T</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>It can be seen that simply adding Transformer modules does not necessarily improve model performance, whereas hybrid stacked structures such as C-C-T-T and C-T-T-T can more effectively enhance the model&#8217;s expressive capability. Considering generalization performance, the performance of C-C-T-T is comparable to that of C-C-C-T. Therefore, CoAtNet ultimately adopts the C-C-T-T structure to balance model performance and generalization ability. This architecture effectively enhances the overall performance through the reasonable configuration of convolutional layers and Transformer layers. In the S0 stage, two <inline-formula><mml:math id=\"mm13\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutional layers are used for preliminary downsampling; in the S1 and S2 stages, convolutional modules are stacked to strengthen local feature learning; in the S3 and S4 stages, Transformer modules are introduced to capture long-range dependencies and global semantic information, and finally classification output is achieved through global average pooling and fully connected layers. In order to deal with the problems of gradient disappearance and explosion, the model introduces residual connections [<xref rid=\"B26-sensors-25-06963\" ref-type=\"bibr\">26</xref>] in each submodule from S1 to S4 to promote gradient propagation and improve the training stability, convergence speed and generalization ability of the network. The architecture diagram of the CoAtNet network is shown in <xref rid=\"sensors-25-06963-f005\" ref-type=\"fig\">Figure 5</xref>.</p></sec><sec id=\"sec2dot3dot3-sensors-25-06963\"><title>2.3.3. Fused-MBConv Module and MBConv Module</title><p>In CAM-UNet, the encoder&#8217;s S1 stage employs the Fused-MBConv module, while the S2 stage incorporates an enhanced MBConv block, enabling efficient extraction of multi-scale features. This configuration effectively enhances the network&#8217;s ability to extract complex textures and semantic features and significantly improves the model&#8217;s performance in fine-grained target recognition, precise boundary positioning, and adaptation to dynamic environments.</p><p>The Fused-MBConv module was first proposed in EfficientNetV2 as an improvement to the traditional MBConv architecture. Unlike MBConv, which performs <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> dimension-increasing convolutions and <inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> depthwise separable convolutions in a stepwise manner, Fused-MBConv combines the dimension-increasing and spatial feature extraction processes into a single standard convolution operation, simplifying the network structure and significantly improving computational efficiency and representation capabilities in the early feature extraction stages. This is shown in <xref rid=\"sensors-25-06963-f006\" ref-type=\"fig\">Figure 6</xref>.</p><p>The Fused-MBConv module uses standard convolution at the front end to simultaneously complete channel expansion and spatial feature extraction, then introduces nonlinear activation functions to enhance expression capabilities, and finally achieves dimensionality reduction and restores the number of channels through point-by-point convolution. This structure has significant advantages in shallow feature extraction. It avoids the information loss that may be caused by deep separable convolution in low-level feature extraction and effectively improves the modeling capabilities of texture edges, small-scale targets and complex backgrounds while maintaining low computational overhead. It is particularly suitable for the recognition of high-frequency disturbances and small obstacles in dynamic water surfaces.</p><p>As the network deepens, the MBConv [<xref rid=\"B27-sensors-25-06963\" ref-type=\"bibr\">27</xref>] module is retained in the S2 stage. It adopts an inverted bottleneck structure consisting of <inline-formula><mml:math id=\"mm16\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> dimensionality increase convolution, <inline-formula><mml:math id=\"mm17\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> depth convolution, SE attention mechanism and <inline-formula><mml:math id=\"mm18\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> dimensionality reduction convolution, as shown in <xref rid=\"sensors-25-06963-f007\" ref-type=\"fig\">Figure 7</xref>. After the dimensionality increase, the structure performs deep convolution in high-dimensional space, which helps to more fully extract spatial structural features and strengthen the response of key areas through channel attention. Deep convolution significantly reduces parameters and computation by processing channels independently and enhances semantic expression capabilities while maintaining model compactness.</p><p>The traditional MBConv in MobileNetV3 uses the SE module to strengthen important features through global channel weighting, but it ignores spatial dimension information and has difficulty dealing with problems such as dynamic reflection and complex background interference. Therefore, in order to improve the model&#8217;s expression of important areas in complex water surface environments, this paper improves the attention mechanism in the MBConv module. The SE module is replaced with a CBAM that simultaneously models channel and spatial attention to achieve more refined feature focus and information guidance. CBAM [<xref rid=\"B28-sensors-25-06963\" ref-type=\"bibr\">28</xref>] connects the channel attention and spatial attention mechanisms in series to achieve joint modeling of feature maps in both channel and spatial dimensions, significantly enhancing the model&#8217;s target perception ability in scenes with complex lighting, dynamic backgrounds, and partial occlusions. The channel attention submodule adopts a dual-branch structure of global average pooling and maximum pooling to extract global statistical information to evaluate the importance of each channel; the spatial attention submodule dynamically weights in the spatial dimension through convolution and activation functions to highlight key areas and structural edges. The CBAM architecture is shown in <xref rid=\"sensors-25-06963-f008\" ref-type=\"fig\">Figure 8</xref>.</p></sec><sec id=\"sec2dot3dot4-sensors-25-06963\"><title>2.3.4. BiFormer Module</title><p>In the encoder&#8217;s high-level semantic extraction stages (S3 and S4), this paper introduces the BiFormer module to enhance the model&#8217;s global modeling and detail representation capabilities in complex water surface scenes. The architecture of the Biformer module is shown in <xref rid=\"sensors-25-06963-f009\" ref-type=\"fig\">Figure 9</xref>.</p><p>BiFormer adopts a four-stage pyramid architecture. The initial stage generates primary features via overlapping patch embedding, while the subsequent second, third, and fourth stages progressively downsample the spatial resolution and expand the channel dimensions. Each stage is composed of several consecutive BiFormer blocks. Among the various module structures, depthwise separable convolution is primarily used to encode the spatial relative positional relationships, followed by a BRA (Bi-level Routing Attention) module and an MLP layer.</p><p>The BRA module applies a two-level routing strategy with dynamic sparse attention to filter out irrelevant key&#8211;value pairs, allowing the network to concentrate on salient regions. This design enhances the efficiency of feature extraction, strengthens multi-scale feature representation, and simultaneously reduces computational costs.</p><p>The 2D input feature map is divided into distinct, non-overlapping regions., each represented by feature vectors, from which query, key, and value tensors are obtained via separate linear transformations.<disp-formula id=\"FD5-sensors-25-06963\"><label>(5)</label><mml:math id=\"mm19\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>v</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the projection weights of <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. <inline-formula><mml:math id=\"mm22\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents query, key, and value. <inline-formula><mml:math id=\"mm23\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the input feature of the attention module. The superscript <inline-formula><mml:math id=\"mm24\" overflow=\"scroll\"><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> indicates that this is the feature tensor after regional division.</p><p>A feature averaging operation is performed within each non-overlapping region to aggregate local information and form region-level queries and keys.<disp-formula id=\"FD6-sensors-25-06963\"><label>(6)</label><mml:math id=\"mm25\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>K</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm26\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> is the length (height/width) of each non-overlapping area. <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the total number of tokens within an area.</p><p>Construct an adjacency matrix representing the relationships between regions.<disp-formula id=\"FD7-sensors-25-06963\"><label>(7)</label><mml:math id=\"mm28\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Only the top k connections of each region are kept to prune the dependency graph.<disp-formula id=\"FD8-sensors-25-06963\"><label>(8)</label><mml:math id=\"mm29\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">o</mml:mi><mml:mi mathvariant=\"normal\">p</mml:mi><mml:mi mathvariant=\"normal\">k</mml:mi><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mi mathvariant=\"normal\">d</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm30\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>I</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the index matrix, <inline-formula><mml:math id=\"mm31\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the maximum number of connections each region can establish with other regions.</p><p>Considering that each query requires access to the complete routing zone information, to improve memory access efficiency, the key&#8211;value pair tensors for the corresponding zones are pre-assembled.<disp-formula id=\"FD9-sensors-25-06963\"><label>(9)</label><mml:math id=\"mm32\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mi>g</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">g</mml:mi><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">h</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">r</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&#160;</mml:mtext><mml:msup><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">g</mml:mi><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">h</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">r</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To improve the processing efficiency of the model under high-resolution or large-scale input, a more detailed token-to-token attention mechanism is applied to the pre-extracted key&#8211;value pairs.<disp-formula id=\"FD10-sensors-25-06963\"><label>(10)</label><mml:math id=\"mm33\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">O</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">A</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mi mathvariant=\"normal\">t</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">o</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>g</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>g</mml:mi></mml:msup><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi mathvariant=\"normal\">C</mml:mi><mml:mi mathvariant=\"normal\">E</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The overall architecture of the BRA module is shown in the <xref rid=\"sensors-25-06963-f010\" ref-type=\"fig\">Figure 10</xref> below.</p></sec><sec id=\"sec2dot3dot5-sensors-25-06963\"><title>2.3.5. MSAA Module</title><p>The MSAA module consists of three main components: multi-scale fusion, spatial aggregation, and channel aggregation. Its design enhances the spatial awareness and channel representational capacity of feature maps, while keeping the additional computational cost minimal.</p><p>In the multi-scale fusion module, the input feature map <inline-formula><mml:math id=\"mm34\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is convolved for initial channel compression, producing an intermediate feature., resulting in an intermediate feature map <inline-formula><mml:math id=\"mm35\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This is then processed in parallel using three convolution kernels of different sizes: <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm37\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"mm38\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to extract feature information within the local and global receptive fields, respectively. These features, when concatenated in the channel dimension, fuse semantic representations from different scales, enhancing the model&#8217;s ability to model complex structures.</p><p>In the spatial attention aggregation module, the multi-scale fused feature map undergoes global average pooling to capture spatial context. A <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution then generates a spatial attention map (the size of it is <inline-formula><mml:math id=\"mm40\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) that is normalized using a sigmoid activation function. The spatial attention map is applied element by element to the original feature map, highlighting spatially salient regions, suppressing background interference, and improving spatial discrimination.</p><p>The channel attention aggregation module first performs global average pooling on the input feature map to obtain a channel description vector which size is <inline-formula><mml:math id=\"mm41\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. This is then generated through two channel-by-channel <inline-formula><mml:math id=\"mm42\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolutions (the first of which uses a ReLU activation function). This channel attention map is then multiplied channel-by-channel with the spatially weighted feature map to enhance salient channels and suppress redundant channels.</p><p>The enhanced features are resized using a <inline-formula><mml:math id=\"mm43\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> convolution and then added to the original input feature map using a residual connection, achieving a fusion of preserved original information and enhanced semantics.</p><p>The MSAA module is shown in <xref rid=\"sensors-25-06963-f011\" ref-type=\"fig\">Figure 11</xref>. It enhances spatial position sensitivity and channel selectivity and is capable of addressing the perception challenges faced by convolutional networks in complex backgrounds and multi-scale object scenes. This enables its features to be more semantically rich and spatially aligned. It can provide more precise segmentation boundaries and more reliable small object detection for the decoder stage.</p></sec><sec id=\"sec2dot3dot6-sensors-25-06963\"><title>2.3.6. U-Net Decoder</title><p>The decoder stage employs the U-Net decoder architecture [<xref rid=\"B29-sensors-25-06963\" ref-type=\"bibr\">29</xref>]. In complex aquatic environments, as the encoding path abstracts higher-level semantics, spatial details gradually diminish. Accurately depicting irregular obstacles and detecting small targets are of crucial importance. The symmetrical U-Net structure and its skip connection mechanism can bridge the gap between semantic abstraction and spatial accuracy, effectively reversing the information loss of the encoder.</p><p>The high-level semantic features of the MSAA module are used as the input for the decoding stage, and the transposed convolutional layer restores the spatial resolution through progressive upsampling. Unlike traditional upsampling methods, the transposed convolution is learnable and can reconstruct the spatial structure more accurately.</p><p>One of the key features of this design is the skip connection, which combines the feature maps of the encoder with the corresponding decoder layers. This process injects the high-resolution spatial details from the early encoding stage into the decoding process, significantly improving the boundary accuracy. Additionally, it performs multi-scale feature fusion by combining deep, semantically rich features with shallow, spatially precise features, thereby enhancing the accuracy of target detection at different scales. It also supports gradient flow during the training process, which helps overcome the problem of gradient vanishing and ensures effective learning of the entire network.</p><p>After upsampling, by connecting each layer with the corresponding feature map in the encoder, the key spatial details and edge information are retained, thereby improving the accuracy and stability of the segmentation results. Then, the fused feature maps are refined using convolution and ReLU activation functions, which not only maintain the structural integrity of the decoder but also its interpretability. The symmetric architecture and skip connections of the U-Net decoder effectively utilize multi-scale context, successfully balancing deep semantic understanding with shallow spatial information. The segmentation prediction is both semantically accurate and spatially precise, meeting the high-performance requirements for analyzing complex aquatic environments.</p></sec></sec></sec><sec sec-type=\"results\" id=\"sec3-sensors-25-06963\"><title>3. Results and Discussion</title><sec id=\"sec3dot1-sensors-25-06963\"><title>3.1. Evaluation Metrics</title><p>This study evaluates the model&#8217;s performance using several metrics, including Mean Intersection over Union (mIoU), F1 score, Params, and FLOPs. A detailed explanation of each metric is provided below.</p><p>Using the confusion matrix, predictions are classified as positive or negative, with each prediction being either correct or incorrect. This results in four categories for segmentation outcomes: True Positives (TPs), True Negatives (TNs), False Positives (FPs), and False Negatives (FNs).</p><p>Mean Intersection over Union (mIoU) is a commonly used metric for evaluating semantic segmentation models. It measures the overlap between the predicted segmentation and the ground truth annotations, offering a comprehensive assessment of the model&#8217;s performance across multiple classes. To calculate mIoU, the Intersection over Union (IoU) is computed for each class individually, and the average is then taken. Specifically, IoU is the ratio of the intersection to the union of the predicted region and the corresponding ground truth region. Higher IoU values indicate a closer alignment between the model&#8217;s segmentation and the true labels. The formula is given as follows:<disp-formula id=\"FD11-sensors-25-06963\"><label>(11)</label><mml:math id=\"mm44\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD12-sensors-25-06963\"><label>(12)</label><mml:math id=\"mm45\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:munderover><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>I</mml:mi></mml:mstyle><mml:mi>o</mml:mi><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The F1 score serves as a balanced metric for evaluating binary classification performance by computing the harmonic mean of Precision and Recall. Its formula is presented in Equation (15). The F1 score ranges from 0 to 1, where values closer to 1 signify superior model accuracy.<disp-formula id=\"FD13-sensors-25-06963\"><label>(13)</label><mml:math id=\"mm46\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD14-sensors-25-06963\"><label>(14)</label><mml:math id=\"mm47\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD15-sensors-25-06963\"><label>(15)</label><mml:math id=\"mm48\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Params refer to the total number of trainable parameters in a model. This metric captures the model&#8217;s size and its spatial complexity. Generally, a higher parameter count indicates a more complex model, which imposes a direct impact on the memory and computational power required for both training and inference.<disp-formula id=\"FD16-sensors-25-06963\"><label>(16)</label><mml:math id=\"mm49\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mi mathvariant=\"normal\">r</mml:mi><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi><mml:mo>=</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm50\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the kernel height, <inline-formula><mml:math id=\"mm51\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the kernel width, <inline-formula><mml:math id=\"mm52\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of input channels, <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of output channels, and the <inline-formula><mml:math id=\"mm54\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> accounts for the bias term.</p><p>FLOPs measure the total number of floating point operations a model performs, indicating its computational cost or time complexity. Since each addition or multiplication counts as one FLOP, a higher FLOP count generally implies a greater demand for computational resources.<disp-formula id=\"FD17-sensors-25-06963\"><label>(17)</label><mml:math id=\"mm55\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">F</mml:mi><mml:mi mathvariant=\"normal\">L</mml:mi><mml:mi mathvariant=\"normal\">O</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm56\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the height of the output feature map, <inline-formula><mml:math id=\"mm57\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the width of the output feature map, and multiplication by 2 indicates that a convolution operation includes one multiplication and one addition.</p></sec><sec id=\"sec3dot2-sensors-25-06963\"><title>3.2. Ablation Experiments</title><p>To verify the effectiveness of the key modules proposed in this paper in the task of water surface feasible domain and obstacle segmentation, we conducted a series of ablation experiments on a self-constructed dataset. The basic model uses CoAtNet as the backbone network and U-Net as the decoder. IoU1 and IoU2 represent the Intersection over Union (IoU) of the obstacle and feasible domain categories, respectively. The experimental results are shown in <xref rid=\"sensors-25-06963-t002\" ref-type=\"table\">Table 2</xref>.</p><p>The basic model without other structural optimizations, the model achieves mIoU of 89.31%. However, its IoU1 performance in obstacle areas is relatively weak, at only 84.23%, indicating its limited ability to represent boundary structures and small objects. <xref rid=\"sensors-25-06963-f012\" ref-type=\"fig\">Figure 12</xref> shows that the CoAtNet-Base model performs poorly in segmentation, with issues such as blurred boundaries around obstacles and feasible regions, and errors in obstacle segmentation.</p><p>In model (a), the Fused-MBConv and CBAM (model b) were introduced. The mIoU increased from 89.31% to 92.37%, and the obstacle IoU1 rose from 84.23% to 88.91%. This indicates that by improving local feature extraction and spatial attention, making it more efficient in capturing texture and edge details, it greatly contributes to enhancing the model performance.</p><p>The base of model (b) has enhanced the local capabilities. Based on this, we introduced the BiFormer module (model d) at the advanced semantic stage (S3/S4). This version achieved an mIoU of 94.15%, and its performance surpassed that of model (c)&#8212;including local enhancement and the MSAA module (93.1%), while requiring fewer computing resources. This indicates that using BiFormer for global context modeling at the high semantic stage brings more benefits than adding the MSAA module, providing significant advantages in terms of efficiency and performance.</p><p>When all the modules are integrated into the complete model (e), the mIoU reaches 95.15%. The improvement compared to model (d) confirms that the combination of MSAA with a powerful local&#8211;global representation is very valuable. The comparison of the experimental results can be seen in <xref rid=\"sensors-25-06963-f013\" ref-type=\"fig\">Figure 13</xref>.</p></sec><sec id=\"sec3dot3-sensors-25-06963\"><title>3.3. Comparative Experiments</title><p>To evaluate the performance of the proposed CAM-UNet model, we compared it with several advanced image segmentation models under the same experimental conditions. These included the classic U-Net [<xref rid=\"B29-sensors-25-06963\" ref-type=\"bibr\">29</xref>], the U-Net variant with the enhanced CoatNet backbone (U-Net (CoAtNet)), DeepLabv3+ [<xref rid=\"B30-sensors-25-06963\" ref-type=\"bibr\">30</xref>], DANet [<xref rid=\"B31-sensors-25-06963\" ref-type=\"bibr\">31</xref>], SegFormer-B1 [<xref rid=\"B19-sensors-25-06963\" ref-type=\"bibr\">19</xref>], and SeaFormer-B [<xref rid=\"B32-sensors-25-06963\" ref-type=\"bibr\">32</xref>], along with the domain-specific model WaSRNet [<xref rid=\"B33-sensors-25-06963\" ref-type=\"bibr\">33</xref>].</p><p>U-Net is renowned for its simplicity and powerful boundary restoration capabilities and is the preferred choice for many segmentation tasks. The U-Net (CoatNet) version improves the traditional design by using the powerful CoatNet backbone network. DeepLabv3+ introduces the Atrous Spatial Pyramid Pooling (ASPP) module, enhancing multi-scale context modeling and achieving high accuracy in semantic segmentation. DANet combines channel-level and spatial attention modules within the standard CNN framework, improving the model&#8217;s focus on key areas and enhancing object recognition in complex scenes. SegFormer is a relatively new lightweight Transformer-based model that effectively combines global context modeling with fast decoding processes, providing excellent inference speed without sacrificing accuracy.</p><p>SegFormer is a Transformer-based model that effectively combines global context modeling with fast decoding processes, providing excellent inference speed. SeaFormer balances performance and computational efficiency through lightweight CNN and attention mechanisms, enhancing feature representation. It is particularly suitable for environments with limited computing resources. WaSRNet is a model for the marine segmentation field. Although it has high computational requirements, it is good at identifying marine objects and is a key benchmark in this field.</p><p>All models were trained and tested on the self-built WSM USV dataset. The experimental results are shown in <xref rid=\"sensors-25-06963-t003\" ref-type=\"table\">Table 3</xref>.</p><p>U-Net demonstrates the reliability of its simple architecture but also reveals limitations in computational efficiency. U-Net (CoatNet) significantly improves efficiency while maintaining model accuracy by introducing a more advanced backbone network, showcasing the impact of backbone network choice on model performance. DeepLabv3+ improves segmentation accuracy through multi-scale context modeling, but this increases the number of parameters and reduces efficiency. DANet achieves excellent performance using a complex dual-attention mechanism, but this also incurs very high computational costs. In contrast, SegFormer-B1 achieves a better balance between complexity and performance with its innovative hierarchical Transformer design. SeaFormer-B, as a lightweight model, offers both efficiency and performance. WaSRNet performs well in ocean segmentation, but its high computational cost limits its practical application in real-world scenarios.</p><p>CAM-UNet integrates several key modules: The encoder combines Fused-MBConv and CBAM to enhance shallow feature extraction and improve perception of important areas; the BiFormer module at a higher semantic level strengthens long-range dependency modeling and semantic representation; the MSAA module effectively integrates multi-scale semantics and spatial information; the decoder maintains the skip connections of U-Net style to retain spatial details during upsampling. These designs give CAM-UNet greater adaptability and stability in complex water surface scenarios, and can effectively handle small obstacles, blurred edges, etc.</p><p>Although the number of parameters in CAM-UNet is greater than that of some lightweight models, in scenarios where maritime navigation safety is of utmost importance, we will prioritize accuracy. Despite the increase in computational complexity, this model has achieved a significant improvement in accuracy, which is of great help in ensuring the perception system of unmanned surface vessels (USVs) operating in challenging marine environments.</p><p><xref rid=\"sensors-25-06963-f014\" ref-type=\"fig\">Figure 14</xref> presents semantic segmentation results for five representative images across different methods. The visual comparisons clearly show that CAM-UNet outperforms other models in various challenging water scenes, accurately detecting small obstacles and fine shoreline details with clear boundaries and well-preserved structures.</p><p>Having established CAM-UNet&#8217;s effectiveness on the specialized WSM USV dataset, we now assess its generalizability on public benchmarks. Experiments were conducted on the MaSTr1325.</p><p>The MaSTr1325 dataset is widely used in the field of unmanned surface vessels (USVs) in the ocean. This dataset can meet the obstacle detection requirements of small coastal USVs, and it contains 1325 images captured on-site. These photos were taken during a two-year coastal patrol mission by a USV, and each image includes pixel-level semantic annotations of three categories: sky, ocean, and obstacle. The evaluation uses three IoU metrics: IoU1 (sky), IoU2 (ocean), and IoU3 (obstacle). The experimental results are shown in <xref rid=\"sensors-25-06963-t004\" ref-type=\"table\">Table 4</xref>.</p><p>Our model achieved an mIoU of 98.48% on the dataset, surpassing all other methods, and obtained good IoU in the three categories of sky, ocean, and obstacles. In the crucial &#8220;obstacle&#8221; category, CAM-UNet achieved an IoU of 96.37%, significantly outperforming other models. This result demonstrates its ability to accurately identify obstacles in complex marine environments. Compared with WaSRNet, which is specifically designed for marine applications, CAM-UNet not only achieved higher segmentation accuracy but also maintained strong model efficiency, showcasing excellent overall performance. These results indicate that CAM-UNet not only performs well on the dedicated local dataset but also impressively performs in public benchmark tests. It has been proven to be highly competitive and adaptable, capable of handling diverse marine environments and task requirements.</p></sec><sec id=\"sec3dot4-sensors-25-06963\"><title>3.4. Robustness Evaluation</title><p>To further assess the model&#8217;s robustness, we subjected the test set to Gaussian noise, color jitter, and image blurring, creating three sub-test sets that simulate more complex surface disturbance conditions. We then performed semantic segmentation experiments on these sets using CAM-UNet, SeaFormer-B, and WaSRNet, with mean Intersection over Union (mIoU) as the primary evaluation metric.</p><p>The experimental results shown in <xref rid=\"sensors-25-06963-f015\" ref-type=\"fig\">Figure 15</xref> indicate that CAM-UNet outperforms other models in terms of stability under various disturbances. It maintains the highest mIoU across all test conditions, demonstrating particularly strong resilience to Gaussian noise and image blurring. In contrast, CAM-UNet excels in maintaining stability in disturbed environments while also preserving high accuracy on original images, thanks to its effective feature extraction and fusion mechanisms. While WaSRNet performs well on original images, its significant drop in performance under disturbances highlights its sensitivity to changes in image quality. These findings confirm that CAM-UNet&#8217;s architecture offers enhanced reliability in challenging maritime environments, where image quality can often be compromised.</p></sec></sec><sec sec-type=\"conclusions\" id=\"sec4-sensors-25-06963\"><title>4. Conclusions</title><p>The proposed CAM-UNet has performed exceptionally well in semantic segmentation tasks in complex water environments. This model is constructed based on the improved CoAtNet backbone network and uses U-Net as the decoder. In the encoder, it integrates Fused-MBConv, CBAM, and BiFormer modules, and uses the MSAA module for multi-scale feature fusion. Experimental results show that CAM-UNet outperforms mainstream models such as DeepLabV3+, SegFormer, and SeaFormer in multiple key evaluation metrics. It excels in boundary restoration and small object detection. However, while the model can be deployed on most modern USV embedded systems, its implementation on resource-constrained platforms may require additional optimization. Future research will focus on optimizing model performance from multiple aspects, including fusing multi-source image information (such as visible light and infrared) to enhance stability in low-light and high-interference environments, expanding the model&#8217;s temporal modeling capabilities to support video-level semantic segmentation tasks, and compressing the model size through lightweight design and efficient feature extraction strategies to improve its deployment efficiency and application value in practical scenarios such as intelligent navigation, water monitoring, and port management.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.W.; methodology, X.G.; validation, J.L.; investigation, J.L.; resources, Y.L.; data curation, X.G. and J.L.; writing&#8212;original draft, X.G.; project administration, Y.W.; funding acquisition, Y.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The datasets presented in this article are not readily available because the data are part of an ongoing study. Requests to access the datasets should be directed to nfugxy@njfu.edu.cn.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>Authors Jie Liu was employed by the company Qingdao Guoshi Intelligent Equipment Technology Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-06963\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Barrera</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Padr&#243;n Armas</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Luis</surname><given-names>F.S.</given-names></name><name name-style=\"western\"><surname>Llin&#226;s</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Marichal</surname><given-names>N.</given-names></name></person-group><article-title>Trends and Challenges in Unmanned Surface Vehicles (USV): From Survey to Shipping</article-title><source>TransNav Int. J. Mar. Navig. Saf. Sea Transp.</source><year>2021</year><volume>15</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type=\"doi\">10.12716/1001.15.01.13</pub-id></element-citation></ref><ref id=\"B2-sensors-25-06963\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dai</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Leng</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Han</surname><given-names>S.</given-names></name></person-group><article-title>LDMNet: Enhancing the Segmentation Capabilities of Unmanned Surface Vehicles in Complex Waterway Scenarios</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><elocation-id>7706</elocation-id><pub-id pub-id-type=\"doi\">10.3390/app14177706</pub-id></element-citation></ref><ref id=\"B3-sensors-25-06963\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shao</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yuan</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>C.</given-names></name></person-group><article-title>DMTN-Net: Semantic Segmentation Architecture for Surface Unmanned Vessels</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>4539</elocation-id><pub-id pub-id-type=\"doi\">10.3390/electronics13224539</pub-id></element-citation></ref><ref id=\"B4-sensors-25-06963\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>J.</given-names></name></person-group><article-title>Cross-Granularity Infrared Image Segmentation Network for Nighttime Marine Observations</article-title><source>J. Mar. Sci. Eng.</source><year>2024</year><volume>12</volume><elocation-id>2082</elocation-id><pub-id pub-id-type=\"doi\">10.3390/jmse12112082</pub-id></element-citation></ref><ref id=\"B5-sensors-25-06963\"><label>5.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tong</surname><given-names>Q.</given-names></name></person-group><article-title>Research on Semantic Segmentation and Coastline Extraction of Remote Sensing Images Based on Deep Learning</article-title><source>Master&#8217;s Thesis</source><publisher-name>Hainan Normal University</publisher-name><publisher-loc>Haikou, China</publisher-loc><year>2024</year></element-citation></ref><ref id=\"B6-sensors-25-06963\"><label>6.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Karakus</surname><given-names>P.</given-names></name></person-group><article-title>Detection of Water Surface Using Canny and Otsu Threshold Methods with Machine Learning Algorithms on Google Earth Engine: A Case Study of Lake Van</article-title><source>Appl. Sci.</source><year>2025</year><volume>15</volume><elocation-id>2903</elocation-id><pub-id pub-id-type=\"doi\">10.3390/app15062903</pub-id></element-citation></ref><ref id=\"B7-sensors-25-06963\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zheng</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Yasir</surname><given-names>M.</given-names></name></person-group><article-title>Automatic coastline extraction based on the improved instantaneous waterline extraction method and correction criteria using SAR imagery</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><elocation-id>7199</elocation-id><pub-id pub-id-type=\"doi\">10.3390/su15097199</pub-id></element-citation></ref><ref id=\"B8-sensors-25-06963\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wen</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Cong</surname><given-names>X.</given-names></name></person-group><article-title>Shoreline detection based on water surface image features in HSV space</article-title><source>J. Image Graph.</source><year>2018</year><volume>23</volume><fpage>526</fpage><lpage>533</lpage></element-citation></ref><ref id=\"B9-sensors-25-06963\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zheng</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Cun</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>R.</given-names></name></person-group><article-title>Research on shoreline extraction method based on improved region growing</article-title><source>J. Comput. Appl.</source><year>2020</year><volume>37</volume><fpage>1876</fpage><lpage>1881</lpage><pub-id pub-id-type=\"doi\">10.19734/j.issn.1001-3695.2018.12.0917</pub-id></element-citation></ref><ref id=\"B10-sensors-25-06963\"><label>10.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ling</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>J.</given-names></name></person-group><article-title>Design of water area segmentation model and channel pruning acceleration method for unmanned boats</article-title><source>Ind. Control Comput.</source><year>2022</year><volume>35</volume><fpage>73</fpage><lpage>75</lpage></element-citation></ref><ref id=\"B11-sensors-25-06963\"><label>11.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Jafari</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name></person-group><article-title>WaterNet: An adaptive matching pipeline for segmenting water with volatile appearance</article-title><source>Comput. Vis. Media</source><year>2020</year><volume>6</volume><fpage>65</fpage><lpage>78</lpage><pub-id pub-id-type=\"doi\">10.1007/s41095-020-0156-x</pub-id></element-citation></ref><ref id=\"B12-sensors-25-06963\"><label>12.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Asaro</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Murdaca</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Prati</surname><given-names>C.M.</given-names></name></person-group><article-title>Learning deep models from weak labels for water surface segmentation in sar images</article-title><source>Proceedings of the 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</source><conf-loc>Brussels, Belgium</conf-loc><conf-date>11&#8211;16 July 2021</conf-date><fpage>6048</fpage><lpage>6051</lpage></element-citation></ref><ref id=\"B13-sensors-25-06963\"><label>13.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Akiyama</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Marcato Junior</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Gon&#231;alves</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Bressan</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Eltner</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Binder</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Singer</surname><given-names>T.</given-names></name></person-group><article-title>Deep learning applied to water segmentation</article-title><source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2020</year><volume>43</volume><fpage>1189</fpage><lpage>1193</lpage><pub-id pub-id-type=\"doi\">10.5194/isprs-archives-XLIII-B2-2020-1189-2020</pub-id></element-citation></ref><ref id=\"B14-sensors-25-06963\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cui</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Jing</surname><given-names>W.-P.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name></person-group><article-title>SANet: A Sea&#8211;Land Segmentation Network Via Adaptive Multiscale Feature Learning</article-title><source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source><year>2021</year><volume>14</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type=\"doi\">10.1109/JSTARS.2020.3040176</pub-id></element-citation></ref><ref id=\"B15-sensors-25-06963\"><label>15.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>Y.</given-names></name></person-group><article-title>Research on Shoreline Detection in Inland River Scenes Based on Semantic Segmentation</article-title><source>Master&#8217;s Thesis</source><publisher-name>Harbin University of Science and Technology</publisher-name><publisher-loc>Harbin, China</publisher-loc><year>2023</year></element-citation></ref><ref id=\"B16-sensors-25-06963\"><label>16.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xiong</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>Y.</given-names></name></person-group><article-title>Research on feasible region and fast obstacle segmentation algorithm for unmanned surface vehicles</article-title><source>J. Electron. Meas. Instrum.</source><year>2023</year><volume>37</volume><fpage>11</fpage><lpage>20</lpage><pub-id pub-id-type=\"doi\">10.13382/j.jemi.B2205804</pub-id></element-citation></ref><ref id=\"B17-sensors-25-06963\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Kong</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Ni</surname><given-names>P.</given-names></name></person-group><article-title>Boundary Enhancement-Driven Accurate Semantic Segmentation Networks for Unmanned Surface Vessels in Complex Marine Environments</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>24972</fpage><lpage>24987</lpage><pub-id pub-id-type=\"doi\">10.1109/JSEN.2024.3409756</pub-id></element-citation></ref><ref id=\"B18-sensors-25-06963\"><label>18.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>B.</given-names></name></person-group><article-title>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#8211;17 October 2021</conf-date><fpage>9992</fpage><lpage>10002</lpage></element-citation></ref><ref id=\"B19-sensors-25-06963\"><label>19.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Anandkumar</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>&#193;lvarez</surname><given-names>J.M.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Virtual</conf-loc><conf-date>6&#8211;14 December 2021</conf-date></element-citation></ref><ref id=\"B20-sensors-25-06963\"><label>20.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ke</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Lau</surname><given-names>R.W.H.</given-names></name></person-group><article-title>BiFormer: Vision Transformer with Bi-Level Routing Attention</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#8211;24 June 2023</conf-date><fpage>10323</fpage><lpage>10333</lpage></element-citation></ref><ref id=\"B21-sensors-25-06963\"><label>21.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Bai</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>R.</given-names></name></person-group><article-title>3D medical image registration model based on large kernel convolution and Transformer parallelism</article-title><source>Chin. J. Lasers</source><year>2025</year><volume>52</volume><fpage>71</fpage><lpage>81</lpage></element-citation></ref><ref id=\"B22-sensors-25-06963\"><label>22.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Albawi</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Mohammed</surname><given-names>T.A.</given-names></name><name name-style=\"western\"><surname>Al-Zawi</surname><given-names>S.</given-names></name></person-group><article-title>Understanding of a convolutional neural network</article-title><source>Proceedings of the 2017 International Conference on Engineering and Technology (ICET)</source><conf-loc>Antalya, Turkey</conf-loc><conf-date>21&#8211;23 August 2017</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id=\"B23-sensors-25-06963\"><label>23.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vaswani</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Shazeer</surname><given-names>N.M.</given-names></name><name name-style=\"western\"><surname>Parmar</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Uszkoreit</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Jones</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Gomez</surname><given-names>A.N.</given-names></name><name name-style=\"western\"><surname>Kaiser</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Polosukhin</surname><given-names>I.</given-names></name></person-group><article-title>Attention is All you Need</article-title><source>Proceedings of the Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#8211;9 December 2017</conf-date></element-citation></ref><ref id=\"B24-sensors-25-06963\"><label>24.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dai</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Le</surname><given-names>Q.V.</given-names></name><name name-style=\"western\"><surname>Tan</surname><given-names>M.</given-names></name></person-group><article-title>CoAtNet: Marrying Convolution and Attention for All Data Sizes</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"arxiv\">2106.04803</pub-id></element-citation></ref><ref id=\"B25-sensors-25-06963\"><label>25.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Weissenborn</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhai</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Unterthiner</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Dehghani</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Minderer</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Heigold</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Gelly</surname><given-names>S.</given-names></name><etal/></person-group><article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"arxiv\">2010.11929</pub-id></element-citation></ref><ref id=\"B26-sensors-25-06963\"><label>26.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id=\"B27-sensors-25-06963\"><label>27.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Howard</surname><given-names>A.G.</given-names></name><name name-style=\"western\"><surname>Sandler</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Chu</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L.-C.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Tan</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Pang</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Vasudevan</surname><given-names>V.</given-names></name><etal/></person-group><article-title>Searching for MobileNetV3</article-title><source>Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>1314</fpage><lpage>1324</lpage></element-citation></ref><ref id=\"B28-sensors-25-06963\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Woo</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Park</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>J.-Y.</given-names></name><name name-style=\"western\"><surname>Kweon</surname><given-names>I.-S.</given-names></name></person-group><article-title>CBAM: Convolutional Block Attention Module</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1807.06521</pub-id><pub-id pub-id-type=\"arxiv\">1807.06521</pub-id></element-citation></ref><ref id=\"B29-sensors-25-06963\"><label>29.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1505.04597</pub-id><pub-id pub-id-type=\"arxiv\">1505.04597</pub-id></element-citation></ref><ref id=\"B30-sensors-25-06963\"><label>30.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>L.-C.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Schroff</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Adam</surname><given-names>H.</given-names></name></person-group><article-title>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#8211;14 September 2018</conf-date></element-citation></ref><ref id=\"B31-sensors-25-06963\"><label>31.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tian</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>H.</given-names></name></person-group><article-title>Dual Attention Network for Scene Segmentation</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#8211;20 June 2019</conf-date><fpage>3141</fpage><lpage>3149</lpage></element-citation></ref><ref id=\"B32-sensors-25-06963\"><label>32.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"arxiv\">2301.13156</pub-id></element-citation></ref><ref id=\"B33-sensors-25-06963\"><label>33.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bovcon</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Kristan</surname><given-names>M.</given-names></name></person-group><article-title>WaSR&#8212;A water segmentation and refinement maritime obstacle detection network</article-title><source>IEEE Trans. Cybern.</source><year>2021</year><volume>52</volume><fpage>12661</fpage><lpage>12674</lpage><pub-id pub-id-type=\"doi\">10.1109/TCYB.2021.3085856</pub-id><pub-id pub-id-type=\"pmid\">34232901</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-06963-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Examples of WSM USV data in the self-built dataset. (<bold>a</bold>) Daytime with clear sky; (<bold>b</bold>) daytime with rain; (<bold>c</bold>) evening with fog; (<bold>d</bold>) nighttime.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Example of annotation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Examples of data augmentation. (<bold>a</bold>) Original image; (<bold>b</bold>) horizontal flip; (<bold>c</bold>) brightness adjustment; (<bold>d</bold>) Gaussian noise; (<bold>e</bold>) color jittering; (<bold>f</bold>) image blurring (<bold>g</bold>) contrast adjustment; (<bold>h</bold>) salt and pepper noise; (<bold>i</bold>) vertical flip.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Overall framework diagram of CAM-UNet.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Architecture diagram of CoAtNet network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>The Fused-MBConv module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>The Mbconv module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Structure of CBAM.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g008.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f009\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Structure diagram of the Biformer module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g009.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f010\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Structure of BRA.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g010.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f011\" orientation=\"portrait\"><label>Figure 11</label><caption><p>Structure diagram of MSAA module.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g011.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f012\" orientation=\"portrait\"><label>Figure 12</label><caption><p>Structure diagram of U-Net decoder.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g012.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f013\" orientation=\"portrait\"><label>Figure 13</label><caption><p>Comparison of ablation experiment results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g013.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f014\" orientation=\"portrait\"><label>Figure 14</label><caption><p>Comparison of experimental results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g014.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06963-f015\" orientation=\"portrait\"><label>Figure 15</label><caption><p>Robustness experiment results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06963-g015.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-06963-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06963-t001_Table 1</object-id><label>Table 1</label><caption><p>Experimental equipment.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Software and Hardware Configuration</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Version Model</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">GPU</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">NVIDIA GeForce RTX 4070 (12 GB VRAM)</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">CPU</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Intel Core i7-13700KF</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Framework</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PyTorch 1.13.0 + Python 3.8</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Acceleration</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CUDA 11.6, cuDNN 8.4</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06963-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06963-t002_Table 2</object-id><label>Table 2</label><caption><p>Results of the ablation experiment.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Sequence Number</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CoAtNet-Base</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Fused-MBConv &amp; CBAM</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">BiFormer</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">+MSAA</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU (%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">IoU1 </th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">IoU2 </th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(a)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">32.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">29.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.31</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.23</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.39</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(b)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">30.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.37</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.91</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">95.83</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(c)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">35.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">96.7</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">(d)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">36.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.15</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.44</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">96.86</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">(e)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">42.3</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">37.8</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">95.15</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">92.86</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">97.44</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06963-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06963-t003_Table 3</object-id><label>Table 3</label><caption><p>Results of comparative experiments on custom dataset.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU (%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">F1-Score (%)</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">31.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.42</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.7</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Net (CoatNet)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">29.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">32.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.31</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.6</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DeepLabv3+</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">64.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.04</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.5</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DANet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">67.4</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.6</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.6</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegFormer-B1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">13.7</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">16.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">90.77</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.1</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SeaFormer-B</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">8.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.8</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">WaSRNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.27</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.8</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CAM-UNet (Ours)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">37.8</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">42.3</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">95.15</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">95.3</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06963-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06963-t004_Table 4</object-id><label>Table 4</label><caption><p>Results of comparative experiments on MaSTr1325.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU (%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">IoU1 (%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">IoU2 (%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">IoU3 (%)</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.21</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.86</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.65</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DeepLabV3+</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">96.73</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.09</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.75</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegFormer-B1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.07</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.55</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.92</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.73</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SeaFormer-B</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.42</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.95</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">WaSRNet</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.27</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">95.87</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CAM-UNet (Ours)</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">98.48</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">99.66</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">99.42</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">96.37</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656184 PMC12656184.1 12656184 12656184 41305170 10.3390/s25226963 sensors-25-06963 1 Article CAM-UNet: A Novel Water Environment Perception Method Integrating CoAtNet Structure https://orcid.org/0009-0009-3468-4125 Gao Xingyi Methodology Data curation Writing &#8211; original draft 1 Liu Jie Validation Investigation Data curation 2 Liu Yanyi Resources 1 https://orcid.org/0000-0001-8473-8990 Wu Yin Conceptualization Project administration Funding acquisition 1 * Chen Honggang Academic Editor 1 College of Information Science and Technology &amp; Artificial Intelligence, Nanjing Forestry University, Nanjing 210037, China; nfugxy@njfu.edu.cn (X.G.); yyliu@njfu.edu.cn (Y.L.) 2 Qingdao Guoshi Intelligent Equipment Technology Co., Ltd., Qingdao 266000, China; jliu05328@gmail.com * Correspondence: wuyin@njfu.edu.cn 14 11 2025 11 2025 25 22 501335 6963 24 9 2025 07 11 2025 12 11 2025 14 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Accurate segmentation of navigable waters and obstacles is critical for unmanned surface vessel navigation yet remains challenging in real aquatic environments characterized by complex water textures and blurred boundaries. Current models often struggle to simultaneously capture long-range contextual dependencies and fine spatial details, frequently leading to fragmented segmentation results. In order to resolve these issues, we present a novel segmentation model based on the CoAtNet architecture. Our framework employs an enhanced convolutional attention encoder, where a Fused Mobile Inverted Bottleneck Convolution (Fused-MBConv) module refines boundary features while a Convolutional Block Attention Module (CBAM) enhances feature awareness. The model incorporates a Bi-level Former (BiFormer) to enable collaborative modeling of global and local features, complemented by a Multi-scale Attention Aggregation (MSAA) module that effectively captures contextual information across different scales. The decoder, based on U-Net, restores spatial resolution gradually through skip connections and upsampling. In our experiments, the model achieves 95.15% mIoU on a self-collected dataset and 98.48% on the public MaSTr1325 dataset, outperforming DeepLabV3+, SeaFormer, and WaSRNet. These results show the model&#8217;s ability to effectively interpret complex aquatic environments for autonomous navigation. deep learning CoAtNet semantic segmentation image processing unmanned surface vessel National Natural Science Foundation of China 32171788 Jiangsu Provincial Government Scholarship for Overseas Studies JS-2018-043 This research was funded by the National Natural Science Foundation of China grant number 32171788 and the the Jiangsu Provincial Government Scholarship for Overseas Studies grant number JS-2018-043. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction In recent years, with the continuous advancement of artificial intelligence technology, unmanned systems such as drones and self-driving cars have received widespread attention. Unmanned surface vehicles (USVs) are an important branch of this field and have shown broad application prospects in the fields of ocean surveys, environmental monitoring, water rescue and resource development [ 1 ]. In order to achieve autonomous navigation and mission planning, USVs need to have efficient environmental perception capabilities. Among them, accurately segmenting the traversable area and obstacles on the water surface is a key task in its visual perception system, which affects the safety and execution efficiency of the navigation path [ 2 ]. The environmental perception of USVs is the basis for their autonomous navigation and mission execution. They usually collect visible light images by carrying visual sensors and perform semantic segmentation of the water surface, land and obstacles [ 3 ]. Common sensors include monocular or binocular RGB cameras, panoramic cameras, infrared imaging equipment, LiDAR, radar and multispectral/hyperspectral sensors. Among them, visual cameras have the advantages of low cost and rich information and are the most widely used perception devices; infrared sensors can assist in identifying targets at night or in low-light environments; lidar can provide accurate three-dimensional spatial information, which is suitable for structural mapping and obstacle detection; and radar has strong penetration and can maintain stable perception under adverse weather conditions [ 4 ]. Current image segmentation methods mainly include traditional image processing and deep learning. Traditional image segmentation methods mainly include edge detection, threshold segmentation, active contour model and region growing, etc. [ 5 ]. These methods have low computing power and data requirements and can run efficiently on low-power edge devices, but they are difficult to handle complex scenes. Edge detection methods (e.g., Canny) often produce fragmented boundaries, while thresholding techniques require manual parameter tuning; active contour models are highly sensitive to initialization and image intensity variations [ 6 ]; region growing can improve local accuracy but suffers from high computational cost and sensitivity to seed selection [ 7 ]. To address these issues, Peng et al. [ 8 ] proposed combining the HSV color model with edge detection for shoreline extraction, though the method lacked robustness for irregular boundaries. Zheng et al. [ 9 ] integrated Otsu thresholding with region growing to enhance accuracy, but at the cost of increased computation. Deep learning-based methods have been extensively applied to water surface segmentation [ 10 , 11 , 12 , 13 ], with Convolutional Neural Networks (CNNs) demonstrating strong capability in extracting local features. The adaptive scale network SANet proposed by Cui et al. [ 14 ] performed well in the segmentation task of the coastal area of Lianyungang, Jiangsu, but it has not been verified on data from other regions, which limits its applicability in a wider range of scenarios. Guo Yangang et al. [ 15 ] improved PSPNet by introducing transfer learning and attention mechanism to reduce the false detection rate, but the recognition of details in the fuzzy boundary area is still limited; Xiong Rui et al. [ 16 ] proposed DeepLabV3-CSPNet based on DeepLabV3, which improved the accuracy, but the stability is still insufficient under extreme lighting conditions. Zhang Liya et al. [ 17 ] proposed BEMSNet, which effectively improved the boundary recognition accuracy in the semantic segmentation of unmanned surface vehicle navigation images through the innovative design of boundary abstraction module and boundary enhancement module, However, it still has limitations, such as poor adaptability to extreme weather scenarios. On the other hand, the Transformer architecture has demonstrated superior semantic understanding performance due to its excellent global modeling capabilities. Representative models such as Swin Transformer [ 18 ] and SegFormer [ 19 ] have achieved leading results in multiple segmentation tasks. However, the Transformer model has large parameter scale, high computational resource consumption, complex deployment, and is prone to overfitting in small sample tasks. Its application in USV visual perception still faces challenges. Based on the above problems, this paper proposes an improved semantic segmentation network CAM-UNet based on CoAtNet. The main contributions are as follows: (1) We proposed CAM-UNet, an improved encoder&#8211;decoder network that combines convolutional structures and attention mechanisms to capture both local textures and global semantics. The model was specifically designed to handle water surface segmentation tasks, especially those involving complex boundaries or small targets. (2) We implemented a multi-stage feature extraction framework based on the CoAtNet backbone, incorporating various modules to improve both local details and global context understanding. To handle challenging conditions such as surface ripples and strong reflections, we introduced a module between the encoder and decoder to improve multi-scale feature representation. (3) We created and manually annotated a semantic segmentation dataset containing 4950 images covering seven types of water surfaces and six obstacle categories. The dataset included more than 10 representative scenarios and was used for both model training and performance evaluation. 2. Materials and Methods 2.1. Experimental Details All experiments were conducted on a workstation equipped with an NVIDIA GeForce RTX 4070 GPU (12 GB VRAM) from ASUS (Taipei, China), an Intel Core i7-13700KF processor from Intel Corporation (Santa Clara, CA, USA), and 64 GB of memory. The model was implemented using the PyTorch 1.13.0 framework with Python 3.8 and accelerated by CUDA 11.6 and cuDNN 8.4. During training, the Adam optimizer was employed with an initial learning rate of 1 &#215; 10 &#8722;4 and a batch size of 8. The learning rate was gradually decayed using a cosine annealing strategy, and the model was trained for a total of 200 epochs. The experimental equipment configuration is shown in Table 1 . The loss functions used in this experiment are Cross Entropy Loss and Dice Loss. Since the foreground regions&#8212;especially obstacle classes&#8212;occupy relatively small areas in the dataset, while the background covers a much larger proportion, using a conventional Cross Entropy Loss (CE Loss) alone may lead to the positive samples being overwhelmed by the negative ones. Dice Loss is more sensitive to positive samples in the early stages of training and thus helps to better capture the foreground regions. However, it may suffer from loss saturation; therefore, it is combined with Cross Entropy Loss to achieve a balanced optimization. 2.2. Dataset Construction and Processing The task of segmenting feasible water domains and obstacles involves high scene complexity and substantial visual interference, placing stringent requirements on the quality, diversity, and annotation precision of the training data. To ensure reliable performance in various environments, the dataset should cover multiple types of water bodies and scene settings. Additionally, a wide range of lighting conditions, weather patterns, and time of day should be taken into account to make the dataset content rich and diverse. At the object level, the dataset should include various common surface structures as well as dynamic obstacles. These obstacles should vary in size, direction, material properties, and occlusion level. This diversity ensures that the dataset closely reflects the real-world challenges faced in navigable water environments. Additionally, the dataset should also contain a large number of pure water scenes, which will help the model better learn the navigable boundaries, surface textures, and shape changes, ultimately facilitating clearer classification during the segmentation process. Sampling. Based on these requirements, this paper constructed a water surface image dataset suitable for this study through self-collection and online search. The final dataset contains a total of 4950 images, which are divided into a training set of 3465 images, a validation set of 990 images, and a test set of 495 images according to a 7:2:1 ratio. Image acquisition was conducted in several representative aquatic environments using high-definition RGB cameras mounted on an autonomous surface platform and fixed onshore equipment. The captured images cover various times of day and incorporate perspective variations and reflection disturbances in various natural environments. The captured images are shown in Figure 1 , encompassing daytime, nighttime, sunny, rainy, and foggy conditions. Labels include feasible regions and obstacles. The original image was manually annotated frame by frame using Labelme 5.8.1 software. The polygonal outline of the target area was accurately drawn to generate the corresponding segmentation label. The annotation process is as follows. The example of annotation is shown in Figure 2 . To enhance the model&#8217;s generalization capabilities and adaptability to diverse environmental changes, this study performed data augmentation on the training set. Obstacle types and backgrounds in water scenes are complex and varied, and images may be subject to lighting variations, angle offsets, and object occlusions, all of which can affect model training. To this end, we employed common image augmentation techniques, including rotation, flipping, scaling, cropping, brightness adjustment, and salt-and-pepper noise. Figure 3 shows some of these data augmentation methods. 2.3. Methods 2.3.1. CAM-UNet Overall Framework CAM-UNet is an improved encoder&#8211;decoder network architecture for water surface feasible domain and obstacle segmentation tasks. The encoder is structurally optimized based on CoAtNet and employs a multi-scale attention mechanism to improve segmentation accuracy. The decoder adopts a U-Net architecture. Figure 4 shows the overall framework of CAM-UNet. In the encoder, the architecture is refined from CoAtNet and designed to perform hierarchical, progressive feature extraction. At the initial S0 stage, two successive 3 &#215; 3 convolutional layers capture fundamental texture patterns, supplying abundant low-level cues to support higher-level semantic reasoning. The S1 stage incorporates the Fused-MBConv module, which enhances the representation of local textures and boundary structures while keeping computational costs low. In the S2 stage, we use an improved MBConv block adapted from MobileNetV3, where the original SE attention mechanism is replaced by the CBAM mechanism. This change enables the model to more accurately focus on key areas under conditions such as complex water surface reflections, changing lighting, and deformable obstacles. As a result, the model&#8217;s ability to distinguish different features has been significantly enhanced. For the deeper semantic stages S3 and S4, we employed the BiFormer [ 20 ] module, which features a bidirectional attention mechanism that can effectively capture local dependencies and long-distance context relationships. This enables the model to significantly enhance its ability to identify distant obstacles and large-scale scene layouts, while reducing the impact of background noise. The MSAA [ 21 ] module is located between the encoder and the decoder, and it can collect multi-scale context cues, thereby further improving the segmentation accuracy. The decoder part employs the U-Net architecture, which gradually reconstructs high-resolution feature maps through upsampling and skip connections. It has excellent symmetry and strong feature restoration capabilities, providing robust performance and improving segmentation quality. Especially in complex environments, it can handle tasks that require high-precision outputs. 2.3.2. CoAtNet Network Structure Due to its local perception ability and inductive bias, CNN [ 22 ] has outstanding generalization ability and convergence speed during training and is suitable for small sample scenarios and low-latency tasks. However, the limitation of CNN is that its receptive field is limited, and it is difficult to capture long-distance dependencies. Compared with CNN, Transformer [ 23 ] achieves global modeling through the self-attention mechanism, which is suitable for modeling complex semantic relationships. However, it lacks local inductive priors, has poor training stability, and its computational complexity increases with the square of the input resolution, making it difficult to efficiently process high-resolution images. CoAtNet [ 24 ] is a combined model of Convolution and Attention. It captures local and global information of input data by introducing depthwise separable convolution (MBConv) and relative attention mechanism (Rel-attention). MBConv improves the generalization performance of the model under small sample conditions through the inductive bias of the convolutional network; Rel-attention uses relative position encoding to make up for the shortcomings of CNN and Transformer in processing position information. CoAtNet combines convolution operations with self-attention mechanisms into a basic computational unit and vertically stacks multiple computational units in an organized manner to construct a complete network architecture. In this model, the convolution kernel operates as a fixed filter, making it well-suited for capturing translation-invariant features. In contrast, the self-attention mechanism enriches the model&#8217;s representational capacity and ability to model complex spatial dependencies through the incorporation of relative positional encoding. An effective design is achieved by integrating the convolution layer with an adaptive attention matrix&#8212;combining a static, globally applied convolution kernel with a dynamically updated attention matrix, applied before and after the softmax normalization. This hybrid approach capitalizes on the strengths of both operations. Equations (1) and (2) describe the stages prior to and following the softmax operation, respectively. (1) y i p o s t = &#8721; j &#8712; G e x p x i T x j &#8721; k &#8712; G e x p x i T x k + &#969; i - j x j (2) y i p r e = &#8721; j &#8712; G e x p x i T x j + &#969; i - j &#8721; k &#8712; G e x p x i T x k + &#969; i - k x j where i = ( x i , y i ) , j = ( x j , y j ) , k = x k , y k represent specific coordinate positions in the global spatial space &#160; G , &#969; i - j represents the weight matrix of position i - j , and G represents the global spatial space. After combining the advantages of convolutional layers and self-attention layers, the network constructs a complete architecture by stacking. Due to the complexity of global context calculation, directly applying the relative attention mechanism will result in slow calculation. Therefore, it is necessary to downsample the feature map to reduce the spatial size and then use the global relative attention mechanism to enhance the feature representation. CoAtNet designed four model variants: C-C-C-C, C-C-C-T, C-C-T-T, and C-T-T-T (C represents the convolutional layer and T represents the self-attention layer), and compared them with the VIT [ 25 ] (Vision Transformer) model. The model capability and generalization ability results are shown in Equations (3) and (4). (3) C C T T &#8776; C T T T &gt; V I T &gt; C C C T &gt; C C C C (4) C C C C &#8776; C C C T &#8805; C C T T &gt; C T T T &#8805; V I T It can be seen that simply adding Transformer modules does not necessarily improve model performance, whereas hybrid stacked structures such as C-C-T-T and C-T-T-T can more effectively enhance the model&#8217;s expressive capability. Considering generalization performance, the performance of C-C-T-T is comparable to that of C-C-C-T. Therefore, CoAtNet ultimately adopts the C-C-T-T structure to balance model performance and generalization ability. This architecture effectively enhances the overall performance through the reasonable configuration of convolutional layers and Transformer layers. In the S0 stage, two 3 &#215; 3 convolutional layers are used for preliminary downsampling; in the S1 and S2 stages, convolutional modules are stacked to strengthen local feature learning; in the S3 and S4 stages, Transformer modules are introduced to capture long-range dependencies and global semantic information, and finally classification output is achieved through global average pooling and fully connected layers. In order to deal with the problems of gradient disappearance and explosion, the model introduces residual connections [ 26 ] in each submodule from S1 to S4 to promote gradient propagation and improve the training stability, convergence speed and generalization ability of the network. The architecture diagram of the CoAtNet network is shown in Figure 5 . 2.3.3. Fused-MBConv Module and MBConv Module In CAM-UNet, the encoder&#8217;s S1 stage employs the Fused-MBConv module, while the S2 stage incorporates an enhanced MBConv block, enabling efficient extraction of multi-scale features. This configuration effectively enhances the network&#8217;s ability to extract complex textures and semantic features and significantly improves the model&#8217;s performance in fine-grained target recognition, precise boundary positioning, and adaptation to dynamic environments. The Fused-MBConv module was first proposed in EfficientNetV2 as an improvement to the traditional MBConv architecture. Unlike MBConv, which performs 1 &#215; 1 dimension-increasing convolutions and 3 &#215; 3 depthwise separable convolutions in a stepwise manner, Fused-MBConv combines the dimension-increasing and spatial feature extraction processes into a single standard convolution operation, simplifying the network structure and significantly improving computational efficiency and representation capabilities in the early feature extraction stages. This is shown in Figure 6 . The Fused-MBConv module uses standard convolution at the front end to simultaneously complete channel expansion and spatial feature extraction, then introduces nonlinear activation functions to enhance expression capabilities, and finally achieves dimensionality reduction and restores the number of channels through point-by-point convolution. This structure has significant advantages in shallow feature extraction. It avoids the information loss that may be caused by deep separable convolution in low-level feature extraction and effectively improves the modeling capabilities of texture edges, small-scale targets and complex backgrounds while maintaining low computational overhead. It is particularly suitable for the recognition of high-frequency disturbances and small obstacles in dynamic water surfaces. As the network deepens, the MBConv [ 27 ] module is retained in the S2 stage. It adopts an inverted bottleneck structure consisting of 1 &#215; 1 dimensionality increase convolution, 3 &#215; 3 depth convolution, SE attention mechanism and 1 &#215; 1 dimensionality reduction convolution, as shown in Figure 7 . After the dimensionality increase, the structure performs deep convolution in high-dimensional space, which helps to more fully extract spatial structural features and strengthen the response of key areas through channel attention. Deep convolution significantly reduces parameters and computation by processing channels independently and enhances semantic expression capabilities while maintaining model compactness. The traditional MBConv in MobileNetV3 uses the SE module to strengthen important features through global channel weighting, but it ignores spatial dimension information and has difficulty dealing with problems such as dynamic reflection and complex background interference. Therefore, in order to improve the model&#8217;s expression of important areas in complex water surface environments, this paper improves the attention mechanism in the MBConv module. The SE module is replaced with a CBAM that simultaneously models channel and spatial attention to achieve more refined feature focus and information guidance. CBAM [ 28 ] connects the channel attention and spatial attention mechanisms in series to achieve joint modeling of feature maps in both channel and spatial dimensions, significantly enhancing the model&#8217;s target perception ability in scenes with complex lighting, dynamic backgrounds, and partial occlusions. The channel attention submodule adopts a dual-branch structure of global average pooling and maximum pooling to extract global statistical information to evaluate the importance of each channel; the spatial attention submodule dynamically weights in the spatial dimension through convolution and activation functions to highlight key areas and structural edges. The CBAM architecture is shown in Figure 8 . 2.3.4. BiFormer Module In the encoder&#8217;s high-level semantic extraction stages (S3 and S4), this paper introduces the BiFormer module to enhance the model&#8217;s global modeling and detail representation capabilities in complex water surface scenes. The architecture of the Biformer module is shown in Figure 9 . BiFormer adopts a four-stage pyramid architecture. The initial stage generates primary features via overlapping patch embedding, while the subsequent second, third, and fourth stages progressively downsample the spatial resolution and expand the channel dimensions. Each stage is composed of several consecutive BiFormer blocks. Among the various module structures, depthwise separable convolution is primarily used to encode the spatial relative positional relationships, followed by a BRA (Bi-level Routing Attention) module and an MLP layer. The BRA module applies a two-level routing strategy with dynamic sparse attention to filter out irrelevant key&#8211;value pairs, allowing the network to concentrate on salient regions. This design enhances the efficiency of feature extraction, strengthens multi-scale feature representation, and simultaneously reduces computational costs. The 2D input feature map is divided into distinct, non-overlapping regions., each represented by feature vectors, from which query, key, and value tensors are obtained via separate linear transformations. (5) Q = X r W q , K = X r W k , V = X r W v where W q , W k , W v represents the projection weights of Q , K , V , respectively. Q , K , V represents query, key, and value. X r is the input feature of the attention module. The superscript r indicates that this is the feature tensor after regional division. A feature averaging operation is performed within each non-overlapping region to aggregate local information and form region-level queries and keys. (6) Q r = &#8721; i = 1 n Q i S 2 , &#160; K r = &#8721; i = 1 n K i S 2 where S is the length (height/width) of each non-overlapping area. S 2 represents the total number of tokens within an area. Construct an adjacency matrix representing the relationships between regions. (7) A r = Q r ( K r ) T Only the top k connections of each region are kept to prune the dependency graph. (8) I r = t o p k I n d e x ( A r ) where I r is the index matrix, k represents the maximum number of connections each region can establish with other regions. Considering that each query requires access to the complete routing zone information, to improve memory access efficiency, the key&#8211;value pair tensors for the corresponding zones are pre-assembled. (9) K g = g a t h e r ( K , I r ) , &#160; V g = g a t h e r ( V , I r ) To improve the processing efficiency of the model under high-resolution or large-scale input, a more detailed token-to-token attention mechanism is applied to the pre-extracted key&#8211;value pairs. (10) O = A t t e n t i o n ( Q , K g , V g ) + L C E ( V ) The overall architecture of the BRA module is shown in the Figure 10 below. 2.3.5. MSAA Module The MSAA module consists of three main components: multi-scale fusion, spatial aggregation, and channel aggregation. Its design enhances the spatial awareness and channel representational capacity of feature maps, while keeping the additional computational cost minimal. In the multi-scale fusion module, the input feature map C 1 &#215; H &#215; W is convolved for initial channel compression, producing an intermediate feature., resulting in an intermediate feature map C 2 &#215; H &#215; W . This is then processed in parallel using three convolution kernels of different sizes: 3 &#215; 3 , 5 &#215; 5 , and 7 &#215; 7 to extract feature information within the local and global receptive fields, respectively. These features, when concatenated in the channel dimension, fuse semantic representations from different scales, enhancing the model&#8217;s ability to model complex structures. In the spatial attention aggregation module, the multi-scale fused feature map undergoes global average pooling to capture spatial context. A 7 &#215; 7 convolution then generates a spatial attention map (the size of it is 1 &#215; H &#215; W ) that is normalized using a sigmoid activation function. The spatial attention map is applied element by element to the original feature map, highlighting spatially salient regions, suppressing background interference, and improving spatial discrimination. The channel attention aggregation module first performs global average pooling on the input feature map to obtain a channel description vector which size is C 1 &#215; 1 &#215; 1 . This is then generated through two channel-by-channel 1 &#215; 1 convolutions (the first of which uses a ReLU activation function). This channel attention map is then multiplied channel-by-channel with the spatially weighted feature map to enhance salient channels and suppress redundant channels. The enhanced features are resized using a 1 &#215; 1 convolution and then added to the original input feature map using a residual connection, achieving a fusion of preserved original information and enhanced semantics. The MSAA module is shown in Figure 11 . It enhances spatial position sensitivity and channel selectivity and is capable of addressing the perception challenges faced by convolutional networks in complex backgrounds and multi-scale object scenes. This enables its features to be more semantically rich and spatially aligned. It can provide more precise segmentation boundaries and more reliable small object detection for the decoder stage. 2.3.6. U-Net Decoder The decoder stage employs the U-Net decoder architecture [ 29 ]. In complex aquatic environments, as the encoding path abstracts higher-level semantics, spatial details gradually diminish. Accurately depicting irregular obstacles and detecting small targets are of crucial importance. The symmetrical U-Net structure and its skip connection mechanism can bridge the gap between semantic abstraction and spatial accuracy, effectively reversing the information loss of the encoder. The high-level semantic features of the MSAA module are used as the input for the decoding stage, and the transposed convolutional layer restores the spatial resolution through progressive upsampling. Unlike traditional upsampling methods, the transposed convolution is learnable and can reconstruct the spatial structure more accurately. One of the key features of this design is the skip connection, which combines the feature maps of the encoder with the corresponding decoder layers. This process injects the high-resolution spatial details from the early encoding stage into the decoding process, significantly improving the boundary accuracy. Additionally, it performs multi-scale feature fusion by combining deep, semantically rich features with shallow, spatially precise features, thereby enhancing the accuracy of target detection at different scales. It also supports gradient flow during the training process, which helps overcome the problem of gradient vanishing and ensures effective learning of the entire network. After upsampling, by connecting each layer with the corresponding feature map in the encoder, the key spatial details and edge information are retained, thereby improving the accuracy and stability of the segmentation results. Then, the fused feature maps are refined using convolution and ReLU activation functions, which not only maintain the structural integrity of the decoder but also its interpretability. The symmetric architecture and skip connections of the U-Net decoder effectively utilize multi-scale context, successfully balancing deep semantic understanding with shallow spatial information. The segmentation prediction is both semantically accurate and spatially precise, meeting the high-performance requirements for analyzing complex aquatic environments. 3. Results and Discussion 3.1. Evaluation Metrics This study evaluates the model&#8217;s performance using several metrics, including Mean Intersection over Union (mIoU), F1 score, Params, and FLOPs. A detailed explanation of each metric is provided below. Using the confusion matrix, predictions are classified as positive or negative, with each prediction being either correct or incorrect. This results in four categories for segmentation outcomes: True Positives (TPs), True Negatives (TNs), False Positives (FPs), and False Negatives (FNs). Mean Intersection over Union (mIoU) is a commonly used metric for evaluating semantic segmentation models. It measures the overlap between the predicted segmentation and the ground truth annotations, offering a comprehensive assessment of the model&#8217;s performance across multiple classes. To calculate mIoU, the Intersection over Union (IoU) is computed for each class individually, and the average is then taken. Specifically, IoU is the ratio of the intersection to the union of the predicted region and the corresponding ground truth region. Higher IoU values indicate a closer alignment between the model&#8217;s segmentation and the true labels. The formula is given as follows: (11) I o U = T P F P + F N + T P (12) M I o U = &#8721; i = 1 n I o U i n The F1 score serves as a balanced metric for evaluating binary classification performance by computing the harmonic mean of Precision and Recall. Its formula is presented in Equation (15). The F1 score ranges from 0 to 1, where values closer to 1 signify superior model accuracy. (13) R e c a l l = T P T P + F N (14) P r e c i s i o n = T P T P + F P (15) F 1 = 2 &#215; P r e c i s i o n &#215; R e c a l l P r e c i s i o n + R e c a l l Params refer to the total number of trainable parameters in a model. This metric captures the model&#8217;s size and its spatial complexity. Generally, a higher parameter count indicates a more complex model, which imposes a direct impact on the memory and computational power required for both training and inference. (16) P a r a m s = k h &#215; k w &#215; c i n + 1 &#215; c o u t where k h represents the kernel height, k w represents the kernel width, c i n represents the number of input channels, c o u t represents the number of output channels, and the + 1 accounts for the bias term. FLOPs measure the total number of floating point operations a model performs, indicating its computational cost or time complexity. Since each addition or multiplication counts as one FLOP, a higher FLOP count generally implies a greater demand for computational resources. (17) F L O P s = 2 &#215; k h &#215; k w &#215; c i n &#215; H o u t &#215; W o u t &#215; c o u t where H o u t represents the height of the output feature map, W o u t represents the width of the output feature map, and multiplication by 2 indicates that a convolution operation includes one multiplication and one addition. 3.2. Ablation Experiments To verify the effectiveness of the key modules proposed in this paper in the task of water surface feasible domain and obstacle segmentation, we conducted a series of ablation experiments on a self-constructed dataset. The basic model uses CoAtNet as the backbone network and U-Net as the decoder. IoU1 and IoU2 represent the Intersection over Union (IoU) of the obstacle and feasible domain categories, respectively. The experimental results are shown in Table 2 . The basic model without other structural optimizations, the model achieves mIoU of 89.31%. However, its IoU1 performance in obstacle areas is relatively weak, at only 84.23%, indicating its limited ability to represent boundary structures and small objects. Figure 12 shows that the CoAtNet-Base model performs poorly in segmentation, with issues such as blurred boundaries around obstacles and feasible regions, and errors in obstacle segmentation. In model (a), the Fused-MBConv and CBAM (model b) were introduced. The mIoU increased from 89.31% to 92.37%, and the obstacle IoU1 rose from 84.23% to 88.91%. This indicates that by improving local feature extraction and spatial attention, making it more efficient in capturing texture and edge details, it greatly contributes to enhancing the model performance. The base of model (b) has enhanced the local capabilities. Based on this, we introduced the BiFormer module (model d) at the advanced semantic stage (S3/S4). This version achieved an mIoU of 94.15%, and its performance surpassed that of model (c)&#8212;including local enhancement and the MSAA module (93.1%), while requiring fewer computing resources. This indicates that using BiFormer for global context modeling at the high semantic stage brings more benefits than adding the MSAA module, providing significant advantages in terms of efficiency and performance. When all the modules are integrated into the complete model (e), the mIoU reaches 95.15%. The improvement compared to model (d) confirms that the combination of MSAA with a powerful local&#8211;global representation is very valuable. The comparison of the experimental results can be seen in Figure 13 . 3.3. Comparative Experiments To evaluate the performance of the proposed CAM-UNet model, we compared it with several advanced image segmentation models under the same experimental conditions. These included the classic U-Net [ 29 ], the U-Net variant with the enhanced CoatNet backbone (U-Net (CoAtNet)), DeepLabv3+ [ 30 ], DANet [ 31 ], SegFormer-B1 [ 19 ], and SeaFormer-B [ 32 ], along with the domain-specific model WaSRNet [ 33 ]. U-Net is renowned for its simplicity and powerful boundary restoration capabilities and is the preferred choice for many segmentation tasks. The U-Net (CoatNet) version improves the traditional design by using the powerful CoatNet backbone network. DeepLabv3+ introduces the Atrous Spatial Pyramid Pooling (ASPP) module, enhancing multi-scale context modeling and achieving high accuracy in semantic segmentation. DANet combines channel-level and spatial attention modules within the standard CNN framework, improving the model&#8217;s focus on key areas and enhancing object recognition in complex scenes. SegFormer is a relatively new lightweight Transformer-based model that effectively combines global context modeling with fast decoding processes, providing excellent inference speed without sacrificing accuracy. SegFormer is a Transformer-based model that effectively combines global context modeling with fast decoding processes, providing excellent inference speed. SeaFormer balances performance and computational efficiency through lightweight CNN and attention mechanisms, enhancing feature representation. It is particularly suitable for environments with limited computing resources. WaSRNet is a model for the marine segmentation field. Although it has high computational requirements, it is good at identifying marine objects and is a key benchmark in this field. All models were trained and tested on the self-built WSM USV dataset. The experimental results are shown in Table 3 . U-Net demonstrates the reliability of its simple architecture but also reveals limitations in computational efficiency. U-Net (CoatNet) significantly improves efficiency while maintaining model accuracy by introducing a more advanced backbone network, showcasing the impact of backbone network choice on model performance. DeepLabv3+ improves segmentation accuracy through multi-scale context modeling, but this increases the number of parameters and reduces efficiency. DANet achieves excellent performance using a complex dual-attention mechanism, but this also incurs very high computational costs. In contrast, SegFormer-B1 achieves a better balance between complexity and performance with its innovative hierarchical Transformer design. SeaFormer-B, as a lightweight model, offers both efficiency and performance. WaSRNet performs well in ocean segmentation, but its high computational cost limits its practical application in real-world scenarios. CAM-UNet integrates several key modules: The encoder combines Fused-MBConv and CBAM to enhance shallow feature extraction and improve perception of important areas; the BiFormer module at a higher semantic level strengthens long-range dependency modeling and semantic representation; the MSAA module effectively integrates multi-scale semantics and spatial information; the decoder maintains the skip connections of U-Net style to retain spatial details during upsampling. These designs give CAM-UNet greater adaptability and stability in complex water surface scenarios, and can effectively handle small obstacles, blurred edges, etc. Although the number of parameters in CAM-UNet is greater than that of some lightweight models, in scenarios where maritime navigation safety is of utmost importance, we will prioritize accuracy. Despite the increase in computational complexity, this model has achieved a significant improvement in accuracy, which is of great help in ensuring the perception system of unmanned surface vessels (USVs) operating in challenging marine environments. Figure 14 presents semantic segmentation results for five representative images across different methods. The visual comparisons clearly show that CAM-UNet outperforms other models in various challenging water scenes, accurately detecting small obstacles and fine shoreline details with clear boundaries and well-preserved structures. Having established CAM-UNet&#8217;s effectiveness on the specialized WSM USV dataset, we now assess its generalizability on public benchmarks. Experiments were conducted on the MaSTr1325. The MaSTr1325 dataset is widely used in the field of unmanned surface vessels (USVs) in the ocean. This dataset can meet the obstacle detection requirements of small coastal USVs, and it contains 1325 images captured on-site. These photos were taken during a two-year coastal patrol mission by a USV, and each image includes pixel-level semantic annotations of three categories: sky, ocean, and obstacle. The evaluation uses three IoU metrics: IoU1 (sky), IoU2 (ocean), and IoU3 (obstacle). The experimental results are shown in Table 4 . Our model achieved an mIoU of 98.48% on the dataset, surpassing all other methods, and obtained good IoU in the three categories of sky, ocean, and obstacles. In the crucial &#8220;obstacle&#8221; category, CAM-UNet achieved an IoU of 96.37%, significantly outperforming other models. This result demonstrates its ability to accurately identify obstacles in complex marine environments. Compared with WaSRNet, which is specifically designed for marine applications, CAM-UNet not only achieved higher segmentation accuracy but also maintained strong model efficiency, showcasing excellent overall performance. These results indicate that CAM-UNet not only performs well on the dedicated local dataset but also impressively performs in public benchmark tests. It has been proven to be highly competitive and adaptable, capable of handling diverse marine environments and task requirements. 3.4. Robustness Evaluation To further assess the model&#8217;s robustness, we subjected the test set to Gaussian noise, color jitter, and image blurring, creating three sub-test sets that simulate more complex surface disturbance conditions. We then performed semantic segmentation experiments on these sets using CAM-UNet, SeaFormer-B, and WaSRNet, with mean Intersection over Union (mIoU) as the primary evaluation metric. The experimental results shown in Figure 15 indicate that CAM-UNet outperforms other models in terms of stability under various disturbances. It maintains the highest mIoU across all test conditions, demonstrating particularly strong resilience to Gaussian noise and image blurring. In contrast, CAM-UNet excels in maintaining stability in disturbed environments while also preserving high accuracy on original images, thanks to its effective feature extraction and fusion mechanisms. While WaSRNet performs well on original images, its significant drop in performance under disturbances highlights its sensitivity to changes in image quality. These findings confirm that CAM-UNet&#8217;s architecture offers enhanced reliability in challenging maritime environments, where image quality can often be compromised. 4. Conclusions The proposed CAM-UNet has performed exceptionally well in semantic segmentation tasks in complex water environments. This model is constructed based on the improved CoAtNet backbone network and uses U-Net as the decoder. In the encoder, it integrates Fused-MBConv, CBAM, and BiFormer modules, and uses the MSAA module for multi-scale feature fusion. Experimental results show that CAM-UNet outperforms mainstream models such as DeepLabV3+, SegFormer, and SeaFormer in multiple key evaluation metrics. It excels in boundary restoration and small object detection. However, while the model can be deployed on most modern USV embedded systems, its implementation on resource-constrained platforms may require additional optimization. Future research will focus on optimizing model performance from multiple aspects, including fusing multi-source image information (such as visible light and infrared) to enhance stability in low-light and high-interference environments, expanding the model&#8217;s temporal modeling capabilities to support video-level semantic segmentation tasks, and compressing the model size through lightweight design and efficient feature extraction strategies to improve its deployment efficiency and application value in practical scenarios such as intelligent navigation, water monitoring, and port management. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, Y.W.; methodology, X.G.; validation, J.L.; investigation, J.L.; resources, Y.L.; data curation, X.G. and J.L.; writing&#8212;original draft, X.G.; project administration, Y.W.; funding acquisition, Y.W. All authors have read and agreed to the published version of the manuscript. Data Availability Statement The datasets presented in this article are not readily available because the data are part of an ongoing study. Requests to access the datasets should be directed to nfugxy@njfu.edu.cn. Conflicts of Interest Authors Jie Liu was employed by the company Qingdao Guoshi Intelligent Equipment Technology Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. References 1. Barrera C. Padr&#243;n Armas I. Luis F.S. Llin&#226;s O. Marichal N. Trends and Challenges in Unmanned Surface Vehicles (USV): From Survey to Shipping TransNav Int. J. Mar. Navig. Saf. Sea Transp. 2021 15 135 142 10.12716/1001.15.01.13 2. Dai T. Xiang H. Leng C. Huang S. He G. Han S. LDMNet: Enhancing the Segmentation Capabilities of Unmanned Surface Vehicles in Complex Waterway Scenarios Appl. Sci. 2024 14 7706 10.3390/app14177706 3. Shao M. Liu X. Zhang T. Zhang Q. Sun Y. Yuan H. Xiao C. DMTN-Net: Semantic Segmentation Architecture for Surface Unmanned Vessels Electronics 2024 13 4539 10.3390/electronics13224539 4. Xu H. Yu Y. Zhang X. He J. Cross-Granularity Infrared Image Segmentation Network for Nighttime Marine Observations J. Mar. Sci. Eng. 2024 12 2082 10.3390/jmse12112082 5. Tong Q. Research on Semantic Segmentation and Coastline Extraction of Remote Sensing Images Based on Deep Learning Master&#8217;s Thesis Hainan Normal University Haikou, China 2024 6. Karakus P. Detection of Water Surface Using Canny and Otsu Threshold Methods with Machine Learning Algorithms on Google Earth Engine: A Case Study of Lake Van Appl. Sci. 2025 15 2903 10.3390/app15062903 7. Zheng H. Li X. Wan J. Xu M. Liu S. Yasir M. Automatic coastline extraction based on the improved instantaneous waterline extraction method and correction criteria using SAR imagery Sustainability 2023 15 7199 10.3390/su15097199 8. Peng M. Wang J. Wen X. Cong X. Shoreline detection based on water surface image features in HSV space J. Image Graph. 2018 23 526 533 9. Zheng Y. Xiao Y. Cun C. Xiang K. Zhang H. Liu R. Research on shoreline extraction method based on improved region growing J. Comput. Appl. 2020 37 1876 1881 10.19734/j.issn.1001-3695.2018.12.0917 10. Ling G. Li Y. Zhou W. Liu Y. Xiang J. Design of water area segmentation model and channel pruning acceleration method for unmanned boats Ind. Control Comput. 2022 35 73 75 11. Liang Y. Jafari N. Luo X. Chen Q. Cao Y. Li X. WaterNet: An adaptive matching pipeline for segmenting water with volatile appearance Comput. Vis. Media 2020 6 65 78 10.1007/s41095-020-0156-x 12. Asaro F. Murdaca G. Prati C.M. Learning deep models from weak labels for water surface segmentation in sar images Proceedings of the 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS Brussels, Belgium 11&#8211;16 July 2021 6048 6051 13. Akiyama T. Marcato Junior J. Gon&#231;alves W. Bressan P. Eltner A. Binder F. Singer T. Deep learning applied to water segmentation Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2020 43 1189 1193 10.5194/isprs-archives-XLIII-B2-2020-1189-2020 14. Cui B. Jing W.-P. Huang L. Li Z. Lu Y. SANet: A Sea&#8211;Land Segmentation Network Via Adaptive Multiscale Feature Learning IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021 14 116 126 10.1109/JSTARS.2020.3040176 15. Guo Y. Research on Shoreline Detection in Inland River Scenes Based on Semantic Segmentation Master&#8217;s Thesis Harbin University of Science and Technology Harbin, China 2023 16. Xiong R. Cheng L. Hu T. Wu J. Wang H. Yan X. He Y. Research on feasible region and fast obstacle segmentation algorithm for unmanned surface vehicles J. Electron. Meas. Instrum. 2023 37 11 20 10.13382/j.jemi.B2205804 17. Zhang L. Sun X. Li Z. Kong D. Liu J. Ni P. Boundary Enhancement-Driven Accurate Semantic Segmentation Networks for Unmanned Surface Vessels in Complex Marine Environments IEEE Sens. J. 2024 24 24972 24987 10.1109/JSEN.2024.3409756 18. Liu Z. Lin Y. Cao Y. Hu H. Wei Y. Zhang Z. Lin S. Guo B. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV) Montreal, QC, Canada 10&#8211;17 October 2021 9992 10002 19. Xie E. Wang W. Yu Z. Anandkumar A. &#193;lvarez J.M. Luo P. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers Proceedings of the Neural Information Processing Systems Virtual 6&#8211;14 December 2021 20. Zhu L. Wang X. Ke Z. Zhang W. Lau R.W.H. BiFormer: Vision Transformer with Bi-Level Routing Attention Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Vancouver, BC, Canada 17&#8211;24 June 2023 10323 10333 21. Peng J. Yan J. Shen Y. Liu J. Wei Z. Bai S. Li J. Ma Y. Wang R. 3D medical image registration model based on large kernel convolution and Transformer parallelism Chin. J. Lasers 2025 52 71 81 22. Albawi S. Mohammed T.A. Al-Zawi S. Understanding of a convolutional neural network Proceedings of the 2017 International Conference on Engineering and Technology (ICET) Antalya, Turkey 21&#8211;23 August 2017 1 6 23. Vaswani A. Shazeer N.M. Parmar N. Uszkoreit J. Jones L. Gomez A.N. Kaiser L. Polosukhin I. Attention is All you Need Proceedings of the Neural Information Processing Systems Long Beach, CA, USA 4&#8211;9 December 2017 24. Dai Z. Liu H. Le Q.V. Tan M. CoAtNet: Marrying Convolution and Attention for All Data Sizes arXiv 2021 2106.04803 25. Dosovitskiy A. Beyer L. Kolesnikov A. Weissenborn D. Zhai X. Unterthiner T. Dehghani M. Minderer M. Heigold G. Gelly S. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale arXiv 2020 2010.11929 26. He K. Zhang X. Ren S. Sun J. Deep Residual Learning for Image Recognition Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Las Vegas, NV, USA 27&#8211;30 June 2016 770 778 27. Howard A.G. Sandler M. Chu G. Chen L.-C. Chen B. Tan M. Wang W. Zhu Y. Pang R. Vasudevan V. Searching for MobileNetV3 Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision (ICCV) Seoul, Republic of Korea 27 October&#8211;2 November 2019 1314 1324 28. Woo S. Park J. Lee J.-Y. Kweon I.-S. CBAM: Convolutional Block Attention Module arXiv 2018 10.48550/arXiv.1807.06521 1807.06521 29. Ronneberger O. Fischer P. Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation arXiv 2015 10.48550/arXiv.1505.04597 1505.04597 30. Chen L.-C. Zhu Y. Papandreou G. Schroff F. Adam H. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Proceedings of the European Conference on Computer Vision Munich, Germany 8&#8211;14 September 2018 31. Fu J. Liu J. Tian H. Fang Z. Lu H. Dual Attention Network for Scene Segmentation Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Long Beach, CA, USA 15&#8211;20 June 2019 3141 3149 32. Wan Q. Huang Z. Lu J. Yu G. Zhang L. SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation arXiv 2023 2301.13156 33. Bovcon B. Kristan M. WaSR&#8212;A water segmentation and refinement maritime obstacle detection network IEEE Trans. Cybern. 2021 52 12661 12674 10.1109/TCYB.2021.3085856 34232901 Figure 1 Examples of WSM USV data in the self-built dataset. ( a ) Daytime with clear sky; ( b ) daytime with rain; ( c ) evening with fog; ( d ) nighttime. Figure 2 Example of annotation. Figure 3 Examples of data augmentation. ( a ) Original image; ( b ) horizontal flip; ( c ) brightness adjustment; ( d ) Gaussian noise; ( e ) color jittering; ( f ) image blurring ( g ) contrast adjustment; ( h ) salt and pepper noise; ( i ) vertical flip. Figure 4 Overall framework diagram of CAM-UNet. Figure 5 Architecture diagram of CoAtNet network. Figure 6 The Fused-MBConv module. Figure 7 The Mbconv module. Figure 8 Structure of CBAM. Figure 9 Structure diagram of the Biformer module. Figure 10 Structure of BRA. Figure 11 Structure diagram of MSAA module. Figure 12 Structure diagram of U-Net decoder. Figure 13 Comparison of ablation experiment results. Figure 14 Comparison of experimental results. Figure 15 Robustness experiment results. sensors-25-06963-t001_Table 1 Table 1 Experimental equipment. Software and Hardware Configuration Version Model GPU NVIDIA GeForce RTX 4070 (12 GB VRAM) CPU Intel Core i7-13700KF Framework PyTorch 1.13.0 + Python 3.8 Acceleration CUDA 11.6, cuDNN 8.4 sensors-25-06963-t002_Table 2 Table 2 Results of the ablation experiment. Sequence Number CoAtNet-Base Fused-MBConv &amp; CBAM BiFormer +MSAA FLOPs Params (M) mIoU (%) IoU1 IoU2 (a) &#10003; 32.5 29.8 89.31 84.23 94.39 (b) &#10003; &#10003; 33.2 30.5 92.37 88.91 95.83 (c) &#10003; &#10003; &#10003; 41.5 35.3 93.1 89.5 96.7 (d) &#10003; &#10003; &#10003; 36.8 33.1 94.15 91.44 96.86 (e) &#10003; &#10003; &#10003; &#10003; 42.3 37.8 95.15 92.86 97.44 sensors-25-06963-t003_Table 3 Table 3 Results of comparative experiments on custom dataset. Method Params (M) FLOPs mIoU (%) F1-Score (%) U-Net 31.2 65.2 88.42 88.7 U-Net (CoatNet) 29.8 32.5 89.31 89.6 DeepLabv3+ 41.3 64.3 91.04 91.5 DANet 67.4 86.6 92.13 92.6 SegFormer-B1 13.7 16.8 90.77 91.1 SeaFormer-B 8.2 9.3 93.22 93.8 WaSRNet 70.8 87.2 94.27 94.8 CAM-UNet (Ours) 37.8 42.3 95.15 95.3 sensors-25-06963-t004_Table 4 Table 4 Results of comparative experiments on MaSTr1325. Method mIoU (%) IoU1 (%) IoU2 (%) IoU3 (%) U-Net 97.57 99.21 98.86 94.65 DeepLabV3+ 96.73 99.09 98.36 92.75 SegFormer-B1 97.07 98.55 98.92 93.73 SeaFormer-B 97.90 99.42 99.33 94.95 WaSRNet 98.27 99.54 99.39 95.87 CAM-UNet (Ours) 98.48 99.66 99.42 96.37"
}