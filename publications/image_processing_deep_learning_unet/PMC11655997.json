{
  "pmcid": "PMC11655997",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.825220",
  "metadata": {
    "journal_title": "Cognitive Neurodynamics",
    "journal_nlm_ta": "Cogn Neurodyn",
    "journal_iso_abbrev": "Cogn Neurodyn",
    "journal": "Cognitive Neurodynamics",
    "pmcid": "PMC11655997",
    "pmid": "39712088",
    "doi": "10.1007/s11571-024-10162-5",
    "title": "Set-pMAE: spatial-spEctral-temporal based parallel masked autoEncoder for EEG emotion recognition",
    "year": "2024",
    "month": "8",
    "day": "14",
    "pub_date": {
      "year": "2024",
      "month": "8",
      "day": "14"
    },
    "authors": [
      "Pan Chenyu",
      "Lu Huimin",
      "Lin Chenglin",
      "Zhong Zeyi",
      "Liu Bing"
    ],
    "abstract": "The utilization of Electroencephalography (EEG) for emotion recognition has emerged as the primary tool in the field of affective computing. Traditional supervised learning methods are typically constrained by the availability of labeled data, which can result in weak generalizability of learned features. Additionally, EEG signals are highly correlated with human emotional states across temporal, spatial, and spectral dimensions. In this paper, we propose a Spatial-spEctral-Temporal based parallel Masked Autoencoder (SET-pMAE) model for EEG emotion recognition. SET-pMAE learns generic representations of spatial-temporal features and spatial-spectral features through a dual-branch self-supervised task. The reconstruction task of the spatial-temporal branch aims to capture the spatial-temporal contextual dependencies of EEG signals, while the reconstruction task of the spatial-spectral branch focuses on capturing the intrinsic spatial associations of the spectral domain across different brain regions. By learning from both tasks simultaneously, SET-pMAE can capture the generalized representations of features from the both tasks, thereby reducing the risk of overfitting. In order to verify the effectiveness of the proposed model, a series of experiments are conducted on the DEAP and DREAMER datasets. Results from experiments reveal that by employing self-supervised learning, the proposed model effectively captures more discriminative and generalized features, thereby attaining excellent performance.",
    "keywords": [
      "EEG",
      "Emotion recognition",
      "Self-supervised learning",
      "Transformer"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><!--The publisher of this article does not allow downloading of the full text in XML form.--><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Cogn Neurodyn</journal-id><journal-id journal-id-type=\"iso-abbrev\">Cogn Neurodyn</journal-id><journal-id journal-id-type=\"pmc-domain-id\">538</journal-id><journal-id journal-id-type=\"pmc-domain\">cogneuro</journal-id><journal-title-group><journal-title>Cognitive Neurodynamics</journal-title></journal-title-group><issn pub-type=\"ppub\">1871-4080</issn><issn pub-type=\"epub\">1871-4099</issn><publisher><publisher-name>Springer</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC11655997</article-id><article-id pub-id-type=\"pmcid-ver\">PMC11655997.1</article-id><article-id pub-id-type=\"pmcaid\">11655997</article-id><article-id pub-id-type=\"pmcaiid\">11655997</article-id><article-id pub-id-type=\"pmid\">39712088</article-id><article-id pub-id-type=\"doi\">10.1007/s11571-024-10162-5</article-id><article-id pub-id-type=\"publisher-id\">10162</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Set-pMAE: spatial-spEctral-temporal based parallel masked autoEncoder for EEG emotion recognition</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Pan</surname><given-names initials=\"C\">Chenyu</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">http://orcid.org/0000-0002-3786-2363</contrib-id><name name-style=\"western\"><surname>Lu</surname><given-names initials=\"H\">Huimin</given-names></name><address><email>luhuimin@ccut.edu.cn</email></address><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Lin</surname><given-names initials=\"C\">Chenglin</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zhong</surname><given-names initials=\"Z\">Zeyi</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"B\">Bing</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/052pakb34</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1761 6995</institution-id><institution>School of Computer Science and Engineering, </institution><institution>Changchun University of Technology, </institution></institution-wrap>Changchun, 130102 Jilin People&#8217;s Republic of China </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/052pakb34</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1761 6995</institution-id><institution>Jilin Provincial Smart Health Joint Innovation Laboratory for the New Generation of Al, </institution><institution>Changchun University of Technology, </institution></institution-wrap>Changchun, 130102 Jilin People&#8217;s Republic of China </aff></contrib-group><pub-date pub-type=\"epub\"><day>14</day><month>8</month><year>2024</year></pub-date><pub-date pub-type=\"ppub\"><month>12</month><year>2024</year></pub-date><volume>18</volume><issue>6</issue><issue-id pub-id-type=\"pmc-issue-id\">477357</issue-id><fpage>3757</fpage><lpage>3773</lpage><history><date date-type=\"received\"><day>14</day><month>5</month><year>2024</year></date><date date-type=\"rev-recd\"><day>29</day><month>7</month><year>2024</year></date><date date-type=\"accepted\"><day>7</day><month>8</month><year>2024</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-released-under-embargo\"><date><day>20</day><month>12</month><year>2024</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-01 00:25:13.587\"><day>01</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s), under exclusive licence to Springer Nature B.V. 2024. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</copyright-statement><copyright-year>2024</copyright-year></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"11571_2024_Article_10162.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">The utilization of Electroencephalography (EEG) for emotion recognition has emerged as the primary tool in the field of affective computing. Traditional supervised learning methods are typically constrained by the availability of labeled data, which can result in weak generalizability of learned features. Additionally, EEG signals are highly correlated with human emotional states across temporal, spatial, and spectral dimensions. In this paper, we propose a Spatial-spEctral-Temporal based parallel Masked Autoencoder (SET-pMAE) model for EEG emotion recognition. SET-pMAE learns generic representations of spatial-temporal features and spatial-spectral features through a dual-branch self-supervised task. The reconstruction task of the spatial-temporal branch aims to capture the spatial-temporal contextual dependencies of EEG signals, while the reconstruction task of the spatial-spectral branch focuses on capturing the intrinsic spatial associations of the spectral domain across different brain regions. By learning from both tasks simultaneously, SET-pMAE can capture the generalized representations of features from the both tasks, thereby reducing the risk of overfitting. In order to verify the effectiveness of the proposed model, a series of experiments are conducted on the DEAP and DREAMER datasets. Results from experiments reveal that by employing self-supervised learning, the proposed model effectively captures more discriminative and generalized features, thereby attaining excellent performance.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>EEG</kwd><kwd>Emotion recognition</kwd><kwd>Self-supervised learning</kwd><kwd>Transformer</kwd></kwd-group><funding-group><award-group><funding-source><institution>the Industrial Technology Research and Development Special Project of Jilin Provincial Development and Reform Commission in 2023</institution></funding-source><award-id>2023C042-6</award-id><principal-award-recipient><name name-style=\"western\"><surname>Pan</surname><given-names>Chenyu</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>the Key Project of Science and Technology Research Plan of Jilin Provincial Department of Education in 2023</institution></funding-source><award-id>JJKH20230763KJ</award-id><principal-award-recipient><name name-style=\"western\"><surname>Lu</surname><given-names>Huimin</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>the Project of Science and Technology Research Plan of Jilin Provincial Department of Education in 2023</institution></funding-source><award-id>JJKH20230765KJ</award-id><principal-award-recipient><name name-style=\"western\"><surname>Lin</surname><given-names>Chenglin</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature B.V. 2024</meta-value></custom-meta></custom-meta-group></article-meta></front></article></pmc-articleset>",
  "text": "Cogn Neurodyn Cogn Neurodyn 538 cogneuro Cognitive Neurodynamics 1871-4080 1871-4099 Springer PMC11655997 PMC11655997.1 11655997 11655997 39712088 10.1007/s11571-024-10162-5 10162 1 Research Article Set-pMAE: spatial-spEctral-temporal based parallel masked autoEncoder for EEG emotion recognition Pan Chenyu 1 2 http://orcid.org/0000-0002-3786-2363 Lu Huimin luhuimin@ccut.edu.cn 1 2 Lin Chenglin 1 2 Zhong Zeyi 1 2 Liu Bing 1 1 https://ror.org/052pakb34 0000 0004 1761 6995 School of Computer Science and Engineering, Changchun University of Technology, Changchun, 130102 Jilin People&#8217;s Republic of China 2 https://ror.org/052pakb34 0000 0004 1761 6995 Jilin Provincial Smart Health Joint Innovation Laboratory for the New Generation of Al, Changchun University of Technology, Changchun, 130102 Jilin People&#8217;s Republic of China 14 8 2024 12 2024 18 6 477357 3757 3773 14 5 2024 29 7 2024 7 8 2024 01 12 2025 01 12 2025 20 12 2024 01 12 2025 &#169; The Author(s), under exclusive licence to Springer Nature B.V. 2024. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. 2024 The utilization of Electroencephalography (EEG) for emotion recognition has emerged as the primary tool in the field of affective computing. Traditional supervised learning methods are typically constrained by the availability of labeled data, which can result in weak generalizability of learned features. Additionally, EEG signals are highly correlated with human emotional states across temporal, spatial, and spectral dimensions. In this paper, we propose a Spatial-spEctral-Temporal based parallel Masked Autoencoder (SET-pMAE) model for EEG emotion recognition. SET-pMAE learns generic representations of spatial-temporal features and spatial-spectral features through a dual-branch self-supervised task. The reconstruction task of the spatial-temporal branch aims to capture the spatial-temporal contextual dependencies of EEG signals, while the reconstruction task of the spatial-spectral branch focuses on capturing the intrinsic spatial associations of the spectral domain across different brain regions. By learning from both tasks simultaneously, SET-pMAE can capture the generalized representations of features from the both tasks, thereby reducing the risk of overfitting. In order to verify the effectiveness of the proposed model, a series of experiments are conducted on the DEAP and DREAMER datasets. Results from experiments reveal that by employing self-supervised learning, the proposed model effectively captures more discriminative and generalized features, thereby attaining excellent performance. Keywords EEG Emotion recognition Self-supervised learning Transformer the Industrial Technology Research and Development Special Project of Jilin Provincial Development and Reform Commission in 2023 2023C042-6 Pan Chenyu the Key Project of Science and Technology Research Plan of Jilin Provincial Department of Education in 2023 JJKH20230763KJ Lu Huimin the Project of Science and Technology Research Plan of Jilin Provincial Department of Education in 2023 JJKH20230765KJ Lin Chenglin pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access no pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature B.V. 2024"
}