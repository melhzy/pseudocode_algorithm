{
  "pmcid": "PMC12656116",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:31.652711",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656116",
    "pmid": "41305245",
    "doi": "10.3390/s25227039",
    "title": "LDLK-U-Mamba: An Efficient and Highly Accurate Method for 3D Rock Pore Segmentation",
    "year": "2025",
    "month": "11",
    "day": "18",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "18"
    },
    "authors": [
      "Chen Guojun",
      "Li Huihui",
      "Liu Chang",
      "Li Pengxia",
      "Kong Yunyi"
    ],
    "abstract": "Three-dimensional rock pore segmentation is crucial in fields such as geology and petroleum exploration, holding significant importance for oil and gas resource exploration and development. However, existing segmentation methods still present two main limitations: (1) they fail to capture the spatial relationships of pores in 3D when directly applied to 3D rock pore segmentation, inevitably leading to inaccurate segmentation results; (2) they struggle to apply efficiently in resource-constrained scenarios due to the high computational complexity and costly computational demands. To solve the above issues, we propose a novel and lightweight method based on the Mamba architecture, termed LDLK-U-Mamba, for precise and efficient 3D rock pore segmentation. Specifically, we design a Lightweight Dynamic Large Kernel (LDLK) module to capture global contextual information and develop an InceptionDSConv3d module for multi-scale feature fusion and refinement, further yielding more accurate segmentation results. In addition, the Basic Residual Depthwise Separable Block (BasicResDWSBlock) module is proposed to utilize depthwise separable convolutions and the Squeeze-and-Excitation (SE) module to reduce model parameters and computational complexity. Extensive qualitative and quantitative experiments demonstrate that our LDLK-U-Mamba outperforms current mainstream segmentation approaches, validating its effectiveness for rock pore segmentationâ€”particularly in capturing the 3D spatial relationships of pores.",
    "keywords": [
      "3D rock pore segmentation",
      "Mamba",
      "lightweight"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656116</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656116.1</article-id><article-id pub-id-type=\"pmcaid\">12656116</article-id><article-id pub-id-type=\"pmcaiid\">12656116</article-id><article-id pub-id-type=\"pmid\">41305245</article-id><article-id pub-id-type=\"doi\">10.3390/s25227039</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-07039</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>LDLK-U-Mamba: An Efficient and Highly Accurate Method for 3D Rock Pore Segmentation</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names initials=\"G\">Guojun</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><xref rid=\"af1-sensors-25-07039\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07039\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0009-0000-0875-2125</contrib-id><name name-style=\"western\"><surname>Li</surname><given-names initials=\"H\">Huihui</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><xref rid=\"af1-sensors-25-07039\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07039\" ref-type=\"aff\">2</xref><xref rid=\"c1-sensors-25-07039\" ref-type=\"corresp\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"C\">Chang</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><xref rid=\"af1-sensors-25-07039\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07039\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"P\">Pengxia</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><xref rid=\"af1-sensors-25-07039\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07039\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Kong</surname><given-names initials=\"Y\">Yunyi</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><xref rid=\"af1-sensors-25-07039\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-07039\" ref-type=\"aff\">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Doulamis</surname><given-names initials=\"A\">Anastasios</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-07039\"><label>1</label>Qingdao Institute of Software, College of Computer Science and Technology, China University of Petroleum (East China), Qingdao 266580, China; <email>chengj@upc.edu.cn</email> (G.C.); <email>z23070142@s.upc.edu.cn</email> (C.L.); <email>s23070018@s.upc.edu.cn</email> (P.L.); <email>s24070043@s.upc.edu.cn</email> (Y.K.)</aff><aff id=\"af2-sensors-25-07039\"><label>2</label>Shandong Key Laboratory of Intelligent Oil &amp; Gas Industrial Software, Qingdao 266580, China</aff><author-notes><corresp id=\"c1-sensors-25-07039\"><label>*</label>Correspondence: <email>s23070002@s.upc.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>18</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>7039</elocation-id><history><date date-type=\"received\"><day>11</day><month>9</month><year>2025</year></date><date date-type=\"rev-recd\"><day>08</day><month>11</month><year>2025</year></date><date date-type=\"accepted\"><day>14</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>18</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-07039.pdf\"/><abstract><p>Three-dimensional rock pore segmentation is crucial in fields such as geology and petroleum exploration, holding significant importance for oil and gas resource exploration and development. However, existing segmentation methods still present two main limitations: (1) they fail to capture the spatial relationships of pores in 3D when directly applied to 3D rock pore segmentation, inevitably leading to inaccurate segmentation results; (2) they struggle to apply efficiently in resource-constrained scenarios due to the high computational complexity and costly computational demands. To solve the above issues, we propose a novel and lightweight method based on the Mamba architecture, termed LDLK-U-Mamba, for precise and efficient 3D rock pore segmentation. Specifically, we design a Lightweight Dynamic Large Kernel (LDLK) module to capture global contextual information and develop an InceptionDSConv3d module for multi-scale feature fusion and refinement, further yielding more accurate segmentation results. In addition, the Basic Residual Depthwise Separable Block (BasicResDWSBlock) module is proposed to utilize depthwise separable convolutions and the Squeeze-and-Excitation (SE) module to reduce model parameters and computational complexity. Extensive qualitative and quantitative experiments demonstrate that our LDLK-U-Mamba outperforms current mainstream segmentation approaches, validating its effectiveness for rock pore segmentation&#8212;particularly in capturing the 3D spatial relationships of pores.</p></abstract><kwd-group><kwd>3D rock pore segmentation</kwd><kwd>Mamba</kwd><kwd>lightweight</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-07039\"><title>1. Introduction</title><p>In geological science and geology research, the analysis of rock structures and dimensional structures is crucial for understanding the evolutionary processes of underground oil and gas reservoirs [<xref rid=\"B1-sensors-25-07039\" ref-type=\"bibr\">1</xref>]. In particular, the pore structures within rock images play a vital role in determining water resource planning, geothermal energy utilization and underground resource exploration [<xref rid=\"B2-sensors-25-07039\" ref-type=\"bibr\">2</xref>]. However, due to the inherent complexity of rock structures and the inability to clearly separate pore boundaries from the rock matrix, segmenting pore regions within rock images presents a formidable challenge.</p><p>In the task of segmenting two-dimensional images, such as in the fields of geology [<xref rid=\"B3-sensors-25-07039\" ref-type=\"bibr\">3</xref>], remote sensing [<xref rid=\"B4-sensors-25-07039\" ref-type=\"bibr\">4</xref>], and medicine [<xref rid=\"B5-sensors-25-07039\" ref-type=\"bibr\">5</xref>], the most commonly used deep learning architecture is U-Net [<xref rid=\"B6-sensors-25-07039\" ref-type=\"bibr\">6</xref>]. Beyond U-Net, existing research has proposed numerous improved network architectures, such as Attention U-Net [<xref rid=\"B7-sensors-25-07039\" ref-type=\"bibr\">7</xref>], U-ResNet [<xref rid=\"B8-sensors-25-07039\" ref-type=\"bibr\">8</xref>], Dense U-Net [<xref rid=\"B9-sensors-25-07039\" ref-type=\"bibr\">9</xref>], SFMRNet [<xref rid=\"B10-sensors-25-07039\" ref-type=\"bibr\">10</xref>] and TransUNet [<xref rid=\"B11-sensors-25-07039\" ref-type=\"bibr\">11</xref>]. However, these 2D segmentation methods rely solely on feature extraction from a single planar image, resulting in the loss of depth information and spatial correlations within rock pores. This leads to discrepancies between segmentation results and actual pore morphology. To address these limitations, 3D segmentation methods have gradually emerged as a key research focus.</p><p>In recent years, 3D U-Net [<xref rid=\"B2-sensors-25-07039\" ref-type=\"bibr\">2</xref>] and 3D U-ResNet [<xref rid=\"B8-sensors-25-07039\" ref-type=\"bibr\">8</xref>] have been applied to mineral segmentation in 3D rock images. However, they struggle to capture edge textures, resulting in low segmentation accuracy. Given the limited number of existing networks for 3D rock image segmentation, while popular networks in 3D medical image segmentation&#8212;such as 3D SegNet [<xref rid=\"B12-sensors-25-07039\" ref-type=\"bibr\">12</xref>], Kiu-net [<xref rid=\"B13-sensors-25-07039\" ref-type=\"bibr\">13</xref>], nnUNet [<xref rid=\"B14-sensors-25-07039\" ref-type=\"bibr\">14</xref>], V-Net [<xref rid=\"B15-sensors-25-07039\" ref-type=\"bibr\">15</xref>], UNETR [<xref rid=\"B16-sensors-25-07039\" ref-type=\"bibr\">16</xref>] and U-Mamba [<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>]&#8212;demonstrate remarkable performance in medical image segmentation, but they struggle to segment elongated pores effectively when applied to rock segmentation.</p><p>In our study, we propose a segmentation-based LDLK-U-Mamba model capable of segmenting pores in 3D rock images efficiently and accurately. Specifically, we propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed LDLK-U-Mamba, for segmenting pores in 3D rock images. We propose using an LDLK module during the encoding phase to capture global contextual information and utilizing the InceptionDSConv3d module during the decoding phase for multi-scale feature fusion and refinement, achieving more accurate segmentation results. Additionally, we propose the BasicResDWSBlock module, utilizing deep separable convolutions and the SE module to reduce model parameters and computational complexity. We conduct several comparative experiments for rock pore segmentation to illustrate performance differences among algorithms. Experimental results on the Leopard Sandstone Images dataset demonstrate that our model achieves <inline-formula><mml:math id=\"mm1\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>99.38</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> accuracy, <inline-formula><mml:math id=\"mm2\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>99.62</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Dice, and <inline-formula><mml:math id=\"mm3\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>99.25</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> IOU, while reducing parameters from 42.12 M to 13.97 M, and computational FLOPs decreased from 973.91 GFLOPs to 426.64 GFLOPs. Compared with other 3D segmentation networks, our proposed LDLK-U-Mamba demonstrates outstanding comprehensive performance.</p><p>Overall, the contributions of our research can be summarized as follows:\n<list list-type=\"simple\"><list-item><label>(1)</label><p>We propose a 3D rock pore segmentation model, termed LDLK-U-Mamba, which is based on the mamba network for precise and efficient 3D rock pore segmentation.</p></list-item><list-item><label>(2)</label><p>We propose a Lightweight Dynamic Large Kernel (LDLK) module to capture global contextual information and design an InceptionDSConv3d module to fuse and refine multi-scale features, thereby achieving more accurate segmentation results.</p></list-item><list-item><label>(3)</label><p>We propose a Basic Residual Depthwise Separable Block (BasicResDWSBlock) module, which employs separable convolutions and the Squeeze-and-Excitation (SE) module to reduce model parameters and computational complexity.</p></list-item><list-item><label>(4)</label><p>The comparative experiments demonstrate LDLK-U-Mamba outperforms the existing 3D segmentation networks.</p></list-item></list></p></sec><sec id=\"sec2-sensors-25-07039\"><title>2. Related&#160;Work</title><sec id=\"sec2dot1-sensors-25-07039\"><title>2.1. 2D Image&#160;Segmentation</title><p>In 2D images, accurately segmenting digital rocks into categories such as pores and rocks remains a formidable challenge [<xref rid=\"B18-sensors-25-07039\" ref-type=\"bibr\">18</xref>]. Traditional image segmentation techniques, including thresholding (such as the maximum interclass variance method [<xref rid=\"B19-sensors-25-07039\" ref-type=\"bibr\">19</xref>]), watershed segmentation [<xref rid=\"B20-sensors-25-07039\" ref-type=\"bibr\">20</xref>], clustering analysis, and edge detection [<xref rid=\"B21-sensors-25-07039\" ref-type=\"bibr\">21</xref>], rely heavily on parameter selection influenced by user experience and subjective judgment, which lead to inconsistent segmentation results [<xref rid=\"B22-sensors-25-07039\" ref-type=\"bibr\">22</xref>]. Among deep learning architectures, U-Net [<xref rid=\"B6-sensors-25-07039\" ref-type=\"bibr\">6</xref>] fuses multi-scale information, yet its skip connections naively concatenate features across scales without accounting for 3D structural coherence such as voxel spatial relationships and pore dimensional connectivity, thus struggling to capture fine 3D spatial details like intricate rock pore structures. Attention U-Net [<xref rid=\"B7-sensors-25-07039\" ref-type=\"bibr\">7</xref>] introduces an attention mechanism in the decoder, enabling the network to focus on more important feature regions. However, it lacks the targeted modeling of geometric features, such as spatial orientation and hierarchy in 3D space, with hierarchy referring to the micro-to-macro progression of pore-related features (throats, pore clusters, and pore zones) that are interconnected in the 3D space. U-ResNet [<xref rid=\"B8-sensors-25-07039\" ref-type=\"bibr\">8</xref>] introduces the residual structure from ResNet, mitigating the gradient vanishing problem in deep networks through shortcut connections. Nevertheless, it struggles to accurately distinguish subtle structural differences within the 3D space. Dense U-Net [<xref rid=\"B9-sensors-25-07039\" ref-type=\"bibr\">9</xref>] enhances feature reuse efficiency, but its dense connections cause feature map counts to increase exponentially with network depth and fail to effectively capture cross-dimensional feature correlations in the 3D space. SFMRNet [<xref rid=\"B10-sensors-25-07039\" ref-type=\"bibr\">10</xref>] excels in segmentation tasks for ambiguous regions in remote sensing images through its multi-feature correlation module, but it is limited to two-dimensional data processing. TransUNet [<xref rid=\"B11-sensors-25-07039\" ref-type=\"bibr\">11</xref>] combines the global modeling advantages of Transformers with UNet&#8217;s capability for fine-grained detail recovery, demonstrating exceptional performance in capturing long-range dependencies. However, its reliance on two-dimensional slice processing prevents it from leveraging the spatial correlation information inherent in three-dimensional data. Zunair et al. [<xref rid=\"B23-sensors-25-07039\" ref-type=\"bibr\">23</xref>] proposed Masked Supervised Learning (MaskSup) for semantic segmentation, a single-stage method that uses a Siamese network and random masking to model both short- and long-range context, addressing small target segmentation, ambiguous boundaries, and class imbalance. Like the other 2D methods above, it fails to capture fine 3D pore details. In contrast, within our proposed LDLK-U-Mamba model, the LDLK module and InceptionDSConv3d module effectively address the limitations of 2D segmentation methods in capturing 3D spatial features.</p></sec><sec id=\"sec2dot2-sensors-25-07039\"><title>2.2. 3D Image&#160;Segmentation</title><p>In recent years, the advent of high-resolution micro-computed tomography (&#181;CT) imaging of rock samples [<xref rid=\"B8-sensors-25-07039\" ref-type=\"bibr\">8</xref>] has enabled the acquisition of 3D image data, paving the way for refined segmentation of rock images. In mineral segmentation within 3D rock images, while 3D U-Net [<xref rid=\"B6-sensors-25-07039\" ref-type=\"bibr\">6</xref>] can capture richer contextual information spatially, it struggles to adequately capture edge and texture details, resulting in lower segmentation accuracy. Furthermore, while 3D U-ResNet [<xref rid=\"B24-sensors-25-07039\" ref-type=\"bibr\">24</xref>] demonstrates improved robustness in handling complex 3D structures through residual connections, it fails to resolve the boundary distinction between pores and matrix in rock samples. In domains like medical image segmentation, 3D SegNet [<xref rid=\"B12-sensors-25-07039\" ref-type=\"bibr\">12</xref>] employs a symmetric encoder&#8211;decoder architecture to optimize information recovery during upscaling. However, this approach significantly increases computational demands when processing ultra-large volumetric data. Three-dimensional KiUNet [<xref rid=\"B13-sensors-25-07039\" ref-type=\"bibr\">13</xref>] proposes block-based segmentation methods, but the loss of spatial correlation between blocks often leads to edge discontinuities after stitching. Meanwhile, nnUNet [<xref rid=\"B14-sensors-25-07039\" ref-type=\"bibr\">14</xref>], as a highly adaptive framework, requires multi-round hyperparameter optimization validation, inevitably extending training cycles. V-Net [<xref rid=\"B15-sensors-25-07039\" ref-type=\"bibr\">15</xref>] excels at capturing three-dimensional spatial features in 3D medical images with high segmentation accuracy, but it has numerous model parameters and demands substantial computational resources. UNETR [<xref rid=\"B16-sensors-25-07039\" ref-type=\"bibr\">16</xref>] combines Transformer with 3D U-Net, offering strong global modeling capabilities, yet it suffers from insufficient small-object segmentation accuracy and requires large amounts of data for training. Although the U-Mamba [<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>] network performs well on large-scale data due to its multi-scale feature extraction, the computational complexity of 3D convolutions still results in lengthy processing times when handling 3D data. Furthermore, when dealing with extremely fine pores in rocks, multi-scale feature extraction struggles to fully capture the details of microscopic structures, leaving room for improvement in segmentation accuracy. To date, all existing 2D or 3D rock image segmentation networks have been developed for studying rock minerals. Our work represents the first research focused on pore segmentation in 3D rock images, which is performed precisely.</p></sec><sec id=\"sec2dot3-sensors-25-07039\"><title>2.3. Neural Network&#160;Lightweighting</title><p>In deep learning image segmentation and feature extraction tasks, enhancing network performance under limited computational resources and memory constraints has become a significant research focus. To address this challenge, Szegedy et al. [<xref rid=\"B25-sensors-25-07039\" ref-type=\"bibr\">25</xref>] introduced the Inception module, the first network to efficiently model image features through the parallel computation of multi-scale features. Subsequently, Szegedy et al. [<xref rid=\"B26-sensors-25-07039\" ref-type=\"bibr\">26</xref>] further proposed Inception-v2 and Inception-v3, introducing factorized convolutions and batch normalization to further reduce memory consumption and computational costs. To minimize computational resource requirements, Chollet et al. [<xref rid=\"B27-sensors-25-07039\" ref-type=\"bibr\">27</xref>] proposed the Xception network, which further optimized the computation of convolutional operations through deep separable convolutions. Inspired by Xception, MobileNet [<xref rid=\"B28-sensors-25-07039\" ref-type=\"bibr\">28</xref>] introduced a lightweight network based on deep separable convolutions, enabling efficient image classification and segmentation on mobile and embedded devices. Building upon this foundation, MobileNetV2 [<xref rid=\"B29-sensors-25-07039\" ref-type=\"bibr\">29</xref>] further introduced reverse residual structures and linear bottlenecks to reduce memory requirements and computational costs. Zhao et al. [<xref rid=\"B30-sensors-25-07039\" ref-type=\"bibr\">30</xref>] also proposed a lightweight 3D segmentation network based on Inception and deep separable convolutions. This approach achieved excellent segmentation results on multiple 3D medical image datasets while maintaining low computational costs, offering new insights for pore segmentation in 3D rock images.</p></sec></sec><sec sec-type=\"methods\" id=\"sec3-sensors-25-07039\"><title>3. Methods</title><sec id=\"sec3dot1-sensors-25-07039\"><title>3.1. Overview</title><p>We propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed LDLK-U-Mamba, for segmenting pores in 3D rock images. It aims to enhance segmentation performance while reducing the number of parameters and FLOPs compared to Mamba. The structure of our LDLK-U-Mamba model is shown in <xref rid=\"sensors-25-07039-f001\" ref-type=\"fig\">Figure 1</xref>. The model includes an encoder, a decoder, and skip connections. The LDLK module in the encoder captures global contextual information, while the InceptionDSConv3d module in the decoder achieves more accurate segmentation results through multi-scale feature fusion and refinement. The BasicResDWSBlock module employs separable convolutions and the SE module to significantly reduce the model&#8217;s parameter count and computational complexity.</p></sec><sec id=\"sec3dot2-sensors-25-07039\"><title>3.2. The LDLK&#160;Module</title><p>Molina et al. [<xref rid=\"B3-sensors-25-07039\" ref-type=\"bibr\">3</xref>] achieved a refined analysis of geological segmentation through multi-source geoscience data in their study of Chile&#8217;s giant thrust belt. Similarly, in the field of rock pore segmentation, the integrated extraction of multidimensional features such as pore morphology and connectivity has become a key direction for enhancing segmentation accuracy. In existing research, Transformer models have achieved remarkable success in image segmentation due to their extensive receptive fields and ability to effectively utilize long-range contextual information across the entire image [<xref rid=\"B31-sensors-25-07039\" ref-type=\"bibr\">31</xref>]. Convolutional neural networks (CNNs) can also achieve large receptive fields by employing large-sized convolutional kernels, enabling them to attain competitive performance with fewer model parameters. However, in 3D rock image segmentation, fixed-size convolutional kernels lack the flexibility to handle pores and fractures at different scales. CNNs using large convolutional kernels remain constrained in capturing multi-scale features of pore structures with significantly varying shapes and sizes. This limitation may cause models to underperform when handling complex and variable rock microstructures, particularly when precise identification of pore features of varying sizes and morphologies is required. Additionally, they fail to efficiently utilize global contextual information. To address these limitations, we propose the LDLK module with dynamic feature fusion capabilities [<xref rid=\"B32-sensors-25-07039\" ref-type=\"bibr\">32</xref>]. The structure of the LDLK module is shown in <xref rid=\"sensors-25-07039-f002\" ref-type=\"fig\">Figure 2</xref>. To maintain low computational and parameter complexity, our LDLK utilizes convolutional kernel sizes (3 &#215; 3 &#215; 3 and 5 &#215; 5 &#215; 5) that are more suitable for our dataset, combined with a reasonable dilation factor (dilation = 2).</p><p>As indicated by Equation (<xref rid=\"FD1-sensors-25-07039\" ref-type=\"disp-formula\">1</xref>), let <inline-formula><mml:math id=\"mm4\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> be the input feature map, where <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">X</mml:mi></mml:mrow></mml:math></inline-formula> is the z-dimension (slice) of the sample object, <italic toggle=\"yes\">l</italic> is the number of layers the feature map is located in, <italic toggle=\"yes\">c</italic> is the channel dimension, and <italic toggle=\"yes\">h</italic> is the spatial dimension information. To create the attention weights, we utilize grouped convolution (groups = 2) in conjunction with a tiny convolution kernel (3 &#215; 3 &#215; 3), following Zhang et al. [<xref rid=\"B33-sensors-25-07039\" ref-type=\"bibr\">33</xref>]&#8217;s design of dynamic grouped convolution.<disp-formula id=\"FD1-sensors-25-07039\"><label>(1)</label><mml:math id=\"mm6\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold-italic\">w</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Sigmoid</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>GConv</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>AVGPool</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Second, as shown in Equation (<xref rid=\"FD2-sensors-25-07039\" ref-type=\"disp-formula\">2</xref>), we refer to the channel compression method proposed by Han et al. in [<xref rid=\"B34-sensors-25-07039\" ref-type=\"bibr\">34</xref>] and propose to use channel dimensionality reduction. Let the number of channels of the input feature map be <inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and reduce the number of channels to 1/32 of the original one by 1 &#215; 1 &#215; 1 convolution, then the number of channels after dimensionality reduction <inline-formula><mml:math id=\"mm8\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators=\"\" open=\"&#x230A;\" close=\"&#x230B;\"><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mn>32</mml:mn></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>,<disp-formula id=\"FD2-sensors-25-07039\"><label>(2)</label><mml:math id=\"mm9\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id=\"mm11\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and the previously mentioned <inline-formula><mml:math id=\"mm12\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent a reasonable distinction in symbols to accommodate different operational requirements) is the input feature map, <inline-formula><mml:math id=\"mm13\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>Conv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the 1 &#215; 1 &#215; 1 convolution operation, and <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>u</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the feature map after dimensionality reduction. The original number of channels is then restored after activation and spatial gating unit processing, and this bottleneck structure significantly reduces the intermediate computational cost.</p><p>The above process can be expressed as Equation (<xref rid=\"FD3-sensors-25-07039\" ref-type=\"disp-formula\">3</xref>),<disp-formula id=\"FD3-sensors-25-07039\"><label>(3)</label><mml:math id=\"mm15\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>LDLK</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In order to increase operating efficiency, we decide to utilize the lightweight ReLU function for the activation function. Overall, our design focuses more on efficient computation on the basis of guaranteeing the segmentation effect, which is suitable for the case of limited computational resources.</p></sec><sec id=\"sec3dot3-sensors-25-07039\"><title>3.3. The InceptionDSConv3d&#160;Module</title><p>The task of the decoding stage is to progressively restore segmented spatial details, necessitating multidirectional and multi-scale reconstruction of the high-dimensional features extracted by the encoder. To achieve fusion and refinement of the multi-scale features, we combine the Inception module with deep separable convolutions in the decoder, proposing the InceptionDSConv3d module. The Inception module was first introduced by Szegedy et al. [<xref rid=\"B25-sensors-25-07039\" ref-type=\"bibr\">25</xref>] to capture features at different scales through parallel operations of multi-scale convolutional kernels. Subsequently, Chollet et al. [<xref rid=\"B27-sensors-25-07039\" ref-type=\"bibr\">27</xref>] introduced Deep Separable Convolution in 2017. This method decomposes standard convolutions into depthwise convolutions and pointwise convolutions, thereby reducing computational complexity and enhancing model performance. Therefore, we propose combining the multi-scale feature extraction of the Inception module with the efficient computation of Deep Separable Convolution to achieve the fusion and refinement of multi-scale features [<xref rid=\"B35-sensors-25-07039\" ref-type=\"bibr\">35</xref>].</p><p><xref rid=\"sensors-25-07039-f003\" ref-type=\"fig\">Figure 3</xref> illustrates a multi-branch convolutional module architecture designed to enhance the network&#8217;s feature extraction capabilities through Deep Separable Convolution (DSConv) and feature fusion operations. For the input, it is first divided into five groups along the channel dimension:<disp-formula id=\"FD4-sensors-25-07039\"><label>(4)</label><mml:math id=\"mm16\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>id</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Split</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mn>2</mml:mn><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mn>4</mml:mn><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mn>4</mml:mn><mml:mi>g</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm17\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Split</mml:mi><mml:mo>(</mml:mo><mml:mo>&#183;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes grouping and splitting along the channel dimension, <italic toggle=\"yes\">g</italic> represents the number of channels in each group, <inline-formula><mml:math id=\"mm18\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates taking the first <italic toggle=\"yes\">g</italic> channels of <inline-formula><mml:math id=\"mm19\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">X</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:mn>2</mml:mn><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates taking the second <italic toggle=\"yes\">g</italic> channels of <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">X</mml:mi></mml:mrow></mml:math></inline-formula>, and so on. Then, these five features are processed through five distinct operators:<disp-formula id=\"FD5-sensors-25-07039\"><label>(5)</label><mml:math id=\"mm22\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>DSConv</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD6-sensors-25-07039\"><label>(6)</label><mml:math id=\"mm23\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>2</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>DSConv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD7-sensors-25-07039\"><label>(7)</label><mml:math id=\"mm24\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>3</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>DSConv</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>11</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD8-sensors-25-07039\"><label>(8)</label><mml:math id=\"mm25\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>4</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>DSConv</mml:mi><mml:mrow><mml:mn>11</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD9-sensors-25-07039\"><label>(9)</label><mml:math id=\"mm26\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mtable displaystyle=\"true\"><mml:mtr><mml:mtd columnalign=\"right\"><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>id</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup></mml:mtd><mml:mtd columnalign=\"left\"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>id</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>\nwhere <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi>DSConv</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#8594;</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes a separable convolution with depth, where both input and output channels are of size <italic toggle=\"yes\">g</italic> and the kernel size is <inline-formula><mml:math id=\"mm28\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"mm29\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>id</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mi>id</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the &#8220;identity mapping&#8221;. Finally, the outputs from each branch are concatenated:<disp-formula id=\"FD10-sensors-25-07039\"><label>(10)</label><mml:math id=\"mm30\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant=\"bold-italic\">X</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo form=\"prefix\">Concat</mml:mo><mml:mrow><mml:mspace width=\"-3pt\"/><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>1</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width=\"4pt\"/><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>2</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width=\"4pt\"/><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>3</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width=\"4pt\"/><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mn>4</mml:mn><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width=\"4pt\"/><mml:msubsup><mml:mi mathvariant=\"bold-italic\">X</mml:mi><mml:mrow><mml:mi>id</mml:mi></mml:mrow><mml:mo>&#8242;</mml:mo></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Our proposed InceptionDSConv3d module utilizes a multi-branch architecture combined with separable convolutional kernels of varying sizes to enable fine-grained feature processing across multiple dimensions. Simultaneously, deep separable convolutions substantially reduce parameter count and computational complexity, making the decoder more efficient while maintaining performance. This design is particularly well suited for restoring minute pores and complex boundaries in rock images. By capturing multi-scale contextual information and fusing features from different directions, it significantly enhances the decoder&#8217;s ability to reconstruct spatial details.</p></sec><sec id=\"sec3dot4-sensors-25-07039\"><title>3.4. The BasicResDWSBlock&#160;Module</title><p>Building upon the above work, to significantly reduce the number of parameters and FLOPs in the model, we propose the BasicResDWSBlock module by drawing inspiration from the Deep Separable Convolution approach [<xref rid=\"B27-sensors-25-07039\" ref-type=\"bibr\">27</xref>]. <xref rid=\"sensors-25-07039-f004\" ref-type=\"fig\">Figure 4</xref> depicts the BasicResDWSBlock module&#8217;s structure. In this module, depth-separable convolution is used in place of regular convolution. In contrast to a standard convolution, a depth-separable convolution will first convolve each channel of the input feature map independently in spatial dimension before combining the channels linearly by pointwise convolution.</p><p>Assume that the convolution kernel&#8217;s size in 3D is <inline-formula><mml:math id=\"mm31\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the input feature map&#8217;s size is <inline-formula><mml:math id=\"mm32\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the number of input channels is <inline-formula><mml:math id=\"mm33\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the number of output channels is <inline-formula><mml:math id=\"mm34\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Then, the parametric quantities <inline-formula><mml:math id=\"mm35\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and FLOPs <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the ordinary 3D convolution are as in Equations (11) and (12),<disp-formula id=\"FD11-sensors-25-07039\"><label>(11)</label><mml:math id=\"mm37\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD12-sensors-25-07039\"><label>(12)</label><mml:math id=\"mm38\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nand the parametric quantities <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and FLOPs <inline-formula><mml:math id=\"mm40\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the 3D depth-separable convolution are as in Equations (13) and (14),<disp-formula id=\"FD13-sensors-25-07039\"><label>(13)</label><mml:math id=\"mm41\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD14-sensors-25-07039\"><label>(14)</label><mml:math id=\"mm42\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi mathvariant=\"bold-italic\">K</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\ncomparison of the formulas allows visualization of the changes in the number of parameters and calculations, which not only drastically reduces the number of parameters and calculations, but also speeds up the training and reasoning process. We also refer to the method of Hu et al. [<xref rid=\"B36-sensors-25-07039\" ref-type=\"bibr\">36</xref>] to include the SE attention mechanism after the second depth-separable convolutional block of the BasicResDWSBlock module, and determined under several experiments that the channel compression parameter of the SE module is set to 16, and the number of compressed channels, <inline-formula><mml:math id=\"mm43\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, is given by Equation (<xref rid=\"FD15-sensors-25-07039\" ref-type=\"disp-formula\">15</xref>),<disp-formula id=\"FD15-sensors-25-07039\"><label>(15)</label><mml:math id=\"mm44\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant=\"italic\">squeezed</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi mathvariant=\"italic\">in</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>\nafter operations such as compression and activation functions, the channels are then expanded back for weighting operations on the original feature map.</p></sec></sec><sec id=\"sec4-sensors-25-07039\"><title>4. Experiments and&#160;Results</title><sec id=\"sec4dot1-sensors-25-07039\"><title>4.1. Data Acquisition and&#160;Labeling</title><p>As shown in <xref rid=\"sensors-25-07039-f005\" ref-type=\"fig\">Figure 5</xref>, we utilized two datasets: one private dataset, Shale Images (dimensions 200 &#215; 512 &#215; 512), and another public dataset, Leopard Sandstone Images (dimensions 1000 &#215; 1000 &#215; 1000), sourced from the Digital Rock website [<xref rid=\"B18-sensors-25-07039\" ref-type=\"bibr\">18</xref>].</p><p>During the annotation process of private data, the raw data is first sliced along a specific dimension to yield 200 two-dimensional images measuring 512 &#215; 512 pixels. These images are then manually annotated using tools like LabelMe (v4.5.0) [<xref rid=\"B37-sensors-25-07039\" ref-type=\"bibr\">37</xref>] and categorized into two classes: rock and pore. After annotation, a Python (v3.10.16) program converts multiple two-dimensional images into a three-dimensional format. Finally, the labels are saved in the .nii.gz format, suitable for deep learning training. In contrast, the raw and annotated data for the publicly available Leopard Sandstone Image dataset originate from the open-access Digital Rock website. Compared to this public dataset, our annotation process explicitly marks elongated pores and rigorously verifies annotation results. This distinction represents the core value of our dataset relative to the publicly available one.</p><p>Due to the limited amount of data, we propose using a slicing method to create the dataset. After slicing the private dataset, Shale Images, using a Python program, we obtained 1024 image samples with dimensions of 200 &#215; 200 &#215; 200. The public Leopard Sandstone Images dataset was segmented using a Python program, yielding 216 image samples with dimensions of 250 &#215; 250 &#215; 250. <xref rid=\"sensors-25-07039-f006\" ref-type=\"fig\">Figure 6</xref> illustrates selected segmented samples from both the Shale Images and Leopard Sandstone Images datasets.</p><p>As indicated in <xref rid=\"sensors-25-07039-t001\" ref-type=\"table\">Table 1</xref>, we combine the two produced datasets into distinct resolutions and randomly separate them into training, validation, and test sets in a 4:1:1 ratio to guarantee that each set has rich pore distributions in order to increase the segmentation accuracy.</p></sec><sec id=\"sec4dot2-sensors-25-07039\"><title>4.2. Experimental&#160;Setup</title><p>Our experimental environment configuration is shown in <xref rid=\"sensors-25-07039-t002\" ref-type=\"table\">Table 2</xref>. The hyperparameter settings for the training phase include a batch size of 64 and a training cycle count of 500 epochs. Hyperparameters for the model training phase include an input image size of 200 &#215; 200 &#215; 200, an SGD optimizer (Meta Platforms, Inc., Menlo Park, CA, USA), a learning rate initialized at 0.001, and momentum decay and weight decay values set to 0.99 and 0.00003, respectively. During model training, a learning rate decay strategy was utilized, with all other hyperparameters set to the default values of U-Mamba-Bot.</p></sec><sec id=\"sec4dot3-sensors-25-07039\"><title>4.3. Assessment of&#160;Indicators</title><p>In this study, we utilized the Dice similarity coefficient to evaluate the degree of overlap similarity between predicted and actual results, used IOU to measure the proportion of overlap between predicted and actual regions, and assessed the accuracy of the model&#8217;s predictions using the Accuracy metric [<xref rid=\"B38-sensors-25-07039\" ref-type=\"bibr\">38</xref>]. To evaluate the scale and computational load of each model, we expressed the total number of floating-point operations (FLOPs) as the ratio of all floating-point operations to the time required to complete the task. In addition to the above performance metrics, we also calculated the number of network parameters (Params) and the inference time (Time) of the model on the test set.</p></sec><sec id=\"sec4dot4-sensors-25-07039\"><title>4.4. Comparison of Different Segmentation&#160;Networks</title><p>We chose seven networks as comparison networks to assess the efficacy of our proposed approach. These include the most recent Mamba-based segmentation networks (U-Mamba-Bot and U-Mamba-Enc) and CNN-based techniques (3D U-Net, 3D SegNet, 3D U-ResNet, 3D KiUNet, and nnUNet). We train the aforementioned networks using the above two datasets in the same server environment, equipped with an NVIDIA GeForce RTX 4090 24 G GPU. Following training, performance metrics like accuracy, precision accuracy, and Dice similarity coefficient are computed to assess how well the model separates the segmented area from the background. For precise assessment of pore-scale properties [<xref rid=\"B39-sensors-25-07039\" ref-type=\"bibr\">39</xref>], including fluid saturation, pore connectivity, and porosity, accurate segmentation is essential. We carefully designed the experimental methodology to periodically evaluate the performance of the model during training and saved checkpoints at the point of minimum validation loss to ensure that the most efficient version of the model was retained for subsequent use or further improvement. <xref rid=\"sensors-25-07039-t003\" ref-type=\"table\">Table 3</xref> shows the results of the comparison experiments using the dataset Shale Images, and <xref rid=\"sensors-25-07039-t004\" ref-type=\"table\">Table 4</xref> shows the results of the comparison experiments using the dataset Leopard Sandstone Images.</p><p>In comparison to 3D U-Net, 3D SegNet, 3D U-ResNet, 3D KiUNet, nnUNet, U-Mamba-Enc, and U-Mamba-Bot, our proposed approach better balances computational cost and performance among the benchmarks and produces excellent results with the same hyperparameters. While the amount of parameters, FLOPs, and inference time are relatively low, 3D U-Net, 3D SegNet, 3D U-ResNet, and 3D KiUNet exhibit poor segmentation accuracy and precision. In terms of segmentation accuracy and precision, UNet, U-Mamba-Enc, and U-Mamba-Bot outperform models like 3D U-Net; nevertheless, the model complexity is comparatively large. On the other hand, our enhanced model&#8217;s parameters, FLOPs, and inference time are, respectively, <inline-formula><mml:math id=\"mm45\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>66.83</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm46\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>56.19</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm47\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>50.41</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> lower than those of the baseline. In addition to being smaller than the baseline, our new model also has better accuracy, intersection ratio, and similarity coefficient. The comparison results show that for the pore segmentation task of 3D rock images, the method proposed in this paper can more efficiently balance the model&#8217;s lightweight characteristics and segmentation accuracy, which fully demonstrates the model&#8217;s suitability for real-time environments or resource-constrained environments.</p><p><xref rid=\"sensors-25-07039-f007\" ref-type=\"fig\">Figure 7</xref> shows some of the segmentation results of the above models on the Shale Images test set and the Leopard Sandstone Images test set, and the visualization of the segmentation results shows that our proposed network not only accurately extracts the narrow and curved throat portion and extracts a more complete pore structure, but our proposed method also has stronger robustness and has fewer segmentation outliers.</p></sec><sec id=\"sec4dot5-sensors-25-07039\"><title>4.5. Ablation&#160;Experiments</title><p>We created five sets of ablation experiments to compare the impact of the different improvement techniques on the model performance for two datasets, Shale Images and Leopard Sandstone Images, in order to assess the efficacy of the enhanced algorithms and demonstrate the model&#8217;s capacity for generalization. To guarantee comparable findings, we maintained the other hyperparameters at their default settings and utilized the same tools and datasets for the studies. The results of the ablation experiments using the Shale Images dataset are displayed in <xref rid=\"sensors-25-07039-t005\" ref-type=\"table\">Table 5</xref>, and the results of the ablation experiments using the Leopard Sandstone Images dataset are displayed in <xref rid=\"sensors-25-07039-t006\" ref-type=\"table\">Table 6</xref>. A indicates the use of the BasicResDWSBlock module, B indicates the use of the LDLK module, and C indicates the use of the InceptionDSConv3d module. The first row uses U-Mamba-Bot as the baseline, and each module can be added to the model separately.</p><p>To start, the LDLK module is used in the first three stages of the baseline encoder after taking into account Dice, accuracy, IOU, the number of parameters, and FLOPs. This module focuses on capturing global contextual information with dynamic large kernels to better model long-range 3D pore structures, so the ablation experiments&#8217; results indicate that while the model&#8217;s number of parameters and FLOPs is slightly increased, Dice, accuracy, and IOU are improved. The InceptionDSConv3d module performs multi-scale feature fusion and refinement to accurately restore fine-grained pore boundaries, which is then used in the final two stages of the baseline decoder in order to improve the Dice, accuracy, and IOU metrics with the least amount of computational cost. The ablation experiments&#8217; results indicate that the model&#8217;s number of parameters and FLOPs are slightly higher than the baseline, with an increase in Dice, accuracy, and IOU. Building upon the above work, to significantly reduce the number of parameters and FLOPs in the model, we propose the BasicResDWSBlock module. Ablation experiments demonstrate that BasicResDWSBlock reduces parameters and floating-point operations while maintaining accuracy through deep separable convolutions and SE attention mechanisms. Ultimately, we integrate all the enhancements to attain the best equilibrium outcomes. In comparison to the baseline, the LDLK-U-Mamba model&#8217;s parameters and FLOPs are reduced by 28.09 M and 547.27 GFLOPs, respectively, while accuracy, Dice, and IOU are improved by <inline-formula><mml:math id=\"mm48\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>0.12</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm49\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1.25</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"mm50\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>2.35</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, according to the results of the ablation experiments conducted using the Shale Images dataset. In comparison to the baseline, the LDLK-U-Mamba model&#8217;s parameters and FLOPs decreased by 28.15 M and 547.27 GFLOPs, respectively, while accuracy, Dice, and IOU improved by <inline-formula><mml:math id=\"mm51\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1.18</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm52\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>0.73</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mn>1.39</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, according to the results of the ablation experiments conducted with the dataset Leopard Sandstone Images.</p></sec></sec><sec sec-type=\"conclusions\" id=\"sec5-sensors-25-07039\"><title>5. Conclusions</title><p>In this study, we propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed the LDLK-U-Mamba model. To enhance segmentation performance, we propose the LDLK module during the encoding phase to capture global contextual information, and utilize the InceptionDSConv3d module during decoding to fuse and refine multi-scale features, yielding more accurate segmentation results. We also propose the BasicResDWSBlock module, utilizing deep separable convolutions and the SE module to reduce model parameters and computational complexity.</p><p>To evaluate the model&#8217;s performance, we tested segmentation accuracy, Dice score, intersection-over-union ratio, parameter count, FLOPs, and inference time. Segmentation results on the Leopard Sandstone Images dataset achieved a 99.38% accuracy, a 99.62% Dice score, and a 99.25% intersection-over-union ratio. The parameter count decreased from 42.12 M to 13.97 M, and FLOPs decreased from 973.91 G to 426.64 G. Experimental results demonstrate that the LDLK-U-Mamba model outperforms current mainstream segmentation methods across multiple key metrics.</p><p>Though our method demonstrates superior performance in 3D porous rock segmentation, it is difficult to generalize to other 3D segmentation tasks in different domains due to the inevitable domain differences. To address this, we plan to utilize transfer learning and fine-tuning strategies to optimize the model to enhance its generalizability for achieving 3D segmentation tasks in other domains.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors express their gratitude to China University of Petroleum (East China).</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, G.C. and H.L.; software, H.L.; investigation, H.L.; writing&#8212;original draft preparation, H.L.; writing&#8212;review and editing, C.L. and P.L.; supervision, Y.K.; project administration, C.L. and P.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The original contributions presented in the study are included in the article. Further inquiries can be directed at the corresponding author.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-07039\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Karimpouli</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Tahmasebi</surname><given-names>P.</given-names></name></person-group><article-title>Segmentation of digital rock images using deep convolutional autoencoder networks</article-title><source>Comput. Geosci.</source><year>2019</year><volume>126</volume><fpage>142</fpage><lpage>150</lpage><pub-id pub-id-type=\"doi\">10.1016/j.cageo.2019.02.003</pub-id></element-citation></ref><ref id=\"B2-sensors-25-07039\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Varfolomeev</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Yakimchuk</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Safonov</surname><given-names>I.</given-names></name></person-group><article-title>An Application of Deep Neural Networks for Segmentation of Microtomographic Images of Rock Samples</article-title><source>Computers</source><year>2019</year><volume>8</volume><elocation-id>72</elocation-id><pub-id pub-id-type=\"doi\">10.3390/computers8040072</pub-id></element-citation></ref><ref id=\"B3-sensors-25-07039\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Molina</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Tassara</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Abarca</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Melnick</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Madella</surname><given-names>A.</given-names></name></person-group><article-title>Frictional Segmentation of the Chilean Megathrust from a Multivariate Analysis of Geophysical, Geological and Geodetic Data</article-title><source>J. Geophys. Res. Solid Earth</source><year>2021</year><volume>126</volume><fpage>e2020JB020647</fpage><pub-id pub-id-type=\"doi\">10.1029/2020JB020647</pub-id></element-citation></ref><ref id=\"B4-sensors-25-07039\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Miao</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>L.</given-names></name></person-group><article-title>A Review of Deep Learning-Based Methods for Road Extraction from High-Resolution Remote Sensing Images</article-title><source>Remote Sens.</source><year>2024</year><volume>16</volume><elocation-id>2056</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs16122056</pub-id></element-citation></ref><ref id=\"B5-sensors-25-07039\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>F.</given-names></name></person-group><article-title>Advances in Medical Image Segmentation: A Comprehensive Review of Traditional, Deep Learning and Hybrid Approaches</article-title><source>Bioengineering</source><year>2024</year><volume>11</volume><elocation-id>1034</elocation-id><pub-id pub-id-type=\"doi\">10.3390/bioengineering11101034</pub-id><pub-id pub-id-type=\"pmid\">39451409</pub-id><pub-id pub-id-type=\"pmcid\">PMC11505408</pub-id></element-citation></ref><ref id=\"B6-sensors-25-07039\"><label>6.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>&#199;i&#231;ek</surname><given-names>&#214;.</given-names></name><name name-style=\"western\"><surname>Abdulkadir</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Lienkamp</surname><given-names>S.S.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group><article-title>3D U-Net: Learning dense volumetric segmentation from sparse annotation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2016: 19th International Conference</source><conf-loc>Athens, Greece</conf-loc><conf-date>17&#8211;21 October 2016</conf-date><comment>Part II 19</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2016</year><fpage>424</fpage><lpage>432</lpage></element-citation></ref><ref id=\"B7-sensors-25-07039\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oktay</surname><given-names>O.</given-names></name></person-group><article-title>Attention u-net: Learning where to look for the pancreas</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1804.03999</pub-id><pub-id pub-id-type=\"arxiv\">1804.03999</pub-id></element-citation></ref><ref id=\"B8-sensors-25-07039\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y.D.</given-names></name><name name-style=\"western\"><surname>Shabaninejad</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Armstrong</surname><given-names>R.T.</given-names></name><name name-style=\"western\"><surname>Mostaghimi</surname><given-names>P.</given-names></name></person-group><article-title>Deep neural networks for improving physical accuracy of 2D and 3D multi-mineral segmentation of rock micro-CT images</article-title><source>Appl. Soft Comput.</source><year>2021</year><volume>104</volume><fpage>107185</fpage><pub-id pub-id-type=\"doi\">10.1016/j.asoc.2021.107185</pub-id></element-citation></ref><ref id=\"B9-sensors-25-07039\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Ren</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Y.</given-names></name></person-group><article-title>Dense U-net based on patch-based learning for retinal vessel segmentation</article-title><source>Entropy</source><year>2019</year><volume>21</volume><elocation-id>168</elocation-id><pub-id pub-id-type=\"doi\">10.3390/e21020168</pub-id><pub-id pub-id-type=\"pmid\">33266884</pub-id><pub-id pub-id-type=\"pmcid\">PMC7514650</pub-id></element-citation></ref><ref id=\"B10-sensors-25-07039\"><label>10.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Cui</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>SFMRNet: Specific Feature Fusion and Multibranch Feature Refinement Network for Land Use Classification</article-title><source>Sel. Top. Appl. Earth Obs. Remote Sens. IEEE J.</source><year>2024</year><volume>17</volume><fpage>16206</fpage><lpage>16221</lpage><pub-id pub-id-type=\"doi\">10.1109/JSTARS.2024.3456842</pub-id></element-citation></ref><ref id=\"B11-sensors-25-07039\"><label>11.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group><article-title>TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2102.04306</pub-id><pub-id pub-id-type=\"arxiv\">2102.04306</pub-id></element-citation></ref><ref id=\"B12-sensors-25-07039\"><label>12.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dong</surname><given-names>S.</given-names></name></person-group><article-title>A Separate 3D-SegNet Based on Priority Queue for Brain Tumor Segmentation</article-title><source>Proceedings of the 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)</source><conf-loc>Hangzhou, China</conf-loc><conf-date>22&#8211;23 August 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year></element-citation></ref><ref id=\"B13-sensors-25-07039\"><label>13.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Valanarasu</surname><given-names>J.M.J.</given-names></name><name name-style=\"western\"><surname>Sindagi</surname><given-names>V.A.</given-names></name><name name-style=\"western\"><surname>Hacihaliloglu</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Patel</surname><given-names>V.M.</given-names></name></person-group><article-title>KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>41</volume><fpage>965</fpage><lpage>976</lpage><pub-id pub-id-type=\"doi\">10.1109/TMI.2021.3130469</pub-id><pub-id pub-id-type=\"pmid\">34813472</pub-id></element-citation></ref><ref id=\"B14-sensors-25-07039\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isensee</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Jaeger</surname><given-names>P.F.</given-names></name><name name-style=\"western\"><surname>Kohl</surname><given-names>S.A.</given-names></name><name name-style=\"western\"><surname>Petersen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Maier-Hein</surname><given-names>K.H.</given-names></name></person-group><article-title>nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nat. Methods</source><year>2021</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type=\"doi\">10.1038/s41592-020-01008-z</pub-id><pub-id pub-id-type=\"pmid\">33288961</pub-id></element-citation></ref><ref id=\"B15-sensors-25-07039\"><label>15.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Milletari</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Navab</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Ahmadi</surname><given-names>S.A.</given-names></name></person-group><article-title>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</article-title><source>Proceedings of the 2016 Fourth International Conference on 3D Vision</source><conf-loc>Stanford, CA, USA</conf-loc><conf-date>25&#8211;28 October 2016</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2016</year></element-citation></ref><ref id=\"B16-sensors-25-07039\"><label>16.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hatamizadeh</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Roth</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>D.</given-names></name></person-group><article-title>UNETR: Transformers for 3D Medical Image Segmentation</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Virtual</conf-loc><conf-date>3&#8211;8 January 2021</conf-date></element-citation></ref><ref id=\"B17-sensors-25-07039\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name></person-group><article-title>U-mamba: Enhancing long-range dependency for biomedical image segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2401.04722</pub-id></element-citation></ref><ref id=\"B18-sensors-25-07039\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Kwak</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>B.</given-names></name></person-group><article-title>Enhancing rock image segmentation in digital rock physics: A fusion of generative ai and state-of-the-art neural networks</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type=\"arxiv\">2311.06079</pub-id></element-citation></ref><ref id=\"B19-sensors-25-07039\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Kang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Xiong</surname><given-names>G.</given-names></name></person-group><article-title>Construction of pore structure and lithology of digital rock physics based on laboratory experiments</article-title><source>J. Pet. Explor. Prod. Technol.</source><year>2021</year><volume>11</volume><fpage>2113</fpage><lpage>2125</lpage><pub-id pub-id-type=\"doi\">10.1007/s13202-021-01149-7</pub-id></element-citation></ref><ref id=\"B20-sensors-25-07039\"><label>20.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Guo</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Xiang</surname><given-names>Z.</given-names></name></person-group><article-title>A method of blasted rock image segmentation based on improved watershed algorithm</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><elocation-id>7143</elocation-id><pub-id pub-id-type=\"doi\">10.1038/s41598-022-11351-0</pub-id><pub-id pub-id-type=\"pmid\">35505086</pub-id><pub-id pub-id-type=\"pmcid\">PMC9065013</pub-id></element-citation></ref><ref id=\"B21-sensors-25-07039\"><label>21.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vincent</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Soille</surname><given-names>P.</given-names></name></person-group><article-title>Watersheds in digital spaces: An efficient algorithm based on immersion simulations</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1991</year><volume>13</volume><fpage>583</fpage><lpage>598</lpage><pub-id pub-id-type=\"doi\">10.1109/34.87344</pub-id></element-citation></ref><ref id=\"B22-sensors-25-07039\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Reinhardt</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Jacob</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Sadeghnejad</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Cappuccio</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Arnold</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Frank</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Enzmann</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Kersten</surname><given-names>M.</given-names></name></person-group><article-title>Benchmarking conventional and machine learning segmentation techniques for digital rock physics analysis of fractured rocks</article-title><source>Environ. Earth Sci.</source><year>2022</year><volume>81</volume><fpage>71</fpage><pub-id pub-id-type=\"doi\">10.1007/s12665-021-10133-7</pub-id></element-citation></ref><ref id=\"B23-sensors-25-07039\"><label>23.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zunair</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Hamza</surname><given-names>A.B.</given-names></name></person-group><article-title>Masked supervised learning for semantic segmentation</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.2210.00923</pub-id><pub-id pub-id-type=\"arxiv\">2210.00923</pub-id></element-citation></ref><ref id=\"B24-sensors-25-07039\"><label>24.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y.D.</given-names></name><name name-style=\"western\"><surname>Shabaninejad</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Armstrong</surname><given-names>R.T.</given-names></name><name name-style=\"western\"><surname>Mostaghimi</surname><given-names>P.</given-names></name></person-group><article-title>Physical Accuracy of Deep Neural Networks for 2D and 3D Multi- Mineral Segmentation of Rock micro-CT Images</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type=\"arxiv\">2002.05322</pub-id><pub-id pub-id-type=\"doi\">10.1016/j.asoc.2021.107185</pub-id></element-citation></ref><ref id=\"B25-sensors-25-07039\"><label>25.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Sermanet</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Reed</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Anguelov</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Erhan</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Rabinovich</surname><given-names>A.</given-names></name></person-group><article-title>Going deeper with convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#8211;12 June 2015</conf-date><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id=\"B26-sensors-25-07039\"><label>26.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Szegedy</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Vanhoucke</surname><given-names>V.</given-names></name><name name-style=\"western\"><surname>Ioffe</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Shlens</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wojna</surname><given-names>Z.</given-names></name></person-group><article-title>Rethinking the inception architecture for computer vision</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#8211;30 June 27 2016</conf-date><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id=\"B27-sensors-25-07039\"><label>27.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><article-title>Xception: Deep learning with depthwise separable convolutions</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>1251</fpage><lpage>1258</lpage></element-citation></ref><ref id=\"B28-sensors-25-07039\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Howard</surname><given-names>A.G.</given-names></name></person-group><article-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type=\"doi\">10.48550/arXiv.1704.04861</pub-id><pub-id pub-id-type=\"arxiv\">1704.04861</pub-id></element-citation></ref><ref id=\"B29-sensors-25-07039\"><label>29.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>L.C.</given-names></name><name name-style=\"western\"><surname>Papandreou</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Kokkinos</surname><given-names>I.</given-names></name><name name-style=\"western\"><surname>Murphy</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Yuille</surname><given-names>A.L.</given-names></name></person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type=\"pmid\">28463186</pub-id></element-citation></ref><ref id=\"B30-sensors-25-07039\"><label>30.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Jia</surname><given-names>J.</given-names></name></person-group><article-title>Pyramid scene parsing network</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#8211;26 July 2017</conf-date><fpage>2881</fpage><lpage>2890</lpage></element-citation></ref><ref id=\"B31-sensors-25-07039\"><label>31.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zha</surname><given-names>S.</given-names></name></person-group><article-title>TransBTS: Multimodal Brain Tumor Segmentation Using Transformer</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Strasbourg, France</conf-loc><conf-date>27 September&#8211;1 October 2021</conf-date></element-citation></ref><ref id=\"B32-sensors-25-07039\"><label>32.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Marcus</surname><given-names>D.S.</given-names></name><name name-style=\"western\"><surname>Sotiras</surname><given-names>A.</given-names></name></person-group><article-title>D-net: Dynamic large kernel with dynamic feature fusion for volumetric medical image segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type=\"arxiv\">2403.10674</pub-id><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2025.108837</pub-id></element-citation></ref><ref id=\"B33-sensors-25-07039\"><label>33.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shao</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>P.</given-names></name></person-group><article-title>Differentiable learning-to-group channels via groupable convolutional neural networks</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#8211;2 November 2019</conf-date><fpage>3542</fpage><lpage>3551</lpage></element-citation></ref><ref id=\"B34-sensors-25-07039\"><label>34.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Yun</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Heo</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Yoo</surname><given-names>Y.</given-names></name></person-group><article-title>Rethinking channel dimensions for efficient model design</article-title><source>Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#8211;25 June 2021</conf-date><fpage>732</fpage><lpage>741</lpage></element-citation></ref><ref id=\"B35-sensors-25-07039\"><label>35.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>InceptionNeXt: When Inception Meets ConvNeXt</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#8211;22 June 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year></element-citation></ref><ref id=\"B36-sensors-25-07039\"><label>36.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>G.</given-names></name></person-group><article-title>Squeeze-and-excitation networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#8211;22 June 2018</conf-date><fpage>7132</fpage><lpage>7141</lpage></element-citation></ref><ref id=\"B37-sensors-25-07039\"><label>37.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Russell</surname><given-names>B.C.</given-names></name><name name-style=\"western\"><surname>Torralba</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Murphy</surname><given-names>K.P.</given-names></name><name name-style=\"western\"><surname>Freeman</surname><given-names>W.T.</given-names></name></person-group><article-title>LabelMe: A Database and Web-Based Tool for Image Annotation</article-title><source>Int. J. Comput. Vis.</source><year>2008</year><volume>77</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type=\"doi\">10.1007/s11263-007-0090-8</pub-id></element-citation></ref><ref id=\"B38-sensors-25-07039\"><label>38.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Marques</surname><given-names>V.G.</given-names></name><name name-style=\"western\"><surname>Silva</surname><given-names>L.R.D.D.</given-names></name><name name-style=\"western\"><surname>Carvalho</surname><given-names>B.M.</given-names></name><name name-style=\"western\"><surname>Lucena</surname><given-names>L.R.F.D.</given-names></name><name name-style=\"western\"><surname>Vieira</surname><given-names>M.M.</given-names></name></person-group><article-title>Deep Learning-Based Pore Segmentation of Thin Rock Sections for Aquifer Characterization Using Color Space Reduction</article-title><source>Proceedings of the 2019 International Conference on Systems, Signals and Image Processing (IWSSIP)</source><conf-loc>Osijek, Croatia</conf-loc><conf-date>5&#8211;7 June 2019</conf-date></element-citation></ref><ref id=\"B39-sensors-25-07039\"><label>39.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Purswani</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Karpyn</surname><given-names>Z.T.</given-names></name><name name-style=\"western\"><surname>Enab</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>X.</given-names></name></person-group><article-title>Evaluation of image segmentation techniques for image-based rock property estimation</article-title><source>J. Pet. Sci. Eng.</source><year>2020</year><volume>195</volume><fpage>107890</fpage><pub-id pub-id-type=\"doi\">10.1016/j.petrol.2020.107890</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-07039-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>The structure of our LDLK-U-Mamba. The model includes an encoder, a decoder, and skip connections. The LDLK module in the encoder captures global contextual information, while the InceptionDSConv3d module in the decoder achieves more accurate segmentation results through multi-scale feature fusion and refinement. The BasicResDWSBlock module employs separable convolutions and the SE module to significantly reduce the model&#8217;s parameter count and computational complexity. The BasicBlock acts as a basic feature extraction unit with stacked 3D convolutions, instance normalization, and activation for 3D feature capture.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>The LDLK module. This module includes Reduce Channels, a RELU, and an LDLK. Reduce Channels and ReLU effectively reduce computational load, while LDLK&#8217;s dynamic group convolution enables dynamic feature fusion with low computational overhead, efficiently utilizing global contextual information.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>The InceptionDSConv3d module. This module divides the input channel dimensions into four depthwise separable convolutional branches and one identity branch, with the outputs of each branch ultimately concatenated. This multi-branch architecture, combined with depthwise separable convolutional kernels of varying sizes, enables fine-grained feature processing across multiple dimensions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>The BasicResDWSBlock module. This module adopts depthwise separable convolutions instead of standard convolutions and incorporates a separable activation layer after the second separable convolution block, achieving a significant reduction in model parameters and FLOPs.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>(<bold>a</bold>) &#181;CT image of shale image. (<bold>b</bold>) &#181;CT image of leopard sandstone image.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>(<bold>a</bold>) shale images. (<bold>b</bold>) leopard sandstone images.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-07039-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Qualitative comparison between LDLK-U-Mamba, U-Mamba-Bot, U-Mamba-Enc, nnUNet, 3D U-Net, 3D U-ResNet, 3D SegNet, and 3D KiUNet. Additionally, LDLK-U-Mamba shows better segmentation quality than others in terms of the segmentation of elongated pores and the integrity of the pore structure.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-07039-g007.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-07039-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t001_Table 1</object-id><label>Table 1</label><caption><p>Dataset division.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Resolution</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Train</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Validation</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Test</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Total</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Shale Images</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">200&#160;&#215;&#160;120&#160;&#215;&#160;120</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">683</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">171</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">170</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1024</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Leopard Sandstone Images</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">250&#160;&#215;&#160;250&#160;&#215;&#160;250</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">144</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">36</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">36</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">216</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07039-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t002_Table 2</object-id><label>Table 2</label><caption><p>Configuration of the experimental environment.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Category</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Configuration</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">GPU</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">NVIDIA GeForce RTX 4090 24 G</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">System environment</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Ubuntu 20.04</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Torch version</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.6.0 + cu118</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Programming language</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Python 3.10.16</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07039-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparative experiment results utilizing the shale images dataset. <bold>Bold</bold> represents the best results, and <underline>underline</underline> represents the second-best results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Accuracy (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dice (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">IOU (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs (G) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Time (ms) &#8595;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D U-Net&#160;[<xref rid=\"B6-sensors-25-07039\" ref-type=\"bibr\">6</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>2.32</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">117.98</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.5</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D U-ResNet&#160;[<xref rid=\"B24-sensors-25-07039\" ref-type=\"bibr\">24</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">35.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">21.37</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9.49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1699.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1180.3</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D SegNet&#160;[<xref rid=\"B12-sensors-25-07039\" ref-type=\"bibr\">12</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.40</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">26.10</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">15.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>116.52</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>76.8</underline>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D KiUNet&#160;[<xref rid=\"B13-sensors-25-07039\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">55.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">39.09</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">130.17</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.2</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnUNet&#160;[<xref rid=\"B14-sensors-25-07039\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.32</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.32</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.51</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">31.19</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">534.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">365.7</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Mamba-Enc&#160;[<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.79</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.58</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">78.07</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.75</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">990.65</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">620.4</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Mamba-Bot&#160;[<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>99.34</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>93.50</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>87.84</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">973.91</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">598.1</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Ours</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.46</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>94.67</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>89.90</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>14.03</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>426.64</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>295.3</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07039-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparative experiment results utilizing the leopard sandstone images dataset. <bold>Bold</bold> represents the best results, and <underline>underline</underline> represents the second-best results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Accuracy (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dice (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">IOU (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs (G) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Time (ms) &#8595;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D U-Net&#160;[<xref rid=\"B6-sensors-25-07039\" ref-type=\"bibr\">6</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.77</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">92.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>2.32</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">117.98</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.2</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D U-ResNet&#160;[<xref rid=\"B24-sensors-25-07039\" ref-type=\"bibr\">24</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.32</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.10</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">9.49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1660.00</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1165.8</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D SegNet&#160;[<xref rid=\"B12-sensors-25-07039\" ref-type=\"bibr\">12</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.44</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.55</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">60.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>116.54</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>77.4</underline>\n</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D KiUNet&#160;[<xref rid=\"B13-sensors-25-07039\" ref-type=\"bibr\">13</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">57.72</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">67.08</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">50.54</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">178.73</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.6</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">nnUNet&#160;[<xref rid=\"B14-sensors-25-07039\" ref-type=\"bibr\">14</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.04</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.80</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.66</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">31.19</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">534.03</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">368.2</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Mamba-Enc&#160;[<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">81.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.55</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">81.13</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.75</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">990.65</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">625.1</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Mamba-Bot&#160;[<xref rid=\"B17-sensors-25-07039\" ref-type=\"bibr\">17</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>98.22</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>98.90</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<underline>97.89</underline>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">973.91</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">602.5</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>Ours</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.38</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.62</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.25</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>13.97</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>426.64</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>298.8</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07039-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t005_Table 5</object-id><label>Table 5</label><caption><p>Results of the ablation experiments utilizing the dataset, Shale Images. <bold>Bold</bold> represents the best results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">A</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">B</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">C</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Accuracy (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dice (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">IOU (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs (G) &#8595;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.34</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.50</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.84</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">973.91</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.32</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">93.28</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.45</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">13.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">425.28</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.43</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.39</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.40</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">975.00</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.42</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">94.25</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.16</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">974.18</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.46</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>94.67</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>89.90</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>14.03</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>426.64</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-07039-t006\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-07039-t006_Table 6</object-id><label>Table 6</label><caption><p>Results of the ablation experiments utilizing the dataset, Leopard Sandstone Images. <bold>Bold</bold> represents the best results.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">A</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">B</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">C</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Accuracy (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Dice (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">IOU (%) &#8593;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">Params (M) &#8595;</th><th align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin;border-top:solid thin\" rowspan=\"1\" colspan=\"1\">FLOPs (G) &#8595;</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.22</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.89</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">973.91</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">97.27</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98.31</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">96.76</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">13.96</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">425.28</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.37</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.61</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.23</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">974.78</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#160;&#160;&#160;&#160;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.34</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.60</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">99.20</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">42.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">974.18</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.38</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.62</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>99.25</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>13.97</bold>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<bold>426.64</bold>\n</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656116 PMC12656116.1 12656116 12656116 41305245 10.3390/s25227039 sensors-25-07039 1 Article LDLK-U-Mamba: An Efficient and Highly Accurate Method for 3D Rock Pore Segmentation Chen Guojun Conceptualization 1 2 https://orcid.org/0009-0000-0875-2125 Li Huihui Conceptualization Software Investigation Writing &#8211; original draft 1 2 * Liu Chang Writing &#8211; review &amp; editing Project administration 1 2 Li Pengxia Writing &#8211; review &amp; editing Project administration 1 2 Kong Yunyi Supervision 1 2 Doulamis Anastasios Academic Editor 1 Qingdao Institute of Software, College of Computer Science and Technology, China University of Petroleum (East China), Qingdao 266580, China; chengj@upc.edu.cn (G.C.); z23070142@s.upc.edu.cn (C.L.); s23070018@s.upc.edu.cn (P.L.); s24070043@s.upc.edu.cn (Y.K.) 2 Shandong Key Laboratory of Intelligent Oil &amp; Gas Industrial Software, Qingdao 266580, China * Correspondence: s23070002@s.upc.edu.cn 18 11 2025 11 2025 25 22 501335 7039 11 9 2025 08 11 2025 14 11 2025 18 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Three-dimensional rock pore segmentation is crucial in fields such as geology and petroleum exploration, holding significant importance for oil and gas resource exploration and development. However, existing segmentation methods still present two main limitations: (1) they fail to capture the spatial relationships of pores in 3D when directly applied to 3D rock pore segmentation, inevitably leading to inaccurate segmentation results; (2) they struggle to apply efficiently in resource-constrained scenarios due to the high computational complexity and costly computational demands. To solve the above issues, we propose a novel and lightweight method based on the Mamba architecture, termed LDLK-U-Mamba, for precise and efficient 3D rock pore segmentation. Specifically, we design a Lightweight Dynamic Large Kernel (LDLK) module to capture global contextual information and develop an InceptionDSConv3d module for multi-scale feature fusion and refinement, further yielding more accurate segmentation results. In addition, the Basic Residual Depthwise Separable Block (BasicResDWSBlock) module is proposed to utilize depthwise separable convolutions and the Squeeze-and-Excitation (SE) module to reduce model parameters and computational complexity. Extensive qualitative and quantitative experiments demonstrate that our LDLK-U-Mamba outperforms current mainstream segmentation approaches, validating its effectiveness for rock pore segmentation&#8212;particularly in capturing the 3D spatial relationships of pores. 3D rock pore segmentation Mamba lightweight This research received no external funding. pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction In geological science and geology research, the analysis of rock structures and dimensional structures is crucial for understanding the evolutionary processes of underground oil and gas reservoirs [ 1 ]. In particular, the pore structures within rock images play a vital role in determining water resource planning, geothermal energy utilization and underground resource exploration [ 2 ]. However, due to the inherent complexity of rock structures and the inability to clearly separate pore boundaries from the rock matrix, segmenting pore regions within rock images presents a formidable challenge. In the task of segmenting two-dimensional images, such as in the fields of geology [ 3 ], remote sensing [ 4 ], and medicine [ 5 ], the most commonly used deep learning architecture is U-Net [ 6 ]. Beyond U-Net, existing research has proposed numerous improved network architectures, such as Attention U-Net [ 7 ], U-ResNet [ 8 ], Dense U-Net [ 9 ], SFMRNet [ 10 ] and TransUNet [ 11 ]. However, these 2D segmentation methods rely solely on feature extraction from a single planar image, resulting in the loss of depth information and spatial correlations within rock pores. This leads to discrepancies between segmentation results and actual pore morphology. To address these limitations, 3D segmentation methods have gradually emerged as a key research focus. In recent years, 3D U-Net [ 2 ] and 3D U-ResNet [ 8 ] have been applied to mineral segmentation in 3D rock images. However, they struggle to capture edge textures, resulting in low segmentation accuracy. Given the limited number of existing networks for 3D rock image segmentation, while popular networks in 3D medical image segmentation&#8212;such as 3D SegNet [ 12 ], Kiu-net [ 13 ], nnUNet [ 14 ], V-Net [ 15 ], UNETR [ 16 ] and U-Mamba [ 17 ]&#8212;demonstrate remarkable performance in medical image segmentation, but they struggle to segment elongated pores effectively when applied to rock segmentation. In our study, we propose a segmentation-based LDLK-U-Mamba model capable of segmenting pores in 3D rock images efficiently and accurately. Specifically, we propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed LDLK-U-Mamba, for segmenting pores in 3D rock images. We propose using an LDLK module during the encoding phase to capture global contextual information and utilizing the InceptionDSConv3d module during the decoding phase for multi-scale feature fusion and refinement, achieving more accurate segmentation results. Additionally, we propose the BasicResDWSBlock module, utilizing deep separable convolutions and the SE module to reduce model parameters and computational complexity. We conduct several comparative experiments for rock pore segmentation to illustrate performance differences among algorithms. Experimental results on the Leopard Sandstone Images dataset demonstrate that our model achieves 99.38 % accuracy, 99.62 % Dice, and 99.25 % IOU, while reducing parameters from 42.12 M to 13.97 M, and computational FLOPs decreased from 973.91 GFLOPs to 426.64 GFLOPs. Compared with other 3D segmentation networks, our proposed LDLK-U-Mamba demonstrates outstanding comprehensive performance. Overall, the contributions of our research can be summarized as follows: (1) We propose a 3D rock pore segmentation model, termed LDLK-U-Mamba, which is based on the mamba network for precise and efficient 3D rock pore segmentation. (2) We propose a Lightweight Dynamic Large Kernel (LDLK) module to capture global contextual information and design an InceptionDSConv3d module to fuse and refine multi-scale features, thereby achieving more accurate segmentation results. (3) We propose a Basic Residual Depthwise Separable Block (BasicResDWSBlock) module, which employs separable convolutions and the Squeeze-and-Excitation (SE) module to reduce model parameters and computational complexity. (4) The comparative experiments demonstrate LDLK-U-Mamba outperforms the existing 3D segmentation networks. 2. Related&#160;Work 2.1. 2D Image&#160;Segmentation In 2D images, accurately segmenting digital rocks into categories such as pores and rocks remains a formidable challenge [ 18 ]. Traditional image segmentation techniques, including thresholding (such as the maximum interclass variance method [ 19 ]), watershed segmentation [ 20 ], clustering analysis, and edge detection [ 21 ], rely heavily on parameter selection influenced by user experience and subjective judgment, which lead to inconsistent segmentation results [ 22 ]. Among deep learning architectures, U-Net [ 6 ] fuses multi-scale information, yet its skip connections naively concatenate features across scales without accounting for 3D structural coherence such as voxel spatial relationships and pore dimensional connectivity, thus struggling to capture fine 3D spatial details like intricate rock pore structures. Attention U-Net [ 7 ] introduces an attention mechanism in the decoder, enabling the network to focus on more important feature regions. However, it lacks the targeted modeling of geometric features, such as spatial orientation and hierarchy in 3D space, with hierarchy referring to the micro-to-macro progression of pore-related features (throats, pore clusters, and pore zones) that are interconnected in the 3D space. U-ResNet [ 8 ] introduces the residual structure from ResNet, mitigating the gradient vanishing problem in deep networks through shortcut connections. Nevertheless, it struggles to accurately distinguish subtle structural differences within the 3D space. Dense U-Net [ 9 ] enhances feature reuse efficiency, but its dense connections cause feature map counts to increase exponentially with network depth and fail to effectively capture cross-dimensional feature correlations in the 3D space. SFMRNet [ 10 ] excels in segmentation tasks for ambiguous regions in remote sensing images through its multi-feature correlation module, but it is limited to two-dimensional data processing. TransUNet [ 11 ] combines the global modeling advantages of Transformers with UNet&#8217;s capability for fine-grained detail recovery, demonstrating exceptional performance in capturing long-range dependencies. However, its reliance on two-dimensional slice processing prevents it from leveraging the spatial correlation information inherent in three-dimensional data. Zunair et al. [ 23 ] proposed Masked Supervised Learning (MaskSup) for semantic segmentation, a single-stage method that uses a Siamese network and random masking to model both short- and long-range context, addressing small target segmentation, ambiguous boundaries, and class imbalance. Like the other 2D methods above, it fails to capture fine 3D pore details. In contrast, within our proposed LDLK-U-Mamba model, the LDLK module and InceptionDSConv3d module effectively address the limitations of 2D segmentation methods in capturing 3D spatial features. 2.2. 3D Image&#160;Segmentation In recent years, the advent of high-resolution micro-computed tomography (&#181;CT) imaging of rock samples [ 8 ] has enabled the acquisition of 3D image data, paving the way for refined segmentation of rock images. In mineral segmentation within 3D rock images, while 3D U-Net [ 6 ] can capture richer contextual information spatially, it struggles to adequately capture edge and texture details, resulting in lower segmentation accuracy. Furthermore, while 3D U-ResNet [ 24 ] demonstrates improved robustness in handling complex 3D structures through residual connections, it fails to resolve the boundary distinction between pores and matrix in rock samples. In domains like medical image segmentation, 3D SegNet [ 12 ] employs a symmetric encoder&#8211;decoder architecture to optimize information recovery during upscaling. However, this approach significantly increases computational demands when processing ultra-large volumetric data. Three-dimensional KiUNet [ 13 ] proposes block-based segmentation methods, but the loss of spatial correlation between blocks often leads to edge discontinuities after stitching. Meanwhile, nnUNet [ 14 ], as a highly adaptive framework, requires multi-round hyperparameter optimization validation, inevitably extending training cycles. V-Net [ 15 ] excels at capturing three-dimensional spatial features in 3D medical images with high segmentation accuracy, but it has numerous model parameters and demands substantial computational resources. UNETR [ 16 ] combines Transformer with 3D U-Net, offering strong global modeling capabilities, yet it suffers from insufficient small-object segmentation accuracy and requires large amounts of data for training. Although the U-Mamba [ 17 ] network performs well on large-scale data due to its multi-scale feature extraction, the computational complexity of 3D convolutions still results in lengthy processing times when handling 3D data. Furthermore, when dealing with extremely fine pores in rocks, multi-scale feature extraction struggles to fully capture the details of microscopic structures, leaving room for improvement in segmentation accuracy. To date, all existing 2D or 3D rock image segmentation networks have been developed for studying rock minerals. Our work represents the first research focused on pore segmentation in 3D rock images, which is performed precisely. 2.3. Neural Network&#160;Lightweighting In deep learning image segmentation and feature extraction tasks, enhancing network performance under limited computational resources and memory constraints has become a significant research focus. To address this challenge, Szegedy et al. [ 25 ] introduced the Inception module, the first network to efficiently model image features through the parallel computation of multi-scale features. Subsequently, Szegedy et al. [ 26 ] further proposed Inception-v2 and Inception-v3, introducing factorized convolutions and batch normalization to further reduce memory consumption and computational costs. To minimize computational resource requirements, Chollet et al. [ 27 ] proposed the Xception network, which further optimized the computation of convolutional operations through deep separable convolutions. Inspired by Xception, MobileNet [ 28 ] introduced a lightweight network based on deep separable convolutions, enabling efficient image classification and segmentation on mobile and embedded devices. Building upon this foundation, MobileNetV2 [ 29 ] further introduced reverse residual structures and linear bottlenecks to reduce memory requirements and computational costs. Zhao et al. [ 30 ] also proposed a lightweight 3D segmentation network based on Inception and deep separable convolutions. This approach achieved excellent segmentation results on multiple 3D medical image datasets while maintaining low computational costs, offering new insights for pore segmentation in 3D rock images. 3. Methods 3.1. Overview We propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed LDLK-U-Mamba, for segmenting pores in 3D rock images. It aims to enhance segmentation performance while reducing the number of parameters and FLOPs compared to Mamba. The structure of our LDLK-U-Mamba model is shown in Figure 1 . The model includes an encoder, a decoder, and skip connections. The LDLK module in the encoder captures global contextual information, while the InceptionDSConv3d module in the decoder achieves more accurate segmentation results through multi-scale feature fusion and refinement. The BasicResDWSBlock module employs separable convolutions and the SE module to significantly reduce the model&#8217;s parameter count and computational complexity. 3.2. The LDLK&#160;Module Molina et al. [ 3 ] achieved a refined analysis of geological segmentation through multi-source geoscience data in their study of Chile&#8217;s giant thrust belt. Similarly, in the field of rock pore segmentation, the integrated extraction of multidimensional features such as pore morphology and connectivity has become a key direction for enhancing segmentation accuracy. In existing research, Transformer models have achieved remarkable success in image segmentation due to their extensive receptive fields and ability to effectively utilize long-range contextual information across the entire image [ 31 ]. Convolutional neural networks (CNNs) can also achieve large receptive fields by employing large-sized convolutional kernels, enabling them to attain competitive performance with fewer model parameters. However, in 3D rock image segmentation, fixed-size convolutional kernels lack the flexibility to handle pores and fractures at different scales. CNNs using large convolutional kernels remain constrained in capturing multi-scale features of pore structures with significantly varying shapes and sizes. This limitation may cause models to underperform when handling complex and variable rock microstructures, particularly when precise identification of pore features of varying sizes and morphologies is required. Additionally, they fail to efficiently utilize global contextual information. To address these limitations, we propose the LDLK module with dynamic feature fusion capabilities [ 32 ]. The structure of the LDLK module is shown in Figure 2 . To maintain low computational and parameter complexity, our LDLK utilizes convolutional kernel sizes (3 &#215; 3 &#215; 3 and 5 &#215; 5 &#215; 5) that are more suitable for our dataset, combined with a reasonable dilation factor (dilation = 2). As indicated by Equation ( 1 ), let X c h l be the input feature map, where X is the z-dimension (slice) of the sample object, l is the number of layers the feature map is located in, c is the channel dimension, and h is the spatial dimension information. To create the attention weights, we utilize grouped convolution (groups = 2) in conjunction with a tiny convolution kernel (3 &#215; 3 &#215; 3), following Zhang et al. [ 33 ]&#8217;s design of dynamic grouped convolution. (1) w c h = Sigmoid ( GConv ( AVGPool ( X c h l ) ) ) . Second, as shown in Equation ( 2 ), we refer to the channel compression method proposed by Han et al. in [ 34 ] and propose to use channel dimensionality reduction. Let the number of channels of the input feature map be C i n , and reduce the number of channels to 1/32 of the original one by 1 &#215; 1 &#215; 1 convolution, then the number of channels after dimensionality reduction C r e d u c e d = C i n 32 , (2) X r e d u c e d l = Conv 1 &#215; 1 &#215; 1 ( X l , C r e d u c e d ) , where X l ( X l and the previously mentioned X c h l represent a reasonable distinction in symbols to accommodate different operational requirements) is the input feature map, Conv 1 &#215; 1 &#215; 1 denotes the 1 &#215; 1 &#215; 1 convolution operation, and X r e d u c e d l is the feature map after dimensionality reduction. The original number of channels is then restored after activation and spatial gating unit processing, and this bottleneck structure significantly reduces the intermediate computational cost. The above process can be expressed as Equation ( 3 ), (3) X l = LDLK ( ReLU ( X l ) ) . In order to increase operating efficiency, we decide to utilize the lightweight ReLU function for the activation function. Overall, our design focuses more on efficient computation on the basis of guaranteeing the segmentation effect, which is suitable for the case of limited computational resources. 3.3. The InceptionDSConv3d&#160;Module The task of the decoding stage is to progressively restore segmented spatial details, necessitating multidirectional and multi-scale reconstruction of the high-dimensional features extracted by the encoder. To achieve fusion and refinement of the multi-scale features, we combine the Inception module with deep separable convolutions in the decoder, proposing the InceptionDSConv3d module. The Inception module was first introduced by Szegedy et al. [ 25 ] to capture features at different scales through parallel operations of multi-scale convolutional kernels. Subsequently, Chollet et al. [ 27 ] introduced Deep Separable Convolution in 2017. This method decomposes standard convolutions into depthwise convolutions and pointwise convolutions, thereby reducing computational complexity and enhancing model performance. Therefore, we propose combining the multi-scale feature extraction of the Inception module with the efficient computation of Deep Separable Convolution to achieve the fusion and refinement of multi-scale features [ 35 ]. Figure 3 illustrates a multi-branch convolutional module architecture designed to enhance the network&#8217;s feature extraction capabilities through Deep Separable Convolution (DSConv) and feature fusion operations. For the input, it is first divided into five groups along the channel dimension: (4) X 1 , X 2 , X 3 , X 4 , X id = Split ( X ) = X : g , X g : 2 g , X 2 g : 3 g , X 3 g : 4 g , X 4 g : , where Split ( &#183; ) denotes grouping and splitting along the channel dimension, g represents the number of channels in each group, X : g indicates taking the first g channels of X , X g : 2 g indicates taking the second g channels of X , and so on. Then, these five features are processed through five distinct operators: (5) X 1 &#8242; = DSConv 3 &#215; 3 &#215; 3 g &#8594; g ( X 1 ) , (6) X 2 &#8242; = DSConv 1 &#215; 1 &#215; 11 g &#8594; g ( X 2 ) , (7) X 3 &#8242; = DSConv 1 &#215; 11 &#215; 1 g &#8594; g ( X 3 ) , (8) X 4 &#8242; = DSConv 11 &#215; 1 &#215; 1 g &#8594; g ( X 4 ) , (9) X id &#8242; = X id , where DSConv k 1 &#215; k 2 &#215; k 3 g &#8594; g denotes a separable convolution with depth, where both input and output channels are of size g and the kernel size is k 1 &#215; k 2 &#215; k 3 , and X id &#8242; = X id denotes the &#8220;identity mapping&#8221;. Finally, the outputs from each branch are concatenated: (10) X &#8242; = Concat ( X 1 &#8242; , X 2 &#8242; , X 3 &#8242; , X 4 &#8242; , X id &#8242; ) . Our proposed InceptionDSConv3d module utilizes a multi-branch architecture combined with separable convolutional kernels of varying sizes to enable fine-grained feature processing across multiple dimensions. Simultaneously, deep separable convolutions substantially reduce parameter count and computational complexity, making the decoder more efficient while maintaining performance. This design is particularly well suited for restoring minute pores and complex boundaries in rock images. By capturing multi-scale contextual information and fusing features from different directions, it significantly enhances the decoder&#8217;s ability to reconstruct spatial details. 3.4. The BasicResDWSBlock&#160;Module Building upon the above work, to significantly reduce the number of parameters and FLOPs in the model, we propose the BasicResDWSBlock module by drawing inspiration from the Deep Separable Convolution approach [ 27 ]. Figure 4 depicts the BasicResDWSBlock module&#8217;s structure. In this module, depth-separable convolution is used in place of regular convolution. In contrast to a standard convolution, a depth-separable convolution will first convolve each channel of the input feature map independently in spatial dimension before combining the channels linearly by pointwise convolution. Assume that the convolution kernel&#8217;s size in 3D is K x &#215; K y &#215; K z , the input feature map&#8217;s size is D &#215; H &#215; W , the number of input channels is C i n , and the number of output channels is C o u t . Then, the parametric quantities P c o n v and FLOPs F c o n v of the ordinary 3D convolution are as in Equations (11) and (12), (11) P c o n v = C i n &#215; C o u t &#215; K x &#215; K y &#215; K z , (12) F c o n v = D &#215; H &#215; W &#215; C i n &#215; C o u t &#215; K x &#215; K y &#215; K z , and the parametric quantities P d w s and FLOPs F d w s of the 3D depth-separable convolution are as in Equations (13) and (14), (13) P d w s = C i n &#215; K x &#215; K y &#215; K z + C i n &#215; C o u t , (14) F d w s = D &#215; H &#215; W &#215; C i n &#215; K x &#215; K y &#215; K z + D &#215; H &#215; W &#215; C i n &#215; C o u t , comparison of the formulas allows visualization of the changes in the number of parameters and calculations, which not only drastically reduces the number of parameters and calculations, but also speeds up the training and reasoning process. We also refer to the method of Hu et al. [ 36 ] to include the SE attention mechanism after the second depth-separable convolutional block of the BasicResDWSBlock module, and determined under several experiments that the channel compression parameter of the SE module is set to 16, and the number of compressed channels, C s q u e e z e d , is given by Equation ( 15 ), (15) C squeezed = C in / 16 , after operations such as compression and activation functions, the channels are then expanded back for weighting operations on the original feature map. 4. Experiments and&#160;Results 4.1. Data Acquisition and&#160;Labeling As shown in Figure 5 , we utilized two datasets: one private dataset, Shale Images (dimensions 200 &#215; 512 &#215; 512), and another public dataset, Leopard Sandstone Images (dimensions 1000 &#215; 1000 &#215; 1000), sourced from the Digital Rock website [ 18 ]. During the annotation process of private data, the raw data is first sliced along a specific dimension to yield 200 two-dimensional images measuring 512 &#215; 512 pixels. These images are then manually annotated using tools like LabelMe (v4.5.0) [ 37 ] and categorized into two classes: rock and pore. After annotation, a Python (v3.10.16) program converts multiple two-dimensional images into a three-dimensional format. Finally, the labels are saved in the .nii.gz format, suitable for deep learning training. In contrast, the raw and annotated data for the publicly available Leopard Sandstone Image dataset originate from the open-access Digital Rock website. Compared to this public dataset, our annotation process explicitly marks elongated pores and rigorously verifies annotation results. This distinction represents the core value of our dataset relative to the publicly available one. Due to the limited amount of data, we propose using a slicing method to create the dataset. After slicing the private dataset, Shale Images, using a Python program, we obtained 1024 image samples with dimensions of 200 &#215; 200 &#215; 200. The public Leopard Sandstone Images dataset was segmented using a Python program, yielding 216 image samples with dimensions of 250 &#215; 250 &#215; 250. Figure 6 illustrates selected segmented samples from both the Shale Images and Leopard Sandstone Images datasets. As indicated in Table 1 , we combine the two produced datasets into distinct resolutions and randomly separate them into training, validation, and test sets in a 4:1:1 ratio to guarantee that each set has rich pore distributions in order to increase the segmentation accuracy. 4.2. Experimental&#160;Setup Our experimental environment configuration is shown in Table 2 . The hyperparameter settings for the training phase include a batch size of 64 and a training cycle count of 500 epochs. Hyperparameters for the model training phase include an input image size of 200 &#215; 200 &#215; 200, an SGD optimizer (Meta Platforms, Inc., Menlo Park, CA, USA), a learning rate initialized at 0.001, and momentum decay and weight decay values set to 0.99 and 0.00003, respectively. During model training, a learning rate decay strategy was utilized, with all other hyperparameters set to the default values of U-Mamba-Bot. 4.3. Assessment of&#160;Indicators In this study, we utilized the Dice similarity coefficient to evaluate the degree of overlap similarity between predicted and actual results, used IOU to measure the proportion of overlap between predicted and actual regions, and assessed the accuracy of the model&#8217;s predictions using the Accuracy metric [ 38 ]. To evaluate the scale and computational load of each model, we expressed the total number of floating-point operations (FLOPs) as the ratio of all floating-point operations to the time required to complete the task. In addition to the above performance metrics, we also calculated the number of network parameters (Params) and the inference time (Time) of the model on the test set. 4.4. Comparison of Different Segmentation&#160;Networks We chose seven networks as comparison networks to assess the efficacy of our proposed approach. These include the most recent Mamba-based segmentation networks (U-Mamba-Bot and U-Mamba-Enc) and CNN-based techniques (3D U-Net, 3D SegNet, 3D U-ResNet, 3D KiUNet, and nnUNet). We train the aforementioned networks using the above two datasets in the same server environment, equipped with an NVIDIA GeForce RTX 4090 24 G GPU. Following training, performance metrics like accuracy, precision accuracy, and Dice similarity coefficient are computed to assess how well the model separates the segmented area from the background. For precise assessment of pore-scale properties [ 39 ], including fluid saturation, pore connectivity, and porosity, accurate segmentation is essential. We carefully designed the experimental methodology to periodically evaluate the performance of the model during training and saved checkpoints at the point of minimum validation loss to ensure that the most efficient version of the model was retained for subsequent use or further improvement. Table 3 shows the results of the comparison experiments using the dataset Shale Images, and Table 4 shows the results of the comparison experiments using the dataset Leopard Sandstone Images. In comparison to 3D U-Net, 3D SegNet, 3D U-ResNet, 3D KiUNet, nnUNet, U-Mamba-Enc, and U-Mamba-Bot, our proposed approach better balances computational cost and performance among the benchmarks and produces excellent results with the same hyperparameters. While the amount of parameters, FLOPs, and inference time are relatively low, 3D U-Net, 3D SegNet, 3D U-ResNet, and 3D KiUNet exhibit poor segmentation accuracy and precision. In terms of segmentation accuracy and precision, UNet, U-Mamba-Enc, and U-Mamba-Bot outperform models like 3D U-Net; nevertheless, the model complexity is comparatively large. On the other hand, our enhanced model&#8217;s parameters, FLOPs, and inference time are, respectively, 66.83 % , 56.19 % and 50.41 % lower than those of the baseline. In addition to being smaller than the baseline, our new model also has better accuracy, intersection ratio, and similarity coefficient. The comparison results show that for the pore segmentation task of 3D rock images, the method proposed in this paper can more efficiently balance the model&#8217;s lightweight characteristics and segmentation accuracy, which fully demonstrates the model&#8217;s suitability for real-time environments or resource-constrained environments. Figure 7 shows some of the segmentation results of the above models on the Shale Images test set and the Leopard Sandstone Images test set, and the visualization of the segmentation results shows that our proposed network not only accurately extracts the narrow and curved throat portion and extracts a more complete pore structure, but our proposed method also has stronger robustness and has fewer segmentation outliers. 4.5. Ablation&#160;Experiments We created five sets of ablation experiments to compare the impact of the different improvement techniques on the model performance for two datasets, Shale Images and Leopard Sandstone Images, in order to assess the efficacy of the enhanced algorithms and demonstrate the model&#8217;s capacity for generalization. To guarantee comparable findings, we maintained the other hyperparameters at their default settings and utilized the same tools and datasets for the studies. The results of the ablation experiments using the Shale Images dataset are displayed in Table 5 , and the results of the ablation experiments using the Leopard Sandstone Images dataset are displayed in Table 6 . A indicates the use of the BasicResDWSBlock module, B indicates the use of the LDLK module, and C indicates the use of the InceptionDSConv3d module. The first row uses U-Mamba-Bot as the baseline, and each module can be added to the model separately. To start, the LDLK module is used in the first three stages of the baseline encoder after taking into account Dice, accuracy, IOU, the number of parameters, and FLOPs. This module focuses on capturing global contextual information with dynamic large kernels to better model long-range 3D pore structures, so the ablation experiments&#8217; results indicate that while the model&#8217;s number of parameters and FLOPs is slightly increased, Dice, accuracy, and IOU are improved. The InceptionDSConv3d module performs multi-scale feature fusion and refinement to accurately restore fine-grained pore boundaries, which is then used in the final two stages of the baseline decoder in order to improve the Dice, accuracy, and IOU metrics with the least amount of computational cost. The ablation experiments&#8217; results indicate that the model&#8217;s number of parameters and FLOPs are slightly higher than the baseline, with an increase in Dice, accuracy, and IOU. Building upon the above work, to significantly reduce the number of parameters and FLOPs in the model, we propose the BasicResDWSBlock module. Ablation experiments demonstrate that BasicResDWSBlock reduces parameters and floating-point operations while maintaining accuracy through deep separable convolutions and SE attention mechanisms. Ultimately, we integrate all the enhancements to attain the best equilibrium outcomes. In comparison to the baseline, the LDLK-U-Mamba model&#8217;s parameters and FLOPs are reduced by 28.09 M and 547.27 GFLOPs, respectively, while accuracy, Dice, and IOU are improved by 0.12 % , 1.25 % , and 2.35 % , respectively, according to the results of the ablation experiments conducted using the Shale Images dataset. In comparison to the baseline, the LDLK-U-Mamba model&#8217;s parameters and FLOPs decreased by 28.15 M and 547.27 GFLOPs, respectively, while accuracy, Dice, and IOU improved by 1.18 % , 0.73 % , and 1.39 % , respectively, according to the results of the ablation experiments conducted with the dataset Leopard Sandstone Images. 5. Conclusions In this study, we propose a lightweight 3D rock image pore segmentation method based on the Mamba architecture, termed the LDLK-U-Mamba model. To enhance segmentation performance, we propose the LDLK module during the encoding phase to capture global contextual information, and utilize the InceptionDSConv3d module during decoding to fuse and refine multi-scale features, yielding more accurate segmentation results. We also propose the BasicResDWSBlock module, utilizing deep separable convolutions and the SE module to reduce model parameters and computational complexity. To evaluate the model&#8217;s performance, we tested segmentation accuracy, Dice score, intersection-over-union ratio, parameter count, FLOPs, and inference time. Segmentation results on the Leopard Sandstone Images dataset achieved a 99.38% accuracy, a 99.62% Dice score, and a 99.25% intersection-over-union ratio. The parameter count decreased from 42.12 M to 13.97 M, and FLOPs decreased from 973.91 G to 426.64 G. Experimental results demonstrate that the LDLK-U-Mamba model outperforms current mainstream segmentation methods across multiple key metrics. Though our method demonstrates superior performance in 3D porous rock segmentation, it is difficult to generalize to other 3D segmentation tasks in different domains due to the inevitable domain differences. To address this, we plan to utilize transfer learning and fine-tuning strategies to optimize the model to enhance its generalizability for achieving 3D segmentation tasks in other domains. Acknowledgments The authors express their gratitude to China University of Petroleum (East China). Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, G.C. and H.L.; software, H.L.; investigation, H.L.; writing&#8212;original draft preparation, H.L.; writing&#8212;review and editing, C.L. and P.L.; supervision, Y.K.; project administration, C.L. and P.L. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement The original contributions presented in the study are included in the article. Further inquiries can be directed at the corresponding author. Conflicts of Interest The authors declare no conflicts of interest. References 1. Karimpouli S. Tahmasebi P. Segmentation of digital rock images using deep convolutional autoencoder networks Comput. Geosci. 2019 126 142 150 10.1016/j.cageo.2019.02.003 2. Varfolomeev I. Yakimchuk I. Safonov I. An Application of Deep Neural Networks for Segmentation of Microtomographic Images of Rock Samples Computers 2019 8 72 10.3390/computers8040072 3. Molina D. Tassara A. Abarca R. Melnick D. Madella A. Frictional Segmentation of the Chilean Megathrust from a Multivariate Analysis of Geophysical, Geological and Geodetic Data J. Geophys. Res. Solid Earth 2021 126 e2020JB020647 10.1029/2020JB020647 4. Liu R. Wu J. Lu W. Miao Q. Zhang H. Liu X. Lu Z. Li L. A Review of Deep Learning-Based Methods for Road Extraction from High-Resolution Remote Sensing Images Remote Sens. 2024 16 2056 10.3390/rs16122056 5. Liu F. Advances in Medical Image Segmentation: A Comprehensive Review of Traditional, Deep Learning and Hybrid Approaches Bioengineering 2024 11 1034 10.3390/bioengineering11101034 39451409 PMC11505408 6. &#199;i&#231;ek &#214;. Abdulkadir A. Lienkamp S.S. Brox T. Ronneberger O. 3D U-Net: Learning dense volumetric segmentation from sparse annotation Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2016: 19th International Conference Athens, Greece 17&#8211;21 October 2016 Part II 19 Springer Berlin/Heidelberg, Germany 2016 424 432 7. Oktay O. Attention u-net: Learning where to look for the pancreas arXiv 2018 10.48550/arXiv.1804.03999 1804.03999 8. Wang Y.D. Shabaninejad M. Armstrong R.T. Mostaghimi P. Deep neural networks for improving physical accuracy of 2D and 3D multi-mineral segmentation of rock micro-CT images Appl. Soft Comput. 2021 104 107185 10.1016/j.asoc.2021.107185 9. Wang C. Zhao Z. Ren Q. Xu Y. Yu Y. Dense U-net based on patch-based learning for retinal vessel segmentation Entropy 2019 21 168 10.3390/e21020168 33266884 PMC7514650 10. Chen G. Chen H. Cui T. Li H. SFMRNet: Specific Feature Fusion and Multibranch Feature Refinement Network for Land Use Classification Sel. Top. Appl. Earth Obs. Remote Sens. IEEE J. 2024 17 16206 16221 10.1109/JSTARS.2024.3456842 11. Chen J. Lu Y. Yu Q. Luo X. Zhou Y. TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation arXiv 2021 10.48550/arXiv.2102.04306 2102.04306 12. Dong S. A Separate 3D-SegNet Based on Priority Queue for Brain Tumor Segmentation Proceedings of the 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC) Hangzhou, China 22&#8211;23 August 2020 IEEE Piscataway, NJ, USA 2020 13. Valanarasu J.M.J. Sindagi V.A. Hacihaliloglu I. Patel V.M. KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation IEEE Trans. Med. Imaging 2020 41 965 976 10.1109/TMI.2021.3130469 34813472 14. Isensee F. Jaeger P.F. Kohl S.A. Petersen J. Maier-Hein K.H. nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation Nat. Methods 2021 18 203 211 10.1038/s41592-020-01008-z 33288961 15. Milletari F. Navab N. Ahmadi S.A. V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation Proceedings of the 2016 Fourth International Conference on 3D Vision Stanford, CA, USA 25&#8211;28 October 2016 IEEE Piscataway, NJ, USA 2016 16. Hatamizadeh A. Yang D. Roth H. Xu D. UNETR: Transformers for 3D Medical Image Segmentation Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision Virtual 3&#8211;8 January 2021 17. Ma J. Li F. Wang B. U-mamba: Enhancing long-range dependency for biomedical image segmentation arXiv 2024 2401.04722 18. Ma Z. He X. Kwak H. Gao J. Sun S. Yan B. Enhancing rock image segmentation in digital rock physics: A fusion of generative ai and state-of-the-art neural networks arXiv 2023 2311.06079 19. Huang C. Zhang X. Liu S. Li N. Kang J. Xiong G. Construction of pore structure and lithology of digital rock physics based on laboratory experiments J. Pet. Explor. Prod. Technol. 2021 11 2113 2125 10.1007/s13202-021-01149-7 20. Guo Q. Wang Y. Yang S. Xiang Z. A method of blasted rock image segmentation based on improved watershed algorithm Sci. Rep. 2022 12 7143 10.1038/s41598-022-11351-0 35505086 PMC9065013 21. Vincent L. Soille P. Watersheds in digital spaces: An efficient algorithm based on immersion simulations IEEE Trans. Pattern Anal. Mach. Intell. 1991 13 583 598 10.1109/34.87344 22. Reinhardt M. Jacob A. Sadeghnejad S. Cappuccio F. Arnold P. Frank S. Enzmann F. Kersten M. Benchmarking conventional and machine learning segmentation techniques for digital rock physics analysis of fractured rocks Environ. Earth Sci. 2022 81 71 10.1007/s12665-021-10133-7 23. Zunair H. Hamza A.B. Masked supervised learning for semantic segmentation arXiv 2022 10.48550/arXiv.2210.00923 2210.00923 24. Wang Y.D. Shabaninejad M. Armstrong R.T. Mostaghimi P. Physical Accuracy of Deep Neural Networks for 2D and 3D Multi- Mineral Segmentation of Rock micro-CT Images arXiv 2020 2002.05322 10.1016/j.asoc.2021.107185 25. Szegedy C. Liu W. Jia Y. Sermanet P. Reed S. Anguelov D. Erhan D. Vanhoucke V. Rabinovich A. Going deeper with convolutions Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Boston, MA, USA 7&#8211;12 June 2015 1 9 26. Szegedy C. Vanhoucke V. Ioffe S. Shlens J. Wojna Z. Rethinking the inception architecture for computer vision Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Las Vegas, NV, USA 27&#8211;30 June 27 2016 2818 2826 27. Chollet F. Xception: Deep learning with depthwise separable convolutions Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Honolulu, HI, USA 21&#8211;26 July 2017 1251 1258 28. Howard A.G. Mobilenets: Efficient convolutional neural networks for mobile vision applications arXiv 2017 10.48550/arXiv.1704.04861 1704.04861 29. Chen L.C. Papandreou G. Kokkinos I. Murphy K. Yuille A.L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs IEEE Trans. Pattern Anal. Mach. Intell. 2017 40 834 848 10.1109/TPAMI.2017.2699184 28463186 30. Zhao H. Shi J. Qi X. Wang X. Jia J. Pyramid scene parsing network Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Honolulu, HI, USA 21&#8211;26 July 2017 2881 2890 31. Wang W. Chen C. Ding M. Li J. Zha S. TransBTS: Multimodal Brain Tumor Segmentation Using Transformer Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention Strasbourg, France 27 September&#8211;1 October 2021 32. Yang J. Qiu P. Zhang Y. Marcus D.S. Sotiras A. D-net: Dynamic large kernel with dynamic feature fusion for volumetric medical image segmentation arXiv 2024 2403.10674 10.1016/j.bspc.2025.108837 33. Zhang Z. Li J. Shao W. Peng Z. Zhang R. Wang X. Luo P. Differentiable learning-to-group channels via groupable convolutional neural networks Proceedings of the IEEE/CVF International Conference on Computer Vision Seoul, Republic of Korea 27 October&#8211;2 November 2019 3542 3551 34. Han D. Yun S. Heo B. Yoo Y. Rethinking channel dimensions for efficient model design Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition Nashville, TN, USA 20&#8211;25 June 2021 732 741 35. Yu W. Zhou P. Yan S. Wang X. InceptionNeXt: When Inception Meets ConvNeXt Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Seattle, WA, USA 16&#8211;22 June 2024 IEEE Piscataway, NJ, USA 2024 36. Hu J. Shen L. Sun G. Squeeze-and-excitation networks Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Salt Lake City, UT, USA 18&#8211;22 June 2018 7132 7141 37. Russell B.C. Torralba A. Murphy K.P. Freeman W.T. LabelMe: A Database and Web-Based Tool for Image Annotation Int. J. Comput. Vis. 2008 77 157 173 10.1007/s11263-007-0090-8 38. Marques V.G. Silva L.R.D.D. Carvalho B.M. Lucena L.R.F.D. Vieira M.M. Deep Learning-Based Pore Segmentation of Thin Rock Sections for Aquifer Characterization Using Color Space Reduction Proceedings of the 2019 International Conference on Systems, Signals and Image Processing (IWSSIP) Osijek, Croatia 5&#8211;7 June 2019 39. Purswani P. Karpyn Z.T. Enab K. Xue Y. Huang X. Evaluation of image segmentation techniques for image-based rock property estimation J. Pet. Sci. Eng. 2020 195 107890 10.1016/j.petrol.2020.107890 Figure 1 The structure of our LDLK-U-Mamba. The model includes an encoder, a decoder, and skip connections. The LDLK module in the encoder captures global contextual information, while the InceptionDSConv3d module in the decoder achieves more accurate segmentation results through multi-scale feature fusion and refinement. The BasicResDWSBlock module employs separable convolutions and the SE module to significantly reduce the model&#8217;s parameter count and computational complexity. The BasicBlock acts as a basic feature extraction unit with stacked 3D convolutions, instance normalization, and activation for 3D feature capture. Figure 2 The LDLK module. This module includes Reduce Channels, a RELU, and an LDLK. Reduce Channels and ReLU effectively reduce computational load, while LDLK&#8217;s dynamic group convolution enables dynamic feature fusion with low computational overhead, efficiently utilizing global contextual information. Figure 3 The InceptionDSConv3d module. This module divides the input channel dimensions into four depthwise separable convolutional branches and one identity branch, with the outputs of each branch ultimately concatenated. This multi-branch architecture, combined with depthwise separable convolutional kernels of varying sizes, enables fine-grained feature processing across multiple dimensions. Figure 4 The BasicResDWSBlock module. This module adopts depthwise separable convolutions instead of standard convolutions and incorporates a separable activation layer after the second separable convolution block, achieving a significant reduction in model parameters and FLOPs. Figure 5 ( a ) &#181;CT image of shale image. ( b ) &#181;CT image of leopard sandstone image. Figure 6 ( a ) shale images. ( b ) leopard sandstone images. Figure 7 Qualitative comparison between LDLK-U-Mamba, U-Mamba-Bot, U-Mamba-Enc, nnUNet, 3D U-Net, 3D U-ResNet, 3D SegNet, and 3D KiUNet. Additionally, LDLK-U-Mamba shows better segmentation quality than others in terms of the segmentation of elongated pores and the integrity of the pore structure. sensors-25-07039-t001_Table 1 Table 1 Dataset division. Dataset Resolution Train Validation Test Total Shale Images 200&#160;&#215;&#160;120&#160;&#215;&#160;120 683 171 170 1024 Leopard Sandstone Images 250&#160;&#215;&#160;250&#160;&#215;&#160;250 144 36 36 216 sensors-25-07039-t002_Table 2 Table 2 Configuration of the experimental environment. Category Configuration GPU NVIDIA GeForce RTX 4090 24 G System environment Ubuntu 20.04 Torch version 2.6.0 + cu118 Programming language Python 3.10.16 sensors-25-07039-t003_Table 3 Table 3 Comparative experiment results utilizing the shale images dataset. Bold represents the best results, and underline represents the second-best results. Method Accuracy (%) &#8593; Dice (%) &#8593; IOU (%) &#8593; Params (M) &#8595; FLOPs (G) &#8595; Time (ms) &#8595; 3D U-Net&#160;[ 6 ] 98.13 82.74 70.74 2.32 117.98 82.5 3D U-ResNet&#160;[ 24 ] 94.84 35.12 21.37 9.49 1699.84 1180.3 3D SegNet&#160;[ 12 ] 73.40 26.10 15.08 2.33 116.52 76.8 3D KiUNet&#160;[ 13 ] 94.57 55.90 39.09 2.33 130.17 89.2 nnUNet&#160;[ 14 ] 99.32 93.32 87.51 31.19 534.03 365.7 U-Mamba-Enc&#160;[ 17 ] 98.79 87.58 78.07 42.75 990.65 620.4 U-Mamba-Bot&#160;[ 17 ] 99.34 93.50 87.84 42.12 973.91 598.1 Ours 99.46 94.67 89.90 14.03 426.64 295.3 sensors-25-07039-t004_Table 4 Table 4 Comparative experiment results utilizing the leopard sandstone images dataset. Bold represents the best results, and underline represents the second-best results. Method Accuracy (%) &#8593; Dice (%) &#8593; IOU (%) &#8593; Params (M) &#8595; FLOPs (G) &#8595; Time (ms) &#8595; 3D U-Net&#160;[ 6 ] 88.77 92.90 86.74 2.32 117.98 83.2 3D U-ResNet&#160;[ 24 ] 86.32 91.36 84.10 9.49 1660.00 1165.8 3D SegNet&#160;[ 12 ] 65.44 75.55 60.74 2.33 116.54 77.4 3D KiUNet&#160;[ 13 ] 57.72 67.08 50.54 2.33 178.73 91.6 nnUNet&#160;[ 14 ] 98.04 98.80 97.66 31.19 534.03 368.2 U-Mamba-Enc&#160;[ 17 ] 81.13 89.55 81.13 42.75 990.65 625.1 U-Mamba-Bot&#160;[ 17 ] 98.22 98.90 97.89 42.12 973.91 602.5 Ours 99.38 99.62 99.25 13.97 426.64 298.8 sensors-25-07039-t005_Table 5 Table 5 Results of the ablation experiments utilizing the dataset, Shale Images. Bold represents the best results. A B C Accuracy (%) &#8593; Dice (%) &#8593; IOU (%) &#8593; Params (M) &#8595; FLOPs (G) &#8595; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; 99.34 93.50 87.84 42.12 973.91 &#10003; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; 99.32 93.28 87.45 13.96 425.28 &#160;&#160;&#160;&#160; &#10003; &#160;&#160;&#160;&#160; 99.43 94.39 89.40 42.12 975.00 &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; &#10003; 99.42 94.25 89.16 42.12 974.18 &#10003; &#10003; &#10003; 99.46 94.67 89.90 14.03 426.64 sensors-25-07039-t006_Table 6 Table 6 Results of the ablation experiments utilizing the dataset, Leopard Sandstone Images. Bold represents the best results. A B C Accuracy (%) &#8593; Dice (%) &#8593; IOU (%) &#8593; Params (M) &#8595; FLOPs (G) &#8595; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; 98.22 98.90 97.89 42.12 973.91 &#10003; &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; 97.27 98.31 96.76 13.96 425.28 &#160;&#160;&#160;&#160; &#10003; &#160;&#160;&#160;&#160; 99.37 99.61 99.23 42.12 974.78 &#160;&#160;&#160;&#160; &#160;&#160;&#160;&#160; &#10003; 99.34 99.60 99.20 42.12 974.18 &#10003; &#10003; &#10003; 99.38 99.62 99.25 13.97 426.64"
}