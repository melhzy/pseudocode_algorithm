{
  "pmcid": "PMC12654947",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:32.094289",
  "metadata": {
    "journal_title": "Journal of Biomedical Optics",
    "journal_nlm_ta": "J Biomed Opt",
    "journal_iso_abbrev": "J Biomed Opt",
    "journal": "Journal of Biomedical Optics",
    "pmcid": "PMC12654947",
    "pmid": "41312519",
    "doi": "10.1117/1.JBO.30.11.116004",
    "title": "Transformer-based optical attenuation compensation and denoising in photoacoustic imaging",
    "year": "2025",
    "month": "11",
    "day": "26",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "26"
    },
    "authors": [
      "Jensen Cristian Perez",
      "Awasthi Navchetan",
      "Francis Kalloor Joseph"
    ],
    "abstract": "Abstract. Significance Linear-array-based photoacoustic imaging (PAI) combines functional imaging with structural imaging from ultrasound. However, it suffers from depth-dependent optical attenuation due to surface illumination, resulting in decreased signal amplitude and image contrast with depth. Existing attenuation compensation methods often amplify noise, creating a trade-off between depth enhancement and image quality. Aim We aim to develop a deep learning method that addresses the coupled problem of optical attenuation compensation and denoising for linear-array-based PAI and test the applicability  in vivo . Approach We propose a vision-transformer-based generative model to address this coupled problem. A diverse dataset was created using simulated data and experimental twin phantoms. Vascular twin phantoms were made by printing digital images onto polyurethane films to test the performance of the model. We trained and compared three deep learning architectures, Pix2Pix, Residual U-Net, and the proposed Transformer U-Net, using various loss functions, including adversarial, MSE, PSNR, SSIM, and a combined PSNR + SSIM. We tested the model on small animal tumor images. Results Quantitative evaluation shows that PSNR + SSIM loss is robust in preserving structural details and suppressing noise. Under the pre-specified SSIM + PSNR training objective, Trans U-Net achieves the highest SSIM and PSNR across noise levels on both datasets.  In vivo  validation using murine breast tumor models and  in vivo  breast imaging confirmed the modelâ€™s ability to enhance visualization of deep vascular structures without introducing noise amplification. Conclusions The proposed Trans U-Net effectively addresses the coupled problem of attenuation correction and denoising in handheld PAI. This method improves depth-resolved vascular imaging and is potentially useful in clinical and preclinical photoacoustic applications.",
    "keywords": [
      "photoacoustic imaging",
      "deep learning models",
      "Transformer U-Net",
      "fluence compensation",
      "image quality enhancement",
      "tumor imaging"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">J Biomed Opt</journal-id><journal-id journal-id-type=\"iso-abbrev\">J Biomed Opt</journal-id><journal-id journal-id-type=\"pmc-domain-id\">953</journal-id><journal-id journal-id-type=\"pmc-domain\">jbiomedopt</journal-id><journal-id journal-id-type=\"publisher-id\">JBO</journal-id><journal-title-group><journal-title>Journal of Biomedical Optics</journal-title></journal-title-group><issn pub-type=\"ppub\">1083-3668</issn><issn pub-type=\"epub\">1560-2281</issn><publisher><publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12654947</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12654947.1</article-id><article-id pub-id-type=\"pmcaid\">12654947</article-id><article-id pub-id-type=\"pmcaiid\">12654947</article-id><article-id pub-id-type=\"pmid\">41312519</article-id><article-id pub-id-type=\"doi\">10.1117/1.JBO.30.11.116004</article-id><article-id pub-id-type=\"publisher-id\">250222GRR</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Imaging</subject></subj-group><subj-group subj-group-type=\"SPIE-art-type\"><subject>Paper</subject></subj-group></article-categories><title-group><article-title>Transformer-based optical attenuation compensation and denoising in photoacoustic imaging</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Jensen</surname><given-names initials=\"CP\">Cristian Perez</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">a</xref><xref rid=\"b1\" ref-type=\"bio\"/><email>cristianpjensen@gmail.com</email></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0001-8153-2786</contrib-id><name name-style=\"western\"><surname>Awasthi</surname><given-names initials=\"N\">Navchetan</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">a</xref><xref rid=\"aff2\" ref-type=\"aff\">b</xref><xref rid=\"cor1\" ref-type=\"corresp\">*</xref><xref rid=\"b2\" ref-type=\"bio\"/><xref rid=\"fn1\" ref-type=\"author-notes\">&#8224;</xref><email>navchetanawasthi@gmail.com</email></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0001-9766-7140</contrib-id><name name-style=\"western\"><surname>Francis</surname><given-names initials=\"KJ\">Kalloor Joseph</given-names></name><xref rid=\"aff3\" ref-type=\"aff\">c</xref><xref rid=\"cor1\" ref-type=\"corresp\">*</xref><xref rid=\"fn1\" ref-type=\"author-notes\">&#8224;</xref><xref rid=\"b3\" ref-type=\"bio\"/><email>f.kalloorjoseph@erasmusmc.nl</email></contrib><aff id=\"aff1\"><label>a</label><institution>University of Amsterdam</institution>, Informatics Institute, Faculty of Science, Mathematics and Computer Science, Amsterdam, <country>The Netherlands</country></aff><aff id=\"aff2\"><label>b</label><institution>Amsterdam UMC</institution>, Department of Biomedical Engineering and Physics, Amsterdam, <country>The Netherlands</country></aff><aff id=\"aff3\"><label>c</label><institution>Erasmus MC</institution>, Cardiovascular Institute, Department of Cardiology, Biomedical Engineering, Rotterdam, <country>The Netherlands</country></aff></contrib-group><author-notes><corresp id=\"cor1\"><label>*</label>Address all correspondence to Navchetan Awasthi, <email>n.awasthi@uva.nl</email>; Kalloor Joseph Francis, <email>f.kalloorjoseph@erasmusmc.nl</email></corresp><fn id=\"fn1\"><label>&#8224;</label><p>Equal Contribution</p></fn></author-notes><pub-date pub-type=\"epub\"><day>26</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"ppub\"><month>11</month><year>2025</year></pub-date><volume>30</volume><issue>11</issue><issue-id pub-id-type=\"pmc-issue-id\">499866</issue-id><elocation-id>116004</elocation-id><history><date date-type=\"received\"><day>18</day><month>7</month><year>2025</year></date><date date-type=\"rev-recd\"><day>26</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>27</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>26</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Authors</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"JBO-030-116004.pdf\"/><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:title=\"pdf\" xlink:href=\"JBO_30_11_116004.pdf\"/><abstract><title>Abstract.</title><sec><title>Significance</title><p>Linear-array-based photoacoustic imaging (PAI) combines functional imaging with structural imaging from ultrasound. However, it suffers from depth-dependent optical attenuation due to surface illumination, resulting in decreased signal amplitude and image contrast with depth. Existing attenuation compensation methods often amplify noise, creating a trade-off between depth enhancement and image quality.</p></sec><sec><title>Aim</title><p>We aim to develop a deep learning method that addresses the coupled problem of optical attenuation compensation and denoising for linear-array-based PAI and test the applicability <italic toggle=\"yes\">in vivo</italic>.</p></sec><sec><title>Approach</title><p>We propose a vision-transformer-based generative model to address this coupled problem. A diverse dataset was created using simulated data and experimental twin phantoms. Vascular twin phantoms were made by printing digital images onto polyurethane films to test the performance of the model. We trained and compared three deep learning architectures, Pix2Pix, Residual U-Net, and the proposed Transformer U-Net, using various loss functions, including adversarial, MSE, PSNR, SSIM, and a combined PSNR + SSIM. We tested the model on small animal tumor images.</p></sec><sec><title>Results</title><p>Quantitative evaluation shows that PSNR + SSIM loss is robust in preserving structural details and suppressing noise. Under the pre-specified SSIM + PSNR training objective, Trans U-Net achieves the highest SSIM and PSNR across noise levels on both datasets. <italic toggle=\"yes\">In vivo</italic> validation using murine breast tumor models and <italic toggle=\"yes\">in vivo</italic> breast imaging confirmed the model&#8217;s ability to enhance visualization of deep vascular structures without introducing noise amplification.</p></sec><sec><title>Conclusions</title><p>The proposed Trans U-Net effectively addresses the coupled problem of attenuation correction and denoising in handheld PAI. This method improves depth-resolved vascular imaging and is potentially useful in clinical and preclinical photoacoustic applications.</p></sec></abstract><kwd-group><title>Keywords:</title><kwd>photoacoustic imaging</kwd><kwd>deep learning models</kwd><kwd>Transformer U-Net</kwd><kwd>fluence compensation</kwd><kwd>image quality enhancement</kwd><kwd>tumor imaging</kwd></kwd-group><funding-group><award-group id=\"sp1\"><funding-source>Dutch Research Council (NWO) for the project NWO-VENI</funding-source><award-id>19165</award-id></award-group><award-group id=\"sp2\"><funding-source>ZonMw for the project Off Road</funding-source><award-id>04510012210042</award-id></award-group><funding-statement>This work has received financial support from the Dutch Research Council (NWO) for the project NWO-VENI (Grant No. 19165) and ZonMw for the project Off Road (Grant No. 04510012210042).</funding-statement></funding-group><counts><fig-count count=\"7\"/><table-count count=\"1\"/><ref-count count=\"43\"/><page-count count=\"16\"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>running-head</meta-name><meta-value>Jensen, Awasthi, and Francis: Transformer-based optical attenuation&#8230;</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"sec1\"><label>1</label><title>Introduction</title><p>Photoacoustic imaging (PAI) is a promising biomedical imaging modality that combines optical contrast with ultrasonic spatial resolution.<xref rid=\"r1\" ref-type=\"bibr\"><sup>1</sup></xref> When integrated with conventional ultrasound systems, PAI enhances functional imaging capabilities, making it valuable for clinical applications such as cancer diagnosis, vascular imaging, and functional monitoring, including oxygen saturation.<xref rid=\"r2\" ref-type=\"bibr\"><sup>2</sup></xref> The probe configuration using a linear ultrasound array with an optical source is particularly popular due to its compatibility with existing ultrasound systems.<xref rid=\"r3\" ref-type=\"bibr\"><sup>3</sup></xref><named-content content-type=\"online\"><xref rid=\"r4\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r5\" ref-type=\"bibr\"><sup>5</sup></xref> Despite its potential, PAI with conventional ultrasound imaging systems faces significant challenges, including optical attenuation, limited target view, and the band-limited nature of the conventional linear transducers, all of which contribute to degraded image quality.<xref rid=\"r6\" ref-type=\"bibr\"><sup>6</sup></xref></p><p>Depth-dependent attenuation in PAI degrades deep-tissue contrast with the exponential decay of optical fluence. A common approach is to assume average tissue optical properties and invert fluence decay via the Beer&#8211;Lambert law, the diffusion approximation, or Monte Carlo simulations. In early work, Zhao et&#160;al.<xref rid=\"r7\" ref-type=\"bibr\"><sup>7</sup></xref> employed Monte Carlo&#8211;based light-transport simulations to generate pixel-wise fluence maps for real-time amplitude correction in a handheld probe. Although these model-based schemes enhance deep-tissue contrast, they amplify noise at greater depths.<xref rid=\"r7\" ref-type=\"bibr\"><sup>7</sup></xref><named-content content-type=\"online\"><xref rid=\"r8\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r9\" ref-type=\"bibr\"><sup>9</sup></xref> Hardware-augmented strategies combining diffuse optical tomography,<xref rid=\"r10\" ref-type=\"bibr\"><sup>10</sup></xref> acousto-optic tagging,<xref rid=\"r11\" ref-type=\"bibr\"><sup>11</sup></xref> or multi-illumination approaches<xref rid=\"r12\" ref-type=\"bibr\"><sup>12</sup></xref> can obtain a fluence map but require complex systems, larger probes, and computationally intensive inversions, limiting their clinical translation. Spectroscopic methods integrate wavelength-dependent diffusion models with reference-phantom calibration to provide real-time fluence correction alongside motion compensation.<xref rid=\"r12\" ref-type=\"bibr\"><sup>12</sup></xref><sup>,</sup><xref rid=\"r13\" ref-type=\"bibr\"><sup>13</sup></xref> Learning-based frameworks have recently emerged as an alternative by training deep networks to perform simultaneous depth-dependent optical attenuation correction and denoising in an end-to-end fashion.<xref rid=\"r14\" ref-type=\"bibr\"><sup>14</sup></xref> Implementations using U-Nets (including residual and transformer variants), Pix2Pix GANs, and attention mechanisms have demonstrated marked improvements across simulated, phantom, and <italic toggle=\"yes\">in vivo</italic> breast and brain imaging studies.<xref rid=\"r15\" ref-type=\"bibr\"><sup>15</sup></xref><named-content content-type=\"online\"><xref rid=\"r16\" ref-type=\"bibr\"/><xref rid=\"r17\" ref-type=\"bibr\"/></named-content><named-content content-type=\"print\"><sup>&#8211;</sup></named-content><xref rid=\"r18\" ref-type=\"bibr\"><sup>18</sup></xref> Several deep-learning architectures have been developed for photoacoustic fluence compensation and optical parameter estimation, including the Residual U-Net by Cai et&#160;al.,<xref rid=\"r19\" ref-type=\"bibr\"><sup>19</sup></xref> the deep residual recurrent U-Net (DR2U-Net) by Chang et&#160;al.,<xref rid=\"r20\" ref-type=\"bibr\"><sup>20</sup></xref> O-Net by Luke et&#160;al.,<xref rid=\"r21\" ref-type=\"bibr\"><sup>21</sup></xref> EDA-Net by Yang et&#160;al.,<xref rid=\"r22\" ref-type=\"bibr\"><sup>22</sup></xref> and various encoder&#8211;decoder combinations.<xref rid=\"r23\" ref-type=\"bibr\"><sup>23</sup></xref> Most prior deep-learning methods for PAI focus on denoising, contrast enhancement, or quantitative oxygenation mapping. Although effective in those tasks, they generally do not address depth-dependent optical attenuation in tomographic and handheld linear-array systems. Further, these works do not address the optical attenuation correction and denoising as a coupled problem. Our work fills this gap by introducing a transformer-based model that jointly compensates attenuation and suppresses noise, enabling robust depth-resolved vascular imaging.</p><p>In this work, we address the challenge of depth-dependent optical attenuation and noise amplification in PAI as a joint problem, using a vision transformer (ViT)-based Trans U-Net. To train and test the models, we developed a comprehensive dataset that bridges simulation and experiment. It includes: (1)&#160;simulated photoacoustic images using combined optical and acoustic modeling, with uniform fluence images as ground truth and their counterparts incorporating realistic light propagation in tissue-mimicking media; (2)&#160;experimental images of printed phantoms acquired in water as a non-scattering medium as reference images and in tissue-mimicking scattering phantoms, capturing real-world complexity of imaging using a handheld probe. By utilizing the same digital phantoms and recreating them through printing, we created &#8220;digital and experimental twins,&#8221; enabling controlled validation against known ground truth. We systematically evaluated a set of conditional generative models, Pix2Pix, lightweight Residual U-Net (Res18), and also proposed a Transformer U-Net model, across different measurement noise levels and experimental setups, including <italic toggle=\"yes\">in vivo</italic> data. We also explored multiple loss functions, including adversarial loss, mean squared error (MSE), peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and a combined PSNR + SSIM formulation. Finally, we test the proposed model on small animal tumor imaging.</p></sec><sec id=\"sec2\"><label>2</label><title>Methods</title><p>We used two vascular datasets and optical simulations to model light propagation, followed by acoustic simulations and image reconstruction for our PAI dataset. We printed these images onto polyurethane films and imaged them with our PAI system to create experimental datasets. We generated both ideal and realistic PAI for model training and testing, and evaluated deep learning models and loss outputs with various image quality metrics. Subsequent sections provide details about the dataset generation, model training, and image evaluation.</p><sec id=\"sec2.1\"><label>2.1</label><title>Dataset Generation</title><p>The DRIVE dataset<xref rid=\"r24\" ref-type=\"bibr\"><sup>24</sup></xref> contains 48 segmented retinal vessel images, each transformed into five variants using augmentation, creating 240 images. From each image, we extracted five non-overlapping regions containing vasculature in different orientations, as shown in <xref rid=\"f1\" ref-type=\"fig\">Fig.&#160;1</xref>. To simulate extracorporeal illumination, a skin-like layer was added to the top of each cropped image. These skin layers were obtained by scanning the forearms of two volunteers at multiple locations using the same imaging system. The average thickness of the skin layer was <inline-formula><mml:math id=\"math1\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo form=\"prefix\">&#8764;</mml:mo><mml:mn>0.6</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>, which corresponds to about five pixels in the reconstructed images. The NNE dataset<xref rid=\"r25\" ref-type=\"bibr\"><sup>25</sup></xref> includes 9531 images from 2-photon single-vessel measurements of the mouse SI cortex. <xref rid=\"f1\" ref-type=\"fig\">Figure&#160;1</xref> outlines the dataset generation pipeline, showing simulation and experimental paths. Initially, vascular images are resized to <inline-formula><mml:math id=\"math2\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>40</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>40</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula>. The simulation path involves ideal simulations with uniform light fluence and realistic simulations accounting for fluence decay. In the experimental path, phantoms printed on polyurethane films are imaged in water and tissue-mimicking medium, generating both ideal and realistic PAI. The twin-phantom imaging setup is shown in Fig.&#160;S1 in the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1117/1.JBO.30.11.116004.s01\" ext-link-type=\"uri\">Supplementary Material</ext-link>.</p><fig position=\"float\" id=\"f1\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Simulation and experimental twin phantom. The pipeline shows the generation of the simulated and experimental dataset from the digital vascular image. Simulated phantoms consist of acoustic simulation and photoacoustic reconstruction with and without the added light fluence. Experimental phantoms are printed images on polyurethane films imaged in water and an absorbing and scattering medium.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g001.jpg\"/></fig><sec id=\"sec2.1.1\"><label>2.1.1</label><title>Optical simulation</title><p>We used the LED-based PAI system AcousticX (Cyberdyne Inc., Japan). Light propagation from the integrated LED arrays into tissue was simulated using the GPU-accelerated Monte Carlo eXtreme (MCX) photon transport simulator.<xref rid=\"r26\" ref-type=\"bibr\"><sup>26</sup></xref> The probe integrates two LED units mounted at a 41.4&#160;deg angle, positioned 0.15&#160;mm in front of the ultrasound transducer and spaced 9.56&#160;mm apart. Each unit measures <inline-formula><mml:math id=\"math3\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>50</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>10</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>mm</mml:mi></mml:mrow></mml:math></inline-formula> and contains four arrays of 144 LED elements (36 per row, 4 rows), emitting at 850&#160;nm with a 120&#160;deg opening angle.</p><p>The simulation domain was <inline-formula><mml:math id=\"math4\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>55</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>55</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>40</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, represented by a <inline-formula><mml:math id=\"math5\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>744</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>744</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>512</mml:mn><mml:mtext>&#8201;&#8201;voxel</mml:mtext></mml:mrow></mml:math></inline-formula> grid with a uniform voxel size of <inline-formula><mml:math id=\"math6\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>74</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:math></inline-formula>. Tissue-mimicking optical properties were set as: absorption coefficient <inline-formula><mml:math id=\"math7\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>0.01</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, reduced scattering coefficient <inline-formula><mml:math id=\"math8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and anisotropy factor 0.9. Simulations yielded fluence maps in the center plane between the LED arrays. These were normalized and multiplied with digital phantom images to generate initial pressure maps, assuming a constant Gr&#252;neisen parameter.</p></sec><sec id=\"sec2.1.2\"><label>2.1.2</label><title>Acoustic simulation and image reconstruction</title><p>Acoustic simulations were carried out using the k-Wave toolbox<xref rid=\"r27\" ref-type=\"bibr\"><sup>27</sup></xref> on a <inline-formula><mml:math id=\"math9\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>512</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula> grid. Input vascular images were resized and normalized. The corresponding fluence map was cropped and normalized, then multiplied by the image to generate the initial pressure map.</p><p>The transducer was modeled as a 128-element linear array with a pitch of 0.315&#160;mm and a center frequency of 7&#160;MHz (bandwidth 80%). Transducer directivity was accounted for via finite element size by summing up sensor data from multiple point sensors. The medium was assumed homogeneous with density <inline-formula><mml:math id=\"math10\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1000</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>kg</mml:mi><mml:mo stretchy=\"false\">/</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and sound speed <inline-formula><mml:math id=\"math11\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1500</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi mathvariant=\"normal\">m</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant=\"normal\">s</mml:mi></mml:mrow></mml:math></inline-formula>. Additive white noise ranging from 10 to 50&#160;dB (in 10&#160;dB steps) was added to simulate measurement noise. Image reconstruction was performed using a Fourier domain algorithm.<xref rid=\"r28\" ref-type=\"bibr\"><sup>28</sup></xref></p></sec><sec id=\"sec2.1.3\"><label>2.1.3</label><title>Phantom printing</title><p>Digital phantoms were fabricated using polyurethane, chosen for its suitable thermal, optical, and acoustic properties.<xref rid=\"r29\" ref-type=\"bibr\"><sup>29</sup></xref> The material tolerates the 175&#176;C fuser temperature of the Xerox 7800i printer, with a melting point of 180&#176;C. Its absorption coefficient ranges from 0.001 to <inline-formula><mml:math id=\"math12\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>0.005</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and its refractive index (1.41 to 1.58) closely matches soft tissue (1.35 to 1.55).<xref rid=\"r30\" ref-type=\"bibr\"><sup>30</sup></xref> Acoustic properties include frequency-dependent attenuation of <inline-formula><mml:math id=\"math13\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>4</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>dB</mml:mi><mml:mtext>&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>cm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>MHz</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and impedance of <inline-formula><mml:math id=\"math14\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1.7</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msup><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>Pa</mml:mi><mml:mo>&#183;</mml:mo><mml:mi mathvariant=\"normal\">s</mml:mi><mml:mo>&#183;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.<xref rid=\"r31\" ref-type=\"bibr\"><sup>31</sup></xref> The speed of sound ranges from 1450 to <inline-formula><mml:math id=\"math15\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1730</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi mathvariant=\"normal\">m</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant=\"normal\">s</mml:mi></mml:mrow></mml:math></inline-formula>.<xref rid=\"r32\" ref-type=\"bibr\"><sup>32</sup></xref></p><p>Test vascular phantoms from the DRIVE and NNE datasets were printed on <inline-formula><mml:math id=\"math16\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>60</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>&#956;</mml:mi><mml:mi mathvariant=\"normal\">m</mml:mi></mml:mrow></mml:math></inline-formula> thick PCU Protection Cover (Ultrasound B.V., The Netherlands) using black toner, following the method in Ref.&#160;<xref rid=\"r29\" ref-type=\"bibr\">29</xref>.</p></sec><sec id=\"sec2.1.4\"><label>2.1.4</label><title>Experimental phantom imaging</title><p>Imaging experiments used the handheld AcousticX system.<xref rid=\"r8\" ref-type=\"bibr\"><sup>8</sup></xref><sup>,</sup><xref rid=\"r33\" ref-type=\"bibr\"><sup>33</sup></xref> Phantoms were held in place using a 3D-printed holder with a clamping mechanism to ensure alignment at the center of the transducer array (see <xref rid=\"f1\" ref-type=\"fig\">Fig.&#160;1</xref>). Imaging was conducted in water and in a tissue-mimicking medium. The medium was prepared using 5.5% by volume of a 20% Intralipid solution (Fresenius Kabi, Bad Homburg, Germany) in water for scattering, and Indian ink (Talens, The Netherlands) with a dilution factor of 69,642 for absorption. The resulting optical properties at 850&#160;nm were an absorption coefficient of <inline-formula><mml:math id=\"math17\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>0.01</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and a reduced scattering coefficient of <inline-formula><mml:math id=\"math18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>1</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, tuned to match the simulation.</p></sec></sec><sec id=\"sec2.2\"><label>2.2</label><title>Models and Loss Functions</title><p>In this section, we detail various generative models and loss functions used in this study. We utilize Pix2Pix GAN, noted for its enhancement of photoacoustic images, as our initial model. Additionally, we explored modified U-Net architectures, including Residual U-Net, and proposed a ViT-based Trans U-Net as generators within the GAN framework. We assess multiple loss functions; MSE, PSNR, SSIM, a combined SSIM and PSNR metric, and GAN loss. Subsequent sections provide detailed implementation, starting with U-Net as the foundational model, followed by descriptions of modifications in the compared models.</p><sec id=\"sec2.2.1\"><label>2.2.1</label><title>Generalized U-Net architecture</title><p>A generalized U-Net architecture is illustrated in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2(a)</xref>. U-Nets transform images via a two-stage process.<xref rid=\"r35\" ref-type=\"bibr\"><sup>35</sup></xref> Initially, the input image <inline-formula><mml:math id=\"math19\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">x</mml:mi></mml:mrow></mml:math></inline-formula> is encoded into a high-level latent representation <inline-formula><mml:math id=\"math20\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">z</mml:mi></mml:mrow></mml:math></inline-formula> through a series of encoding steps that incrementally capture information at multiple detail levels. Early layers encode low-level features, such as texture, whereas deeper layers encode high-level features. This encoding phase alternates between encoders and max pooling layers to produce compact latent representations. The second stage involves decoding the latent representation back into an output image <inline-formula><mml:math id=\"math21\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow></mml:math></inline-formula>. Each decoder layer takes the output from the previous decoder and receives additional input via skip connections from the corresponding encoder layer, enhancing localization accuracy by combining detailed and contextual information, as depicted in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2(a)</xref>.</p><fig position=\"float\" id=\"f2\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>General U-Net architecture, Residual layers of Res18,<xref rid=\"r34\" ref-type=\"bibr\"><sup>34</sup></xref> and vision transformer. The <italic toggle=\"yes\">bn</italic> block indicates a batchnorm layer and the channel counts at each step are indicated by a dotted line.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g002.jpg\"/></fig><p>Differences among U-Net architectures primarily arise from variations in the encoder, decoder, and bottleneck designs, which influence the model&#8217;s expressive capability. Additionally, the architecture&#8217;s depth and width are controlled by the channel array <inline-formula><mml:math id=\"math22\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>, which defines the number of channels at each level. For instance, in this work, we configure <inline-formula><mml:math id=\"math23\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> as (64,128,256,512,512,512,512,512), resulting in a latent representation <inline-formula><mml:math id=\"math24\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mn>512</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> from an input image <inline-formula><mml:math id=\"math25\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mn>256</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, indicating a deep network where each value in the latent representation relates to every input pixel. However, practical applications may require adjustments in <inline-formula><mml:math id=\"math26\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> due to rapid increases in parameter count with network size.<xref rid=\"r35\" ref-type=\"bibr\"><sup>35</sup></xref></p></sec><sec id=\"sec2.2.2\"><label>2.2.2</label><title>Pix2Pix</title><p>The first model considered in this work is the generator of the Pix2Pix<xref rid=\"r36\" ref-type=\"bibr\"><sup>36</sup></xref> architecture, designed initially as a generative adversarial network (GAN). Pix2Pix uses <inline-formula><mml:math id=\"math27\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> convolutional layers in both its encoder and decoder architecture. A ReLU layer precedes the convolutional layer, and a normalization layer comes after it. In the bottom three decoder layers, 50% dropout layers simulate noise to avoid overfitting. This model acts as a baseline for more complex models due to its simplicity.</p></sec><sec id=\"sec2.2.3\"><label>2.2.3</label><title>Residual U-Nets</title><p>To overcome the challenge of training very deep convolutional neural networks, we integrated residual layers as proposed by Ref.&#160;<xref rid=\"r34\" ref-type=\"bibr\">34</xref>. We defined a learnable mapping <inline-formula><mml:math id=\"math28\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>&#8594;</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>&#8242;</mml:mo></mml:msup><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, composed of convolutional layers, normalization layers, and pointwise non-linearities. The weights <inline-formula><mml:math id=\"math29\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">W</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> were configured such that the residual layer adds the input directly to the mapped output, as follows, <disp-formula id=\"e001\"><mml:math id=\"math30\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold-italic\">x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold-italic\">x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant=\"bold-italic\">Wx</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(1)</label></disp-formula></p><p>This configuration allows for an easier propagation of the gradient through the network, simplifying optimization. Furthermore, it makes it possible to initialize at the identity function by initializing <inline-formula><mml:math id=\"math31\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> to always output <inline-formula><mml:math id=\"math32\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn mathvariant=\"bold\">0</mml:mn></mml:mrow></mml:math></inline-formula>, which is generally close to the desired mapping. We utilized ResNet-18 architecture which comprises basic blocks of two convolutional layers with normalization and ReLU activation as depicted in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2(b)</xref>.</p></sec><sec id=\"sec2.2.4\"><label>2.2.4</label><title>Trans U-Net</title><p>We employed the Trans U-Net<xref rid=\"r37\" ref-type=\"bibr\"><sup>37</sup></xref> with a ViT<xref rid=\"r38\" ref-type=\"bibr\"><sup>38</sup></xref> embedded in its bottleneck, aiming to improve high-level image representation for the decoding phase. The architecture of a ViT layer is depicted in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2(c)</xref>. Initially, the input image is segmented into patches of dimension <inline-formula><mml:math id=\"math33\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>. These patches are then flattened into vectors and transformed to <inline-formula><mml:math id=\"math34\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> patches of dimension <inline-formula><mml:math id=\"math35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula>. Each patch vector undergoes a linear transformation, followed by the addition of learned positional embeddings. A sequence of 12 transformer layers processes these vectors, as shown in <xref rid=\"f2\" ref-type=\"fig\">Fig.&#160;2(c)</xref>, before the decoder reshapes them into a <inline-formula><mml:math id=\"math36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:math></inline-formula> format. We augmented the Pix2Pix generator with a ViT to directly compare its performance against traditional Pix2Pix models.</p></sec></sec><sec id=\"sec2.3\"><label>2.3</label><title>Loss Functions and Image Quality Metrics</title><p>To optimize and evaluate the generative models for optical attenuation compensation in PAI, we employed a combination of loss functions and standard image quality metrics. These metrics ensured both robust training and comprehensive performance assessment.</p><sec id=\"sec2.3.1\"><label>2.3.1</label><title>Adversarial loss</title><p>Following the Pix2Pix framework,<xref rid=\"r36\" ref-type=\"bibr\"><sup>36</sup></xref> adversarial loss encourages realistic image generation by introducing a discriminator network <inline-formula><mml:math id=\"math37\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant=\"double-struck\">R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup><mml:mo>&#8594;</mml:mo><mml:mo stretchy=\"false\">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow></mml:math></inline-formula> that learns to discriminate between real outputs of the dataset and fake outputs of the generator network. The generator minimizes a loss combining Binary Cross-Entropy (BCE) and <inline-formula><mml:math id=\"math38\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#8467;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>-norm, <disp-formula id=\"e002\"><mml:math id=\"math39\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8467;</mml:mi></mml:mrow><mml:mrow><mml:mi>GAN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>BCE</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#955;</mml:mi><mml:mo stretchy=\"false\">&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mo stretchy=\"false\">&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula>where <inline-formula><mml:math id=\"math40\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula> is the generated image, <inline-formula><mml:math id=\"math41\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow></mml:math></inline-formula> is the target, and <inline-formula><mml:math id=\"math42\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>. The <inline-formula><mml:math id=\"math43\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#8467;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> term ensures fidelity to the target, whereas the discriminator term promotes realism. The generator and discriminator are optimized in an alternating fashion, where the discriminator minimizes the following loss, <disp-formula id=\"e003\"><mml:math id=\"math44\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8467;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>BCE</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>+</mml:mo><mml:mi>BCE</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo stretchy=\"false\">)</mml:mo><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula></p><p>Intuitively, the discriminator wants to assign 1 to the real data point <inline-formula><mml:math id=\"math45\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow></mml:math></inline-formula> and 0 to the fake data point from the generator <inline-formula><mml:math id=\"math46\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></inline-formula>. On the other side, the generator wants to &#8220;fool&#8221; the discriminator by making it output 1 to the generator&#8217;s outputs.</p></sec><sec id=\"sec2.3.2\"><label>2.3.2</label><title>Pixel-level loss</title><p>We employed MSE loss, defined as <disp-formula id=\"e004\"><mml:math id=\"math47\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8467;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy=\"false\">&#8214;</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:msubsup><mml:mrow><mml:mo stretchy=\"false\">&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(4)</label></disp-formula>to quantify pixel-wise differences, emphasizing larger errors due to the squaring of terms.</p></sec><sec id=\"sec2.3.3\"><label>2.3.3</label><title>Perceptual and structural loss</title><p>We incorporated PSNR and SSIM to evaluate perceptual quality and structural fidelity. These metrics were used both as loss functions during training and as evaluation metrics post-training, <disp-formula id=\"e005\"><mml:math id=\"math48\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8467;</mml:mi></mml:mrow><mml:mrow><mml:mi>PSNR</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>PSNR</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(5)</label></disp-formula><disp-formula id=\"e006\"><mml:math id=\"math49\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#8467;</mml:mi></mml:mrow><mml:mrow><mml:mi>SSIM</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mi>SSIM</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mover accent=\"true\"><mml:mrow><mml:mi mathvariant=\"bold-italic\">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy=\"false\">^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mi mathvariant=\"bold-italic\">y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(6)</label></disp-formula></p><p>To balance pixel-level accuracy and structural similarity, we combined PSNR and SSIM in a single loss function, <disp-formula id=\"e007\"><mml:math id=\"math50\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#8467;</mml:mi><mml:mrow><mml:mi>PSNR</mml:mi><mml:mo>+</mml:mo><mml:mi>SSIM</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#8467;</mml:mi><mml:mi>PSNR</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#955;</mml:mi><mml:mo>&#183;</mml:mo><mml:msub><mml:mi>&#8467;</mml:mi><mml:mi>SSIM</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(7)</label></disp-formula>with <inline-formula><mml:math id=\"math51\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#955;</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula>, determined empirically.</p></sec></sec><sec id=\"sec2.4\"><label>2.4</label><title>Training Details</title><p>The simulated photoacoustic images from the DRIVE dataset are split into an 80/20 train-test ratio, with the last 20% of the train split acting as validation data. Thus, 154 images are used for training, 38 for validation, and 48 for testing. Similarly, the photoacoustic images from the NNE dataset are divided into a 67/33 train-test ratio, with the last 20% of the train split serving as validation data. Thus, 5084 images are used for training, 1271 for validation, and 3176 for testing. All these computations are done with a GPU H100 using Adam as an optimizer with a learning rate of <inline-formula><mml:math id=\"math52\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and 200 epochs for training the model.<xref rid=\"r39\" ref-type=\"bibr\"><sup>39</sup></xref> Furthermore, an experimental dataset comprising six test images, three test images from the DRIVE dataset, and three test images from NNE dataset. In this study, the experimental phantoms are only utilized to validate the findings on non-simulated data. The phantoms captured in water serve as ground truth for the images captured in a solution approximating tissue characteristics. The checkpoint for the final model is chosen by maximizing the SSIM score on the validation dataset.</p></sec><sec id=\"sec2.5\"><label>2.5</label><title>Application in Breast Cancer Imaging</title><p>To demonstrate applicability in breast cancer imaging, we have tested the models on both <italic toggle=\"yes\">in vivo</italic> human breast tissue and a small animal tumor model. Acquired images were used to test the trained models.</p><sec id=\"sec2.5.1\"><label>2.5.1</label><title>Imaging in a small animal tumor model</title><p>An imaging experiment was conducted on a small animal tumor model, part of a related study.<xref rid=\"r40\" ref-type=\"bibr\"><sup>40</sup></xref> All animal protocols and experimental procedures were approved by the Institutional Animal Care and Use Committee (IACUC) of the University of Twente. Female SCID mice aged 5 to 7 weeks were procured from Janvier Laboratories and housed under appropriate conditions. The MDA-MB-231 human breast cancer cell line was cultured in high-glucose DMEM supplemented with L-glutamine (GE Healthcare) and 10% fetal bovine serum (FBS). Cells were maintained at 37&#176;C in a humidified incubator with 5% <inline-formula><mml:math id=\"math53\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>CO</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Each mouse was orthotopically injected with <inline-formula><mml:math id=\"math54\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> MDA-MB-231 cells to induce tumor formation. The endpoint is determined by the tumor volume of <inline-formula><mml:math id=\"math55\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo form=\"prefix\">&#8764;</mml:mo><mml:mn>1000</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The animal was euthanized, and the tumor was imaged immediately.</p></sec><sec id=\"sec2.5.2\"><label>2.5.2</label><title><italic toggle=\"yes\">In vivo</italic> breast imaging</title><p>The AcousticX system was utilized for <italic toggle=\"yes\">in vivo</italic> breast imaging. Imaging was conducted on the left breast of a 31-year-old volunteer with Fitzpatrick skin type IV. Freehand PAI and ultrasound scanning were performed to collect data. Combined ultrasound and photoacoustic images of the breast vasculature were collected and used as test images.</p></sec></sec></sec><sec id=\"sec3\"><label>3</label><title>Results and Discussion</title><p>In this section, we first compare fluence (optical attenuation) compensation with Pix2Pix GANs as a baseline generative model, next we show the model predictions on simulated and experimental twin phantoms, followed by a detailed comparison of the generative models and loss function, and finally, we demonstrate its application in <italic toggle=\"yes\">in vivo</italic> imaging.</p><sec id=\"sec3.1\"><label>3.1</label><title>Generative Model versus Attenuation Compensation</title><p>First, we compare the predictions of a generative model (Pix2Pix GAN) with fluence compensation in PAI. Pix2Pix GANs have previously been employed to enhance photoacoustic images.<xref rid=\"r41\" ref-type=\"bibr\"><sup>41</sup></xref><sup>,</sup><xref rid=\"r42\" ref-type=\"bibr\"><sup>42</sup></xref>\n<xref rid=\"f3\" ref-type=\"fig\">Figures&#160;3(a)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(e)</xref> show the input photoacoustic images from the DRIVE and NNE datasets, respectively. These simulations were performed on an absorbing and scattering medium, resulting in attenuation of fluence with depth. <xref rid=\"f3\" ref-type=\"fig\">Figures&#160;3(b)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(f)</xref> display the ground truth photoacoustic images generated using only acoustic simulation and reconstruction, without an optical model, under the assumption of uniform fluence. <xref rid=\"f3\" ref-type=\"fig\">Figures&#160;3(c)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(g)</xref> present the predictions from the Pix2Pix GANs model, whereas <xref rid=\"f3\" ref-type=\"fig\">Figs.&#160;3(d)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(h)</xref> show the fluence-compensated images. <xref rid=\"f3\" ref-type=\"fig\">Figures&#160;3(a)</xref>&#8211;<xref rid=\"f3\" ref-type=\"fig\">3(h)</xref> are obtained at a 30&#160;dB noise level in the simulation. In <xref rid=\"f3\" ref-type=\"fig\">Figs.&#160;3(i)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(j)</xref>, we compare mean SSIM values computed over depth for the test images of the DRIVE dataset, for the Pix2Pix and fluence compensation methods, respectively. SSIM was computed along the depth direction by extracting image rows corresponding to 2&#160;mm depth intervals and including all columns for each slice. A similar comparison is presented for the NNE dataset in <xref rid=\"f3\" ref-type=\"fig\">Figs.&#160;3(k)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(l)</xref>.</p><fig position=\"float\" id=\"f3\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Generative model and fluence compensation comparison. (A&#8211;D) Input image, ground truth, Pix2Pix GANs prediction, and fluence compensated image on a test image from the DRIVE dataset. (E&#8211;H) Input image, ground truth, Pix2Pix GANs prediction, and fluence-compensated image on a test image from the NNE dataset. (I) Mean SSIM along depth for the Pix2Pix GANs prediction and (J) for the fluence compensation output on the DRIVE dataset test set. (K) Mean SSIM along depth for the Pix2Pix GANs prediction and (L) for the fluence compensation output on the NNE dataset test set.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g003.jpg\"/></fig><p>The results from different noise levels indicate that generative models, such as Pix2Pix GANs, can perform better than the fluence compensation methods by predicting the structure from the noisy input images. Given that the measurement noise also distorts the structures in the image, a direct compensation results in poor structural quality and amplifies noise with depth as evident in <xref rid=\"f3\" ref-type=\"fig\">Figs.&#160;3(j)</xref> and <xref rid=\"f3\" ref-type=\"fig\">3(l)</xref>, where the SSIM value declines with depth at signal-to-noise ratio (SNR) levels of 10 and 20&#160;dB. Although Pix2Pix predictions also show a slight degradation in performance at higher noise levels, they remain comparatively better than the fluence compensation method, as seen in <xref rid=\"f3\" ref-type=\"fig\">Figs.&#160;3(i)</xref>&#8211;<xref rid=\"f3\" ref-type=\"fig\">3(l)</xref>. These results underscore the potential of deep learning-based methods for compensating for optical attenuation and enhancing structures.</p></sec><sec id=\"sec3.2\"><label>3.2</label><title>Simulation and Experimental Twin Phantoms</title><p>We present the use of simulation and experimental twin phantoms to find the practical capability of predictive models. <xref rid=\"f4\" ref-type=\"fig\">Figure&#160;4(a)</xref> shows the digital phantom. <xref rid=\"f4\" ref-type=\"fig\">Figure&#160;4(b)</xref> displays the simulated photoacoustic image incorporating attenuation due to fluence, whereas <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4(c)</xref> presents the simulation considering uniform illumination. <xref rid=\"f4\" ref-type=\"fig\">Figure&#160;4(d)</xref> illustrates the Pix2Pix prediction using <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4(b)</xref> as input. <xref rid=\"f4\" ref-type=\"fig\">Figure&#160;4(e)</xref> shows the printed phantom on a polyurethane film. <xref rid=\"f4\" ref-type=\"fig\">Figure&#160;4(f)</xref> depicts imaging in an absorbing and scattering medium, and <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4(g)</xref> shows imaging in water. Finally, <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4(h)</xref> demonstrates the Pix2Pix prediction using <xref rid=\"f4\" ref-type=\"fig\">Fig.&#160;4(f)</xref> as input.</p><fig position=\"float\" id=\"f4\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Simulation and experimental photoacoustic twin phantoms: (A) Digital image of the phantom. (B) Simulated photoacoustic image with fluence in a tissue-mimicking medium. (C) Ground truth photoacoustic image assuming uniform fluence. (D) Pix2Pix prediction with (B) as input. (E) Printed phantom on a polyurethane film. (F) PAI in a tissue-mimicking medium. (G) PAI in water. (H) Pix2Pix prediction with (F) as input.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g004.jpg\"/></fig><p>The results demonstrate that we can obtain experimental phantoms with structural ground truth information to compare with model predictions. This approach facilitates testing of deep learning models using similar simulated and experimental data. Furthermore, the ground truth information available in the experimental data enables verification of the prediction accuracy. However, discrepancies between the simulated and experimental results suggest that accurate modeling of all experimental parameters is necessary. Although we have considered the transducer and medium parameters here, the modeling is based on ideal system specifications and lacks fine-tuning based on actual measurements. The key takeaway is the availability of structural information to evaluate how the model performs on experimental data. It can be observed that Pix2Pix predictions enhance imaging depth compared with the input in both simulated and experimental phantoms. However, when compared with the ground truth images, the depth recovery in the Pix2Pix predictions is limited. The vascular structures that appear enhanced are largely the same ones already faintly visible in the input images at lower pixel intensities, rather than new structures revealed at greater depth. Pix2Pix predictions enhanced low photoacoustic signal structures with low background noise. Given that the Pix2Pix prediction only provided marginal improvement at deeper structures, we further compare Res18&#160;U-Net and the proposed Trans U-Net and loss functions to identify the most suitable model for this task.</p></sec><sec id=\"sec3.3\"><label>3.3</label><title>Comparison of Generative Models and Loss Functions</title><p>In <xref rid=\"f5\" ref-type=\"fig\">Fig.&#160;5</xref>, we compare the performance of the three models, Pix2Pix, Res18&#160;U-Net and Trans U-Net on input images from noise levels ranging from 10 to 50&#160;dB. We also compare different loss functions. The performance is compared in terms of image quality metrics SSIM and PSNR listed together. The heatmap shows SSIM to provide a visual impression of model performance. <xref rid=\"f5\" ref-type=\"fig\">Figure&#160;5</xref> left, shows the performance of models on the DRIVE dataset, and <xref rid=\"f5\" ref-type=\"fig\">Fig.&#160;5</xref> right corresponds to the NNE dataset.</p><fig position=\"float\" id=\"f5\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>The table shows SSIM and PSNR with heatmaps based on SSIM value, illustrate the performance of three models, Pix2Pix, Res18&#160;U-Net, and Trans U-Net, evaluated using various loss functions: GAN, <inline-formula><mml:math id=\"math56\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (MSE), PSNR, SSIM, and a combined SSIM + PSNR (S + P). Performance is assessed across noise levels ranging from 10&#160;dB to 50&#160;dB. The table on the left presents results for models trained on the DRIVE dataset, whereas the table on the right shows results for models trained on the NNE dataset.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g005.jpg\"/></fig><p>When trained with the pre-specified SSIM + PSNR loss, Trans U-Net is statistically superior to Res18-UNet and Pix2Pix for SSIM and PSNR on both datasets across noise levels, except for a single non-significant PSNR comparison on DRIVE at 10&#160;dB. Under alternative loss functions, performance is mixed and in some settings other models are comparable or better, as detailed in Table&#160;S1 in the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1117/1.JBO.30.11.116004.s01\" ext-link-type=\"uri\">Supplementary Material</ext-link>. This suggests that the proposed transformer-based architecture is more robust in handling fluence attenuation and noise in photoacoustic images. Among the loss functions tested, SSIM and combined SSIM + PSNR yield the highest SSIM and PSNR values, indicating their effectiveness in preserving structural and perceptual quality. Notably, the SSIM loss function performs best on the DRIVE dataset, whereas the combined SSIM + PSNR achieves the highest scores on the NNE dataset, suggesting that the optimal loss function may be dataset-dependent. The NNE dataset consistently shows higher SSIM and PSNR values across all models and loss functions compared with DRIVE, implying that NNE images may be less complex or contain more consistent features that are easier to reconstruct. Overall, the combination of Trans U-Net with loss functions SSIM or SSIM + PSNR provides the most reliable performance, and the results highlight the importance of tailoring model-loss configurations to specific dataset characteristics.</p><sec id=\"sec3.3.1\"><label>3.3.1</label><title>Statistical significance</title><p>We have performed a statistical paired t-test to show which model is superior in terms of PSNR and SSIM for the different loss function compared for different models in various noise conditions. For comparisons, we have compared the best performing model with the second best model.</p><p>Trans U-Net trained with SSIM + PSNR shows significant improvements in most comparisons on both datasets, with all NNE comparisons significant and one non-significant PSNR comparison on DRIVE at 10&#160;dB. The best results were obtained using the combined SSIM + PSNR loss. These improvements were statistically significant (<inline-formula><mml:math id=\"math57\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) in most cases (<xref rid=\"t001\" ref-type=\"table\">Table&#160;1</xref>). Detailed per-loss and per-noise-level comparisons are provided in Table&#160;S1 in the <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://doi.org/10.1117/1.JBO.30.11.116004.s01\" ext-link-type=\"uri\">Supplementary Material</ext-link>.</p><table-wrap position=\"float\" id=\"t001\" orientation=\"portrait\"><label>Table 1</label><caption><p>Statistical significance of Trans U-Net compared with Pix2Pix and Res18 U-Net across different metrics, noise levels, and two datasets. Significance determined by paired <inline-formula><mml:math id=\"math58\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>-test (<inline-formula><mml:math id=\"math59\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>) for the S + P loss function.</p></caption><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/><col align=\"center\" span=\"1\"/></colgroup><thead><tr><th rowspan=\"2\" align=\"left\" valign=\"top\" colspan=\"1\">Dataset</th><th rowspan=\"2\" align=\"center\" valign=\"top\" colspan=\"1\">Noise (dB)</th><th colspan=\"2\" align=\"center\" valign=\"top\" rowspan=\"1\">Trans U-Net versus Pix2Pix</th><th colspan=\"2\" align=\"center\" valign=\"top\" rowspan=\"1\">Trans U-Net versus Res18 U-Net</th></tr><tr><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">PSNR</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SSIM</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">PSNR</th><th align=\"center\" valign=\"top\" colspan=\"1\" rowspan=\"1\">SSIM</th></tr></thead><tbody><tr><td rowspan=\"5\" align=\"left\" colspan=\"1\">DRIVE Dataset</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">10</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">No</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">No</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">20</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">30</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">40</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">50</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td rowspan=\"5\" align=\"left\" colspan=\"1\">NNE 10 dataset</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">10</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">20</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">30</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">40</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr><tr><td align=\"center\" colspan=\"1\" rowspan=\"1\">50</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td><td align=\"center\" colspan=\"1\" rowspan=\"1\">Yes</td></tr></tbody></table></table-wrap></sec><sec id=\"sec3.3.2\"><label>3.3.2</label><title>Testing on a vascular printed phantom</title><p>We tested trained models on a printed vascular phantom. <xref rid=\"f6\" ref-type=\"fig\">Figure&#160;6</xref> shows the phantoms and the model prediction. Ground truth vascular structures are shown in <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(a)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(g)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(m)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(s)</xref>. The corresponding physical phantoms printed in water are presented in <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(b)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(h)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(n)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(t)</xref>, as a reference in a non-scattering medium to visualize deeper structures, whereas <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(c)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(i)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(o)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(u)</xref> display the same phantoms embedded in a tissue-mimicking medium, demonstrating depth-dependent photoacoustic signal attenuation. Model predictions are shown in the subsequent rows: Pix2Pix outputs are illustrated in <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(d)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(j)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(p)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(v)</xref>, Res18&#160;U-Net predictions in <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(e)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(k)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(q)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(w)</xref>, and Trans U-Net predictions in <xref rid=\"f6\" ref-type=\"fig\">Figs.&#160;6(f)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(l)</xref>, <xref rid=\"f6\" ref-type=\"fig\">6(r)</xref>, and <xref rid=\"f6\" ref-type=\"fig\">6(x)</xref>. The colored arrows in ground truth digital images and in the model predictions show structures that are enhanced by the model. Visually, it can be observed that Trans U-Net enhanced most structures compared with the digital ground truth image, followed by Res18&#160;U-Net. Pix2Pix predictions resulted in the least depth enhancement. We computed the relative SNR as 10log<sub>10</sub>() of the ratio between the mean squared value of the reference image patch and the MSE between the predicted image patch and the reference patch obtained in water. Quantitative evaluation of the relative SNR for marked vascular structures revealed the models&#8217; performance. Pix2Pix achieved an average SNR of <inline-formula><mml:math id=\"math60\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>2.3</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.5</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>dB</mml:mi></mml:mrow></mml:math></inline-formula>, indicating limited enhancement and high variability. In contrast, Res18&#160;U-Net and Trans U-Net demonstrated substantially higher SNRs of <inline-formula><mml:math id=\"math61\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>5.4</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"math62\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mn>5.3</mml:mn><mml:mo>&#177;</mml:mo><mml:mn>1.2</mml:mn><mml:mtext>&#8201;&#8201;</mml:mtext><mml:mi>dB</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, reflecting improved signal clarity and consistency. For the printed phantoms, Res18&#160;U-Net and the proposed Trans U-Net showed comparable performance in terms of SNR.</p><fig position=\"float\" id=\"f6\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Imaging of Printed Vascular Phantoms. (A, G, M, S) show the ground truth vascular phantoms. Among these, (A, G, M) are from the test dataset, whereas (S) represents a mouse brain vascular phantom not included in the dataset.<xref rid=\"r27\" ref-type=\"bibr\"><sup>27</sup></xref> (B, H, N, T) display the phantoms printed in water. (C, I, O, U) show the printed phantoms embedded in tissue-mimicking medium, illustrating depth-dependent attenuation. (D, J, P, V) present predictions from the Pix2Pix model. (E, K, Q, W) show predictions from the Res18&#160;U-Net model. (F, L, R, X) depict predictions from the Trans U-Net model. Colored arrows highlight vascular structures accurately predicted by the models.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g006.jpg\"/></fig></sec></sec><sec id=\"sec3.4\"><label>3.4</label><title><italic toggle=\"yes\">In Vivo</italic> Tumor Imaging</title><p>We evaluated Pix2Pix, Res18&#160;U-Net, and Trans U-Net on <italic toggle=\"yes\">in vivo</italic> murine breast tumor models and on <italic toggle=\"yes\">in vivo</italic> human breast images to demonstrate applicability to breast cancer imaging. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(a)</xref>&#8211;<xref rid=\"f7\" ref-type=\"fig\">7(l)</xref> show two small-animal tumors and <xref rid=\"f7\" ref-type=\"fig\">Figs.&#160;7(m)</xref>&#8211;<xref rid=\"f7\" ref-type=\"fig\">7(r)</xref> show human breast imaging. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(a)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(g)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(m)</xref> are ultrasound. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(b)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(h)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(n)</xref> are the corresponding photoacoustic images in log scale. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(c)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(i)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(o)</xref> are photoacoustic overlaid on ultrasound. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(d)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(j)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(p)</xref> are Pix2Pix predictions. <xref rid=\"f7\" ref-type=\"fig\">Figures&#160;7(e)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(k)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(q)</xref> are Res18&#160;U-Net, and <xref rid=\"f7\" ref-type=\"fig\">Figs.&#160;7(f)</xref>, <xref rid=\"f7\" ref-type=\"fig\">7(l)</xref>, and <xref rid=\"f7\" ref-type=\"fig\">7(r)</xref> are Trans U-Net. The arrows locate vascular features that are faint but present in the log-scale input photoacoustic images, and, where visible, hypoechoic tracks on ultrasound that often co-localize with blood vessels. These markings anchor the qualitative assessment to signals already present in the raw data rather than to structures created by the networks. Feeding blood vessels (FV) and their branching into the tumor can be observed in the enhanced images in both tumors.</p><fig position=\"float\" id=\"f7\" orientation=\"portrait\"><label>Fig. 7</label><caption><p><italic toggle=\"yes\">In vivo</italic> testing. (A&#8211;L) Model testing on small animal breast tumor images, (M&#8211;R) Model testing on human breast imaging. (A, G, and M) are ultrasound images. (B, H, and N) Photoacoustic images in log scale. (C, I, O) Overlaid photoacoustic images on ultrasound images. (D, J, and P) Prediction using Pix2Pix. (E, K, and Q) Prediction using Res18 and (F, L, and R) prediction using Trans U-Net. (Arrows with FV and A indicate feeding blood vessel and artefacts, respectively).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"JBO-030-116004-g007.jpg\"/></fig><p>Across both murine tumors and human breast, Trans U-Net produced the clearest enhancement of the vascular signals than Pix2Pix, and generally stronger contrast than Res18&#160;U-Net. Some examples of Trans U-Net enhancing vascular structures better than Res18 and Pix2Pix are arrow marked 1 in <xref rid=\"f7\" ref-type=\"fig\">Figs.&#160;7(b)</xref> and <xref rid=\"f7\" ref-type=\"fig\">7(f)</xref>, and arrow marked 7 in <xref rid=\"f7\" ref-type=\"fig\">Figs.&#160;7(n)</xref> and <xref rid=\"f7\" ref-type=\"fig\">7(r)</xref>. Some false predictions are also visible. Limited-view, low-SNR traces can be overconnected into vessel-like strands, and reconstruction artefacts outside tissue can be modified into vascular-like patterns by all three models, most prominently by Pix2Pix, but also Res18 and Trans U-Net. We therefore restrict interpretation to regions where the enhanced predictions correspond to signals visible in the log-scale image and, when available, to hypoechoic tracks on ultrasound. These <italic toggle=\"yes\">in vivo</italic> observations are consistent with quantitative trends in simulated test sets, with the Trans U-Net trained with SSIM + PSNR loss achieving the highest attenuation compensation and denoising.</p></sec></sec><sec id=\"sec4\"><label>4</label><title>Limitations and Future Work</title><p>The experimental phantoms were used only for testing the model. Training the model on experimental data could improve performance, but this requires a scalable method to automate the imaging of printed films and generate larger datasets. The phantoms, constrained by limited thickness, do not fully replicate the 3D nature of blood vessels. Our focus was on structural recovery rather than signal quantification. We have previously shown that varying film opacity can simulate different absorption levels.<xref rid=\"r29\" ref-type=\"bibr\"><sup>29</sup></xref> Future work will investigate the quantification of recovered signals for applications such as oxygen saturation.</p><p><italic toggle=\"yes\">In vivo</italic>, there is no absolute ground truth. To avoid over-claiming, we anchored our interpretation to two forms of evidence visible in the photoacoustic signal in the log-scale image; second, spatial co-localization and ultrasound hypoechoic tracks. Using this conservative approach, Trans U-Net most consistently enhanced vessel-like structures that were already detectable. At the same time, we explicitly show false predictions, including the conversion of reconstruction artefacts into vessel-like patterns and the overconnection of weak, limited-view traces. These observations caution against treating the network output as a vascular prediction <italic toggle=\"yes\">in vivo</italic> and motivate future validation with an independent modality such as Doppler or contrast-enhanced ultrasound and inclusion of artifact exemplars during training. Our simulation and phantom experiments, where ground truth is available, show that Trans U-Net is the most accurate model by SSIM and PSNR and has a slight advantage over Res18&#160;U-Net on printed phantoms and <italic toggle=\"yes\">in vivo</italic> scenarios.</p><p>We also observed variability in model performance across datasets. Although we compared a state-of-the-art Transformer-based model, this variability remains evident. In contrast to common observations that U-Nets perform better on smaller datasets, in our study the transformer-based model outperformed both Res18&#160;U-Net and Pix2Pix, likely reflecting the combined effect of dataset quality and the ability of transformers to capture long-range dependencies. Although the model was trained exclusively on simulated data, which allowed systematic validation with ground-truth access, we acknowledge that this may limit its direct generalization to experimental and <italic toggle=\"yes\">in vivo</italic> conditions; future work will focus on fine-tuning and validating the framework on empirical datasets using transfer learning and domain adaptation strategies. Developing a universal model<xref rid=\"r43\" ref-type=\"bibr\"><sup>43</sup></xref> that is robust across multiple PAI systems and tissue types remains a key goal for advancing PAI. Extending this approach using newer generative models, such as Trans U-Net, is a promising future direction.</p></sec><sec id=\"sec5\"><label>5</label><title>Conclusion</title><p>We addressed the coupled problem of fluence attenuation and denoising using generative models. We proposed a ViT-based Trans U-Net and compared it with state-of-the-art models such as Pix2Pix and Residual U-Net (Res18). On simulated datasets, Trans U-Net combined with the SSIM + PSNR loss function showed superior performance in recovering vascular structures and denoising photoacoustic images. In printed experimental phantoms, however, Res18&#160;U-Net and Trans U-Net showed comparable performance, whereas Pix2Pix performed relatively worse. The SSIM + PSNR loss function emerged as the most effective, balancing structural fidelity and noise suppression. <italic toggle=\"yes\">In vivo</italic> experiments further validated the practical utility of these models, with Trans U-Net notably enhancing vascular visualization in small animal tumor imaging. These findings highlight the potential of generative models, particularly Trans U-Net, to improve image quality in PAI and support more reliable and informative biomedical applications.</p></sec><sec id=\"sec6\" sec-type=\"supplementary-material\"><title>Supplementary Material</title><supplementary-material id=\"s01\" position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.1117/1.JBO.30.11.116004.s01</object-id><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"JBO_030_116004_SD001.pdf\" id=\"d2777e2155\" position=\"anchor\" orientation=\"portrait\"/></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>We extend our gratitude to Jai Prakash and Mithun Kuniyil Ajith Singh for their support in the <italic toggle=\"yes\">in vivo</italic> imaging study. We are also grateful to Sumohana S. Channappayya for his valuable insights on using generative models to enhance image quality. Finally, we thank Maura Dantuma for her assistance in developing the simulation pipeline.</p></ack><bio id=\"b1\"><p><bold>Cristian Perez Jensen</bold> is a master&#8217;s student in computer science at ETH Z&#252;rich, specializing in machine intelligence with a minor in theoretical computer science. He holds a bachelor&#8217;s degree in artificial intelligence from the University of Amsterdam.</p></bio><bio id=\"b2\"><p><bold>Navchetan Awasthi</bold> received his PhD in medical imaging from the Indian Institute of Science (IISc), Bangalore, India, in 2019. He is currently an assistant professor at the University of Amsterdam, The Netherlands. His research interests include inverse problems in biomedical optics, medical image analysis, medical image reconstruction, biomedical signal processing, and deep learning.</p></bio><bio id=\"b3\"><p><bold>Kalloor Joseph Francis</bold>, an assistant professor in the Biomedical Engineering Group at the Cardiology Department of Erasmus MC, Netherlands. He completed his PhD at the Indian Institute of Technology, Hyderabad, India. He was a former Fulbright fellow at the Rochester Institute of Technology. His research encompasses the development of photoacoustic imaging systems, tissue quantification algorithms, and their clinical applications.</p></bio><sec sec-type=\"conflict\"><title>Disclosures</title><p>The authors declare that there are no financial interests, commercial affiliations, or other potential conflicts of interest that could have influenced the objectivity of this research or the writing of this paper.</p></sec><sec sec-type=\"data-availability\"><title>Code and Data Availability</title><p>The code and dataset for the models are available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://github.com/navchetan-awasthi/pai-reconstruction\" ext-link-type=\"uri\">https://github.com/navchetan-awasthi/pai-reconstruction</ext-link>.</p></sec><sec><title>Funding</title><p>\n<named-content content-type=\"funding-statement\">This work has received financial support from the Dutch Research Council (NWO) for the project NWO-VENI (Grant No. 19165) and ZonMw for the project Off Road (Grant No. 04510012210042).</named-content>\n</p></sec><ref-list><title>References</title><ref id=\"r1\"><label>1.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>L. V.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>S.</given-names></name></person-group>, &#8220;<article-title>Photoacoustic tomography: in vivo imaging from organelles to organs</article-title>,&#8221; <source>Science</source><volume>335</volume>(<issue>6075</issue>), <fpage>1458</fpage>&#8211;<lpage>1462</lpage> (<year>2012</year>).<pub-id pub-id-type=\"coden\">SCIEAS</pub-id><issn>0036-8075</issn><pub-id pub-id-type=\"doi\">10.1126/science.1216210</pub-id><pub-id pub-id-type=\"pmid\">22442475</pub-id><pub-id pub-id-type=\"pmcid\">PMC3322413</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Steinberg</surname><given-names>I.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Photoacoustic clinical imaging</article-title>,&#8221; <source>Photoacoustics</source><volume>14</volume>, <fpage>77</fpage>&#8211;<lpage>98</lpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1016/j.pacs.2019.05.001</pub-id><pub-id pub-id-type=\"pmid\">31293884</pub-id><pub-id pub-id-type=\"pmcid\">PMC6595011</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optimizing the light delivery of linear-array-based photoacoustic systems by double acoustic reflectors</article-title>,&#8221; <source>Sci. Rep.</source><volume>8</volume>(<issue>1</issue>), <fpage>13004</fpage> (<year>2018</year>).<pub-id pub-id-type=\"coden\">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type=\"doi\">10.1038/s41598-018-31430-5</pub-id><pub-id pub-id-type=\"pmid\">30158556</pub-id><pub-id pub-id-type=\"pmcid\">PMC6115359</pub-id></mixed-citation></ref><ref id=\"r4\"><label>4.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Francis</surname><given-names>K. J.</given-names></name><name name-style=\"western\"><surname>Rascevska</surname><given-names>E.</given-names></name><name name-style=\"western\"><surname>Manohar</surname><given-names>S.</given-names></name></person-group>, &#8220;<article-title>Photoacoustic imaging assisted radiofrequency ablation: Illumination strategies and prospects</article-title>,&#8221; in <conf-name>TENCON 2019-2019 IEEE Region 10 Conf. (TENCON)</conf-name>, <publisher-name>IEEE</publisher-name>, pp.&#160;<fpage>118</fpage>&#8211;<lpage>122</lpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1109/TENCON.2019.8929646</pub-id></mixed-citation></ref><ref id=\"r5\"><label>5.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Linear array-based real-time photoacoustic imaging system with a compact coaxial excitation handheld probe for noninvasive sentinel lymph node mapping</article-title>,&#8221; <source>Biomed. Opt. Express</source><volume>9</volume>(<issue>4</issue>), <fpage>1408</fpage>&#8211;<lpage>1422</lpage> (<year>2018</year>).<pub-id pub-id-type=\"coden\">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type=\"doi\">10.1364/BOE.9.001408</pub-id><pub-id pub-id-type=\"pmid\">29675292</pub-id><pub-id pub-id-type=\"pmcid\">PMC5905896</pub-id></mixed-citation></ref><ref id=\"r6\"><label>6.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Awasthi</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep neural network-based sinogram super-resolution and bandwidth enhancement for limited-data photoacoustic tomography</article-title>,&#8221; <source>IEEE Trans. Ultrason. Ferroelectr. Freq. Control</source><volume>67</volume>(<issue>12</issue>), <fpage>2660</fpage>&#8211;<lpage>2673</lpage> (<year>2020</year>).<pub-id pub-id-type=\"coden\">ITUCER</pub-id><issn>0885-3010</issn><pub-id pub-id-type=\"doi\">10.1109/TUFFC.2020.2977210</pub-id><pub-id pub-id-type=\"pmid\">32142429</pub-id></mixed-citation></ref><ref id=\"r7\"><label>7.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhao</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Optical fluence compensation for handheld photoacoustic probe: an in vivo human study case</article-title>,&#8221; <source>J. Innov. Opt. Health Sci.</source><volume>10</volume>(<issue>4</issue>), <fpage>1740002</fpage> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.1142/S1793545817400028</pub-id></mixed-citation></ref><ref id=\"r8\"><label>8.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bulsink</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Oxygen saturation imaging using led-based photoacoustic system</article-title>,&#8221; <source>Sensors</source><volume>21</volume>(<issue>1</issue>), <fpage>283</fpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">SNSRES</pub-id><issn>0746-9462</issn><pub-id pub-id-type=\"doi\">10.3390/s21010283</pub-id><pub-id pub-id-type=\"pmid\">33406653</pub-id><pub-id pub-id-type=\"pmcid\">PMC7795655</pub-id></mixed-citation></ref><ref id=\"r9\"><label>9.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A three-dimensional modeling method for quantitative photoacoustic breast imaging with handheld probe</article-title>,&#8221; <source>Photoacoustics</source><volume>21</volume>, <fpage>100222</fpage> (<year>2021</year>).<pub-id pub-id-type=\"doi\">10.1016/j.pacs.2020.100222</pub-id><pub-id pub-id-type=\"pmid\">33318929</pub-id><pub-id pub-id-type=\"pmcid\">PMC7726342</pub-id></mixed-citation></ref><ref id=\"r10\"><label>10.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bauer</surname><given-names>A. Q.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Quantitative photoacoustic imaging: correcting for heterogeneous light fluence distributions using diffuse optical tomography</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>16</volume>(<issue>9</issue>), <fpage>096016</fpage> (<year>2011</year>).<pub-id pub-id-type=\"coden\">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type=\"doi\">10.1117/1.3626212</pub-id><pub-id pub-id-type=\"pmid\">21950930</pub-id><pub-id pub-id-type=\"pmcid\">PMC3188642</pub-id></mixed-citation></ref><ref id=\"r11\"><label>11.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hussain</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Photoacoustic and acousto-optic tomography for quantitative and functional imaging</article-title>,&#8221; <source>Optica</source><volume>5</volume>(<issue>12</issue>), <fpage>1579</fpage>&#8211;<lpage>1589</lpage> (<year>2018</year>).<pub-id pub-id-type=\"doi\">10.1364/OPTICA.5.001579</pub-id></mixed-citation></ref><ref id=\"r12\"><label>12.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Correction of wavelength-dependent laser fluence in swept-beam spectroscopic photoacoustic imaging with a hand-held probe</article-title>,&#8221; <source>Photoacoustics</source><volume>19</volume>, <fpage>100192</fpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.1016/j.pacs.2020.100192</pub-id><pub-id pub-id-type=\"pmid\">32670789</pub-id><pub-id pub-id-type=\"pmcid\">PMC7339128</pub-id></mixed-citation></ref><ref id=\"r13\"><label>13.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jeng</surname><given-names>G.-S.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Real-time interleaved spectroscopic photoacoustic and ultrasound (paus) scanning with simultaneous fluence compensation and motion correction</article-title>,&#8221; <source>Nat. Commun.</source><volume>12</volume>(<issue>1</issue>), <fpage>716</fpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type=\"doi\">10.1038/s41467-021-20947-5</pub-id><pub-id pub-id-type=\"pmid\">33514737</pub-id><pub-id pub-id-type=\"pmcid\">PMC7846772</pub-id></mixed-citation></ref><ref id=\"r14\"><label>14.</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gholampour</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep learning-based methods for photoacoustic imaging reconstruction: concepts, promises, pitfalls, and futures</article-title>,&#8221; in <source>Biomedical Photoacoustics: Technology and Applications</source>, <person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Xia</surname><given-names>W.</given-names></name></person-group>, Eds., pp.&#160;<fpage>155</fpage>&#8211;<lpage>177</lpage>, <publisher-name>Springer</publisher-name> (<year>2024</year>).</mixed-citation></ref><ref id=\"r15\"><label>15.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hariri</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep learning improves contrast in low-fluence photoacoustic imaging</article-title>,&#8221; <source>Biomed. Opt. Express</source><volume>11</volume>(<issue>6</issue>), <fpage>3360</fpage>&#8211;<lpage>3373</lpage> (<year>2020</year>).<pub-id pub-id-type=\"coden\">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type=\"doi\">10.1364/BOE.395683</pub-id><pub-id pub-id-type=\"pmid\">32637260</pub-id><pub-id pub-id-type=\"pmcid\">PMC7316023</pub-id></mixed-citation></ref><ref id=\"r16\"><label>16.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Manwar</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep learning protocol for improved photoacoustic brain imaging</article-title>,&#8221; <source>J. Biophotonics</source><volume>13</volume>(<issue>10</issue>), <fpage>e202000212</fpage> (<year>2020</year>).<pub-id pub-id-type=\"doi\">10.1002/jbio.202000212</pub-id><pub-id pub-id-type=\"pmid\">33405275</pub-id><pub-id pub-id-type=\"pmcid\">PMC10906453</pub-id></mixed-citation></ref><ref id=\"r17\"><label>17.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Paul</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Mallidi</surname><given-names>S.</given-names></name></person-group>, &#8220;<article-title>U-Net enhanced real-time led-based photoacoustic imaging</article-title>,&#8221; <source>J. Biophotonics</source><volume>17</volume>(<issue>6</issue>), <fpage>e202300465</fpage> (<year>2024</year>).<pub-id pub-id-type=\"doi\">10.1002/jbio.202300465</pub-id><pub-id pub-id-type=\"pmid\">38622811</pub-id><pub-id pub-id-type=\"pmcid\">PMC11164633</pub-id></mixed-citation></ref><ref id=\"r18\"><label>18.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yazdani</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Simultaneous denoising and localization network for photoacoustic target localization</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>40</volume>(<issue>9</issue>), <fpage>2367</fpage>&#8211;<lpage>2379</lpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type=\"doi\">10.1109/TMI.2021.3077187</pub-id><pub-id pub-id-type=\"pmid\">33939612</pub-id><pub-id pub-id-type=\"pmcid\">PMC8526152</pub-id></mixed-citation></ref><ref id=\"r19\"><label>19.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cai</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>End-to-end deep neural network for optical inversion in quantitative photoacoustic imaging</article-title>,&#8221; <source>Opt. Lett.</source><volume>43</volume>(<issue>12</issue>), <fpage>2752</fpage>&#8211;<lpage>2755</lpage> (<year>2018</year>).<pub-id pub-id-type=\"coden\">OPLEDP</pub-id><issn>0146-9592</issn><pub-id pub-id-type=\"doi\">10.1364/OL.43.002752</pub-id><pub-id pub-id-type=\"pmid\">29905680</pub-id></mixed-citation></ref><ref id=\"r20\"><label>20.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Quantitative photoacoustic blood oxygenation imaging using deep residual and recurrent neural network</article-title>,&#8221; in <conf-name>IEEE 16th Int. Symp. Biomed. Imaging (ISBI 2019)</conf-name>, <publisher-name>IEEE</publisher-name>, pp.&#160;<fpage>741</fpage>&#8211;<lpage>744</lpage> (<year>2019</year>).<pub-id pub-id-type=\"doi\">10.1109/ISBI.2019.8759438</pub-id></mixed-citation></ref><ref id=\"r21\"><label>21.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Luke</surname><given-names>G. P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>O-Net: a convolutional neural network for quantitative photoacoustic image segmentation and oximetry</article-title>,&#8221; arXiv:1911.01935 (<year>2019</year>).</mixed-citation></ref><ref id=\"r22\"><label>22.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Gao</surname><given-names>F.</given-names></name></person-group>, &#8220;<article-title>EDA-Net: dense aggregation of deep and shallow information achieves quantitative photoacoustic blood oxygenation imaging deep in human breast</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>11764</volume>, <fpage>246</fpage>&#8211;<lpage>254</lpage> (<year>2019</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/978-3-030-32239-7_28</pub-id></mixed-citation></ref><ref id=\"r23\"><label>23.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gr&#246;hl</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Estimation of blood oxygenation with learned spectral decoloring for quantitative photoacoustic imaging (LSD-QPAI)</article-title>,&#8221; arXiv:1902.05839 (<year>2019</year>).</mixed-citation></ref><ref id=\"r24\"><label>24.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Staal</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Ridge-based vessel segmentation in color images of the retina</article-title>,&#8221; <source>IEEE Trans. Med. Imaging</source><volume>23</volume>(<issue>4</issue>), <fpage>501</fpage>&#8211;<lpage>509</lpage> (<year>2004</year>).<pub-id pub-id-type=\"coden\">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type=\"doi\">10.1109/TMI.2004.825627</pub-id><pub-id pub-id-type=\"pmid\">15084075</pub-id></mixed-citation></ref><ref id=\"r25\"><label>25.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Uhlirova</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Neurovascular network explorer 2.0: a database of 2-photon single-vessel diameter measurements from mouse Si cortex in response to optogenetic stimulation</article-title>,&#8221; <source>Front. Neuroinform.</source><volume>11</volume>, <fpage>4</fpage> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.3389/fninf.2017.00004</pub-id><pub-id pub-id-type=\"pmid\">28203155</pub-id><pub-id pub-id-type=\"pmcid\">PMC5285378</pub-id></mixed-citation></ref><ref id=\"r26\"><label>26.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fang</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Boas</surname><given-names>D. A.</given-names></name></person-group>, &#8220;<article-title>Monte Carlo simulation of photon migration in 3d turbid media accelerated by graphics processing units</article-title>,&#8221; <source>Opt. express</source><volume>17</volume>(<issue>22</issue>), <fpage>20178</fpage>&#8211;<lpage>20190</lpage> (<year>2009</year>).<pub-id pub-id-type=\"coden\">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type=\"doi\">10.1364/OE.17.020178</pub-id><pub-id pub-id-type=\"pmid\">19997242</pub-id><pub-id pub-id-type=\"pmcid\">PMC2863034</pub-id></mixed-citation></ref><ref id=\"r27\"><label>27.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Treeby</surname><given-names>B. E.</given-names></name><name name-style=\"western\"><surname>Cox</surname><given-names>B. T.</given-names></name></person-group>, &#8220;<article-title>k-wave: Matlab toolbox for the simulation and reconstruction of photoacoustic wave fields</article-title>,&#8221; <source>J. Biomed. Opt.</source><volume>15</volume>(<issue>2</issue>), <fpage>021314</fpage> (<year>2010</year>).<pub-id pub-id-type=\"coden\">JBOPFO</pub-id><issn>1083-3668</issn><pub-id pub-id-type=\"doi\">10.1117/1.3360308</pub-id><pub-id pub-id-type=\"pmid\">20459236</pub-id></mixed-citation></ref><ref id=\"r28\"><label>28.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jaeger</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Fourier reconstruction in optoacoustic imaging using truncated regularized inverse k-space interpolation</article-title>,&#8221; <source>Inverse Probl.</source><volume>23</volume>(<issue>6</issue>), <fpage>S51</fpage> (<year>2007</year>).<pub-id pub-id-type=\"coden\">INPEEY</pub-id><issn>0266-5611</issn><pub-id pub-id-type=\"doi\">10.1088/0266-5611/23/6/S05</pub-id></mixed-citation></ref><ref id=\"r29\"><label>29.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Morsink</surname><given-names>C. F.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Design and characterization of color printed polyurethane films as biomedical phantom layers</article-title>,&#8221; <source>Biomed. Opt. Express</source><volume>14</volume>(<issue>9</issue>), <fpage>4485</fpage>&#8211;<lpage>4506</lpage> (<year>2023</year>).<pub-id pub-id-type=\"coden\">BOEICL</pub-id><issn>2156-7085</issn><pub-id pub-id-type=\"doi\">10.1364/BOE.491695</pub-id><pub-id pub-id-type=\"pmid\">37791261</pub-id><pub-id pub-id-type=\"pmcid\">PMC10545194</pub-id></mixed-citation></ref><ref id=\"r30\"><label>30.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jacques</surname><given-names>S. L.</given-names></name></person-group>, &#8220;<article-title>Optical properties of biological tissues: a review</article-title>,&#8221; <source>Phys. Med. Biol.</source><volume>58</volume>(<issue>11</issue>), <fpage>R37</fpage> (<year>2013</year>).<pub-id pub-id-type=\"coden\">PHMBA7</pub-id><issn>0031-9155</issn><pub-id pub-id-type=\"doi\">10.1088/0031-9155/58/11/R37</pub-id><pub-id pub-id-type=\"pmid\">23666068</pub-id></mixed-citation></ref><ref id=\"r31\"><label>31.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Maruvada</surname><given-names>S.</given-names></name></person-group>, &#8220;<article-title>Development and characterization of polyurethane-based tissue and blood mimicking materials for high intensity therapeutic ultrasound</article-title>,&#8221; <source>J. Acoust. Soc. Amer.</source><volume>151</volume>(<issue>5</issue>), <fpage>3043</fpage>&#8211;<lpage>3051</lpage> (<year>2022</year>).<pub-id pub-id-type=\"coden\">JASMAN</pub-id><issn>0001-4966</issn><pub-id pub-id-type=\"doi\">10.1121/10.0010385</pub-id><pub-id pub-id-type=\"pmid\">35649924</pub-id></mixed-citation></ref><ref id=\"r32\"><label>32.</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azhari</surname><given-names>H.</given-names></name></person-group>, <source>Basics of Biomedical Ultrasound for Engineers</source>, <publisher-name>John Wiley &amp; Sons</publisher-name> (<year>2010</year>).</mixed-citation></ref><ref id=\"r33\"><label>33.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Francis</surname><given-names>K. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Tomographic ultrasound and led-based photoacoustic system for preclinical imaging</article-title>,&#8221; <source>Sensors</source><volume>20</volume>(<issue>10</issue>), <fpage>2793</fpage> (<year>2020</year>).<pub-id pub-id-type=\"coden\">SNSRES</pub-id><issn>0746-9462</issn><pub-id pub-id-type=\"doi\">10.3390/s20102793</pub-id><pub-id pub-id-type=\"pmid\">32422995</pub-id><pub-id pub-id-type=\"pmcid\">PMC7294432</pub-id></mixed-citation></ref><ref id=\"r34\"><label>34.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>He</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Deep residual learning for image recognition</article-title>,&#8221; in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp.&#160;<fpage>770</fpage>&#8211;<lpage>778</lpage> (<year>2016</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref><ref id=\"r35\"><label>35.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group>, &#8220;<article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>,&#8221; <source>Lect. Notes Comput. Sci.</source><volume>9351</volume>, <fpage>234</fpage>&#8211;<lpage>241</lpage> (<year>2015</year>).<pub-id pub-id-type=\"coden\">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type=\"doi\">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation></ref><ref id=\"r36\"><label>36.</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isola</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Image-to-image translation with conditional adversarial networks</article-title>,&#8221; in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp.&#160;<fpage>1125</fpage>&#8211;<lpage>1134</lpage> (<year>2017</year>).<pub-id pub-id-type=\"doi\">10.1109/CVPR.2017.632</pub-id></mixed-citation></ref><ref id=\"r37\"><label>37.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>TransUNet: transformers make strong encoders for medical image segmentation</article-title>,&#8221; arXiv:2102.04306 (<year>2021</year>).</mixed-citation></ref><ref id=\"r38\"><label>38.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A.</given-names></name></person-group>, &#8220;<article-title>An image is worth 16x16 words: transformers for image recognition at scale</article-title>,&#8221; arXiv:2010.11929 (<year>2020</year>).</mixed-citation></ref><ref id=\"r39\"><label>39.</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kingma</surname><given-names>D. P.</given-names></name></person-group>, &#8220;<article-title>Adam: a method for stochastic optimization</article-title>,&#8221; arXiv:1412.6980 (<year>2014</year>).</mixed-citation></ref><ref id=\"r40\"><label>40.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>In vivo detection of circulating cancer-associated fibroblasts in breast tumor mouse xenograft: impact of tumor stroma and chemotherapy</article-title>,&#8221; <source>Cancers</source><volume>15</volume>(<issue>4</issue>), <fpage>1127</fpage> (<year>2023</year>).<pub-id pub-id-type=\"doi\">10.3390/cancers15041127</pub-id><pub-id pub-id-type=\"pmid\">36831470</pub-id><pub-id pub-id-type=\"pmcid\">PMC9954095</pub-id></mixed-citation></ref><ref id=\"r41\"><label>41.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Francis</surname><given-names>K. J.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>Generative adversarial network-based photoacoustic image reconstruction from bandlimited and limited-view data</article-title>,&#8221; <source>Proc. SPIE</source><volume>11642</volume>, <fpage>1164235</fpage> (<year>2021</year>).<pub-id pub-id-type=\"coden\">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type=\"doi\">10.1117/12.2577750</pub-id></mixed-citation></ref><ref id=\"r42\"><label>42.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vu</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, &#8220;<article-title>A generative adversarial network for artifact removal in photoacoustic computed tomography with a linear-array transducer</article-title>,&#8221; <source>Exp. Biol. Med.</source><volume>245</volume>(<issue>7</issue>), <fpage>597</fpage>&#8211;<lpage>605</lpage> (<year>2020</year>).<pub-id pub-id-type=\"coden\">EXBMAA</pub-id><issn>0071-3384</issn><pub-id pub-id-type=\"doi\">10.1177/1535370220914285</pub-id><pub-id pub-id-type=\"pmcid\">PMC7153213</pub-id><pub-id pub-id-type=\"pmid\">32208974</pub-id></mixed-citation></ref><ref id=\"r43\"><label>43.</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Stam</surname><given-names>O. J.</given-names></name><name name-style=\"western\"><surname>Francis</surname><given-names>K. J.</given-names></name><name name-style=\"western\"><surname>Awasthi</surname><given-names>N.</given-names></name></person-group>, &#8220;<article-title>PA OmniNet: a retraining-free, generalizable deep learning framework for robust photoacoustic image reconstruction</article-title>,&#8221; <source>Photoacoustics</source><volume>45</volume>, <fpage>100740</fpage> (<year>2025</year>).<pub-id pub-id-type=\"doi\">10.1016/j.pacs.2025.100740</pub-id><pub-id pub-id-type=\"pmid\">40703536</pub-id><pub-id pub-id-type=\"pmcid\">PMC12284555</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc J Biomed Opt J Biomed Opt 953 jbiomedopt JBO Journal of Biomedical Optics 1083-3668 1560-2281 Society of Photo-Optical Instrumentation Engineers PMC12654947 PMC12654947.1 12654947 12654947 41312519 10.1117/1.JBO.30.11.116004 250222GRR 1 Imaging Paper Transformer-based optical attenuation compensation and denoising in photoacoustic imaging Jensen Cristian Perez a cristianpjensen@gmail.com https://orcid.org/0000-0001-8153-2786 Awasthi Navchetan a b * &#8224; navchetanawasthi@gmail.com https://orcid.org/0000-0001-9766-7140 Francis Kalloor Joseph c * &#8224; f.kalloorjoseph@erasmusmc.nl a University of Amsterdam , Informatics Institute, Faculty of Science, Mathematics and Computer Science, Amsterdam, The Netherlands b Amsterdam UMC , Department of Biomedical Engineering and Physics, Amsterdam, The Netherlands c Erasmus MC , Cardiovascular Institute, Department of Cardiology, Biomedical Engineering, Rotterdam, The Netherlands * Address all correspondence to Navchetan Awasthi, n.awasthi@uva.nl ; Kalloor Joseph Francis, f.kalloorjoseph@erasmusmc.nl &#8224; Equal Contribution 26 11 2025 11 2025 30 11 499866 116004 18 7 2025 26 10 2025 27 10 2025 26 11 2025 27 11 2025 28 11 2025 &#169; 2025 The Authors 2025 The Authors https://creativecommons.org/licenses/by/4.0/ Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI. Abstract. Significance Linear-array-based photoacoustic imaging (PAI) combines functional imaging with structural imaging from ultrasound. However, it suffers from depth-dependent optical attenuation due to surface illumination, resulting in decreased signal amplitude and image contrast with depth. Existing attenuation compensation methods often amplify noise, creating a trade-off between depth enhancement and image quality. Aim We aim to develop a deep learning method that addresses the coupled problem of optical attenuation compensation and denoising for linear-array-based PAI and test the applicability in vivo . Approach We propose a vision-transformer-based generative model to address this coupled problem. A diverse dataset was created using simulated data and experimental twin phantoms. Vascular twin phantoms were made by printing digital images onto polyurethane films to test the performance of the model. We trained and compared three deep learning architectures, Pix2Pix, Residual U-Net, and the proposed Transformer U-Net, using various loss functions, including adversarial, MSE, PSNR, SSIM, and a combined PSNR + SSIM. We tested the model on small animal tumor images. Results Quantitative evaluation shows that PSNR + SSIM loss is robust in preserving structural details and suppressing noise. Under the pre-specified SSIM + PSNR training objective, Trans U-Net achieves the highest SSIM and PSNR across noise levels on both datasets. In vivo validation using murine breast tumor models and in vivo breast imaging confirmed the model&#8217;s ability to enhance visualization of deep vascular structures without introducing noise amplification. Conclusions The proposed Trans U-Net effectively addresses the coupled problem of attenuation correction and denoising in handheld PAI. This method improves depth-resolved vascular imaging and is potentially useful in clinical and preclinical photoacoustic applications. Keywords: photoacoustic imaging deep learning models Transformer U-Net fluence compensation image quality enhancement tumor imaging Dutch Research Council (NWO) for the project NWO-VENI 19165 ZonMw for the project Off Road 04510012210042 This work has received financial support from the Dutch Research Council (NWO) for the project NWO-VENI (Grant No. 19165) and ZonMw for the project Off Road (Grant No. 04510012210042). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes running-head Jensen, Awasthi, and Francis: Transformer-based optical attenuation&#8230; 1 Introduction Photoacoustic imaging (PAI) is a promising biomedical imaging modality that combines optical contrast with ultrasonic spatial resolution. 1 When integrated with conventional ultrasound systems, PAI enhances functional imaging capabilities, making it valuable for clinical applications such as cancer diagnosis, vascular imaging, and functional monitoring, including oxygen saturation. 2 The probe configuration using a linear ultrasound array with an optical source is particularly popular due to its compatibility with existing ultrasound systems. 3 &#8211; 5 Despite its potential, PAI with conventional ultrasound imaging systems faces significant challenges, including optical attenuation, limited target view, and the band-limited nature of the conventional linear transducers, all of which contribute to degraded image quality. 6 Depth-dependent attenuation in PAI degrades deep-tissue contrast with the exponential decay of optical fluence. A common approach is to assume average tissue optical properties and invert fluence decay via the Beer&#8211;Lambert law, the diffusion approximation, or Monte Carlo simulations. In early work, Zhao et&#160;al. 7 employed Monte Carlo&#8211;based light-transport simulations to generate pixel-wise fluence maps for real-time amplitude correction in a handheld probe. Although these model-based schemes enhance deep-tissue contrast, they amplify noise at greater depths. 7 &#8211; 9 Hardware-augmented strategies combining diffuse optical tomography, 10 acousto-optic tagging, 11 or multi-illumination approaches 12 can obtain a fluence map but require complex systems, larger probes, and computationally intensive inversions, limiting their clinical translation. Spectroscopic methods integrate wavelength-dependent diffusion models with reference-phantom calibration to provide real-time fluence correction alongside motion compensation. 12 , 13 Learning-based frameworks have recently emerged as an alternative by training deep networks to perform simultaneous depth-dependent optical attenuation correction and denoising in an end-to-end fashion. 14 Implementations using U-Nets (including residual and transformer variants), Pix2Pix GANs, and attention mechanisms have demonstrated marked improvements across simulated, phantom, and in vivo breast and brain imaging studies. 15 &#8211; 18 Several deep-learning architectures have been developed for photoacoustic fluence compensation and optical parameter estimation, including the Residual U-Net by Cai et&#160;al., 19 the deep residual recurrent U-Net (DR2U-Net) by Chang et&#160;al., 20 O-Net by Luke et&#160;al., 21 EDA-Net by Yang et&#160;al., 22 and various encoder&#8211;decoder combinations. 23 Most prior deep-learning methods for PAI focus on denoising, contrast enhancement, or quantitative oxygenation mapping. Although effective in those tasks, they generally do not address depth-dependent optical attenuation in tomographic and handheld linear-array systems. Further, these works do not address the optical attenuation correction and denoising as a coupled problem. Our work fills this gap by introducing a transformer-based model that jointly compensates attenuation and suppresses noise, enabling robust depth-resolved vascular imaging. In this work, we address the challenge of depth-dependent optical attenuation and noise amplification in PAI as a joint problem, using a vision transformer (ViT)-based Trans U-Net. To train and test the models, we developed a comprehensive dataset that bridges simulation and experiment. It includes: (1)&#160;simulated photoacoustic images using combined optical and acoustic modeling, with uniform fluence images as ground truth and their counterparts incorporating realistic light propagation in tissue-mimicking media; (2)&#160;experimental images of printed phantoms acquired in water as a non-scattering medium as reference images and in tissue-mimicking scattering phantoms, capturing real-world complexity of imaging using a handheld probe. By utilizing the same digital phantoms and recreating them through printing, we created &#8220;digital and experimental twins,&#8221; enabling controlled validation against known ground truth. We systematically evaluated a set of conditional generative models, Pix2Pix, lightweight Residual U-Net (Res18), and also proposed a Transformer U-Net model, across different measurement noise levels and experimental setups, including in vivo data. We also explored multiple loss functions, including adversarial loss, mean squared error (MSE), peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and a combined PSNR + SSIM formulation. Finally, we test the proposed model on small animal tumor imaging. 2 Methods We used two vascular datasets and optical simulations to model light propagation, followed by acoustic simulations and image reconstruction for our PAI dataset. We printed these images onto polyurethane films and imaged them with our PAI system to create experimental datasets. We generated both ideal and realistic PAI for model training and testing, and evaluated deep learning models and loss outputs with various image quality metrics. Subsequent sections provide details about the dataset generation, model training, and image evaluation. 2.1 Dataset Generation The DRIVE dataset 24 contains 48 segmented retinal vessel images, each transformed into five variants using augmentation, creating 240 images. From each image, we extracted five non-overlapping regions containing vasculature in different orientations, as shown in Fig.&#160;1 . To simulate extracorporeal illumination, a skin-like layer was added to the top of each cropped image. These skin layers were obtained by scanning the forearms of two volunteers at multiple locations using the same imaging system. The average thickness of the skin layer was &#8764; 0.6 &#8201;&#8201; mm , which corresponds to about five pixels in the reconstructed images. The NNE dataset 25 includes 9531 images from 2-photon single-vessel measurements of the mouse SI cortex. Figure&#160;1 outlines the dataset generation pipeline, showing simulation and experimental paths. Initially, vascular images are resized to 40 &#215; 40 &#8201;&#8201; mm . The simulation path involves ideal simulations with uniform light fluence and realistic simulations accounting for fluence decay. In the experimental path, phantoms printed on polyurethane films are imaged in water and tissue-mimicking medium, generating both ideal and realistic PAI. The twin-phantom imaging setup is shown in Fig.&#160;S1 in the Supplementary Material . Fig. 1 Simulation and experimental twin phantom. The pipeline shows the generation of the simulated and experimental dataset from the digital vascular image. Simulated phantoms consist of acoustic simulation and photoacoustic reconstruction with and without the added light fluence. Experimental phantoms are printed images on polyurethane films imaged in water and an absorbing and scattering medium. 2.1.1 Optical simulation We used the LED-based PAI system AcousticX (Cyberdyne Inc., Japan). Light propagation from the integrated LED arrays into tissue was simulated using the GPU-accelerated Monte Carlo eXtreme (MCX) photon transport simulator. 26 The probe integrates two LED units mounted at a 41.4&#160;deg angle, positioned 0.15&#160;mm in front of the ultrasound transducer and spaced 9.56&#160;mm apart. Each unit measures 50 &#215; 10 &#8201;&#8201; mm and contains four arrays of 144 LED elements (36 per row, 4 rows), emitting at 850&#160;nm with a 120&#160;deg opening angle. The simulation domain was 55 &#215; 55 &#215; 40 &#8201;&#8201; mm 3 , represented by a 744 &#215; 744 &#215; 512 &#8201;&#8201;voxel grid with a uniform voxel size of 74 &#8201;&#8201; &#956; m . Tissue-mimicking optical properties were set as: absorption coefficient 0.01 &#8201;&#8201; mm &#8722; 1 , reduced scattering coefficient 1 &#8201;&#8201; mm &#8722; 1 , and anisotropy factor 0.9. Simulations yielded fluence maps in the center plane between the LED arrays. These were normalized and multiplied with digital phantom images to generate initial pressure maps, assuming a constant Gr&#252;neisen parameter. 2.1.2 Acoustic simulation and image reconstruction Acoustic simulations were carried out using the k-Wave toolbox 27 on a 512 &#215; 512 grid. Input vascular images were resized and normalized. The corresponding fluence map was cropped and normalized, then multiplied by the image to generate the initial pressure map. The transducer was modeled as a 128-element linear array with a pitch of 0.315&#160;mm and a center frequency of 7&#160;MHz (bandwidth 80%). Transducer directivity was accounted for via finite element size by summing up sensor data from multiple point sensors. The medium was assumed homogeneous with density 1000 &#8201;&#8201; kg / m 3 and sound speed 1500 &#8201;&#8201; m / s . Additive white noise ranging from 10 to 50&#160;dB (in 10&#160;dB steps) was added to simulate measurement noise. Image reconstruction was performed using a Fourier domain algorithm. 28 2.1.3 Phantom printing Digital phantoms were fabricated using polyurethane, chosen for its suitable thermal, optical, and acoustic properties. 29 The material tolerates the 175&#176;C fuser temperature of the Xerox 7800i printer, with a melting point of 180&#176;C. Its absorption coefficient ranges from 0.001 to 0.005 &#8201;&#8201; mm &#8722; 1 , and its refractive index (1.41 to 1.58) closely matches soft tissue (1.35 to 1.55). 30 Acoustic properties include frequency-dependent attenuation of 4 &#8201;&#8201; dB &#8201; cm &#8722; 1 &#8201; MHz &#8722; 1 and impedance of 1.7 &#215; 10 6 &#8201;&#8201; Pa &#183; s &#183; m &#8722; 1 . 31 The speed of sound ranges from 1450 to 1730 &#8201;&#8201; m / s . 32 Test vascular phantoms from the DRIVE and NNE datasets were printed on 60 &#8201;&#8201; &#956; m thick PCU Protection Cover (Ultrasound B.V., The Netherlands) using black toner, following the method in Ref.&#160; 29 . 2.1.4 Experimental phantom imaging Imaging experiments used the handheld AcousticX system. 8 , 33 Phantoms were held in place using a 3D-printed holder with a clamping mechanism to ensure alignment at the center of the transducer array (see Fig.&#160;1 ). Imaging was conducted in water and in a tissue-mimicking medium. The medium was prepared using 5.5% by volume of a 20% Intralipid solution (Fresenius Kabi, Bad Homburg, Germany) in water for scattering, and Indian ink (Talens, The Netherlands) with a dilution factor of 69,642 for absorption. The resulting optical properties at 850&#160;nm were an absorption coefficient of 0.01 &#8201;&#8201; mm &#8722; 1 and a reduced scattering coefficient of 1 &#8201;&#8201; mm &#8722; 1 , tuned to match the simulation. 2.2 Models and Loss Functions In this section, we detail various generative models and loss functions used in this study. We utilize Pix2Pix GAN, noted for its enhancement of photoacoustic images, as our initial model. Additionally, we explored modified U-Net architectures, including Residual U-Net, and proposed a ViT-based Trans U-Net as generators within the GAN framework. We assess multiple loss functions; MSE, PSNR, SSIM, a combined SSIM and PSNR metric, and GAN loss. Subsequent sections provide detailed implementation, starting with U-Net as the foundational model, followed by descriptions of modifications in the compared models. 2.2.1 Generalized U-Net architecture A generalized U-Net architecture is illustrated in Fig.&#160;2(a) . U-Nets transform images via a two-stage process. 35 Initially, the input image x is encoded into a high-level latent representation z through a series of encoding steps that incrementally capture information at multiple detail levels. Early layers encode low-level features, such as texture, whereas deeper layers encode high-level features. This encoding phase alternates between encoders and max pooling layers to produce compact latent representations. The second stage involves decoding the latent representation back into an output image y . Each decoder layer takes the output from the previous decoder and receives additional input via skip connections from the corresponding encoder layer, enhancing localization accuracy by combining detailed and contextual information, as depicted in Fig.&#160;2(a) . Fig. 2 General U-Net architecture, Residual layers of Res18, 34 and vision transformer. The bn block indicates a batchnorm layer and the channel counts at each step are indicated by a dotted line. Differences among U-Net architectures primarily arise from variations in the encoder, decoder, and bottleneck designs, which influence the model&#8217;s expressive capability. Additionally, the architecture&#8217;s depth and width are controlled by the channel array C , which defines the number of channels at each level. For instance, in this work, we configure C as (64,128,256,512,512,512,512,512), resulting in a latent representation z &#8712; R 512 from an input image x &#8712; R 256 &#215; 256 , indicating a deep network where each value in the latent representation relates to every input pixel. However, practical applications may require adjustments in C due to rapid increases in parameter count with network size. 35 2.2.2 Pix2Pix The first model considered in this work is the generator of the Pix2Pix 36 architecture, designed initially as a generative adversarial network (GAN). Pix2Pix uses 4 &#215; 4 convolutional layers in both its encoder and decoder architecture. A ReLU layer precedes the convolutional layer, and a normalization layer comes after it. In the bottom three decoder layers, 50% dropout layers simulate noise to avoid overfitting. This model acts as a baseline for more complex models due to its simplicity. 2.2.3 Residual U-Nets To overcome the challenge of training very deep convolutional neural networks, we integrated residual layers as proposed by Ref.&#160; 34 . We defined a learnable mapping f : R C &#215; H &#215; W &#8594; R C &#8242; &#215; H &#215; W , composed of convolutional layers, normalization layers, and pointwise non-linearities. The weights W &#8712; R C &#8242; &#215; C were configured such that the residual layer adds the input directly to the mapped output, as follows, r ( x ) = f ( x ) + Wx . (1) This configuration allows for an easier propagation of the gradient through the network, simplifying optimization. Furthermore, it makes it possible to initialize at the identity function by initializing f to always output 0 , which is generally close to the desired mapping. We utilized ResNet-18 architecture which comprises basic blocks of two convolutional layers with normalization and ReLU activation as depicted in Fig.&#160;2(b) . 2.2.4 Trans U-Net We employed the Trans U-Net 37 with a ViT 38 embedded in its bottleneck, aiming to improve high-level image representation for the decoding phase. The architecture of a ViT layer is depicted in Fig.&#160;2(c) . Initially, the input image is segmented into patches of dimension C &#215; P &#215; P . These patches are then flattened into vectors and transformed to H W P 2 patches of dimension P 2 C . Each patch vector undergoes a linear transformation, followed by the addition of learned positional embeddings. A sequence of 12 transformer layers processes these vectors, as shown in Fig.&#160;2(c) , before the decoder reshapes them into a C &#215; H &#215; W format. We augmented the Pix2Pix generator with a ViT to directly compare its performance against traditional Pix2Pix models. 2.3 Loss Functions and Image Quality Metrics To optimize and evaluate the generative models for optical attenuation compensation in PAI, we employed a combination of loss functions and standard image quality metrics. These metrics ensured both robust training and comprehensive performance assessment. 2.3.1 Adversarial loss Following the Pix2Pix framework, 36 adversarial loss encourages realistic image generation by introducing a discriminator network d : R C &#215; H &#215; W &#8594; [ 0,1 ] that learns to discriminate between real outputs of the dataset and fake outputs of the generator network. The generator minimizes a loss combining Binary Cross-Entropy (BCE) and &#8467; 1 -norm, &#8467; GAN ( y ^ , y ) = BCE ( 1 , d ( y ^ ) ) + &#955; &#8214; y &#8722; y ^ &#8214; 1 , (2) where y ^ is the generated image, y is the target, and &#955; = 50 . The &#8467; 1 term ensures fidelity to the target, whereas the discriminator term promotes realism. The generator and discriminator are optimized in an alternating fashion, where the discriminator minimizes the following loss, &#8467; D ( y ^ , y ) = BCE ( 1 , d ( y ) ) + BCE ( 0 , d ( y ^ ) ) . (3) Intuitively, the discriminator wants to assign 1 to the real data point y and 0 to the fake data point from the generator y ^ . On the other side, the generator wants to &#8220;fool&#8221; the discriminator by making it output 1 to the generator&#8217;s outputs. 2.3.2 Pixel-level loss We employed MSE loss, defined as &#8467; 2 ( y ^ , y ) = &#8214; y &#8722; y ^ &#8214; 2 2 , (4) to quantify pixel-wise differences, emphasizing larger errors due to the squaring of terms. 2.3.3 Perceptual and structural loss We incorporated PSNR and SSIM to evaluate perceptual quality and structural fidelity. These metrics were used both as loss functions during training and as evaluation metrics post-training, &#8467; PSNR ( y ^ , y ) = &#8722; PSNR ( y ^ , y ) , (5) &#8467; SSIM ( y ^ , y ) = &#8722; SSIM ( y ^ , y ) . (6) To balance pixel-level accuracy and structural similarity, we combined PSNR and SSIM in a single loss function, &#8467; PSNR + SSIM = &#8467; PSNR + &#955; &#183; &#8467; SSIM , (7) with &#955; = 30 , determined empirically. 2.4 Training Details The simulated photoacoustic images from the DRIVE dataset are split into an 80/20 train-test ratio, with the last 20% of the train split acting as validation data. Thus, 154 images are used for training, 38 for validation, and 48 for testing. Similarly, the photoacoustic images from the NNE dataset are divided into a 67/33 train-test ratio, with the last 20% of the train split serving as validation data. Thus, 5084 images are used for training, 1271 for validation, and 3176 for testing. All these computations are done with a GPU H100 using Adam as an optimizer with a learning rate of 1 e &#8722; 4 and 200 epochs for training the model. 39 Furthermore, an experimental dataset comprising six test images, three test images from the DRIVE dataset, and three test images from NNE dataset. In this study, the experimental phantoms are only utilized to validate the findings on non-simulated data. The phantoms captured in water serve as ground truth for the images captured in a solution approximating tissue characteristics. The checkpoint for the final model is chosen by maximizing the SSIM score on the validation dataset. 2.5 Application in Breast Cancer Imaging To demonstrate applicability in breast cancer imaging, we have tested the models on both in vivo human breast tissue and a small animal tumor model. Acquired images were used to test the trained models. 2.5.1 Imaging in a small animal tumor model An imaging experiment was conducted on a small animal tumor model, part of a related study. 40 All animal protocols and experimental procedures were approved by the Institutional Animal Care and Use Committee (IACUC) of the University of Twente. Female SCID mice aged 5 to 7 weeks were procured from Janvier Laboratories and housed under appropriate conditions. The MDA-MB-231 human breast cancer cell line was cultured in high-glucose DMEM supplemented with L-glutamine (GE Healthcare) and 10% fetal bovine serum (FBS). Cells were maintained at 37&#176;C in a humidified incubator with 5% CO 2 . Each mouse was orthotopically injected with 2 &#215; 10 6 MDA-MB-231 cells to induce tumor formation. The endpoint is determined by the tumor volume of &#8764; 1000 &#8201;&#8201; mm 3 . The animal was euthanized, and the tumor was imaged immediately. 2.5.2 In vivo breast imaging The AcousticX system was utilized for in vivo breast imaging. Imaging was conducted on the left breast of a 31-year-old volunteer with Fitzpatrick skin type IV. Freehand PAI and ultrasound scanning were performed to collect data. Combined ultrasound and photoacoustic images of the breast vasculature were collected and used as test images. 3 Results and Discussion In this section, we first compare fluence (optical attenuation) compensation with Pix2Pix GANs as a baseline generative model, next we show the model predictions on simulated and experimental twin phantoms, followed by a detailed comparison of the generative models and loss function, and finally, we demonstrate its application in in vivo imaging. 3.1 Generative Model versus Attenuation Compensation First, we compare the predictions of a generative model (Pix2Pix GAN) with fluence compensation in PAI. Pix2Pix GANs have previously been employed to enhance photoacoustic images. 41 , 42 Figures&#160;3(a) and 3(e) show the input photoacoustic images from the DRIVE and NNE datasets, respectively. These simulations were performed on an absorbing and scattering medium, resulting in attenuation of fluence with depth. Figures&#160;3(b) and 3(f) display the ground truth photoacoustic images generated using only acoustic simulation and reconstruction, without an optical model, under the assumption of uniform fluence. Figures&#160;3(c) and 3(g) present the predictions from the Pix2Pix GANs model, whereas Figs.&#160;3(d) and 3(h) show the fluence-compensated images. Figures&#160;3(a) &#8211; 3(h) are obtained at a 30&#160;dB noise level in the simulation. In Figs.&#160;3(i) and 3(j) , we compare mean SSIM values computed over depth for the test images of the DRIVE dataset, for the Pix2Pix and fluence compensation methods, respectively. SSIM was computed along the depth direction by extracting image rows corresponding to 2&#160;mm depth intervals and including all columns for each slice. A similar comparison is presented for the NNE dataset in Figs.&#160;3(k) and 3(l) . Fig. 3 Generative model and fluence compensation comparison. (A&#8211;D) Input image, ground truth, Pix2Pix GANs prediction, and fluence compensated image on a test image from the DRIVE dataset. (E&#8211;H) Input image, ground truth, Pix2Pix GANs prediction, and fluence-compensated image on a test image from the NNE dataset. (I) Mean SSIM along depth for the Pix2Pix GANs prediction and (J) for the fluence compensation output on the DRIVE dataset test set. (K) Mean SSIM along depth for the Pix2Pix GANs prediction and (L) for the fluence compensation output on the NNE dataset test set. The results from different noise levels indicate that generative models, such as Pix2Pix GANs, can perform better than the fluence compensation methods by predicting the structure from the noisy input images. Given that the measurement noise also distorts the structures in the image, a direct compensation results in poor structural quality and amplifies noise with depth as evident in Figs.&#160;3(j) and 3(l) , where the SSIM value declines with depth at signal-to-noise ratio (SNR) levels of 10 and 20&#160;dB. Although Pix2Pix predictions also show a slight degradation in performance at higher noise levels, they remain comparatively better than the fluence compensation method, as seen in Figs.&#160;3(i) &#8211; 3(l) . These results underscore the potential of deep learning-based methods for compensating for optical attenuation and enhancing structures. 3.2 Simulation and Experimental Twin Phantoms We present the use of simulation and experimental twin phantoms to find the practical capability of predictive models. Figure&#160;4(a) shows the digital phantom. Figure&#160;4(b) displays the simulated photoacoustic image incorporating attenuation due to fluence, whereas Fig.&#160;4(c) presents the simulation considering uniform illumination. Figure&#160;4(d) illustrates the Pix2Pix prediction using Fig.&#160;4(b) as input. Figure&#160;4(e) shows the printed phantom on a polyurethane film. Figure&#160;4(f) depicts imaging in an absorbing and scattering medium, and Fig.&#160;4(g) shows imaging in water. Finally, Fig.&#160;4(h) demonstrates the Pix2Pix prediction using Fig.&#160;4(f) as input. Fig. 4 Simulation and experimental photoacoustic twin phantoms: (A) Digital image of the phantom. (B) Simulated photoacoustic image with fluence in a tissue-mimicking medium. (C) Ground truth photoacoustic image assuming uniform fluence. (D) Pix2Pix prediction with (B) as input. (E) Printed phantom on a polyurethane film. (F) PAI in a tissue-mimicking medium. (G) PAI in water. (H) Pix2Pix prediction with (F) as input. The results demonstrate that we can obtain experimental phantoms with structural ground truth information to compare with model predictions. This approach facilitates testing of deep learning models using similar simulated and experimental data. Furthermore, the ground truth information available in the experimental data enables verification of the prediction accuracy. However, discrepancies between the simulated and experimental results suggest that accurate modeling of all experimental parameters is necessary. Although we have considered the transducer and medium parameters here, the modeling is based on ideal system specifications and lacks fine-tuning based on actual measurements. The key takeaway is the availability of structural information to evaluate how the model performs on experimental data. It can be observed that Pix2Pix predictions enhance imaging depth compared with the input in both simulated and experimental phantoms. However, when compared with the ground truth images, the depth recovery in the Pix2Pix predictions is limited. The vascular structures that appear enhanced are largely the same ones already faintly visible in the input images at lower pixel intensities, rather than new structures revealed at greater depth. Pix2Pix predictions enhanced low photoacoustic signal structures with low background noise. Given that the Pix2Pix prediction only provided marginal improvement at deeper structures, we further compare Res18&#160;U-Net and the proposed Trans U-Net and loss functions to identify the most suitable model for this task. 3.3 Comparison of Generative Models and Loss Functions In Fig.&#160;5 , we compare the performance of the three models, Pix2Pix, Res18&#160;U-Net and Trans U-Net on input images from noise levels ranging from 10 to 50&#160;dB. We also compare different loss functions. The performance is compared in terms of image quality metrics SSIM and PSNR listed together. The heatmap shows SSIM to provide a visual impression of model performance. Figure&#160;5 left, shows the performance of models on the DRIVE dataset, and Fig.&#160;5 right corresponds to the NNE dataset. Fig. 5 The table shows SSIM and PSNR with heatmaps based on SSIM value, illustrate the performance of three models, Pix2Pix, Res18&#160;U-Net, and Trans U-Net, evaluated using various loss functions: GAN, l 2 (MSE), PSNR, SSIM, and a combined SSIM + PSNR (S + P). Performance is assessed across noise levels ranging from 10&#160;dB to 50&#160;dB. The table on the left presents results for models trained on the DRIVE dataset, whereas the table on the right shows results for models trained on the NNE dataset. When trained with the pre-specified SSIM + PSNR loss, Trans U-Net is statistically superior to Res18-UNet and Pix2Pix for SSIM and PSNR on both datasets across noise levels, except for a single non-significant PSNR comparison on DRIVE at 10&#160;dB. Under alternative loss functions, performance is mixed and in some settings other models are comparable or better, as detailed in Table&#160;S1 in the Supplementary Material . This suggests that the proposed transformer-based architecture is more robust in handling fluence attenuation and noise in photoacoustic images. Among the loss functions tested, SSIM and combined SSIM + PSNR yield the highest SSIM and PSNR values, indicating their effectiveness in preserving structural and perceptual quality. Notably, the SSIM loss function performs best on the DRIVE dataset, whereas the combined SSIM + PSNR achieves the highest scores on the NNE dataset, suggesting that the optimal loss function may be dataset-dependent. The NNE dataset consistently shows higher SSIM and PSNR values across all models and loss functions compared with DRIVE, implying that NNE images may be less complex or contain more consistent features that are easier to reconstruct. Overall, the combination of Trans U-Net with loss functions SSIM or SSIM + PSNR provides the most reliable performance, and the results highlight the importance of tailoring model-loss configurations to specific dataset characteristics. 3.3.1 Statistical significance We have performed a statistical paired t-test to show which model is superior in terms of PSNR and SSIM for the different loss function compared for different models in various noise conditions. For comparisons, we have compared the best performing model with the second best model. Trans U-Net trained with SSIM + PSNR shows significant improvements in most comparisons on both datasets, with all NNE comparisons significant and one non-significant PSNR comparison on DRIVE at 10&#160;dB. The best results were obtained using the combined SSIM + PSNR loss. These improvements were statistically significant ( p &lt; 0.05 ) in most cases ( Table&#160;1 ). Detailed per-loss and per-noise-level comparisons are provided in Table&#160;S1 in the Supplementary Material . Table 1 Statistical significance of Trans U-Net compared with Pix2Pix and Res18 U-Net across different metrics, noise levels, and two datasets. Significance determined by paired t -test ( p &lt; 0.05 ) for the S + P loss function. Dataset Noise (dB) Trans U-Net versus Pix2Pix Trans U-Net versus Res18 U-Net PSNR SSIM PSNR SSIM DRIVE Dataset 10 No Yes No Yes 20 Yes Yes Yes Yes 30 Yes Yes Yes Yes 40 Yes Yes Yes Yes 50 Yes Yes Yes Yes NNE 10 dataset 10 Yes Yes Yes Yes 20 Yes Yes Yes Yes 30 Yes Yes Yes Yes 40 Yes Yes Yes Yes 50 Yes Yes Yes Yes 3.3.2 Testing on a vascular printed phantom We tested trained models on a printed vascular phantom. Figure&#160;6 shows the phantoms and the model prediction. Ground truth vascular structures are shown in Figs.&#160;6(a) , 6(g) , 6(m) , and 6(s) . The corresponding physical phantoms printed in water are presented in Figs.&#160;6(b) , 6(h) , 6(n) , and 6(t) , as a reference in a non-scattering medium to visualize deeper structures, whereas Figs.&#160;6(c) , 6(i) , 6(o) , and 6(u) display the same phantoms embedded in a tissue-mimicking medium, demonstrating depth-dependent photoacoustic signal attenuation. Model predictions are shown in the subsequent rows: Pix2Pix outputs are illustrated in Figs.&#160;6(d) , 6(j) , 6(p) , and 6(v) , Res18&#160;U-Net predictions in Figs.&#160;6(e) , 6(k) , 6(q) , and 6(w) , and Trans U-Net predictions in Figs.&#160;6(f) , 6(l) , 6(r) , and 6(x) . The colored arrows in ground truth digital images and in the model predictions show structures that are enhanced by the model. Visually, it can be observed that Trans U-Net enhanced most structures compared with the digital ground truth image, followed by Res18&#160;U-Net. Pix2Pix predictions resulted in the least depth enhancement. We computed the relative SNR as 10log 10 () of the ratio between the mean squared value of the reference image patch and the MSE between the predicted image patch and the reference patch obtained in water. Quantitative evaluation of the relative SNR for marked vascular structures revealed the models&#8217; performance. Pix2Pix achieved an average SNR of 2.3 &#177; 1.5 &#8201;&#8201; dB , indicating limited enhancement and high variability. In contrast, Res18&#160;U-Net and Trans U-Net demonstrated substantially higher SNRs of 5.4 &#177; 1.3 and 5.3 &#177; 1.2 &#8201;&#8201; dB , respectively, reflecting improved signal clarity and consistency. For the printed phantoms, Res18&#160;U-Net and the proposed Trans U-Net showed comparable performance in terms of SNR. Fig. 6 Imaging of Printed Vascular Phantoms. (A, G, M, S) show the ground truth vascular phantoms. Among these, (A, G, M) are from the test dataset, whereas (S) represents a mouse brain vascular phantom not included in the dataset. 27 (B, H, N, T) display the phantoms printed in water. (C, I, O, U) show the printed phantoms embedded in tissue-mimicking medium, illustrating depth-dependent attenuation. (D, J, P, V) present predictions from the Pix2Pix model. (E, K, Q, W) show predictions from the Res18&#160;U-Net model. (F, L, R, X) depict predictions from the Trans U-Net model. Colored arrows highlight vascular structures accurately predicted by the models. 3.4 In Vivo Tumor Imaging We evaluated Pix2Pix, Res18&#160;U-Net, and Trans U-Net on in vivo murine breast tumor models and on in vivo human breast images to demonstrate applicability to breast cancer imaging. Figures&#160;7(a) &#8211; 7(l) show two small-animal tumors and Figs.&#160;7(m) &#8211; 7(r) show human breast imaging. Figures&#160;7(a) , 7(g) , and 7(m) are ultrasound. Figures&#160;7(b) , 7(h) , and 7(n) are the corresponding photoacoustic images in log scale. Figures&#160;7(c) , 7(i) , and 7(o) are photoacoustic overlaid on ultrasound. Figures&#160;7(d) , 7(j) , and 7(p) are Pix2Pix predictions. Figures&#160;7(e) , 7(k) , and 7(q) are Res18&#160;U-Net, and Figs.&#160;7(f) , 7(l) , and 7(r) are Trans U-Net. The arrows locate vascular features that are faint but present in the log-scale input photoacoustic images, and, where visible, hypoechoic tracks on ultrasound that often co-localize with blood vessels. These markings anchor the qualitative assessment to signals already present in the raw data rather than to structures created by the networks. Feeding blood vessels (FV) and their branching into the tumor can be observed in the enhanced images in both tumors. Fig. 7 In vivo testing. (A&#8211;L) Model testing on small animal breast tumor images, (M&#8211;R) Model testing on human breast imaging. (A, G, and M) are ultrasound images. (B, H, and N) Photoacoustic images in log scale. (C, I, O) Overlaid photoacoustic images on ultrasound images. (D, J, and P) Prediction using Pix2Pix. (E, K, and Q) Prediction using Res18 and (F, L, and R) prediction using Trans U-Net. (Arrows with FV and A indicate feeding blood vessel and artefacts, respectively). Across both murine tumors and human breast, Trans U-Net produced the clearest enhancement of the vascular signals than Pix2Pix, and generally stronger contrast than Res18&#160;U-Net. Some examples of Trans U-Net enhancing vascular structures better than Res18 and Pix2Pix are arrow marked 1 in Figs.&#160;7(b) and 7(f) , and arrow marked 7 in Figs.&#160;7(n) and 7(r) . Some false predictions are also visible. Limited-view, low-SNR traces can be overconnected into vessel-like strands, and reconstruction artefacts outside tissue can be modified into vascular-like patterns by all three models, most prominently by Pix2Pix, but also Res18 and Trans U-Net. We therefore restrict interpretation to regions where the enhanced predictions correspond to signals visible in the log-scale image and, when available, to hypoechoic tracks on ultrasound. These in vivo observations are consistent with quantitative trends in simulated test sets, with the Trans U-Net trained with SSIM + PSNR loss achieving the highest attenuation compensation and denoising. 4 Limitations and Future Work The experimental phantoms were used only for testing the model. Training the model on experimental data could improve performance, but this requires a scalable method to automate the imaging of printed films and generate larger datasets. The phantoms, constrained by limited thickness, do not fully replicate the 3D nature of blood vessels. Our focus was on structural recovery rather than signal quantification. We have previously shown that varying film opacity can simulate different absorption levels. 29 Future work will investigate the quantification of recovered signals for applications such as oxygen saturation. In vivo , there is no absolute ground truth. To avoid over-claiming, we anchored our interpretation to two forms of evidence visible in the photoacoustic signal in the log-scale image; second, spatial co-localization and ultrasound hypoechoic tracks. Using this conservative approach, Trans U-Net most consistently enhanced vessel-like structures that were already detectable. At the same time, we explicitly show false predictions, including the conversion of reconstruction artefacts into vessel-like patterns and the overconnection of weak, limited-view traces. These observations caution against treating the network output as a vascular prediction in vivo and motivate future validation with an independent modality such as Doppler or contrast-enhanced ultrasound and inclusion of artifact exemplars during training. Our simulation and phantom experiments, where ground truth is available, show that Trans U-Net is the most accurate model by SSIM and PSNR and has a slight advantage over Res18&#160;U-Net on printed phantoms and in vivo scenarios. We also observed variability in model performance across datasets. Although we compared a state-of-the-art Transformer-based model, this variability remains evident. In contrast to common observations that U-Nets perform better on smaller datasets, in our study the transformer-based model outperformed both Res18&#160;U-Net and Pix2Pix, likely reflecting the combined effect of dataset quality and the ability of transformers to capture long-range dependencies. Although the model was trained exclusively on simulated data, which allowed systematic validation with ground-truth access, we acknowledge that this may limit its direct generalization to experimental and in vivo conditions; future work will focus on fine-tuning and validating the framework on empirical datasets using transfer learning and domain adaptation strategies. Developing a universal model 43 that is robust across multiple PAI systems and tissue types remains a key goal for advancing PAI. Extending this approach using newer generative models, such as Trans U-Net, is a promising future direction. 5 Conclusion We addressed the coupled problem of fluence attenuation and denoising using generative models. We proposed a ViT-based Trans U-Net and compared it with state-of-the-art models such as Pix2Pix and Residual U-Net (Res18). On simulated datasets, Trans U-Net combined with the SSIM + PSNR loss function showed superior performance in recovering vascular structures and denoising photoacoustic images. In printed experimental phantoms, however, Res18&#160;U-Net and Trans U-Net showed comparable performance, whereas Pix2Pix performed relatively worse. The SSIM + PSNR loss function emerged as the most effective, balancing structural fidelity and noise suppression. In vivo experiments further validated the practical utility of these models, with Trans U-Net notably enhancing vascular visualization in small animal tumor imaging. These findings highlight the potential of generative models, particularly Trans U-Net, to improve image quality in PAI and support more reliable and informative biomedical applications. Supplementary Material 10.1117/1.JBO.30.11.116004.s01 Acknowledgments We extend our gratitude to Jai Prakash and Mithun Kuniyil Ajith Singh for their support in the in vivo imaging study. We are also grateful to Sumohana S. Channappayya for his valuable insights on using generative models to enhance image quality. Finally, we thank Maura Dantuma for her assistance in developing the simulation pipeline. Cristian Perez Jensen is a master&#8217;s student in computer science at ETH Z&#252;rich, specializing in machine intelligence with a minor in theoretical computer science. He holds a bachelor&#8217;s degree in artificial intelligence from the University of Amsterdam. Navchetan Awasthi received his PhD in medical imaging from the Indian Institute of Science (IISc), Bangalore, India, in 2019. He is currently an assistant professor at the University of Amsterdam, The Netherlands. His research interests include inverse problems in biomedical optics, medical image analysis, medical image reconstruction, biomedical signal processing, and deep learning. Kalloor Joseph Francis , an assistant professor in the Biomedical Engineering Group at the Cardiology Department of Erasmus MC, Netherlands. He completed his PhD at the Indian Institute of Technology, Hyderabad, India. He was a former Fulbright fellow at the Rochester Institute of Technology. His research encompasses the development of photoacoustic imaging systems, tissue quantification algorithms, and their clinical applications. Disclosures The authors declare that there are no financial interests, commercial affiliations, or other potential conflicts of interest that could have influenced the objectivity of this research or the writing of this paper. Code and Data Availability The code and dataset for the models are available at https://github.com/navchetan-awasthi/pai-reconstruction . Funding This work has received financial support from the Dutch Research Council (NWO) for the project NWO-VENI (Grant No. 19165) and ZonMw for the project Off Road (Grant No. 04510012210042). References 1. Wang L. V. Hu S. , &#8220; Photoacoustic tomography: in vivo imaging from organelles to organs ,&#8221; Science 335 ( 6075 ), 1458 &#8211; 1462 ( 2012 ). SCIEAS 0036-8075 10.1126/science.1216210 22442475 PMC3322413 2. Steinberg I. et al. , &#8220; Photoacoustic clinical imaging ,&#8221; Photoacoustics 14 , 77 &#8211; 98 ( 2019 ). 10.1016/j.pacs.2019.05.001 31293884 PMC6595011 3. Wang Y. et al. , &#8220; Optimizing the light delivery of linear-array-based photoacoustic systems by double acoustic reflectors ,&#8221; Sci. Rep. 8 ( 1 ), 13004 ( 2018 ). SRCEC3 2045-2322 10.1038/s41598-018-31430-5 30158556 PMC6115359 4. Francis K. J. Rascevska E. Manohar S. , &#8220; Photoacoustic imaging assisted radiofrequency ablation: Illumination strategies and prospects ,&#8221; in TENCON 2019-2019 IEEE Region 10 Conf. (TENCON) , IEEE , pp.&#160; 118 &#8211; 122 ( 2019 ). 10.1109/TENCON.2019.8929646 5. Li M. et al. , &#8220; Linear array-based real-time photoacoustic imaging system with a compact coaxial excitation handheld probe for noninvasive sentinel lymph node mapping ,&#8221; Biomed. Opt. Express 9 ( 4 ), 1408 &#8211; 1422 ( 2018 ). BOEICL 2156-7085 10.1364/BOE.9.001408 29675292 PMC5905896 6. Awasthi N. et al. , &#8220; Deep neural network-based sinogram super-resolution and bandwidth enhancement for limited-data photoacoustic tomography ,&#8221; IEEE Trans. Ultrason. Ferroelectr. Freq. Control 67 ( 12 ), 2660 &#8211; 2673 ( 2020 ). ITUCER 0885-3010 10.1109/TUFFC.2020.2977210 32142429 7. Zhao L. et al. , &#8220; Optical fluence compensation for handheld photoacoustic probe: an in vivo human study case ,&#8221; J. Innov. Opt. Health Sci. 10 ( 4 ), 1740002 ( 2017 ). 10.1142/S1793545817400028 8. Bulsink R. et al. , &#8220; Oxygen saturation imaging using led-based photoacoustic system ,&#8221; Sensors 21 ( 1 ), 283 ( 2021 ). SNSRES 0746-9462 10.3390/s21010283 33406653 PMC7795655 9. Han T. et al. , &#8220; A three-dimensional modeling method for quantitative photoacoustic breast imaging with handheld probe ,&#8221; Photoacoustics 21 , 100222 ( 2021 ). 10.1016/j.pacs.2020.100222 33318929 PMC7726342 10. Bauer A. Q. et al. , &#8220; Quantitative photoacoustic imaging: correcting for heterogeneous light fluence distributions using diffuse optical tomography ,&#8221; J. Biomed. Opt. 16 ( 9 ), 096016 ( 2011 ). JBOPFO 1083-3668 10.1117/1.3626212 21950930 PMC3188642 11. Hussain A. et al. , &#8220; Photoacoustic and acousto-optic tomography for quantitative and functional imaging ,&#8221; Optica 5 ( 12 ), 1579 &#8211; 1589 ( 2018 ). 10.1364/OPTICA.5.001579 12. Kim M. et al. , &#8220; Correction of wavelength-dependent laser fluence in swept-beam spectroscopic photoacoustic imaging with a hand-held probe ,&#8221; Photoacoustics 19 , 100192 ( 2020 ). 10.1016/j.pacs.2020.100192 32670789 PMC7339128 13. Jeng G.-S. et al. , &#8220; Real-time interleaved spectroscopic photoacoustic and ultrasound (paus) scanning with simultaneous fluence compensation and motion correction ,&#8221; Nat. Commun. 12 ( 1 ), 716 ( 2021 ). NCAOBW 2041-1723 10.1038/s41467-021-20947-5 33514737 PMC7846772 14. Gholampour A. et al. , &#8220; Deep learning-based methods for photoacoustic imaging reconstruction: concepts, promises, pitfalls, and futures ,&#8221; in Biomedical Photoacoustics: Technology and Applications , Xia W. , Eds., pp.&#160; 155 &#8211; 177 , Springer ( 2024 ). 15. Hariri A. et al. , &#8220; Deep learning improves contrast in low-fluence photoacoustic imaging ,&#8221; Biomed. Opt. Express 11 ( 6 ), 3360 &#8211; 3373 ( 2020 ). BOEICL 2156-7085 10.1364/BOE.395683 32637260 PMC7316023 16. Manwar R. et al. , &#8220; Deep learning protocol for improved photoacoustic brain imaging ,&#8221; J. Biophotonics 13 ( 10 ), e202000212 ( 2020 ). 10.1002/jbio.202000212 33405275 PMC10906453 17. Paul A. Mallidi S. , &#8220; U-Net enhanced real-time led-based photoacoustic imaging ,&#8221; J. Biophotonics 17 ( 6 ), e202300465 ( 2024 ). 10.1002/jbio.202300465 38622811 PMC11164633 18. Yazdani A. et al. , &#8220; Simultaneous denoising and localization network for photoacoustic target localization ,&#8221; IEEE Trans. Med. Imaging 40 ( 9 ), 2367 &#8211; 2379 ( 2021 ). ITMID4 0278-0062 10.1109/TMI.2021.3077187 33939612 PMC8526152 19. Cai C. et al. , &#8220; End-to-end deep neural network for optical inversion in quantitative photoacoustic imaging ,&#8221; Opt. Lett. 43 ( 12 ), 2752 &#8211; 2755 ( 2018 ). OPLEDP 0146-9592 10.1364/OL.43.002752 29905680 20. Yang C. et al. , &#8220; Quantitative photoacoustic blood oxygenation imaging using deep residual and recurrent neural network ,&#8221; in IEEE 16th Int. Symp. Biomed. Imaging (ISBI 2019) , IEEE , pp.&#160; 741 &#8211; 744 ( 2019 ). 10.1109/ISBI.2019.8759438 21. Luke G. P. et al. , &#8220; O-Net: a convolutional neural network for quantitative photoacoustic image segmentation and oximetry ,&#8221; arXiv:1911.01935 ( 2019 ). 22. Yang C. Gao F. , &#8220; EDA-Net: dense aggregation of deep and shallow information achieves quantitative photoacoustic blood oxygenation imaging deep in human breast ,&#8221; Lect. Notes Comput. Sci. 11764 , 246 &#8211; 254 ( 2019 ). LNCSD9 0302-9743 10.1007/978-3-030-32239-7_28 23. Gr&#246;hl J. et al. , &#8220; Estimation of blood oxygenation with learned spectral decoloring for quantitative photoacoustic imaging (LSD-QPAI) ,&#8221; arXiv:1902.05839 ( 2019 ). 24. Staal J. et al. , &#8220; Ridge-based vessel segmentation in color images of the retina ,&#8221; IEEE Trans. Med. Imaging 23 ( 4 ), 501 &#8211; 509 ( 2004 ). ITMID4 0278-0062 10.1109/TMI.2004.825627 15084075 25. Uhlirova H. et al. , &#8220; Neurovascular network explorer 2.0: a database of 2-photon single-vessel diameter measurements from mouse Si cortex in response to optogenetic stimulation ,&#8221; Front. Neuroinform. 11 , 4 ( 2017 ). 10.3389/fninf.2017.00004 28203155 PMC5285378 26. Fang Q. Boas D. A. , &#8220; Monte Carlo simulation of photon migration in 3d turbid media accelerated by graphics processing units ,&#8221; Opt. express 17 ( 22 ), 20178 &#8211; 20190 ( 2009 ). OPEXFF 1094-4087 10.1364/OE.17.020178 19997242 PMC2863034 27. Treeby B. E. Cox B. T. , &#8220; k-wave: Matlab toolbox for the simulation and reconstruction of photoacoustic wave fields ,&#8221; J. Biomed. Opt. 15 ( 2 ), 021314 ( 2010 ). JBOPFO 1083-3668 10.1117/1.3360308 20459236 28. Jaeger M. et al. , &#8220; Fourier reconstruction in optoacoustic imaging using truncated regularized inverse k-space interpolation ,&#8221; Inverse Probl. 23 ( 6 ), S51 ( 2007 ). INPEEY 0266-5611 10.1088/0266-5611/23/6/S05 29. Morsink C. F. et al. , &#8220; Design and characterization of color printed polyurethane films as biomedical phantom layers ,&#8221; Biomed. Opt. Express 14 ( 9 ), 4485 &#8211; 4506 ( 2023 ). BOEICL 2156-7085 10.1364/BOE.491695 37791261 PMC10545194 30. Jacques S. L. , &#8220; Optical properties of biological tissues: a review ,&#8221; Phys. Med. Biol. 58 ( 11 ), R37 ( 2013 ). PHMBA7 0031-9155 10.1088/0031-9155/58/11/R37 23666068 31. Liu Y. Maruvada S. , &#8220; Development and characterization of polyurethane-based tissue and blood mimicking materials for high intensity therapeutic ultrasound ,&#8221; J. Acoust. Soc. Amer. 151 ( 5 ), 3043 &#8211; 3051 ( 2022 ). JASMAN 0001-4966 10.1121/10.0010385 35649924 32. Azhari H. , Basics of Biomedical Ultrasound for Engineers , John Wiley &amp; Sons ( 2010 ). 33. Francis K. J. et al. , &#8220; Tomographic ultrasound and led-based photoacoustic system for preclinical imaging ,&#8221; Sensors 20 ( 10 ), 2793 ( 2020 ). SNSRES 0746-9462 10.3390/s20102793 32422995 PMC7294432 34. He K. et al. , &#8220; Deep residual learning for image recognition ,&#8221; in Proc. IEEE Conf. Comput. Vision and Pattern Recognit. , pp.&#160; 770 &#8211; 778 ( 2016 ). 10.1109/CVPR.2016.90 35. Ronneberger O. Fischer P. Brox T. , &#8220; U-Net: convolutional networks for biomedical image segmentation ,&#8221; Lect. Notes Comput. Sci. 9351 , 234 &#8211; 241 ( 2015 ). LNCSD9 0302-9743 10.1007/978-3-319-24574-4_28 36. Isola P. et al. , &#8220; Image-to-image translation with conditional adversarial networks ,&#8221; in Proc. IEEE Conf. Comput. Vision and Pattern Recognit. , pp.&#160; 1125 &#8211; 1134 ( 2017 ). 10.1109/CVPR.2017.632 37. Chen J. et al. , &#8220; TransUNet: transformers make strong encoders for medical image segmentation ,&#8221; arXiv:2102.04306 ( 2021 ). 38. Dosovitskiy A. , &#8220; An image is worth 16x16 words: transformers for image recognition at scale ,&#8221; arXiv:2010.11929 ( 2020 ). 39. Kingma D. P. , &#8220; Adam: a method for stochastic optimization ,&#8221; arXiv:1412.6980 ( 2014 ). 40. Lu T. et al. , &#8220; In vivo detection of circulating cancer-associated fibroblasts in breast tumor mouse xenograft: impact of tumor stroma and chemotherapy ,&#8221; Cancers 15 ( 4 ), 1127 ( 2023 ). 10.3390/cancers15041127 36831470 PMC9954095 41. Francis K. J. et al. , &#8220; Generative adversarial network-based photoacoustic image reconstruction from bandlimited and limited-view data ,&#8221; Proc. SPIE 11642 , 1164235 ( 2021 ). PSISDG 0277-786X 10.1117/12.2577750 42. Vu T. et al. , &#8220; A generative adversarial network for artifact removal in photoacoustic computed tomography with a linear-array transducer ,&#8221; Exp. Biol. Med. 245 ( 7 ), 597 &#8211; 605 ( 2020 ). EXBMAA 0071-3384 10.1177/1535370220914285 PMC7153213 32208974 43. Stam O. J. Francis K. J. Awasthi N. , &#8220; PA OmniNet: a retraining-free, generalizable deep learning framework for robust photoacoustic image reconstruction ,&#8221; Photoacoustics 45 , 100740 ( 2025 ). 10.1016/j.pacs.2025.100740 40703536 PMC12284555"
}