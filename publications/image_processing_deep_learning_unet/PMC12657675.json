{
  "pmcid": "PMC12657675",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.337267",
  "metadata": {
    "journal_title": "Brain Informatics",
    "journal_nlm_ta": "Brain Inform",
    "journal_iso_abbrev": "Brain Inform",
    "journal": "Brain Informatics",
    "pmcid": "PMC12657675",
    "pmid": "41296223",
    "doi": "10.1186/s40708-025-00280-z",
    "title": "Synergistic medical genetic evolutionary optimization and deep convolutional generative augmentation with SHAP-driven interpretability for precise Alzheimer’s disease severity grading",
    "year": "2025",
    "month": "11",
    "day": "26",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "26"
    },
    "authors": [
      "Bharath H. C.",
      "Pradeep N.",
      "Shashidhar R.",
      "Nanjappa Yashwanth"
    ],
    "abstract": "Alzheimer’s disease (AD) diagnosis at an early yet accurate stage is critical to support effective treatment or intervention. Still it is not very feasible due to the presence of image data class imbalance, low interpretability of models, and a high computational cost. This research proposes a novel, end-to-end diagnostic framework that considers a Medical Genetic Algorithm (MedGA)-optimized Convolutional Neural Network (CNN) with a Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic MRIs and SHapley Additive Explanations (SHAP) to analyse and interpret the model. The given methodology is trained and tested on the Open Access Series of Imaging Studies (OASIS) dataset. The DCGAN component introduces 700 structurally coherent synthetic images (SSIM = 0.92) into the underrepresented Moderate Dementia class, improving the overall recall by 10% and balancing the dataset. MedGA succeeds in optimizing CNN hyperparameters and resulting in complexity reduction (20%) in networks without loss of testing accuracy (97%) at the four demonstrated stages of AD: Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented. SHAP analysis emphasises the role of key brain areas, the hippocampus and the amygdala in the results of classification accuracy, leading to 25% greater interpretability and clinician confidence. Comparative evaluation shows that the current framework is exceptionally better in terms of predictive performance and explainability than current state-of-the-art approaches. This combined method provides a powerful and adaptable device to categorize AD at an early age, with promising outcomes in precise diagnosis in health facilities.",
    "keywords": [
      "Alzheimer’s disease (AD)",
      "Medical genetic algorithm (MedGA)",
      "Convolutional neural networks (CNN)",
      "Deep convolutional generative adversarial network (DCGAN)",
      "SHAP (SHapley additive explanations) and augmentation"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Brain Inform</journal-id><journal-id journal-id-type=\"iso-abbrev\">Brain Inform</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3006</journal-id><journal-id journal-id-type=\"pmc-domain\">bi</journal-id><journal-title-group><journal-title>Brain Informatics</journal-title></journal-title-group><issn pub-type=\"ppub\">2198-4018</issn><issn pub-type=\"epub\">2198-4026</issn><publisher><publisher-name>Springer</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12657675</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12657675.1</article-id><article-id pub-id-type=\"pmcaid\">12657675</article-id><article-id pub-id-type=\"pmcaiid\">12657675</article-id><article-id pub-id-type=\"pmid\">41296223</article-id><article-id pub-id-type=\"doi\">10.1186/s40708-025-00280-z</article-id><article-id pub-id-type=\"publisher-id\">280</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Synergistic medical genetic evolutionary optimization and deep convolutional generative augmentation with SHAP-driven interpretability for precise Alzheimer&#8217;s disease severity grading</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Bharath</surname><given-names initials=\"HC\">H. C.</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Pradeep</surname><given-names initials=\"N\">N.</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\">1</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Shashidhar</surname><given-names initials=\"R\">R.</given-names></name><xref ref-type=\"aff\" rid=\"Aff2\">2</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Nanjappa</surname><given-names initials=\"Y\">Yashwanth</given-names></name><address><email>yashwanth.n@manipal.edu</email></address><xref ref-type=\"aff\" rid=\"Aff3\">3</xref></contrib><aff id=\"Aff1\"><label>1</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/05ddbg479</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 0501 3484</institution-id><institution>Bapuji Institute of Engineering and Technology, Davangere, </institution><institution>Affiliated to Visvesvaraya Technological University, </institution></institution-wrap>Belagavi, 590018 India </aff><aff id=\"Aff2\"><label>2</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04mnmkz07</institution-id><institution-id institution-id-type=\"GRID\">grid.512757.3</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1761 9897</institution-id><institution>Department of Electronics and Communication Engineering, </institution><institution>JSS Science and Technology University, </institution></institution-wrap>Mysuru, 570006 India </aff><aff id=\"Aff3\"><label>3</label><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/02xzytt36</institution-id><institution-id institution-id-type=\"GRID\">grid.411639.8</institution-id><institution-id institution-id-type=\"ISNI\">0000 0001 0571 5193</institution-id><institution>Department of Electronics and Communication Engineering, </institution><institution>Manipal Institute of Technology, Manipal Academy of Higher Education, </institution></institution-wrap>Manipal, 576104 India </aff></contrib-group><pub-date pub-type=\"epub\"><day>26</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>12</month><year>2025</year></pub-date><volume>12</volume><issue>1</issue><issue-id pub-id-type=\"pmc-issue-id\">479604</issue-id><elocation-id>31</elocation-id><history><date date-type=\"received\"><day>20</day><month>7</month><year>2025</year></date><date date-type=\"accepted\"><day>18</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>26</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>28</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 12:25:12.270\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"40708_2025_Article_280.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Alzheimer&#8217;s disease (AD) diagnosis at an early yet accurate stage is critical to support effective treatment or intervention. Still it is not very feasible due to the presence of image data class imbalance, low interpretability of models, and a high computational cost. This research proposes a novel, end-to-end diagnostic framework that considers a Medical Genetic Algorithm (MedGA)-optimized Convolutional Neural Network (CNN) with a Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic MRIs and SHapley Additive Explanations (SHAP) to analyse and interpret the model. The given methodology is trained and tested on the Open Access Series of Imaging Studies (OASIS) dataset. The DCGAN component introduces 700 structurally coherent synthetic images (SSIM&#8201;=&#8201;0.92) into the underrepresented Moderate Dementia class, improving the overall recall by 10% and balancing the dataset. MedGA succeeds in optimizing CNN hyperparameters and resulting in complexity reduction (20%) in networks without loss of testing accuracy (97%) at the four demonstrated stages of AD: Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented. SHAP analysis emphasises the role of key brain areas, the hippocampus and the amygdala in the results of classification accuracy, leading to 25% greater interpretability and clinician confidence. Comparative evaluation shows that the current framework is exceptionally better in terms of predictive performance and explainability than current state-of-the-art approaches. This combined method provides a powerful and adaptable device to categorize AD at an early age, with promising outcomes in precise diagnosis in health facilities.</p></abstract><abstract id=\"Abs2\" abstract-type=\"Highlights\"><title>Highlights</title><p id=\"Par2\">Introducing a novel framework MedGA-CNN, combined with DCGAN and SHAP for grading and interpreting the Alzheimer&#8217;s disease.</p><p id=\"Par3\">Utilizing the DC-GAN architecture for generating synthetic images in the minority class to handle data imbalance.</p><p id=\"Par4\">Incorporating MedGA with custom CNN for optimization of the parameters and computational complexity with high accuracy.</p><p id=\"Par5\">SHAP is applied to identify important areas in the brain (e.g., hippocampus, amygdala) for grading AD classes, which is compared with clinical biomarkers and enhances confidence in predictions made with the model.</p><p id=\"Par06\">Comparing the proposed work against the latest state-of-the-art (SOTA) techniques used for MRI image classification toward AD.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Alzheimer&#8217;s disease (AD)</kwd><kwd>Medical genetic algorithm (MedGA)</kwd><kwd>Convolutional neural networks (CNN)</kwd><kwd>Deep convolutional generative adversarial network (DCGAN)</kwd><kwd>SHAP (SHapley additive explanations) and augmentation</kwd></kwd-group><funding-group><award-group><funding-source><institution>Manipal Academy of Higher Education, Manipal</institution></funding-source></award-group><open-access><p>Open access funding provided by Manipal Academy of Higher Education, Manipal</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer-Verlag GmbH Germany, part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par7\">Alzheimer&#8217;s disease (AD) is a neurodegenerative disorder affecting over 50 million patients worldwide, and the prevalence rate is expected to triple by the year 2050 due to population aging [<xref ref-type=\"bibr\" rid=\"CR1\">1</xref>, <xref ref-type=\"bibr\" rid=\"CR2\">2</xref>]. Early and accurate diagnosis plays a significant role in making sure the disease is treated before it escalates to a serious condition that threatens the quality of life of the affected individual [<xref ref-type=\"bibr\" rid=\"CR3\">3</xref>]. Neuroimaging, in broad terms, and magnetic resonance imaging (MRI) in specific, have become an exciting way of detecting AD and recently, deep learning models, such as CNN have achieved excellent classification rates [<xref ref-type=\"bibr\" rid=\"CR4\">4</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR6\">6</xref>]. However, these models have two key limitations that limit their use in clinical practice: the very significant class imbalance handling, especially the insufficiency of representation of some phenotypic levels such as the presence of moderate dementia, and the limited interpretability of high-order models such neural networks which risks clinician trust, and makes informed medical decision-makers using such models more difficult.</p><p id=\"Par8\">Alzheimer&#8217;s is also among the top four leading causes of death among elderly people between 2022 and 2024 in the US, with the number of deaths in the U.S. increasing by about 114,000 deaths in 2022 to 121,500 deaths in 2023. In 2024, it was estimated that 6.9 million Americans aged 65&#160;years and above lived with Alzheimer&#8217;s dementia. The number of patients with moderate to severe AD levels increases worldwide, indicating the importance of diagnostic models capable of quick and easy identification of the problem to perform clinical countermeasures [<xref ref-type=\"bibr\" rid=\"CR3\">3</xref>, <xref ref-type=\"bibr\" rid=\"CR7\">7</xref>].</p><p id=\"Par9\">To overcome these problems, a new schematic technique is developed that synergistically combines and includes Medical Genetic Algorithm (MedGA) to optimize the CNN architecture, Deep Convolutional Generative Adversarial Network (DCGAN) to balance out minority class data, and SHAP (SHapley Additive Explanations) to give these interpretable predictions [<xref ref-type=\"bibr\" rid=\"CR8\">8</xref>, <xref ref-type=\"bibr\" rid=\"CR9\">9</xref>]. This method aims to address the issue of class imbalance within the OASIS dataset by synthesizing high-quality MRI images of the moderate dementia class. MedGA streamlines the CNN towards accuracy and computation efficiency by reducing the parameters, and SHAP visualizes the information about the brain structures and regions (e.g., hippocampus, amygdala) that contribute to classification, which corresponds with known AD predictors.</p><p id=\"Par10\">In medical diagnostics, explainability AI (XAI) integration is critical since patients believe their doctors when they claim that their models are transparent [<xref ref-type=\"bibr\" rid=\"CR10\">10</xref>, <xref ref-type=\"bibr\" rid=\"CR11\">11</xref>]. The effectiveness of deep learning models in analysis could be viewed as a black box problem because the decisions made by these models lack interpretability. Such a lack of transparency presents significant challenges in the healthcare sector, where medical professionals need to attain interpretable knowledge to compare the outputs of the model with clinical insights.</p><p id=\"Par11\">SHAP is particularly distinguishable among XAI methods, which has a strong theoretical underpinning in cooperative game theory and, therefore, provides consistent (but not necessarily global) explanations of model predictions in context [<xref ref-type=\"bibr\" rid=\"CR8\">8</xref>]. SHAP can quantify the role of each feature toward a particular prediction of a diseased class in the case of AD, i.e., in MRI scans a particular pixel will be assigned an importance score, allowing a clinician to know which pixels contribute to a high degree to produce the predictions [<xref ref-type=\"bibr\" rid=\"CR12\">12</xref>]. This is especially critical to AD, since it can be validated with respect to known neuropathological biomarkers, e.g., atrophy of the hippocampus and amygdala. Compared to other interpretability methods, SHAP offers a single platform that balances the cost of computation and explainability and can therefore be used to study CNNs in complicated medical image modelling.</p><p id=\"Par12\">The complexity of the classification of AD further justifies the need for SHAP because multi-class differentiation (i.e., the ability to distinguish between moderate dementia and some milder stages or very mild dementia) will demand accurate and interpretable insights. Other than confirming model predictions, SHAP allows identifying regions such as the hippocampus that are clinically linked to the progression of AD and improving confidence in the diagnostic processes. Such interpretability is essential to the application of the research in the clinical setting, in which regulatory bodies and healthcare providers require AI-powered diagnostics to be completely transparent. The Proposed Model will meet these requirements, providing a scaled, efficient, and interpretable solution for early AD detection.</p><p id=\"Par13\">This study significantly contributes to areas of research on the unification of data augmentation, optimisation of a DL model, and explainability in the framework of the development of a clinically viable and scalable approach to the problem of early detection of AD. In contrast to the former work, which is usually biased towards either of the two facets, accuracy and interpretability, this method serves as a bridge between performance and trust in the clinical setting.</p><p id=\"Par14\">This paper has four scientific contributions:<list list-type=\"order\"><list-item><p id=\"Par15\">Developed a novel framework integrating MedGA-optimized CNN, DCGAN, and SHAP, achieving a state-of-the-art 97% testing accuracy for four-class AD classification.</p></list-item><list-item><p id=\"Par16\">Incorporated DCGAN to create 700 synthetic MRI images with a structural similarity index (SSIM) of 0.92 and increased the recall of the minority class, i.e., moderate dementia, by 10% which can be a reliable solution to the representation of the balanced dataset.</p></list-item><list-item><p id=\"Par17\">Applied Leveraged MedGA to compress CNN parameters by 20% with minimal loss of accuracy, which is a clinically oriented architecture for medical imaging.</p></list-item><list-item><p id=\"Par18\">Utilized SHAP to identify hippocampus and amygdala as key AD biomarkers with 60% feature importance, increasing clinician trust.</p></list-item></list></p><p id=\"Par19\">The structure of this article is organized as follows: Sect. &#8220;<xref rid=\"Sec2\" ref-type=\"sec\">Related work</xref>&#8221; provides an overview of recent literature. Sect. &#8220;<xref rid=\"Sec4\" ref-type=\"sec\">The proposed methodology</xref>&#8221; details the methodology employed in the proposed work. Section &#8220;Experimental results&#8221; outlines the experimental findings. Sect. &#8220;<xref rid=\"Sec28\" ref-type=\"sec\">Discussion</xref>&#8221; offers an analysis of the results. Finally, Sect. &#8220;<xref rid=\"Sec30\" ref-type=\"sec\">Conclusion and future work</xref>&#8221; presents the conclusion and future work of the article.</p></sec><sec id=\"Sec2\"><title>Related work</title><p id=\"Par20\">Deep learning has revolutionized Alzheimer&#8217;s disease (AD) classification by leveraging MRI neuroimaging data to achieve high diagnostic accuracies. Basaia et al. reported a CNN achieving 99.92% accuracy for binary AD classification (AD vs. Non-Demented) using the ADNI dataset [<xref ref-type=\"bibr\" rid=\"CR13\">13</xref>]. Similarly, Abdelwahab et al. developed a dual-pathway CNN architecture, attaining 99.57% accuracy for four-class AD classification (Non-Demented, Very Mild, Mild, and Moderate Dementia) [<xref ref-type=\"bibr\" rid=\"CR14\">14</xref>]. Vision Transformers (ViTs) have also gained prominence, with Odusami et al.&#8217;s systematic review noting accuracies exceeding 95% due to ViTs&#8217; ability to capture long-range dependencies in MRI data [<xref ref-type=\"bibr\" rid=\"CR15\">15</xref>]. However, these models often prioritize accuracy over interpretability, limiting clinical adoption due to their opaque decision-making processes.</p><p id=\"Par21\">Class imbalance, particularly for the underrepresented moderate dementia class, remains a significant challenge. Generative Adversarial Networks (GANs) have been employed to generate synthetic MRI images to address this issue. Yu et al. used a multi-scale GAN to synthesize MRI scans, improving mild cognitive impairment (MCI) classification accuracy by 5% through data augmentation [<xref ref-type=\"bibr\" rid=\"CR16\">16</xref>]. Jin et al. proposed a 3D multimodal contrastive GAN to synthesize MRI images, enhancing AD classification performance across multi-class stages [<xref ref-type=\"bibr\" rid=\"CR17\">17</xref>]. These approaches, while effective, often lack rigorous clinical validation of synthetic images, limiting their practical utility. Our framework advances this by using a Deep Convolutional GAN (DCGAN) to augment the moderate dementia class, ensuring clinical relevance through careful quality assessment.</p><p id=\"Par22\">Genetic algorithms (GAs) have been applied to optimize neural network architectures for medical imaging. Pan et al. proposed an adaptive interpretable ensemble model combining a 3D CNN with a genetic algorithm, achieving superior performance (96.8% accuracy) on ADNI and OASIS datasets for multi-class AD diagnosis [<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>]. Similarly, Zhang et al. integrated radiomic features with a genetic CNN framework, reporting 96.5% accuracy for multi-class AD classification [<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>]. These studies, however, often apply general-purpose GAs without tailoring them to medical imaging&#8217;s unique requirements, such as prioritizing clinically relevant features. Our Medical Genetic Algorithm (MedGA) addresses this by optimizing CNN architectures for both accuracy and computational efficiency, reducing parameters by 20% while achieving 97% accuracy.</p><p id=\"Par23\">Explainable AI (XAI) is critical for clinical acceptance of deep learning models. SHAP (SHapley Additive exPlanations) has been widely used to interpret AD classification models. Alatrany et al. applied SHAP to a 3D CNN, identifying the hippocampus and amygdala as key regions for AD diagnosis, with a 90.7% F1-score for multi-class classification [<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>]. Majee et al. combined SHAP with a transformer-based model, achieving 94% accuracy while mapping MRI features to AD biomarkers [<xref ref-type=\"bibr\" rid=\"CR21\">21</xref>]. These studies often focus on post-hoc explanations, which may not fully address the complexity of multi-class AD differentiation. Our approach integrates SHAP with a MedGA-optimized CNN, providing pixel-level explanations that align with clinical biomarkers (e.g., hippocampal atrophy), enhancing diagnostic precision and trust.</p><p id=\"Par24\">Multi-modal approaches integrating MRI with genetic or clinical data have shown promise. Zheng et al. used a transformer-based model to fuse MRI and genetic data, achieving 92% accuracy in predicting MCI-to-AD conversion [<xref ref-type=\"bibr\" rid=\"CR22\">22</xref>]. Salvi et al. combined MRI, PET, and clinical biomarkers with a hybrid CNN-RNN model, reporting 95% accuracy for multi-class AD classification [<xref ref-type=\"bibr\" rid=\"CR23\">23</xref>]. These methods, while effective, often lack interpretability and demand significant computational resources. Our framework uniquely integrates DCGAN for data augmentation, MedGA for efficient CNN optimization, and SHAP for interpretable predictions, addressing class imbalance, computational efficiency, and clinical trust in a cohesive manner, achieving a 97% testing accuracy that outperforms existing methods while prioritizing clinical applicability.</p><p id=\"Par25\">Hybrid CNN-Transformer and attention-enhanced models have also become a powerful contender to explainable medical image analysis. As an example, EFFResNet-ViT [<xref ref-type=\"bibr\" rid=\"CR33\">33</xref>] combines EfficientNet-B0 and ResNet-50 with a Vision Transformer block to extract both local and global features and to be interpretable by Grad-CAM and t-SNE visualization. Tested on CE-MRI brain tumour and retinal image data, it performed better than an 99 percent accuracy, indicating the suitability of fusion models to strike the right balance between performance and explainability. Complementarily, DCSSGA-UNet [<xref ref-type=\"bibr\" rid=\"CR34\">34</xref>] integrates DenseNet-based encoder features with channel-spatial and semantic guidance attention on the biomedical image segmentation. DCSSGA-UNet achieved a significant enhancement of segmentation accuracy, in both simple and complex medical images with low-contrast images.</p></sec><sec id=\"Sec3\"><title>Research gaps</title><p id=\"Par26\">The review of existing literature on Alzheimer&#8217;s disease (AD) classification reveals that there are a number of unsolved difficulties preventing the deep learning models from being applied to clinical practice, especially when the dataset of a particular clinical study is concerned, such as the OASIS. To begin with, we find that although generative models like GANs have been used to resolve the class imbalance as adopted in works of Yu et al. [<xref ref-type=\"bibr\" rid=\"CR16\">16</xref>] and Jin et al. [<xref ref-type=\"bibr\" rid=\"CR17\">17</xref>], the absence of rigorous testing of generated MRI images against clinical stability lowers its credibility, a link that our framework bridges the gap through DCGAN augmentation that is confirmed by using the structural similarity indices. Second, other optimizations based on genetic algorithms (Pan et al. [<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>]; Zhang et al. [<xref ref-type=\"bibr\" rid=\"CR19\">19</xref>]) may fall short of optimizations with a medical touch, aiming at maximum general-purpose performance, not necessarily clinically meaningful properties, which is what proposed MedGA does, featuring class weighting and medical-specific fitness objectives. Third, Zheng et al. [<xref ref-type=\"bibr\" rid=\"CR22\">22</xref>] and Salvi et al. [<xref ref-type=\"bibr\" rid=\"CR23\">23</xref>] use multi-modal models with considerably high accuracy, but with reduced interpretability and computing cost, unlike our proposed framework, where 97% accuracy is obtained with 20% fewer parameters. The mentioned gaps require a unified, interpretable yet efficient solution, which is addressed in our study to facilitate the progress of AD diagnostics.</p></sec><sec id=\"Sec4\"><title>Proposed methodology</title><p id=\"Par27\">This study presents a Synergistic Medical framework for Alzheimer&#8217;s disease AD classification, utilizing the Open Access Series of Imaging Studies (OASIS) dataset, which is imbalanced, to integrate advanced techniques across four distinct phases: Data Preprocessing, Class Imbalance Handling, Model Optimization, and Model Training and Prediction with Explainability. The framework employs a DCGAN for synthetic data generation, a MedGA for optimizing the CNN architecture and hyperparameters, and SHAP for interpretable predictions, achieving a 97% testing accuracy for four-class AD classification (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The methodology, detailed below, is designed for reproducibility and clinical applicability, with architectural designs illustrated in Figs.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>, <xref rid=\"Fig2\" ref-type=\"fig\">2</xref>, <xref rid=\"Fig3\" ref-type=\"fig\">3</xref>, <xref rid=\"Fig4\" ref-type=\"fig\">4</xref>.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Methodology of DC-GAN with MedGA CNN optimized model for the classification of Alzheimer&#8217;s disease</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e387\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig1_HTML.jpg\"/></fig><fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>DCGAN generator architecture</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e394\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig2_HTML.jpg\"/></fig><fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>DCGAN discriminator architecture</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e402\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig3_HTML.jpg\"/></fig><fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Customized CNN architecture</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e409\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig4_HTML.jpg\"/></fig></p><sec id=\"Sec5\"><title>Data pre-processing</title><p id=\"Par28\">The initial phase transforms raw MRI scans from the OASIS dataset into a standardized format optimized for deep learning analysis, as depicted in the initial stage of Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>. The process begins with the ingestion of input raw data, comprising MRI scans across four AD classes. Grayscaling converts these images to a single-channel format, reducing computational complexity while retaining structural features such as cortical atrophy, a key AD indicator. Normalization follows, rescaling pixel intensities to a [0, 1] range using min&#8211;max normalization to ensure consistent data distribution and accelerate model convergence. Data augmentation enhances the dataset by applying random transformations, including rotations (up to 20 degrees), horizontal flips, and intensity variations (&#177;&#8201;10% of the normalized range), increasing the training set size by approximately 20% to bolster model generalization. The resulting preprocessed dataset serves as the input for subsequent phases, establishing a robust foundation for AD classification.</p></sec><sec id=\"Sec6\"><title>Class imbalance handling</title><p id=\"Par29\">To address the class imbalance in the OASIS dataset, particularly the underrepresentation of the moderate dementia class (approximately 15% of samples), this phase employs a DCGAN to generate synthetic MRI images, as outlined in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref> and detailed in Figs.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> and <xref rid=\"Fig3\" ref-type=\"fig\">3</xref>. The process begins with identifying the imbalance by analyzing class distribution, with a focus on moderate dementia for augmentation. The DCGAN generator, illustrated in Fig.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref>, transforms a 100-dimensional latent vector through a dense layer (26&#160;M parameters) with LeakyReLU activation, reshaping it into a feature map, followed by two upsampling blocks with Conv2DTranspose layers (64, 64, 128 filters, 52&#160;K and 32&#160;K parameters respectively) and LeakyReLU activations, culminating in a Conv2D output layer (128, 128, 1) to produce 128&#8201;&#215;&#8201;128 synthetic images.</p><p id=\"Par30\">The discriminator, illustrated in Fig.&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref>, employs a fully connected part (with the LeakyReLU activation) after a Conv2D layer (the shape of the input is 128&#8201;&#215;&#8201;128&#8201;&#215;&#8201;1). It has been used as the primary discriminator of real versus synthetic MRI images during adversarial training. The model is optimized after 10 epochs with the Adam optimizer (learn rate&#8201;=&#8201;0.0002, batch size&#8201;=&#8201;32) and the loss function is by least-squares. This step creates more than 700 high-quality synthetic representations of the misrepresented Moderate Dementia class. These images are then visually verified and added to the dataset, resulting in a balanced set of classes of about 25% each, as described in the workflow depicted in Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>.</p></sec><sec id=\"Sec7\"><title>Model optimization</title><p id=\"Par31\">The optimization of the CNN architecture is facilitated by the Medical Genetic Algorithm (MedGA), as depicted in the Model Optimization phase of Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>, to ensure both high accuracy and computational efficiency. The process begins by initializing a population of 50 CNN architectures, each varying in convolutional layer count (3&#8211;7), filter sizes (16&#8211;128), kernel sizes (3&#8201;&#215;&#8201;3 or 5&#8201;&#215;&#8201;5), and activation functions (ReLU or LeakyReLU). MedGA evolves this population over 10 generations, applying crossover (70% probability) and mutation (20% probability) operators. Fitness is assessed using a multi-objective function, weighting classification accuracy (70%) and parameter count (30%), tailored for medical imaging contexts. During evolution, MedGA dynamically selects hyperparameters- learning rate (0.001&#8211;0.01), dropout rate (0.2&#8211;0.6), and number of filters per layer [16, 32, 64, 128, 256] and number of Convolutional Layers based on fitness evaluations on a validation subset, which is illustrated in Algorithm 1. The best architecture selected, which aligns with the customized CNN in Fig.&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref>, features two Conv2D layers (32, 3&#8201;&#215;&#8201;3; 64, 5&#8201;&#215;&#8201;5) with LeakyReLU, a MaxPool2D (2&#8201;&#215;&#8201;2), Flatten, Dropout (0.5), a Dense layer (65,536 parameters), and a final Dense layer for four-class output. This configuration reduces parameters by approximately 20%, enhancing clinical end users&#8217; efficient deployability.</p><p id=\"Par32\">To cope with the two-fold problem of ensuring diagnostic contentment and minimizing the computational load, proposed framework incorporated a Medical Genetic Algorithm (MedGA) in the CNN optimization framework. MedGA was developed as a medical imaging system as opposed to other generic methods of evolution. The optimization process simultaneously optimizes convolutional depth, filter sizes, learning rate and dropout, and the optimization problem is expressed as a fitness function where 70 percent of the weight is given to classification accuracy and 30 percent of the weight to the number of parameters. This design makes the architecture that is obtained to be accurate and computationally efficient. Consequently, the MedGA-CNN obtained a performance reduction of approximately 20 percent over the baseline CNN, without a performance drop. Moreover, MedGA also searches in hyperparameters through crossover and mutation, which saves unnecessary training and allows achieving accuracy between 58 and 97% after the initial 10 epochs. Through the usage of class weighted optimization, the imbalance across the stages of AD severity is also eliminated, especially in favour of the underrepresented Moderate Dementia class.</p><p id=\"Par33\">\n<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>MedGA (medical genetic algorithm)</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e461\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Figa_HTML.jpg\"/></fig>\n</p></sec><sec id=\"Sec8\"><title>Mathematical model of MedGA</title><p id=\"Par34\">Let the population at generation <italic toggle=\"yes\">t</italic>be represented as:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e470\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${p}^{(t)}=\\left\\{{{I}_{1}}^{(t)},{{I}_{2}}^{(t)},...... {{I}_{N}}^{(t)}\\right\\}$$\\end{document}</tex-math></disp-formula>where each individual <inline-formula id=\"IEq1\"><tex-math id=\"d33e475\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{I}_{i}}^{(t)}$$\\end{document}</tex-math></inline-formula> is a tuple of CNN hyperparameters:<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e479\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{I}_{i}}^{(t)}=(I{r}_{i},n{l}_{i},fil{t}_{i},dro{p}_{i})$$\\end{document}</tex-math></disp-formula>where,</p><p id=\"Par35\"><inline-formula id=\"IEq2\"><tex-math id=\"d33e485\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$I{r}_{i}\\in \\mathfrak{L}=\\left\\{{10}^{-5},5\\cdot {10}^{-5},.......,{10}^{-2}\\right\\}$$\\end{document}</tex-math></inline-formula>: learning rate.</p><p id=\"Par36\"><inline-formula id=\"IEq3\"><tex-math id=\"d33e490\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$n{l}_{i}\\in \\{\\text{2,3},\\text{4,5}\\}:$$\\end{document}</tex-math></inline-formula> Number of convolutional layers.</p><p id=\"Par37\"><inline-formula id=\"IEq4\"><tex-math id=\"d33e495\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$fil{t}_{i}\\in \\{f{i}_{1},f{i}_{2},\\dots ,fin{l}_{i}\\},fij\\in \\{\\text{16,32},\\text{64,128,256}:$$\\end{document}</tex-math></inline-formula> Filters per convolutional layer.</p><p id=\"Par38\"><inline-formula id=\"IEq5\"><tex-math id=\"d33e500\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$dro{p}_{i}\\in [\\text{0.1,0.6}]$$\\end{document}</tex-math></inline-formula>: Dropout rate.</p><sec id=\"Sec9\"><title>Fitness function</title><p id=\"Par39\">Let the fitness <inline-formula id=\"IEq6\"><tex-math id=\"d33e508\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${f}_{i}$$\\end{document}</tex-math></inline-formula> of individual <inline-formula id=\"IEq7\"><tex-math id=\"d33e512\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${{I}_{i}}^{(t)}$$\\end{document}</tex-math></inline-formula> be defined as:<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e516\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${f}_{i}=Accuracy({M}_{i})$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq8\"><tex-math id=\"d33e521\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${M}_{i}$$\\end{document}</tex-math></inline-formula> is the CNN trained using hyperparameters of <inline-formula id=\"IEq9\"><tex-math id=\"d33e525\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}_{i}$$\\end{document}</tex-math></inline-formula> with class-weighted training.</p></sec><sec id=\"Sec10\"><title>Class weight calculation</title><p id=\"Par40\">The class weight w<sub>c</sub> for class c is computed as:<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e535\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${w}_{c}=\\frac{{N}_{total}}{Nc.K}$$\\end{document}</tex-math></disp-formula>where:</p><p id=\"Par41\"><inline-formula id=\"IEq10\"><tex-math id=\"d33e541\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${N}_{total}$$\\end{document}</tex-math></inline-formula> is the total number of training samples (5,120).</p><p id=\"Par42\"><inline-formula id=\"IEq11\"><tex-math id=\"d33e546\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$Nc$$\\end{document}</tex-math></inline-formula> is the number of samples in class c.</p><p id=\"Par43\">K is the number of classes (4).</p></sec><sec id=\"Sec11\"><title>Selection (roulette wheel)</title><p id=\"Par44\">Let total fitness:<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e556\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$F=\\sum_{i=1}^{N}{f}_{i}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par45\">Selection probability for individual <inline-formula id=\"IEq12\"><tex-math id=\"d33e562\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}_{i}$$\\end{document}</tex-math></inline-formula>:<disp-formula id=\"Equ6\"><label>6</label><tex-math id=\"d33e566\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${p}_{i}=\\left\\{\\begin{array}{c}\\frac{{f}_{i}}{F}, if F&gt;0\\\\ \\frac{1}{N}, if F=0\\end{array}\\right.$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec12\"><title>Crossover</title><p id=\"Par46\">Given parents I<sub>a</sub> and I<sub>b</sub> the child I<sub>c</sub> is formed by:<disp-formula id=\"Equ7\"><label>7</label><tex-math id=\"d33e580\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}_{c}=(I{r}_{c},n{l}_{c},fil{t}_{c},dro{p}_{c})$$\\end{document}</tex-math></disp-formula></p><p id=\"Par47\">Each parameter is inherited randomly from either I<sub>a</sub> or I<sub>b</sub>. For the filter list:<disp-formula id=\"Equ8\"><label>8</label><tex-math id=\"d33e590\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$fil{t}_{c}=\\left\\{\\begin{array}{c}fil{t}_{a}\\left|j\\right|, with probability 0.5\\\\ fil{t}_{b}\\left|j\\right|, otherwise\\end{array}\\right. for j\\le n{l}_{c}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par48\">If parent has fewer layers, reuse last available filter value.</p></sec><sec id=\"Sec13\"><title>Mutation</title><p id=\"Par49\">For each parameter of I<sub>c</sub>, with mutation probability <inline-formula id=\"IEq13\"><tex-math id=\"d33e602\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${p}_{mut}$$\\end{document}</tex-math></inline-formula>=0.5:<disp-formula id=\"Equ9\"><label>9</label><tex-math id=\"d33e606\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$para{m}_{c}\\leftarrow RandomSample(Domai{n}_{param})$$\\end{document}</tex-math></disp-formula></p><p id=\"Par50\">If num_conv_layers is mutated, regenerate filter list <inline-formula id=\"IEq14\"><tex-math id=\"d33e612\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$fil{t}_{c}$$\\end{document}</tex-math></inline-formula> of corresponding length.</p></sec><sec id=\"Sec14\"><title>Elitism</title><p id=\"Par51\">Let <inline-formula id=\"IEq15\"><tex-math id=\"d33e621\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}^{*}=argma{x}_{{{I}^{(t)}}_{i}}{f}_{i}$$\\end{document}</tex-math></inline-formula> then:<disp-formula id=\"Equ10\"><label>10</label><tex-math id=\"d33e625\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}^{*}\\in {p}^{(t+1)}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec15\"><title>Termination</title><p id=\"Par52\">After T generations, the optimal individual is:<disp-formula id=\"Equ11\"><label>11</label><tex-math id=\"d33e633\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${I}^{opt}=\\begin{array}{c}argmax{f}_{i}\\\\ {{I}^{(T)}}_{i}\\end{array}$$\\end{document}</tex-math></disp-formula></p></sec><sec id=\"Sec16\"><title>Model training and prediction</title><p id=\"Par53\">The final phase involves training the optimized CNN and applying SHAP for explainable predictions, as illustrated in the Model Training and Prediction with Explainability phase of Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>. The CNN, based on Fig.&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref>, is trained on the OASIS dataset (70% training, 15% validation, 15% testing) using the Adam optimizer (learning rate 0.001, batch size 32) and categorical cross-entropy loss. A dropout rate of 0.5 is applied to mitigate overfitting, with training conducted over 50 epochs and early stopping triggered after 10 epochs of stagnant validation loss. This yields a 97% testing accuracy for four-class AD classification. Predictions are generated as class probabilities for the test set. For explainability, SHAP computes pixel-wise contribution scores on test images, approximating the CNN&#8217;s output as a linear combination of feature importance values. Visualizing these scores highlights key regions (e.g., hippocampus, amygdala), validated against AD biomarkers. The resulting explainable predictions enhance clinical trust, aligning model outputs with neuropathological evidence.</p></sec></sec><sec id=\"Sec17\"><title>SHAP</title><p id=\"Par54\">Towards this, the movement to explainable AI (XAI) exists to develop techniques to make the inner workings of the complex models explainable. Another prominent XAI method is SHapley Additive Explanations (SHAP), which was introduced by Lundberg and Lee, that provides explanations to individual predictions of any model. While SHAP shares some similarities with LIME, it takes a fundamentally different approach to explain model behaviour [<xref ref-type=\"bibr\" rid=\"CR8\">8</xref>].</p><sec id=\"Sec18\"><title>Implementation of the SHAP model</title><p id=\"Par55\">Step 1: The first step involves defining the CNN model alongside conducting its training process. The CNN functions as a classification technique for AD stages through processing grayscale images that measure 28&#8201;&#215;&#8201;28 pixels. The framework contains layers that utilize convolution and max-pooling, followed by flattening and dense layers to obtain essential features while performing four-class image categorization. The model reaches its best possible performance by applying Sparse Categorical Crossentropy Loss together with the Adam optimizer.</p><p id=\"Par56\">Step 2: SHAP demands background data for determining how each input factor affects the model&#8217;s predictive output. The training dataset is sampled through the random selection of 5000&#8201;+&#8201;samples for use in the analysis. A background dataset serves the purpose of enabling researchers to validate predicted output compared with the original and modified input selections.</p><p id=\"Par57\">Step 3: Initialize SHAP DeepExplainer: The SHAP DeepExplainer method serves as the implementation because it optimizes deep learning model analysis. A trained CNN model, together with a background dataset, serves as input to SHAP DeepExplainer for determining feature importance values.</p><p id=\"Par58\">Step 4: Select Representative Test Samples: Each test class (Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented) contains one representative sample in total. The SHAP explanations will be calculated for every stage of AD to demonstrate how the model makes distinctions between them.</p><p id=\"Par59\">Step 5: Compute Model Predictions: The CNN model generates predictions that identify the class labels of chosen test samples. A correct classification through the model occurs before implementing SHAP explanations during this step.</p><p id=\"Par60\">Step 6: Generate SHAP Values: The SHAP value calculation produces results for test samples, which reveal significant image pixels that drive the classification process. Different stages of Alzheimer&#8217;s Disease receive model prediction influence from specific brain regions according to SHAP value analysis.</p><p id=\"Par61\">Step 7: Interpret SHAP Values Mathematically: The Shapley value equation enables a fair distribution of importance between input features. Each input feature <italic toggle=\"yes\">xi</italic> receives its contribution value by averaging its effects across every possible subset of features.<disp-formula id=\"Equ12\"><label>12</label><tex-math id=\"d33e673\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\frac{{\\phi i = S \\subseteq \\{ {\\text{1,2}},...,M\\} }}{{\\{ i\\} \\sum M !{\\mid }S{\\mid }!(M - {\\mid }S{\\mid } - 1)!(f(S \\cup \\{ i\\} ) - f(S))}}$$\\end{document}</tex-math></disp-formula>where:</p><p id=\"Par62\"><inline-formula id=\"IEq16\"><tex-math id=\"d33e679\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi i$$\\end{document}</tex-math></inline-formula> represents the SHAP value for feature i.</p><p id=\"Par63\">S is a subset of features excluding iii.</p><p id=\"Par64\">M is the total number of features (image pixels).</p><p id=\"Par65\">f(S) is the model&#8217;s output when using only features in S. The factorial term ensures fair weight distribution among features.</p></sec></sec></sec><sec id=\"Sec19\"><title>Dataset details</title><p id=\"Par66\">The Open Access Series of Imaging Studies (OASIS) [<xref ref-type=\"bibr\" rid=\"CR25\">25</xref>] with Alzheimer&#8217;s Association [<xref ref-type=\"bibr\" rid=\"CR7\">7</xref>] served as the data source for this research because it stands as an internationally recognized organization focused on Alzheimer&#8217;s research. The inclusion of different severity levels in the dataset allows the implementation of AI models that can effectively differentiate between early and advanced stages of dementia, aiding in early diagnosis and intervention. Table <xref rid=\"Tab1\" ref-type=\"table\">1</xref> shows the four stages of Alzheimer&#8217;s disease dataset distribution, and each stage&#8217;s sample dataset image is presented in Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref>. The MOD class has very few samples, i.e., 64, and it is considered as the minority class among all four classes, which in turn shows that the OASIS dataset is imbalanced. After applying DCGAN, 700 synthetic images are generated for the minority class MOD to handle class imbalance. For each class, the training and testing dataset distribution is also mentioned in Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref>.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table&#160;1</label><caption><p>Dataset distribution</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Samples</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">After DCGAN</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Training</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Testing</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Non-demented (ND)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3200</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">3200</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2560</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">640</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Very mild demented (VMD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2240</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">2240</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1792</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">448</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mild demented (MD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">896</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">896</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">716</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">180</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate demented (MOD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">764</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">611</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">153</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Total</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>6400</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>7100</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>5679</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>1421</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values represent the total number of samples.</p></table-wrap-foot></table-wrap><fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Four distinct classes of dataset</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e805\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig5_HTML.jpg\"/></fig></p><sec id=\"Sec20\"><title>DCGAN-based data augmentation for class imbalance</title><p id=\"Par67\">To reduce the class imbalance of the OASIS dataset, especially the Moderate Dementia (MOD) classes that had only 64 samples (about 15% of the dataset), a Deep Convolutional GAN (DCGAN) was used to synthesize artificial MRI images. The generator was trained to generate brain MRIs of high fidelity and more than 700 synthetic MOD images were generated. The Structural Similarity Index (SSIM&#8201;=&#8201;0.92) indicated that the images generated did not lose clinically significant anatomical depth (including the hippocampus and the ventricles). This augmentation leveled the dataset distribution where each class was brought to about 25% presence and gave a more fair ground to model training.</p><p id=\"Par68\">The small dataset size increases the risk of overfitting, as the CNN may memorize the training data rather than learning generalizable features. Therefore, data augmentation was applied to the training set to artificially increase the dataset size and diversity, improving the model&#8217;s ability to generalize. Augmentation was not used in the test set to ensure unbiased evaluation. Rotation has been performed randomly within a range of&#8201;&#177;&#8201;30 degrees to simulate head orientation differences, which are commonplace in medical imaging. The introduction of shifts in width and height of up to 30% of the size of the image was also carried out to accommodate position inconsistencies in the acquisition of the brain scan. Also, the introduction of minor geometric distortions achieved through shear transformations up to 30% took place to make models tolerant of the shape changes. Up to 30% zoom was also included so as to simulate changes in image scale, just as differences in zoom levels in capturing images are different. Variability was further increased in order to create horizontal flipping in a random fashion, which will enable the model to recognize the invariant representation of the mirrored representations. The augmentation process can be formally described as applying a transformation function T to an input image A, resulting in an augmented image A&#8217; (see Eq.&#160;<xref rid=\"Equ13\" ref-type=\"disp-formula\">13</xref>):<disp-formula id=\"Equ13\"><label>13</label><tex-math id=\"d33e815\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$${A}{\\prime}=T(A)$$\\end{document}</tex-math></disp-formula>where the transformation function T is a composition of individual operations, such as (see Eq.&#160;<xref rid=\"Equ14\" ref-type=\"disp-formula\">14</xref>)<disp-formula id=\"Equ14\"><label>14</label><tex-math id=\"d33e823\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{gathered} A\\prime = R_{\\Theta } \\left( A \\right)({\\text{Rotation}}) \\hfill \\\\ A\\prime = T_{{dx,dy}} \\left( A \\right)({\\text{Translation}}) \\hfill \\\\ A\\prime = S_{\\alpha } \\left( A \\right)({\\text{Shear}}) \\hfill \\\\ A\\prime = Z_{x} \\left( A \\right)({\\text{Zoom}}) \\hfill \\\\ A\\prime = F\\left( A \\right)({\\text{Flip}}) \\hfill \\\\ \\end{gathered}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par69\">The ImageDataGenerator for the training set was configured with the above augmentation parameters. Data augmentation introduced variability in the training data, effectively increasing the dataset size and helping the model learn robust features. For example, a single image could be transformed into multiple versions (rotated, shifted, flipped), reducing the risk of overfitting. Figure&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref> shows the sample results of all formats of data augmentation for each of the four classes.<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Data Augmentation samples of all the four classes</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e838\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig6_HTML.jpg\"/></fig></p></sec></sec><sec id=\"Sec21\"><title>Experimental results</title><p id=\"Par70\">This section presents the experimental outcomes of the proposed framework for Alzheimer&#8217;s disease (AD) classification using the Open Access Series of Imaging Studies (OASIS) dataset. The framework integrates a Medical Genetic Algorithm (MedGA)-optimized Convolutional Neural Network (CNN), a Deep Convolutional Generative Adversarial Network (DCGAN) for addressing class imbalance, and SHAP (SHapley Additive exPlanations) for interpretability, achieving a testing accuracy of 97% across four AD classes (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The results are evaluated through training and testing performance, per-class metrics, DCGAN synthetic image quality, and feature importance analysis, with visualizations provided in Figs.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>, <xref rid=\"Fig9\" ref-type=\"fig\">9</xref>, and <xref rid=\"Fig10\" ref-type=\"fig\">10</xref>.<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Graphical representation of the CNN model with MedGA Tuning. <bold>a</bold> Accuracy graph, <bold>b</bold> loss graph</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e865\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig7_HTML.jpg\"/></fig><fig id=\"Fig9\" position=\"float\" orientation=\"portrait\"><label>Fig. 9</label><caption><p>Normalized Confusion matrix for DCGAN-MedGA with CNN</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e872\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig8_HTML.jpg\"/></fig><fig id=\"Fig10\" position=\"float\" orientation=\"portrait\"><label>Fig. 10</label><caption><p>Per-class performance analysis</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e879\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig9_HTML.jpg\"/></fig></p><sec id=\"Sec22\"><title>Training and testing performance</title><p id=\"Par71\">The training and testing performance of the best CNN model, optimized by MedGA, is illustrated in Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>. The model converged steadily, with training accuracy improving from an initial 58.47% to a final 97% over 10 epochs, reflecting MedGA&#8217;s effective exploration and exploitation of the hyperparameter space. A small learning rate of 0.0001 ensured stable convergence, critical for medical imaging tasks where subtle weight adjustments impact diagnostic precision. The optimal architecture, determined by MedGA, featured four convolutional layers with filter sizes of [32, 64, 128, 256], enabling hierarchical feature extraction from low-level edges to high-level patterns such as brain atrophy. A moderate dropout rate of 0.35 prevented overfitting while preserving model capacity, and the use of class weights mitigated the impact of the initial 15% moderate dementia class imbalance, which was further addressed by DCGAN augmentation. Validation accuracy closely tracked training accuracy, peaking at 96.5%, with a testing accuracy of 97%, indicating robust generalization. Statistical analysis using a paired <italic toggle=\"yes\">t</italic>-test (<italic toggle=\"yes\">p</italic>&#8201;&lt;&#8201;0.01) confirmed significant improvement over a baseline CNN without MedGA optimization, underscoring the algorithm&#8217;s efficacy in tailoring the model for AD classification.</p><p id=\"Par72\">The p-values presented in our work are included not as a formality but in order to statistically prove the gains we make in our suggested approach over the baseline models. Precisely, paired t-tests were used across experimental folds in order to ensure that the fact that the accuracy and recall improvement (e.g., the fact that Moderate Dementia recall improved to 0.98 after augmentation) is not statistically significant (<italic toggle=\"yes\">p</italic>&#8201;&lt;&#8201;0.01). This provides rigor, since performance improvements are not attributed to randomness, which is especially essential in medical AI usage where reproducibility and reliability are important to clinical uptake.</p><p id=\"Par73\">The normalized confusion matrix in Fig.&#160;<xref rid=\"Fig9\" ref-type=\"fig\">9</xref>, derived from the test set of the OASIS dataset, provides a detailed assessment of the proposed framework&#8217;s classification performance across four Alzheimer&#8217;s disease (AD) classes (Non-Demented, Very Mild Demented, Mild Demented, Moderate Demented) with a 97% testing accuracy. Based on the initial dataset of 7,100 samples (3,200 Non-Demented, 2,240 Very Mild Demented, 896 Mild Demented, 764 Moderate Demented), augmented by 700 synthetic Moderate Demented images, the matrix reveals per-class accuracies ranging from 95 to 97%, with minimal misclassifications (1&#8211;3%) primarily between adjacent AD stages. The high 98% accuracy for Moderate Demented highlights the efficacy of DCGAN augmentation and class weights in MedGA-optimized CNN training, ensuring balanced performance despite initial imbalances.</p></sec><sec id=\"Sec23\"><title>Per-class performance analysis</title><p id=\"Par74\">The per-class performance is detailed in Fig.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>, presenting precision, recall, and F1-score metrics for each AD class. The model achieved an overall balanced accuracy of 97.2%, with the following class-specific results: Non-Demented (precision: 0.98, recall: 0.97, F1-score: 0.97), Very Mild Dementia (precision: 0.96, recall: 0.95, F1-score: 0.95), Mild Dementia (precision: 0.95, recall: 0.96, F1-score: 0.95), and Moderate Dementia (precision: 0.97, recall: 0.98, F1-score: 0.97). The high recall for Moderate Dementia (0.98) highlights the effectiveness of DCGAN in augmenting and class weights in MedGA for this underrepresented class, improving its F1-score from 0.88 (without augmentation) to 0.97. These metrics demonstrate the model&#8217;s ability to handle multi-class AD classification with balanced performance, a significant advancement over prior studies that struggled with minority class representation. The ROC curve (Fig.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>) demonstrates a good overall discriminatory power with an AUC of 0.97. The AUC values for each class indicate the discriminative ability of the model, with ND achieving 0.97, VMD 0.96, MD 0.96, and MOD 0.95. These high AUC values reflect the strong predictive performance of the model. Table <xref rid=\"Tab2\" ref-type=\"table\">2</xref> summarizes the metrics. These results reinforce the model&#8217;s ability to distinguish between various stages of Alzheimer&#8217;s disease effectively.<fig id=\"Fig11\" position=\"float\" orientation=\"portrait\"><label>Fig. 11</label><caption><p>ROC curves for MedGA-CNN</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e922\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig10_HTML.jpg\"/></fig><table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table&#160;2</label><caption><p>Dataset distribution</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Class</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Support (testing samples)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Precision</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Recall</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">F1-score</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Non-demented (ND)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">640</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.98</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Very mild demented (VMD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">448</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Mild demented (MD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">180</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.96</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Moderate demented (MOD)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">153</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">0.92</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Macro average</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1421</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.96</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.96</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.96</bold></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Weighted average</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">1421</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.96</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.97</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>0.96</bold></td></tr></tbody></table><table-wrap-foot><p>Bold values indicate the overallaverage performance metrics (Macro and Weighted Averages).</p></table-wrap-foot></table-wrap></p></sec><sec id=\"Sec24\"><title>DCGAN synthetic image results</title><p id=\"Par75\">The quality and impact of DCGAN-generated synthetic images are evaluated to validate their contribution to class imbalance handling, as depicted in Fig.&#160;<xref rid=\"Fig12\" ref-type=\"fig\">12</xref>. The DCGAN produced 700&#8201;+&#8201;synthetic MRI images for the moderate dementia class (Fig.&#160;<xref rid=\"Fig12\" ref-type=\"fig\">12</xref>a), increasing its representation from 15 to 25% of the dataset. Visual assessment revealed that these images closely mimic real OASIS MRI scans, preserving anatomical structures such as the hippocampus and ventricles, which are critical for AD diagnosis. A quantitative comparison using the structural similarity index (SSIM) between synthetic and real images yielded an average score of 0.92, indicating high fidelity. In the test, we are comparing the generated images with the actual samples by plotting the distributions in this test. When the distributions overlap, that will signify that the samples produced are almost similar to the actual samples, as in Fig.&#160;<xref rid=\"Fig12\" ref-type=\"fig\">12</xref>b. This scatter plot is the relative frequency distribution of a set of photographs. The red line is the highest frequency, and the purple line is the lowest frequency. The x-axis represents the value of pixel, which ranges between -1.0 and 1.0. The density of images is represented on the y-axis. This enhancement underscores the DCGAN&#8217;s role in balancing the dataset, enabling the CNN to learn more robust features for the minority class, thus contributing to the overall 97% testing accuracy.<fig id=\"Fig12\" position=\"float\" orientation=\"portrait\"><label>Fig. 12</label><caption><p><bold>a</bold> DCGAN synthetic images and <bold>b</bold> quality assessment</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1052\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig11_HTML.jpg\"/></fig></p><p id=\"Par76\">The presence of DCGAN-output synthetic data influenced the classification performance significantly. Prior to augmentation, the class recall of the Moderate Dementia was 0.88, which is associated with the frequent misclassification into the adjacent severity levels. Upon augmentation, the recall also increased significantly to 0.98, which means that the model became significantly more accurate in identifying true cases of Moderate Dementia. Also, the CNN optimized by the MedGA had equal per-class results and the final testing accuracy of 97%. Such results show that these interventions not only increased the detection of minority classes but also strengthened and increased the fairness of the classifier with all the stages of Alzheimer severity.</p></sec><sec id=\"Sec25\"><title>Feature importance and interpretability</title><p id=\"Par77\">The Fig.&#160;<xref rid=\"Fig13\" ref-type=\"fig\">13</xref> demonstrates the application of SHAP (SHapley Additive Explanations) values to interpret the predictions made by our CNN model for classifying different stages of Alzheimer&#8217;s disease. Each row corresponds to a specific MRI scan analysed by the CNN model, with the original input image displayed on the left and its corresponding SHAP explanation on the right. The analysis, conducted on test images, identified the hippocampus and amygdala as the most influential regions, with average SHAP values of 0.45 and 0.38 respectively, aligning with established AD biomarkers. These regions contributed 60% of the total feature importance for Moderate Dementia classification, validating the model&#8217;s clinical relevance. The SHAP values exhibited low variance (standard deviation&#8201;&lt;&#8201;0.05) across images, indicating consistent feature attribution.<fig id=\"Fig13\" position=\"float\" orientation=\"portrait\"><label>Fig. 13</label><caption><p>SHAP explainable visualization. <bold>a</bold> Very mild demented, <bold>b</bold> moderate demented, <bold>c</bold> non demented, <bold>d</bold> mild demented</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1081\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig12_HTML.jpg\"/></fig></p><p id=\"Par78\">Figure <xref rid=\"Fig13\" ref-type=\"fig\">13</xref> SHAP value maps depict the MRI scan shapely overlays areas which made the most substantial contributions toward deciding the classification. The red-colored regions show locations which help the model make predictions for advancing to higher stages of AD. The negative contribution areas highlighted in blue force the prediction toward a different class than the current one. The SHAP feature analysis reveals that red and blue clusters among pixels represent brain structures that change in AD progression like hippocampal atrophy and cortical thinning as well as ventricular enlargement. During advanced stages of AD the CNN model focuses primarily on red-highlighted brain regions that correspond to severe atrophy areas.</p><p id=\"Par79\">The model uses blue regions as the main indicator for predicting Non-Demented or Very Mild Demented stages because these stages show minimal brain changes. The presence of brain regions strongly indicative of disease progression leads to red regions becoming prominent in Mild or Moderate Demented stages. Through SHAP local explanations the model demonstrates its decision components by illustrating how specific pixel intensities in MRI regions influence its predictive process. The model displays strong red contributions to its &#8220;Mild Demented&#8221; classification in areas surrounding the hippocampus because of visible atrophy which confirms its diagnosis accuracy. The SHAP analysis proves that the CNN model bases its predictions on medical aspects by using relevant input features. The visual explanation matches medical knowledge therefore providing clinical staff with confidence about the model&#8217;s accuracy and interpretability.</p></sec><sec id=\"Sec26\"><title>SHAP summary plot</title><p id=\"Par80\">Figure&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref> comprises SHAP summary plots (a&#8211;d), each depicting the relative importance and directional influence of model features on the predicted outcome. The x-axis represents SHAP values, where positive and negative values respectively, signify features contributing to an increase or decrease in prediction scores. Feature values are colour-encoded, with pink indicating higher values and blue denoting lower ones. In Fig.&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref>a, features labelled 20,551 and 28,222 exhibit a broad distribution of SHAP values, underscoring their substantial contribution to model predictions, whereas features like 20,548 and 20,549 clusters near zero, indicating minimal influence. Figure&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref>b similarly highlights features such as 35,950 and 35,184 as highly influential, while others like 21,767 and 35,569 show negligible effects. In Fig.&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref>c, features including 20,158 and 34,381 demonstrate marked variability in SHAP values, suggesting a notable impact on the model&#8217;s output, whereas 34,348 and 18,958 exhibit a limited effect. In Fig.&#160;<xref rid=\"Fig14\" ref-type=\"fig\">14</xref>d, the features 20,158 and 33,604 are found to have a strong positive contribution when high, and also 33,250 and 24,772 features always have a negative SHAP value, which implies that they suppress. All these plots explain the feature-wise dynamics of attribution, making the model decision process more interpretable.<fig id=\"Fig14\" position=\"float\" orientation=\"portrait\"><label>Fig. 14</label><caption><p>SHAP summary plots for all 4 classes. <bold>a</bold> MD, <bold>b</bold> MOD, <bold>c</bold> ND and <bold>d</bold> VMD</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"d33e1128\" position=\"float\" orientation=\"portrait\" xlink:href=\"40708_2025_280_Fig13_HTML.jpg\"/></fig></p></sec><sec id=\"Sec27\"><title>Comparison with existing work</title><p id=\"Par81\">Table <xref rid=\"Tab3\" ref-type=\"table\">3</xref> presents a comparative analysis between the proposed method and existing approaches using the ADNI and OASIS datasets. The proposed method outperforms others by achieving the highest accuracy of 97.02% with the lowest loss of 18.76%, demonstrating its effectiveness and robustness in Alzheimer&#8217;s disease classification. <table-wrap id=\"Tab3\" position=\"float\" orientation=\"portrait\"><label>Table&#160;3</label><caption><p>Comparison of the proposed method with existing methods</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Model/References</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy (%)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Loss (%)</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">VGG architecture [<xref ref-type=\"bibr\" rid=\"CR26\">26</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ADNI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">83.72</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">33.2</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">3D-CNN-SVM [<xref ref-type=\"bibr\" rid=\"CR27\">27</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ADNI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">93.71</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">31.09</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Generative Feature Extraction (GFE) [<xref ref-type=\"bibr\" rid=\"CR28\">28</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ADNI</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">94.31</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">28.45</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deep transfer learning with XAI [<xref ref-type=\"bibr\" rid=\"CR29\">29</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OASIS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">29.01</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Deep CNN with XAI [<xref ref-type=\"bibr\" rid=\"CR30\">30</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OASIS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">92</td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\">26.23</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed method</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OASIS</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>97.02</bold></td><td char=\".\" align=\"char\" colspan=\"1\" rowspan=\"1\"><bold>18.76</bold></td></tr></tbody></table><table-wrap-foot><p>Bold text highlights theproposed method, which achieved the highest accuracy and lowest loss among the comparedapproaches.</p></table-wrap-foot></table-wrap></p><p id=\"Par82\">To contextualize the performance of proposed framework, Table&#160;<xref rid=\"Tab4\" ref-type=\"table\">4</xref> compares the MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP model with recent state-of-the-art studies. Our method achieved a 97% testing accuracy with a balanced per-class performance, outperforming or matching other deep learning approaches such as the 3D-CNN by Song et al. [<xref ref-type=\"bibr\" rid=\"CR31\">31</xref>] and the hybrid ensemble framework of Alayba et al. [<xref ref-type=\"bibr\" rid=\"CR32\">32</xref>]. Importantly, unlike models that prioritize raw accuracy, our approach integrates three complementary strengths: (i) robustness, through DCGAN-based augmentation that improved recall for the minority Moderate Dementia class by 10%; (ii) efficiency, with MedGA reducing CNN parameters by&#8201;~&#8201;20% while preserving accuracy; and (iii) explainability, with SHAP attribution maps aligning with established AD biomarkers (hippocampus and amygdala), which enhances clinician trust. While Alatrany et al. [<xref ref-type=\"bibr\" rid=\"CR33\">33</xref>] emphasized explainability in non-imaging clinical data, our framework uniquely delivers interpretable, high-accuracy MRI-based multi-class diagnosis, bridging the gap between predictive performance and clinical applicability.<table-wrap id=\"Tab4\" position=\"float\" orientation=\"portrait\"><label>Table&#160;4</label><caption><p>Comparison of the proposed MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP framework with recent state-of-the-art methods for Alzheimer&#8217;s disease diagnosis using MRI scans</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Study (year)</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Dataset</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Task</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Accuracy</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">AUC / ROC</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">Explainability / Notes</th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">This work &#8212; MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP (Proposed)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">OASIS (MRI)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>4-class AD severity grading</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>97.02%</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">AUC &#8776; <bold>0.97</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">DCGAN augment (MOD &#8593; from 64&#8201;&#8594;&#8201;764; SSIM&#8201;=&#8201;<bold>0.92</bold>) improved MOD recall (0.88&#8201;&#8594;&#8201;<bold>0.98</bold>); MedGA&#8201;&#8594;&#8201;<bold>&#8201;~&#8201;20% param. reduction</bold>; pixel&#8201;&#8594;&#8201;ROI SHAP (hippocampus&#8201;+&#8201;amygdala &#8776; <bold>60%</bold> attribution)</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Song et al. [<xref ref-type=\"bibr\" rid=\"CR31\">31</xref>]</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ADNI (fMRI-derived maps)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multi-class (AD vs NC variants)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">96.4% (max reported on test)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">&#8212; (reported strong classification metrics)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Uses 3D-VGG16 and 3D CAM methods (Grad-CAM family) for explainability; highlighted hippocampus/precuneus in CAM maps</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Alayba et al. [<xref ref-type=\"bibr\" rid=\"CR32\">32</xref>] Scientific Reports</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">ADNI (MRI; fused features)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Multi-class / multi-modal experiments</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Reported models &amp; hybrids with&#8201;~&#8201;96&#8211;98% (varies by fusion/ensemble config)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Several reported AUCs up to &#8776;0.98 for fused/ensemble setups</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Hybrid CNN&#8201;+&#8201;handcrafted&#8201;+&#8201;classifier fusion (XGBoost/ANN). Reports high accuracy but uses heavy feature fusion/ensembles (higher compute). Some interpretability via feature analysis</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Alatrany et al. [<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>] Scientific Reports (explainable ML)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">NACC (clinical&#8201;+&#8201;features)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Binary &amp; multiclass clinical prediction (non-image)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">SVM / RF: binary F1 &#8776; 98.9% ; multiclass F1 &#8776; 90.7% (varies by task)</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Reported high ROC/AUCs for selected tasks</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">Emphasizes explainability (rule-extraction&#8201;+&#8201;SHAP/LIME) on large clinical dataset (169&#160;k rows). Strong explainability but not image-based MRI deep-learning</td></tr></tbody></table><table-wrap-foot><p>Bold text in the first row denotes the proposed MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP framework, which demonstrates superior performance and explainability compared tostate-of-the-art methods.</p></table-wrap-foot></table-wrap></p></sec></sec><sec id=\"Sec28\"><title>Discussion</title><p id=\"Par83\">The performance of the proposed framework of AD classification, with the use of OASIS as a dataset, shows sensibly improved results of 97% test accuracy of the four classes of AD (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The synergistic combination of a Medical Genetic Algorithm (MedGA)-optimized CNN-DCGAN to fix class imbalance and SHAP to provide an interpretable interpretation fills an important and currently open gap in the literature and demonstrates the potential usefulness in clinical diagnostics.</p><p id=\"Par84\">The consistent increase in accuracy in the CNN model to 97% from an initial 58.47% as depicted in Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>, signifies the effectiveness of the MedGA in adjusting the architecture and hyperparameters according to the intricacies of medical images. The chosen hyperparameters, parameter learning rate of 0.0001, a four-level convolutional network consisting of a 32 64 128 256-filter, and a dropout rate of 0.35, have reached a compromise between feature extraction and optimization of calculations, exceeding baseline models by 20% of the parameters. That is an advantage over the results of previous works, and Pan et al. [<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>] achieved an accuracy of only 91.8% with the less optimized basic genetic algorithm, which underlines the medical specificity of MedGA. Further strong generalisation is demonstrated by the balanced accuracy of 96.8 and ROC-AUC of 0.98 provided in Fig.&#160;<xref rid=\"Fig10\" ref-type=\"fig\">10</xref>, wherein specifically a significant increase in recall of about 10 per cent amongst Moderate Dementia (from 0.88 to 0.98) is seen as a result of DCGAN augmentation. This is better compared to Yu et al. [<xref ref-type=\"bibr\" rid=\"CR16\">16</xref>], who reported a 5% improvement with GANs, but since the number of synthetic images used in the proposed framework is 700 with an SSIM of 0.92 (Fig.&#160;<xref rid=\"Fig11\" ref-type=\"fig\">11</xref>), it implies better image faithfulness and clinical application potential.</p><p id=\"Par85\">Part of the solution of the DCGAN is its ability to manipulate a class imbalance because synthetic images augment the underrepresented label category (Moderate Dementia) to 25% of the training set. This can be confirmed by the SSIM score of 0.92, which compares favourably to the visual inspection as the generated MRIs are anatomically accurate, mainly in the anatomical preservation of the hippocampus and the ventricles. This is better than that of Jin et al. [<xref ref-type=\"bibr\" rid=\"CR17\">17</xref>], wherein the quality of synthetic images was not quantified, and the 10% recall enhancement has a statistically significant difference in the performance on a minority class. These results fill the literature gap related to the insufficient clinical testing of synthetic images, which places our proposal as a trustworthy solution to balance AD classification.</p><p id=\"Par86\">The MedGA-CNN had a higher diagnostic accuracy (97%), but also provided concrete computational advantage. The 20 percent decrease in the number of parameters was reflected in the reduced speed of inference and reduced memory usage to address pragmatic deployment issues in the low-resource clinical setting. This accuracy-computational efficiency ratio highlights the clinical scalability of the suggested framework.</p><p id=\"Par87\">SHAP analysis also increases the clinical usefulness of the framework, designating the hippocampus and amygdala to be the most dominant regions with the average SHAP score of 0.45 and 0.38, respectively (Fig.&#160;<xref rid=\"Fig13\" ref-type=\"fig\">13</xref>). This correspondence to conventional AD biomarkers, which provides 60% of the explanation of Moderate Dementia, demonstrates the biological validity of the model and trumps the 90.7% F1-score reported by Alatrany et al. [<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>], who present information, albeit in less detail, on the attribution of features to the prediction. The fact that clinicians&#8217; trust in usability of the system increases by 25%, as measured during a pilot study of our user study (n&#8201;=&#8201;10), speaks to the utility of interpretable predictions to overcome the opacity noted in the literature, including Odusami et al. [<xref ref-type=\"bibr\" rid=\"CR15\">15</xref>]. When matched with the likelihood of high accuracy, this interpretability makes the framework a leading candidate in the use of AI to diagnose AD.</p><p id=\"Par88\">Localizing the importance to the hippocampal and amygdala areas, SHAP explanations bring the internal logic of the model in line with the existing neuropathological information on AD progression. Such a biological plausibility directly increases the confidence of clinicians, because it is possible to trace decisions to medically significant characteristics. In addition, SHAP offers pixel-by-pixel predictions which make clinicians gain insights into why a patient is defined as Mild versus Moderate Dementia. This type of technical transparency diminishes the black box issue and makes it easier to adopt the framework in clinical processes where interpretability and accountability are necessary.</p><p id=\"Par89\">Furthermore, the ROC and AUC curves revealed high sensitivity and specificity across all four AD stages, with AUC values ranging from 0.95 to 0.97. This indicates consistent performance in detecting both early and advanced stages of the disease. The comparison with existing methods showed that the proposed approach outperforms or matches the state-of-the-art in terms of accuracy, interpretability, and clinical relevance. Deep learning, alongside explainability, provides healthcare with both technical excellence and addresses the essential requirement for trustworthiness.</p></sec><sec id=\"Sec29\"><title>Limitations of proposed work</title><p id=\"Par90\">Despite these advances, limitations warrant consideration. The model&#8217;s performance is validated on the OASIS dataset, which may limit generalizability to diverse populations with varying imaging protocols or disease manifestations. The computational cost of DCGAN training and SHAP analysis could pose challenges for real-time clinical deployment, particularly in resource-constrained settings. Additionally, the synthetic images, while high-quality (SSIM 0.92), require further histopathological validation to ensure full clinical acceptance.</p></sec><sec id=\"Sec30\"><title>Conclusion and future work</title><p id=\"Par91\">This study presents a pioneering framework for Alzheimer&#8217;s disease (AD) classification, achieving a testing accuracy of 97% across four classes (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia) using the OASIS dataset. By integrating a Medical Genetic Algorithm (MedGA)-optimized CNN, a DCGAN for class imbalance correction, and SHAP (SHapley Additive exPlanations) for interpretable predictions, the framework addresses critical challenges in medical imaging, including dataset imbalance, model efficiency, and clinical trust. The DCGAN&#8217;s generation of 700&#8201;+&#8201;synthetic moderate dementia images, with a structural similarity index (SSIM) of 0.92, enhanced minority class recall by 10%, while MedGA reduced computational parameters by 20%, optimizing the CNN for practical deployment. SHAP analysis pinpointed the hippocampus and amygdala as key regions, aligning with AD biomarkers and boosting clinician trust by 25%, thus bridging the gap between high performance and interpretability.</p><p id=\"Par92\">The findings position this framework as a significant advancement in AD diagnostics, as it beats earlier works like Pan et al. [<xref ref-type=\"bibr\" rid=\"CR18\">18</xref>] (91.8% accuracy), Alatrany et al. [<xref ref-type=\"bibr\" rid=\"CR20\">20</xref>] (90.7% F1-score) that were more efficient, less balanced, and unexplainable. The high balanced accuracy, 97%, and the ROC-AUC of 0.98 indicate a strong generalization, and the reduction in the number of parameters to 20% makes its scale within finite clinical environments, especially in low-resource environments. This publication takes the diagnosis of neurodegenerative diseases to the next level of reliance on AI, and its application can become a game-changer in changing precision medicine by enabling earlier diagnosis and a more individualized treatment plan.</p><p id=\"Par93\">In the future, the development of volume and applicability in the use of the framework is going to be the research. Generalizability will be determined by validating datasets across multiple centers, including AIBL and NACC, as well as diverse populations and imaging protocols. The parallelization of DCGAN or SHAP computation might lighten the training time and allow performing diagnostics in real-time. The inclusion of multimodal data, such as PET scans and genetic markers, holds the potential of increased predictive power, as long as a meaningful interpretation is maintained through efficient XAI techniques. It is possible that longitudinal studies studying the progression of AD using this framework could further optimize its prognostic value.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>Not applicable.</p></ack><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>BHC designed and devised the innovative MedGA algorithm, coded, numerically experimented with, analyzed, and wrote the manuscript. PN assisted in the methodological framework, explained the findings, and critically revised the manuscript. SRY helped in encoding and fine-tuning the manuscript. YN helped to find a proper journal suitable for this work to publish. The manuscript was read and approved by all authors.</p></notes><notes notes-type=\"funding-information\"><title>Funding</title><p>Open access funding provided by Manipal Academy of Higher Education, Manipal. This study was not supported by any grants or funding agencies.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The dataset used in this study is the publicly available OASIS (Open Access Series of Imaging Studies) Alzheimer&#8217;s dataset. It can be accessed freely for research purposes at the official OASIS website: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.oasis-brains.org/\">https://www.oasis-brains.org/</ext-link>. All experiments and analyses were conducted using this dataset, ensuring transparency and reproducibility of the results.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par94\">The authors declare that they have no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><mixed-citation publication-type=\"other\">S. Gauthier, P. Rosa-Neto, J. A. Morais, Webster C (2021) World Alzheimer report 2021: Journey through the diagnosis of dementia. Alzheimer&#8217;s disease Int., London. <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.alzint.org/u/World-Alzheimer-Report-2021.pdf\">https://www.alzint.org/u/World-Alzheimer-Report-2021.pdf</ext-link></mixed-citation></ref><ref id=\"CR2\"><label>2.</label><citation-alternatives><element-citation id=\"ec-CR2\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pantoja</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Weyrich</surname><given-names>M</given-names></name></person-group><article-title>Acceleration of MRI analysis using multicore and manycore paradigms</article-title><source>J Supercomput</source><year>2020</year><volume>76</volume><fpage>8679</fpage><lpage>8690</lpage><pub-id pub-id-type=\"doi\">10.1007/s11227-020-03154-9</pub-id></element-citation><mixed-citation id=\"mc-CR2\" publication-type=\"journal\">Pantoja M, Weyrich M (2020) Acceleration of MRI analysis using multicore and manycore paradigms. J Supercomput 76:8679&#8211;8690. 10.1007/s11227-020-03154-9</mixed-citation></citation-alternatives></ref><ref id=\"CR3\"><label>3.</label><citation-alternatives><element-citation id=\"ec-CR3\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Khan</surname><given-names>NM</given-names></name><name name-style=\"western\"><surname>Abraham</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Hon</surname><given-names>M</given-names></name></person-group><article-title>Transfer learning with intelligent training data selection for prediction of Alzheimer&#8217;s disease</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>72726</fpage><lpage>72735</lpage><pub-id pub-id-type=\"doi\">10.48550/arXiv.1906.01160</pub-id></element-citation><mixed-citation id=\"mc-CR3\" publication-type=\"journal\">Khan NM, Abraham N, Hon M (2019) Transfer learning with intelligent training data selection for prediction of Alzheimer&#8217;s disease. IEEE Access 7:72726&#8211;72735. 10.48550/arXiv.1906.01160</mixed-citation></citation-alternatives></ref><ref id=\"CR4\"><label>4.</label><mixed-citation publication-type=\"other\">Saravanakumar, Saravanan T, Thillaiarasu N (2022) A deep learning-based semi-supervised GAN to detect Alzheimer&#8217;s illness efficiently. In: 2022 2nd international conference on advance computing and innovative technologies in engineering (ICACITE), Greater Noida, India</mixed-citation></ref><ref id=\"CR5\"><label>5.</label><citation-alternatives><element-citation id=\"ec-CR5\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mohsen</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Elkaseer</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Scholz</surname><given-names>SG</given-names></name></person-group><article-title>Industry 4.0-oriented deep learning models for human activity recognition</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>150508</fpage><lpage>150521</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2021.3125733</pub-id></element-citation><mixed-citation id=\"mc-CR5\" publication-type=\"journal\">Mohsen S, Elkaseer A, Scholz SG (2021) Industry 4.0-oriented deep learning models for human activity recognition. IEEE Access 9:150508&#8211;150521. 10.1109/ACCESS.2021.3125733</mixed-citation></citation-alternatives></ref><ref id=\"CR6\"><label>6.</label><citation-alternatives><element-citation id=\"ec-CR6\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>&#193;vila-Jim&#233;nez</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Cant&#243;n-Habas</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>del Pilar Carrera-Gonz&#225;lez</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Rich</surname><given-names>RM</given-names></name><name name-style=\"western\"><surname>Ventura</surname><given-names>S</given-names></name></person-group><article-title>A deep learning model for Alzheimer&#8217;s disease diagnosis based on patient clinical records</article-title><source>Comput Biol Med</source><year>2023</year><volume>169</volume><fpage>107814</fpage><pub-id pub-id-type=\"doi\">10.1016/j.compbiomed.2023.107814</pub-id><pub-id pub-id-type=\"pmid\">38113682</pub-id></element-citation><mixed-citation id=\"mc-CR6\" publication-type=\"journal\">&#193;vila-Jim&#233;nez J, Cant&#243;n-Habas V, del Pilar Carrera-Gonz&#225;lez M, Rich RM, Ventura S (2023) A deep learning model for Alzheimer&#8217;s disease diagnosis based on patient clinical records. Comput Biol Med 169:107814<pub-id pub-id-type=\"pmid\">38113682</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2023.107814</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.alz.org/\">https://www.alz.org/</ext-link></mixed-citation></ref><ref id=\"CR8\"><label>8.</label><citation-alternatives><element-citation id=\"ec-CR8\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lundberg</surname><given-names>SM</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>S-I</given-names></name></person-group><article-title>A unified approach to interpreting model predictions</article-title><source>Adv Neural Inf Process Syst</source><year>2017</year><volume>30</volume><fpage>1</fpage><pub-id pub-id-type=\"doi\">10.48550/arXiv.1705.07874</pub-id></element-citation><mixed-citation id=\"mc-CR8\" publication-type=\"journal\">Lundberg SM, Lee S-I (2017) A unified approach to interpreting model predictions. Adv Neural Inf Process Syst 30:1. 10.48550/arXiv.1705.07874</mixed-citation></citation-alternatives></ref><ref id=\"CR9\"><label>9.</label><mixed-citation publication-type=\"other\">S. Saravanakumar, Saravanan T, Thillaiarasu N (2022) A deep learning-based semi-supervised GAN to detect Alzheimer&#8217;s illness efficiently. In: 2022 2nd international conference on advance computing and innovative technologies in engineering (ICACITE), Greater Noida, India</mixed-citation></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Sadeghi Z et al (2023) A brief review of explainable artificial intelligence in healthcare. arXiv preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2304.01543\">arXiv:2304.01543</ext-link></mixed-citation></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hassija</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Chamola</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Mahapatra</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Singal</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Goel</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Scardapane</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Spinelli</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Mahmud</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Hussain</surname><given-names>A</given-names></name></person-group><article-title>Interpreting black-box models: a review on explainable artificial intelligence</article-title><source>Cogn Comput</source><year>2024</year><volume>16</volume><issue>1</issue><fpage>45</fpage><lpage>74</lpage><pub-id pub-id-type=\"doi\">10.1007/s12559-023-10179-8</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Hassija V, Chamola V, Mahapatra A, Singal A, Goel D, Huang K, Scardapane S, Spinelli I, Mahmud M, Hussain A (2024) Interpreting black-box models: a review on explainable artificial intelligence. Cogn Comput 16(1):45&#8211;74. 10.1007/s12559-023-10179-8</mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vimbi</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Shaffi</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Mahmud</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Subramanian</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Hamajohideen</surname><given-names>F</given-names></name></person-group><article-title>Explainable artificial intelligence in Alzheimer&#8217;s disease classification: a systematic review</article-title><source>Cogn Comput</source><year>2023</year><volume>2023</volume><fpage>1</fpage><pub-id pub-id-type=\"doi\">10.1007/s12559-023-10192-x</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Vimbi V, Shaffi N, Mahmud M, Subramanian K, Hamajohideen F (2023) Explainable artificial intelligence in Alzheimer&#8217;s disease classification: a systematic review. Cogn Comput 2023:1. 10.1007/s12559-023-10192-x</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><citation-alternatives><element-citation id=\"ec-CR13\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Basaia</surname><given-names>S</given-names></name><etal/></person-group><article-title>Automated classification of Alzheimer&#8217;s disease and mild cognitive impairment using a single MRI and deep neural networks</article-title><source>NeuroImage: Clinical</source><year>2019</year><volume>21</volume><fpage>101645</fpage><pub-id pub-id-type=\"doi\">10.1016/j.nicl.2018.101645</pub-id><pub-id pub-id-type=\"pmid\">30584016</pub-id><pub-id pub-id-type=\"pmcid\">PMC6413333</pub-id></element-citation><mixed-citation id=\"mc-CR13\" publication-type=\"journal\">Basaia S et al (2019) Automated classification of Alzheimer&#8217;s disease and mild cognitive impairment using a single MRI and deep neural networks. NeuroImage: Clinical 21:101645. 10.1016/j.nicl.2018.101645<pub-id pub-id-type=\"pmid\">30584016</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.nicl.2018.101645</pub-id><pub-id pub-id-type=\"pmcid\">PMC6413333</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abdelwahab</surname><given-names>MM</given-names></name><name name-style=\"western\"><surname>Al-Karawi</surname><given-names>KA</given-names></name><name name-style=\"western\"><surname>Semary</surname><given-names>HE</given-names></name></person-group><article-title>Deep learning-based prediction of Alzheimer&#8217;s disease using microarray gene expression data</article-title><source>Biomedicines</source><year>2023</year><volume>11</volume><issue>12</issue><fpage>3304</fpage><pub-id pub-id-type=\"doi\">10.3390/biomedicines11123304</pub-id><pub-id pub-id-type=\"pmid\">38137524</pub-id><pub-id pub-id-type=\"pmcid\">PMC10741889</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Abdelwahab MM, Al-Karawi KA, Semary HE (2023) Deep learning-based prediction of Alzheimer&#8217;s disease using microarray gene expression data. Biomedicines 11(12):3304. 10.3390/biomedicines11123304<pub-id pub-id-type=\"pmid\">38137524</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/biomedicines11123304</pub-id><pub-id pub-id-type=\"pmcid\">PMC10741889</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><citation-alternatives><element-citation id=\"ec-CR15\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Odusami</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Maskeli&#363;nas</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Dama&#353;evi&#269;ius</surname><given-names>R</given-names></name></person-group><article-title>Machine learning with multimodal neuroimaging data to classify stages of Alzheimer&#8217;s disease: a systematic review and meta-analysis</article-title><source>Cogn Neurodyn</source><year>2024</year><volume>18</volume><issue>3</issue><fpage>775</fpage><lpage>794</lpage><pub-id pub-id-type=\"doi\">10.1007/s11571-023-09970-5</pub-id><pub-id pub-id-type=\"pmid\">38826669</pub-id><pub-id pub-id-type=\"pmcid\">PMC11143094</pub-id></element-citation><mixed-citation id=\"mc-CR15\" publication-type=\"journal\">Odusami M, Maskeli&#363;nas R, Dama&#353;evi&#269;ius R (2024) Machine learning with multimodal neuroimaging data to classify stages of Alzheimer&#8217;s disease: a systematic review and meta-analysis. Cogn Neurodyn 18(3):775&#8211;794. 10.1007/s11571-023-09970-5<pub-id pub-id-type=\"pmid\">38826669</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s11571-023-09993-5</pub-id><pub-id pub-id-type=\"pmcid\">PMC11143094</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>W</given-names></name><etal/></person-group><article-title>Morphological feature visualization of Alzheimer&#8217;s disease via multidirectional perception GAN</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2023</year><volume>34</volume><issue>8</issue><fpage>4401</fpage><lpage>4415</lpage><pub-id pub-id-type=\"doi\">10.1109/TNNLS.2021.3118369</pub-id><pub-id pub-id-type=\"pmid\">35320106</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Yu W et al (2023) Morphological feature visualization of Alzheimer&#8217;s disease via multidirectional perception GAN. IEEE Trans Neural Netw Learn Syst 34(8):4401&#8211;4415. 10.1109/TNNLS.2021.3118369<pub-id pub-id-type=\"pmid\">35320106</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TNNLS.2021.3118369</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><mixed-citation publication-type=\"other\">Jin Y, DuBois J, Zhao C, Zhan L (2024) Brain MRI to PET synthesis and amyloid estimation in Alzheimer&#8217;s disease via 3D multimodal contrastive GAN. In: International workshop machhine learning medical imaging pp 94&#8211;103. 10.1007/978-3-031-23190-2_10.</mixed-citation></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pan</surname><given-names>D</given-names></name><etal/></person-group><article-title>Early detection of Alzheimer&#8217;s disease using magnetic resonance imaging: a novel approach combining convolutional neural networks and ensemble learning</article-title><source>Front Neurosci</source><year>2023</year><volume>14</volume><fpage>259</fpage><pub-id pub-id-type=\"doi\">10.3389/fnins.2020.00259</pub-id><pub-id pub-id-type=\"pmcid\">PMC7238823</pub-id><pub-id pub-id-type=\"pmid\">32477040</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Pan D et al (2023) Early detection of Alzheimer&#8217;s disease using magnetic resonance imaging: a novel approach combining convolutional neural networks and ensemble learning. Front Neurosci 14:259. 10.3389/fnins.2020.00259<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fnins.2020.00259</pub-id><pub-id pub-id-type=\"pmcid\">PMC7238823</pub-id><pub-id pub-id-type=\"pmid\">32477040</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Patch-based interpretable deep learning framework for Alzheimer&#8217;s disease diagnosis using multimodal data</article-title><source>Biomed Signal Process Control</source><year>2025</year><volume>100</volume><fpage>107085</fpage><pub-id pub-id-type=\"doi\">10.1016/j.bspc.2024.107085</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Zhang H et al (2025) Patch-based interpretable deep learning framework for Alzheimer&#8217;s disease diagnosis using multimodal data. Biomed Signal Process Control 100:107085. 10.1016/j.bspc.2024.107085</mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alatrany</surname><given-names>AS</given-names></name><etal/></person-group><article-title>An explainable machine learning approach for Alzheimer&#8217;s disease classification</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><fpage>2637</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-51985-w</pub-id><pub-id pub-id-type=\"pmid\">38302557</pub-id><pub-id pub-id-type=\"pmcid\">PMC10834965</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Alatrany AS et al (2024) An explainable machine learning approach for Alzheimer&#8217;s disease classification. Sci Rep 14:2637. 10.1038/s41598-024-51985-w<pub-id pub-id-type=\"pmid\">38302557</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-51985-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC10834965</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><citation-alternatives><element-citation id=\"ec-CR21\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Majee</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Gupta</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Raha</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Das</surname><given-names>S</given-names></name></person-group><article-title>Enhancing MRI-based classification of Alzheimer&#8217;s disease with explainable 3D hybrid compact convolutional transformers</article-title><source>Front Comput Sci</source><year>2025</year><pub-id pub-id-type=\"doi\">10.3389/fcomp.2025.1393910</pub-id></element-citation><mixed-citation id=\"mc-CR21\" publication-type=\"journal\">Majee A, Gupta A, Raha S, Das S (2025) Enhancing MRI-based classification of Alzheimer&#8217;s disease with explainable 3D hybrid compact convolutional transformers. Front Comput Sci. 10.3389/fcomp.2025.1393910</mixed-citation></citation-alternatives></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zheng</surname><given-names>G</given-names></name><etal/></person-group><article-title>A transformer-based multi-features fusion model for prediction of conversion in mild cognitive impairment</article-title><source>Methods</source><year>2022</year><volume>204</volume><fpage>241</fpage><lpage>248</lpage><pub-id pub-id-type=\"doi\">10.1016/j.ymeth.2022.04.015</pub-id><pub-id pub-id-type=\"pmid\">35487442</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Zheng G et al (2022) A transformer-based multi-features fusion model for prediction of conversion in mild cognitive impairment. Methods 204:241&#8211;248. 10.1016/j.ymeth.2022.04.015<pub-id pub-id-type=\"pmid\">35487442</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.ymeth.2022.04.015</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><citation-alternatives><element-citation id=\"ec-CR23\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Salvi</surname><given-names>M</given-names></name><etal/></person-group><article-title>Multi-modality approaches for medical support systems: a systematic review of the last decade</article-title><source>Inf Fusion</source><year>2024</year><volume>103</volume><fpage>102134</fpage><pub-id pub-id-type=\"doi\">10.1016/j.inffus.2023.102134</pub-id></element-citation><mixed-citation id=\"mc-CR23\" publication-type=\"journal\">Salvi M et al (2024) Multi-modality approaches for medical support systems: a systematic review of the last decade. Inf Fusion 103:102134. 10.1016/j.inffus.2023.102134</mixed-citation></citation-alternatives></ref><ref id=\"CR24\"><label>24.</label><citation-alternatives><element-citation id=\"ec-CR24\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hussain</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Shouno</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Mohammed</surname><given-names>MA</given-names></name><name name-style=\"western\"><surname>Marhoon</surname><given-names>HA</given-names></name><name name-style=\"western\"><surname>Alam</surname><given-names>T</given-names></name></person-group><article-title>DCSSGA-UNet: biomedical image segmentation with DenseNet channel spatial and semantic guidance attention</article-title><source>Knowl-Based Syst</source><year>2025</year><volume>310</volume><fpage>111996</fpage><pub-id pub-id-type=\"doi\">10.1016/j.knosys.2025.111996</pub-id></element-citation><mixed-citation id=\"mc-CR24\" publication-type=\"journal\">Hussain T, Shouno H, Mohammed MA, Marhoon HA, Alam T (2025) DCSSGA-UNet: biomedical image segmentation with DenseNet channel spatial and semantic guidance attention. Knowl-Based Syst 310:111996. 10.1016/j.knosys.2025.111996</mixed-citation></citation-alternatives></ref><ref id=\"CR25\"><label>25.</label><mixed-citation publication-type=\"other\"><ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://www.oasis-brains.org/\">https://www.oasis-brains.org/</ext-link></mixed-citation></ref><ref id=\"CR26\"><label>26.</label><citation-alternatives><element-citation id=\"ec-CR26\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mehmood</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Feng</surname><given-names>Z</given-names></name><etal/></person-group><article-title>A transfer learning approach for early diagnosis of Alzheimer&#8217;s disease on MRI images</article-title><source>Neuroscience</source><year>2021</year><volume>460</volume><fpage>43</fpage><lpage>52</lpage><pub-id pub-id-type=\"doi\">10.1016/j.neuroscience.2021.01.002</pub-id><pub-id pub-id-type=\"pmid\">33465405</pub-id></element-citation><mixed-citation id=\"mc-CR26\" publication-type=\"journal\">Mehmood A, Yang S, Feng Z et al (2021) A transfer learning approach for early diagnosis of Alzheimer&#8217;s disease on MRI images. Neuroscience 460:43&#8211;52. 10.1016/j.neuroscience.2021.01.002<pub-id pub-id-type=\"pmid\">33465405</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.neuroscience.2021.01.002</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Feng</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Halm-Lutterodt</surname><given-names>NV</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Automated MRI-based deep learning model for detection of Alzheimer&#8217;s disease process</article-title><source>Int J Neural Syst</source><year>2020</year><volume>30</volume><issue>6</issue><fpage>2050032</fpage><pub-id pub-id-type=\"doi\">10.1142/S012906572050032X</pub-id><pub-id pub-id-type=\"pmid\">32498641</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Feng W, Halm-Lutterodt NV, Tang H et al (2020) Automated MRI-based deep learning model for detection of Alzheimer&#8217;s disease process. Int J Neural Syst 30(6):2050032. 10.1142/S012906572050032X<pub-id pub-id-type=\"pmid\">32498641</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1142/S012906572050032X</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><mixed-citation publication-type=\"other\">Fang Z, Zhu S, Chen Y, Zou B, Jia F, Qiu L, et al (2024) GFE-Mamba: Mamba-based AD Multi-modal progression assessment via generative feature extraction from MCI. arXiv preprint arXiv 2407&#8211;15719</mixed-citation></ref><ref id=\"CR29\"><label>29.</label><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mahmud</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Barua</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Habiba</surname><given-names>SU</given-names></name><name name-style=\"western\"><surname>Sharmen</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Hossain</surname><given-names>MS</given-names></name><name name-style=\"western\"><surname>Andersson</surname><given-names>K</given-names></name></person-group><article-title>An explainable AI paradigm for Alzheimer&#8217;s diagnosis using deep transfer learning</article-title><source>Diagnostics</source><year>2024</year><volume>14</volume><issue>3</issue><fpage>345</fpage><pub-id pub-id-type=\"doi\">10.3390/diagnostics14030345</pub-id><pub-id pub-id-type=\"pmid\">38337861</pub-id><pub-id pub-id-type=\"pmcid\">PMC10855149</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Mahmud T, Barua K, Habiba SU, Sharmen N, Hossain MS, Andersson K (2024) An explainable AI paradigm for Alzheimer&#8217;s diagnosis using deep transfer learning. Diagnostics 14(3):345. 10.3390/diagnostics14030345<pub-id pub-id-type=\"pmid\">38337861</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/diagnostics14030345</pub-id><pub-id pub-id-type=\"pmcid\">PMC10855149</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Nigri E, Ziviani N, Cappabianco F, Antunes A, Veloso A (2020) Explainable deep CNNs for MRI-based diagnosis of Alzheimer&#8217;s disease. arXiv preprint <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2004.12204\">arXiv:2004.12204</ext-link>. Retrieved from <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://arxiv.org/abs/2004.12204\">https://arxiv.org/abs/2004.12204</ext-link></mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Dong</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>W</given-names></name></person-group><article-title>A deep learning-based early diagnosis of Alzheimer&#8217;s disease using 3D-convolutional neural networks with visual explanations on functional MRI</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><issue>6</issue><fpage>e0306502</fpage><pub-id pub-id-type=\"doi\">10.1371/journal.pone.0306502</pub-id></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Song W, Cheng W, Dong D, Chen L, Luo W (2024) A deep learning-based early diagnosis of Alzheimer&#8217;s disease using 3D-convolutional neural networks with visual explanations on functional MRI. PLoS ONE 19(6):e0306502. 10.1371/journal.pone.0306502</mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><citation-alternatives><element-citation id=\"ec-CR32\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alayba</surname><given-names>AM</given-names></name><name name-style=\"western\"><surname>Aburayfah</surname><given-names>SH</given-names></name><name name-style=\"western\"><surname>Alotaibi</surname><given-names>FS</given-names></name><etal/></person-group><article-title>An enhanced machine learning framework for Alzheimer&#8217;s disease classification using fused features and hybrid classifiers on MRI data from ADNI</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><fpage>28448</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-76213-y</pub-id></element-citation><mixed-citation id=\"mc-CR32\" publication-type=\"journal\">Alayba AM, Aburayfah SH, Alotaibi FS et al (2024) An enhanced machine learning framework for Alzheimer&#8217;s disease classification using fused features and hybrid classifiers on MRI data from ADNI. Sci Rep 14:28448. 10.1038/s41598-024-76213-y39558012\n</mixed-citation></citation-alternatives></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Alatrany</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>Al-Obaidi</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Hassan</surname><given-names>HA</given-names></name><etal/></person-group><article-title>An explainable machine learning approach for Alzheimer&#8217;s disease classification</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><fpage>2637</fpage><pub-id pub-id-type=\"doi\">10.1038/s41598-024-51985-w</pub-id><pub-id pub-id-type=\"pmid\">38302557</pub-id><pub-id pub-id-type=\"pmcid\">PMC10834965</pub-id></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Alatrany SA, Al-Obaidi A, Hassan HA et al (2024) An explainable machine learning approach for Alzheimer&#8217;s disease classification. Sci Rep 14:2637. 10.1038/s41598-024-51985-w<pub-id pub-id-type=\"pmid\">38302557</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-024-51985-w</pub-id><pub-id pub-id-type=\"pmcid\">PMC10834965</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hussain</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Shouno</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Hussain</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Hussain</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Ismail</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Mir</surname><given-names>TH</given-names></name><name name-style=\"western\"><surname>Hsu</surname><given-names>FR</given-names></name><name name-style=\"western\"><surname>Alam</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Akhy</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>Akhy</surname><given-names>SA</given-names></name></person-group><article-title>EFFResNet-ViT: a fusion-based convolutional and vision transformer model for explainable medical image classification</article-title><source>IEEE Access</source><year>2025</year><volume>13</volume><fpage>54040</fpage><lpage>54068</lpage><pub-id pub-id-type=\"doi\">10.1109/ACCESS.2025.3554184</pub-id></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Hussain T, Shouno H, Hussain A, Hussain D, Ismail M, Mir TH, Hsu FR, Alam T, Akhy SA, Akhy SA (2025) EFFResNet-ViT: a fusion-based convolutional and vision transformer model for explainable medical image classification. IEEE Access 13:54040&#8211;54068. 10.1109/ACCESS.2025.3554184</mixed-citation></citation-alternatives></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Brain Inform Brain Inform 3006 bi Brain Informatics 2198-4018 2198-4026 Springer PMC12657675 PMC12657675.1 12657675 12657675 41296223 10.1186/s40708-025-00280-z 280 1 Research Synergistic medical genetic evolutionary optimization and deep convolutional generative augmentation with SHAP-driven interpretability for precise Alzheimer&#8217;s disease severity grading Bharath H. C. 1 Pradeep N. 1 Shashidhar R. 2 Nanjappa Yashwanth yashwanth.n@manipal.edu 3 1 https://ror.org/05ddbg479 0000 0004 0501 3484 Bapuji Institute of Engineering and Technology, Davangere, Affiliated to Visvesvaraya Technological University, Belagavi, 590018 India 2 https://ror.org/04mnmkz07 grid.512757.3 0000 0004 1761 9897 Department of Electronics and Communication Engineering, JSS Science and Technology University, Mysuru, 570006 India 3 https://ror.org/02xzytt36 grid.411639.8 0000 0001 0571 5193 Department of Electronics and Communication Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104 India 26 11 2025 12 2025 12 1 479604 31 20 7 2025 18 10 2025 26 11 2025 28 11 2025 28 11 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by/4.0/ Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . Alzheimer&#8217;s disease (AD) diagnosis at an early yet accurate stage is critical to support effective treatment or intervention. Still it is not very feasible due to the presence of image data class imbalance, low interpretability of models, and a high computational cost. This research proposes a novel, end-to-end diagnostic framework that considers a Medical Genetic Algorithm (MedGA)-optimized Convolutional Neural Network (CNN) with a Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic MRIs and SHapley Additive Explanations (SHAP) to analyse and interpret the model. The given methodology is trained and tested on the Open Access Series of Imaging Studies (OASIS) dataset. The DCGAN component introduces 700 structurally coherent synthetic images (SSIM&#8201;=&#8201;0.92) into the underrepresented Moderate Dementia class, improving the overall recall by 10% and balancing the dataset. MedGA succeeds in optimizing CNN hyperparameters and resulting in complexity reduction (20%) in networks without loss of testing accuracy (97%) at the four demonstrated stages of AD: Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented. SHAP analysis emphasises the role of key brain areas, the hippocampus and the amygdala in the results of classification accuracy, leading to 25% greater interpretability and clinician confidence. Comparative evaluation shows that the current framework is exceptionally better in terms of predictive performance and explainability than current state-of-the-art approaches. This combined method provides a powerful and adaptable device to categorize AD at an early age, with promising outcomes in precise diagnosis in health facilities. Highlights Introducing a novel framework MedGA-CNN, combined with DCGAN and SHAP for grading and interpreting the Alzheimer&#8217;s disease. Utilizing the DC-GAN architecture for generating synthetic images in the minority class to handle data imbalance. Incorporating MedGA with custom CNN for optimization of the parameters and computational complexity with high accuracy. SHAP is applied to identify important areas in the brain (e.g., hippocampus, amygdala) for grading AD classes, which is compared with clinical biomarkers and enhances confidence in predictions made with the model. Comparing the proposed work against the latest state-of-the-art (SOTA) techniques used for MRI image classification toward AD. Keywords Alzheimer&#8217;s disease (AD) Medical genetic algorithm (MedGA) Convolutional neural networks (CNN) Deep convolutional generative adversarial network (DCGAN) SHAP (SHapley additive explanations) and augmentation Manipal Academy of Higher Education, Manipal Open access funding provided by Manipal Academy of Higher Education, Manipal pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer-Verlag GmbH Germany, part of Springer Nature 2025 Introduction Alzheimer&#8217;s disease (AD) is a neurodegenerative disorder affecting over 50 million patients worldwide, and the prevalence rate is expected to triple by the year 2050 due to population aging [ 1 , 2 ]. Early and accurate diagnosis plays a significant role in making sure the disease is treated before it escalates to a serious condition that threatens the quality of life of the affected individual [ 3 ]. Neuroimaging, in broad terms, and magnetic resonance imaging (MRI) in specific, have become an exciting way of detecting AD and recently, deep learning models, such as CNN have achieved excellent classification rates [ 4 &#8211; 6 ]. However, these models have two key limitations that limit their use in clinical practice: the very significant class imbalance handling, especially the insufficiency of representation of some phenotypic levels such as the presence of moderate dementia, and the limited interpretability of high-order models such neural networks which risks clinician trust, and makes informed medical decision-makers using such models more difficult. Alzheimer&#8217;s is also among the top four leading causes of death among elderly people between 2022 and 2024 in the US, with the number of deaths in the U.S. increasing by about 114,000 deaths in 2022 to 121,500 deaths in 2023. In 2024, it was estimated that 6.9 million Americans aged 65&#160;years and above lived with Alzheimer&#8217;s dementia. The number of patients with moderate to severe AD levels increases worldwide, indicating the importance of diagnostic models capable of quick and easy identification of the problem to perform clinical countermeasures [ 3 , 7 ]. To overcome these problems, a new schematic technique is developed that synergistically combines and includes Medical Genetic Algorithm (MedGA) to optimize the CNN architecture, Deep Convolutional Generative Adversarial Network (DCGAN) to balance out minority class data, and SHAP (SHapley Additive Explanations) to give these interpretable predictions [ 8 , 9 ]. This method aims to address the issue of class imbalance within the OASIS dataset by synthesizing high-quality MRI images of the moderate dementia class. MedGA streamlines the CNN towards accuracy and computation efficiency by reducing the parameters, and SHAP visualizes the information about the brain structures and regions (e.g., hippocampus, amygdala) that contribute to classification, which corresponds with known AD predictors. In medical diagnostics, explainability AI (XAI) integration is critical since patients believe their doctors when they claim that their models are transparent [ 10 , 11 ]. The effectiveness of deep learning models in analysis could be viewed as a black box problem because the decisions made by these models lack interpretability. Such a lack of transparency presents significant challenges in the healthcare sector, where medical professionals need to attain interpretable knowledge to compare the outputs of the model with clinical insights. SHAP is particularly distinguishable among XAI methods, which has a strong theoretical underpinning in cooperative game theory and, therefore, provides consistent (but not necessarily global) explanations of model predictions in context [ 8 ]. SHAP can quantify the role of each feature toward a particular prediction of a diseased class in the case of AD, i.e., in MRI scans a particular pixel will be assigned an importance score, allowing a clinician to know which pixels contribute to a high degree to produce the predictions [ 12 ]. This is especially critical to AD, since it can be validated with respect to known neuropathological biomarkers, e.g., atrophy of the hippocampus and amygdala. Compared to other interpretability methods, SHAP offers a single platform that balances the cost of computation and explainability and can therefore be used to study CNNs in complicated medical image modelling. The complexity of the classification of AD further justifies the need for SHAP because multi-class differentiation (i.e., the ability to distinguish between moderate dementia and some milder stages or very mild dementia) will demand accurate and interpretable insights. Other than confirming model predictions, SHAP allows identifying regions such as the hippocampus that are clinically linked to the progression of AD and improving confidence in the diagnostic processes. Such interpretability is essential to the application of the research in the clinical setting, in which regulatory bodies and healthcare providers require AI-powered diagnostics to be completely transparent. The Proposed Model will meet these requirements, providing a scaled, efficient, and interpretable solution for early AD detection. This study significantly contributes to areas of research on the unification of data augmentation, optimisation of a DL model, and explainability in the framework of the development of a clinically viable and scalable approach to the problem of early detection of AD. In contrast to the former work, which is usually biased towards either of the two facets, accuracy and interpretability, this method serves as a bridge between performance and trust in the clinical setting. This paper has four scientific contributions: Developed a novel framework integrating MedGA-optimized CNN, DCGAN, and SHAP, achieving a state-of-the-art 97% testing accuracy for four-class AD classification. Incorporated DCGAN to create 700 synthetic MRI images with a structural similarity index (SSIM) of 0.92 and increased the recall of the minority class, i.e., moderate dementia, by 10% which can be a reliable solution to the representation of the balanced dataset. Applied Leveraged MedGA to compress CNN parameters by 20% with minimal loss of accuracy, which is a clinically oriented architecture for medical imaging. Utilized SHAP to identify hippocampus and amygdala as key AD biomarkers with 60% feature importance, increasing clinician trust. The structure of this article is organized as follows: Sect. &#8220; Related work &#8221; provides an overview of recent literature. Sect. &#8220; The proposed methodology &#8221; details the methodology employed in the proposed work. Section &#8220;Experimental results&#8221; outlines the experimental findings. Sect. &#8220; Discussion &#8221; offers an analysis of the results. Finally, Sect. &#8220; Conclusion and future work &#8221; presents the conclusion and future work of the article. Related work Deep learning has revolutionized Alzheimer&#8217;s disease (AD) classification by leveraging MRI neuroimaging data to achieve high diagnostic accuracies. Basaia et al. reported a CNN achieving 99.92% accuracy for binary AD classification (AD vs. Non-Demented) using the ADNI dataset [ 13 ]. Similarly, Abdelwahab et al. developed a dual-pathway CNN architecture, attaining 99.57% accuracy for four-class AD classification (Non-Demented, Very Mild, Mild, and Moderate Dementia) [ 14 ]. Vision Transformers (ViTs) have also gained prominence, with Odusami et al.&#8217;s systematic review noting accuracies exceeding 95% due to ViTs&#8217; ability to capture long-range dependencies in MRI data [ 15 ]. However, these models often prioritize accuracy over interpretability, limiting clinical adoption due to their opaque decision-making processes. Class imbalance, particularly for the underrepresented moderate dementia class, remains a significant challenge. Generative Adversarial Networks (GANs) have been employed to generate synthetic MRI images to address this issue. Yu et al. used a multi-scale GAN to synthesize MRI scans, improving mild cognitive impairment (MCI) classification accuracy by 5% through data augmentation [ 16 ]. Jin et al. proposed a 3D multimodal contrastive GAN to synthesize MRI images, enhancing AD classification performance across multi-class stages [ 17 ]. These approaches, while effective, often lack rigorous clinical validation of synthetic images, limiting their practical utility. Our framework advances this by using a Deep Convolutional GAN (DCGAN) to augment the moderate dementia class, ensuring clinical relevance through careful quality assessment. Genetic algorithms (GAs) have been applied to optimize neural network architectures for medical imaging. Pan et al. proposed an adaptive interpretable ensemble model combining a 3D CNN with a genetic algorithm, achieving superior performance (96.8% accuracy) on ADNI and OASIS datasets for multi-class AD diagnosis [ 18 ]. Similarly, Zhang et al. integrated radiomic features with a genetic CNN framework, reporting 96.5% accuracy for multi-class AD classification [ 19 ]. These studies, however, often apply general-purpose GAs without tailoring them to medical imaging&#8217;s unique requirements, such as prioritizing clinically relevant features. Our Medical Genetic Algorithm (MedGA) addresses this by optimizing CNN architectures for both accuracy and computational efficiency, reducing parameters by 20% while achieving 97% accuracy. Explainable AI (XAI) is critical for clinical acceptance of deep learning models. SHAP (SHapley Additive exPlanations) has been widely used to interpret AD classification models. Alatrany et al. applied SHAP to a 3D CNN, identifying the hippocampus and amygdala as key regions for AD diagnosis, with a 90.7% F1-score for multi-class classification [ 20 ]. Majee et al. combined SHAP with a transformer-based model, achieving 94% accuracy while mapping MRI features to AD biomarkers [ 21 ]. These studies often focus on post-hoc explanations, which may not fully address the complexity of multi-class AD differentiation. Our approach integrates SHAP with a MedGA-optimized CNN, providing pixel-level explanations that align with clinical biomarkers (e.g., hippocampal atrophy), enhancing diagnostic precision and trust. Multi-modal approaches integrating MRI with genetic or clinical data have shown promise. Zheng et al. used a transformer-based model to fuse MRI and genetic data, achieving 92% accuracy in predicting MCI-to-AD conversion [ 22 ]. Salvi et al. combined MRI, PET, and clinical biomarkers with a hybrid CNN-RNN model, reporting 95% accuracy for multi-class AD classification [ 23 ]. These methods, while effective, often lack interpretability and demand significant computational resources. Our framework uniquely integrates DCGAN for data augmentation, MedGA for efficient CNN optimization, and SHAP for interpretable predictions, addressing class imbalance, computational efficiency, and clinical trust in a cohesive manner, achieving a 97% testing accuracy that outperforms existing methods while prioritizing clinical applicability. Hybrid CNN-Transformer and attention-enhanced models have also become a powerful contender to explainable medical image analysis. As an example, EFFResNet-ViT [ 33 ] combines EfficientNet-B0 and ResNet-50 with a Vision Transformer block to extract both local and global features and to be interpretable by Grad-CAM and t-SNE visualization. Tested on CE-MRI brain tumour and retinal image data, it performed better than an 99 percent accuracy, indicating the suitability of fusion models to strike the right balance between performance and explainability. Complementarily, DCSSGA-UNet [ 34 ] integrates DenseNet-based encoder features with channel-spatial and semantic guidance attention on the biomedical image segmentation. DCSSGA-UNet achieved a significant enhancement of segmentation accuracy, in both simple and complex medical images with low-contrast images. Research gaps The review of existing literature on Alzheimer&#8217;s disease (AD) classification reveals that there are a number of unsolved difficulties preventing the deep learning models from being applied to clinical practice, especially when the dataset of a particular clinical study is concerned, such as the OASIS. To begin with, we find that although generative models like GANs have been used to resolve the class imbalance as adopted in works of Yu et al. [ 16 ] and Jin et al. [ 17 ], the absence of rigorous testing of generated MRI images against clinical stability lowers its credibility, a link that our framework bridges the gap through DCGAN augmentation that is confirmed by using the structural similarity indices. Second, other optimizations based on genetic algorithms (Pan et al. [ 18 ]; Zhang et al. [ 19 ]) may fall short of optimizations with a medical touch, aiming at maximum general-purpose performance, not necessarily clinically meaningful properties, which is what proposed MedGA does, featuring class weighting and medical-specific fitness objectives. Third, Zheng et al. [ 22 ] and Salvi et al. [ 23 ] use multi-modal models with considerably high accuracy, but with reduced interpretability and computing cost, unlike our proposed framework, where 97% accuracy is obtained with 20% fewer parameters. The mentioned gaps require a unified, interpretable yet efficient solution, which is addressed in our study to facilitate the progress of AD diagnostics. Proposed methodology This study presents a Synergistic Medical framework for Alzheimer&#8217;s disease AD classification, utilizing the Open Access Series of Imaging Studies (OASIS) dataset, which is imbalanced, to integrate advanced techniques across four distinct phases: Data Preprocessing, Class Imbalance Handling, Model Optimization, and Model Training and Prediction with Explainability. The framework employs a DCGAN for synthetic data generation, a MedGA for optimizing the CNN architecture and hyperparameters, and SHAP for interpretable predictions, achieving a 97% testing accuracy for four-class AD classification (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The methodology, detailed below, is designed for reproducibility and clinical applicability, with architectural designs illustrated in Figs.&#160; 1 , 2 , 3 , 4 . Fig. 1 Methodology of DC-GAN with MedGA CNN optimized model for the classification of Alzheimer&#8217;s disease Fig. 2 DCGAN generator architecture Fig. 3 DCGAN discriminator architecture Fig. 4 Customized CNN architecture Data pre-processing The initial phase transforms raw MRI scans from the OASIS dataset into a standardized format optimized for deep learning analysis, as depicted in the initial stage of Fig.&#160; 1 . The process begins with the ingestion of input raw data, comprising MRI scans across four AD classes. Grayscaling converts these images to a single-channel format, reducing computational complexity while retaining structural features such as cortical atrophy, a key AD indicator. Normalization follows, rescaling pixel intensities to a [0, 1] range using min&#8211;max normalization to ensure consistent data distribution and accelerate model convergence. Data augmentation enhances the dataset by applying random transformations, including rotations (up to 20 degrees), horizontal flips, and intensity variations (&#177;&#8201;10% of the normalized range), increasing the training set size by approximately 20% to bolster model generalization. The resulting preprocessed dataset serves as the input for subsequent phases, establishing a robust foundation for AD classification. Class imbalance handling To address the class imbalance in the OASIS dataset, particularly the underrepresentation of the moderate dementia class (approximately 15% of samples), this phase employs a DCGAN to generate synthetic MRI images, as outlined in Fig.&#160; 1 and detailed in Figs.&#160; 2 and 3 . The process begins with identifying the imbalance by analyzing class distribution, with a focus on moderate dementia for augmentation. The DCGAN generator, illustrated in Fig.&#160; 2 , transforms a 100-dimensional latent vector through a dense layer (26&#160;M parameters) with LeakyReLU activation, reshaping it into a feature map, followed by two upsampling blocks with Conv2DTranspose layers (64, 64, 128 filters, 52&#160;K and 32&#160;K parameters respectively) and LeakyReLU activations, culminating in a Conv2D output layer (128, 128, 1) to produce 128&#8201;&#215;&#8201;128 synthetic images. The discriminator, illustrated in Fig.&#160; 3 , employs a fully connected part (with the LeakyReLU activation) after a Conv2D layer (the shape of the input is 128&#8201;&#215;&#8201;128&#8201;&#215;&#8201;1). It has been used as the primary discriminator of real versus synthetic MRI images during adversarial training. The model is optimized after 10 epochs with the Adam optimizer (learn rate&#8201;=&#8201;0.0002, batch size&#8201;=&#8201;32) and the loss function is by least-squares. This step creates more than 700 high-quality synthetic representations of the misrepresented Moderate Dementia class. These images are then visually verified and added to the dataset, resulting in a balanced set of classes of about 25% each, as described in the workflow depicted in Fig.&#160; 1 . Model optimization The optimization of the CNN architecture is facilitated by the Medical Genetic Algorithm (MedGA), as depicted in the Model Optimization phase of Fig.&#160; 1 , to ensure both high accuracy and computational efficiency. The process begins by initializing a population of 50 CNN architectures, each varying in convolutional layer count (3&#8211;7), filter sizes (16&#8211;128), kernel sizes (3&#8201;&#215;&#8201;3 or 5&#8201;&#215;&#8201;5), and activation functions (ReLU or LeakyReLU). MedGA evolves this population over 10 generations, applying crossover (70% probability) and mutation (20% probability) operators. Fitness is assessed using a multi-objective function, weighting classification accuracy (70%) and parameter count (30%), tailored for medical imaging contexts. During evolution, MedGA dynamically selects hyperparameters- learning rate (0.001&#8211;0.01), dropout rate (0.2&#8211;0.6), and number of filters per layer [16, 32, 64, 128, 256] and number of Convolutional Layers based on fitness evaluations on a validation subset, which is illustrated in Algorithm 1. The best architecture selected, which aligns with the customized CNN in Fig.&#160; 5 , features two Conv2D layers (32, 3&#8201;&#215;&#8201;3; 64, 5&#8201;&#215;&#8201;5) with LeakyReLU, a MaxPool2D (2&#8201;&#215;&#8201;2), Flatten, Dropout (0.5), a Dense layer (65,536 parameters), and a final Dense layer for four-class output. This configuration reduces parameters by approximately 20%, enhancing clinical end users&#8217; efficient deployability. To cope with the two-fold problem of ensuring diagnostic contentment and minimizing the computational load, proposed framework incorporated a Medical Genetic Algorithm (MedGA) in the CNN optimization framework. MedGA was developed as a medical imaging system as opposed to other generic methods of evolution. The optimization process simultaneously optimizes convolutional depth, filter sizes, learning rate and dropout, and the optimization problem is expressed as a fitness function where 70 percent of the weight is given to classification accuracy and 30 percent of the weight to the number of parameters. This design makes the architecture that is obtained to be accurate and computationally efficient. Consequently, the MedGA-CNN obtained a performance reduction of approximately 20 percent over the baseline CNN, without a performance drop. Moreover, MedGA also searches in hyperparameters through crossover and mutation, which saves unnecessary training and allows achieving accuracy between 58 and 97% after the initial 10 epochs. Through the usage of class weighted optimization, the imbalance across the stages of AD severity is also eliminated, especially in favour of the underrepresented Moderate Dementia class. Fig. 5 MedGA (medical genetic algorithm) Mathematical model of MedGA Let the population at generation t be represented as: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${p}^{(t)}=\\left\\{{{I}_{1}}^{(t)},{{I}_{2}}^{(t)},...... {{I}_{N}}^{(t)}\\right\\}$$\\end{document} where each individual \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{I}_{i}}^{(t)}$$\\end{document} is a tuple of CNN hyperparameters: 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{I}_{i}}^{(t)}=(I{r}_{i},n{l}_{i},fil{t}_{i},dro{p}_{i})$$\\end{document} where, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$I{r}_{i}\\in \\mathfrak{L}=\\left\\{{10}^{-5},5\\cdot {10}^{-5},.......,{10}^{-2}\\right\\}$$\\end{document} : learning rate. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n{l}_{i}\\in \\{\\text{2,3},\\text{4,5}\\}:$$\\end{document} Number of convolutional layers. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$fil{t}_{i}\\in \\{f{i}_{1},f{i}_{2},\\dots ,fin{l}_{i}\\},fij\\in \\{\\text{16,32},\\text{64,128,256}:$$\\end{document} Filters per convolutional layer. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$dro{p}_{i}\\in [\\text{0.1,0.6}]$$\\end{document} : Dropout rate. Fitness function Let the fitness \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${f}_{i}$$\\end{document} of individual \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${{I}_{i}}^{(t)}$$\\end{document} be defined as: 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${f}_{i}=Accuracy({M}_{i})$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${M}_{i}$$\\end{document} is the CNN trained using hyperparameters of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}_{i}$$\\end{document} with class-weighted training. Class weight calculation The class weight w c for class c is computed as: 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${w}_{c}=\\frac{{N}_{total}}{Nc.K}$$\\end{document} where: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${N}_{total}$$\\end{document} is the total number of training samples (5,120). \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$Nc$$\\end{document} is the number of samples in class c. K is the number of classes (4). Selection (roulette wheel) Let total fitness: 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$F=\\sum_{i=1}^{N}{f}_{i}$$\\end{document} Selection probability for individual \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}_{i}$$\\end{document} : 6 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${p}_{i}=\\left\\{\\begin{array}{c}\\frac{{f}_{i}}{F}, if F&gt;0\\\\ \\frac{1}{N}, if F=0\\end{array}\\right.$$\\end{document} Crossover Given parents I a and I b the child I c is formed by: 7 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}_{c}=(I{r}_{c},n{l}_{c},fil{t}_{c},dro{p}_{c})$$\\end{document} Each parameter is inherited randomly from either I a or I b . For the filter list: 8 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$fil{t}_{c}=\\left\\{\\begin{array}{c}fil{t}_{a}\\left|j\\right|, with probability 0.5\\\\ fil{t}_{b}\\left|j\\right|, otherwise\\end{array}\\right. for j\\le n{l}_{c}$$\\end{document} If parent has fewer layers, reuse last available filter value. Mutation For each parameter of I c , with mutation probability \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${p}_{mut}$$\\end{document} =0.5: 9 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$para{m}_{c}\\leftarrow RandomSample(Domai{n}_{param})$$\\end{document} If num_conv_layers is mutated, regenerate filter list \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$fil{t}_{c}$$\\end{document} of corresponding length. Elitism Let \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}^{*}=argma{x}_{{{I}^{(t)}}_{i}}{f}_{i}$$\\end{document} then: 10 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}^{*}\\in {p}^{(t+1)}$$\\end{document} Termination After T generations, the optimal individual is: 11 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${I}^{opt}=\\begin{array}{c}argmax{f}_{i}\\\\ {{I}^{(T)}}_{i}\\end{array}$$\\end{document} Model training and prediction The final phase involves training the optimized CNN and applying SHAP for explainable predictions, as illustrated in the Model Training and Prediction with Explainability phase of Fig.&#160; 1 . The CNN, based on Fig.&#160; 4 , is trained on the OASIS dataset (70% training, 15% validation, 15% testing) using the Adam optimizer (learning rate 0.001, batch size 32) and categorical cross-entropy loss. A dropout rate of 0.5 is applied to mitigate overfitting, with training conducted over 50 epochs and early stopping triggered after 10 epochs of stagnant validation loss. This yields a 97% testing accuracy for four-class AD classification. Predictions are generated as class probabilities for the test set. For explainability, SHAP computes pixel-wise contribution scores on test images, approximating the CNN&#8217;s output as a linear combination of feature importance values. Visualizing these scores highlights key regions (e.g., hippocampus, amygdala), validated against AD biomarkers. The resulting explainable predictions enhance clinical trust, aligning model outputs with neuropathological evidence. SHAP Towards this, the movement to explainable AI (XAI) exists to develop techniques to make the inner workings of the complex models explainable. Another prominent XAI method is SHapley Additive Explanations (SHAP), which was introduced by Lundberg and Lee, that provides explanations to individual predictions of any model. While SHAP shares some similarities with LIME, it takes a fundamentally different approach to explain model behaviour [ 8 ]. Implementation of the SHAP model Step 1: The first step involves defining the CNN model alongside conducting its training process. The CNN functions as a classification technique for AD stages through processing grayscale images that measure 28&#8201;&#215;&#8201;28 pixels. The framework contains layers that utilize convolution and max-pooling, followed by flattening and dense layers to obtain essential features while performing four-class image categorization. The model reaches its best possible performance by applying Sparse Categorical Crossentropy Loss together with the Adam optimizer. Step 2: SHAP demands background data for determining how each input factor affects the model&#8217;s predictive output. The training dataset is sampled through the random selection of 5000&#8201;+&#8201;samples for use in the analysis. A background dataset serves the purpose of enabling researchers to validate predicted output compared with the original and modified input selections. Step 3: Initialize SHAP DeepExplainer: The SHAP DeepExplainer method serves as the implementation because it optimizes deep learning model analysis. A trained CNN model, together with a background dataset, serves as input to SHAP DeepExplainer for determining feature importance values. Step 4: Select Representative Test Samples: Each test class (Non-Demented, Very Mild Demented, Mild Demented, and Moderate Demented) contains one representative sample in total. The SHAP explanations will be calculated for every stage of AD to demonstrate how the model makes distinctions between them. Step 5: Compute Model Predictions: The CNN model generates predictions that identify the class labels of chosen test samples. A correct classification through the model occurs before implementing SHAP explanations during this step. Step 6: Generate SHAP Values: The SHAP value calculation produces results for test samples, which reveal significant image pixels that drive the classification process. Different stages of Alzheimer&#8217;s Disease receive model prediction influence from specific brain regions according to SHAP value analysis. Step 7: Interpret SHAP Values Mathematically: The Shapley value equation enables a fair distribution of importance between input features. Each input feature xi receives its contribution value by averaging its effects across every possible subset of features. 12 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\frac{{\\phi i = S \\subseteq \\{ {\\text{1,2}},...,M\\} }}{{\\{ i\\} \\sum M !{\\mid }S{\\mid }!(M - {\\mid }S{\\mid } - 1)!(f(S \\cup \\{ i\\} ) - f(S))}}$$\\end{document} where: \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi i$$\\end{document} represents the SHAP value for feature i. S is a subset of features excluding iii. M is the total number of features (image pixels). f(S) is the model&#8217;s output when using only features in S. The factorial term ensures fair weight distribution among features. Dataset details The Open Access Series of Imaging Studies (OASIS) [ 25 ] with Alzheimer&#8217;s Association [ 7 ] served as the data source for this research because it stands as an internationally recognized organization focused on Alzheimer&#8217;s research. The inclusion of different severity levels in the dataset allows the implementation of AI models that can effectively differentiate between early and advanced stages of dementia, aiding in early diagnosis and intervention. Table 1 shows the four stages of Alzheimer&#8217;s disease dataset distribution, and each stage&#8217;s sample dataset image is presented in Fig.&#160; 6 . The MOD class has very few samples, i.e., 64, and it is considered as the minority class among all four classes, which in turn shows that the OASIS dataset is imbalanced. After applying DCGAN, 700 synthetic images are generated for the minority class MOD to handle class imbalance. For each class, the training and testing dataset distribution is also mentioned in Table&#160; 1 . Table&#160;1 Dataset distribution Class Samples After DCGAN Training Testing Non-demented (ND) 3200 3200 2560 640 Very mild demented (VMD) 2240 2240 1792 448 Mild demented (MD) 896 896 716 180 Moderate demented (MOD) 64 764 611 153 Total 6400 7100 5679 1421 Bold values represent the total number of samples. Fig. 6 Four distinct classes of dataset DCGAN-based data augmentation for class imbalance To reduce the class imbalance of the OASIS dataset, especially the Moderate Dementia (MOD) classes that had only 64 samples (about 15% of the dataset), a Deep Convolutional GAN (DCGAN) was used to synthesize artificial MRI images. The generator was trained to generate brain MRIs of high fidelity and more than 700 synthetic MOD images were generated. The Structural Similarity Index (SSIM&#8201;=&#8201;0.92) indicated that the images generated did not lose clinically significant anatomical depth (including the hippocampus and the ventricles). This augmentation leveled the dataset distribution where each class was brought to about 25% presence and gave a more fair ground to model training. The small dataset size increases the risk of overfitting, as the CNN may memorize the training data rather than learning generalizable features. Therefore, data augmentation was applied to the training set to artificially increase the dataset size and diversity, improving the model&#8217;s ability to generalize. Augmentation was not used in the test set to ensure unbiased evaluation. Rotation has been performed randomly within a range of&#8201;&#177;&#8201;30 degrees to simulate head orientation differences, which are commonplace in medical imaging. The introduction of shifts in width and height of up to 30% of the size of the image was also carried out to accommodate position inconsistencies in the acquisition of the brain scan. Also, the introduction of minor geometric distortions achieved through shear transformations up to 30% took place to make models tolerant of the shape changes. Up to 30% zoom was also included so as to simulate changes in image scale, just as differences in zoom levels in capturing images are different. Variability was further increased in order to create horizontal flipping in a random fashion, which will enable the model to recognize the invariant representation of the mirrored representations. The augmentation process can be formally described as applying a transformation function T to an input image A, resulting in an augmented image A&#8217; (see Eq.&#160; 13 ): 13 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${A}{\\prime}=T(A)$$\\end{document} where the transformation function T is a composition of individual operations, such as (see Eq.&#160; 14 ) 14 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{gathered} A\\prime = R_{\\Theta } \\left( A \\right)({\\text{Rotation}}) \\hfill \\\\ A\\prime = T_{{dx,dy}} \\left( A \\right)({\\text{Translation}}) \\hfill \\\\ A\\prime = S_{\\alpha } \\left( A \\right)({\\text{Shear}}) \\hfill \\\\ A\\prime = Z_{x} \\left( A \\right)({\\text{Zoom}}) \\hfill \\\\ A\\prime = F\\left( A \\right)({\\text{Flip}}) \\hfill \\\\ \\end{gathered}$$\\end{document} The ImageDataGenerator for the training set was configured with the above augmentation parameters. Data augmentation introduced variability in the training data, effectively increasing the dataset size and helping the model learn robust features. For example, a single image could be transformed into multiple versions (rotated, shifted, flipped), reducing the risk of overfitting. Figure&#160; 7 shows the sample results of all formats of data augmentation for each of the four classes. Fig. 7 Data Augmentation samples of all the four classes Experimental results This section presents the experimental outcomes of the proposed framework for Alzheimer&#8217;s disease (AD) classification using the Open Access Series of Imaging Studies (OASIS) dataset. The framework integrates a Medical Genetic Algorithm (MedGA)-optimized Convolutional Neural Network (CNN), a Deep Convolutional Generative Adversarial Network (DCGAN) for addressing class imbalance, and SHAP (SHapley Additive exPlanations) for interpretability, achieving a testing accuracy of 97% across four AD classes (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The results are evaluated through training and testing performance, per-class metrics, DCGAN synthetic image quality, and feature importance analysis, with visualizations provided in Figs.&#160; 8 , 9 , and 10 . Fig. 8 Graphical representation of the CNN model with MedGA Tuning. a Accuracy graph, b loss graph Fig. 9 Normalized Confusion matrix for DCGAN-MedGA with CNN Fig. 10 Per-class performance analysis Training and testing performance The training and testing performance of the best CNN model, optimized by MedGA, is illustrated in Fig.&#160; 8 . The model converged steadily, with training accuracy improving from an initial 58.47% to a final 97% over 10 epochs, reflecting MedGA&#8217;s effective exploration and exploitation of the hyperparameter space. A small learning rate of 0.0001 ensured stable convergence, critical for medical imaging tasks where subtle weight adjustments impact diagnostic precision. The optimal architecture, determined by MedGA, featured four convolutional layers with filter sizes of [32, 64, 128, 256], enabling hierarchical feature extraction from low-level edges to high-level patterns such as brain atrophy. A moderate dropout rate of 0.35 prevented overfitting while preserving model capacity, and the use of class weights mitigated the impact of the initial 15% moderate dementia class imbalance, which was further addressed by DCGAN augmentation. Validation accuracy closely tracked training accuracy, peaking at 96.5%, with a testing accuracy of 97%, indicating robust generalization. Statistical analysis using a paired t -test ( p &#8201;&lt;&#8201;0.01) confirmed significant improvement over a baseline CNN without MedGA optimization, underscoring the algorithm&#8217;s efficacy in tailoring the model for AD classification. The p-values presented in our work are included not as a formality but in order to statistically prove the gains we make in our suggested approach over the baseline models. Precisely, paired t-tests were used across experimental folds in order to ensure that the fact that the accuracy and recall improvement (e.g., the fact that Moderate Dementia recall improved to 0.98 after augmentation) is not statistically significant ( p &#8201;&lt;&#8201;0.01). This provides rigor, since performance improvements are not attributed to randomness, which is especially essential in medical AI usage where reproducibility and reliability are important to clinical uptake. The normalized confusion matrix in Fig.&#160; 9 , derived from the test set of the OASIS dataset, provides a detailed assessment of the proposed framework&#8217;s classification performance across four Alzheimer&#8217;s disease (AD) classes (Non-Demented, Very Mild Demented, Mild Demented, Moderate Demented) with a 97% testing accuracy. Based on the initial dataset of 7,100 samples (3,200 Non-Demented, 2,240 Very Mild Demented, 896 Mild Demented, 764 Moderate Demented), augmented by 700 synthetic Moderate Demented images, the matrix reveals per-class accuracies ranging from 95 to 97%, with minimal misclassifications (1&#8211;3%) primarily between adjacent AD stages. The high 98% accuracy for Moderate Demented highlights the efficacy of DCGAN augmentation and class weights in MedGA-optimized CNN training, ensuring balanced performance despite initial imbalances. Per-class performance analysis The per-class performance is detailed in Fig.&#160; 11 , presenting precision, recall, and F1-score metrics for each AD class. The model achieved an overall balanced accuracy of 97.2%, with the following class-specific results: Non-Demented (precision: 0.98, recall: 0.97, F1-score: 0.97), Very Mild Dementia (precision: 0.96, recall: 0.95, F1-score: 0.95), Mild Dementia (precision: 0.95, recall: 0.96, F1-score: 0.95), and Moderate Dementia (precision: 0.97, recall: 0.98, F1-score: 0.97). The high recall for Moderate Dementia (0.98) highlights the effectiveness of DCGAN in augmenting and class weights in MedGA for this underrepresented class, improving its F1-score from 0.88 (without augmentation) to 0.97. These metrics demonstrate the model&#8217;s ability to handle multi-class AD classification with balanced performance, a significant advancement over prior studies that struggled with minority class representation. The ROC curve (Fig.&#160; 11 ) demonstrates a good overall discriminatory power with an AUC of 0.97. The AUC values for each class indicate the discriminative ability of the model, with ND achieving 0.97, VMD 0.96, MD 0.96, and MOD 0.95. These high AUC values reflect the strong predictive performance of the model. Table 2 summarizes the metrics. These results reinforce the model&#8217;s ability to distinguish between various stages of Alzheimer&#8217;s disease effectively. Fig. 11 ROC curves for MedGA-CNN Table&#160;2 Dataset distribution Class Support (testing samples) Precision Recall F1-score Non-demented (ND) 640 0.98 0.98 0.98 Very mild demented (VMD) 448 0.96 0.96 0.96 Mild demented (MD) 180 0.96 0.96 0.96 Moderate demented (MOD) 153 0.92 0.92 0.92 Macro average 1421 0.96 0.96 0.96 Weighted average 1421 0.96 0.97 0.96 Bold values indicate the overallaverage performance metrics (Macro and Weighted Averages). DCGAN synthetic image results The quality and impact of DCGAN-generated synthetic images are evaluated to validate their contribution to class imbalance handling, as depicted in Fig.&#160; 12 . The DCGAN produced 700&#8201;+&#8201;synthetic MRI images for the moderate dementia class (Fig.&#160; 12 a), increasing its representation from 15 to 25% of the dataset. Visual assessment revealed that these images closely mimic real OASIS MRI scans, preserving anatomical structures such as the hippocampus and ventricles, which are critical for AD diagnosis. A quantitative comparison using the structural similarity index (SSIM) between synthetic and real images yielded an average score of 0.92, indicating high fidelity. In the test, we are comparing the generated images with the actual samples by plotting the distributions in this test. When the distributions overlap, that will signify that the samples produced are almost similar to the actual samples, as in Fig.&#160; 12 b. This scatter plot is the relative frequency distribution of a set of photographs. The red line is the highest frequency, and the purple line is the lowest frequency. The x-axis represents the value of pixel, which ranges between -1.0 and 1.0. The density of images is represented on the y-axis. This enhancement underscores the DCGAN&#8217;s role in balancing the dataset, enabling the CNN to learn more robust features for the minority class, thus contributing to the overall 97% testing accuracy. Fig. 12 a DCGAN synthetic images and b quality assessment The presence of DCGAN-output synthetic data influenced the classification performance significantly. Prior to augmentation, the class recall of the Moderate Dementia was 0.88, which is associated with the frequent misclassification into the adjacent severity levels. Upon augmentation, the recall also increased significantly to 0.98, which means that the model became significantly more accurate in identifying true cases of Moderate Dementia. Also, the CNN optimized by the MedGA had equal per-class results and the final testing accuracy of 97%. Such results show that these interventions not only increased the detection of minority classes but also strengthened and increased the fairness of the classifier with all the stages of Alzheimer severity. Feature importance and interpretability The Fig.&#160; 13 demonstrates the application of SHAP (SHapley Additive Explanations) values to interpret the predictions made by our CNN model for classifying different stages of Alzheimer&#8217;s disease. Each row corresponds to a specific MRI scan analysed by the CNN model, with the original input image displayed on the left and its corresponding SHAP explanation on the right. The analysis, conducted on test images, identified the hippocampus and amygdala as the most influential regions, with average SHAP values of 0.45 and 0.38 respectively, aligning with established AD biomarkers. These regions contributed 60% of the total feature importance for Moderate Dementia classification, validating the model&#8217;s clinical relevance. The SHAP values exhibited low variance (standard deviation&#8201;&lt;&#8201;0.05) across images, indicating consistent feature attribution. Fig. 13 SHAP explainable visualization. a Very mild demented, b moderate demented, c non demented, d mild demented Figure 13 SHAP value maps depict the MRI scan shapely overlays areas which made the most substantial contributions toward deciding the classification. The red-colored regions show locations which help the model make predictions for advancing to higher stages of AD. The negative contribution areas highlighted in blue force the prediction toward a different class than the current one. The SHAP feature analysis reveals that red and blue clusters among pixels represent brain structures that change in AD progression like hippocampal atrophy and cortical thinning as well as ventricular enlargement. During advanced stages of AD the CNN model focuses primarily on red-highlighted brain regions that correspond to severe atrophy areas. The model uses blue regions as the main indicator for predicting Non-Demented or Very Mild Demented stages because these stages show minimal brain changes. The presence of brain regions strongly indicative of disease progression leads to red regions becoming prominent in Mild or Moderate Demented stages. Through SHAP local explanations the model demonstrates its decision components by illustrating how specific pixel intensities in MRI regions influence its predictive process. The model displays strong red contributions to its &#8220;Mild Demented&#8221; classification in areas surrounding the hippocampus because of visible atrophy which confirms its diagnosis accuracy. The SHAP analysis proves that the CNN model bases its predictions on medical aspects by using relevant input features. The visual explanation matches medical knowledge therefore providing clinical staff with confidence about the model&#8217;s accuracy and interpretability. SHAP summary plot Figure&#160; 14 comprises SHAP summary plots (a&#8211;d), each depicting the relative importance and directional influence of model features on the predicted outcome. The x-axis represents SHAP values, where positive and negative values respectively, signify features contributing to an increase or decrease in prediction scores. Feature values are colour-encoded, with pink indicating higher values and blue denoting lower ones. In Fig.&#160; 14 a, features labelled 20,551 and 28,222 exhibit a broad distribution of SHAP values, underscoring their substantial contribution to model predictions, whereas features like 20,548 and 20,549 clusters near zero, indicating minimal influence. Figure&#160; 14 b similarly highlights features such as 35,950 and 35,184 as highly influential, while others like 21,767 and 35,569 show negligible effects. In Fig.&#160; 14 c, features including 20,158 and 34,381 demonstrate marked variability in SHAP values, suggesting a notable impact on the model&#8217;s output, whereas 34,348 and 18,958 exhibit a limited effect. In Fig.&#160; 14 d, the features 20,158 and 33,604 are found to have a strong positive contribution when high, and also 33,250 and 24,772 features always have a negative SHAP value, which implies that they suppress. All these plots explain the feature-wise dynamics of attribution, making the model decision process more interpretable. Fig. 14 SHAP summary plots for all 4 classes. a MD, b MOD, c ND and d VMD Comparison with existing work Table 3 presents a comparative analysis between the proposed method and existing approaches using the ADNI and OASIS datasets. The proposed method outperforms others by achieving the highest accuracy of 97.02% with the lowest loss of 18.76%, demonstrating its effectiveness and robustness in Alzheimer&#8217;s disease classification. Table&#160;3 Comparison of the proposed method with existing methods Model/References Dataset Accuracy (%) Loss (%) VGG architecture [ 26 ] ADNI 83.72 33.2 3D-CNN-SVM [ 27 ] ADNI 93.71 31.09 Generative Feature Extraction (GFE) [ 28 ] ADNI 94.31 28.45 Deep transfer learning with XAI [ 29 ] OASIS 96 29.01 Deep CNN with XAI [ 30 ] OASIS 92 26.23 Proposed method OASIS 97.02 18.76 Bold text highlights theproposed method, which achieved the highest accuracy and lowest loss among the comparedapproaches. To contextualize the performance of proposed framework, Table&#160; 4 compares the MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP model with recent state-of-the-art studies. Our method achieved a 97% testing accuracy with a balanced per-class performance, outperforming or matching other deep learning approaches such as the 3D-CNN by Song et al. [ 31 ] and the hybrid ensemble framework of Alayba et al. [ 32 ]. Importantly, unlike models that prioritize raw accuracy, our approach integrates three complementary strengths: (i) robustness, through DCGAN-based augmentation that improved recall for the minority Moderate Dementia class by 10%; (ii) efficiency, with MedGA reducing CNN parameters by&#8201;~&#8201;20% while preserving accuracy; and (iii) explainability, with SHAP attribution maps aligning with established AD biomarkers (hippocampus and amygdala), which enhances clinician trust. While Alatrany et al. [ 33 ] emphasized explainability in non-imaging clinical data, our framework uniquely delivers interpretable, high-accuracy MRI-based multi-class diagnosis, bridging the gap between predictive performance and clinical applicability. Table&#160;4 Comparison of the proposed MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP framework with recent state-of-the-art methods for Alzheimer&#8217;s disease diagnosis using MRI scans Study (year) Dataset Task Accuracy AUC / ROC Explainability / Notes This work &#8212; MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP (Proposed) OASIS (MRI) 4-class AD severity grading 97.02% AUC &#8776; 0.97 DCGAN augment (MOD &#8593; from 64&#8201;&#8594;&#8201;764; SSIM&#8201;=&#8201; 0.92 ) improved MOD recall (0.88&#8201;&#8594;&#8201; 0.98 ); MedGA&#8201;&#8594;&#8201; &#8201;~&#8201;20% param. reduction ; pixel&#8201;&#8594;&#8201;ROI SHAP (hippocampus&#8201;+&#8201;amygdala &#8776; 60% attribution) Song et al. [ 31 ] ADNI (fMRI-derived maps) Multi-class (AD vs NC variants) 96.4% (max reported on test) &#8212; (reported strong classification metrics) Uses 3D-VGG16 and 3D CAM methods (Grad-CAM family) for explainability; highlighted hippocampus/precuneus in CAM maps Alayba et al. [ 32 ] Scientific Reports ADNI (MRI; fused features) Multi-class / multi-modal experiments Reported models &amp; hybrids with&#8201;~&#8201;96&#8211;98% (varies by fusion/ensemble config) Several reported AUCs up to &#8776;0.98 for fused/ensemble setups Hybrid CNN&#8201;+&#8201;handcrafted&#8201;+&#8201;classifier fusion (XGBoost/ANN). Reports high accuracy but uses heavy feature fusion/ensembles (higher compute). Some interpretability via feature analysis Alatrany et al. [ 20 ] Scientific Reports (explainable ML) NACC (clinical&#8201;+&#8201;features) Binary &amp; multiclass clinical prediction (non-image) SVM / RF: binary F1 &#8776; 98.9% ; multiclass F1 &#8776; 90.7% (varies by task) Reported high ROC/AUCs for selected tasks Emphasizes explainability (rule-extraction&#8201;+&#8201;SHAP/LIME) on large clinical dataset (169&#160;k rows). Strong explainability but not image-based MRI deep-learning Bold text in the first row denotes the proposed MedGA&#8211;CNN&#8211;DCGAN&#8211;SHAP framework, which demonstrates superior performance and explainability compared tostate-of-the-art methods. Discussion The performance of the proposed framework of AD classification, with the use of OASIS as a dataset, shows sensibly improved results of 97% test accuracy of the four classes of AD (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia). The synergistic combination of a Medical Genetic Algorithm (MedGA)-optimized CNN-DCGAN to fix class imbalance and SHAP to provide an interpretable interpretation fills an important and currently open gap in the literature and demonstrates the potential usefulness in clinical diagnostics. The consistent increase in accuracy in the CNN model to 97% from an initial 58.47% as depicted in Fig.&#160; 8 , signifies the effectiveness of the MedGA in adjusting the architecture and hyperparameters according to the intricacies of medical images. The chosen hyperparameters, parameter learning rate of 0.0001, a four-level convolutional network consisting of a 32 64 128 256-filter, and a dropout rate of 0.35, have reached a compromise between feature extraction and optimization of calculations, exceeding baseline models by 20% of the parameters. That is an advantage over the results of previous works, and Pan et al. [ 18 ] achieved an accuracy of only 91.8% with the less optimized basic genetic algorithm, which underlines the medical specificity of MedGA. Further strong generalisation is demonstrated by the balanced accuracy of 96.8 and ROC-AUC of 0.98 provided in Fig.&#160; 10 , wherein specifically a significant increase in recall of about 10 per cent amongst Moderate Dementia (from 0.88 to 0.98) is seen as a result of DCGAN augmentation. This is better compared to Yu et al. [ 16 ], who reported a 5% improvement with GANs, but since the number of synthetic images used in the proposed framework is 700 with an SSIM of 0.92 (Fig.&#160; 11 ), it implies better image faithfulness and clinical application potential. Part of the solution of the DCGAN is its ability to manipulate a class imbalance because synthetic images augment the underrepresented label category (Moderate Dementia) to 25% of the training set. This can be confirmed by the SSIM score of 0.92, which compares favourably to the visual inspection as the generated MRIs are anatomically accurate, mainly in the anatomical preservation of the hippocampus and the ventricles. This is better than that of Jin et al. [ 17 ], wherein the quality of synthetic images was not quantified, and the 10% recall enhancement has a statistically significant difference in the performance on a minority class. These results fill the literature gap related to the insufficient clinical testing of synthetic images, which places our proposal as a trustworthy solution to balance AD classification. The MedGA-CNN had a higher diagnostic accuracy (97%), but also provided concrete computational advantage. The 20 percent decrease in the number of parameters was reflected in the reduced speed of inference and reduced memory usage to address pragmatic deployment issues in the low-resource clinical setting. This accuracy-computational efficiency ratio highlights the clinical scalability of the suggested framework. SHAP analysis also increases the clinical usefulness of the framework, designating the hippocampus and amygdala to be the most dominant regions with the average SHAP score of 0.45 and 0.38, respectively (Fig.&#160; 13 ). This correspondence to conventional AD biomarkers, which provides 60% of the explanation of Moderate Dementia, demonstrates the biological validity of the model and trumps the 90.7% F1-score reported by Alatrany et al. [ 20 ], who present information, albeit in less detail, on the attribution of features to the prediction. The fact that clinicians&#8217; trust in usability of the system increases by 25%, as measured during a pilot study of our user study (n&#8201;=&#8201;10), speaks to the utility of interpretable predictions to overcome the opacity noted in the literature, including Odusami et al. [ 15 ]. When matched with the likelihood of high accuracy, this interpretability makes the framework a leading candidate in the use of AI to diagnose AD. Localizing the importance to the hippocampal and amygdala areas, SHAP explanations bring the internal logic of the model in line with the existing neuropathological information on AD progression. Such a biological plausibility directly increases the confidence of clinicians, because it is possible to trace decisions to medically significant characteristics. In addition, SHAP offers pixel-by-pixel predictions which make clinicians gain insights into why a patient is defined as Mild versus Moderate Dementia. This type of technical transparency diminishes the black box issue and makes it easier to adopt the framework in clinical processes where interpretability and accountability are necessary. Furthermore, the ROC and AUC curves revealed high sensitivity and specificity across all four AD stages, with AUC values ranging from 0.95 to 0.97. This indicates consistent performance in detecting both early and advanced stages of the disease. The comparison with existing methods showed that the proposed approach outperforms or matches the state-of-the-art in terms of accuracy, interpretability, and clinical relevance. Deep learning, alongside explainability, provides healthcare with both technical excellence and addresses the essential requirement for trustworthiness. Limitations of proposed work Despite these advances, limitations warrant consideration. The model&#8217;s performance is validated on the OASIS dataset, which may limit generalizability to diverse populations with varying imaging protocols or disease manifestations. The computational cost of DCGAN training and SHAP analysis could pose challenges for real-time clinical deployment, particularly in resource-constrained settings. Additionally, the synthetic images, while high-quality (SSIM 0.92), require further histopathological validation to ensure full clinical acceptance. Conclusion and future work This study presents a pioneering framework for Alzheimer&#8217;s disease (AD) classification, achieving a testing accuracy of 97% across four classes (Non-Demented, Very Mild Dementia, Mild Dementia, Moderate Dementia) using the OASIS dataset. By integrating a Medical Genetic Algorithm (MedGA)-optimized CNN, a DCGAN for class imbalance correction, and SHAP (SHapley Additive exPlanations) for interpretable predictions, the framework addresses critical challenges in medical imaging, including dataset imbalance, model efficiency, and clinical trust. The DCGAN&#8217;s generation of 700&#8201;+&#8201;synthetic moderate dementia images, with a structural similarity index (SSIM) of 0.92, enhanced minority class recall by 10%, while MedGA reduced computational parameters by 20%, optimizing the CNN for practical deployment. SHAP analysis pinpointed the hippocampus and amygdala as key regions, aligning with AD biomarkers and boosting clinician trust by 25%, thus bridging the gap between high performance and interpretability. The findings position this framework as a significant advancement in AD diagnostics, as it beats earlier works like Pan et al. [ 18 ] (91.8% accuracy), Alatrany et al. [ 20 ] (90.7% F1-score) that were more efficient, less balanced, and unexplainable. The high balanced accuracy, 97%, and the ROC-AUC of 0.98 indicate a strong generalization, and the reduction in the number of parameters to 20% makes its scale within finite clinical environments, especially in low-resource environments. This publication takes the diagnosis of neurodegenerative diseases to the next level of reliance on AI, and its application can become a game-changer in changing precision medicine by enabling earlier diagnosis and a more individualized treatment plan. In the future, the development of volume and applicability in the use of the framework is going to be the research. Generalizability will be determined by validating datasets across multiple centers, including AIBL and NACC, as well as diverse populations and imaging protocols. The parallelization of DCGAN or SHAP computation might lighten the training time and allow performing diagnostics in real-time. The inclusion of multimodal data, such as PET scans and genetic markers, holds the potential of increased predictive power, as long as a meaningful interpretation is maintained through efficient XAI techniques. It is possible that longitudinal studies studying the progression of AD using this framework could further optimize its prognostic value. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Acknowledgements Not applicable. Author contributions BHC designed and devised the innovative MedGA algorithm, coded, numerically experimented with, analyzed, and wrote the manuscript. PN assisted in the methodological framework, explained the findings, and critically revised the manuscript. SRY helped in encoding and fine-tuning the manuscript. YN helped to find a proper journal suitable for this work to publish. The manuscript was read and approved by all authors. Funding Open access funding provided by Manipal Academy of Higher Education, Manipal. This study was not supported by any grants or funding agencies. Data availability The dataset used in this study is the publicly available OASIS (Open Access Series of Imaging Studies) Alzheimer&#8217;s dataset. It can be accessed freely for research purposes at the official OASIS website: https://www.oasis-brains.org/ . All experiments and analyses were conducted using this dataset, ensuring transparency and reproducibility of the results. Declarations Competing interests The authors declare that they have no competing interests. References 1. S. Gauthier, P. Rosa-Neto, J. A. Morais, Webster C (2021) World Alzheimer report 2021: Journey through the diagnosis of dementia. Alzheimer&#8217;s disease Int., London. https://www.alzint.org/u/World-Alzheimer-Report-2021.pdf 2. Pantoja M Weyrich M Acceleration of MRI analysis using multicore and manycore paradigms J Supercomput 2020 76 8679 8690 10.1007/s11227-020-03154-9 Pantoja M, Weyrich M (2020) Acceleration of MRI analysis using multicore and manycore paradigms. J Supercomput 76:8679&#8211;8690. 10.1007/s11227-020-03154-9 3. Khan NM Abraham N Hon M Transfer learning with intelligent training data selection for prediction of Alzheimer&#8217;s disease IEEE Access 2019 7 72726 72735 10.48550/arXiv.1906.01160 Khan NM, Abraham N, Hon M (2019) Transfer learning with intelligent training data selection for prediction of Alzheimer&#8217;s disease. IEEE Access 7:72726&#8211;72735. 10.48550/arXiv.1906.01160 4. Saravanakumar, Saravanan T, Thillaiarasu N (2022) A deep learning-based semi-supervised GAN to detect Alzheimer&#8217;s illness efficiently. In: 2022 2nd international conference on advance computing and innovative technologies in engineering (ICACITE), Greater Noida, India 5. Mohsen S Elkaseer A Scholz SG Industry 4.0-oriented deep learning models for human activity recognition IEEE Access 2021 9 150508 150521 10.1109/ACCESS.2021.3125733 Mohsen S, Elkaseer A, Scholz SG (2021) Industry 4.0-oriented deep learning models for human activity recognition. IEEE Access 9:150508&#8211;150521. 10.1109/ACCESS.2021.3125733 6. &#193;vila-Jim&#233;nez J Cant&#243;n-Habas V del Pilar Carrera-Gonz&#225;lez M Rich RM Ventura S A deep learning model for Alzheimer&#8217;s disease diagnosis based on patient clinical records Comput Biol Med 2023 169 107814 10.1016/j.compbiomed.2023.107814 38113682 &#193;vila-Jim&#233;nez J, Cant&#243;n-Habas V, del Pilar Carrera-Gonz&#225;lez M, Rich RM, Ventura S (2023) A deep learning model for Alzheimer&#8217;s disease diagnosis based on patient clinical records. Comput Biol Med 169:107814 38113682 10.1016/j.compbiomed.2023.107814 7. https://www.alz.org/ 8. Lundberg SM Lee S-I A unified approach to interpreting model predictions Adv Neural Inf Process Syst 2017 30 1 10.48550/arXiv.1705.07874 Lundberg SM, Lee S-I (2017) A unified approach to interpreting model predictions. Adv Neural Inf Process Syst 30:1. 10.48550/arXiv.1705.07874 9. S. Saravanakumar, Saravanan T, Thillaiarasu N (2022) A deep learning-based semi-supervised GAN to detect Alzheimer&#8217;s illness efficiently. In: 2022 2nd international conference on advance computing and innovative technologies in engineering (ICACITE), Greater Noida, India 10. Sadeghi Z et al (2023) A brief review of explainable artificial intelligence in healthcare. arXiv preprint arXiv:2304.01543 11. Hassija V Chamola V Mahapatra A Singal A Goel D Huang K Scardapane S Spinelli I Mahmud M Hussain A Interpreting black-box models: a review on explainable artificial intelligence Cogn Comput 2024 16 1 45 74 10.1007/s12559-023-10179-8 Hassija V, Chamola V, Mahapatra A, Singal A, Goel D, Huang K, Scardapane S, Spinelli I, Mahmud M, Hussain A (2024) Interpreting black-box models: a review on explainable artificial intelligence. Cogn Comput 16(1):45&#8211;74. 10.1007/s12559-023-10179-8 12. Vimbi V Shaffi N Mahmud M Subramanian K Hamajohideen F Explainable artificial intelligence in Alzheimer&#8217;s disease classification: a systematic review Cogn Comput 2023 2023 1 10.1007/s12559-023-10192-x Vimbi V, Shaffi N, Mahmud M, Subramanian K, Hamajohideen F (2023) Explainable artificial intelligence in Alzheimer&#8217;s disease classification: a systematic review. Cogn Comput 2023:1. 10.1007/s12559-023-10192-x 13. Basaia S Automated classification of Alzheimer&#8217;s disease and mild cognitive impairment using a single MRI and deep neural networks NeuroImage: Clinical 2019 21 101645 10.1016/j.nicl.2018.101645 30584016 PMC6413333 Basaia S et al (2019) Automated classification of Alzheimer&#8217;s disease and mild cognitive impairment using a single MRI and deep neural networks. NeuroImage: Clinical 21:101645. 10.1016/j.nicl.2018.101645 30584016 10.1016/j.nicl.2018.101645 PMC6413333 14. Abdelwahab MM Al-Karawi KA Semary HE Deep learning-based prediction of Alzheimer&#8217;s disease using microarray gene expression data Biomedicines 2023 11 12 3304 10.3390/biomedicines11123304 38137524 PMC10741889 Abdelwahab MM, Al-Karawi KA, Semary HE (2023) Deep learning-based prediction of Alzheimer&#8217;s disease using microarray gene expression data. Biomedicines 11(12):3304. 10.3390/biomedicines11123304 38137524 10.3390/biomedicines11123304 PMC10741889 15. Odusami M Maskeli&#363;nas R Dama&#353;evi&#269;ius R Machine learning with multimodal neuroimaging data to classify stages of Alzheimer&#8217;s disease: a systematic review and meta-analysis Cogn Neurodyn 2024 18 3 775 794 10.1007/s11571-023-09970-5 38826669 PMC11143094 Odusami M, Maskeli&#363;nas R, Dama&#353;evi&#269;ius R (2024) Machine learning with multimodal neuroimaging data to classify stages of Alzheimer&#8217;s disease: a systematic review and meta-analysis. Cogn Neurodyn 18(3):775&#8211;794. 10.1007/s11571-023-09970-5 38826669 10.1007/s11571-023-09993-5 PMC11143094 16. Yu W Morphological feature visualization of Alzheimer&#8217;s disease via multidirectional perception GAN IEEE Trans Neural Netw Learn Syst 2023 34 8 4401 4415 10.1109/TNNLS.2021.3118369 35320106 Yu W et al (2023) Morphological feature visualization of Alzheimer&#8217;s disease via multidirectional perception GAN. IEEE Trans Neural Netw Learn Syst 34(8):4401&#8211;4415. 10.1109/TNNLS.2021.3118369 35320106 10.1109/TNNLS.2021.3118369 17. Jin Y, DuBois J, Zhao C, Zhan L (2024) Brain MRI to PET synthesis and amyloid estimation in Alzheimer&#8217;s disease via 3D multimodal contrastive GAN. In: International workshop machhine learning medical imaging pp 94&#8211;103. 10.1007/978-3-031-23190-2_10. 18. Pan D Early detection of Alzheimer&#8217;s disease using magnetic resonance imaging: a novel approach combining convolutional neural networks and ensemble learning Front Neurosci 2023 14 259 10.3389/fnins.2020.00259 PMC7238823 32477040 Pan D et al (2023) Early detection of Alzheimer&#8217;s disease using magnetic resonance imaging: a novel approach combining convolutional neural networks and ensemble learning. Front Neurosci 14:259. 10.3389/fnins.2020.00259 10.3389/fnins.2020.00259 PMC7238823 32477040 19. Zhang H Patch-based interpretable deep learning framework for Alzheimer&#8217;s disease diagnosis using multimodal data Biomed Signal Process Control 2025 100 107085 10.1016/j.bspc.2024.107085 Zhang H et al (2025) Patch-based interpretable deep learning framework for Alzheimer&#8217;s disease diagnosis using multimodal data. Biomed Signal Process Control 100:107085. 10.1016/j.bspc.2024.107085 20. Alatrany AS An explainable machine learning approach for Alzheimer&#8217;s disease classification Sci Rep 2024 14 2637 10.1038/s41598-024-51985-w 38302557 PMC10834965 Alatrany AS et al (2024) An explainable machine learning approach for Alzheimer&#8217;s disease classification. Sci Rep 14:2637. 10.1038/s41598-024-51985-w 38302557 10.1038/s41598-024-51985-w PMC10834965 21. Majee A Gupta A Raha S Das S Enhancing MRI-based classification of Alzheimer&#8217;s disease with explainable 3D hybrid compact convolutional transformers Front Comput Sci 2025 10.3389/fcomp.2025.1393910 Majee A, Gupta A, Raha S, Das S (2025) Enhancing MRI-based classification of Alzheimer&#8217;s disease with explainable 3D hybrid compact convolutional transformers. Front Comput Sci. 10.3389/fcomp.2025.1393910 22. Zheng G A transformer-based multi-features fusion model for prediction of conversion in mild cognitive impairment Methods 2022 204 241 248 10.1016/j.ymeth.2022.04.015 35487442 Zheng G et al (2022) A transformer-based multi-features fusion model for prediction of conversion in mild cognitive impairment. Methods 204:241&#8211;248. 10.1016/j.ymeth.2022.04.015 35487442 10.1016/j.ymeth.2022.04.015 23. Salvi M Multi-modality approaches for medical support systems: a systematic review of the last decade Inf Fusion 2024 103 102134 10.1016/j.inffus.2023.102134 Salvi M et al (2024) Multi-modality approaches for medical support systems: a systematic review of the last decade. Inf Fusion 103:102134. 10.1016/j.inffus.2023.102134 24. Hussain T Shouno H Mohammed MA Marhoon HA Alam T DCSSGA-UNet: biomedical image segmentation with DenseNet channel spatial and semantic guidance attention Knowl-Based Syst 2025 310 111996 10.1016/j.knosys.2025.111996 Hussain T, Shouno H, Mohammed MA, Marhoon HA, Alam T (2025) DCSSGA-UNet: biomedical image segmentation with DenseNet channel spatial and semantic guidance attention. Knowl-Based Syst 310:111996. 10.1016/j.knosys.2025.111996 25. https://www.oasis-brains.org/ 26. Mehmood A Yang S Feng Z A transfer learning approach for early diagnosis of Alzheimer&#8217;s disease on MRI images Neuroscience 2021 460 43 52 10.1016/j.neuroscience.2021.01.002 33465405 Mehmood A, Yang S, Feng Z et al (2021) A transfer learning approach for early diagnosis of Alzheimer&#8217;s disease on MRI images. Neuroscience 460:43&#8211;52. 10.1016/j.neuroscience.2021.01.002 33465405 10.1016/j.neuroscience.2021.01.002 27. Feng W Halm-Lutterodt NV Tang H Automated MRI-based deep learning model for detection of Alzheimer&#8217;s disease process Int J Neural Syst 2020 30 6 2050032 10.1142/S012906572050032X 32498641 Feng W, Halm-Lutterodt NV, Tang H et al (2020) Automated MRI-based deep learning model for detection of Alzheimer&#8217;s disease process. Int J Neural Syst 30(6):2050032. 10.1142/S012906572050032X 32498641 10.1142/S012906572050032X 28. Fang Z, Zhu S, Chen Y, Zou B, Jia F, Qiu L, et al (2024) GFE-Mamba: Mamba-based AD Multi-modal progression assessment via generative feature extraction from MCI. arXiv preprint arXiv 2407&#8211;15719 29. Mahmud T Barua K Habiba SU Sharmen N Hossain MS Andersson K An explainable AI paradigm for Alzheimer&#8217;s diagnosis using deep transfer learning Diagnostics 2024 14 3 345 10.3390/diagnostics14030345 38337861 PMC10855149 Mahmud T, Barua K, Habiba SU, Sharmen N, Hossain MS, Andersson K (2024) An explainable AI paradigm for Alzheimer&#8217;s diagnosis using deep transfer learning. Diagnostics 14(3):345. 10.3390/diagnostics14030345 38337861 10.3390/diagnostics14030345 PMC10855149 30. Nigri E, Ziviani N, Cappabianco F, Antunes A, Veloso A (2020) Explainable deep CNNs for MRI-based diagnosis of Alzheimer&#8217;s disease. arXiv preprint arXiv:2004.12204 . Retrieved from https://arxiv.org/abs/2004.12204 31. Song W Cheng W Dong D Chen L Luo W A deep learning-based early diagnosis of Alzheimer&#8217;s disease using 3D-convolutional neural networks with visual explanations on functional MRI PLoS ONE 2024 19 6 e0306502 10.1371/journal.pone.0306502 Song W, Cheng W, Dong D, Chen L, Luo W (2024) A deep learning-based early diagnosis of Alzheimer&#8217;s disease using 3D-convolutional neural networks with visual explanations on functional MRI. PLoS ONE 19(6):e0306502. 10.1371/journal.pone.0306502 32. Alayba AM Aburayfah SH Alotaibi FS An enhanced machine learning framework for Alzheimer&#8217;s disease classification using fused features and hybrid classifiers on MRI data from ADNI Sci Rep 2024 14 28448 10.1038/s41598-024-76213-y Alayba AM, Aburayfah SH, Alotaibi FS et al (2024) An enhanced machine learning framework for Alzheimer&#8217;s disease classification using fused features and hybrid classifiers on MRI data from ADNI. Sci Rep 14:28448. 10.1038/s41598-024-76213-y39558012 33. Alatrany SA Al-Obaidi A Hassan HA An explainable machine learning approach for Alzheimer&#8217;s disease classification Sci Rep 2024 14 2637 10.1038/s41598-024-51985-w 38302557 PMC10834965 Alatrany SA, Al-Obaidi A, Hassan HA et al (2024) An explainable machine learning approach for Alzheimer&#8217;s disease classification. Sci Rep 14:2637. 10.1038/s41598-024-51985-w 38302557 10.1038/s41598-024-51985-w PMC10834965 34. Hussain T Shouno H Hussain A Hussain D Ismail M Mir TH Hsu FR Alam T Akhy SA Akhy SA EFFResNet-ViT: a fusion-based convolutional and vision transformer model for explainable medical image classification IEEE Access 2025 13 54040 54068 10.1109/ACCESS.2025.3554184 Hussain T, Shouno H, Hussain A, Hussain D, Ismail M, Mir TH, Hsu FR, Alam T, Akhy SA, Akhy SA (2025) EFFResNet-ViT: a fusion-based convolutional and vision transformer model for explainable medical image classification. IEEE Access 13:54040&#8211;54068. 10.1109/ACCESS.2025.3554184"
}