{
  "pmcid": "PMC12682993",
  "source": "PMC",
  "download_date": "2025-12-09T16:06:41.097721",
  "metadata": {
    "journal_title": "Technology in Cancer Research & Treatment",
    "journal_nlm_ta": "Technol Cancer Res Treat",
    "journal_iso_abbrev": "Technol Cancer Res Treat",
    "journal": "Technology in Cancer Research & Treatment",
    "pmcid": "PMC12682993",
    "pmid": "41348514",
    "doi": "10.1177/15330338251397035",
    "title": "MDA-TransUNet: A Deep Learning-Based Automatic Segmentation Method for Cervical Cancer Brachytherapy",
    "year": "2025",
    "month": "12",
    "day": "5",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "5"
    },
    "authors": [
      "Cao Dezheng",
      "Jin Jianhua",
      "Han Jihua",
      "Yang Bo",
      "Shi Tingting",
      "Kong Yan",
      "Liu Cong",
      "Qian Chunjun",
      "Xie Kai",
      "Song Lintao",
      "Zhang Heng",
      "Huang Qianjia",
      "Ni Xinye"
    ],
    "abstract": "Introduction Accurate delineation of the high-risk clinical target volume (HR-CTV) and organs at risk (OARs) is critical for cervical cancer brachytherapy. However, treatment planning is time-consuming, and prolonged waiting can lead to organ displacement and patient discomfort. Additionally, the steep dose gradients around HR-CTV amplify segmentation errors in HR-CTV and OARs. Therefore, achieving rapid and precise delineation of HR-CTV and OARs remains challenging. This study proposes a novel network model, MDA-TransUNet, for fast segmentation of HR-CTV and OARs in cervical cancer. Methods We applied MDA-TransUnet, a CNN-Transformer hybrid model, to segment the bladder, colon, rectum, small bowel, and HR-CTV on cervical cancer CT images. 122 cervical cancer brachytherapy patientsâ€™ CT images from three clinical centers were utilized for training and testing, with 80 cases allocated to training, 22 to testing, and 20 to external validation. Segmentation accuracy was quantified using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD95), and Average Surface Distance (ASD). Dosimetric differences were analyzed via paired t-tests. Results Compared to other methods, MDA-TransUnet achieved superior segmentation performance on the test dataset. The DSCs for the bladder, colon, rectum, small bowel, and HR-CTV were 94.54%, 79.27%, 79.27%, 88.90%, and 82.35%, respectively. Paired t-tests on five dosimetric metrics (D5cc, D2cc, D0.1cc, D90%, and Dmean) showed no significant differences. For OARs, the average difference in D2cc was less than 12%. For HR-CTV, the average difference in Dmean was less than 8%, and D90% was less than 11%. Conclusion This work demonstrates the superiority of MDA-TransUnet in segmenting OARs and HR-CTV for cervical cancer brachytherapy, with robust performance across multi-center datasets.",
    "keywords": [
      "deep learning",
      "high-dose-rate brachytherapy",
      "auto-segmentation",
      "cervical cancer",
      "image-guided"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Technol Cancer Res Treat</journal-id><journal-id journal-id-type=\"iso-abbrev\">Technol Cancer Res Treat</journal-id><journal-id journal-id-type=\"pmc-domain-id\">3299</journal-id><journal-id journal-id-type=\"pmc-domain\">tct</journal-id><journal-id journal-id-type=\"publisher-id\">TCT</journal-id><journal-title-group><journal-title>Technology in Cancer Research &amp; Treatment</journal-title></journal-title-group><issn pub-type=\"ppub\">1533-0346</issn><issn pub-type=\"epub\">1533-0338</issn><publisher><publisher-name>SAGE Publications</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682993</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682993.1</article-id><article-id pub-id-type=\"pmcaid\">12682993</article-id><article-id pub-id-type=\"pmcaiid\">12682993</article-id><article-id pub-id-type=\"pmid\">41348514</article-id><article-id pub-id-type=\"doi\">10.1177/15330338251397035</article-id><article-id pub-id-type=\"publisher-id\">10.1177_15330338251397035</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Research Article</subject></subj-group></article-categories><title-group><article-title>MDA-TransUNet: A Deep Learning-Based Automatic Segmentation Method for Cervical Cancer Brachytherapy</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0009-0001-1064-1034</contrib-id><name name-style=\"western\"><surname>Cao</surname><given-names initials=\"D\">Dezheng</given-names></name><degrees>MS</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref><xref rid=\"aff4-15330338251397035\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Jin</surname><given-names initials=\"J\">Jianhua</given-names></name><degrees>BS</degrees><xref rid=\"aff5-15330338251397035\" ref-type=\"aff\">5</xref><xref rid=\"fn11-15330338251397035\" ref-type=\"author-notes\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names initials=\"J\">Jihua</given-names></name><degrees>MS</degrees><xref rid=\"aff6-15330338251397035\" ref-type=\"aff\">6</xref><xref rid=\"fn11-15330338251397035\" ref-type=\"author-notes\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names initials=\"B\">Bo</given-names></name><degrees>BS</degrees><xref rid=\"aff7-15330338251397035\" ref-type=\"aff\">7</xref><xref rid=\"fn11-15330338251397035\" ref-type=\"author-notes\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names initials=\"T\">Tingting</given-names></name><degrees>BS</degrees><xref rid=\"aff6-15330338251397035\" ref-type=\"aff\">6</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Kong</surname><given-names initials=\"Y\">Yan</given-names></name><degrees>PhD</degrees><xref rid=\"aff7-15330338251397035\" ref-type=\"aff\">7</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"C\">Cong</given-names></name><degrees>PhD</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref><xref rid=\"aff8-15330338251397035\" ref-type=\"aff\">8</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Qian</surname><given-names initials=\"C\">Chunjun</given-names></name><degrees>PhD</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref><xref rid=\"aff9-15330338251397035\" ref-type=\"aff\">9</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names initials=\"K\">Kai</given-names></name><degrees>MS</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names initials=\"L\">Lintao</given-names></name><degrees>MS</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0002-2019-0038</contrib-id><name name-style=\"western\"><surname>Zhang</surname><given-names initials=\"H\">Heng</given-names></name><degrees>MS</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0009-0005-1320-4824</contrib-id><name name-style=\"western\"><surname>Huang</surname><given-names initials=\"Q\">Qianjia</given-names></name><degrees>MS</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref><xref rid=\"aff4-15330338251397035\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0000-0002-2402-9719</contrib-id><name name-style=\"western\"><surname>Ni</surname><given-names initials=\"X\">Xinye</given-names></name><degrees>PhD</degrees><xref rid=\"aff1-15330338251397035\" ref-type=\"aff\">1</xref><xref rid=\"aff2-15330338251397035\" ref-type=\"aff\">2</xref><xref rid=\"aff3-15330338251397035\" ref-type=\"aff\">3</xref><xref rid=\"corresp1-15330338251397035\" ref-type=\"corresp\"/></contrib></contrib-group><aff id=\"aff1-15330338251397035\">\n<label>1</label>Department of Radiotherapy, <institution-wrap><institution-id institution-id-type=\"Ringgold\">599923</institution-id><institution content-type=\"university\">The Second People's Hospital of Changzhou, the Third Affiliated Hospital of Nanjing Medical University</institution></institution-wrap>, Changzhou, China</aff><aff id=\"aff2-15330338251397035\">\n<label>2</label>Jiangsu Province Engineering Research Center of Medical Physics, Changzhou, China</aff><aff id=\"aff3-15330338251397035\">\n<label>3</label>Center of Medical Physics, Nanjing Medical University, Changzhou, China</aff><aff id=\"aff4-15330338251397035\">\n<label>4</label>School of Computer Science and Artificial Intelligence, <institution-wrap><institution-id institution-id-type=\"Ringgold\">12412</institution-id><institution content-type=\"university\">Changzhou University</institution></institution-wrap>, Changzhou, China</aff><aff id=\"aff5-15330338251397035\">\n<label>5</label>Department of Radiotherapy, <institution-wrap><institution-id institution-id-type=\"Ringgold\">377323</institution-id><institution content-type=\"university\">Affiliated Tumor Hospital of Nantong University</institution></institution-wrap>, Nantong, China</aff><aff id=\"aff6-15330338251397035\">\n<label>6</label>Department of Radiotherapy, the Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University, Huai'an, China</aff><aff id=\"aff7-15330338251397035\">\n<label>7</label>Department of Radiation Oncology, <institution-wrap><institution-id institution-id-type=\"Ringgold\">199193</institution-id><institution content-type=\"university\">Affiliated Hospital of Jiangnan University</institution></institution-wrap>, Wuxi, China</aff><aff id=\"aff8-15330338251397035\">\n<label>8</label>Faculty of Business Information, <institution-wrap><institution-id institution-id-type=\"Ringgold\">127231</institution-id><institution content-type=\"university\">Shanghai Business School</institution></institution-wrap>, Shanghai, China</aff><aff id=\"aff9-15330338251397035\">\n<label>9</label>Hertfordshire College, <institution-wrap><institution-id institution-id-type=\"Ringgold\">164387</institution-id><institution content-type=\"university\">Changzhou Institute of Technology</institution></institution-wrap>, Changzhou, China</aff><author-notes><fn fn-type=\"other\" id=\"fn11-15330338251397035\"><label>*</label><p>These authors are co-first authors.</p></fn><corresp id=\"corresp1-15330338251397035\">Xinye Ni, Department of Radiotherapy, The Second People's Hospital of Changzhou, the Third Affiliated Hospital of Nanjing Medical University, Changzhou 213003, China. \nEmail: <email>nxy@njmu.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>5</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>24</volume><issue-id pub-id-type=\"pmc-issue-id\">479399</issue-id><elocation-id>15330338251397035</elocation-id><history><date date-type=\"received\"><day>5</day><month>6</month><year>2025</year></date><date date-type=\"rev-recd\"><day>11</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>29</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>05</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 00:25:14.317\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><copyright-holder content-type=\"sage\">SAGE Publications</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbynclicense\">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc/4.0/\">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>) which permits non-commercial use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://us.sagepub.com/en-us/nam/open-access-at-sage\">https://us.sagepub.com/en-us/nam/open-access-at-sage</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"10.1177_15330338251397035.pdf\"/><abstract><sec><title>Introduction</title><p>Accurate delineation of the high-risk clinical target volume (HR-CTV) and organs at risk (OARs) is critical for cervical cancer brachytherapy. However, treatment planning is time-consuming, and prolonged waiting can lead to organ displacement and patient discomfort. Additionally, the steep dose gradients around HR-CTV amplify segmentation errors in HR-CTV and OARs. Therefore, achieving rapid and precise delineation of HR-CTV and OARs remains challenging. This study proposes a novel network model, MDA-TransUNet, for fast segmentation of HR-CTV and OARs in cervical cancer.</p></sec><sec><title>Methods</title><p>We applied MDA-TransUnet, a CNN-Transformer hybrid model, to segment the bladder, colon, rectum, small bowel, and HR-CTV on cervical cancer CT images. 122 cervical cancer brachytherapy patients&#8217; CT images from three clinical centers were utilized for training and testing, with 80 cases allocated to training, 22 to testing, and 20 to external validation. Segmentation accuracy was quantified using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD95), and Average Surface Distance (ASD). Dosimetric differences were analyzed via paired t-tests.</p></sec><sec><title>Results</title><p>Compared to other methods, MDA-TransUnet achieved superior segmentation performance on the test dataset. The DSCs for the bladder, colon, rectum, small bowel, and HR-CTV were 94.54%, 79.27%, 79.27%, 88.90%, and 82.35%, respectively. Paired t-tests on five dosimetric metrics (D5cc, D2cc, D0.1cc, D90%, and Dmean) showed no significant differences. For OARs, the average difference in D2cc was less than 12%. For HR-CTV, the average difference in Dmean was less than 8%, and D90% was less than 11%.</p></sec><sec><title>Conclusion</title><p>This work demonstrates the superiority of MDA-TransUnet in segmenting OARs and HR-CTV for cervical cancer brachytherapy, with robust performance across multi-center datasets.</p></sec></abstract><kwd-group><kwd>deep learning</kwd><kwd>high-dose-rate brachytherapy</kwd><kwd>auto-segmentation</kwd><kwd>cervical cancer</kwd><kwd>image-guided</kwd></kwd-group><funding-group specific-use=\"FundRef\"><award-group id=\"award1-15330338251397035\"><funding-source id=\"funding1-15330338251397035\"><institution-wrap><institution>Social Development Project of Jiangsu Provincial Key Research &amp; Development Plan</institution></institution-wrap></funding-source><award-id rid=\"funding1-15330338251397035\">No. BE2022720</award-id></award-group><award-group id=\"award2-15330338251397035\"><funding-source id=\"funding2-15330338251397035\"><institution-wrap><institution>Jiangsu Provincial Medical Key Discipline Construction Unit (Oncology Therapeutics (Radiotherapy))</institution></institution-wrap></funding-source><award-id rid=\"funding2-15330338251397035\">No. JSDW202237</award-id></award-group><award-group id=\"award3-15330338251397035\"><funding-source id=\"funding3-15330338251397035\"><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type=\"FundRef\">https://doi.org/10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id rid=\"funding3-15330338251397035\">No. 62371243</award-id></award-group><award-group id=\"award4-15330338251397035\"><funding-source id=\"funding4-15330338251397035\"><institution-wrap><institution>Changzhou Social Development Project</institution></institution-wrap></funding-source><award-id rid=\"funding4-15330338251397035\">No. CE20235063</award-id></award-group><award-group id=\"award5-15330338251397035\"><funding-source id=\"funding5-15330338251397035\"><institution-wrap><institution>Applied Basic Research Project of Changzhou</institution></institution-wrap></funding-source><award-id rid=\"funding5-15330338251397035\">No. CJ20244020</award-id></award-group><award-group id=\"award6-15330338251397035\"><funding-source id=\"funding6-15330338251397035\"><institution-wrap><institution>Natural Science Foundation of Jiangsu Province</institution><institution-id institution-id-type=\"FundRef\">https://doi.org/10.13039/501100004608</institution-id></institution-wrap></funding-source><award-id rid=\"funding6-15330338251397035\">No. BK20231190</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>typesetter</meta-name><meta-value>ts19</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>January-December 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"section1-15330338251397035\"><title>Introduction</title><p>Cervical cancer ranks as the fourth most common malignant tumor among women worldwide, with over 600,000 new cases annually, approximately 90% of which occur in developing countries with limited healthcare resources.<sup>\n<xref rid=\"bibr1-15330338251397035\" ref-type=\"bibr\">1</xref>\n</sup> Its high incidence and mortality rates pose a significant threat to women's health. Precision radiotherapy, particularly combining external beam radiation therapy (EBRT) and brachytherapy (BT), is a cornerstone in treating locally advanced cervical cancer.<sup>\n<xref rid=\"bibr2-15330338251397035\" ref-type=\"bibr\">2</xref>\n</sup> However, the unique anatomical location of cervical tumors&#8212;where high-risk clinical target volumes (HR-CTV) are closely adjacent to organs at risk (OARs) such as the bladder and rectum&#8212;necessitates a delicate balance between tumor eradication and normal tissue sparing during treatment planning.</p><p>Brachytherapy (BT), which involves placing radioactive sources directly into or near the tumor target, exploits the rapid dose fall-off with distance to deliver high local tumor doses while minimizing damage to OARs.<sup>\n<xref rid=\"bibr3-15330338251397035\" ref-type=\"bibr\">3</xref>\n</sup> In cervical cancer treatment, BT has proven to significantly improve local control rates and patient survival. However, BT planning heavily relies on manual delineation of targets and OARs by physicians, leading to subjectivity, inefficiency,<sup>\n<xref rid=\"bibr4-15330338251397035\" ref-type=\"bibr\">4</xref>\n</sup> and prolonged workflows. On average, radiation oncologists require 32&#8197;min to delineate HR-CTV and OARs for gynecological malignancies.<sup>\n<xref rid=\"bibr5-15330338251397035\" ref-type=\"bibr\">5</xref>\n</sup> The demand for rapid yet precise planning creates high-pressure workflows prone to human error, while extended procedures exacerbate patient discomfort.</p><p>In recent years, with the growing adoption of deep learning, various neural network architectures&#8212;primarily convolutional neural networks (CNNs)&#8212;have been developed for segmentation in cervical cancer high-dose-rate brachytherapy (HDR-BT).<sup><xref rid=\"bibr6-15330338251397035\" ref-type=\"bibr\">6</xref><xref rid=\"bibr7-15330338251397035\" ref-type=\"bibr\"/><xref rid=\"bibr8-15330338251397035\" ref-type=\"bibr\"/>&#8211;<xref rid=\"bibr9-15330338251397035\" ref-type=\"bibr\">9</xref></sup> The classic CNN-based segmentation model, U-Net,<sup>\n<xref rid=\"bibr10-15330338251397035\" ref-type=\"bibr\">10</xref>\n</sup> and its variants employ symmetric encoder-decoder networks to automatically extract multi-level features through CNNs, significantly improving segmentation efficiency and accuracy.<sup><xref rid=\"bibr11-15330338251397035\" ref-type=\"bibr\">11</xref><xref rid=\"bibr12-15330338251397035\" ref-type=\"bibr\"/><xref rid=\"bibr13-15330338251397035\" ref-type=\"bibr\"/>&#8211;<xref rid=\"bibr14-15330338251397035\" ref-type=\"bibr\">14</xref></sup> For example, Li et al<sup>\n<xref rid=\"bibr15-15330338251397035\" ref-type=\"bibr\">15</xref>\n</sup> applied the adaptive deep CNN framework nnU-Net to segment the bladder, rectum, and HR-CTV on cervical cancer CT images. The nnU-Net method integrates three architectures&#8212;2D U-Net, 3D U-Net, and 3D cascade U-Net&#8212;to adaptively select the optimal architecture for each task. Zhang et al<sup>\n<xref rid=\"bibr16-15330338251397035\" ref-type=\"bibr\">16</xref>\n</sup> developed a DSD U-Net model for automated delineation of the bladder, rectum, sigmoid colon, small bowel, and HR-CTV, achieving high accuracy as evaluated by the Dice similarity coefficient (DSC). Chang et al<sup>\n<xref rid=\"bibr17-15330338251397035\" ref-type=\"bibr\">17</xref>\n</sup> proposed a hybrid network combining 3D U-Net and long short-term memory (LSTM) for HR-CTV and OAR segmentation, demonstrating superior performance over 2D U-Net.</p><p>Despite CNNs&#8217; strengths in capturing local spatial features, they face limitations in modeling long-range dependencies between pixels.<sup>\n<xref rid=\"bibr18-15330338251397035\" ref-type=\"bibr\">18</xref>\n</sup> Transformer architectures, enhanced by self-attention mechanisms, overcome this limitation by enabling global context modeling, showing promising potential in medical image segmentation tasks.<sup><xref rid=\"bibr19-15330338251397035\" ref-type=\"bibr\">19</xref><xref rid=\"bibr20-15330338251397035\" ref-type=\"bibr\"/><xref rid=\"bibr21-15330338251397035\" ref-type=\"bibr\"/><xref rid=\"bibr22-15330338251397035\" ref-type=\"bibr\"/>&#8211;<xref rid=\"bibr23-15330338251397035\" ref-type=\"bibr\">23</xref></sup> To synergize the advantages of CNNs (local perception) and Transformers (global reasoning), hybrid CNN-Transformer architectures have emerged as a cutting-edge direction in medical image segmentation, exemplified by TransUNet.<sup>\n<xref rid=\"bibr24-15330338251397035\" ref-type=\"bibr\">24</xref>\n</sup> Gu et al<sup>\n<xref rid=\"bibr25-15330338251397035\" ref-type=\"bibr\">25</xref>\n</sup> pioneered the integration of Transformer's self-attention with CNN frameworks for segmenting the bladder, rectum, and colon, demonstrating significant effectiveness. However, their work focused solely on OARs segmentation in cervical cancer brachytherapy and did not extend the CNN-Transformer framework to HR-CTV segmentation.</p><p>Although deep learning-based automatic segmentation methods show promise in cervical cancer brachytherapy, existing studies still exhibit certain limitations. Firstly, most research relies on single-center datasets, struggling to adequately validate model generalizability and robustness in multi-center scenarios, and failing to effectively address challenges such as variations in imaging protocols and inconsistencies in contouring criteria across different centers.<sup>\n<xref rid=\"bibr26-15330338251397035\" ref-type=\"bibr\">26</xref>\n</sup> Secondly, existing CNN-Transformer hybrid architectures primarily focus on the segmentation of Organs at Risk and have not yet been applied to the automatic segmentation of the High-Risk Clinical Target Volume. The HR-CTV is characterized by indistinct boundaries, variable morphology, and close proximity to OARs, making its accurate segmentation crucial for treatment planning.<sup>\n<xref rid=\"bibr27-15330338251397035\" ref-type=\"bibr\">27</xref>\n</sup> Therefore, there is an urgent need to develop a novel segmentation method capable of simultaneously achieving precise segmentation of both HR-CTV and OARs while maintaining stable performance on heterogeneous multi-center data.</p><p>To address these challenges, this study proposes, for the first time, a CNN-Transformer hybrid network named MDA-TransUnet for segmenting both OARs and HR-CTV in cervical cancer brachytherapy. To validate the model's robustness across centers, this study integrated 122 cervical cancer brachytherapy patients&#8217; CT images across three clinical centers for training and testing, providing robust support for evaluating the model's performance in heterogeneous scenarios. This design significantly differs from previous single-center studies and aligns more closely with clinical practical needs. Furthermore, we introduced a Multi-scale Adaptive Spatial Attention Gate (MASAG) and a Deformable Convolutional Attention Module (DCAM) into the CNN-Transformer framework to further enhance adaptability to organ deformation and multi-center variations. Compared to manual contouring, MDA-TransUnet achieved an average segmentation time of approximately 1.2&#8197;min per patient for both HR-CTV and OARs in cervical cancer CT images, representing a 26-fold speed increase. This holds promise for significantly reducing patient waiting time post-applicator insertion in clinical practice, alleviating patient discomfort, and mitigating the risk of organ displacement associated with prolonged waiting periods. MDA-TransUnet thus offers a novel technical solution for cervical cancer brachytherapy image segmentation.</p></sec><sec sec-type=\"materials|methods\" id=\"section2-15330338251397035\"><title>Materials and Methods</title><sec id=\"section2A-15330338251397035\"><title>Dataset</title><p>A total of 122 patients were enrolled in this Institutional Review Board (IRB)-approved retrospective study, including 52 patients from The Third Affiliated Hospital of Nanjing Medical University, 50 patients from The Affiliated Tumor Hospital of Nantong University, and 20 patients from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University. Among these, 102 patients&#8217; data (The Third Affiliated Hospital of Nanjing Medical University and The Affiliated Tumor Hospital of Nantong University) were utilized for training and testing, with 80 cases allocated for training and 22 cases for testing. 20 patients&#8217; data from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University served as an external validation set to further evaluate the model's generalization performance. This study was approved by the Medical Ethics Committee of The Third Affiliated Hospital of Nanjing Medical University (#2024KY213-01), the Medical Ethics Committee of The Affiliated Tumor Hospital of Nantong University (#2020-031), and the Medical Ethics Committee of The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University (#IIT2024101). Informed consent is waived for all participants with the approval of the Medical Ethics Committee. CT images were reconstructed using the Philips Brilliant Big Bore CT scanner (Philips Healthcare, Best, the Netherlands) with a matrix size of 512&#8201;&#215;&#8201;512 and a slice thickness of 3&#8197;mm. All patients were treated using tandem and ovoid applicators (T&#8201;+&#8201;O). The HR-CTV, bladder, colon, rectum, and small bowel were manually contoured by experienced radiation oncologists (with over ten years of clinical expertise) using Monaco 5.40.01 (Elekta, Stockholm, Sweden). The radiation oncologists contoured the HR-CTV on CT images according to ICRU Report 89, with reference to MRI images obtained prior to the first brachytherapy session.<sup>\n<xref rid=\"bibr28-15330338251397035\" ref-type=\"bibr\">28</xref>\n</sup></p></sec><sec id=\"section2B-15330338251397035\"><title>Data Preprocessing</title><p>The 3D-Slicer software and RT structure data were used to generate binary masks for HR-CTV and OARs for each patient. All binary masks were converted into one-hot encoded vectors with values ranging from 0 to 5. To mitigate overfitting, we implement strictly synchronized data augmentation: with 50% probability, applying random 90k&#176; rotation (k&#8712;{0,1,2,3}) followed by axial random flipping (horizontal/vertical); with 25% probability, performing random rotation between &#8722;20&#176; to 20&#176;; and with 25% probability, maintaining the identity transformation.</p></sec><sec id=\"section2C-15330338251397035\"><title>Network Architecture</title><p>In this section, we introduce our proposed MDA-TransUnet. Given the outstanding performance of TransUnet in medical image segmentation, we adopt TransUnet as the baseline model. For the encoder, we retain TransUnet's hybrid CNN-Transformer design: the CNN serves as a feature extractor to generate input feature maps, while the Transformer captures global and spatial relationships between features. The decoder consists of three key components: (1) UpConv blocks for feature upsampling, the Multi-Scale Adaptive Spatial Attention Gate (MASAG) to enhance feature representation, and the Deformable Convolutional Attention Module (DCAM) to robustly refine feature maps, as <xref rid=\"fig1-15330338251397035\" ref-type=\"fig\">Figure 1</xref>.</p><fig position=\"float\" id=\"fig1-15330338251397035\" orientation=\"portrait\"><label>Figure 1.</label><caption><p>MDA-TransUnet Network Architecture Diagram. (a) Overall Framework of MDA-TransUnet, (b) Multi-Scale Adaptive Spatial Attention Gate (MASAG), (c) Multi-Scale Feature Fusion (MSF), (d) Deformable Convolutional Attention Module (DCAM), (e) Spatial Attention (SA), (f) Deformable Convolutional Block (DCB), (g) UpConv.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-fig1.jpg\"/></fig><sec id=\"section2C1-15330338251397035\"><title>Multi-Scale Adaptive Spatial Attention Gate (MASAG)</title><p>We employ the Multi-Scale Adaptive Spatial Attention Gate (MASAG) module to enhance feature representation. MASAG aims to effectively integrate multi-scale information and guide the aggregation of spatial features, thereby improving overall segmentation performance.<sup>\n<xref rid=\"bibr29-15330338251397035\" ref-type=\"bibr\">29</xref>\n</sup> Through a four-stage collaborative process&#8212;Multi-scale Fusion (MSF), Spatial Selection (SS), Spatial Interaction and Cross-Modulation (SICM), and Recalibration (RC)&#8212;MASAG progressively optimizes feature representations, addressing limitations of traditional methods in cross-scale information fusion and spatial weight allocation. The MASAG framework comprises the following stages:</p></sec></sec><sec id=\"section2D-15330338251397035\"><title>Multi-Scale Feature Fusion (MSF)</title><p>The encoder feature maps (X) and decoder feature maps (Y) are fused through local and global context extraction branches:</p><p><bold>Local Context Extraction:</bold> Utilizes Depthwise Separable Convolution and Dilated Convolution to extract local details from encoder features (X), as defined in Equation (<xref rid=\"disp-formula1-15330338251397035\" ref-type=\"disp-formula\">1</xref>):<disp-formula id=\"disp-formula1-15330338251397035\"><label>(1)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq1.jpg\"/><mml:math id=\"mml-disp1\" display=\"block\" overflow=\"scroll\"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula>where DW denotes Depthwise Separable Convolution, composed of depthwise convolution and pointwise convolution. This architecture reduces computational costs and parameter counts while effectively extracting local features. DW-D represents Dilated Depthwise Separable Convolution, which incorporates dilation rates into the convolution kernel. This significantly expands the receptive field without increasing parameters, enabling the capture of spatial dependencies among local features across broader contexts.</p><p><bold>Global Context Extraction:</bold> Applies global average pooling and max pooling to the decoder feature maps (Y) to capture broad contextual information, as Equation (<xref rid=\"disp-formula2-15330338251397035\" ref-type=\"disp-formula\">2</xref>):<disp-formula id=\"disp-formula2-15330338251397035\"><label>(2)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq2.jpg\"/><mml:math id=\"mml-disp2\" display=\"block\" overflow=\"scroll\"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mo stretchy=\"false\">[</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline1\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula> and <inline-formula>\n<mml:math id=\"mml-inline2\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula> denote global average pooling and max pooling, respectively. Global Average Pooling obtains the overall mean response of the feature maps, reflecting the global feature distribution. Global Max Pooling captures the most salient features within the feature maps. Their combined output provides complementary global semantic information that is lacking in the decoder features.</p><p><bold>Feature Fusion:</bold> The local features extracted by the encoder are added to the global features provided by the decoder, generating the fused feature map U that simultaneously incorporates both local details and global information, as shown in Equation (<xref rid=\"disp-formula3-15330338251397035\" ref-type=\"disp-formula\">3</xref>):<disp-formula id=\"disp-formula3-15330338251397035\"><label>(3)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq3.jpg\"/><mml:math id=\"mml-disp3\" display=\"block\" overflow=\"scroll\"><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></disp-formula></p><p><bold>Spatial Selection (SS):</bold> The spatial selection module dynamically assigns spatial weights to highlight critical anatomical regions and suppress noise:</p><p>Generates a two-channel weight map, as Equation (<xref rid=\"disp-formula4-15330338251397035\" ref-type=\"disp-formula\">4</xref>):<disp-formula id=\"disp-formula4-15330338251397035\"><label>(4)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq4.jpg\"/><mml:math id=\"mml-disp4\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mrow><mml:mo>,</mml:mo><mml:mspace width=\".1em\"/></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant=\"normal\">&#8704;</mml:mi><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mo stretchy=\"false\">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline3\" display=\"inline\" overflow=\"scroll\"><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> represents the Softmax activation function, ensuring that the weights sum to 1 at each spatial location. By computing a dedicated weight at each pixel location for both encoder features and decoder features, indicating which feature holds greater importance for that specific position.</p><p>Applies spatial weighting to encoder features (X) and decoder features (Y) separately, as Equation (<xref rid=\"disp-formula5-15330338251397035\" ref-type=\"disp-formula\">5</xref>):<disp-formula id=\"disp-formula5-15330338251397035\"><label>(5)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq5.jpg\"/><mml:math id=\"mml-disp5\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mi>Y</mml:mi><mml:mo>+</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline4\" display=\"inline\" overflow=\"scroll\"><mml:mo>&#8855;</mml:mo></mml:math>\n</inline-formula> denotes element-wise multiplication, <inline-formula>\n<mml:math id=\"mml-inline5\" display=\"inline\" overflow=\"scroll\"><mml:mi>S</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mi>X</mml:mi></mml:math>\n</inline-formula> and <inline-formula>\n<mml:math id=\"mml-inline6\" display=\"inline\" overflow=\"scroll\"><mml:mi>S</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#8855;</mml:mo><mml:mi>Y</mml:mi></mml:math>\n</inline-formula> representing the spatially-weighted versions of encoder features X and decoder features Y, respectively. The residual connection preserves original feature information while mitigating potential information loss during the weighting process.</p></sec><sec id=\"section2E-15330338251397035\"><title>Spatial Interaction and Cross-Modulation (SICM)</title><p>This module facilitates feature interaction across spatial locations and enables cross-channel modulation of multi-scale information:</p><p>Encoder features are modulated by decoder global context, while decoder features are modulated by encoder local details, addressing the misalignment of low-level and high-level features in traditional U-Net skip connections, as Equation (<xref rid=\"disp-formula6-15330338251397035\" ref-type=\"disp-formula\">6</xref>):<disp-formula id=\"disp-formula6-15330338251397035\"><label>(6)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq6.jpg\"/><mml:math id=\"mml-disp6\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8243;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>&#8855;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8243;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>&#8855;</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline7\" display=\"inline\" overflow=\"scroll\"><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> is the Sigmoid activation function. The global contextual information from Y&#8217; augments X&#8217; to yield X'&#8217;, while the detailed information from X&#8217; complements Y&#8217; to produce Y'&#8217;. This bidirectional enhancement ensures both X'&#8217; and Y'&#8217; incorporate integrated local and global information.</p><p>Final fused features are integrated via element-wise multiplication, as Equation (<xref rid=\"disp-formula7-15330338251397035\" ref-type=\"disp-formula\">7</xref>):<disp-formula id=\"disp-formula7-15330338251397035\"><label>(7)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq7.jpg\"/><mml:math id=\"mml-disp7\" display=\"block\" overflow=\"scroll\"><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8243;</mml:mi></mml:mrow></mml:msup><mml:mo>&#8855;</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8243;</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></disp-formula></p><p>The mutually modulated X'&#8217; and Y'&#8217; are fused via element-wise multiplication to generate the integrated feature U&#8217;. This multiplicative fusion more effectively highlights regions deemed significant by both feature maps.</p></sec><sec id=\"section2F-15330338251397035\"><title>Recalibration (RC)</title><p>Finally, the recalibration module adjusts feature map responses to emphasize meaningful information, as Equation (<xref rid=\"disp-formula8-15330338251397035\" ref-type=\"disp-formula\">8</xref>):<disp-formula id=\"disp-formula8-15330338251397035\"><label>(8)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq8.jpg\"/><mml:math id=\"mml-disp8\" display=\"block\" overflow=\"scroll\"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi mathvariant=\"normal\">&#8242;</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8855;</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula></p><p>This step refines the fused feature map through pointwise convolution, which is then activated by a sigmoid function to generate the attention map. Finally, the initial input X is recalibrated by performing element-wise multiplication with the attention map, followed by further processing through another pointwise convolution. The recalibrated feature map <inline-formula>\n<mml:math id=\"mml-inline8\" display=\"inline\" overflow=\"scroll\"><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> serves as the skip connection output for decoder feature fusion.</p><sec id=\"section2F1-15330338251397035\"><title>Deformable Convolutional Attention Module (DCAM)</title><p>We employ the Deformable Convolutional Attention Module (DCAM) to enhance the network's adaptability to irregular shapes, size variations, and geometric deformations in medical imaging, challenges particularly prominent in cervical cancer brachytherapy segmentation tasks. By integrating the context-aware feature refinement of the Spatial Attention (SA) mechanism and the dynamic geometric adaptation of the Deformable Convolution Block (DCB), DCAM overcomes the limitations of conventional convolutional operations in modeling anatomical diversity. The DCAM module combines SA and DCB, as defined in Equation (<xref rid=\"disp-formula9-15330338251397035\" ref-type=\"disp-formula\">9</xref>):<disp-formula id=\"disp-formula9-15330338251397035\"><label>(9)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq9.jpg\"/><mml:math id=\"mml-disp9\" display=\"block\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula>where <italic toggle=\"yes\">x</italic> is the input tensor. <inline-formula>\n<mml:math id=\"mml-inline9\" display=\"inline\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> denotes the Deformable Convolutional Attention Module. The DCAM module first applies a Spatial Attention mechanism to the input features, focusing on critical spatial regions. The output of SA is then processed by a Deformable Convolution Block, which leverages the dynamic sampling capability of deformable convolutions to adapt to organ geometric deformations.</p></sec></sec><sec id=\"section2G-15330338251397035\"><title>Spatial Attention (SA)</title><p>The Spatial Attention mechanism identifies and amplifies critical spatial regions in feature maps, emphasizing high-signal areas (eg, HR-CTV boundaries) while suppressing low-contrast or irrelevant regions and noise interference, as formulated in Equation (<xref rid=\"disp-formula10-15330338251397035\" ref-type=\"disp-formula\">10</xref>):<disp-formula id=\"disp-formula10-15330338251397035\"><label>(10)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq10.jpg\"/><mml:math id=\"mml-disp10\" display=\"block\" overflow=\"scroll\"><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mo stretchy=\"false\">[</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">]</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>&#8855;</mml:mo><mml:mi>x</mml:mi></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline10\" display=\"inline\" overflow=\"scroll\"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> denotes a 7&#8201;&#215;&#8201;7 convolutional layer with padding of 3. The large 7&#8201;&#215;&#8201;7 kernel effectively captures broader contextual information and enhances spatial perception capabilities. This process first applies both average pooling and max pooling along the channel dimension to the input feature x, yielding two distinct feature maps. These feature maps are concatenated and processed through the large-kernel <inline-formula>\n<mml:math id=\"mml-inline11\" display=\"inline\" overflow=\"scroll\"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> to generate a Spatial Attention Map. Finally, a sigmoid activation function normalizes the attention weights to the [0,1] range. The normalized map is then multiplied element-wise with the original input x to achieve spatial re-weighting: regions with weights approaching 1 are enhanced while those approaching 0 are suppressed.</p></sec><sec id=\"section2H-15330338251397035\"><title>Deformable Convolution Block (DCB)</title><p>We introduce deformable convolution to further refine features generated by SA and enhance the network's adaptability to geometric deformations&#8212;crucial for addressing anatomical heterogeneity in multi-center datasets. Unlike the fixed grid sampling in traditional convolution, deformable convolution dynamically adjusts kernel sampling positions by learning offset parameters, enabling the kernel to adaptively &#8220;warp&#8221; and align with irregular anatomical contours (eg, HR-CTV or colon boundaries), as detailed in Equation (<xref rid=\"disp-formula11-15330338251397035\" ref-type=\"disp-formula\">11</xref>):<disp-formula id=\"disp-formula11-15330338251397035\"><label>(11)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq11.jpg\"/><mml:math id=\"mml-disp11\" display=\"block\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula>where <inline-formula>\n<mml:math id=\"mml-inline12\" display=\"inline\" overflow=\"scroll\"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> is the ReLU activation layer, <inline-formula>\n<mml:math id=\"mml-inline13\" display=\"inline\" overflow=\"scroll\"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> denotes batch normalization, and <inline-formula>\n<mml:math id=\"mml-inline14\" display=\"inline\" overflow=\"scroll\"><mml:mi>D</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> represents deformable convolution.</p><sec id=\"section2H1-15330338251397035\"><title>Upsampling (UpConv)</title><p>The UpConv layer progressively upsamples the features of the current layer to align the dimensions with the subsequent skip connection. Each UpConv layer consists of an UpSampling <inline-formula>\n<mml:math id=\"mml-inline15\" display=\"inline\" overflow=\"scroll\"><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula> with scale-factor 2, a 3&#8201;&#215;&#8201;3 convolution <inline-formula>\n<mml:math id=\"mml-inline16\" display=\"inline\" overflow=\"scroll\"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula><italic toggle=\"yes\">,</italic> a batch normalization <inline-formula>\n<mml:math id=\"mml-inline17\" display=\"inline\" overflow=\"scroll\"><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>, and a ReLU activation layers, as formulated in Equation (<xref rid=\"disp-formula12-15330338251397035\" ref-type=\"disp-formula\">12</xref>):<disp-formula id=\"disp-formula12-15330338251397035\"><label>(12)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq12.jpg\"/><mml:math id=\"mml-disp12\" display=\"block\" overflow=\"scroll\"><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:math></alternatives></disp-formula></p></sec></sec><sec id=\"section2I-15330338251397035\"><title>Loss Function</title><p>Our method employs a multi-scale supervision strategy and a hybrid loss function to optimize model performance and mitigate class imbalance. The model generates predictions at four decoder levels (p1, p2, p3, p4), with each level contributing to the loss calculation. This approach reduces reliance on single-scale features by integrating multi-scale contextual information. The loss at each level is a weighted combination of Cross-Entropy Loss and Dice Loss, as defined in Equation (<xref rid=\"disp-formula13-15330338251397035\" ref-type=\"disp-formula\">13</xref>):<disp-formula id=\"disp-formula13-15330338251397035\"><label>(13)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq13.jpg\"/><mml:math id=\"mml-disp13\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.7</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>where Cross-Entropy Loss(<inline-formula>\n<mml:math id=\"mml-inline18\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>) and Dice Loss(<inline-formula>\n<mml:math id=\"mml-inline19\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math>\n</inline-formula>) are formulated as follows:<disp-formula id=\"disp-formula14-15330338251397035\"><label>(14)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq14.jpg\"/><mml:math id=\"mml-disp14\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits=\"false\">&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mspace width=\"0.2em\"/><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mspace width=\"0.2em\"/><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mspace width=\".1em\"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id=\"disp-formula15-15330338251397035\"><label>(15)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq15.jpg\"/><mml:math id=\"mml-disp15\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mstyle displaystyle=\"true\" scriptlevel=\"0\"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mo movablelimits=\"false\">&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width=\"0.2em\"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo movablelimits=\"false\">&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width=\"0.2em\"/><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo movablelimits=\"false\">&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mspace width=\"0.2em\"/><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p><p>Here, <inline-formula>\n<mml:math id=\"mml-inline20\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>\n</inline-formula> denotes the ground truth label, <inline-formula>\n<mml:math id=\"mml-inline21\" display=\"inline\" overflow=\"scroll\"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>\n</inline-formula> represents the predicted probability, <italic toggle=\"yes\">N</italic> is the number of classes, and <inline-formula>\n<mml:math id=\"mml-inline22\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#1013;</mml:mi></mml:math>\n</inline-formula> is a smoothing factor to prevent division by zero.</p><p>The final loss is the weighted sum of losses from all four decoder levels, with equal weights assigned <inline-formula>\n<mml:math id=\"mml-inline23\" display=\"inline\" overflow=\"scroll\"><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>&#945;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#946;</mml:mi><mml:mo>=</mml:mo></mml:math>\n</inline-formula>\n<inline-formula>\n<mml:math id=\"mml-inline24\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#947;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#948;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy=\"false\">)</mml:mo></mml:math>\n</inline-formula>, as shown in Equation (<xref rid=\"disp-formula16-15330338251397035\" ref-type=\"disp-formula\">16</xref>):<disp-formula id=\"disp-formula16-15330338251397035\"><label>(16)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq16.jpg\"/><mml:math id=\"mml-disp16\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#945;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#947;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#948;</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"script\">L</mml:mi></mml:mrow><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p></sec><sec id=\"section2J-15330338251397035\"><title>Evaluation Metrics</title><p>Both geometric and dosimetric methods were employed for quantitative analysis. Geometric performance was assessed using the Dice similarity coefficient (DSC), defined as:<disp-formula id=\"disp-formula17-15330338251397035\"><label>(17)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq17.jpg\"/><mml:math id=\"mml-disp17\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle=\"true\" scriptlevel=\"0\"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8745;</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo></mml:mrow><mml:mrow><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo><mml:mi>X</mml:mi><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo><mml:mo>+</mml:mo><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo><mml:mi>Y</mml:mi><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id=\"disp-formula18-15330338251397035\"><label>(18)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq18.jpg\"/><mml:math id=\"mml-disp18\" display=\"block\" overflow=\"scroll\"><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:mi>H</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo form=\"prefix\" movablelimits=\"true\">max</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mspace width=\"0.25em\"/><mml:mi>h</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo form=\"prefix\">max</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mspace width=\"0.2em\"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mo form=\"prefix\">min</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mspace width=\"0.2em\"/><mml:mi>a</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula><disp-formula id=\"disp-formula19-15330338251397035\"><label>(19)</label><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-eq19.jpg\"/><mml:math id=\"mml-disp19\" display=\"block\" overflow=\"scroll\"><mml:mtable columnalign=\"right left\" columnspacing=\"thickmathspace\" displaystyle=\"true\" rowspacing=\".5em\"><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mo fence=\"false\" stretchy=\"false\">{</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence=\"false\" stretchy=\"false\">}</mml:mo></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable columnspacing=\"1em\" rowspacing=\"4pt\"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence=\"false\" stretchy=\"false\">{</mml:mo><mml:mrow><mml:mi mathvariant=\"normal\">&#8704;</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mspace width=\".1em\"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mrow><mml:mspace width=\".1em\"/><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy=\"false\">)</mml:mo><mml:mo fence=\"false\" stretchy=\"false\">|</mml:mo><mml:mi mathvariant=\"normal\">&#8707;</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence=\"false\" stretchy=\"false\">}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p><p>Here, smaller HD95 (95% Hausdorff Distance) and ASD (Average Surface Distance) indicate better shape agreement between segmentation results and ground truth contours, while a larger DSC (Dice Similarity Coefficient) reflects higher spatial overlap with the ground truth.</p><p>For dosimetric comparisons, dose-volume indices (DVIs) were utilized. For HR-CTV, we focused on Dmean (mean dose to the target) and D90% (minimum dose covering 90% of the target volume). For OARs, we evaluated D5cc, D2cc, and D0.1cc, where DXcc denotes the minimum dose to the hottest X cubic centimeters (cc) of the organ. Paired sample t-tests were performed to compare dosimetric differences, with p&#8201;&lt;&#8201;0.05p&#8201;&lt;&#8201;0.05 indicating statistical significance. All statistical analyses were conducted using Python 3.8.</p></sec></sec><sec id=\"section3-15330338251397035\"><title>Experiments and Results</title><sec id=\"section3A-15330338251397035\"><title>Experimental Details</title><p>MDA-TransUnet was implemented in PyCharm on a computer equipped with an Intel<sup>&#174;</sup> Core&#8482; i7-10700 CPU and an NVIDIA GeForce RTX 3090 GPU. For fair comparison with other methods, all networks were trained under identical configurations. The model was optimized using the AdamW optimizer, which is better suited for complex multi-scale feature learning and improves convergence stability. The learning rate was adjusted via a cosine decay schedule, starting with an initial value of 0.0001 and a minimum of 1e-7. The batch size was set to 24, and training proceeded for 150 epochs.</p></sec><sec id=\"section3B-15330338251397035\"><title>Geometric Metric Analysis</title><p>As shown in <xref rid=\"table1-15330338251397035\" ref-type=\"table\">Table 1</xref>, our proposed MDA-TransUnet achieves the highest Dice Similarity Coefficient (DSC) across all five target regions compared to other methods. The mean DSCs for the bladder, colon, rectum, small bowel, and HR-CTV are 94.54%, 79.27%, 79.27%, 88.90%, and 82.35%, respectively. Notably, for the small bowel (88.90%) and HR-CTV (82.35%), our method significantly outperforms suboptimal approaches (Trans-CASCADE: 87.55% for small bowel; EMCAD: 81.49% for HR-CTV). Regarding boundary precision, our method achieves the lowest average HD95 (95% Hausdorff Distance) among all OARs. For the bladder, small bowel, and HR-CTV, HD95 values are significantly reduced compared to suboptimal methods, demonstrating enhanced boundary control. In terms of Average Surface Distance (ASD), MDA-TransUnet delivers optimal ASD values for all OARs except the colon, with the bladder and HR-CTV showing an 8.5% reduction in ASD versus the second-best method. Compared to the classic TransUnet, our method achieves DSC improvements of 5.95% (bladder), 14.7% (colon), 9.05% (rectum), 11.67% (small bowel), and 4.93% (HR-CTV). These results validate that the Multi-Scale Adaptive Spatial Attention Gate (MASAG) and Deformable Convolutional Attention Module (DCAM) effectively mitigate limitations of skip connections while enhancing the network's ability to model local details and geometric deformations. Furthermore, <xref rid=\"fig2-15330338251397035\" ref-type=\"fig\">Figures 2</xref> and <xref rid=\"fig3-15330338251397035\" ref-type=\"fig\">3</xref> visually compare segmentation results, confirming that contours generated by our method exhibit superior agreement with radiation physicist-delineated ground truth in shape, volume, and spatial localization.</p><fig position=\"float\" id=\"fig2-15330338251397035\" orientation=\"portrait\"><label>Figure 2.</label><caption><p>Visual Comparison of our Method with Other Methods, Where red Indicates the Ground Truth and Green Represents AI-Based Segmentation Results. Subfigures Correspond to: (a) TransUnet, (b) DLKA-net, (c) SelfRag-Unet, (d) Unet, (e) Trans-CASCADE, (f) EMCAD, and (g) our Method.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-fig2.jpg\"/></fig><fig position=\"float\" id=\"fig3-15330338251397035\" orientation=\"portrait\"><label>Figure 3.</label><caption><p>Visual Comparison of our Method Versus Other Approaches for HR-CTV Segmentation in Sagittal and Coronal Planes. Blue Contours Denote Ground Truth, While red Contours Represent AI-Generated Segmentations. Left Panels: Coronal Views; Right Panels: Sagittal Views. (a) TransUnet, (b) DLKA-Net, (c) SelfRag-UNet, (d) UNet, (e) Trans-CASCADE, (f) EMCAD, and (g) our Method.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-fig3.jpg\"/></fig><table-wrap position=\"float\" id=\"table1-15330338251397035\" orientation=\"portrait\"><label>Table 1.</label><caption><p>Quantitative Comparison Between our Proposed Method and Other Methods, Where &#8220;our Study&#8221; Denotes our Approach.</p></caption><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-table1.jpg\"/><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">Metrics</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">TransUnet <sup>\n<xref rid=\"bibr24-15330338251397035\" ref-type=\"bibr\">24</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">DLKA-net<sup>\n<xref rid=\"bibr30-15330338251397035\" ref-type=\"bibr\">30</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">SelfRag-Unet<sup>\n<xref rid=\"bibr31-15330338251397035\" ref-type=\"bibr\">31</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Unet<sup>\n<xref rid=\"bibr10-15330338251397035\" ref-type=\"bibr\">10</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Cascade <sup>\n<xref rid=\"bibr32-15330338251397035\" ref-type=\"bibr\">32</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">EMCAD <sup>\n<xref rid=\"bibr33-15330338251397035\" ref-type=\"bibr\">33</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Our Study</th></tr></thead><tbody><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Bladder</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">88.59&#8201;&#177;&#8201;2.79</td><td rowspan=\"1\" colspan=\"1\">92.50&#8201;&#177;&#8201;3.22</td><td rowspan=\"1\" colspan=\"1\">91.93&#8201;&#177;&#8201;2.56</td><td rowspan=\"1\" colspan=\"1\">91.98&#8201;&#177;&#8201;2.37</td><td rowspan=\"1\" colspan=\"1\">94.11&#8201;&#177;&#8201;1.97</td><td rowspan=\"1\" colspan=\"1\">94.20&#8201;&#177;&#8201;1.60</td><td rowspan=\"1\" colspan=\"1\"><bold>94.54</bold>&#8201;&#177;&#8201;<bold>1.54</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">3.38&#8201;&#177;&#8201;1.03</td><td rowspan=\"1\" colspan=\"1\">4.08&#8201;&#177;&#8201;4.85</td><td rowspan=\"1\" colspan=\"1\">2.56&#8201;&#177;&#8201;0.83</td><td rowspan=\"1\" colspan=\"1\">2.49&#8201;&#177;&#8201;0.79</td><td rowspan=\"1\" colspan=\"1\">1.90&#8201;&#177;&#8201;0.71</td><td rowspan=\"1\" colspan=\"1\">1.99&#8201;&#177;&#8201;0.69</td><td rowspan=\"1\" colspan=\"1\"><bold>1.80</bold>&#8201;&#177;&#8201;<bold>0.87</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.23&#8201;&#177;&#8201;0.40</td><td rowspan=\"1\" colspan=\"1\">1.38&#8201;&#177;&#8201;0.99</td><td rowspan=\"1\" colspan=\"1\">1.03&#8201;&#177;&#8201;0.47</td><td rowspan=\"1\" colspan=\"1\">0.91&#8201;&#177;&#8201;0.30</td><td rowspan=\"1\" colspan=\"1\">0.61&#8201;&#177;&#8201;0.27</td><td rowspan=\"1\" colspan=\"1\">0.59&#8201;&#177;&#8201;0.19</td><td rowspan=\"1\" colspan=\"1\"><bold>0.54</bold>&#8201;&#177;&#8201;<bold>0.21</bold></td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Colon</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">64.57&#8201;&#177;&#8201;7.73</td><td rowspan=\"1\" colspan=\"1\">70.25&#8201;&#177;&#8201;11.11</td><td rowspan=\"1\" colspan=\"1\">67.70&#8201;&#177;&#8201;10.01</td><td rowspan=\"1\" colspan=\"1\">68.46&#8201;&#177;&#8201;10.41</td><td rowspan=\"1\" colspan=\"1\">77.77&#8201;&#177;&#8201;8.00</td><td rowspan=\"1\" colspan=\"1\">78.46&#8201;&#177;&#8201;8.13</td><td rowspan=\"1\" colspan=\"1\"><bold>79.27</bold>&#8201;&#177;&#8201;<bold>8.05</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">19.08&#8201;&#177;&#8201;15.18</td><td rowspan=\"1\" colspan=\"1\">14.49&#8201;&#177;&#8201;16.83</td><td rowspan=\"1\" colspan=\"1\">15.92&#8201;&#177;&#8201;13.25</td><td rowspan=\"1\" colspan=\"1\">17.57&#8201;&#177;&#8201;16.83</td><td rowspan=\"1\" colspan=\"1\">11.86&#8201;&#177;&#8201;16.37</td><td rowspan=\"1\" colspan=\"1\"><bold>11.28</bold>&#8201;&#177;&#8201;<bold>14.80</bold></td><td rowspan=\"1\" colspan=\"1\">11.66&#8201;&#177;&#8201;14.50</td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">4.99&#8201;&#177;&#8201;3.35</td><td rowspan=\"1\" colspan=\"1\">4.00&#8201;&#177;&#8201;4.59</td><td rowspan=\"1\" colspan=\"1\">4.38&#8201;&#177;&#8201;3.75</td><td rowspan=\"1\" colspan=\"1\">4.86&#8201;&#177;&#8201;4.51</td><td rowspan=\"1\" colspan=\"1\">2.99&#8201;&#177;&#8201;4.61</td><td rowspan=\"1\" colspan=\"1\"><bold>2.45</bold>&#8201;&#177;&#8201;<bold>2.01</bold></td><td rowspan=\"1\" colspan=\"1\">2.74&#8201;&#177;&#8201;4.09</td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Rectum</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">70.22&#8201;&#177;&#8201;11.24</td><td rowspan=\"1\" colspan=\"1\">73.57&#8201;&#177;&#8201;7.00</td><td rowspan=\"1\" colspan=\"1\">74.76&#8201;&#177;&#8201;7.57</td><td rowspan=\"1\" colspan=\"1\">75.92&#8201;&#177;&#8201;7.41</td><td rowspan=\"1\" colspan=\"1\">78.80&#8201;&#177;&#8201;6.32</td><td rowspan=\"1\" colspan=\"1\">78.41&#8201;&#177;&#8201;5.96</td><td rowspan=\"1\" colspan=\"1\"><bold>79.27</bold>&#8201;&#177;&#8201;<bold>6.79</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">6.83&#8201;&#177;&#8201;3.03</td><td rowspan=\"1\" colspan=\"1\">6.86&#8201;&#177;&#8201;3.72</td><td rowspan=\"1\" colspan=\"1\">6.39&#8201;&#177;&#8201;3.07</td><td rowspan=\"1\" colspan=\"1\"><bold>5.70</bold>&#8201;&#177;&#8201;<bold>2.67</bold></td><td rowspan=\"1\" colspan=\"1\">5.79&#8201;&#177;&#8201;3.62</td><td rowspan=\"1\" colspan=\"1\">6.02&#8201;&#177;&#8201;4.12</td><td rowspan=\"1\" colspan=\"1\">6.28&#8201;&#177;&#8201;4.24</td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.93&#8201;&#177;&#8201;0.64</td><td rowspan=\"1\" colspan=\"1\">2.41&#8201;&#177;&#8201;1.37</td><td rowspan=\"1\" colspan=\"1\">1.91&#8201;&#177;&#8201;1.10</td><td rowspan=\"1\" colspan=\"1\">1.60&#8201;&#177;&#8201;0.74</td><td rowspan=\"1\" colspan=\"1\">1.45&#8201;&#177;&#8201;0.86</td><td rowspan=\"1\" colspan=\"1\">1.56&#8201;&#177;&#8201;0.75</td><td rowspan=\"1\" colspan=\"1\"><bold>1.35</bold>&#8201;&#177;&#8201;<bold>0.77</bold></td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Small intestine</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">77.23&#8201;&#177;&#8201;5.09</td><td rowspan=\"1\" colspan=\"1\">83.46&#8201;&#177;&#8201;4.93</td><td rowspan=\"1\" colspan=\"1\">80.64&#8201;&#177;&#8201;6.15</td><td rowspan=\"1\" colspan=\"1\">81.13&#8201;&#177;&#8201;5.98</td><td rowspan=\"1\" colspan=\"1\">87.55&#8201;&#177;&#8201;4.20</td><td rowspan=\"1\" colspan=\"1\">87.17&#8201;&#177;&#8201;3.86</td><td rowspan=\"1\" colspan=\"1\"><bold>88.90</bold>&#8201;&#177;&#8201;<bold>3.64</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">5.76&#8201;&#177;&#8201;1.73</td><td rowspan=\"1\" colspan=\"1\">4.34&#8201;&#177;&#8201;1.51</td><td rowspan=\"1\" colspan=\"1\">6.00&#8201;&#177;&#8201;3.27</td><td rowspan=\"1\" colspan=\"1\">5.58&#8201;&#177;&#8201;2.74</td><td rowspan=\"1\" colspan=\"1\">3.16&#8201;&#177;&#8201;1.52</td><td rowspan=\"1\" colspan=\"1\">3.40&#8201;&#177;&#8201;2.13</td><td rowspan=\"1\" colspan=\"1\"><bold>2.97</bold>&#8201;&#177;&#8201;<bold>1.44</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.70&#8201;&#177;&#8201;0.46</td><td rowspan=\"1\" colspan=\"1\">1.49&#8201;&#177;&#8201;0.52</td><td rowspan=\"1\" colspan=\"1\">1.89&#8201;&#177;&#8201;0.89</td><td rowspan=\"1\" colspan=\"1\">1.72&#8201;&#177;&#8201;0.79</td><td rowspan=\"1\" colspan=\"1\">0.92&#8201;&#177;&#8201;0.38</td><td rowspan=\"1\" colspan=\"1\">0.96&#8201;&#177;&#8201;0.41</td><td rowspan=\"1\" colspan=\"1\"><bold>0.87</bold>&#8201;&#177;&#8201;<bold>0.48</bold></td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>HR-CTV</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">77.42&#8201;&#177;&#8201;5.18</td><td rowspan=\"1\" colspan=\"1\">77.71&#8201;&#177;&#8201;4.76</td><td rowspan=\"1\" colspan=\"1\">79.69&#8201;&#177;&#8201;4.32</td><td rowspan=\"1\" colspan=\"1\">79.15&#8201;&#177;&#8201;4.47</td><td rowspan=\"1\" colspan=\"1\">80.72&#8201;&#177;&#8201;4.02</td><td rowspan=\"1\" colspan=\"1\">81.49&#8201;&#177;&#8201;4.34</td><td rowspan=\"1\" colspan=\"1\"><bold>82.35</bold>&#8201;&#177;&#8201;<bold>4.07</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">5.06&#8201;&#177;&#8201;1.53</td><td rowspan=\"1\" colspan=\"1\">4.60&#8201;&#177;&#8201;1.60</td><td rowspan=\"1\" colspan=\"1\">4.43&#8201;&#177;&#8201;1.84</td><td rowspan=\"1\" colspan=\"1\">4.53&#8201;&#177;&#8201;1.89</td><td rowspan=\"1\" colspan=\"1\">4.05&#8201;&#177;&#8201;1.56</td><td rowspan=\"1\" colspan=\"1\">3.99&#8201;&#177;&#8201;1.70</td><td rowspan=\"1\" colspan=\"1\"><bold>3.77</bold>&#8201;&#177;&#8201;<bold>1.58</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.94&#8201;&#177;&#8201;0.64</td><td rowspan=\"1\" colspan=\"1\">1.82&#8201;&#177;&#8201;0.54</td><td rowspan=\"1\" colspan=\"1\">1.57&#8201;&#177;&#8201;0.44</td><td rowspan=\"1\" colspan=\"1\">1.64&#8201;&#177;&#8201;0.50</td><td rowspan=\"1\" colspan=\"1\">1.52&#8201;&#177;&#8201;0.52</td><td rowspan=\"1\" colspan=\"1\">1.41&#8201;&#177;&#8201;0.56</td><td rowspan=\"1\" colspan=\"1\"><bold>1.29</bold>&#8201;&#177;&#8201;<bold>0.40</bold></td></tr></tbody></table></alternatives></table-wrap></sec><sec id=\"section3C-15330338251397035\"><title>Ablation Study</title><p>Ablation studies were conducted on the dataset to evaluate the effectiveness of different components in our proposed network. We incrementally removed key modules (MASAG and DCAM) and compared the results. As clearly demonstrated in <xref rid=\"table2-15330338251397035\" ref-type=\"table\">Table 2</xref>, incorporating both MASAG and DCAM modules significantly enhances performance, particularly for HR-CTV segmentation, mitigating performance degradation caused by inter-center variations in contouring protocols and imaging parameters. The MASAG module improves feature representation quality and enhances cross-scale information fusion accuracy, thereby providing more reliable and focused feature inputs to DCAM. The deformable convolution operations in DCAM enable precise geometric adaptation at critical regions emphasized by MASAG (eg, organ boundaries). The resulting deformation-adapted features subsequently deliver more anatomically realistic information to MASAG during later decoding stages. This cascaded processing creates powerful synergies, proving particularly effective when segmenting challenging structures such as morphologically variable colons and boundary-ambiguous HR-CTV. The introduction of MASAG and DCAM collectively strengthens the model's capacity to capture anatomical details, accommodate multi-center heterogeneity, and handle geometric variations, achieving superior performance across all segmentation regions.</p><table-wrap position=\"float\" id=\"table2-15330338251397035\" orientation=\"portrait\"><label>Table 2.</label><caption><p>Ablation Study.</p></caption><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-table2.jpg\"/><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" colspan=\"2\" rowspan=\"1\">Components</th><th align=\"left\" colspan=\"5\" rowspan=\"1\">Segmentation Targets</th></tr><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">MASAG</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">DCAM</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Bladder</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Colon</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Rectum</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Small \nIntestine</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">HR-CTV</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">NO</td><td rowspan=\"1\" colspan=\"1\">NO</td><td rowspan=\"1\" colspan=\"1\">93.37</td><td rowspan=\"1\" colspan=\"1\">76.84</td><td rowspan=\"1\" colspan=\"1\">77.97</td><td rowspan=\"1\" colspan=\"1\">86.38</td><td rowspan=\"1\" colspan=\"1\">79.85</td></tr><tr><td rowspan=\"1\" colspan=\"1\">YES</td><td rowspan=\"1\" colspan=\"1\">NO</td><td rowspan=\"1\" colspan=\"1\">93.96</td><td rowspan=\"1\" colspan=\"1\">77.39</td><td rowspan=\"1\" colspan=\"1\">78.25</td><td rowspan=\"1\" colspan=\"1\">87.10</td><td rowspan=\"1\" colspan=\"1\">80.94</td></tr><tr><td rowspan=\"1\" colspan=\"1\">NO</td><td rowspan=\"1\" colspan=\"1\">YES</td><td rowspan=\"1\" colspan=\"1\">94.39</td><td rowspan=\"1\" colspan=\"1\">78.54</td><td rowspan=\"1\" colspan=\"1\">78.47</td><td rowspan=\"1\" colspan=\"1\">87.90</td><td rowspan=\"1\" colspan=\"1\">80.92</td></tr><tr><td rowspan=\"1\" colspan=\"1\">YES</td><td rowspan=\"1\" colspan=\"1\">YES</td><td rowspan=\"1\" colspan=\"1\">\n<bold>94</bold>\n<bold>.</bold>\n<bold>54</bold>\n</td><td rowspan=\"1\" colspan=\"1\"><bold>79</bold>.<bold>27</bold></td><td rowspan=\"1\" colspan=\"1\"><bold>79</bold>.<bold>27</bold></td><td rowspan=\"1\" colspan=\"1\"><bold>88</bold>.<bold>90</bold></td><td rowspan=\"1\" colspan=\"1\"><bold>82</bold>.<bold>35</bold></td></tr></tbody></table></alternatives></table-wrap></sec><sec id=\"section3D-15330338251397035\"><title>Dosimetric Analysis</title><p>To evaluate dosimetric accuracy, we compared dose-volume metrics between manual and deep learning-based methods using paired sample t-tests, as summarized in <xref rid=\"table3-15330338251397035\" ref-type=\"table\">Table 3</xref>. No statistically significant differences were observed across all dosimetric parameters. For OARs, the average differences in D2cc and D0.1cc were less than 12% and 15%, respectively. For HR-CTV, the average differences in Dmean and D90% were below 8% and 11%, respectively. <xref rid=\"fig4-15330338251397035\" ref-type=\"fig\">Figure 4</xref> illustrates dose distributions for a representative case, comparing our automated segmentation method with manual delineation.</p><fig position=\"float\" id=\"fig4-15330338251397035\" orientation=\"portrait\"><label>Figure 4.</label><caption><p>Delineation of HR-CTV and OARs on CT Slices and Corresponding Dose Distribution Results. Blue Regions Represent the Standard Manual Contours; Green Lines Denote the Contours Segmented by our Method, and Colored Lines Indicate Dose Distributions.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-fig4.jpg\"/></fig><table-wrap position=\"float\" id=\"table3-15330338251397035\" orientation=\"portrait\"><label>Table 3.</label><caption><p>Differences in Dosimetric Parameters Between Manual Methods and our Method Within the Original Clinical Brachytherapy (BT) Plans, Along with Paired t-Test Results.</p></caption><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-table3.jpg\"/><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"left\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">Structure</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Dosimetric Parameters</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Differences</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">P</th><th rowspan=\"1\" colspan=\"1\"/></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">Bladder</td><td rowspan=\"1\" colspan=\"1\">D<sub>5cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.1951&#8201;&#177;&#8201;0.1607</td><td rowspan=\"1\" colspan=\"1\">0.388</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>2cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.2826&#8201;&#177;&#8201;0.2277</td><td rowspan=\"1\" colspan=\"1\">0.435</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>0.1cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.6186&#8201;&#177;&#8201;0.5103</td><td rowspan=\"1\" colspan=\"1\">0.505</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\">Colon</td><td rowspan=\"1\" colspan=\"1\">D<sub>2cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.3173&#8201;&#177;&#8201;0.4511</td><td rowspan=\"1\" colspan=\"1\">0.614</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>0.1cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.4752&#8201;&#177;&#8201;0.6740</td><td rowspan=\"1\" colspan=\"1\">0.507</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\">Rectum</td><td rowspan=\"1\" colspan=\"1\">D<sub>5cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.0975&#8201;&#177;&#8201;0.0756</td><td rowspan=\"1\" colspan=\"1\">0.962</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>2cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.1160&#8201;&#177;&#8201;0.0695</td><td rowspan=\"1\" colspan=\"1\">0.158</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>0.1cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.3238&#8201;&#177;&#8201;0.2612</td><td rowspan=\"1\" colspan=\"1\">0.234</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\">Small intestine</td><td rowspan=\"1\" colspan=\"1\">D<sub>2cc</sub></td><td rowspan=\"1\" colspan=\"1\">0.5215&#8201;&#177;&#8201;0.6269</td><td rowspan=\"1\" colspan=\"1\">0.704</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\"/><td rowspan=\"1\" colspan=\"1\">D<sub>0.1cc</sub></td><td rowspan=\"1\" colspan=\"1\">1.1644&#8201;&#177;&#8201;1.4556</td><td rowspan=\"1\" colspan=\"1\">0.571</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\">HR-CTV</td><td rowspan=\"1\" colspan=\"1\">D<sub>90%</sub></td><td rowspan=\"1\" colspan=\"1\">0.6367&#8201;&#177;&#8201;0.5573</td><td rowspan=\"1\" colspan=\"1\">0.081</td><td rowspan=\"1\" colspan=\"1\"/></tr><tr><td rowspan=\"1\" colspan=\"1\">&#12288;</td><td rowspan=\"1\" colspan=\"1\">D<sub>mean</sub></td><td rowspan=\"1\" colspan=\"1\">0.9659&#8201;&#177;&#8201;0.8036</td><td rowspan=\"1\" colspan=\"1\">0.470</td><td rowspan=\"1\" colspan=\"1\"/></tr></tbody></table></alternatives></table-wrap></sec><sec id=\"section3E-15330338251397035\"><title>External Validation</title><p>To evaluate the model's generalization capability on unseen data from medical institutions, this study additionally incorporated 20 cervical cancer brachytherapy patients&#8217; CT images from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University for independent testing (exclusively for external validation, not involved in training). As demonstrated in <xref rid=\"table4-15330338251397035\" ref-type=\"table\">Table 4</xref>, MDA-TransUnet maintained optimal performance in the external validation set, achieving the highest DSC and HD95 values across all five target regions. Significant improvements were observed particularly for more challenging OARs (colon, rectum, and small bowel) compared to suboptimal methods. Although yielding second-best ASD values for the bladder and colon, it achieved the optimal average ASD across all target regions. These results demonstrate that the synergistic interaction between MASAG and DCAM effectively mitigates impacts caused by inter-center scanner variations and annotation discrepancies, confirming MDA-TransUnet's superior generalization capability.</p><table-wrap position=\"float\" id=\"table4-15330338251397035\" orientation=\"portrait\"><label>Table 4.</label><caption><p>Performance Comparison among Different Methods in External Validation, with &#8220;our Study&#8221; Denoting our Proposed Method.</p></caption><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-table4.jpg\"/><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/><col align=\"char\" char=\"&#xB1;\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">Metrics</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">TransUnet<sup>\n<xref rid=\"bibr24-15330338251397035\" ref-type=\"bibr\">24</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">DLKA-net<sup>\n<xref rid=\"bibr30-15330338251397035\" ref-type=\"bibr\">30</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">SelfRag-Unet<sup>\n<xref rid=\"bibr31-15330338251397035\" ref-type=\"bibr\">31</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Unet<sup>\n<xref rid=\"bibr10-15330338251397035\" ref-type=\"bibr\">10</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Cascade <sup>\n<xref rid=\"bibr32-15330338251397035\" ref-type=\"bibr\">32</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">EMCAD <sup>\n<xref rid=\"bibr33-15330338251397035\" ref-type=\"bibr\">33</xref>\n</sup></th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Our Study</th></tr></thead><tbody><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Bladder</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">86.08&#8201;&#177;&#8201;4.60</td><td rowspan=\"1\" colspan=\"1\">88.66&#8201;&#177;&#8201;4.67</td><td rowspan=\"1\" colspan=\"1\">87.27&#8201;&#177;&#8201;4.35</td><td rowspan=\"1\" colspan=\"1\">87.19&#8201;&#177;&#8201;4.83</td><td rowspan=\"1\" colspan=\"1\">91.43&#8201;&#177;&#8201;3.36</td><td rowspan=\"1\" colspan=\"1\">92.40&#8201;&#177;&#8201;3.07</td><td rowspan=\"1\" colspan=\"1\"><bold>92.52</bold>&#8201;&#177;&#8201;<bold>2.92</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">2.92&#8201;&#177;&#8201;0.71</td><td rowspan=\"1\" colspan=\"1\">3.34&#8201;&#177;&#8201;2.61</td><td rowspan=\"1\" colspan=\"1\">4.22&#8201;&#177;&#8201;5.83</td><td rowspan=\"1\" colspan=\"1\">2.88&#8201;&#177;&#8201;0.88</td><td rowspan=\"1\" colspan=\"1\">1.68&#8201;&#177;&#8201;0.49</td><td rowspan=\"1\" colspan=\"1\">1.58&#8201;&#177;&#8201;0.57</td><td rowspan=\"1\" colspan=\"1\"><bold>1.52</bold>&#8201;&#177;&#8201;<bold>0.51</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.1&#8201;&#177;&#8201;0.25</td><td rowspan=\"1\" colspan=\"1\">1.31&#8201;&#177;&#8201;1.45</td><td rowspan=\"1\" colspan=\"1\">1.30&#8201;&#177;&#8201;1.00</td><td rowspan=\"1\" colspan=\"1\">1.16&#8201;&#177;&#8201;0.39</td><td rowspan=\"1\" colspan=\"1\">0.59&#8201;&#177;&#8201;0.18</td><td rowspan=\"1\" colspan=\"1\"><bold>0.50</bold>&#8201;&#177;&#8201;<bold>0.14</bold></td><td rowspan=\"1\" colspan=\"1\">0.51&#8201;&#177;&#8201;0.18</td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Colon</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">58.58&#8201;&#177;&#8201;11.54</td><td rowspan=\"1\" colspan=\"1\">64.90&#8201;&#177;&#8201;11.07</td><td rowspan=\"1\" colspan=\"1\">62.90&#8201;&#177;&#8201;9.78</td><td rowspan=\"1\" colspan=\"1\">61.61&#8201;&#177;&#8201;10.46</td><td rowspan=\"1\" colspan=\"1\">73.75&#8201;&#177;&#8201;8.03</td><td rowspan=\"1\" colspan=\"1\">73.87&#8201;&#177;&#8201;7.34</td><td rowspan=\"1\" colspan=\"1\"><bold>75.58</bold>&#8201;&#177;&#8201;<bold>7.98</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">15.31&#8201;&#177;&#8201;8.79</td><td rowspan=\"1\" colspan=\"1\">12.81&#8201;&#177;&#8201;6.35</td><td rowspan=\"1\" colspan=\"1\">14.09&#8201;&#177;&#8201;7.37</td><td rowspan=\"1\" colspan=\"1\">14.71&#8201;&#177;&#8201;8.57</td><td rowspan=\"1\" colspan=\"1\">9.67&#8201;&#177;&#8201;5.78</td><td rowspan=\"1\" colspan=\"1\">11.59&#8201;&#177;&#8201;6.46</td><td rowspan=\"1\" colspan=\"1\"><bold>8.86</bold>&#8201;&#177;&#8201;<bold>5.08</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">3.70&#8201;&#177;&#8201;2.48</td><td rowspan=\"1\" colspan=\"1\">3.05&#8201;&#177;&#8201;2.09</td><td rowspan=\"1\" colspan=\"1\">3.81&#8201;&#177;&#8201;2.04</td><td rowspan=\"1\" colspan=\"1\">3.83&#8201;&#177;&#8201;2.33</td><td rowspan=\"1\" colspan=\"1\"><bold>1.73</bold>&#8201;&#177;&#8201;<bold>1.05</bold></td><td rowspan=\"1\" colspan=\"1\">2.17&#8201;&#177;&#8201;0.99</td><td rowspan=\"1\" colspan=\"1\">1.98&#8201;&#177;&#8201;1.06</td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Rectum</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">68.86&#8201;&#177;&#8201;7.64</td><td rowspan=\"1\" colspan=\"1\">75.37&#8201;&#177;&#8201;8.40</td><td rowspan=\"1\" colspan=\"1\">74.59&#8201;&#177;&#8201;9.52</td><td rowspan=\"1\" colspan=\"1\">73.73&#8201;&#177;&#8201;10.18</td><td rowspan=\"1\" colspan=\"1\">78.59&#8201;&#177;&#8201;7.51</td><td rowspan=\"1\" colspan=\"1\">77.80&#8201;&#177;&#8201;7.20</td><td rowspan=\"1\" colspan=\"1\"><bold>79.62</bold>&#8201;&#177;&#8201;<bold>8.96</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">5.94&#8201;&#177;&#8201;1.96</td><td rowspan=\"1\" colspan=\"1\">5.48&#8201;&#177;&#8201;4.16</td><td rowspan=\"1\" colspan=\"1\">4.41&#8201;&#177;&#8201;1.69</td><td rowspan=\"1\" colspan=\"1\">4.13&#8201;&#177;&#8201;1.80</td><td rowspan=\"1\" colspan=\"1\">4.66&#8201;&#177;&#8201;3.39</td><td rowspan=\"1\" colspan=\"1\">5.17&#8201;&#177;&#8201;3.19</td><td rowspan=\"1\" colspan=\"1\"><bold>3.97</bold>&#8201;&#177;&#8201;<bold>3.54</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">2.17&#8201;&#177;&#8201;0.58</td><td rowspan=\"1\" colspan=\"1\">2.20&#8201;&#177;&#8201;1.21</td><td rowspan=\"1\" colspan=\"1\">1.49&#8201;&#177;&#8201;0.75</td><td rowspan=\"1\" colspan=\"1\">1.68&#8201;&#177;&#8201;1.19</td><td rowspan=\"1\" colspan=\"1\">1.33&#8201;&#177;&#8201;0.88</td><td rowspan=\"1\" colspan=\"1\">1.42&#8201;&#177;&#8201;1.01</td><td rowspan=\"1\" colspan=\"1\"><bold>1.32</bold>&#8201;&#177;&#8201;<bold>1.09</bold></td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>Small intestine</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">76.11&#8201;&#177;&#8201;3.92</td><td rowspan=\"1\" colspan=\"1\">81.53&#8201;&#177;&#8201;3.46</td><td rowspan=\"1\" colspan=\"1\">78.49&#8201;&#177;&#8201;4.96</td><td rowspan=\"1\" colspan=\"1\">78.12&#8201;&#177;&#8201;4.67</td><td rowspan=\"1\" colspan=\"1\">86.17&#8201;&#177;&#8201;2.58</td><td rowspan=\"1\" colspan=\"1\">86.01&#8201;&#177;&#8201;2.49</td><td rowspan=\"1\" colspan=\"1\"><bold>87.76</bold>&#8201;&#177;&#8201;<bold>2.48</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">5.05&#8201;&#177;&#8201;1.24</td><td rowspan=\"1\" colspan=\"1\">3.82&#8201;&#177;&#8201;1.46</td><td rowspan=\"1\" colspan=\"1\">4.85&#8201;&#177;&#8201;1.46</td><td rowspan=\"1\" colspan=\"1\">5.46&#8201;&#177;&#8201;1.55</td><td rowspan=\"1\" colspan=\"1\">2.91&#8201;&#177;&#8201;0.86</td><td rowspan=\"1\" colspan=\"1\">2.93&#8201;&#177;&#8201;0.99</td><td rowspan=\"1\" colspan=\"1\"><bold>2.67</bold>&#8201;&#177;&#8201;<bold>0.78</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">1.58&#8201;&#177;&#8201;0.55</td><td rowspan=\"1\" colspan=\"1\">1.23&#8201;&#177;&#8201;0.35</td><td rowspan=\"1\" colspan=\"1\">1.75&#8201;&#177;&#8201;0.42</td><td rowspan=\"1\" colspan=\"1\">1.97&#8201;&#177;&#8201;0.54</td><td rowspan=\"1\" colspan=\"1\">0.90&#8201;&#177;&#8201;0.27</td><td rowspan=\"1\" colspan=\"1\">0.92&#8201;&#177;&#8201;0.20</td><td rowspan=\"1\" colspan=\"1\"><bold>0.84</bold>&#8201;&#177;&#8201;<bold>0.25</bold></td></tr><tr><td colspan=\"8\" rowspan=\"1\">\n<bold>HR-CTV</bold>\n</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DSC</td><td rowspan=\"1\" colspan=\"1\">73.09&#8201;&#177;&#8201;6.96</td><td rowspan=\"1\" colspan=\"1\">72.99&#8201;&#177;&#8201;7.09</td><td rowspan=\"1\" colspan=\"1\">76.30&#8201;&#177;&#8201;5.06</td><td rowspan=\"1\" colspan=\"1\">75.60&#8201;&#177;&#8201;5.51</td><td rowspan=\"1\" colspan=\"1\">76.76&#8201;&#177;&#8201;5.65</td><td rowspan=\"1\" colspan=\"1\">78.26&#8201;&#177;&#8201;4.70</td><td rowspan=\"1\" colspan=\"1\"><bold>78.88</bold>&#8201;&#177;&#8201;<bold>4.53</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">HD95</td><td rowspan=\"1\" colspan=\"1\">5.68&#8201;&#177;&#8201;3.40</td><td rowspan=\"1\" colspan=\"1\">4.91&#8201;&#177;&#8201;1.96</td><td rowspan=\"1\" colspan=\"1\">4.92&#8201;&#177;&#8201;3.67</td><td rowspan=\"1\" colspan=\"1\">4.87&#8201;&#177;&#8201;3.25</td><td rowspan=\"1\" colspan=\"1\">4.67&#8201;&#177;&#8201;2.66</td><td rowspan=\"1\" colspan=\"1\">4.64&#8201;&#177;&#8201;2.13</td><td rowspan=\"1\" colspan=\"1\"><bold>4.33</bold>&#8201;&#177;&#8201;<bold>2.25</bold></td></tr><tr><td rowspan=\"1\" colspan=\"1\">ASD</td><td rowspan=\"1\" colspan=\"1\">2.12&#8201;&#177;&#8201;1.15</td><td rowspan=\"1\" colspan=\"1\">1.88&#8201;&#177;&#8201;0.71</td><td rowspan=\"1\" colspan=\"1\">1.62&#8201;&#177;&#8201;0.89</td><td rowspan=\"1\" colspan=\"1\">1.63&#8201;&#177;&#8201;0.76</td><td rowspan=\"1\" colspan=\"1\">1.66&#8201;&#177;&#8201;0.86</td><td rowspan=\"1\" colspan=\"1\">1.42&#8201;&#177;&#8201;0.45</td><td rowspan=\"1\" colspan=\"1\"><bold>1.40</bold>&#8201;&#177;&#8201;<bold>0.60</bold></td></tr></tbody></table></alternatives></table-wrap></sec><sec id=\"section3F-15330338251397035\"><title>Model Parameters</title><p>As presented in <xref rid=\"table5-15330338251397035\" ref-type=\"table\">Table 5</xref>, MDA-TransUnet exhibits higher parameter counts and GPU memory consumption compared to other models, primarily attributed to increased computational complexity from the MASAG and DCAM modules, along with inherent requirements of the TransUnet encoder architecture. However, this computational overhead delivers substantial performance gains&#8212;particularly in handling multi-center data where it demonstrates superior generalization capability. While UNet and SelfRag-UNet achieve the lowest parameter/GPU footprints, their segmentation accuracy lags significantly behind MDA-TransUnet. Regarding inference speed, MDA-TransUnet matches similarly-sized models (TransUnet and DLKA-Net) while outperforming the smaller EMCAD architecture. Crucially, the 26-fold acceleration in segmentation reduces patient waiting time, mitigates organ displacement risks, and alleviates patient discomfort&#8212;demonstrating substantial clinical significance.</p><table-wrap position=\"float\" id=\"table5-15330338251397035\" orientation=\"portrait\"><label>Table 5.</label><caption><p>Model Parameters Comparison.</p></caption><alternatives><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"10.1177_15330338251397035-table5.jpg\"/><table frame=\"hsides\" rules=\"groups\"><colgroup span=\"1\"><col align=\"left\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/><col align=\"char\" char=\".\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">Params(M)</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">GPU(MiB)</th><th align=\"left\" rowspan=\"1\" colspan=\"1\">inference(Min)</th></tr></thead><tbody><tr><td rowspan=\"1\" colspan=\"1\">TransUnet</td><td rowspan=\"1\" colspan=\"1\">105.27</td><td rowspan=\"1\" colspan=\"1\">11726</td><td rowspan=\"1\" colspan=\"1\">1.18</td></tr><tr><td rowspan=\"1\" colspan=\"1\">DLKA-net</td><td rowspan=\"1\" colspan=\"1\">101.64</td><td rowspan=\"1\" colspan=\"1\">18636</td><td rowspan=\"1\" colspan=\"1\">1.14</td></tr><tr><td rowspan=\"1\" colspan=\"1\">SelfRag-Unet</td><td rowspan=\"1\" colspan=\"1\">17.26</td><td rowspan=\"1\" colspan=\"1\">9034</td><td rowspan=\"1\" colspan=\"1\">1.42</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Unet</td><td rowspan=\"1\" colspan=\"1\">17.26</td><td rowspan=\"1\" colspan=\"1\">8740</td><td rowspan=\"1\" colspan=\"1\">1.38</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Cascade</td><td rowspan=\"1\" colspan=\"1\">108.31</td><td rowspan=\"1\" colspan=\"1\">13846</td><td rowspan=\"1\" colspan=\"1\">1.00</td></tr><tr><td rowspan=\"1\" colspan=\"1\">EMCAD</td><td rowspan=\"1\" colspan=\"1\">26.76</td><td rowspan=\"1\" colspan=\"1\">10102</td><td rowspan=\"1\" colspan=\"1\">1.23</td></tr><tr><td rowspan=\"1\" colspan=\"1\">Our Study</td><td rowspan=\"1\" colspan=\"1\">110.94</td><td rowspan=\"1\" colspan=\"1\">17500</td><td rowspan=\"1\" colspan=\"1\">1.18</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec sec-type=\"discussion\" id=\"section4-15330338251397035\"><title>Discussion</title><p>The combination of external beam radiation therapy and high-dose-rate brachytherapy represents the standard of care for gynecologic cancer treatment, where HDR-BT has proven indispensable and strongly correlates with improved survival rates. Compared to conventional EBRT, brachytherapy's defining feature is its ability to deliver higher radiation doses to tumor regions near the radiation source while effectively sparing normal organs due to rapid dose fall-off. However, brachytherapy faces unique challenges. In HDR-BT, treatment planning must be completed rapidly after applicator insertion, often under time constraints that may introduce human errors. In recent years, deep learning-based methods have emerged as promising solutions to automate workflows, reduce patient wait times, and enhance comfort.</p><p>The purpose of this study is to investigate deep learning-based automatic segmentation methods for cervical cancer brachytherapy CT images. The proposed MDA-TransUnet achieves superior Dice Similarity Coefficient (DSC) across all target regions compared to other methods (<xref rid=\"table1-15330338251397035\" ref-type=\"table\">Table 1</xref>). Our method performs best in bladder segmentation, largely attributed to its relatively regular anatomical structure and high contrast, facilitating clear boundary feature extraction. Compared to the bladder, the rectum, colon, and small bowel present greater segmentation challenges due to their anatomical and imaging characteristics. While the rectum maintains relatively stable positioning, its low contrast on CT scans makes boundary delineation difficult. The colon presents exceptional segmentation challenges due to its characteristically low contrast against surrounding adipose and soft tissues in CT images, combined with high deformability in both shape and position. The small bowel's position is substantially influenced by respiratory motion, peristalsis, and bladder/rectal filling status, while its appearance on CT images often blends with other soft tissues, complicating segmentation. Despite these challenges, MDA-TransUnet achieved the highest DSC scores for all three organs: 79.27% for colon, 79.27% for rectum, and 88.90% for small bowel. Gu et al<sup>\n<xref rid=\"bibr25-15330338251397035\" ref-type=\"bibr\">25</xref>\n</sup> first combined CNN with Transformer in MFFUNet for segmenting brachytherapy OARs (bladder, colon, rectum), reporting DSCs of 92.65%, 61.86%, and 66.55% respectively. Comparatively, our MDA-TransUnet demonstrates significantly better performance on colon and rectum segmentation. In future work, we will explore more effective contrast enhancement techniques to further improve segmentation performance for both the colon and rectum. Additionally, the HR-CTV segmentation achieved 82.35% DSC, outperforming other methods despite variations in delineation protocols across centers.</p><p>Ablation studies (<xref rid=\"table2-15330338251397035\" ref-type=\"table\">Table 2</xref>) demonstrate that incorporating MASAG and DCAM modules significantly enhances performance across all target regions. Particularly for structurally complex areas like colon, small bowel, and HR-CTV, DSC improved by an average of 2.48%. This validates the modules&#8217; effectiveness in enhancing anatomical detail capture while mitigating cross-center variability, ensuring robust performance in multi-center scenarios.</p><p>Beyond geometric metrics, dosimetric evaluation remains crucial for automated segmentation.<sup>\n<xref rid=\"bibr34-15330338251397035\" ref-type=\"bibr\">34</xref>\n</sup> Paired t-tests (<xref rid=\"table3-15330338251397035\" ref-type=\"table\">Table 3</xref>) confirmed no statistically significant differences between our automated method and manual delineation across five dosimetric parameters: D5cc, D2cc, D0.1cc, Dmean, and D90%. Notably, despite poorer geometric accuracy in colon segmentation, dosimetric agreement with manual methods remained high, consistent with Wang et al's findings.<sup>\n<xref rid=\"bibr35-15330338251397035\" ref-type=\"bibr\">35</xref>\n</sup> Dosimetric parameters like D2cc (representing the minimum dose received by the maximally irradiated 2cm3 of a volume) primarily depend on segmentation accuracy in high-dose regions (typically near applicators or targets). Sufficient overlap between automatic segmentation and ground truth in these critical areas can yield comparable dosimetric outcomes, even when contour discrepancies exist in low-dose regions. The Dice Similarity Coefficient (DSC) measures global volume overlap - shape variations in geometrically complex structures within low-dose, non-critical regions may reduce DSC without materially affecting dosimetry. Consequently, both geometric and dosimetric metrics are essential for comprehensive evaluation of automatic segmentation methods. Nevertheless, continuous optimization for improved geometric accuracy (particularly in high-dose regions) remains crucial, potentially further minimizing dosimetric differences while enhancing clinicians&#8217; trust in automated results and overall model reliability.</p><p>To further assess generalization capability, 20 cervical cancer brachytherapy patients&#8217;CT images from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University were independently tested (external validation). As shown in <xref rid=\"table4-15330338251397035\" ref-type=\"table\">Table 4</xref>, MDA-TransUnet achieved optimal DSC and HD95 values across all five targets (bladder, colon, rectum, small bowel, HR-CTV) on this unseen dataset. For the challenging colon segmentation, it outperformed suboptimal methods by 1.74% in DSC and reduced HD95 by 0.81&#8197;mm. These results demonstrate robust adaptability to multi-center heterogeneity (eg, varying CT protocols and contouring practices). Though yielding suboptimal ASD for bladder and colon, the model achieved optimal average ASD across all targets, confirming boundary segmentation accuracy. Compared to other models, MDA-TransUnet exhibits superior performance stability and generalization capability on independent cross-center data. Our approach - incorporating data augmentation, multi-center training, and robustness-enhancing modules (MASAG, DCAM) - effectively mitigates inter-center annotation variations. Future efforts should establish refined contouring guidelines and cross-institutional consistency reviews to standardize ground truth.</p><p>This study has limitations. First, obtaining fully annotated multi-center cervical cancer brachytherapy CT datasets remains challenging, with inter-center variations in region-of-interest delineation. Second, the dataset remains relatively limited. Though multi-center data and independent external validation were employed, small sample sizes may introduce potential bias and constrain model learning capacity. Finally, while achieving excellent segmentation performance, MDA-TransUnet's high parameter count and computational complexity may hinder clinical deployment in resource-constrained settings. Future work will: (1) Expand datasets with multi-center cases, (2) Develop lightweight model compression strategies to enhance computational efficiency, and (3) Optimize clinical applicability.</p></sec><sec sec-type=\"conclusions\" id=\"section5-15330338251397035\"><title>Conclusions</title><p>This study proposes MDA-TransUnet, a deep learning method leveraging a CNN-Transformer hybrid architecture to automate the segmentation of HR-CTV and OARs in planning CT images for cervical cancer brachytherapy. Evaluation results demonstrate that our method outperforms other state-of-the-art (SOTA) approaches and exhibits no statistically significant differences from manual delineation across five dosimetric metrics. The proposed automated segmentation framework facilitates the automation of cervical cancer brachytherapy workflows, enhances treatment consistency, reduces post-applicator-insertion waiting times, and alleviates patient discomfort.</p></sec></body><back><ack><title>Acknowledgements</title><p>Not applicable.</p></ack><fn-group><fn fn-type=\"other\"><p><bold>ORCID iDs:</bold> Dezheng Cao <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://orcid.org/0009-0001-1064-1034\" ext-link-type=\"uri\">https://orcid.org/0009-0001-1064-1034</ext-link></p><p>Heng Zhang <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://orcid.org/0000-0002-2019-0038\" ext-link-type=\"uri\">https://orcid.org/0000-0002-2019-0038</ext-link></p><p>Qianjia Huang <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://orcid.org/0009-0005-1320-4824\" ext-link-type=\"uri\">https://orcid.org/0009-0005-1320-4824</ext-link></p><p>Xinye Ni <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://orcid.org/0000-0002-2402-9719\" ext-link-type=\"uri\">https://orcid.org/0000-0002-2402-9719</ext-link></p></fn><fn fn-type=\"other\"><p><bold>Ethics Approval and Consent to Participate:</bold> This study was approved by the Medical Ethics Committee of Changzhou No.2 People's Hospital Affiliated to Nanjing Medical University (Approval number: 2024KY213-01), the Medical Ethics Committee of Tumor Hospital Affiliated to Nantong University (Approval number: 2020-031) and the Third Affiliated Hospital of Nanjing Medical University (Approval number: ITT2024101). The requirement for informed consent was waived for all participants with the approval of the Medical Ethics Committees. The research was conducted in accordance with the principles embodied in the Declaration of Helsinki and local statutory requirements. All authors have reviewed research data.</p></fn><fn fn-type=\"other\"><p><bold>Consent for Publication:</bold> Not applicable.</p></fn><fn fn-type=\"con\"><p><bold>Author Contributions:</bold> CDZ conceived the study, designed the experiments, analyzed the data and wrote the manuscript. Data were collected by JJH, HJH, YB, STT, KY. NXY edited the manuscript. LC, QCJ, and XK supervised the data analysis. NXY reviewed literature, contributed to the manuscript and acquired the financial support for the project. ZH, SLT, HQJ, and CDZ performed the statistical analyses. All the authors accessed the study data and reviewed and approved the final manuscript.</p></fn><fn fn-type=\"financial-disclosure\"><p><bold>Funding:</bold> The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work is supported by the National Natural Science Foundation of China (No.62371243), Jiangsu Provincial Medical Key Discipline Cultivation Unit of Oncology Therapeutics (Radiotherapy) (No. JSDW202237), Changzhou Social Development Program(Nos. CE20235063 and CJ20244020), and Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province (No. JX13614239).</p></fn><fn fn-type=\"COI-statement\"><p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type=\"COI-statement\"><p>The datasets generated and/or analyzed during the current study are not publicly available due to protection of individual patient privacy and the use of an in-house software but are available from the corresponding author on reasonable request.</p></fn></fn-group><glossary content-type=\"abbr\"><title>Abbreviations</title><def-list><def-item><term>HR-CTV</term><def><p>High-risk clinical target volume</p></def></def-item><def-item><term>OARs</term><def><p>Organs at risk</p></def></def-item><def-item><term>MDA-TransUnet</term><def><p>Multi-Scale convolutional attention transunet</p></def></def-item><def-item><term>CNN</term><def><p>Convolutional neural networks</p></def></def-item><def-item><term>CT</term><def><p>Computed tomography</p></def></def-item><def-item><term>DSC</term><def><p>Dice similarity coefficient</p></def></def-item><def-item><term>HD95</term><def><p>95% Hausdorff distance</p></def></def-item><def-item><term>ASD</term><def><p>Average surface distance</p></def></def-item><def-item><term>EBRT</term><def><p>External beam radiation therapy</p></def></def-item><def-item><term>BT</term><def><p>brachytherapy</p></def></def-item><def-item><term>LSTM</term><def><p>Long short-term memory</p></def></def-item><def-item><term>MASAG</term><def><p>Multi-scale adaptive spatial attention gate</p></def></def-item><def-item><term>DCAM</term><def><p>Deformable convolutional attention module</p></def></def-item><def-item><term>HU</term><def><p>Hounsfield Units</p></def></def-item><def-item><term>MSF</term><def><p>Multi-scale feature fusion</p></def></def-item><def-item><term>SS</term><def><p>Spatial selection</p></def></def-item><def-item><term>SICM</term><def><p>Spatial interaction and cross-modulation</p></def></def-item><def-item><term>RC</term><def><p>Recalibration</p></def></def-item><def-item><term>SA</term><def><p>Spatial attention</p></def></def-item><def-item><term>DCB</term><def><p>Deformable convolution block</p></def></def-item><def-item><term>DVIs</term><def><p>Dose-volume indices</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id=\"bibr1-15330338251397035\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sung</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Ferlay</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Siegel</surname><given-names>RL</given-names></name></person-group>, <etal>et al.</etal><article-title>Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>. <source>CA Cancer J Clin.</source><year>2021</year>;<volume>71</volume>(<issue>3</issue>):<fpage>209</fpage>-<lpage>249</lpage>.<pub-id pub-id-type=\"pmid\">33538338</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3322/caac.21660</pub-id></mixed-citation></ref><ref id=\"bibr2-15330338251397035\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chino</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Annunziata</surname><given-names>CM</given-names></name><name name-style=\"western\"><surname>Beriwal</surname><given-names>S</given-names></name></person-group>, <etal>et al.</etal><article-title>Radiation therapy for cervical cancer: Executive summary of an ASTRO clinical practice guideline</article-title>. <source>Pract Radiat Oncol.</source><year>2020</year>;<volume>10</volume>(<issue>4</issue>):<fpage>220</fpage>-<lpage>234</lpage>.<pub-id pub-id-type=\"pmid\">32473857</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.prro.2020.04.002</pub-id><pub-id pub-id-type=\"pmcid\">PMC8802172</pub-id></mixed-citation></ref><ref id=\"bibr3-15330338251397035\"><label>3</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Milosevic</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Fyles</surname><given-names>A</given-names></name></person-group>, <etal>et al.</etal><article-title>Trends in the utilization of brachytherapy in cervical cancer in the United States</article-title>. <source>Int J Radiation Oncol* Biol* Phys</source>. <year>2013</year>;<volume>87</volume>(<issue>1</issue>):<fpage>111</fpage>-<lpage>119</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.ijrobp.2013.05.033</pub-id><pub-id pub-id-type=\"pmid\">23849695</pub-id></mixed-citation></ref><ref id=\"bibr4-15330338251397035\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Riegel</surname><given-names>AC</given-names></name><name name-style=\"western\"><surname>Antone</surname><given-names>JG</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H</given-names></name></person-group>, <etal>et al.</etal><article-title>Deformable image registration and interobserver variation in contour propagation for radiation therapy planning</article-title>. <source>J Appl Clin Med Phys.</source><year>2016</year>;<volume>17</volume>(<issue>3</issue>):<fpage>347</fpage>-<lpage>357</lpage>.<pub-id pub-id-type=\"pmid\">27167289</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1120/jacmp.v17i3.6110</pub-id><pub-id pub-id-type=\"pmcid\">PMC5690939</pub-id></mixed-citation></ref><ref id=\"bibr5-15330338251397035\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Fujimoto</surname><given-names>DK</given-names></name><name name-style=\"western\"><surname>von Eyben</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Usoz</surname><given-names>M</given-names></name></person-group>, <etal>et al.</etal><article-title>Improving brachytherapy efficiency with dedicated dosimetrist planners</article-title>. <source>Brachytherapy</source>. <year>2019</year>;<volume>18</volume>(<issue>1</issue>):<fpage>103</fpage>-<lpage>107</lpage>.<pub-id pub-id-type=\"pmid\">30391061</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.brachy.2018.10.003</pub-id></mixed-citation></ref><ref id=\"bibr6-15330338251397035\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Boldrini</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Bibault</surname><given-names>JE</given-names></name><name name-style=\"western\"><surname>Masciocchi</surname><given-names>C</given-names></name></person-group>, <etal>et al.</etal><article-title>Deep learning: A review for the radiation oncologist</article-title>. <source>Front Oncol.</source><year>2019</year>;<volume>9</volume>:<fpage>977</fpage>.<pub-id pub-id-type=\"pmid\">31632910</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fonc.2019.00977</pub-id><pub-id pub-id-type=\"pmcid\">PMC6779810</pub-id></mixed-citation></ref><ref id=\"bibr7-15330338251397035\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Meyer</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Noblet</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Mazzara</surname><given-names>C</given-names></name></person-group>, <etal>et al.</etal><article-title>Survey on deep learning for radiotherapy</article-title>. <source>Comput Biol Med.</source><year>2018</year>;<volume>98</volume>:<fpage>126</fpage>-<lpage>146</lpage>.<pub-id pub-id-type=\"pmid\">29787940</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2018.05.018</pub-id></mixed-citation></ref><ref id=\"bibr8-15330338251397035\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Sahiner</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Pezeshk</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Hadjiiski</surname><given-names>LM</given-names></name></person-group>, <etal>et al.</etal><article-title>Deep learning in medical imaging and radiation therapy</article-title>. <source>Med Phys.</source><year>2019</year>;<volume>46</volume>(<issue>1</issue>):<comment>e1</comment>-<lpage>e36</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/mp.13264</pub-id><pub-id pub-id-type=\"pmcid\">PMC9560030</pub-id><pub-id pub-id-type=\"pmid\">30367497</pub-id></mixed-citation></ref><ref id=\"bibr9-15330338251397035\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shi</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>G</given-names></name></person-group>, <etal>et al.</etal><article-title>Artificial intelligence in high-dose-rate brachytherapy treatment planning for cervical cancer: A review</article-title>. <source>Front Oncol.</source><year>2025</year>;<volume>15</volume>:<fpage>1507592</fpage>.<pub-id pub-id-type=\"pmid\">39931087</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3389/fonc.2025.1507592</pub-id><pub-id pub-id-type=\"pmcid\">PMC11808022</pub-id></mixed-citation></ref><ref id=\"bibr10-15330338251397035\"><label>10</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group><conf-name>U-net: convolutional networks for biomedical image segmentation</conf-name>. <conf-name>Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference</conf-name>, <conf-loc>Munich, Germany</conf-loc>, <comment>October 5-9, 2015, proceedings, part III 18. Springer international publishing</comment>, <conf-date>2015</conf-date>: p. <fpage>234</fpage>-<lpage>241</lpage>.</mixed-citation></ref><ref id=\"bibr11-15330338251397035\"><label>11</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Rahman Siddiquee</surname><given-names>MM</given-names></name><name name-style=\"western\"><surname>Tajbakhsh</surname><given-names>N</given-names></name></person-group>, <etal>et al.</etal><conf-name>Unet++: a nested u-net architecture for medical image segmentation</conf-name>. In <conf-name>Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018</conf-name>, <conf-loc>Granada, Spain</conf-loc>, <comment>September 20, 2018, proceedings 4. Springer International Publishing</comment>, <conf-date>2018</conf-date>: p. <fpage>3</fpage>-<lpage>11</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-030-00889-5_1</pub-id><pub-id pub-id-type=\"pmcid\">PMC7329239</pub-id><pub-id pub-id-type=\"pmid\">32613207</pub-id></mixed-citation></ref><ref id=\"bibr12-15330338251397035\"><label>12</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Tong</surname><given-names>R</given-names></name></person-group>, <etal>et al.</etal><conf-name>Unet 3+: a full-scale connected unet for medical image segmentation</conf-name>. In <conf-name>ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE</conf-name>, <conf-date>2020</conf-date>: p. <fpage>1055</fpage>-<lpage>1059</lpage>.</mixed-citation></ref><ref id=\"bibr13-15330338251397035\"><label>13</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lou</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Guan</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Loew</surname><given-names>M.</given-names></name></person-group><conf-name>DC-UNet: rethinking the U-net architecture with dual channel efficient CNN for medical image segmentation</conf-name>. In <conf-name>Medical Imaging 2021: Image Processing. SPIE</conf-name>, <conf-date>2021</conf-date>, <volume>11596</volume>: p. <fpage>758</fpage>-<lpage>768</lpage>.</mixed-citation></ref><ref id=\"bibr14-15330338251397035\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Isensee</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Jaeger</surname><given-names>PF</given-names></name><name name-style=\"western\"><surname>Kohl</surname><given-names>SAA</given-names></name></person-group>, <etal>et al.</etal><article-title>nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation</article-title>. <source>Nat Methods.</source><year>2021</year>;<volume>18</volume>(<issue>2</issue>):<fpage>203</fpage>-<lpage>211</lpage>.<pub-id pub-id-type=\"pmid\">33288961</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41592-020-01008-z</pub-id></mixed-citation></ref><ref id=\"bibr15-15330338251397035\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>L</given-names></name></person-group>, <etal>et al.</etal><article-title>A deep learning-based self-adapting ensemble method for segmentation in gynecological brachytherapy</article-title>. <source>Radiat Oncol.</source><year>2022</year>;<volume>17</volume>(<issue>1</issue>):<fpage>152</fpage>.<pub-id pub-id-type=\"pmid\">36064571</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1186/s13014-022-02121-3</pub-id><pub-id pub-id-type=\"pmcid\">PMC9446699</pub-id></mixed-citation></ref><ref id=\"bibr16-15330338251397035\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Jiang</surname><given-names>S</given-names></name></person-group>, <etal>et al.</etal><article-title>Automatic segmentation and applicator reconstruction for CT-based brachytherapy of cervical cancer using 3D convolutional neural networks</article-title>. <source>J Appl Clin Med Phys.</source><year>2020</year>;<volume>21</volume>(<issue>10</issue>):<fpage>158</fpage>-<lpage>169</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/acm2.13024</pub-id><pub-id pub-id-type=\"pmcid\">PMC7592978</pub-id><pub-id pub-id-type=\"pmid\">32991783</pub-id></mixed-citation></ref><ref id=\"bibr17-15330338251397035\"><label>17</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chang</surname><given-names>JH</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>KH</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>TH</given-names></name></person-group>, <etal>et al.</etal><article-title>Image segmentation in 3D brachytherapy using convolutional LSTM</article-title>. <source>J Med Biol Eng.</source><year>2021</year>;<volume>41</volume>(<issue>5</issue>):<fpage>636</fpage>-<lpage>651</lpage>.</mixed-citation></ref><ref id=\"bibr18-15330338251397035\"><label>18</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cao</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name></person-group>, <etal>et al.</etal><part-title>Swin-unet: unet-like pure transformer for medical image segmentation</part-title>. In <source>European Conference on Computer Vision</source>. <publisher-name>Springer Nature</publisher-name><comment>Switzerland</comment>, <year>2022</year>: pp.<fpage>205</fpage>-<lpage>218</lpage>.</mixed-citation></ref><ref id=\"bibr19-15330338251397035\"><label>19</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dosovitskiy</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Kolesnikov</surname><given-names>A</given-names></name></person-group>, <etal>et al.</etal><comment>An image is worth 16&#8201;&#215;&#8201;16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929</comment>, <year>2020</year>.</mixed-citation></ref><ref id=\"bibr20-15330338251397035\"><label>20</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dong</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Fan</surname><given-names>DP</given-names></name></person-group>, <etal>et al.</etal><comment>Polyp-pvt: Polyp segmentation with pyramid vision transformers. arXiv preprint arXiv:2108.06932</comment>, <year>2021</year>.</mixed-citation></ref><ref id=\"bibr21-15330338251397035\"><label>21</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>F</given-names></name></person-group>, <etal>et al.</etal><part-title>Stepwise feature fusion: local guides global</part-title>. In <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-name>Springer Nature</publisher-name><comment>Switzerland</comment>, <year>2022</year>: <fpage>110</fpage>-<lpage>120</lpage>.</mixed-citation></ref><ref id=\"bibr22-15330338251397035\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name></person-group>, <etal>et al.</etal><article-title>Uctransnet: Rethinking the skip connections in u-net from a channel-wise perspective with transformer</article-title>. <source>Proce AAAI Conf Artif Intell</source>. <year>2022</year>;<volume>36</volume>(<issue>3</issue>):<fpage>2441</fpage>-<lpage>2449</lpage>.</mixed-citation></ref><ref id=\"bibr23-15330338251397035\"><label>23</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>Q.</given-names></name></person-group><conf-name>Transfuse: fusing transformers and cnns for medical image segmentation</conf-name>. In <conf-name>Medical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th international conference</conf-name>, <conf-loc>Strasbourg, France</conf-loc>, <comment>September 27&#8211;October 1, 2021, proceedings, Part I 24. Springer International Publishing,</comment><conf-date>2021</conf-date>: p. <fpage>14</fpage>-<lpage>24</lpage>.</mixed-citation></ref><ref id=\"bibr24-15330338251397035\"><label>24</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q</given-names></name></person-group>, <etal>et al.</etal><comment>Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306</comment>, <year>2021</year>.</mixed-citation></ref><ref id=\"bibr25-15330338251397035\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name></person-group>, <etal>et al.</etal><article-title>MFFUNet: A hybrid model with cross-attention-guided multi-feature fusion for automated segmentation of organs at risk in cervical cancer brachytherapy</article-title>. <source>Comput Med Imaging Graph</source>. <year>2025</year>;<volume>124</volume>:<fpage>102571</fpage>.<pub-id pub-id-type=\"pmid\">40441081</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compmedimag.2025.102571</pub-id></mixed-citation></ref><ref id=\"bibr26-15330338251397035\"><label>26</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Gay</surname><given-names>H</given-names></name></person-group>, <etal>et al.</etal><article-title>Weaving attention U-net: A novel hybrid CNN and attention-based method for organs-at-risk segmentation in head and neck CT images</article-title>. <source>Med Phys.</source><year>2021</year>;<volume>48</volume>(<issue>11</issue>):<fpage>7052</fpage>-<lpage>7062</lpage>.<pub-id pub-id-type=\"pmid\">34655077</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/mp.15287</pub-id></mixed-citation></ref><ref id=\"bibr27-15330338251397035\"><label>27</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Guan</surname><given-names>H</given-names></name></person-group>, <etal>et al.</etal><article-title>Development and validation of a deep learning algorithm for auto-delineation of clinical target volume and organs at risk in cervical cancer radiotherapy</article-title>. <source>Radiother Oncol.</source><year>2020</year>;<volume>153</volume>:<fpage>172</fpage>-<lpage>179</lpage>.<pub-id pub-id-type=\"pmid\">33039424</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.radonc.2020.09.060</pub-id></mixed-citation></ref><ref id=\"bibr28-15330338251397035\"><label>28</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Swamidas</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Mahantshetty</surname><given-names>U.</given-names></name></person-group><comment>ICRU report 89: prescribing, recording, and reporting brachytherapy for cancer of the cervix</comment>. <year>2017</year>.</mixed-citation></ref><ref id=\"bibr29-15330338251397035\"><label>29</label><mixed-citation publication-type=\"other\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kolahi</surname><given-names>SG</given-names></name><name name-style=\"western\"><surname>Chaharsooghi</surname><given-names>SK</given-names></name><name name-style=\"western\"><surname>Khatibi</surname><given-names>T</given-names></name></person-group>, <etal>et al.</etal><comment>MSA $^ 2$ Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation. arXiv preprint arXiv:2407.21640</comment>, <year>2024</year>.</mixed-citation></ref><ref id=\"bibr30-15330338251397035\"><label>30</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Azad</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Niggemeier</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>H&#252;ttemann</surname><given-names>M</given-names></name></person-group>, <etal>et al.</etal><conf-name>Beyond self-attention: deformable large kernel attention for medical image segmentation</conf-name>. <conf-name>Proceedings of the IEEE/CVF winter conference on applications of computer vision</conf-name>. <conf-date>2024</conf-date>: p. <fpage>1287</fpage>-<lpage>1297</lpage>.</mixed-citation></ref><ref id=\"bibr31-15330338251397035\"><label>31</label><mixed-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Qiu</surname><given-names>P</given-names></name></person-group>, <etal>et al.</etal><part-title>Selfreg-unet: self-regularized unet for medical image segmentation</part-title>. In <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-name>Springer Nature</publisher-name><comment>Switzerland</comment>, <year>2024</year>: p. <fpage>601</fpage>-<lpage>611</lpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/978-3-031-72111-3_56</pub-id><pub-id pub-id-type=\"pmcid\">PMC12408486</pub-id><pub-id pub-id-type=\"pmid\">40917447</pub-id></mixed-citation></ref><ref id=\"bibr32-15330338251397035\"><label>32</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rahman</surname><given-names>MM</given-names></name><name name-style=\"western\"><surname>Marculescu</surname><given-names>R.</given-names></name></person-group><conf-name>Medical image segmentation via cascaded attention decoding</conf-name>. In <conf-name>Proceedings of the IEEE/CVF winter conference on applications of computer vision</conf-name>. <conf-date>2023</conf-date>: p. <fpage>6222</fpage>-<lpage>6231</lpage>.</mixed-citation></ref><ref id=\"bibr33-15330338251397035\"><label>33</label><mixed-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Rahman</surname><given-names>MM</given-names></name><name name-style=\"western\"><surname>Munir</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Marculescu</surname><given-names>R.</given-names></name></person-group><conf-name>Emcad: efficient multi-scale convolutional attention decoding for medical image segmentation</conf-name>. In <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>. <conf-date>2024</conf-date>: p. <fpage>11769</fpage>-<lpage>11779</lpage>.</mixed-citation></ref><ref id=\"bibr34-15330338251397035\"><label>34</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yoganathan</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>Paul</surname><given-names>SN</given-names></name><name name-style=\"western\"><surname>Paloor</surname><given-names>S</given-names></name></person-group>, <etal>et al.</etal><article-title>Automatic segmentation of magnetic resonance images for high-dose-rate cervical cancer brachytherapy using deep learning</article-title>. <source>Med Phys.</source><year>2022</year>;<volume>49</volume>(<issue>3</issue>):<fpage>1571</fpage>-<lpage>1584</lpage>.<pub-id pub-id-type=\"pmid\">35094405</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1002/mp.15506</pub-id></mixed-citation></ref><ref id=\"bibr35-15330338251397035\"><label>35</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Tu</surname><given-names>Y</given-names></name></person-group>, <etal>et al.</etal><article-title>Evaluation of auto-segmentation for brachytherapy of postoperative cervical cancer using deep learning-based workflow</article-title>. <source>Physics in Medicine &amp; Biology</source>. <year>2023</year>;<volume>68</volume>(<issue>5</issue>):<fpage>055012</fpage>.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1088/1361-6560/acba76</pub-id><pub-id pub-id-type=\"pmid\">36753762</pub-id></mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Technol Cancer Res Treat Technol Cancer Res Treat 3299 tct TCT Technology in Cancer Research &amp; Treatment 1533-0346 1533-0338 SAGE Publications PMC12682993 PMC12682993.1 12682993 12682993 41348514 10.1177/15330338251397035 10.1177_15330338251397035 1 Original Research Article MDA-TransUNet: A Deep Learning-Based Automatic Segmentation Method for Cervical Cancer Brachytherapy https://orcid.org/0009-0001-1064-1034 Cao Dezheng MS 1 2 3 4 Jin Jianhua BS 5 * Han Jihua MS 6 * Yang Bo BS 7 * Shi Tingting BS 6 Kong Yan PhD 7 Liu Cong PhD 1 2 3 8 Qian Chunjun PhD 1 2 3 9 Xie Kai MS 1 2 3 Song Lintao MS 1 2 3 https://orcid.org/0000-0002-2019-0038 Zhang Heng MS 1 2 3 https://orcid.org/0009-0005-1320-4824 Huang Qianjia MS 1 2 3 4 https://orcid.org/0000-0002-2402-9719 Ni Xinye PhD 1 2 3 1 Department of Radiotherapy, 599923 The Second People's Hospital of Changzhou, the Third Affiliated Hospital of Nanjing Medical University , Changzhou, China 2 Jiangsu Province Engineering Research Center of Medical Physics, Changzhou, China 3 Center of Medical Physics, Nanjing Medical University, Changzhou, China 4 School of Computer Science and Artificial Intelligence, 12412 Changzhou University , Changzhou, China 5 Department of Radiotherapy, 377323 Affiliated Tumor Hospital of Nantong University , Nantong, China 6 Department of Radiotherapy, the Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University, Huai'an, China 7 Department of Radiation Oncology, 199193 Affiliated Hospital of Jiangnan University , Wuxi, China 8 Faculty of Business Information, 127231 Shanghai Business School , Shanghai, China 9 Hertfordshire College, 164387 Changzhou Institute of Technology , Changzhou, China * These authors are co-first authors. Xinye Ni, Department of Radiotherapy, The Second People's Hospital of Changzhou, the Third Affiliated Hospital of Nanjing Medical University, Changzhou 213003, China. Email: nxy@njmu.edu.cn 5 12 2025 2025 24 479399 15330338251397035 5 6 2025 11 10 2025 29 10 2025 05 12 2025 09 12 2025 09 12 2025 &#169; The Author(s) 2025 2025 SAGE Publications https://creativecommons.org/licenses/by-nc/4.0/ This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License ( https://creativecommons.org/licenses/by-nc/4.0/ ) which permits non-commercial use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page ( https://us.sagepub.com/en-us/nam/open-access-at-sage ). Introduction Accurate delineation of the high-risk clinical target volume (HR-CTV) and organs at risk (OARs) is critical for cervical cancer brachytherapy. However, treatment planning is time-consuming, and prolonged waiting can lead to organ displacement and patient discomfort. Additionally, the steep dose gradients around HR-CTV amplify segmentation errors in HR-CTV and OARs. Therefore, achieving rapid and precise delineation of HR-CTV and OARs remains challenging. This study proposes a novel network model, MDA-TransUNet, for fast segmentation of HR-CTV and OARs in cervical cancer. Methods We applied MDA-TransUnet, a CNN-Transformer hybrid model, to segment the bladder, colon, rectum, small bowel, and HR-CTV on cervical cancer CT images. 122 cervical cancer brachytherapy patients&#8217; CT images from three clinical centers were utilized for training and testing, with 80 cases allocated to training, 22 to testing, and 20 to external validation. Segmentation accuracy was quantified using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD95), and Average Surface Distance (ASD). Dosimetric differences were analyzed via paired t-tests. Results Compared to other methods, MDA-TransUnet achieved superior segmentation performance on the test dataset. The DSCs for the bladder, colon, rectum, small bowel, and HR-CTV were 94.54%, 79.27%, 79.27%, 88.90%, and 82.35%, respectively. Paired t-tests on five dosimetric metrics (D5cc, D2cc, D0.1cc, D90%, and Dmean) showed no significant differences. For OARs, the average difference in D2cc was less than 12%. For HR-CTV, the average difference in Dmean was less than 8%, and D90% was less than 11%. Conclusion This work demonstrates the superiority of MDA-TransUnet in segmenting OARs and HR-CTV for cervical cancer brachytherapy, with robust performance across multi-center datasets. deep learning high-dose-rate brachytherapy auto-segmentation cervical cancer image-guided Social Development Project of Jiangsu Provincial Key Research &amp; Development Plan No. BE2022720 Jiangsu Provincial Medical Key Discipline Construction Unit (Oncology Therapeutics (Radiotherapy)) No. JSDW202237 National Natural Science Foundation of China https://doi.org/10.13039/501100001809 No. 62371243 Changzhou Social Development Project No. CE20235063 Applied Basic Research Project of Changzhou No. CJ20244020 Natural Science Foundation of Jiangsu Province https://doi.org/10.13039/501100004608 No. BK20231190 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes typesetter ts19 cover-date January-December 2025 Introduction Cervical cancer ranks as the fourth most common malignant tumor among women worldwide, with over 600,000 new cases annually, approximately 90% of which occur in developing countries with limited healthcare resources. 1 Its high incidence and mortality rates pose a significant threat to women's health. Precision radiotherapy, particularly combining external beam radiation therapy (EBRT) and brachytherapy (BT), is a cornerstone in treating locally advanced cervical cancer. 2 However, the unique anatomical location of cervical tumors&#8212;where high-risk clinical target volumes (HR-CTV) are closely adjacent to organs at risk (OARs) such as the bladder and rectum&#8212;necessitates a delicate balance between tumor eradication and normal tissue sparing during treatment planning. Brachytherapy (BT), which involves placing radioactive sources directly into or near the tumor target, exploits the rapid dose fall-off with distance to deliver high local tumor doses while minimizing damage to OARs. 3 In cervical cancer treatment, BT has proven to significantly improve local control rates and patient survival. However, BT planning heavily relies on manual delineation of targets and OARs by physicians, leading to subjectivity, inefficiency, 4 and prolonged workflows. On average, radiation oncologists require 32&#8197;min to delineate HR-CTV and OARs for gynecological malignancies. 5 The demand for rapid yet precise planning creates high-pressure workflows prone to human error, while extended procedures exacerbate patient discomfort. In recent years, with the growing adoption of deep learning, various neural network architectures&#8212;primarily convolutional neural networks (CNNs)&#8212;have been developed for segmentation in cervical cancer high-dose-rate brachytherapy (HDR-BT). 6 &#8211; 9 The classic CNN-based segmentation model, U-Net, 10 and its variants employ symmetric encoder-decoder networks to automatically extract multi-level features through CNNs, significantly improving segmentation efficiency and accuracy. 11 &#8211; 14 For example, Li et al 15 applied the adaptive deep CNN framework nnU-Net to segment the bladder, rectum, and HR-CTV on cervical cancer CT images. The nnU-Net method integrates three architectures&#8212;2D U-Net, 3D U-Net, and 3D cascade U-Net&#8212;to adaptively select the optimal architecture for each task. Zhang et al 16 developed a DSD U-Net model for automated delineation of the bladder, rectum, sigmoid colon, small bowel, and HR-CTV, achieving high accuracy as evaluated by the Dice similarity coefficient (DSC). Chang et al 17 proposed a hybrid network combining 3D U-Net and long short-term memory (LSTM) for HR-CTV and OAR segmentation, demonstrating superior performance over 2D U-Net. Despite CNNs&#8217; strengths in capturing local spatial features, they face limitations in modeling long-range dependencies between pixels. 18 Transformer architectures, enhanced by self-attention mechanisms, overcome this limitation by enabling global context modeling, showing promising potential in medical image segmentation tasks. 19 &#8211; 23 To synergize the advantages of CNNs (local perception) and Transformers (global reasoning), hybrid CNN-Transformer architectures have emerged as a cutting-edge direction in medical image segmentation, exemplified by TransUNet. 24 Gu et al 25 pioneered the integration of Transformer's self-attention with CNN frameworks for segmenting the bladder, rectum, and colon, demonstrating significant effectiveness. However, their work focused solely on OARs segmentation in cervical cancer brachytherapy and did not extend the CNN-Transformer framework to HR-CTV segmentation. Although deep learning-based automatic segmentation methods show promise in cervical cancer brachytherapy, existing studies still exhibit certain limitations. Firstly, most research relies on single-center datasets, struggling to adequately validate model generalizability and robustness in multi-center scenarios, and failing to effectively address challenges such as variations in imaging protocols and inconsistencies in contouring criteria across different centers. 26 Secondly, existing CNN-Transformer hybrid architectures primarily focus on the segmentation of Organs at Risk and have not yet been applied to the automatic segmentation of the High-Risk Clinical Target Volume. The HR-CTV is characterized by indistinct boundaries, variable morphology, and close proximity to OARs, making its accurate segmentation crucial for treatment planning. 27 Therefore, there is an urgent need to develop a novel segmentation method capable of simultaneously achieving precise segmentation of both HR-CTV and OARs while maintaining stable performance on heterogeneous multi-center data. To address these challenges, this study proposes, for the first time, a CNN-Transformer hybrid network named MDA-TransUnet for segmenting both OARs and HR-CTV in cervical cancer brachytherapy. To validate the model's robustness across centers, this study integrated 122 cervical cancer brachytherapy patients&#8217; CT images across three clinical centers for training and testing, providing robust support for evaluating the model's performance in heterogeneous scenarios. This design significantly differs from previous single-center studies and aligns more closely with clinical practical needs. Furthermore, we introduced a Multi-scale Adaptive Spatial Attention Gate (MASAG) and a Deformable Convolutional Attention Module (DCAM) into the CNN-Transformer framework to further enhance adaptability to organ deformation and multi-center variations. Compared to manual contouring, MDA-TransUnet achieved an average segmentation time of approximately 1.2&#8197;min per patient for both HR-CTV and OARs in cervical cancer CT images, representing a 26-fold speed increase. This holds promise for significantly reducing patient waiting time post-applicator insertion in clinical practice, alleviating patient discomfort, and mitigating the risk of organ displacement associated with prolonged waiting periods. MDA-TransUnet thus offers a novel technical solution for cervical cancer brachytherapy image segmentation. Materials and Methods Dataset A total of 122 patients were enrolled in this Institutional Review Board (IRB)-approved retrospective study, including 52 patients from The Third Affiliated Hospital of Nanjing Medical University, 50 patients from The Affiliated Tumor Hospital of Nantong University, and 20 patients from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University. Among these, 102 patients&#8217; data (The Third Affiliated Hospital of Nanjing Medical University and The Affiliated Tumor Hospital of Nantong University) were utilized for training and testing, with 80 cases allocated for training and 22 cases for testing. 20 patients&#8217; data from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University served as an external validation set to further evaluate the model's generalization performance. This study was approved by the Medical Ethics Committee of The Third Affiliated Hospital of Nanjing Medical University (#2024KY213-01), the Medical Ethics Committee of The Affiliated Tumor Hospital of Nantong University (#2020-031), and the Medical Ethics Committee of The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University (#IIT2024101). Informed consent is waived for all participants with the approval of the Medical Ethics Committee. CT images were reconstructed using the Philips Brilliant Big Bore CT scanner (Philips Healthcare, Best, the Netherlands) with a matrix size of 512&#8201;&#215;&#8201;512 and a slice thickness of 3&#8197;mm. All patients were treated using tandem and ovoid applicators (T&#8201;+&#8201;O). The HR-CTV, bladder, colon, rectum, and small bowel were manually contoured by experienced radiation oncologists (with over ten years of clinical expertise) using Monaco 5.40.01 (Elekta, Stockholm, Sweden). The radiation oncologists contoured the HR-CTV on CT images according to ICRU Report 89, with reference to MRI images obtained prior to the first brachytherapy session. 28 Data Preprocessing The 3D-Slicer software and RT structure data were used to generate binary masks for HR-CTV and OARs for each patient. All binary masks were converted into one-hot encoded vectors with values ranging from 0 to 5. To mitigate overfitting, we implement strictly synchronized data augmentation: with 50% probability, applying random 90k&#176; rotation (k&#8712;{0,1,2,3}) followed by axial random flipping (horizontal/vertical); with 25% probability, performing random rotation between &#8722;20&#176; to 20&#176;; and with 25% probability, maintaining the identity transformation. Network Architecture In this section, we introduce our proposed MDA-TransUnet. Given the outstanding performance of TransUnet in medical image segmentation, we adopt TransUnet as the baseline model. For the encoder, we retain TransUnet's hybrid CNN-Transformer design: the CNN serves as a feature extractor to generate input feature maps, while the Transformer captures global and spatial relationships between features. The decoder consists of three key components: (1) UpConv blocks for feature upsampling, the Multi-Scale Adaptive Spatial Attention Gate (MASAG) to enhance feature representation, and the Deformable Convolutional Attention Module (DCAM) to robustly refine feature maps, as Figure 1 . Figure 1. MDA-TransUnet Network Architecture Diagram. (a) Overall Framework of MDA-TransUnet, (b) Multi-Scale Adaptive Spatial Attention Gate (MASAG), (c) Multi-Scale Feature Fusion (MSF), (d) Deformable Convolutional Attention Module (DCAM), (e) Spatial Attention (SA), (f) Deformable Convolutional Block (DCB), (g) UpConv. Multi-Scale Adaptive Spatial Attention Gate (MASAG) We employ the Multi-Scale Adaptive Spatial Attention Gate (MASAG) module to enhance feature representation. MASAG aims to effectively integrate multi-scale information and guide the aggregation of spatial features, thereby improving overall segmentation performance. 29 Through a four-stage collaborative process&#8212;Multi-scale Fusion (MSF), Spatial Selection (SS), Spatial Interaction and Cross-Modulation (SICM), and Recalibration (RC)&#8212;MASAG progressively optimizes feature representations, addressing limitations of traditional methods in cross-scale information fusion and spatial weight allocation. The MASAG framework comprises the following stages: Multi-Scale Feature Fusion (MSF) The encoder feature maps (X) and decoder feature maps (Y) are fused through local and global context extraction branches: Local Context Extraction: Utilizes Depthwise Separable Convolution and Dilated Convolution to extract local details from encoder features (X), as defined in Equation ( 1 ): (1) F l o c a l = C o n v 1 &#215; 1 ( D W &#8722; D ( D W ( X ) ) ) where DW denotes Depthwise Separable Convolution, composed of depthwise convolution and pointwise convolution. This architecture reduces computational costs and parameter counts while effectively extracting local features. DW-D represents Dilated Depthwise Separable Convolution, which incorporates dilation rates into the convolution kernel. This significantly expands the receptive field without increasing parameters, enabling the capture of spatial dependencies among local features across broader contexts. Global Context Extraction: Applies global average pooling and max pooling to the decoder feature maps (Y) to capture broad contextual information, as Equation ( 2 ): (2) F g l o b a l = C o n v 1 &#215; 1 ( [ C A v g ( Y ) , C M a x ( Y ) ] ) where C A v g and C M a x denote global average pooling and max pooling, respectively. Global Average Pooling obtains the overall mean response of the feature maps, reflecting the global feature distribution. Global Max Pooling captures the most salient features within the feature maps. Their combined output provides complementary global semantic information that is lacking in the decoder features. Feature Fusion: The local features extracted by the encoder are added to the global features provided by the decoder, generating the fused feature map U that simultaneously incorporates both local details and global information, as shown in Equation ( 3 ): (3) U = F l o c a l + F g l o b a l Spatial Selection (SS): The spatial selection module dynamically assigns spatial weights to highlight critical anatomical regions and suppress noise: Generates a two-channel weight map, as Equation ( 4 ): (4) S W i = S o f t m a x ( C o n v 1 &#215; 1 ( U ) ) , &#8704; i &#8712; [ 1 , 2 ] where S o f t m a x ( &#8901; ) represents the Softmax activation function, ensuring that the weights sum to 1 at each spatial location. By computing a dedicated weight at each pixel location for both encoder features and decoder features, indicating which feature holds greater importance for that specific position. Applies spatial weighting to encoder features (X) and decoder features (Y) separately, as Equation ( 5 ): (5) X &#8242; = S W 1 &#8855; X + X Y &#8242; = S W 2 &#8855; Y + Y where &#8855; denotes element-wise multiplication, S W 1 &#8855; X and S W 2 &#8855; Y representing the spatially-weighted versions of encoder features X and decoder features Y, respectively. The residual connection preserves original feature information while mitigating potential information loss during the weighting process. Spatial Interaction and Cross-Modulation (SICM) This module facilitates feature interaction across spatial locations and enables cross-channel modulation of multi-scale information: Encoder features are modulated by decoder global context, while decoder features are modulated by encoder local details, addressing the misalignment of low-level and high-level features in traditional U-Net skip connections, as Equation ( 6 ): (6) X &#8243; = X &#8242; &#8855; S i g m o i d ( Y &#8242; ) Y &#8243; = Y &#8242; &#8855; S i g m o i d ( X &#8242; ) where S i g m o i d ( &#8901; ) is the Sigmoid activation function. The global contextual information from Y&#8217; augments X&#8217; to yield X'&#8217;, while the detailed information from X&#8217; complements Y&#8217; to produce Y'&#8217;. This bidirectional enhancement ensures both X'&#8217; and Y'&#8217; incorporate integrated local and global information. Final fused features are integrated via element-wise multiplication, as Equation ( 7 ): (7) U &#8242; = X &#8243; &#8855; Y &#8243; The mutually modulated X'&#8217; and Y'&#8217; are fused via element-wise multiplication to generate the integrated feature U&#8217;. This multiplicative fusion more effectively highlights regions deemed significant by both feature maps. Recalibration (RC) Finally, the recalibration module adjusts feature map responses to emphasize meaningful information, as Equation ( 8 ): (8) X o u t = C o n v 1 &#215; 1 ( S i g m o i d ( C o n v 1 &#215; 1 ( U &#8242; ) ) &#8855; X ) This step refines the fused feature map through pointwise convolution, which is then activated by a sigmoid function to generate the attention map. Finally, the initial input X is recalibrated by performing element-wise multiplication with the attention map, followed by further processing through another pointwise convolution. The recalibrated feature map ( X o u t ) serves as the skip connection output for decoder feature fusion. Deformable Convolutional Attention Module (DCAM) We employ the Deformable Convolutional Attention Module (DCAM) to enhance the network's adaptability to irregular shapes, size variations, and geometric deformations in medical imaging, challenges particularly prominent in cervical cancer brachytherapy segmentation tasks. By integrating the context-aware feature refinement of the Spatial Attention (SA) mechanism and the dynamic geometric adaptation of the Deformable Convolution Block (DCB), DCAM overcomes the limitations of conventional convolutional operations in modeling anatomical diversity. The DCAM module combines SA and DCB, as defined in Equation ( 9 ): (9) D C A M ( x ) = D C B ( S A ( x ) ) where x is the input tensor. D C A M ( &#8901; ) denotes the Deformable Convolutional Attention Module. The DCAM module first applies a Spatial Attention mechanism to the input features, focusing on critical spatial regions. The output of SA is then processed by a Deformable Convolution Block, which leverages the dynamic sampling capability of deformable convolutions to adapt to organ geometric deformations. Spatial Attention (SA) The Spatial Attention mechanism identifies and amplifies critical spatial regions in feature maps, emphasizing high-signal areas (eg, HR-CTV boundaries) while suppressing low-contrast or irrelevant regions and noise interference, as formulated in Equation ( 10 ): (10) S A ( x ) = S i g m o i d ( C o n v ( [ C A v g ( x ) , C M a x ( x ) ] ) ) &#8855; x where C o n v ( &#8901; ) denotes a 7&#8201;&#215;&#8201;7 convolutional layer with padding of 3. The large 7&#8201;&#215;&#8201;7 kernel effectively captures broader contextual information and enhances spatial perception capabilities. This process first applies both average pooling and max pooling along the channel dimension to the input feature x, yielding two distinct feature maps. These feature maps are concatenated and processed through the large-kernel C o n v ( &#8901; ) to generate a Spatial Attention Map. Finally, a sigmoid activation function normalizes the attention weights to the [0,1] range. The normalized map is then multiplied element-wise with the original input x to achieve spatial re-weighting: regions with weights approaching 1 are enhanced while those approaching 0 are suppressed. Deformable Convolution Block (DCB) We introduce deformable convolution to further refine features generated by SA and enhance the network's adaptability to geometric deformations&#8212;crucial for addressing anatomical heterogeneity in multi-center datasets. Unlike the fixed grid sampling in traditional convolution, deformable convolution dynamically adjusts kernel sampling positions by learning offset parameters, enabling the kernel to adaptively &#8220;warp&#8221; and align with irregular anatomical contours (eg, HR-CTV or colon boundaries), as detailed in Equation ( 11 ): (11) D C B ( x ) = R e L U ( B N ( D C o n v ( R e L U ( B N ( D C o n v ( C o n v 1 &#215; 1 ( x ) ) ) ) ) ) ) where R e L U ( &#8901; ) is the ReLU activation layer, B N ( &#8901; ) denotes batch normalization, and D C o n v ( &#8901; ) represents deformable convolution. Upsampling (UpConv) The UpConv layer progressively upsamples the features of the current layer to align the dimensions with the subsequent skip connection. Each UpConv layer consists of an UpSampling U P ( &#8901; ) with scale-factor 2, a 3&#8201;&#215;&#8201;3 convolution C o n v ( &#8901; ) , a batch normalization B N ( &#8901; ) , and a ReLU activation layers, as formulated in Equation ( 12 ): (12) U p C o n v ( x ) = R e L U ( B N ( C o n v ( U P ( x ) ) ) ) Loss Function Our method employs a multi-scale supervision strategy and a hybrid loss function to optimize model performance and mitigate class imbalance. The model generates predictions at four decoder levels (p1, p2, p3, p4), with each level contributing to the loss calculation. This approach reduces reliance on single-scale features by integrating multi-scale contextual information. The loss at each level is a weighted combination of Cross-Entropy Loss and Dice Loss, as defined in Equation ( 13 ): (13) L l a y e r = 0.3 L C E + 0.7 L D i c e where Cross-Entropy Loss( L C E ) and Dice Loss( L D i c e ) are formulated as follows: (14) L C E = &#8722; &#8721; i = 1 N y i log ( p i ) (15) L D i c e = 1 &#8722; 2 &#8721; i = 1 N p i y i + &#1013; &#8721; i = 1 N p i + &#8721; i = 1 N y i + &#1013; Here, y i denotes the ground truth label, p i represents the predicted probability, N is the number of classes, and &#1013; is a smoothing factor to prevent division by zero. The final loss is the weighted sum of losses from all four decoder levels, with equal weights assigned ( &#945; = &#946; = &#947; = &#948; = 1 ) , as shown in Equation ( 16 ): (16) L t o t a l = &#945; L p 1 + &#946; L p 2 + &#947; L p 3 + &#948; L p 4 Evaluation Metrics Both geometric and dosimetric methods were employed for quantitative analysis. Geometric performance was assessed using the Dice similarity coefficient (DSC), defined as: (17) D S C = 2 | X &#8745; Y | | X | + | Y | (18) H D = max ( h ( A , B ) , h ( B , A ) ) h ( A , B ) = max b &#8712; B ( min a &#8712; A a &#8722; b ) (19) A S D ( X , Y ) = m e a n ( { B p r e d , B g t } ) B p r e d = { &#8704; p 1 &#8712; A p r e d , c l o e s t d i s t a n c e ( p 1 , p 2 ) | &#8707; p 2 &#8712; A g t } Here, smaller HD95 (95% Hausdorff Distance) and ASD (Average Surface Distance) indicate better shape agreement between segmentation results and ground truth contours, while a larger DSC (Dice Similarity Coefficient) reflects higher spatial overlap with the ground truth. For dosimetric comparisons, dose-volume indices (DVIs) were utilized. For HR-CTV, we focused on Dmean (mean dose to the target) and D90% (minimum dose covering 90% of the target volume). For OARs, we evaluated D5cc, D2cc, and D0.1cc, where DXcc denotes the minimum dose to the hottest X cubic centimeters (cc) of the organ. Paired sample t-tests were performed to compare dosimetric differences, with p&#8201;&lt;&#8201;0.05p&#8201;&lt;&#8201;0.05 indicating statistical significance. All statistical analyses were conducted using Python 3.8. Experiments and Results Experimental Details MDA-TransUnet was implemented in PyCharm on a computer equipped with an Intel &#174; Core&#8482; i7-10700 CPU and an NVIDIA GeForce RTX 3090 GPU. For fair comparison with other methods, all networks were trained under identical configurations. The model was optimized using the AdamW optimizer, which is better suited for complex multi-scale feature learning and improves convergence stability. The learning rate was adjusted via a cosine decay schedule, starting with an initial value of 0.0001 and a minimum of 1e-7. The batch size was set to 24, and training proceeded for 150 epochs. Geometric Metric Analysis As shown in Table 1 , our proposed MDA-TransUnet achieves the highest Dice Similarity Coefficient (DSC) across all five target regions compared to other methods. The mean DSCs for the bladder, colon, rectum, small bowel, and HR-CTV are 94.54%, 79.27%, 79.27%, 88.90%, and 82.35%, respectively. Notably, for the small bowel (88.90%) and HR-CTV (82.35%), our method significantly outperforms suboptimal approaches (Trans-CASCADE: 87.55% for small bowel; EMCAD: 81.49% for HR-CTV). Regarding boundary precision, our method achieves the lowest average HD95 (95% Hausdorff Distance) among all OARs. For the bladder, small bowel, and HR-CTV, HD95 values are significantly reduced compared to suboptimal methods, demonstrating enhanced boundary control. In terms of Average Surface Distance (ASD), MDA-TransUnet delivers optimal ASD values for all OARs except the colon, with the bladder and HR-CTV showing an 8.5% reduction in ASD versus the second-best method. Compared to the classic TransUnet, our method achieves DSC improvements of 5.95% (bladder), 14.7% (colon), 9.05% (rectum), 11.67% (small bowel), and 4.93% (HR-CTV). These results validate that the Multi-Scale Adaptive Spatial Attention Gate (MASAG) and Deformable Convolutional Attention Module (DCAM) effectively mitigate limitations of skip connections while enhancing the network's ability to model local details and geometric deformations. Furthermore, Figures 2 and 3 visually compare segmentation results, confirming that contours generated by our method exhibit superior agreement with radiation physicist-delineated ground truth in shape, volume, and spatial localization. Figure 2. Visual Comparison of our Method with Other Methods, Where red Indicates the Ground Truth and Green Represents AI-Based Segmentation Results. Subfigures Correspond to: (a) TransUnet, (b) DLKA-net, (c) SelfRag-Unet, (d) Unet, (e) Trans-CASCADE, (f) EMCAD, and (g) our Method. Figure 3. Visual Comparison of our Method Versus Other Approaches for HR-CTV Segmentation in Sagittal and Coronal Planes. Blue Contours Denote Ground Truth, While red Contours Represent AI-Generated Segmentations. Left Panels: Coronal Views; Right Panels: Sagittal Views. (a) TransUnet, (b) DLKA-Net, (c) SelfRag-UNet, (d) UNet, (e) Trans-CASCADE, (f) EMCAD, and (g) our Method. Table 1. Quantitative Comparison Between our Proposed Method and Other Methods, Where &#8220;our Study&#8221; Denotes our Approach. Metrics TransUnet 24 DLKA-net 30 SelfRag-Unet 31 Unet 10 Cascade 32 EMCAD 33 Our Study Bladder DSC 88.59&#8201;&#177;&#8201;2.79 92.50&#8201;&#177;&#8201;3.22 91.93&#8201;&#177;&#8201;2.56 91.98&#8201;&#177;&#8201;2.37 94.11&#8201;&#177;&#8201;1.97 94.20&#8201;&#177;&#8201;1.60 94.54 &#8201;&#177;&#8201; 1.54 HD95 3.38&#8201;&#177;&#8201;1.03 4.08&#8201;&#177;&#8201;4.85 2.56&#8201;&#177;&#8201;0.83 2.49&#8201;&#177;&#8201;0.79 1.90&#8201;&#177;&#8201;0.71 1.99&#8201;&#177;&#8201;0.69 1.80 &#8201;&#177;&#8201; 0.87 ASD 1.23&#8201;&#177;&#8201;0.40 1.38&#8201;&#177;&#8201;0.99 1.03&#8201;&#177;&#8201;0.47 0.91&#8201;&#177;&#8201;0.30 0.61&#8201;&#177;&#8201;0.27 0.59&#8201;&#177;&#8201;0.19 0.54 &#8201;&#177;&#8201; 0.21 Colon DSC 64.57&#8201;&#177;&#8201;7.73 70.25&#8201;&#177;&#8201;11.11 67.70&#8201;&#177;&#8201;10.01 68.46&#8201;&#177;&#8201;10.41 77.77&#8201;&#177;&#8201;8.00 78.46&#8201;&#177;&#8201;8.13 79.27 &#8201;&#177;&#8201; 8.05 HD95 19.08&#8201;&#177;&#8201;15.18 14.49&#8201;&#177;&#8201;16.83 15.92&#8201;&#177;&#8201;13.25 17.57&#8201;&#177;&#8201;16.83 11.86&#8201;&#177;&#8201;16.37 11.28 &#8201;&#177;&#8201; 14.80 11.66&#8201;&#177;&#8201;14.50 ASD 4.99&#8201;&#177;&#8201;3.35 4.00&#8201;&#177;&#8201;4.59 4.38&#8201;&#177;&#8201;3.75 4.86&#8201;&#177;&#8201;4.51 2.99&#8201;&#177;&#8201;4.61 2.45 &#8201;&#177;&#8201; 2.01 2.74&#8201;&#177;&#8201;4.09 Rectum DSC 70.22&#8201;&#177;&#8201;11.24 73.57&#8201;&#177;&#8201;7.00 74.76&#8201;&#177;&#8201;7.57 75.92&#8201;&#177;&#8201;7.41 78.80&#8201;&#177;&#8201;6.32 78.41&#8201;&#177;&#8201;5.96 79.27 &#8201;&#177;&#8201; 6.79 HD95 6.83&#8201;&#177;&#8201;3.03 6.86&#8201;&#177;&#8201;3.72 6.39&#8201;&#177;&#8201;3.07 5.70 &#8201;&#177;&#8201; 2.67 5.79&#8201;&#177;&#8201;3.62 6.02&#8201;&#177;&#8201;4.12 6.28&#8201;&#177;&#8201;4.24 ASD 1.93&#8201;&#177;&#8201;0.64 2.41&#8201;&#177;&#8201;1.37 1.91&#8201;&#177;&#8201;1.10 1.60&#8201;&#177;&#8201;0.74 1.45&#8201;&#177;&#8201;0.86 1.56&#8201;&#177;&#8201;0.75 1.35 &#8201;&#177;&#8201; 0.77 Small intestine DSC 77.23&#8201;&#177;&#8201;5.09 83.46&#8201;&#177;&#8201;4.93 80.64&#8201;&#177;&#8201;6.15 81.13&#8201;&#177;&#8201;5.98 87.55&#8201;&#177;&#8201;4.20 87.17&#8201;&#177;&#8201;3.86 88.90 &#8201;&#177;&#8201; 3.64 HD95 5.76&#8201;&#177;&#8201;1.73 4.34&#8201;&#177;&#8201;1.51 6.00&#8201;&#177;&#8201;3.27 5.58&#8201;&#177;&#8201;2.74 3.16&#8201;&#177;&#8201;1.52 3.40&#8201;&#177;&#8201;2.13 2.97 &#8201;&#177;&#8201; 1.44 ASD 1.70&#8201;&#177;&#8201;0.46 1.49&#8201;&#177;&#8201;0.52 1.89&#8201;&#177;&#8201;0.89 1.72&#8201;&#177;&#8201;0.79 0.92&#8201;&#177;&#8201;0.38 0.96&#8201;&#177;&#8201;0.41 0.87 &#8201;&#177;&#8201; 0.48 HR-CTV DSC 77.42&#8201;&#177;&#8201;5.18 77.71&#8201;&#177;&#8201;4.76 79.69&#8201;&#177;&#8201;4.32 79.15&#8201;&#177;&#8201;4.47 80.72&#8201;&#177;&#8201;4.02 81.49&#8201;&#177;&#8201;4.34 82.35 &#8201;&#177;&#8201; 4.07 HD95 5.06&#8201;&#177;&#8201;1.53 4.60&#8201;&#177;&#8201;1.60 4.43&#8201;&#177;&#8201;1.84 4.53&#8201;&#177;&#8201;1.89 4.05&#8201;&#177;&#8201;1.56 3.99&#8201;&#177;&#8201;1.70 3.77 &#8201;&#177;&#8201; 1.58 ASD 1.94&#8201;&#177;&#8201;0.64 1.82&#8201;&#177;&#8201;0.54 1.57&#8201;&#177;&#8201;0.44 1.64&#8201;&#177;&#8201;0.50 1.52&#8201;&#177;&#8201;0.52 1.41&#8201;&#177;&#8201;0.56 1.29 &#8201;&#177;&#8201; 0.40 Ablation Study Ablation studies were conducted on the dataset to evaluate the effectiveness of different components in our proposed network. We incrementally removed key modules (MASAG and DCAM) and compared the results. As clearly demonstrated in Table 2 , incorporating both MASAG and DCAM modules significantly enhances performance, particularly for HR-CTV segmentation, mitigating performance degradation caused by inter-center variations in contouring protocols and imaging parameters. The MASAG module improves feature representation quality and enhances cross-scale information fusion accuracy, thereby providing more reliable and focused feature inputs to DCAM. The deformable convolution operations in DCAM enable precise geometric adaptation at critical regions emphasized by MASAG (eg, organ boundaries). The resulting deformation-adapted features subsequently deliver more anatomically realistic information to MASAG during later decoding stages. This cascaded processing creates powerful synergies, proving particularly effective when segmenting challenging structures such as morphologically variable colons and boundary-ambiguous HR-CTV. The introduction of MASAG and DCAM collectively strengthens the model's capacity to capture anatomical details, accommodate multi-center heterogeneity, and handle geometric variations, achieving superior performance across all segmentation regions. Table 2. Ablation Study. Components Segmentation Targets MASAG DCAM Bladder Colon Rectum Small Intestine HR-CTV NO NO 93.37 76.84 77.97 86.38 79.85 YES NO 93.96 77.39 78.25 87.10 80.94 NO YES 94.39 78.54 78.47 87.90 80.92 YES YES 94 . 54 79 . 27 79 . 27 88 . 90 82 . 35 Dosimetric Analysis To evaluate dosimetric accuracy, we compared dose-volume metrics between manual and deep learning-based methods using paired sample t-tests, as summarized in Table 3 . No statistically significant differences were observed across all dosimetric parameters. For OARs, the average differences in D2cc and D0.1cc were less than 12% and 15%, respectively. For HR-CTV, the average differences in Dmean and D90% were below 8% and 11%, respectively. Figure 4 illustrates dose distributions for a representative case, comparing our automated segmentation method with manual delineation. Figure 4. Delineation of HR-CTV and OARs on CT Slices and Corresponding Dose Distribution Results. Blue Regions Represent the Standard Manual Contours; Green Lines Denote the Contours Segmented by our Method, and Colored Lines Indicate Dose Distributions. Table 3. Differences in Dosimetric Parameters Between Manual Methods and our Method Within the Original Clinical Brachytherapy (BT) Plans, Along with Paired t-Test Results. Structure Dosimetric Parameters Differences P Bladder D 5cc 0.1951&#8201;&#177;&#8201;0.1607 0.388 D 2cc 0.2826&#8201;&#177;&#8201;0.2277 0.435 D 0.1cc 0.6186&#8201;&#177;&#8201;0.5103 0.505 Colon D 2cc 0.3173&#8201;&#177;&#8201;0.4511 0.614 D 0.1cc 0.4752&#8201;&#177;&#8201;0.6740 0.507 Rectum D 5cc 0.0975&#8201;&#177;&#8201;0.0756 0.962 D 2cc 0.1160&#8201;&#177;&#8201;0.0695 0.158 D 0.1cc 0.3238&#8201;&#177;&#8201;0.2612 0.234 Small intestine D 2cc 0.5215&#8201;&#177;&#8201;0.6269 0.704 D 0.1cc 1.1644&#8201;&#177;&#8201;1.4556 0.571 HR-CTV D 90% 0.6367&#8201;&#177;&#8201;0.5573 0.081 &#12288; D mean 0.9659&#8201;&#177;&#8201;0.8036 0.470 External Validation To evaluate the model's generalization capability on unseen data from medical institutions, this study additionally incorporated 20 cervical cancer brachytherapy patients&#8217; CT images from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University for independent testing (exclusively for external validation, not involved in training). As demonstrated in Table 4 , MDA-TransUnet maintained optimal performance in the external validation set, achieving the highest DSC and HD95 values across all five target regions. Significant improvements were observed particularly for more challenging OARs (colon, rectum, and small bowel) compared to suboptimal methods. Although yielding second-best ASD values for the bladder and colon, it achieved the optimal average ASD across all target regions. These results demonstrate that the synergistic interaction between MASAG and DCAM effectively mitigates impacts caused by inter-center scanner variations and annotation discrepancies, confirming MDA-TransUnet's superior generalization capability. Table 4. Performance Comparison among Different Methods in External Validation, with &#8220;our Study&#8221; Denoting our Proposed Method. Metrics TransUnet 24 DLKA-net 30 SelfRag-Unet 31 Unet 10 Cascade 32 EMCAD 33 Our Study Bladder DSC 86.08&#8201;&#177;&#8201;4.60 88.66&#8201;&#177;&#8201;4.67 87.27&#8201;&#177;&#8201;4.35 87.19&#8201;&#177;&#8201;4.83 91.43&#8201;&#177;&#8201;3.36 92.40&#8201;&#177;&#8201;3.07 92.52 &#8201;&#177;&#8201; 2.92 HD95 2.92&#8201;&#177;&#8201;0.71 3.34&#8201;&#177;&#8201;2.61 4.22&#8201;&#177;&#8201;5.83 2.88&#8201;&#177;&#8201;0.88 1.68&#8201;&#177;&#8201;0.49 1.58&#8201;&#177;&#8201;0.57 1.52 &#8201;&#177;&#8201; 0.51 ASD 1.1&#8201;&#177;&#8201;0.25 1.31&#8201;&#177;&#8201;1.45 1.30&#8201;&#177;&#8201;1.00 1.16&#8201;&#177;&#8201;0.39 0.59&#8201;&#177;&#8201;0.18 0.50 &#8201;&#177;&#8201; 0.14 0.51&#8201;&#177;&#8201;0.18 Colon DSC 58.58&#8201;&#177;&#8201;11.54 64.90&#8201;&#177;&#8201;11.07 62.90&#8201;&#177;&#8201;9.78 61.61&#8201;&#177;&#8201;10.46 73.75&#8201;&#177;&#8201;8.03 73.87&#8201;&#177;&#8201;7.34 75.58 &#8201;&#177;&#8201; 7.98 HD95 15.31&#8201;&#177;&#8201;8.79 12.81&#8201;&#177;&#8201;6.35 14.09&#8201;&#177;&#8201;7.37 14.71&#8201;&#177;&#8201;8.57 9.67&#8201;&#177;&#8201;5.78 11.59&#8201;&#177;&#8201;6.46 8.86 &#8201;&#177;&#8201; 5.08 ASD 3.70&#8201;&#177;&#8201;2.48 3.05&#8201;&#177;&#8201;2.09 3.81&#8201;&#177;&#8201;2.04 3.83&#8201;&#177;&#8201;2.33 1.73 &#8201;&#177;&#8201; 1.05 2.17&#8201;&#177;&#8201;0.99 1.98&#8201;&#177;&#8201;1.06 Rectum DSC 68.86&#8201;&#177;&#8201;7.64 75.37&#8201;&#177;&#8201;8.40 74.59&#8201;&#177;&#8201;9.52 73.73&#8201;&#177;&#8201;10.18 78.59&#8201;&#177;&#8201;7.51 77.80&#8201;&#177;&#8201;7.20 79.62 &#8201;&#177;&#8201; 8.96 HD95 5.94&#8201;&#177;&#8201;1.96 5.48&#8201;&#177;&#8201;4.16 4.41&#8201;&#177;&#8201;1.69 4.13&#8201;&#177;&#8201;1.80 4.66&#8201;&#177;&#8201;3.39 5.17&#8201;&#177;&#8201;3.19 3.97 &#8201;&#177;&#8201; 3.54 ASD 2.17&#8201;&#177;&#8201;0.58 2.20&#8201;&#177;&#8201;1.21 1.49&#8201;&#177;&#8201;0.75 1.68&#8201;&#177;&#8201;1.19 1.33&#8201;&#177;&#8201;0.88 1.42&#8201;&#177;&#8201;1.01 1.32 &#8201;&#177;&#8201; 1.09 Small intestine DSC 76.11&#8201;&#177;&#8201;3.92 81.53&#8201;&#177;&#8201;3.46 78.49&#8201;&#177;&#8201;4.96 78.12&#8201;&#177;&#8201;4.67 86.17&#8201;&#177;&#8201;2.58 86.01&#8201;&#177;&#8201;2.49 87.76 &#8201;&#177;&#8201; 2.48 HD95 5.05&#8201;&#177;&#8201;1.24 3.82&#8201;&#177;&#8201;1.46 4.85&#8201;&#177;&#8201;1.46 5.46&#8201;&#177;&#8201;1.55 2.91&#8201;&#177;&#8201;0.86 2.93&#8201;&#177;&#8201;0.99 2.67 &#8201;&#177;&#8201; 0.78 ASD 1.58&#8201;&#177;&#8201;0.55 1.23&#8201;&#177;&#8201;0.35 1.75&#8201;&#177;&#8201;0.42 1.97&#8201;&#177;&#8201;0.54 0.90&#8201;&#177;&#8201;0.27 0.92&#8201;&#177;&#8201;0.20 0.84 &#8201;&#177;&#8201; 0.25 HR-CTV DSC 73.09&#8201;&#177;&#8201;6.96 72.99&#8201;&#177;&#8201;7.09 76.30&#8201;&#177;&#8201;5.06 75.60&#8201;&#177;&#8201;5.51 76.76&#8201;&#177;&#8201;5.65 78.26&#8201;&#177;&#8201;4.70 78.88 &#8201;&#177;&#8201; 4.53 HD95 5.68&#8201;&#177;&#8201;3.40 4.91&#8201;&#177;&#8201;1.96 4.92&#8201;&#177;&#8201;3.67 4.87&#8201;&#177;&#8201;3.25 4.67&#8201;&#177;&#8201;2.66 4.64&#8201;&#177;&#8201;2.13 4.33 &#8201;&#177;&#8201; 2.25 ASD 2.12&#8201;&#177;&#8201;1.15 1.88&#8201;&#177;&#8201;0.71 1.62&#8201;&#177;&#8201;0.89 1.63&#8201;&#177;&#8201;0.76 1.66&#8201;&#177;&#8201;0.86 1.42&#8201;&#177;&#8201;0.45 1.40 &#8201;&#177;&#8201; 0.60 Model Parameters As presented in Table 5 , MDA-TransUnet exhibits higher parameter counts and GPU memory consumption compared to other models, primarily attributed to increased computational complexity from the MASAG and DCAM modules, along with inherent requirements of the TransUnet encoder architecture. However, this computational overhead delivers substantial performance gains&#8212;particularly in handling multi-center data where it demonstrates superior generalization capability. While UNet and SelfRag-UNet achieve the lowest parameter/GPU footprints, their segmentation accuracy lags significantly behind MDA-TransUnet. Regarding inference speed, MDA-TransUnet matches similarly-sized models (TransUnet and DLKA-Net) while outperforming the smaller EMCAD architecture. Crucially, the 26-fold acceleration in segmentation reduces patient waiting time, mitigates organ displacement risks, and alleviates patient discomfort&#8212;demonstrating substantial clinical significance. Table 5. Model Parameters Comparison. Model Params(M) GPU(MiB) inference(Min) TransUnet 105.27 11726 1.18 DLKA-net 101.64 18636 1.14 SelfRag-Unet 17.26 9034 1.42 Unet 17.26 8740 1.38 Cascade 108.31 13846 1.00 EMCAD 26.76 10102 1.23 Our Study 110.94 17500 1.18 Discussion The combination of external beam radiation therapy and high-dose-rate brachytherapy represents the standard of care for gynecologic cancer treatment, where HDR-BT has proven indispensable and strongly correlates with improved survival rates. Compared to conventional EBRT, brachytherapy's defining feature is its ability to deliver higher radiation doses to tumor regions near the radiation source while effectively sparing normal organs due to rapid dose fall-off. However, brachytherapy faces unique challenges. In HDR-BT, treatment planning must be completed rapidly after applicator insertion, often under time constraints that may introduce human errors. In recent years, deep learning-based methods have emerged as promising solutions to automate workflows, reduce patient wait times, and enhance comfort. The purpose of this study is to investigate deep learning-based automatic segmentation methods for cervical cancer brachytherapy CT images. The proposed MDA-TransUnet achieves superior Dice Similarity Coefficient (DSC) across all target regions compared to other methods ( Table 1 ). Our method performs best in bladder segmentation, largely attributed to its relatively regular anatomical structure and high contrast, facilitating clear boundary feature extraction. Compared to the bladder, the rectum, colon, and small bowel present greater segmentation challenges due to their anatomical and imaging characteristics. While the rectum maintains relatively stable positioning, its low contrast on CT scans makes boundary delineation difficult. The colon presents exceptional segmentation challenges due to its characteristically low contrast against surrounding adipose and soft tissues in CT images, combined with high deformability in both shape and position. The small bowel's position is substantially influenced by respiratory motion, peristalsis, and bladder/rectal filling status, while its appearance on CT images often blends with other soft tissues, complicating segmentation. Despite these challenges, MDA-TransUnet achieved the highest DSC scores for all three organs: 79.27% for colon, 79.27% for rectum, and 88.90% for small bowel. Gu et al 25 first combined CNN with Transformer in MFFUNet for segmenting brachytherapy OARs (bladder, colon, rectum), reporting DSCs of 92.65%, 61.86%, and 66.55% respectively. Comparatively, our MDA-TransUnet demonstrates significantly better performance on colon and rectum segmentation. In future work, we will explore more effective contrast enhancement techniques to further improve segmentation performance for both the colon and rectum. Additionally, the HR-CTV segmentation achieved 82.35% DSC, outperforming other methods despite variations in delineation protocols across centers. Ablation studies ( Table 2 ) demonstrate that incorporating MASAG and DCAM modules significantly enhances performance across all target regions. Particularly for structurally complex areas like colon, small bowel, and HR-CTV, DSC improved by an average of 2.48%. This validates the modules&#8217; effectiveness in enhancing anatomical detail capture while mitigating cross-center variability, ensuring robust performance in multi-center scenarios. Beyond geometric metrics, dosimetric evaluation remains crucial for automated segmentation. 34 Paired t-tests ( Table 3 ) confirmed no statistically significant differences between our automated method and manual delineation across five dosimetric parameters: D5cc, D2cc, D0.1cc, Dmean, and D90%. Notably, despite poorer geometric accuracy in colon segmentation, dosimetric agreement with manual methods remained high, consistent with Wang et al's findings. 35 Dosimetric parameters like D2cc (representing the minimum dose received by the maximally irradiated 2cm3 of a volume) primarily depend on segmentation accuracy in high-dose regions (typically near applicators or targets). Sufficient overlap between automatic segmentation and ground truth in these critical areas can yield comparable dosimetric outcomes, even when contour discrepancies exist in low-dose regions. The Dice Similarity Coefficient (DSC) measures global volume overlap - shape variations in geometrically complex structures within low-dose, non-critical regions may reduce DSC without materially affecting dosimetry. Consequently, both geometric and dosimetric metrics are essential for comprehensive evaluation of automatic segmentation methods. Nevertheless, continuous optimization for improved geometric accuracy (particularly in high-dose regions) remains crucial, potentially further minimizing dosimetric differences while enhancing clinicians&#8217; trust in automated results and overall model reliability. To further assess generalization capability, 20 cervical cancer brachytherapy patients&#8217;CT images from The Affiliated Huaian NO.1 People's Hospital of Nanjing Medical University were independently tested (external validation). As shown in Table 4 , MDA-TransUnet achieved optimal DSC and HD95 values across all five targets (bladder, colon, rectum, small bowel, HR-CTV) on this unseen dataset. For the challenging colon segmentation, it outperformed suboptimal methods by 1.74% in DSC and reduced HD95 by 0.81&#8197;mm. These results demonstrate robust adaptability to multi-center heterogeneity (eg, varying CT protocols and contouring practices). Though yielding suboptimal ASD for bladder and colon, the model achieved optimal average ASD across all targets, confirming boundary segmentation accuracy. Compared to other models, MDA-TransUnet exhibits superior performance stability and generalization capability on independent cross-center data. Our approach - incorporating data augmentation, multi-center training, and robustness-enhancing modules (MASAG, DCAM) - effectively mitigates inter-center annotation variations. Future efforts should establish refined contouring guidelines and cross-institutional consistency reviews to standardize ground truth. This study has limitations. First, obtaining fully annotated multi-center cervical cancer brachytherapy CT datasets remains challenging, with inter-center variations in region-of-interest delineation. Second, the dataset remains relatively limited. Though multi-center data and independent external validation were employed, small sample sizes may introduce potential bias and constrain model learning capacity. Finally, while achieving excellent segmentation performance, MDA-TransUnet's high parameter count and computational complexity may hinder clinical deployment in resource-constrained settings. Future work will: (1) Expand datasets with multi-center cases, (2) Develop lightweight model compression strategies to enhance computational efficiency, and (3) Optimize clinical applicability. Conclusions This study proposes MDA-TransUnet, a deep learning method leveraging a CNN-Transformer hybrid architecture to automate the segmentation of HR-CTV and OARs in planning CT images for cervical cancer brachytherapy. Evaluation results demonstrate that our method outperforms other state-of-the-art (SOTA) approaches and exhibits no statistically significant differences from manual delineation across five dosimetric metrics. The proposed automated segmentation framework facilitates the automation of cervical cancer brachytherapy workflows, enhances treatment consistency, reduces post-applicator-insertion waiting times, and alleviates patient discomfort. Acknowledgements Not applicable. ORCID iDs: Dezheng Cao https://orcid.org/0009-0001-1064-1034 Heng Zhang https://orcid.org/0000-0002-2019-0038 Qianjia Huang https://orcid.org/0009-0005-1320-4824 Xinye Ni https://orcid.org/0000-0002-2402-9719 Ethics Approval and Consent to Participate: This study was approved by the Medical Ethics Committee of Changzhou No.2 People's Hospital Affiliated to Nanjing Medical University (Approval number: 2024KY213-01), the Medical Ethics Committee of Tumor Hospital Affiliated to Nantong University (Approval number: 2020-031) and the Third Affiliated Hospital of Nanjing Medical University (Approval number: ITT2024101). The requirement for informed consent was waived for all participants with the approval of the Medical Ethics Committees. The research was conducted in accordance with the principles embodied in the Declaration of Helsinki and local statutory requirements. All authors have reviewed research data. Consent for Publication: Not applicable. Author Contributions: CDZ conceived the study, designed the experiments, analyzed the data and wrote the manuscript. Data were collected by JJH, HJH, YB, STT, KY. NXY edited the manuscript. LC, QCJ, and XK supervised the data analysis. NXY reviewed literature, contributed to the manuscript and acquired the financial support for the project. ZH, SLT, HQJ, and CDZ performed the statistical analyses. All the authors accessed the study data and reviewed and approved the final manuscript. Funding: The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work is supported by the National Natural Science Foundation of China (No.62371243), Jiangsu Provincial Medical Key Discipline Cultivation Unit of Oncology Therapeutics (Radiotherapy) (No. JSDW202237), Changzhou Social Development Program(Nos. CE20235063 and CJ20244020), and Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province (No. JX13614239). The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. The datasets generated and/or analyzed during the current study are not publicly available due to protection of individual patient privacy and the use of an in-house software but are available from the corresponding author on reasonable request. Abbreviations HR-CTV High-risk clinical target volume OARs Organs at risk MDA-TransUnet Multi-Scale convolutional attention transunet CNN Convolutional neural networks CT Computed tomography DSC Dice similarity coefficient HD95 95% Hausdorff distance ASD Average surface distance EBRT External beam radiation therapy BT brachytherapy LSTM Long short-term memory MASAG Multi-scale adaptive spatial attention gate DCAM Deformable convolutional attention module HU Hounsfield Units MSF Multi-scale feature fusion SS Spatial selection SICM Spatial interaction and cross-modulation RC Recalibration SA Spatial attention DCB Deformable convolution block DVIs Dose-volume indices References 1 Sung H Ferlay J Siegel RL , et al. Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries . CA Cancer J Clin. 2021 ; 71 ( 3 ): 209 - 249 . 33538338 10.3322/caac.21660 2 Chino J Annunziata CM Beriwal S , et al. Radiation therapy for cervical cancer: Executive summary of an ASTRO clinical practice guideline . Pract Radiat Oncol. 2020 ; 10 ( 4 ): 220 - 234 . 32473857 10.1016/j.prro.2020.04.002 PMC8802172 3 Han K Milosevic M Fyles A , et al. Trends in the utilization of brachytherapy in cervical cancer in the United States . Int J Radiation Oncol* Biol* Phys . 2013 ; 87 ( 1 ): 111 - 119 . 10.1016/j.ijrobp.2013.05.033 23849695 4 Riegel AC Antone JG Zhang H , et al. Deformable image registration and interobserver variation in contour propagation for radiation therapy planning . J Appl Clin Med Phys. 2016 ; 17 ( 3 ): 347 - 357 . 27167289 10.1120/jacmp.v17i3.6110 PMC5690939 5 Fujimoto DK von Eyben R Usoz M , et al. Improving brachytherapy efficiency with dedicated dosimetrist planners . Brachytherapy . 2019 ; 18 ( 1 ): 103 - 107 . 30391061 10.1016/j.brachy.2018.10.003 6 Boldrini L Bibault JE Masciocchi C , et al. Deep learning: A review for the radiation oncologist . Front Oncol. 2019 ; 9 : 977 . 31632910 10.3389/fonc.2019.00977 PMC6779810 7 Meyer P Noblet V Mazzara C , et al. Survey on deep learning for radiotherapy . Comput Biol Med. 2018 ; 98 : 126 - 146 . 29787940 10.1016/j.compbiomed.2018.05.018 8 Sahiner B Pezeshk A Hadjiiski LM , et al. Deep learning in medical imaging and radiation therapy . Med Phys. 2019 ; 46 ( 1 ): e1 - e36 . 10.1002/mp.13264 PMC9560030 30367497 9 Shi J Chen J He G , et al. Artificial intelligence in high-dose-rate brachytherapy treatment planning for cervical cancer: A review . Front Oncol. 2025 ; 15 : 1507592 . 39931087 10.3389/fonc.2025.1507592 PMC11808022 10 Ronneberger O Fischer P Brox T. U-net: convolutional networks for biomedical image segmentation . Medical image computing and computer-assisted intervention&#8211;MICCAI 2015: 18th international conference , Munich, Germany , October 5-9, 2015, proceedings, part III 18. Springer international publishing , 2015 : p. 234 - 241 . 11 Zhou Z Rahman Siddiquee MM Tajbakhsh N , et al. Unet++: a nested u-net architecture for medical image segmentation . In Deep learning in medical image analysis and multimodal learning for clinical decision support: 4th international workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in conjunction with MICCAI 2018 , Granada, Spain , September 20, 2018, proceedings 4. Springer International Publishing , 2018 : p. 3 - 11 . 10.1007/978-3-030-00889-5_1 PMC7329239 32613207 12 Huang H Lin L Tong R , et al. Unet 3+: a full-scale connected unet for medical image segmentation . In ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE , 2020 : p. 1055 - 1059 . 13 Lou A Guan S Loew M. DC-UNet: rethinking the U-net architecture with dual channel efficient CNN for medical image segmentation . In Medical Imaging 2021: Image Processing. SPIE , 2021 , 11596 : p. 758 - 768 . 14 Isensee F Jaeger PF Kohl SAA , et al. nnU-Net: A self-configuring method for deep learning-based biomedical image segmentation . Nat Methods. 2021 ; 18 ( 2 ): 203 - 211 . 33288961 10.1038/s41592-020-01008-z 15 Li Z Zhu Q Zhang L , et al. A deep learning-based self-adapting ensemble method for segmentation in gynecological brachytherapy . Radiat Oncol. 2022 ; 17 ( 1 ): 152 . 36064571 10.1186/s13014-022-02121-3 PMC9446699 16 Zhang D Yang Z Jiang S , et al. Automatic segmentation and applicator reconstruction for CT-based brachytherapy of cervical cancer using 3D convolutional neural networks . J Appl Clin Med Phys. 2020 ; 21 ( 10 ): 158 - 169 . 10.1002/acm2.13024 PMC7592978 32991783 17 Chang JH Lin KH Wang TH , et al. Image segmentation in 3D brachytherapy using convolutional LSTM . J Med Biol Eng. 2021 ; 41 ( 5 ): 636 - 651 . 18 Cao H Wang Y Chen J , et al. Swin-unet: unet-like pure transformer for medical image segmentation . In European Conference on Computer Vision . Springer Nature Switzerland , 2022 : pp. 205 - 218 . 19 Dosovitskiy A Beyer L Kolesnikov A , et al. An image is worth 16&#8201;&#215;&#8201;16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020 . 20 Dong B Wang W Fan DP , et al. Polyp-pvt: Polyp segmentation with pyramid vision transformers. arXiv preprint arXiv:2108.06932 , 2021 . 21 Wang J Huang Q Tang F , et al. Stepwise feature fusion: local guides global . In International Conference on Medical Image Computing and Computer-Assisted Intervention . Springer Nature Switzerland , 2022 : 110 - 120 . 22 Wang H Cao P Wang J , et al. Uctransnet: Rethinking the skip connections in u-net from a channel-wise perspective with transformer . Proce AAAI Conf Artif Intell . 2022 ; 36 ( 3 ): 2441 - 2449 . 23 Zhang Y Liu H Hu Q. Transfuse: fusing transformers and cnns for medical image segmentation . In Medical image computing and computer assisted intervention&#8211;MICCAI 2021: 24th international conference , Strasbourg, France , September 27&#8211;October 1, 2021, proceedings, Part I 24. Springer International Publishing, 2021 : p. 14 - 24 . 24 Chen J Lu Y Yu Q , et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 , 2021 . 25 Gu Y Guo H Zhang J , et al. MFFUNet: A hybrid model with cross-attention-guided multi-feature fusion for automated segmentation of organs at risk in cervical cancer brachytherapy . Comput Med Imaging Graph . 2025 ; 124 : 102571 . 40441081 10.1016/j.compmedimag.2025.102571 26 Zhang Z Zhao T Gay H , et al. Weaving attention U-net: A novel hybrid CNN and attention-based method for organs-at-risk segmentation in head and neck CT images . Med Phys. 2021 ; 48 ( 11 ): 7052 - 7062 . 34655077 10.1002/mp.15287 27 Liu Z Liu X Guan H , et al. Development and validation of a deep learning algorithm for auto-delineation of clinical target volume and organs at risk in cervical cancer radiotherapy . Radiother Oncol. 2020 ; 153 : 172 - 179 . 33039424 10.1016/j.radonc.2020.09.060 28 Swamidas J Mahantshetty U. ICRU report 89: prescribing, recording, and reporting brachytherapy for cancer of the cervix . 2017 . 29 Kolahi SG Chaharsooghi SK Khatibi T , et al. MSA $^ 2$ Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation. arXiv preprint arXiv:2407.21640 , 2024 . 30 Azad R Niggemeier L H&#252;ttemann M , et al. Beyond self-attention: deformable large kernel attention for medical image segmentation . Proceedings of the IEEE/CVF winter conference on applications of computer vision . 2024 : p. 1287 - 1297 . 31 Zhu W Chen X Qiu P , et al. Selfreg-unet: self-regularized unet for medical image segmentation . In International Conference on Medical Image Computing and Computer-Assisted Intervention . Springer Nature Switzerland , 2024 : p. 601 - 611 . 10.1007/978-3-031-72111-3_56 PMC12408486 40917447 32 Rahman MM Marculescu R. Medical image segmentation via cascaded attention decoding . In Proceedings of the IEEE/CVF winter conference on applications of computer vision . 2023 : p. 6222 - 6231 . 33 Rahman MM Munir M Marculescu R. Emcad: efficient multi-scale convolutional attention decoding for medical image segmentation . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2024 : p. 11769 - 11779 . 34 Yoganathan SA Paul SN Paloor S , et al. Automatic segmentation of magnetic resonance images for high-dose-rate cervical cancer brachytherapy using deep learning . Med Phys. 2022 ; 49 ( 3 ): 1571 - 1584 . 35094405 10.1002/mp.15506 35 Wang J Chen Y Tu Y , et al. Evaluation of auto-segmentation for brachytherapy of postoperative cervical cancer using deep learning-based workflow . Physics in Medicine &amp; Biology . 2023 ; 68 ( 5 ): 055012 . 10.1088/1361-6560/acba76 36753762"
}