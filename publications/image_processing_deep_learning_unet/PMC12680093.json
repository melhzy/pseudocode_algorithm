{
  "pmcid": "PMC12680093",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:26.470923",
  "metadata": {
    "journal_title": "IEEE access : practical innovations, open solutions",
    "journal_nlm_ta": "IEEE Access",
    "journal_iso_abbrev": "IEEE Access",
    "journal": "IEEE access : practical innovations, open solutions",
    "pmcid": "PMC12680093",
    "pmid": "41357810",
    "doi": "10.1109/access.2025.3627777",
    "title": "A Multimodal Adaptive Inter-Region Attention-Guided Network for Brain Tumor Classification",
    "year": "2025",
    "month": "10",
    "day": "31",
    "pub_date": {
      "year": "2025",
      "month": "10",
      "day": "31"
    },
    "authors": [
      "ABDELHALIEM IBRAHIM",
      "DIXON JOSE",
      "ABDELHAMID ABEER",
      "SALEH GEHAD A.",
      "KHALIFA FAHMI"
    ],
    "abstract": "Accurate brain tumor classification is critical for ensuring timely and effective medical interventions. In recent years, artificial intelligence (AI)-driven diagnostic systems have emerged as transformative tools that optimize the classification process and enable rapid, objective decision-making. However, existing methods often suffer from limitations such as the loss of high-frequency details during multimodal preprocessing, inadequate cross-modal feature alignment, and insufficient focus on shared tumor regions within 3D architectures. To address these challenges, this study introduces a novel AI-based framework for advanced brain tumor classification. Specifically, we propose a multimodal magnetic resonance imaging (MRI) architecture that integrates Diffusion-Weighted MRI (DW-MRI) and T2-weighted MRI (T2-MRI) modalities, uniquely combining them in a dual-branch 3D neural architecture with advanced preprocessing and attention mechanisms. The preprocessing pipeline employs a learnable High-Frequency Information Retention (HFIR) technique to resize T2-MRI images, maintaining consistent spatial dimensions across modalities while preserving essential image details. The architecture utilizes dual-branch 3D convolutional neural networks (CNN) for modality-specific feature extraction, enhanced by a novel Adaptive Region Attention (ARA) module that dynamically aligns and emphasizes highly informative regions shared across modalities, providing deeper and more consistent insights into tumor characteristics. Rigorous evaluation on a dataset of brain MRI scans including three tumor classes demonstrates that the proposed framework achieves overall accuracy, sensitivity, and specificity of 92.86%, 80.00%, and 94.12%, respectively. Statistical analyses using bootstrap-resampled F1-scores confirm significant outperformance over other state-of-the-art models, underscoring its robust and interpretable potential for precise brain tumor diagnosis.",
    "keywords": [
      "Brain tumor",
      "multimodal",
      "information retention",
      "adaptive region attention",
      "DW-MRI",
      "T2-MRI"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">IEEE Access</journal-id><journal-id journal-id-type=\"iso-abbrev\">IEEE Access</journal-id><journal-id journal-id-type=\"pmc-domain-id\">319</journal-id><journal-id journal-id-type=\"pmc-domain\">nihpa</journal-id><journal-title-group><journal-title>IEEE access : practical innovations, open solutions</journal-title></journal-title-group><issn pub-type=\"epub\">2169-3536</issn><custom-meta-group><custom-meta><meta-name>pmc-is-collection-domain</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-collection-title</meta-name><meta-value>NIHPA Author Manuscripts</meta-value></custom-meta></custom-meta-group></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12680093</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12680093.1</article-id><article-id pub-id-type=\"pmcaid\">12680093</article-id><article-id pub-id-type=\"pmcaiid\">12680093</article-id><article-id pub-id-type=\"manuscript-id\">NIHMS2121738</article-id><article-id pub-id-type=\"pmid\">41357810</article-id><article-id pub-id-type=\"doi\">10.1109/access.2025.3627777</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHMS2121738</article-id><article-id pub-id-type=\"manuscript-id-alternative\">NIHPA2121738</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Multimodal Adaptive Inter-Region Attention-Guided Network for Brain Tumor Classification</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>ABDELHALIEM</surname><given-names initials=\"I\">IBRAHIM</given-names></name><xref rid=\"A1\" ref-type=\"aff\">1</xref><xref rid=\"A2\" ref-type=\"aff\">2</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">http://orcid.org/0009-0007-0304-1659</contrib-id><name name-style=\"western\"><surname>DIXON</surname><given-names initials=\"J\">JOSE</given-names></name><xref rid=\"A3\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>ABDELHAMID</surname><given-names initials=\"A\">ABEER</given-names></name><xref rid=\"A4\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>SALEH</surname><given-names initials=\"GA\">GEHAD A.</given-names></name><xref rid=\"A5\" ref-type=\"aff\">5</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">http://orcid.org/0000-0003-3318-2851</contrib-id><name name-style=\"western\"><surname>KHALIFA</surname><given-names initials=\"F\">FAHMI</given-names></name><role>Senior Member, IEEE</role><xref rid=\"A3\" ref-type=\"aff\">3</xref><xref rid=\"A4\" ref-type=\"aff\">4</xref></contrib></contrib-group><aff id=\"A1\"><label>1</label>Department of Computer Science, Faculty of Computers and Information, Assiut University, Asyut 71515, Egypt</aff><aff id=\"A2\"><label>2</label>Department of Bioengineering, University of Louisville, Louisville, KY 40292, USA</aff><aff id=\"A3\"><label>3</label>Electrical and Computer Engineering Department, School of Engineering, Morgan State University, Baltimore, MD 21251, USA</aff><aff id=\"A4\"><label>4</label>Electronics and Communications Engineering Department, Mansoura University, Mansoura 35516, Egypt</aff><aff id=\"A5\"><label>5</label>Department of Diagnostic and Interventional Radiology, Mansoura University, Mansoura 35516, Egypt</aff><author-notes><fn id=\"FN1\"><p id=\"P1\">The associate editor coordinating the review of this manuscript and approving it for publication was Binit Lukose.</p></fn><corresp id=\"CR1\">Corresponding author: Fahmi Khalifa (<email>fahmikhalifa@mans.edu.eg</email>)</corresp></author-notes><pub-date pub-type=\"ppub\"><year>2025</year></pub-date><pub-date pub-type=\"epub\"><day>31</day><month>10</month><year>2025</year></pub-date><volume>13</volume><issue-id pub-id-type=\"pmc-issue-id\">501912</issue-id><fpage>187964</fpage><lpage>187975</lpage><pub-history><event event-type=\"nihms-submitted\"><date><day>10</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-release\"><date><day>06</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>06</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 05:25:13.847\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"nihms-2121738.pdf\"/><abstract id=\"ABS1\"><p id=\"P2\">Accurate brain tumor classification is critical for ensuring timely and effective medical interventions. In recent years, artificial intelligence (AI)-driven diagnostic systems have emerged as transformative tools that optimize the classification process and enable rapid, objective decision-making. However, existing methods often suffer from limitations such as the loss of high-frequency details during multimodal preprocessing, inadequate cross-modal feature alignment, and insufficient focus on shared tumor regions within 3D architectures. To address these challenges, this study introduces a novel AI-based framework for advanced brain tumor classification. Specifically, we propose a multimodal magnetic resonance imaging (MRI) architecture that integrates Diffusion-Weighted MRI (DW-MRI) and T2-weighted MRI (T2-MRI) modalities, uniquely combining them in a dual-branch 3D neural architecture with advanced preprocessing and attention mechanisms. The preprocessing pipeline employs a learnable High-Frequency Information Retention (HFIR) technique to resize T2-MRI images, maintaining consistent spatial dimensions across modalities while preserving essential image details. The architecture utilizes dual-branch 3D convolutional neural networks (CNN) for modality-specific feature extraction, enhanced by a novel Adaptive Region Attention (ARA) module that dynamically aligns and emphasizes highly informative regions shared across modalities, providing deeper and more consistent insights into tumor characteristics. Rigorous evaluation on a dataset of brain MRI scans including three tumor classes demonstrates that the proposed framework achieves overall accuracy, sensitivity, and specificity of 92.86%, 80.00%, and 94.12%, respectively. Statistical analyses using bootstrap-resampled F1-scores confirm significant outperformance over other state-of-the-art models, underscoring its robust and interpretable potential for precise brain tumor diagnosis.</p></abstract><kwd-group><title>INDEX TERMS</title><kwd>Brain tumor</kwd><kwd>multimodal</kwd><kwd>information retention</kwd><kwd>adaptive region attention</kwd><kwd>DW-MRI</kwd><kwd>T2-MRI</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"S1\"><label>I.</label><title>INTRODUCTION</title><p id=\"P3\">Cancer is the leading cause of death worldwide and hinders life expectancy. Brain tumors, a major type of cancer, result from abnormal cell growth in the brain, damaging vital tissues, influencing the brain or spinal cord, and advancing to cancer [<xref rid=\"R1\" ref-type=\"bibr\">1</xref>], [<xref rid=\"R2\" ref-type=\"bibr\">2</xref>]. The National Brain Tumor Society (NBTS) estimates that over 94K individuals will receive a primary brain tumor diagnosis in 2025 [<xref rid=\"R3\" ref-type=\"bibr\">3</xref>], [<xref rid=\"R4\" ref-type=\"bibr\">4</xref>] while approximately 1M Americans are currently living with a brain tumor [<xref rid=\"R3\" ref-type=\"bibr\">3</xref>]. Radiologists classify less aggressive and non-progressive tumors as benign (grades I and II). Originating in the brain, they develop slowly and cannot reach other parts of the body. Conversely, malignant tumors can appear in various forms and degrees. Meningiomas and gliomas (which combine astrocytomas, oligodendrogliomas, and ependymomas) are the most common brain tumors in adults. With 15 subtypes, the majority of which are benign and regarded as WHO grade I [<xref rid=\"R5\" ref-type=\"bibr\">5</xref>], meningioma is a single category in the WHO&#8217;s central nervous system (CNS)5thedition. Pituitaryadenomas are usually noncancerous, gradually developing masses, and represent the most prevalent form of pituitary gland tumors [<xref rid=\"R6\" ref-type=\"bibr\">6</xref>]. While pituitary tumors form around the pituitary gland, meningiomas grow around the skull area. Thus, early-stage brain tumor detection has become a crucial but difficult task for guiding therapy decisions, thus ensuring the appropriate option to save the patient&#8217;s life.</p><p id=\"P4\">Prompt identification of brain tumors is critical for successful treatment. This information informs treatment plans and significantly impacts patient survival rates [<xref rid=\"R7\" ref-type=\"bibr\">7</xref>], [<xref rid=\"R8\" ref-type=\"bibr\">8</xref>]. However, due to their complex characteristics, medical practitioners find it difficult to diagnose brain tumors. Typically, doctors diagnose brain tumors using a set of physical and neurological tests. Biopsy, examined using various histological approaches, is the most consistent method for detecting brain cancer. However, this procedure is intrusive, time-consuming, subjective, inconsistent, and increases the risk of bleeding, tissue damage, and functional loss [<xref rid=\"R9\" ref-type=\"bibr\">9</xref>]. Given the rapid progression of cancer, where patient survival relies on precise and early diagnosis [<xref rid=\"R10\" ref-type=\"bibr\">10</xref>], [<xref rid=\"R11\" ref-type=\"bibr\">11</xref>], this is extremely concerning.</p><p id=\"P5\">AI-based non-invasive tools effectively utilize various imaging techniques, such as magnetic resonance imaging (MRI) and computed tomography (CT), for diagnosis, followed by biopsy and pathological examination for confirmation. Among non-ionizing and non-invasive techniques, MRI is the most effective, in addition to its ability to provide detailed images of the brain&#8217;s anatomy [<xref rid=\"R12\" ref-type=\"bibr\">12</xref>]. Accurate interpretation of these images is a demanding task for identifying and classifying brain cancer. Radiologists&#8217; experience has historically been the basis for this study; however, manual analysis is time-consuming, subjective, and prone to human error, especially in complex cases or among less experienced staff [<xref rid=\"R13\" ref-type=\"bibr\">13</xref>]. Modern neuroimaging has many new non-invasive tools made possible by artificial intelligence (AI). These tools help doctors understand the structure and function of brain tumors [<xref rid=\"R9\" ref-type=\"bibr\">9</xref>], [<xref rid=\"R10\" ref-type=\"bibr\">10</xref>], [<xref rid=\"R14\" ref-type=\"bibr\">14</xref>]. Recently, AI-based tools that streamline medical imaging have become crucial in determining the type and stage of tumors, as well as in developing treatment plans. Typically, T1-weighted (T1-w) contrast-enhanced MRI helps physicians distinguish primary tumors, such as meningiomas, and secondary tumors.</p><p id=\"P6\">The objective of this study was to enhance brain tumor diagnosis using advanced AI techniques. We propose a multimodal MRI-based integrative tool with the potential to enhance precise diagnosis, leading to significantly more informed treatment. The proposed model integrates a combination of High-Frequency Information Retention (HFIR), 3D convolutional neural network (3D CNN) feature extractors, and Adaptive Region Attention (ARA) modules that have an unrivaled ability to comprehend both the nuanced details and the larger spatial contexts contained within the brain MRI images. Given the size differences between T2- and DWI-weighted MRIs, we incorporated an information retention strategy to prevent information loss during image downscaling. This strategy, in turn, preserves MRI details for better diagnosis. First, the HFIR module preprocesses T2-MRI images to preserve high-frequency information. Second, the model employs a 3D ResNet architecture for feature extraction that delves deeper into spatial relationships to capture the information required for accurate brain tumor analysis. Finally, the model integrates CNN-derived multimodal features using a novel attention module, the ARA, to accentuate common crucial features before classification. The latter obtains its representation through a Dropout layer, a 3D convolutional layer used as a classifier, a Rectified Linear Unit (ReLU) function, and 3D Adaptive Average Pooling (AAP). The proposed framework contributes significantly to the field of brain tumor classification in the following ways:</p><list list-type=\"bullet\" id=\"L2\"><list-item><p id=\"P7\">We developed an integrative multimodal system that combines diverse learning modules to capture salient features while incorporating cross-domain knowledge, thereby providing valuable insights for brain tumor classification.</p></list-item><list-item><p id=\"P8\">The HFIR module is a learnable block that preserves intricate T2-MRI details while resizing for improved diagnosis.</p></list-item><list-item><p id=\"P9\">The framework designs a novel ARA module to emphasize key features shared between MRI modalities. This module, unlike global attention mechanisms in existing literature, complements the multi-branch 3D CNN model, enhancing its capacity to comprehend the interactions and relationships between different modalities thoroughly.</p></list-item></list><p id=\"P10\">The paper organizes the remaining sections into four consecutive parts: related work, methodology, results and discussion, and conclusion. In the related work <xref rid=\"S2\" ref-type=\"sec\">section II</xref>, we examine the current literature and methodologies developed for brain tumor classification, identifying gaps and opportunities for innovation. The methodology in <xref rid=\"S3\" ref-type=\"sec\">section III</xref> describes the proposed architecture and its parameter settings. <xref rid=\"S7\" ref-type=\"sec\">Section IV</xref> presents the quantitative and qualitative findings and discussion from experiments, which we validated using various ablation schemes and evaluation criteria. Finally, the conclusions in <xref rid=\"S13\" ref-type=\"sec\">section VI</xref> summarize key findings, discuss implications, and propose avenues for future research.</p></sec><sec id=\"S2\"><label>II.</label><title>RELATED WORK</title><p id=\"P11\">Medical image analysis, particularly brain disease detection, has seen significant advancements due to AI- and deep learning (DL)-based algorithms. Numerous studies have explored multimodal MRI inputs, CNNs, hybrid models, and attention-based mechanisms to improve accuracy and robustness. For instance, Sekhar et al. [<xref rid=\"R15\" ref-type=\"bibr\">15</xref>] combined traditional ML with a fine-tuned GoogleNet on the (GBCE)-MRI dataset, achieving more than 98% precision in glioma detection. Similarly, transformer-based models, such as Transformer-Enhanced CNN [<xref rid=\"R16\" ref-type=\"bibr\">16</xref>] and hybrid CNN attention models [<xref rid=\"R17\" ref-type=\"bibr\">17</xref>] have achieved high performance on datasets such as BRATS and Figshare. ResNet variants [<xref rid=\"R18\" ref-type=\"bibr\">18</xref>], correlation-based methods [<xref rid=\"R19\" ref-type=\"bibr\">19</xref>], and detection strategies using YOLO and GoogleNet [<xref rid=\"R20\" ref-type=\"bibr\">20</xref>] further demonstrate the versatility of DL architectures. Attention mechanisms, such as the MANet proposed by Shaik and Cherukuri [<xref rid=\"R21\" ref-type=\"bibr\">21</xref>], also enhance classification by focusing on critical features. Comparative analyses using popular architectures (e.g., VGG-16, ResNet-50, and Inception-v3) [<xref rid=\"R22\" ref-type=\"bibr\">22</xref>] and decision support systems using pre-trained DenseNet and SVM classifiers [<xref rid=\"R23\" ref-type=\"bibr\">23</xref>] highlight ongoing efforts to optimize accuracy and interpretability. EfficientNet-based transfer learning models [<xref rid=\"R24\" ref-type=\"bibr\">24</xref>] and multi-scale feature designs such as MultiFeNet [<xref rid=\"R25\" ref-type=\"bibr\">25</xref>] continue to push performance boundaries, with metrics above 98% in most cases. These studies provide a strong foundation for our study, which builds on the strengths of existing methods while addressing key limitations in feature representation and generalizability. Xu et al. [<xref rid=\"R26\" ref-type=\"bibr\">26</xref>] proposed a cross-modality guided ResNet backbone with dual attention for tumor grading, achieving strong performance across BraTS 2018 and 2019. However, the model exhibits modality-dependent variability, suggesting limited adaptability to diverse input combinations. Similarly, Guo et al. [<xref rid=\"R27\" ref-type=\"bibr\">27</xref>] and Fang and Wang [<xref rid=\"R28\" ref-type=\"bibr\">28</xref>] used dual-path or MMDNet-based segmentation approaches but faced constraints from small sample sizes or limited contextual awareness. Current multimodal models often lack consistent performance across modalities and underutilize spatial and structural complementarity between inputs such as T2 and DWI, especially in 3D.</p><p id=\"P12\">Several studies, including those by Eitel et al. [<xref rid=\"R29\" ref-type=\"bibr\">29</xref>] and Juneja et al. [<xref rid=\"R30\" ref-type=\"bibr\">30</xref>] explored denoising and frequency preservation using CNN filters or autoencoder pipelines. Sarah et al. [<xref rid=\"R31\" ref-type=\"bibr\">31</xref>] and Sahu [<xref rid=\"R32\" ref-type=\"bibr\">32</xref>] employ handcrafted preprocessing techniques (e.g., SGLDM, LPIF) to boost performance. However, these methods do not explicitly aim to retain high-frequency diagnostic features during resolution reduction. Most focus on either denoising or traditional filters, not on channel-aware, learnable retention modules such as Pixel Unshuffle or frequency-aware fusion. Attention-enhanced networks, such as MANet [<xref rid=\"R21\" ref-type=\"bibr\">21</xref>], Tabatabaei et al. [<xref rid=\"R17\" ref-type=\"bibr\">17</xref>], and Hekmat et al. [<xref rid=\"R33\" ref-type=\"bibr\">33</xref>] improve saliency detection and feature weighting using channel or spatial attention layers. However, these typically operate at the global level and do not address the local tumor region focus or cross-modality region alignment. Region-specific attention, particularly across multimodal inputs, remains poorly developed. These models do not dynamically align spatial tumor cues across modalities, as the ARA module in our framework does.</p><p id=\"P13\">Recent studies by Cao et al. [<xref rid=\"R34\" ref-type=\"bibr\">34</xref>], Wu et al. [<xref rid=\"R35\" ref-type=\"bibr\">35</xref>], and Li et al. [<xref rid=\"R36\" ref-type=\"bibr\">36</xref>] adopted 3D CNNs or dual-branch designs to improve volumetric context understanding. However, these models still face constraints, such as early fusion bottlenecks, 2D spatial reliance, and incomplete integration of 3D modality-specific branches. Our approach introduces parallel 3D ResNet branches for each modality, retaining modality-specific features and performing late fusion after the region-level alignment, enabling deeper semantic integration.</p><p id=\"P14\">Prior studies have achieved promising results using CNNs, attention, and transformer-based models; however, most studies only use single-modality MRI or rely on standard fusion strategies with limited cross-modal integration. In contrast, our study introduces a multimodal framework that combines DW-MRI and T2-MRI using an HFIR technique and a novel ARA module. This design enables a more effective feature alignment across modalities and improves the focus on the tumor regions, which enhances diagnostic performance and helps clinicians interpret results more effectively.</p><p id=\"P15\">While prior studies demonstrate strong performance, researchers often constrain their methods by using single-modality MRI inputs, relying on pre-segmented tumor regions, or basic early fusion strategies that fail to support effective cross-modal interaction. Even recent dual-branch and attention-based designs often overlook explicit high-frequency feature preservation or region-aware refinement.</p><p id=\"P16\">To the best of our knowledge, no prior study has integrated the specific contributions of this study. In contrast, our proposed framework integrates DW-MRI and T2-MRI within a dual-branch architecture that includes an HFIR module and an ARA mechanism. This combination promotes enhanced cross-modal feature alignment, improved tumor localization, and greater robustness to modality-specific variations&#8211;ultimately contributing to both superior diagnostic accuracy and model interpretability.</p><p id=\"P17\">Although they have drawbacks, most of the cited studies applied ML and DL techniques, which are effective in identifying brain cancer. The quality and quantity of data used in these algorithms determine their outputs. These algorithms may fail to detect complex tumor classes, including outstanding and uncommon brain tumor types. Similarly, some researchers deployed different ViT versions for brain tumor classification. Contrary to current research, solo CNN and ViT-based approaches for brain tumor classification show poor performance and require further development. <xref rid=\"T1\" ref-type=\"table\">Table 1</xref> presents an overview of the examined literature focusing on brain tumor detection using MRI images. The Table summarizes the technical innovations, performance, and limitations of each related study.</p></sec><sec id=\"S3\"><label>III.</label><title>METHODOLOGY</title><p id=\"P18\">The proposed framework is a multi-step approach for brain tumor classification using MRI data. First, the HFIR module preprocesses the higher-resolution T2-MRI sequences to match the dimensions of the DW-MRI inputs, preserving essential high-frequency features during downsampling. Second, two separate backbone networks independently extract features from the <inline-formula><mml:math id=\"M1\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (HFIR-processed T2-MRI) and <inline-formula><mml:math id=\"M2\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (raw DW-MRI) data. The framework then combines these extracted features via the ARA module. Finally, the classification head categorizes the brain MRI data as normal, benign, or malignant. <xref rid=\"F1\" ref-type=\"fig\">Fig. 1</xref> illustrates the workflow of the proposed framework for brain tumor diagnosis.</p><p id=\"P19\">The dataset comprised multimodal MRI images, specifically DW-MRI and T2-MRI, with resolutions varying from 256 &#215; 256 &#215; 18 for DW-MRI to 512 &#215; 512 &#215; 18 for T2-MRI. The initial step in our pipeline involved resizing the T2-MRI images to match the dimensions of the DW-MRI images. Our model resizes the input data using the HFIR module, which preserves the essential high-frequency features needed for efficient learning. The HFIR module produced feature maps and forwarded them to the next stage, which involved a two-branch model. To improve feature extraction functionality, this model uses two 3D ResNet-18 networks: one to handle the HFIR module&#8217;s output (processed T2-MRI) and another to handle the DW-MRI images. The proposed ARA module helps distinguish between important and unimportant regions. The classification head receives the final result from the ARA module. This classification head comprises a Dropout layer with a probability of 0.5, followed by a 3D convolutional layer with a kernel size of 1 &#215; 1, designed to categorize brain tumors into normal, benign, and malignant types. Finally, we applied a 3D average pooling layer to the model.</p><sec id=\"S4\"><label>A.</label><title>HIGH-FREQUENCY INFORMATION RETENTION (HFIR)</title><p id=\"P20\">Accurate brain tumor classification relies on detailed information included in multimodal MRI scans of the brain. Therefore, preserving these details is crucial for this task, particularly for maintaining high-frequency information. Although maintaining high resolution throughout the network structure might seem intuitive, it significantly increases computational demand. However, data loss and performance degradation are inevitable outcomes of downsampling using convolution with stride or pooling algorithms. To tackle these issues, we used Pixel Unshuffle to reduce the image width (W) and height (H) to half their original sizes while simultaneously increasing the number of channels without losing any high-frequency information, as illustrated in <xref rid=\"F2\" ref-type=\"fig\">Fig. 2</xref>.</p><p id=\"P21\">In particular, in our trials, we used a 3 &#215; 3 convolutional layer to extract shallow features <inline-formula><mml:math id=\"M3\" display=\"inline\"><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for every T2-MRI image <inline-formula><mml:math id=\"M4\" display=\"inline\"><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id=\"M5\" display=\"inline\"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>18</mml:mn><mml:mo>,</mml:mo><mml:mspace width=\"0.25em\"/><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M6\" display=\"inline\"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math></inline-formula>. Subsequently, our architecture reduces the channel dimensions to <inline-formula><mml:math id=\"M7\" display=\"inline\"><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> through an additional 3 &#215; 3 convolutional layer. Then, our architecture enlarges the feature map <inline-formula><mml:math id=\"M8\" display=\"inline\"><mml:mi>s</mml:mi></mml:math></inline-formula> using Pixel Unshuffle to <inline-formula><mml:math id=\"M9\" display=\"inline\"><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id=\"M10\" display=\"inline\"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, and then reshapes it to <inline-formula><mml:math id=\"M11\" display=\"inline\"><mml:mi>s</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi mathvariant=\"normal\">*</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Notably, we continued the practice of using pixel-wise displacement [<xref rid=\"R38\" ref-type=\"bibr\">38</xref>] by retaining high-frequency data.</p></sec><sec id=\"S5\"><label>B.</label><title>CNN-BASED BACKBONE</title><p id=\"P22\">Various deep architecture models have been pre-trained on the ImageNet dataset, including notable examples such as ResNet [<xref rid=\"R39\" ref-type=\"bibr\">39</xref>], EfficientNet [<xref rid=\"R40\" ref-type=\"bibr\">40</xref>], DenseNet [<xref rid=\"R41\" ref-type=\"bibr\">41</xref>], Swin Transformer [<xref rid=\"R42\" ref-type=\"bibr\">42</xref>], Global Filter Network [<xref rid=\"R43\" ref-type=\"bibr\">43</xref>], FastViT [<xref rid=\"R44\" ref-type=\"bibr\">44</xref>], Res2Net [<xref rid=\"R45\" ref-type=\"bibr\">45</xref>], and Focal Modulation Network [<xref rid=\"R46\" ref-type=\"bibr\">46</xref>]. In our study, we employed a dual-branch architecture using 3D ResNet-18, which incorporates residual connections to facilitate effective training of deep models. ResNet-18 is particularly helpful in medical imaging applications, where the quality and quantity of data require sophisticated models to ensure high accuracy and reliability in diagnostic tasks. Our approach involves extracting feature maps from layer 4 (i.e., <inline-formula><mml:math id=\"M12\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>512</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>4</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) of the 3D ResNet-18 for each branch. The model subsequently concatenated these feature maps and passed them through the ARA for further refinement.</p></sec><sec id=\"S6\"><label>C.</label><title>ADAPTIVE REGION ATTENTION (ARA)</title><p id=\"P23\">Intuitively, the high-level local feature <inline-formula><mml:math id=\"M13\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id=\"M14\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> originate from two distinct modalities and share the same spatial dimensions <inline-formula><mml:math id=\"M15\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and number of channels <inline-formula><mml:math id=\"M16\" display=\"inline\"><mml:mi>C</mml:mi></mml:math></inline-formula>. Consequently, they potentially lack correspondence and contain mismatched redundant and interference information. We introduced the ARA module, as shown in <xref rid=\"F3\" ref-type=\"fig\">Fig. 3</xref>, to adeptly align and integrate the valuable insights from both modalities, designed to accentuate common, crucial features before further interaction and fusion.</p><p id=\"P24\">In our design, the ARA module reshaped <inline-formula><mml:math id=\"M17\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M18\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to produce <inline-formula><mml:math id=\"M19\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id=\"M20\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Subsequently, the ARA module takes as input the concatenated features from both branches, denoted as <inline-formula><mml:math id=\"M21\" display=\"inline\"><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <inline-formula><mml:math id=\"M22\" display=\"inline\"><mml:mo>[</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>,</mml:mo><mml:mo>&#8901;</mml:mo><mml:mo>]</mml:mo></mml:math></inline-formula> represents concatenation and <inline-formula><mml:math id=\"M23\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. The network processes the features through two fully connected (FC) layers: the first FC layer halves the feature dimension, and the second restores it to its original value. This technique achieves both goals by reducing the number of parameters and creating an information bottleneck effect. This effect compels the network to prioritize and pass only the most relevant information through the bottleneck, effectively sieving out irrelevant or redundant information. This design ensures that the network learns the most salient features of the data, thereby improving the model&#8217;s generalization capability. Subsequently, the model applied a sigmoid function to generate element-wise attention scores that highlighted the salient regions. The model incorporates a skip connection from the input to the output to enhance training stability. The proposed method mathematically represents the overall process as follows:\n<disp-formula id=\"FD1\"><label>(1)</label><mml:math id=\"M24\" display=\"block\"><mml:mi mathvariant=\"normal\">&#915;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi mathvariant=\"normal\">&#937;</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula>\n<disp-formula id=\"FD2\"><label>(2)</label><mml:math id=\"M25\" display=\"block\"><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:math></disp-formula>\n<disp-formula id=\"FD3\"><label>(3)</label><mml:math id=\"M26\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8855;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></disp-formula>\n<disp-formula id=\"FD4\"><label>(4)</label><mml:math id=\"M27\" display=\"block\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8855;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8853;</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p id=\"P25\">Here, <inline-formula><mml:math id=\"M28\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width=\"0.25em\"/><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width=\"0.25em\"/><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, and <inline-formula><mml:math id=\"M29\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> define the weights and biases of the two fully connected layers; <inline-formula><mml:math id=\"M30\" display=\"inline\"><mml:mi mathvariant=\"normal\">&#937;</mml:mi></mml:math></inline-formula> denotes the ReLU activation function, and <inline-formula><mml:math id=\"M31\" display=\"inline\"><mml:mi>&#963;</mml:mi></mml:math></inline-formula> denotes the sigmoid function. The operator <inline-formula><mml:math id=\"M32\" display=\"inline\"><mml:mo>&#8855;</mml:mo></mml:math></inline-formula> indicates element-wise multiplication, and <inline-formula><mml:math id=\"M33\" display=\"inline\"><mml:mo>&#8853;</mml:mo></mml:math></inline-formula> indicates element-wise addition. The model divides <inline-formula><mml:math id=\"M34\" display=\"inline\"><mml:mi mathvariant=\"normal\">&#915;</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> into <inline-formula><mml:math id=\"M35\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id=\"M36\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"normal\">&#915;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, and then uses them to obtain the attended region feature sequences <inline-formula><mml:math id=\"M37\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id=\"M38\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>&#945;</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"double-struck\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> from <inline-formula><mml:math id=\"M39\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M40\" display=\"inline\"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>W</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, respectively.</p></sec></sec><sec id=\"S7\"><label>IV.</label><title>EXPERIMENTAL RESULTS</title><sec id=\"S8\"><label>A.</label><title>SETTING</title><p id=\"P26\">We trained the model using the AdamW optimizer with a learning rate of 0.001, paired with a cosine annealing scheduler to facilitate learning rate decay. We used a consistent batch size of 8 throughout the training process. To optimize the model, we applied a cross-entropy loss function as the objective function. In addition, the model incorporated a Dropout layer with a probability of 0.5 before the final classification layer to mitigate overfitting. We initialized the 3D ResNet-18 backbone using random weights and trained the model from scratch. We adopted a leave-one-out cross-validation (LOOCV) strategy at the patient level for both training and evaluation. In each fold, we reserved all MRI scans (DW-MRI and T2-MRI) from a single patient for testing, while using data from the remaining patients for training. This approach ensured that no data from the same patient appeared in both the training and testing sets, thereby preventing data leakage and providing an unbiased assessment of the model generalizability across different individuals.</p><p id=\"P27\">We implemented the system using the PyTorch framework in Python and conducted all experiments on a Windows-based machine equipped with 32 GB of RAM, a 20 GB NVIDIA graphics card, and a 12-core i7 processor.</p></sec><sec id=\"S9\"><label>B.</label><title>DATASET</title><p id=\"P28\">In this study, our radiologists collected the dataset from 70 patients at the Diagnostic Radiology Department of Mansoura University Hospitals, Egypt. The Institutional Review Board (IRB) approved all procedures under protocol #R.21.09.1437.R1. Each patient underwent a standard MRI protocol on a 1.5T Philips Ingenia MRI scanner using a standard head coil. The dataset included four MRI sequences per subject: T1-weighted (T1-w), T2-weighted (T2-w), fluid-attenuated inversion recovery (FLAIR), and contrast-enhanced T1-weighted (T1CE) images. The acquisition parameters for the T1-w sequence included a repetition time (TR) of 580 ms, echo time (TE) of 15 ms, matrix size of 80 &#215; 80, field of view (FOV) of 250 &#215; 170 mm<sup>2</sup>, and slice thickness of 5 mm. The parameters for the T2-w images were TR: 4432 ms and TE: 100 ms. For the FLAIR sequence, TR: 10,000 ms, TE: 115 ms, and inversion time (TI): 2700 ms. The radiology team acquired contrast-enhanced T1-w images after intravenous administration of a gadolinium-based contrast agent using an automated injector at a dosage of 0.1 mmol/kg, flow rate of 2 mL/s, and maximum dose of 10 mL. Our radiologists divided the dataset into three diagnostic categories: (1) normal (no tumor detected), (2) benign (including meningioma and pituitary macroadenoma), and (3) malignant (including gliomas of various grades). The dataset consisted of 70 MRI samples divided into three classes: normal (20), benign (22), and malignant (28). Although the class distribution is not perfectly balanced, the imbalance reflects the natural availability of data in clinical settings. To mitigate potential bias, we report not only overall accuracy but also class-wise precision, recall, and F1-scores, which provide a fairer assessment of performance across all classes. We initially stored all MRI images in DICOM format and converted them to NIfTI format for subsequent preprocessing and model training. The dataset cannot be shared publicly due to privacy and institutional regulations; however, the IRB may grant access upon reasonable request.</p></sec><sec id=\"S10\"><label>C.</label><title>RESULTS</title><p id=\"P29\">We evaluated the performance of the proposed system using various metrics, including accuracy (ACC), sensitivity (SEN), and specificity (SPE) for each brain tumor type. We conducted various experiments to evaluate the performance of the proposed method. <xref rid=\"T2\" ref-type=\"table\">Tables 2</xref>&#8211;<xref rid=\"T4\" ref-type=\"table\">4</xref> summarize these experimental results. We began our experiments by comparing several well-known classification models with the proposed approach. As shown in <xref rid=\"T2\" ref-type=\"table\">Table 2</xref>, the proposed approach achieved superior performance on all evaluation metrics. We conducted a statistical analysis of the compared systems and <xref rid=\"T3\" ref-type=\"table\">Table 3</xref> reports the results. To compute the 95% confidence intervals (CIs) for the mean F1-score and Cohen&#8217;s kappa, we applied bootstrap resampling to the predictions obtained from the LOOCV folds. Specifically, we generated 10,000 resampled test sets (with replacement) from the LOOCV predictions for all patients. These 10,000 samples do not represent unique patients but are resampled subsets that we used solely for statistical estimation.</p><p id=\"P30\">Following the methodology proposed by Rajpurkar [<xref rid=\"R47\" ref-type=\"bibr\">47</xref>], we identified the 95% CIs as the range between the 2.5<sup>th</sup> and 97.5<sup>th</sup> percentiles of the bootstrap distributions. Furthermore, we computed the mean F1-score difference between our proposed model and the baseline methods using the same bootstrap sample. As shown in the &#8220;Diff&#8221; column of the <xref rid=\"T3\" ref-type=\"table\">Table 3</xref>, the 95% CIs of these differences exclude zero, providing statistical evidence that the proposed approach outperforms the competing models.</p><p id=\"P31\">An additional quantitative evaluation utilizes confusion matrices to provide a detailed breakdown of performance. <xref rid=\"F4\" ref-type=\"fig\">Fig. 4</xref> presents a confusion matrix comparing our approach with other models. The results illustrate that the proposed method outperforms all other models for each brain tumor type. For completeness, we illustrate the distinct regions within the input images emphasized by the proposed approach when classifying brain tumor types using Grad-CAM [<xref rid=\"R52\" ref-type=\"bibr\">52</xref>]. <xref rid=\"F5\" ref-type=\"fig\">Fig. 5</xref> presents the Grad-CAM overlay on cross-sectional DW-MRI images, highlighting the most influential area in the classification process.</p><p id=\"P32\">Although the current dataset is limited to three-class classification (normal, benign, malignant), the multimodal integration and attention mechanisms in the proposed framework indicate the potential for subtype classification. The HFIR module&#8217;s retention of high-frequency details and the ARA module&#8217;s focus on shared tumor regions across modalities enable the model to capture subtle textural and diffusion variations. With an expanded dataset that includes subtype annotations (e.g., meningioma, pituitary macroadenoma, or glioma grades), we anticipate that the existing architecture can achieve high accuracy in subtype tasks, pending validation in future studies.</p></sec><sec id=\"S11\"><label>D.</label><title>ABLATION STUDY</title><p id=\"P33\">While prior studies&#8212;such as those by Guo et al., Zhou et al., Xu et al., and Tabatabaei et al.&#8212;have reported strong performance using multimodal CNNs, attention mechanisms, or graph-based fusion models, their experimental designs often differ significantly. These differences include varying MRI modalities, reliance on 2D inputs, absence of 3D volumetric representation, and dependence on pre-trained feature extractors and shallow architectures. Furthermore, several approaches focus solely on segmentation or binary classification tasks using limited datasets. In contrast, our study introduces a novel full-volume dual-branch 3D CNN architecture enhanced by HFIR and ARA modules and designed specifically for multimodal brain MRI fusion. To fairly assess the contribution of each architectural component, we performed an internal ablation study under consistent training settings, rather than benchmarking against heterogeneous external baselines.</p><p id=\"P34\">To assess the contribution of each component of the proposed approach, we conducted an ablation study, with the results presented in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>. In the first two scenarios, we evaluated the MRI data without integrating HFIR or ARA individually. First, we employed concatenated DW-MRI data (i.e., b0, b500, and b1000) as the inputs for the 3D ResNet-18 encoder. The system achieved an accuracy of 85.71% (first row of <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>). Subsequently, the use of T2-MRI data alone achieved an accuracy of 83.33% (second row of <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>). After applying the HFIR to downsample the T2-MRI images from 18&#215;512&#215;512 to 18&#215;256&#215;256, the 3D ResNet encoder achieved a similar accuracy. Next, we tested a multi-branch multimodality model, where the first branch extracted features from the HFIR-based downsampled T2-MRI images, and the second branch extracted features from the DW-MRI data. The proposed module concatenated the features from both branches and input into the classification head, achieving an accuracy of 78.57% (fourth row of <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>). In the fifth row of <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>, the model input features from both branches into the ARA module to align and highlight common features between them, resulting in an accuracy of 92.86%.</p></sec></sec><sec id=\"S12\"><label>V.</label><title>DISCUSSION</title><p id=\"P35\">This ongoing field of study explores the development of AI-based on streamlined computational approaches for brain tumor diagnosis and detection. This advancement in AI-powered diagnostics holds great promise for improving patient outcomes by allowing earlier detection and more effective treatment plans [<xref rid=\"R53\" ref-type=\"bibr\">53</xref>]. Recent developments in AI/ML methods provide a viable path for creative detection solutions that employ brain MRI images in response to this demand. This study aims to provide a robust multimodal analytical tool backed by a degree of explainability and interpretability for brain tumor identification. We propose a comprehensive and innovative learning architecture that integrates multiple learning modules to analyze brain MRI images for a more accurate brain tumor diagnosis.</p><p id=\"P36\">The proposed pipeline is a multimodal learning architecture that integrates a dual-branch 3D CNN encoder, HFIR, and ARA modules. We introduced the HFIR module to preserve the intricate details essential for accurate brain tumor classification by maintaining high-frequency information. Furthermore, the employed ARA-based strategy not only leverages the various perspectives provided by each modality but also significantly enhances the model&#8217;s performance by accentuating crucial modality features prior to further interaction and fusion. Since these models together, <xref rid=\"T2\" ref-type=\"table\">Table 2</xref> indicates an overall accuracy of 92.86%. Our model performed better than DL architectures [<xref rid=\"R49\" ref-type=\"bibr\">49</xref>], [<xref rid=\"R50\" ref-type=\"bibr\">50</xref>] and modern transformer-based architectures [<xref rid=\"R51\" ref-type=\"bibr\">51</xref>], underscoring its potential as a valuable tool for brain tumor diagnosis. Additionally, bootstrap analysis of the results obtained by the proposed and competing methods documents the robustness of the proposed architecture. As shown in <xref rid=\"T3\" ref-type=\"table\">Table 3</xref>, the 95% confidence intervals for both the F1-score and Cohen&#8217;s kappa indicate that our method is statistically significantly superior to the other models. These results demonstrate the efficacy of the proposed system in achieving improved performance and underscore its robustness and capabilities.</p><p id=\"P37\">Furthermore, we conducted an ablation analysis, broke down the pipeline under various evaluation scenarios, and summarized the results in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref> using ACC, SEN, and SPEC metrics. Typically, we evaluated the contribution of each modality to the overall accuracy. As indicated in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>, the DWI-based diagnosis yielded a very low sensitivity of 63.89%. This low sensitivity is likely due to concatenated DWI data with varying b-values, which may have introduced noise due to the differences in these b-values. The T2-MRI-based diagnosis achieved a sensitivity of 65.00%. The use of three b-value volumes in DWI compared with a single T2 volume contributed to its higher accuracy compared to T2-MRI. Our multi-branch multimodality model, developed with the HFIR module, improved the performance (i.e., sensitivity) to 70.33%. The features from both branches with ARA enhancement increased the system sensitivity to 80.00%. This ablation study highlights the importance of ARA before feature fusion to align commonalities between various MRI-derived features.</p><p id=\"P38\">The superior performance of the proposed multimodal architecture stems from the effective integration of the HFIR and ARA modules. Unlike conventional models such as SqueezeNet, ResNet-50, EfficientNet-B0, and Uniformer-S, which often apply generic processing to entire the brain volume, our approach leverages HFIR to preserve diagnostic details during T2-MRI downsampling. ARA focuses on clinically relevant regions that are shared across modalities within the proposed model. This targeted feature enhancement improves robustness against class imbalance and dataset limitations, as shown in <xref rid=\"F5\" ref-type=\"fig\">Fig. 5</xref> by Grad-CAM visualizations. In addition, as shown in <xref rid=\"T4\" ref-type=\"table\">Table 4</xref>, individual modality performance is strong (83.71% for DW-MRI and 83.33% for T2-MRI), but their integration within the proposed dual-branch framework significantly boosts the accuracy to 92.86%. The ablation study further confirmed the substantial contributions of HFIR and ARA to this improvement, demonstrating their critical role in outperforming existing methods.</p><p id=\"P39\">Generally, there is a trade-off between system complexity (in terms of the number of parameters) and increased accuracy. Our pipeline illustrates that the shallow design of ResNet-18, paired with enhanced feature fusion, enables more effective learning and higher accuracy. Furthermore, openness and interpretability are essential for building confidence and enabling more informed diagnoses. Therefore, our pipeline incorporates the Grad-CAM method to emphasize the MRI areas that greatly affect pipeline predictions. <xref rid=\"F5\" ref-type=\"fig\">Fig. 5</xref> illustrates these results in detail. The first column displays sample images from the many classes (benign, normal, malignant), and the next column shows the Grad-CAM heatmap projected for the three classes superimposed on grayscale images. The model marks the regions that most strongly affect its predictions with heightened intensity. This all-encompassing visualization method improves the interpretability and transparency of the multimodal model&#8217;s decision-making process, in addition to revealing the particular elements guiding the model predictions.</p><p id=\"P40\">The current study presents a viable method for classifying brain tumors. Nonetheless, certain limitations of this study offer opportunities for future research. First, the model considers MRI image data, which may cause it to ignore essential information in pathological or clinical feature data. Second, we utilized a private dataset in this study that was small and affected by class imbalance. Third, the system processes the whole brain volume, and there is no mechanism in the model to specifically identify and concentrate on important brain ROIs in the images. Ultimately, this study offers a comprehensive diagnostic tool that distinguishes between normal, abnormal, and benign cases. However, future studies should focus on diagnosing different brain tumor subtypes to expand the scope of this study and enhance its applicability and usefulness. Future studies that overcome these constraints may provide useful information for clinical decision-making and enhance the algorithm&#8217;s classification performance.</p></sec><sec id=\"S13\"><label>VI.</label><title>CONCLUSION AND FUTURE WORK</title><p id=\"P41\">In this study, we introduced a novel multimodal adaptive inter-region attention-guided framework for brain tumor classification, integrating DW-MRI and T2-MRI through the HFIR and ARA modules. The HFIR module preserved high-frequency details while downscaling resolution to reduce information loss, and the ARA module emphasized shared tumor-relevant regions across modalities within a dual-branch 3D ResNet-18 architecture. On a locally curated dataset of 70 patients categorized into normal, benign, and malignant classes, the proposed framework achieved an accuracy of 92.86%, sensitivity of 80.00%, and specificity of 94.12%. Statistical analyses&#8212;using bootstrap-derived 95% confidence intervals for F1-scores&#8212;confirmed significant outperformance over other methods. Meanwhile, Grad-CAM visualizations highlighted diagnostically salient regions, thereby enhancing interpretability for clinical adoption.</p><p id=\"P42\">While these results validate the framework&#8217;s efficacy, validation on larger and more heterogeneous datasets is essential to assess robustness against domain shifts such as scanner strength (e.g., 1.5T vs. 3T) and acquisition protocols. Future work will therefore focus on comprehensive evaluations using diverse multimodal datasets, potentially incorporating T1, T1c, and FLAIR sequences to extend the framework toward subtype classification and glioma grading. We also aim to expand the system to more complex clinical tasks, including tumor detection and segmentation, leveraging the HFIR module for modality-agnostic preprocessing and the ARA module for cross-modality feature alignment. We will rigorously assess the framework through multi-fold cross-validation and direct comparisons against state-of-the-art architectures across diverse modalities.</p></sec></body><back><ack id=\"S14\"><p id=\"P43\">This work was supported in part by the Center for Equitable Artificial Intelligence and Machine Learning Systems (CEAMLS), Morgan State University, under Project 11202202; and in part by the Office of the Director, National Institutes of Health Common Fund, under Award 1OT2OD032581.</p></ack><bio id=\"d67e1617\"><p id=\"P44\">\n<graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-b0001.gif\"/>\n</p><p id=\"P45\"><bold>IBRAHIM ABDELHALIEM</bold> received the B.Sc. and M.Sc. degrees from the Computer Science Department, Faculty of Computers and Information, Assiut University. He is currently pursuing the Ph.D. degree in computer science and engineering with the Bioengineering Department, Speed School of Engineering, University of Louisville, USA. He is a Graduate Teaching Assistant (GTA) with the Bioengineering Department, Speed School of Engineering, University of Louisville. He is also an Assistant Lecturer with the Computer Science Department, Faculty of Computers and Information, Assiut University. He has been a Reviewer for various international journals, such as the IEEE J<sc>ournal of</sc> B<sc>iomedical and</sc> H<sc>ealth</sc> I<sc>nformatics</sc>, E<sc>xpert</sc> S<sc>ystems with</sc> A<sc>pplications</sc> (Elsevier), and IEEE ACCESS. His research interests include artificial intelligence in medicine, computer vision, and medical image analysis.</p></bio><bio id=\"d67e1640\"><p id=\"P46\">\n<graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-b0002.gif\"/>\n</p><p id=\"P47\"><bold>JOSE DIXON</bold> received the B.Sc. and M.Sc. degrees in computer science from Morgan State University, USA, in 2023. He is currently pursuing the Ph.D. degree in computer and electrical systems engineering with Morgan State University. He is a Graduate Research Assistant with the Center for Equitable Artificial Intelligence and Machine Learning Systems, Morgan State University. He has more than five years of hands-on experience in the fields of data analytics, image processing, machine learning, medical image analysis, and computer-aided diagnosis. He has authored/co-authored about five peer-reviewed publications appearing in high-impact journals, and selective peer-reviewed top-rank international conferences.</p></bio><bio id=\"d67e1648\"><p id=\"P48\">\n<graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-b0003.gif\"/>\n</p><p id=\"P49\"><bold>ABEER ABDELHAMID</bold> received the M.Sc. degree in electronics and communication engineering from the ECE Department, Faculty of Engineering, Mansoura University, Egypt, in 2022, where she is currently pursuing the Ph.D. degree. She has more than four years of hands on experience in the field of image/signal processing and analysis, focused on the application of AI/ML for medical data analysis for disease diagnosis.</p></bio><bio id=\"d67e1656\"><p id=\"P50\"><bold>GEHAD A. SALEH</bold> received the B.Sc. and M.Sc. degrees from Mansoura University, Mansoura, Egypt, in 2010 and 2015, respectively, and the Ph.D. degree, in 2020. She is currently a Lecturer of radiology and intervention radiology with the Faculty of Medicine, Mansoura University. Her main research interests include machine learning application for medical diagnostics with application in liver, oncology and female pelvic, head and neck radiology using various imaging techniques.</p></bio><bio id=\"d67e1660\"><p id=\"P51\">\n<graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-b0004.gif\"/>\n</p><p id=\"P52\"><bold>FAHMI KHALIFA</bold> (Senior Member, IEEE) received the B.Sc. and M.Sc. degrees in electronics and electrical communication engineering from Mansoura University, Egypt, in 2003 and 2007, respectively, and the Ph.D. degree in electrical engineering from the Electrical and Computer Engineering Department, University of Louisville, USA, in 2014. He has more than 16 years of hands-on experience in the fields of image processing, machine learning, medical image analysis, computer-aided diagnosis, and digital and analog signal processing. He has authored/co-authored more than 200 peer-reviewed publications appearing in high-impact journals, selective peer-reviewed top-rank international conferences, and leading edited books. His honors and awards include Mansoura University scholarship for distinctive undergraduate students for four consecutive years; the Theobald Scholarship Award, in 2013 (ECE, UofL); the ECE Outstanding Student Award for two times, in 2012 and 2014 (ECE, UofL); the John M. Houchens Award for the outstanding dissertation (UofL); and the second-place Post-Doctoral Fellow Award, in 2014 Research Louisville, UofL. He was nominated for the &#8220;Faculty Favorite Recognition&#8221; at the Speed School of Engineering, UofL, 2018&#8211;2019. He was a recipient of the PowerLIVE Award for faculty commitment to students and their academic success; on the final list for the &#8220;Instructional Innovator of the Year&#8221;, Morgan State University, in 2023, and the second place winner of the Heath AI Datathon, Emory University, in 2025.</p></bio><ref-list><title>REFERENCES</title><ref id=\"R1\"><label>[1]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hossain</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Islam</surname><given-names>MT</given-names></name>, <name name-style=\"western\"><surname>Abdul Rahim</surname><given-names>SK</given-names></name>, <name name-style=\"western\"><surname>Rahman</surname><given-names>MA</given-names></name>, <name name-style=\"western\"><surname>Rahman</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>Arshad</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Khandakar</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Ayari</surname><given-names>MA</given-names></name>, and <name name-style=\"western\"><surname>Chowdhury</surname><given-names>MEH</given-names></name>, &#8220;<article-title>A lightweight deep learning based microwave brain image network model for brain tumor classification using reconstructed microwave brain (RMB) images</article-title>,&#8221; <source>Biosensors</source>, vol. <volume>13</volume>, no. <issue>2</issue>, p. <fpage>238</fpage>, <month>Feb</month>. <year>2023</year>.<pub-id pub-id-type=\"pmid\">36832004</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/bios13020238</pub-id><pub-id pub-id-type=\"pmcid\">PMC9954219</pub-id></mixed-citation></ref><ref id=\"R2\"><label>[2]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Tandel</surname><given-names>GS</given-names></name>, <name name-style=\"western\"><surname>Biswas</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Kakde</surname><given-names>OG</given-names></name>, <name name-style=\"western\"><surname>Tiwari</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Suri</surname><given-names>HS</given-names></name>, <name name-style=\"western\"><surname>Turk</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Laird</surname><given-names>JR</given-names></name>, <name name-style=\"western\"><surname>Asare</surname><given-names>CK</given-names></name>, <name name-style=\"western\"><surname>Ankrah</surname><given-names>AA</given-names></name>, <name name-style=\"western\"><surname>Khanna</surname><given-names>NN</given-names></name>, <name name-style=\"western\"><surname>Madhusudhan</surname><given-names>BK</given-names></name>, <name name-style=\"western\"><surname>Saba</surname><given-names>L</given-names></name>, and <name name-style=\"western\"><surname>Suri</surname><given-names>JS</given-names></name>, &#8220;<article-title>A review on a deep learning perspective in brain cancer classification</article-title>,&#8221; <source>Cancers</source>, vol. <volume>11</volume>, no. <issue>1</issue>, p. <fpage>111</fpage>, <year>2019</year>.<pub-id pub-id-type=\"pmid\">30669406</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.3390/cancers11010111</pub-id><pub-id pub-id-type=\"pmcid\">PMC6356431</pub-id></mixed-citation></ref><ref id=\"R3\"><label>[3]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Ostrom</surname><given-names>QT</given-names></name>, <name name-style=\"western\"><surname>Patil</surname><given-names>N</given-names></name>, <name name-style=\"western\"><surname>Cioffi</surname><given-names>G</given-names></name>, <name name-style=\"western\"><surname>Waite</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Kruchko</surname><given-names>C</given-names></name>, and <name name-style=\"western\"><surname>Barnholtz-Sloan</surname><given-names>JS</given-names></name>, &#8220;<article-title>CBTRUS statistical report: Primary brain and other central nervous system tumors diagnosed in the United States in 2013&#8211;2017</article-title>,&#8221; <source>Neuro-oncology</source>, vol. <volume>22</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#8211;<lpage>96</lpage>, <year>2020</year>.<pub-id pub-id-type=\"pmid\">31628483</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/neuonc/noz195</pub-id><pub-id pub-id-type=\"pmcid\">PMC7080218</pub-id></mixed-citation></ref><ref id=\"R4\"><label>[4]</label><mixed-citation publication-type=\"webpage\"><collab>National Brain Tumor Society</collab>. (<year>2025</year>). <source>2025 Public Policy Agenda</source>. Accessed: <date-in-citation>Dec. 21, 2024</date-in-citation>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://braintumor.org/advocacy/policy-agenda/\" ext-link-type=\"uri\">https://braintumor.org/advocacy/policy-agenda/</ext-link></mixed-citation></ref><ref id=\"R5\"><label>[5]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Louis</surname><given-names>DN</given-names></name>, <name name-style=\"western\"><surname>Perry</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Wesseling</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Brat</surname><given-names>DJ</given-names></name>, <name name-style=\"western\"><surname>Cree</surname><given-names>IA</given-names></name>, <name name-style=\"western\"><surname>Figarella-Branger</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Hawkins</surname><given-names>C</given-names></name>, <name name-style=\"western\"><surname>Ng</surname><given-names>HK</given-names></name>, <name name-style=\"western\"><surname>Pfister</surname><given-names>SM</given-names></name>, <name name-style=\"western\"><surname>Reifenberger</surname><given-names>G</given-names></name>, <name name-style=\"western\"><surname>Soffietti</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>von Deimling</surname><given-names>A</given-names></name>, and <name name-style=\"western\"><surname>Ellison</surname><given-names>DW</given-names></name>, &#8220;<article-title>The 2021 WHO classification of tumors of the central nervous system: A summary</article-title>,&#8221; <source>Neuro-Oncology</source>, vol. <volume>23</volume>, no. <issue>8</issue>, pp. <fpage>1231</fpage>&#8211;<lpage>1251</lpage>, <month>Aug</month>. <year>2021</year>.<pub-id pub-id-type=\"pmid\">34185076</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/neuonc/noab106</pub-id><pub-id pub-id-type=\"pmcid\">PMC8328013</pub-id></mixed-citation></ref><ref id=\"R6\"><label>[6]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Melmed</surname><given-names>S</given-names></name>, &#8220;<article-title>Pituitary-tumor endocrinopathies</article-title>,&#8221; <source>New England J. Med</source>, vol. <volume>382</volume>, no. <issue>10</issue>, pp. <fpage>937</fpage>&#8211;<lpage>950</lpage>, <month>Mar</month>. <year>2020</year>.<pub-id pub-id-type=\"pmid\">32130815</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1056/NEJMra1810772</pub-id></mixed-citation></ref><ref id=\"R7\"><label>[7]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>McFaline-Figueroa</surname><given-names>JR</given-names></name> and <name name-style=\"western\"><surname>Lee</surname><given-names>EQ</given-names></name>, &#8220;<article-title>Brain tumors</article-title>,&#8221; <source>Amer. J. Med</source>, vol. <volume>344</volume>, no. <issue>2</issue>, pp. <fpage>114</fpage>&#8211;<lpage>123</lpage>, <year>2001</year>.</mixed-citation></ref><ref id=\"R8\"><label>[8]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Abd-Ellah</surname><given-names>MK</given-names></name>, <name name-style=\"western\"><surname>Awad</surname><given-names>AI</given-names></name>, <name name-style=\"western\"><surname>Khalaf</surname><given-names>AAM</given-names></name>, and <name name-style=\"western\"><surname>Hamed</surname><given-names>HFA</given-names></name>, &#8220;<article-title>A review on brain tumor diagnosis from MRI images: Practical implications, key achievements, and lessons learned</article-title>,&#8221; <source>Magn. Reson. Imag</source>, vol. <volume>61</volume>, pp. <fpage>300</fpage>&#8211;<lpage>318</lpage>, <month>Sep</month>. <year>2019</year>.</mixed-citation></ref><ref id=\"R9\"><label>[9]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Villanueva-Meyer</surname><given-names>JE</given-names></name>, <name name-style=\"western\"><surname>Mabray</surname><given-names>MC</given-names></name>, and <name name-style=\"western\"><surname>Cha</surname><given-names>S</given-names></name>, &#8220;<article-title>Current clinical brain tumor imaging</article-title>,&#8221; <source>Neurosurgery</source>, vol. <volume>81</volume>, no. <issue>3</issue>, pp. <fpage>397</fpage>&#8211;<lpage>415</lpage>, <year>2017</year>.<pub-id pub-id-type=\"pmid\">28486641</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1093/neuros/nyx103</pub-id><pub-id pub-id-type=\"pmcid\">PMC5581219</pub-id></mixed-citation></ref><ref id=\"R10\"><label>[10]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Elazab</surname><given-names>N</given-names></name>, <name name-style=\"western\"><surname>Khalifa</surname><given-names>F</given-names></name>, <name name-style=\"western\"><surname>Gab Allah</surname><given-names>W</given-names></name>, and <name name-style=\"western\"><surname>Elmogy</surname><given-names>M</given-names></name>, &#8220;<article-title>Histopathological-based brain tumor grading using 2D-3D multi-modal CNN-transformer combined with stacking classifiers</article-title>,&#8221; <source>Sci. Rep</source>, vol. <volume>15</volume>, no. <issue>1</issue>, p. <fpage>27764</fpage>, <month>Jul</month>. <year>2025</year>.<pub-id pub-id-type=\"pmid\">40739310</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-025-11754-9</pub-id><pub-id pub-id-type=\"pmcid\">PMC12311013</pub-id></mixed-citation></ref><ref id=\"R11\"><label>[11]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Pei</surname><given-names>L</given-names></name>, <name name-style=\"western\"><surname>Vidyaratne</surname><given-names>L</given-names></name>, <name name-style=\"western\"><surname>Hsu</surname><given-names>W-W</given-names></name>, <name name-style=\"western\"><surname>Rahman</surname><given-names>MM</given-names></name>, and <name name-style=\"western\"><surname>Iftekharuddin</surname><given-names>KM</given-names></name>, &#8220;<article-title>Brain tumor classification using 3D convolutional neural network</article-title>,&#8221; in <source>Proc. Int. MICCAI Brainlesion Workshop</source>, pp. <fpage>335</fpage>&#8211;<lpage>342</lpage>.</mixed-citation></ref><ref id=\"R12\"><label>[12]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Bezdan</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>&#381;ivkovic</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Tuba</surname><given-names>E</given-names></name>, <name name-style=\"western\"><surname>Strumberger</surname><given-names>I</given-names></name>, <name name-style=\"western\"><surname>Bacanin</surname><given-names>N</given-names></name>, and <name name-style=\"western\"><surname>Tuba</surname><given-names>M</given-names></name>, &#8220;<article-title>Glioma brain tumor grade classification from MRI using convolutional neural networks designed by modified FA</article-title>,&#8221; in <source>Proc. Int. Conf. Intell. Fuzzy Syst</source>, <year>2020</year>, pp. <fpage>955</fpage>&#8211;<lpage>963</lpage>.</mixed-citation></ref><ref id=\"R13\"><label>[13]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Albalawi</surname><given-names>E</given-names></name>, <name name-style=\"western\"><surname>Mahesh</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>Thakur</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Kumar</surname><given-names>VV</given-names></name>, <name name-style=\"western\"><surname>Gupta</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Khan</surname><given-names>SB</given-names></name>, and <name name-style=\"western\"><surname>Almusharraf</surname><given-names>A</given-names></name>, &#8220;<article-title>Integrated approach of federated learning with transfer learning for classification and diagnosis of brain tumor</article-title>,&#8221; <source>BMC Med. Imag</source>, vol. <volume>24</volume>, no. <issue>1</issue>, <month>May</month><year>2024</year>, <fpage>Art. no. 110</fpage>.</mixed-citation></ref><ref id=\"R14\"><label>[14]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Roberts</surname><given-names>TA</given-names></name>, <name name-style=\"western\"><surname>Hyare</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Agliardi</surname><given-names>G</given-names></name>, <name name-style=\"western\"><surname>Hipwell</surname><given-names>B</given-names></name>, <name name-style=\"western\"><surname>D&#8217;Esposito</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Ianus</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Breen-Norris</surname><given-names>JO</given-names></name>, <name name-style=\"western\"><surname>Ramasawmy</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Taylor</surname><given-names>V</given-names></name>, <name name-style=\"western\"><surname>Atkinson</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Punwani</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Lythgoe</surname><given-names>MF</given-names></name>, <name name-style=\"western\"><surname>Siow</surname><given-names>B</given-names></name>, <name name-style=\"western\"><surname>Brandner</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Rees</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Panagiotaki</surname><given-names>E</given-names></name>, <name name-style=\"western\"><surname>Alexander</surname><given-names>DC</given-names></name>, and <name name-style=\"western\"><surname>Walker-Samuel</surname><given-names>S</given-names></name>, &#8220;<article-title>Noninvasive diffusion magnetic resonance imaging of brain tumour cell size for the early detection of therapeutic response</article-title>,&#8221; <source>Sci. Rep</source>, vol. <volume>10</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#8211;<lpage>13</lpage>, <month>Jun</month>. <year>2020</year>.<pub-id pub-id-type=\"pmid\">31913322</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41598-019-56847-4</pub-id><pub-id pub-id-type=\"pmcid\">PMC6959339</pub-id></mixed-citation></ref><ref id=\"R15\"><label>[15]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Sekhar</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Biswas</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Hazra</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Sunaniya</surname><given-names>AK</given-names></name>, <name name-style=\"western\"><surname>Mukherjee</surname><given-names>A</given-names></name>, and <name name-style=\"western\"><surname>Yang</surname><given-names>L</given-names></name>, &#8220;<article-title>Brain tumor classification using fine-tuned GoogLeNet features and machine learning algorithms: IoMT enabled CAD system</article-title>,&#8221; <source>IEEE J. Biomed. Health Informat</source>, vol. <volume>26</volume>, no. <issue>3</issue>, pp. <fpage>983</fpage>&#8211;<lpage>991</lpage>, <month>Mar</month>. <year>2022</year>.</mixed-citation></ref><ref id=\"R16\"><label>[16]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Aloraini</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Khan</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Aladhadh</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Habib</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Alsharekh</surname><given-names>MF</given-names></name>, and <name name-style=\"western\"><surname>Islam</surname><given-names>M</given-names></name>, &#8220;<article-title>Combining the transformer and convolution for effective brain tumor classification using MRI images</article-title>,&#8221; <source>Appl. Sci</source>, vol. <volume>13</volume>, no. <issue>6</issue>, p. <fpage>3680</fpage>, <month>Mar</month>. <year>2023</year>.</mixed-citation></ref><ref id=\"R17\"><label>[17]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Tabatabaei</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Rezaee</surname><given-names>K</given-names></name>, and <name name-style=\"western\"><surname>Zhu</surname><given-names>M</given-names></name>, &#8220;<article-title>Attention transformer mechanism and fusion-based deep learning architecture for MRI brain tumor classification system</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>86</volume>, <month>Sep</month>. <year>2023</year>, <fpage>Art. no. 105119</fpage>.</mixed-citation></ref><ref id=\"R18\"><label>[18]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Chatterjee</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Nizamani</surname><given-names>FA</given-names></name>, <name name-style=\"western\"><surname>N&#252;rnberger</surname><given-names>A</given-names></name>, and <name name-style=\"western\"><surname>Speck</surname><given-names>O</given-names></name>, &#8220;<article-title>Classification of brain tumours in MR images using deep spatiospatial models</article-title>,&#8221; <source>Sci. Rep</source>, vol. <volume>12</volume>, no. <issue>1</issue>, p. <fpage>1505</fpage>, <month>Jan</month>. <year>2022</year>, doi: <pub-id pub-id-type=\"doi\">10.1038/s41598-022-05572-6</pub-id>.<pub-id pub-id-type=\"pmid\">35087174</pub-id><pub-id pub-id-type=\"pmcid\">PMC8795458</pub-id></mixed-citation></ref><ref id=\"R19\"><label>[19]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Wo&#378;niak</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Si&#322;ka</surname><given-names>J</given-names></name>, and <name name-style=\"western\"><surname>Wieczorek</surname><given-names>M</given-names></name>, &#8220;<article-title>Deep neural network correlation learning mechanism for CT brain tumor detection</article-title>,&#8221; <source>Neural Comput. Appl</source>, vol. <volume>35</volume>, no. <issue>20</issue>, pp. <fpage>14611</fpage>&#8211;<lpage>14626</lpage>, <month>Jul</month>. <year>2023</year>.</mixed-citation></ref><ref id=\"R20\"><label>[20]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Ali</surname><given-names>F</given-names></name>, <name name-style=\"western\"><surname>Khan</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Waseem Abbas</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Shah</surname><given-names>B</given-names></name>, <name name-style=\"western\"><surname>Hussain</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>Song</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Ei-Sappagh</surname><given-names>S</given-names></name>, and <name name-style=\"western\"><surname>Singh</surname><given-names>J</given-names></name>, &#8220;<article-title>A two-tier framework based on GoogLeNet and YOLOv3 models for tumor detection in MRI</article-title>,&#8221; <source>Comput., Mater. Continua</source>, vol. <volume>72</volume>, no. <issue>1</issue>, pp. <fpage>73</fpage>&#8211;<lpage>92</lpage>, <year>2022</year>.</mixed-citation></ref><ref id=\"R21\"><label>[21]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Shaik</surname><given-names>NS</given-names></name> and <name name-style=\"western\"><surname>Cherukuri</surname><given-names>TK</given-names></name>, &#8220;<article-title>Multi-level attention network: Application to brain tumor classification</article-title>,&#8221; <source>Signal, Image Video Process</source>, vol. <volume>16</volume>, no. <issue>3</issue>, pp. <fpage>817</fpage>&#8211;<lpage>824</lpage>, <month>Apr</month>. <year>2022</year>.</mixed-citation></ref><ref id=\"R22\"><label>[22]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Srinivas</surname><given-names>C</given-names></name>, <name name-style=\"western\"><surname>Prasad</surname><given-names>KSN</given-names></name>, <name name-style=\"western\"><surname>Zakariah</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Alothaibi</surname><given-names>YA</given-names></name>, <name name-style=\"western\"><surname>Shaukat</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Partibane</surname><given-names>B</given-names></name>, and <name name-style=\"western\"><surname>Awal</surname><given-names>H</given-names></name>, &#8220;<article-title>Deep transfer learning approaches in performance analysis of brain tumor classification using MRI images</article-title>,&#8221; <source>J. Healthcare Eng</source>, vol. <volume>2022</volume>, pp. <fpage>1</fpage>&#8211;<lpage>17</lpage>, <month>Mar</month>. <year>2022</year>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/3264367\" ext-link-type=\"uri\">https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/3264367</ext-link></mixed-citation></ref><ref id=\"R23\"><label>[23]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Sharif</surname><given-names>MI</given-names></name>, <name name-style=\"western\"><surname>Khan</surname><given-names>MA</given-names></name>, <name name-style=\"western\"><surname>Alhussein</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Aurangzeb</surname><given-names>K</given-names></name>, and <name name-style=\"western\"><surname>Raza</surname><given-names>M</given-names></name>, &#8220;<article-title>A decision support system for multimodal brain tumor classification using deep learning</article-title>,&#8221; <source>Complex Intell. Syst</source>, vol. <volume>8</volume>, no. <issue>4</issue>, pp. <fpage>3007</fpage>&#8211;<lpage>3020</lpage>, <year>2021</year>.</mixed-citation></ref><ref id=\"R24\"><label>[24]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Zulfiqar</surname><given-names>F</given-names></name>, <name name-style=\"western\"><surname>Bajwa</surname><given-names>UI</given-names></name>, and <name name-style=\"western\"><surname>Mehmood</surname><given-names>Y</given-names></name>, &#8220;<article-title>Multi-class classification of brain tumor types from MR images using efficientNets</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>84</volume>, <month>Jul</month>. <year>2023</year>, <fpage>Art. no. 104777</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://api.semanticscholar.org/CorpusID:257410424\" ext-link-type=\"uri\">https://api.semanticscholar.org/CorpusID:257410424</ext-link></mixed-citation></ref><ref id=\"R25\"><label>[25]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Agrawal</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>Choudhary</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Shankar</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Singh</surname><given-names>P</given-names></name>, and <name name-style=\"western\"><surname>Diwakar</surname><given-names>M</given-names></name>, &#8220;<article-title>MultiFeNet: Multi-scale feature scaling in deep neural network for the brain tumour classification in MRI images</article-title>,&#8221; <source>Int. J. Imag. Syst. Technol</source>, vol. <volume>34</volume>, no. <issue>1</issue>, <month>Jan</month>. <year>2024</year>, <fpage>Art. no. e22956</fpage>.</mixed-citation></ref><ref id=\"R26\"><label>[26]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Xu</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name>, <name name-style=\"western\"><surname>Cai</surname><given-names>J</given-names></name>, and <name name-style=\"western\"><surname>Heng</surname><given-names>P-A</given-names></name>, &#8220;<article-title>Cross-modality guidance-aided multi-modal learning with dual attention for MRI brain tumor grading</article-title>,&#8221; <year>2024</year>, <source>arXiv:2401.09029</source>.</mixed-citation></ref><ref id=\"R27\"><label>[27]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Guo</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style=\"western\"><surname>Chen</surname><given-names>Q</given-names></name>, <name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>J</given-names></name>, and <name name-style=\"western\"><surname>Zhu</surname><given-names>Y</given-names></name>, &#8220;<article-title>Multimodal MRI image decision fusion-based network for glioma classification</article-title>,&#8221; <source>Frontiers Oncol</source>., vol. <volume>12</volume>, <month>Feb</month>. <year>2022</year>, <fpage>Art. no. 819673</fpage>.</mixed-citation></ref><ref id=\"R28\"><label>[28]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Fang</surname><given-names>L</given-names></name> and <name name-style=\"western\"><surname>Wang</surname><given-names>X</given-names></name>, &#8220;<article-title>Brain tumor segmentation based on the dual-path network of multi-modal MRI images</article-title>,&#8221; <source>Pattern Recognit</source>., vol. <volume>124</volume>, <month>Apr</month>. <year>2022</year>, <fpage>Art. no. 108434</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S0031320321006105\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S0031320321006105</ext-link></mixed-citation></ref><ref id=\"R29\"><label>[29]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Eitel</surname><given-names>F</given-names></name>, <name name-style=\"western\"><surname>Philipp Albrecht</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Paul</surname><given-names>F</given-names></name>, and <name name-style=\"western\"><surname>Ritter</surname><given-names>K</given-names></name>, &#8220;<article-title>Harnessing spatial MRI normalization: Patch individual filter layers for CNNs</article-title>,&#8221; <year>2019</year>, <source>arXiv:1911.06278</source>.</mixed-citation></ref><ref id=\"R30\"><label>[30]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Juneja</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Rathee</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Verma</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Bhutani</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Baghel</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Saini</surname><given-names>SK</given-names></name>, and <name name-style=\"western\"><surname>Jindal</surname><given-names>P</given-names></name>, &#8220;<article-title>Denoising of magnetic resonance images of brain tumor using BT-autonet</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>87</volume>, <month>Jan</month>. <year>2024</year>, <fpage>Art. no. 105477</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S1746809423009102\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S1746809423009102</ext-link></mixed-citation></ref><ref id=\"R31\"><label>[31]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Sarah</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Krishnapriya</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Saladi</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Karuna</surname><given-names>Y</given-names></name>, and <name name-style=\"western\"><surname>Bavirisetti</surname><given-names>DP</given-names></name>, &#8220;<article-title>A novel approach to brain tumor detection using K-means++, SGLDM, ResNet50, and synthetic data augmentation</article-title>,&#8221; <source>Frontiers Physiol</source>., vol. <volume>15</volume>, <month>Jul</month>. <year>2024</year>, <fpage>Art. no. 1342572</fpage>.</mixed-citation></ref><ref id=\"R32\"><label>[32]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Sahu</surname><given-names>PK</given-names></name>, &#8220;<article-title>LPIF-based image enhancement and hybrid ensemble models for brain tumor detection</article-title>,&#8221; <source>Connection Sci</source>., vol. <volume>37</volume>, no. <issue>1</issue>, <month>Dec</month>. <year>2025</year>, <fpage>Art. no. 2518983</fpage>, doi: <pub-id pub-id-type=\"doi\">10.1080/09540091.2025.2518983</pub-id>.</mixed-citation></ref><ref id=\"R33\"><label>[33]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Hekmat</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Ur Rehman Khan</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Shad</surname><given-names>I</given-names></name>, and <name name-style=\"western\"><surname>Bilal</surname><given-names>O</given-names></name>, &#8220;<article-title>An attention-fused architecture for brain tumor diagnosis</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>101</volume>, <month>Mar</month>. <year>2025</year>, <fpage>Art. no. 107221</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S1746809424012795\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S1746809424012795</ext-link></mixed-citation></ref><ref id=\"R34\"><label>[34]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Cao</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Zhou</surname><given-names>W</given-names></name>, <name name-style=\"western\"><surname>Zang</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>An</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Feng</surname><given-names>Y</given-names></name>, and <name name-style=\"western\"><surname>Yu</surname><given-names>B</given-names></name>, &#8220;<article-title>MBANet: A 3D convolutional neural network with multi-branch attention for brain tumor segmentation from MRI images</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>80</volume>, <month>Feb</month>. <year>2023</year>, <fpage>Art. no. 104296</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S1746809422007509\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S1746809422007509</ext-link></mixed-citation></ref><ref id=\"R35\"><label>[35]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Wu</surname><given-names>Q</given-names></name>, <name name-style=\"western\"><surname>Pei</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Cheng</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Hu</surname><given-names>X</given-names></name>, and <name name-style=\"western\"><surname>Wang</surname><given-names>C</given-names></name>, &#8220;<article-title>SDS-net: A lightweight 3D convolutional neural network with multi-branch attention for multimodal brain tumor accurate segmentation</article-title>,&#8221; <source>Math. Biosciences Eng</source>, vol. <volume>20</volume>, no. <issue>9</issue>, pp. <fpage>17384</fpage>&#8211;<lpage>17406</lpage>, <year>2023</year>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.aimspress.com/article/doi/10.3934/mbe.2023773\" ext-link-type=\"uri\">https://www.aimspress.com/article/doi/10.3934/mbe.2023773</ext-link></mixed-citation></ref><ref id=\"R36\"><label>[36]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name> and <name name-style=\"western\"><surname>Zhou</surname><given-names>X</given-names></name>, &#8220;<article-title>A global-local parallel dual-branch deep learning model with attention-enhanced feature fusion for brain tumor MRI classification</article-title>,&#8221; <source>Comput., Mater. Continua</source>, vol. <volume>83</volume>, no. <issue>1</issue>, pp. <fpage>739</fpage>&#8211;<lpage>760</lpage>, <year>2025</year>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S1546221825002929\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S1546221825002929</ext-link></mixed-citation></ref><ref id=\"R37\"><label>[37]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Chaki</surname><given-names>J</given-names></name> and <name name-style=\"western\"><surname>Wo&#378;niak</surname><given-names>M</given-names></name>, &#8220;<article-title>A deep learning based four-fold approach to classify brain MRI: BTSCNet</article-title>,&#8221; <source>Biomed. Signal Process. Control</source>, vol. <volume>85</volume>, <month>Aug</month>. <year>2023</year>, <fpage>Art. no. 104902</fpage>. [Online]. Available: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.sciencedirect.com/science/article/pii/S174680942300335X\" ext-link-type=\"uri\">https://www.sciencedirect.com/science/article/pii/S174680942300335X</ext-link></mixed-citation></ref><ref id=\"R38\"><label>[38]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Zhao</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Zhou</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Chen</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Yang</surname><given-names>Y</given-names></name>, and <name name-style=\"western\"><surname>Zhao</surname><given-names>Y</given-names></name>, &#8220;<source>High-frequency stereo matching network</source>,&#8221; <conf-name>in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jun</month>. <year>2023</year>, pp. <fpage>1327</fpage>&#8211;<lpage>1336</lpage>.</mixed-citation></ref><ref id=\"R39\"><label>[39]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>He</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name>, <name name-style=\"western\"><surname>Ren</surname><given-names>S</given-names></name>, and <name name-style=\"western\"><surname>Sun</surname><given-names>J</given-names></name>, &#8220;<source>Deep residual learning for image recognition</source>,&#8221; <conf-name>in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jun</month>. <year>2016</year>, pp. <fpage>770</fpage>&#8211;<lpage>778</lpage>.</mixed-citation></ref><ref id=\"R40\"><label>[40]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Tan</surname><given-names>M</given-names></name> and <name name-style=\"western\"><surname>Le</surname><given-names>QV</given-names></name>, &#8220;<source>EfficientNet: Rethinking model scaling for convolutional neural networks</source>,&#8221; <conf-name>in Proc. Int. Conf. Mach. Learn</conf-name>., <year>2019</year>, pp. <fpage>6105</fpage>&#8211;<lpage>6114</lpage>.</mixed-citation></ref><ref id=\"R41\"><label>[41]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Huang</surname><given-names>G</given-names></name>, <name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Van Der Maaten</surname><given-names>L</given-names></name>, and <name name-style=\"western\"><surname>Weinberger</surname><given-names>KQ</given-names></name>, &#8220;<source>Densely connected convolutional networks</source>,&#8221; <conf-name>in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jul</month>. <year>2017</year>, pp. <fpage>4700</fpage>&#8211;<lpage>4708</lpage>.</mixed-citation></ref><ref id=\"R42\"><label>[42]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Lin</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Cao</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Hu</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Wei</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Lin</surname><given-names>S</given-names></name>, and <name name-style=\"western\"><surname>Guo</surname><given-names>B</given-names></name>, &#8220;<source>Swin transformer: Hierarchical vision transformer using shifted windows</source>,&#8221; <conf-name>in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>Oct</month>. <year>2021</year>, pp. <fpage>9992</fpage>&#8211;<lpage>10002</lpage>.</mixed-citation></ref><ref id=\"R43\"><label>[43]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Rao</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Zhao</surname><given-names>W</given-names></name>, <name name-style=\"western\"><surname>Zhu</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Lu</surname><given-names>J</given-names></name>, and <name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name>, &#8220;<article-title>Global filter networks for image classification</article-title>,&#8221; in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2021</year>, pp. <fpage>980</fpage>&#8211;<lpage>993</lpage>.</mixed-citation></ref><ref id=\"R44\"><label>[44]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Anasosalu Vasu</surname><given-names>PK</given-names></name>, <name name-style=\"western\"><surname>Gabriel</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Zhu</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Tuzel</surname><given-names>O</given-names></name>, and <name name-style=\"western\"><surname>Ranjan</surname><given-names>A</given-names></name>, &#8220;<source>FastViT: A fast hybrid vision transformer using structural reparameterization</source>,&#8221; <conf-name>in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>Oct</month>. <year>2023</year>, pp. <fpage>5762</fpage>&#8211;<lpage>5772</lpage>.</mixed-citation></ref><ref id=\"R45\"><label>[45]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Gao</surname><given-names>S-H</given-names></name>, <name name-style=\"western\"><surname>Cheng</surname><given-names>M-M</given-names></name>, <name name-style=\"western\"><surname>Zhao</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Zhang</surname><given-names>X-Y</given-names></name>, <name name-style=\"western\"><surname>Yang</surname><given-names>M-H</given-names></name>, and <name name-style=\"western\"><surname>Torr</surname><given-names>P</given-names></name>, &#8220;<article-title>Res2Net: A new multi-scale backbone architecture</article-title>,&#8221; <source>IEEE Trans. Pattern Anal. Mach. Intell</source>, vol. <volume>43</volume>, no. <issue>2</issue>, pp. <fpage>652</fpage>&#8211;<lpage>662</lpage>, <month>Feb</month>. <year>2021</year>.<pub-id pub-id-type=\"pmid\">31484108</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2019.2938758</pub-id></mixed-citation></ref><ref id=\"R46\"><label>[46]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Yang</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Li</surname><given-names>C</given-names></name>, and <name name-style=\"western\"><surname>Gao</surname><given-names>J</given-names></name>, &#8220;<article-title>Focal modulation networks</article-title>,&#8221; in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2022</year>, pp. <fpage>4203</fpage>&#8211;<lpage>4217</lpage>.</mixed-citation></ref><ref id=\"R47\"><label>[47]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Rajpurkar</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Irvin</surname><given-names>J</given-names></name>, <name name-style=\"western\"><surname>Zhu</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Yang</surname><given-names>B</given-names></name>, <name name-style=\"western\"><surname>Mehta</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Duan</surname><given-names>T</given-names></name>, <name name-style=\"western\"><surname>Ding</surname><given-names>D</given-names></name>, <name name-style=\"western\"><surname>Bagul</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Langlotz</surname><given-names>C</given-names></name>, <name name-style=\"western\"><surname>Shpanskaya</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Lungren</surname><given-names>MP</given-names></name>, and <name name-style=\"western\"><surname>Ng</surname><given-names>AY</given-names></name>, &#8220;<article-title>CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning</article-title>,&#8221; <year>2017</year>, <source>arXiv:1711.05225</source>.</mixed-citation></ref><ref id=\"R48\"><label>[48]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Iandola</surname><given-names>FN</given-names></name>, <name name-style=\"western\"><surname>Han</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Moskewicz</surname><given-names>MW</given-names></name>, <name name-style=\"western\"><surname>Ashraf</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Dally</surname><given-names>WJ</given-names></name>, and <name name-style=\"western\"><surname>Keutzer</surname><given-names>K</given-names></name>, &#8220;<article-title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</article-title>,&#8221; <year>2016</year>, <source>arXiv:1602.07360</source>.</mixed-citation></ref><ref id=\"R49\"><label>[49]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name>, <name name-style=\"western\"><surname>Mao</surname><given-names>H</given-names></name>, <name name-style=\"western\"><surname>Wu</surname><given-names>C-Y</given-names></name>, <name name-style=\"western\"><surname>Feichtenhofer</surname><given-names>C</given-names></name>, <name name-style=\"western\"><surname>Darrell</surname><given-names>T</given-names></name>, and <name name-style=\"western\"><surname>Xie</surname><given-names>S</given-names></name>, &#8220;<source>AConvNetforthe2020s</source>,&#8221;<conf-name>inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jun</month>. <year>2022</year>, pp. <fpage>11976</fpage>&#8211;<lpage>11986</lpage>.</mixed-citation></ref><ref id=\"R50\"><label>[50]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Xie</surname><given-names>S</given-names></name>, <name name-style=\"western\"><surname>Girshick</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Doll&#225;r</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Tu</surname><given-names>Z</given-names></name>, and <name name-style=\"western\"><surname>He</surname><given-names>K</given-names></name>, &#8220;<source>Aggregated residual transformations for deep neural networks</source>,&#8221; <conf-name>in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>Jul</month>. <year>2017</year>, pp. <fpage>5987</fpage>&#8211;<lpage>5995</lpage>.</mixed-citation></ref><ref id=\"R51\"><label>[51]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Li</surname><given-names>K</given-names></name>, <name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Gao</surname><given-names>P</given-names></name>, <name name-style=\"western\"><surname>Song</surname><given-names>G</given-names></name>, <name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name>, <name name-style=\"western\"><surname>Li</surname><given-names>H</given-names></name>, and <name name-style=\"western\"><surname>Qiao</surname><given-names>Y</given-names></name>, &#8220;<article-title>UniFormer: Unified transformer for efficient spatiotemporal representation learning</article-title>,&#8221; <year>2022</year>, <source>arXiv:2201.04676</source>.</mixed-citation></ref><ref id=\"R52\"><label>[52]</label><mixed-citation publication-type=\"confproc\"><name name-style=\"western\"><surname>Selvaraju</surname><given-names>RR</given-names></name>, <name name-style=\"western\"><surname>Cogswell</surname><given-names>M</given-names></name>, <name name-style=\"western\"><surname>Das</surname><given-names>A</given-names></name>, <name name-style=\"western\"><surname>Vedantam</surname><given-names>R</given-names></name>, <name name-style=\"western\"><surname>Parikh</surname><given-names>D</given-names></name>, and <name name-style=\"western\"><surname>Batra</surname><given-names>D</given-names></name>, &#8220;<source>Grad-CAM: Visual explanations from deep networks via gradient-based localization</source>,&#8221; <conf-name>in Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>Oct</month>. <year>2017</year>, pp. <fpage>618</fpage>&#8211;<lpage>626</lpage>.</mixed-citation></ref><ref id=\"R53\"><label>[53]</label><mixed-citation publication-type=\"journal\"><name name-style=\"western\"><surname>Deepak</surname><given-names>S</given-names></name> and <name name-style=\"western\"><surname>Ameer</surname><given-names>PM</given-names></name>, &#8220;<article-title>Brain tumor classification using deep CNN features via transfer learning</article-title>,&#8221; <source>Comput. Biol. Med</source>, vol. <volume>111</volume>, <month>Aug</month>. <year>2019</year>, <fpage>Art. no. 103345</fpage>.<pub-id pub-id-type=\"pmid\">31279167</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1016/j.compbiomed.2019.103345</pub-id></mixed-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"F1\" orientation=\"portrait\"><label>FIGURE 1.</label><caption><p id=\"P53\">A general review of the suggested brain tumor classification architecture.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-f0005.jpg\"/></fig><fig position=\"float\" id=\"F2\" orientation=\"portrait\"><label>FIGURE 2.</label><caption><p id=\"P54\">Detailed High-frequency information retention (HFIR) module. Here, <italic toggle=\"yes\">B</italic>, Conv1, and Conv3 refer to the batch size, the convolution layer with a kernel size of 1 &#215; 1, and the convolution layer with a kernel size of 3 &#215; 3, respectively.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-f0006.jpg\"/></fig><fig position=\"float\" id=\"F3\" orientation=\"portrait\"><label>FIGURE 3.</label><caption><p id=\"P55\">Schematic of the ARA module details. Here, <inline-formula><mml:math id=\"M41\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"italic\">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">D</mml:mi><mml:mi mathvariant=\"italic\">W</mml:mi><mml:mi mathvariant=\"italic\">I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M42\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"italic\">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> refer to the CNN-derived features from layer 4 of the 3D ResNet-18 for MRI-DWI and MRI-T2, respectively, while <inline-formula><mml:math id=\"M43\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"italic\">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the concatenation of <inline-formula><mml:math id=\"M44\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"italic\">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">D</mml:mi><mml:mi mathvariant=\"italic\">W</mml:mi><mml:mi mathvariant=\"italic\">I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id=\"M45\" display=\"inline\"><mml:msub><mml:mrow><mml:mi mathvariant=\"italic\">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"italic\">T</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-f0007.jpg\"/></fig><fig position=\"float\" id=\"F4\" orientation=\"portrait\"><label>FIGURE 4.</label><caption><p id=\"P56\">The confusion matrix for the proposed approach alongside other well-known multiclass classification models. The proposed method outperformed the other models in each class. Here &#8220;B&#8221;, &#8220;M&#8221;, and &#8220;N&#8221; stand for Benign, Malignant, and Normal, respectively.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-f0008.jpg\"/></fig><fig position=\"float\" id=\"F5\" orientation=\"portrait\"><label>FIGURE 5.</label><caption><p id=\"P57\">This figure illustrates the focus areas for the proposed model in classifying patients into different brain tumor types using grad-CAM. It presents a cross-section of DW-MRI with grad-CAM for each tumor class, accompanied by the corresponding overlaid Grad-CAM. The model predominantly emphasizes regions associated with malignant brain tumors, with intensity levels gradually decreasing as the distance from these regions increases.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"nihms-2121738-f0009.jpg\"/></fig><table-wrap position=\"float\" id=\"T1\" orientation=\"landscape\"><label>TABLE 1.</label><caption><p id=\"P58\">Summary of recent literature work for brain tumor detection.</p></caption><table frame=\"hsides\" rules=\"rows\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Ref</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Dataset</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Performance</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Limitation</th><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Focus Area</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R26\" ref-type=\"bibr\">26</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Cross-modality guided ResNet backbone with dual attention for brain tumor grading</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BraTS 2018 and 2019</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy of 97.90% for BraTS 2018 and 97.00% for BraTS for 2019</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Performance varies across modalities; lacks high-frequency detail preservation</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multimodal MRI input</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R27\" ref-type=\"bibr\">27</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MMIDFNet for improving glioma subtype classification accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CPMRP</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">87.80% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Limited tumor type scope, lacks multi-scale fusion and volumetric modeling.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multimodal MRI Input</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R28\" ref-type=\"bibr\">28</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Dual Path Network based on MMD-Net for brain tumor segmentation</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BraTS 2015</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">92.00% precision</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Inadequate spatial context and receptive field, lacks adaptive regional attention</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multimodal MRI Input</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R29\" ref-type=\"bibr\">29</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Introducing Patch Individual Filter for CNNs</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">UK Biobank, ADNI, Private MS</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">89.06% for UK Biobank, 84.43% for ADNI, and 80.92%</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Not optimized for tumor-specific learning; lacks frequency-aware feature retention</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRI Frequency and Processing</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R31\" ref-type=\"bibr\">31</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">ResNet50 with KMeans and SGLDM, including Grad-CAM for classification</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Br35H</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Training accuracy of 99.25% and validation accuracy of 99.50%</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Heavily data-dependent, no volumetric or multimodal analysis.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRI Frequency and Processing</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R37\" ref-type=\"bibr\">37</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">UNet and ResNet brain segmentation framework</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Figshare</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">95% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Uses 2D slices; lacks full 3D feature learning and multimodal integration</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRI Frequency and Processing</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R30\" ref-type=\"bibr\">30</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Five block dense autoencoder (BT-AutoNet) with 7 Noise Filters, RDUNet, and BBAutoNet for denoising</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Local (Nanfang Hospital), Three Noise datasets</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Average SSIM of 94.58% for all noises and datasets</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Denoising is limited to fixed noise types; lacks generalizability to tumor classification.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRI Frequency and Processing</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R32\" ref-type=\"bibr\">32</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Local Pixel Inhomogeneity Factor (LPIF) for CNN</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BRATS 2013</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Accuracy 99.46% for DS1 and 98.63% for DS2</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">High computational cost, lacks modality-aware attention and hierarchical fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MRI Frequency and Processing</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R17\" ref-type=\"bibr\">17</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Hybrid CNN-Transformer with bidirectional fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Figshare</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">98.59&#8211;99.30% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No spatial alignment across modalities; lacks high-frequency and deep-level integration</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Attention Mechanisms or Feature Refinement</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R21\" ref-type=\"bibr\">21</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Multi-level Attention Network (MANet)</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BRATS 2018, FigShare</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">96.51% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Focused only on malignant tumors; no adaptive feature enhancement</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Attention Mechanisms or Feature Refinement</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R33\" ref-type=\"bibr\">33</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">MobileNetvl+v2 with attention + LSTM fusion</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Br35H</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">96.88% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">CNN backbone lacks depth; limited by dataset size and diversity</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Attention Mechanisms or Feature Refinement</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R34\" ref-type=\"bibr\">34</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Custom 3D UNet CNN for segmentation</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BRATS 2018 and 2019</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Average DSC of 85.18% for BraTS 2018 and 83.68% for BraTS 2019</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">No ARA, lacks explicit preservation of frequency features.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">3D Dual-Branch CNN Architecture</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R36\" ref-type=\"bibr\">36</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">2D Parallel, Attention-Enhanced customized CNN: ResNet50 with VGG16</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Figshare</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">98.04% accuracy</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Relies on pre-segmented tumors for accuracy, lacks full 3D feature fusion and HFIR.</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">3D Dual-Branch CNN Architecture</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">[<xref rid=\"R35\" ref-type=\"bibr\">35</xref>]</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Custom 3D CNN with multi-branch attention mechanism</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">BraTS 2020 and 2021</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Average DSC of 87.20% for BraTS 2020 and 87.03% for BRATS 2021</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">Employs only early fusion; no frequency or region-specific module</td><td align=\"left\" valign=\"top\" rowspan=\"1\" colspan=\"1\">3D Dual-Branch CNN Architecture</td></tr></tbody></table><table-wrap-foot><fn id=\"TFN1\"><p id=\"P59\"><bold><italic toggle=\"yes\">Abbreviations:</italic></bold> MANet: Multi-level Attention Network; CNN: Convolutional Neural Network; ADNI: Alzheimer&#8217;s Disease Neuroimaging Initiative; LPIF: Local Pixel Inhomogeneity Factor; MS: Multiple Sclerosis; LSTM: Long Short-Term Memory; BraTS: Brain Tumor Segmentation Challenge; MMIDFNet: Multimodal MRI Image Decision Fusion Network; CPMRP: Computational Precision Medicine: Radiology-Pathology; MMDNet: Multimodal DenseNet; SGLDM: Spatial Gray Level Dependence Matrix; DSC: Dice Similarity Coefficient; Grad-CAM: Gradient-weighted Class Activation Mapping; SSIM: Structural Similarity Index Measure; RDUNet: Residual Dense Network; BBAutoNet: Block-based Autoencoder Network; VGG: Visual Geometry Group.</p></fn></table-wrap-foot></table-wrap><table-wrap position=\"float\" id=\"T2\" orientation=\"portrait\"><label>TABLE 2.</label><caption><p id=\"P60\">Comparison of the proposed approach with several well-known multiclass classification models using accuracy (ACC), sensitivity (SEN), and specificity (SPE). Here, &#8220;T&#8221; indicates the tiny version of the respective model, and &#8220;S&#8221; denotes the small one.</p></caption><table frame=\"box\" rules=\"all\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ACC (%)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SEN (%)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SPE (%)</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SqueezeNet [<xref rid=\"R48\" ref-type=\"bibr\">48</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">61.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">36.11</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">68.63</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ConvNext-T [<xref rid=\"R49\" ref-type=\"bibr\">49</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">21.43</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">34.78</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">67.09</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">EfficientNet-BO [<xref rid=\"R40\" ref-type=\"bibr\">40</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">59.52</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">33.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">66.67</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ResNeXt-50 [<xref rid=\"R50\" ref-type=\"bibr\">50</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.81</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">72.78</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.91</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UniFormer-S [<xref rid=\"R51\" ref-type=\"bibr\">51</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">85.71</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.56</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.30</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>Proposed</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>92.86</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>80.00</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>94.12</bold>\n</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"T3\" orientation=\"landscape\"><label>TABLE 3.</label><caption><p id=\"P61\">Comparison of the proposed approach with several well-known models based on F1-score, Cohen&#8217;s kappa, and their differences.</p></caption><table frame=\"box\" rules=\"all\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Model</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">F1-Score (%) (95% CI)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Cohen Kappa (%) (95% CI)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Diff</th></tr></thead><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SqueezeNet [<xref rid=\"R48\" ref-type=\"bibr\">48</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">30.73 (20.00, 53.33)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">7.13 (0.00, 34.78)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">51.26 (50.80, 51.71)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ConvNext-T [<xref rid=\"R49\" ref-type=\"bibr\">49</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">18.82 (3.51, 38.7)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.33 (0, 17.42)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">63.16 (62.7, 63.60)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">EfficientNet-B0 [<xref rid=\"R40\" ref-type=\"bibr\">40</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">25.3 (18.39, 41.67)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.00 (0.00,0.00)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">56.35 (55.95, 56.73)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ResNeXt-50 [<xref rid=\"R50\" ref-type=\"bibr\">50</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.82 (43.21, 88.57)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">54.75 (24.32, 83.59)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">16.16 (15.67, 16.65)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UniFormer-S [<xref rid=\"R51\" ref-type=\"bibr\">51</xref>]</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">76.37 (48.21, 100.00)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.57 (38.24, 100.00)</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">5.01 (5.09, 6.14)</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>Proposed</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>81.98 (62.21, 100.00)</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>85.69 (62.21, 100.00)</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#8211;</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"T4\" orientation=\"landscape\"><label>TABLE 4.</label><caption><p id=\"P62\">An ablation study of key components of the proposed 3D model.</p></caption><table frame=\"box\" rules=\"all\"><colgroup span=\"1\"><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/><col align=\"left\" valign=\"middle\" span=\"1\"/></colgroup><thead><tr><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DW-MRI</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">T2-MRI</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">3D ResNet</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">HFIR</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ARA</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ACC (%)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SEN (%)</th><th align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SPE (%)</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">85.71</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">63.89</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.08</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">65.00</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.12</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.33</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.12</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\"/><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">78.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">55.56</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.35</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">&#10003;</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>92.86</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>80.00</bold>\n</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">\n<bold>94.12</bold>\n</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc IEEE Access IEEE Access 319 nihpa IEEE access : practical innovations, open solutions 2169-3536 pmc-is-collection-domain yes pmc-collection-title NIHPA Author Manuscripts PMC12680093 PMC12680093.1 12680093 12680093 NIHMS2121738 41357810 10.1109/access.2025.3627777 NIHMS2121738 NIHPA2121738 1 Article A Multimodal Adaptive Inter-Region Attention-Guided Network for Brain Tumor Classification ABDELHALIEM IBRAHIM 1 2 http://orcid.org/0009-0007-0304-1659 DIXON JOSE 3 ABDELHAMID ABEER 4 SALEH GEHAD A. 5 http://orcid.org/0000-0003-3318-2851 KHALIFA FAHMI Senior Member, IEEE 3 4 1 Department of Computer Science, Faculty of Computers and Information, Assiut University, Asyut 71515, Egypt 2 Department of Bioengineering, University of Louisville, Louisville, KY 40292, USA 3 Electrical and Computer Engineering Department, School of Engineering, Morgan State University, Baltimore, MD 21251, USA 4 Electronics and Communications Engineering Department, Mansoura University, Mansoura 35516, Egypt 5 Department of Diagnostic and Interventional Radiology, Mansoura University, Mansoura 35516, Egypt The associate editor coordinating the review of this manuscript and approving it for publication was Binit Lukose. Corresponding author: Fahmi Khalifa ( fahmikhalifa@mans.edu.eg ) 2025 31 10 2025 13 501912 187964 187975 10 11 2025 06 12 2025 06 12 2025 09 12 2025 https://creativecommons.org/licenses/by/4.0/ This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ Accurate brain tumor classification is critical for ensuring timely and effective medical interventions. In recent years, artificial intelligence (AI)-driven diagnostic systems have emerged as transformative tools that optimize the classification process and enable rapid, objective decision-making. However, existing methods often suffer from limitations such as the loss of high-frequency details during multimodal preprocessing, inadequate cross-modal feature alignment, and insufficient focus on shared tumor regions within 3D architectures. To address these challenges, this study introduces a novel AI-based framework for advanced brain tumor classification. Specifically, we propose a multimodal magnetic resonance imaging (MRI) architecture that integrates Diffusion-Weighted MRI (DW-MRI) and T2-weighted MRI (T2-MRI) modalities, uniquely combining them in a dual-branch 3D neural architecture with advanced preprocessing and attention mechanisms. The preprocessing pipeline employs a learnable High-Frequency Information Retention (HFIR) technique to resize T2-MRI images, maintaining consistent spatial dimensions across modalities while preserving essential image details. The architecture utilizes dual-branch 3D convolutional neural networks (CNN) for modality-specific feature extraction, enhanced by a novel Adaptive Region Attention (ARA) module that dynamically aligns and emphasizes highly informative regions shared across modalities, providing deeper and more consistent insights into tumor characteristics. Rigorous evaluation on a dataset of brain MRI scans including three tumor classes demonstrates that the proposed framework achieves overall accuracy, sensitivity, and specificity of 92.86%, 80.00%, and 94.12%, respectively. Statistical analyses using bootstrap-resampled F1-scores confirm significant outperformance over other state-of-the-art models, underscoring its robust and interpretable potential for precise brain tumor diagnosis. INDEX TERMS Brain tumor multimodal information retention adaptive region attention DW-MRI T2-MRI pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript yes pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes I. INTRODUCTION Cancer is the leading cause of death worldwide and hinders life expectancy. Brain tumors, a major type of cancer, result from abnormal cell growth in the brain, damaging vital tissues, influencing the brain or spinal cord, and advancing to cancer [ 1 ], [ 2 ]. The National Brain Tumor Society (NBTS) estimates that over 94K individuals will receive a primary brain tumor diagnosis in 2025 [ 3 ], [ 4 ] while approximately 1M Americans are currently living with a brain tumor [ 3 ]. Radiologists classify less aggressive and non-progressive tumors as benign (grades I and II). Originating in the brain, they develop slowly and cannot reach other parts of the body. Conversely, malignant tumors can appear in various forms and degrees. Meningiomas and gliomas (which combine astrocytomas, oligodendrogliomas, and ependymomas) are the most common brain tumors in adults. With 15 subtypes, the majority of which are benign and regarded as WHO grade I [ 5 ], meningioma is a single category in the WHO&#8217;s central nervous system (CNS)5thedition. Pituitaryadenomas are usually noncancerous, gradually developing masses, and represent the most prevalent form of pituitary gland tumors [ 6 ]. While pituitary tumors form around the pituitary gland, meningiomas grow around the skull area. Thus, early-stage brain tumor detection has become a crucial but difficult task for guiding therapy decisions, thus ensuring the appropriate option to save the patient&#8217;s life. Prompt identification of brain tumors is critical for successful treatment. This information informs treatment plans and significantly impacts patient survival rates [ 7 ], [ 8 ]. However, due to their complex characteristics, medical practitioners find it difficult to diagnose brain tumors. Typically, doctors diagnose brain tumors using a set of physical and neurological tests. Biopsy, examined using various histological approaches, is the most consistent method for detecting brain cancer. However, this procedure is intrusive, time-consuming, subjective, inconsistent, and increases the risk of bleeding, tissue damage, and functional loss [ 9 ]. Given the rapid progression of cancer, where patient survival relies on precise and early diagnosis [ 10 ], [ 11 ], this is extremely concerning. AI-based non-invasive tools effectively utilize various imaging techniques, such as magnetic resonance imaging (MRI) and computed tomography (CT), for diagnosis, followed by biopsy and pathological examination for confirmation. Among non-ionizing and non-invasive techniques, MRI is the most effective, in addition to its ability to provide detailed images of the brain&#8217;s anatomy [ 12 ]. Accurate interpretation of these images is a demanding task for identifying and classifying brain cancer. Radiologists&#8217; experience has historically been the basis for this study; however, manual analysis is time-consuming, subjective, and prone to human error, especially in complex cases or among less experienced staff [ 13 ]. Modern neuroimaging has many new non-invasive tools made possible by artificial intelligence (AI). These tools help doctors understand the structure and function of brain tumors [ 9 ], [ 10 ], [ 14 ]. Recently, AI-based tools that streamline medical imaging have become crucial in determining the type and stage of tumors, as well as in developing treatment plans. Typically, T1-weighted (T1-w) contrast-enhanced MRI helps physicians distinguish primary tumors, such as meningiomas, and secondary tumors. The objective of this study was to enhance brain tumor diagnosis using advanced AI techniques. We propose a multimodal MRI-based integrative tool with the potential to enhance precise diagnosis, leading to significantly more informed treatment. The proposed model integrates a combination of High-Frequency Information Retention (HFIR), 3D convolutional neural network (3D CNN) feature extractors, and Adaptive Region Attention (ARA) modules that have an unrivaled ability to comprehend both the nuanced details and the larger spatial contexts contained within the brain MRI images. Given the size differences between T2- and DWI-weighted MRIs, we incorporated an information retention strategy to prevent information loss during image downscaling. This strategy, in turn, preserves MRI details for better diagnosis. First, the HFIR module preprocesses T2-MRI images to preserve high-frequency information. Second, the model employs a 3D ResNet architecture for feature extraction that delves deeper into spatial relationships to capture the information required for accurate brain tumor analysis. Finally, the model integrates CNN-derived multimodal features using a novel attention module, the ARA, to accentuate common crucial features before classification. The latter obtains its representation through a Dropout layer, a 3D convolutional layer used as a classifier, a Rectified Linear Unit (ReLU) function, and 3D Adaptive Average Pooling (AAP). The proposed framework contributes significantly to the field of brain tumor classification in the following ways: We developed an integrative multimodal system that combines diverse learning modules to capture salient features while incorporating cross-domain knowledge, thereby providing valuable insights for brain tumor classification. The HFIR module is a learnable block that preserves intricate T2-MRI details while resizing for improved diagnosis. The framework designs a novel ARA module to emphasize key features shared between MRI modalities. This module, unlike global attention mechanisms in existing literature, complements the multi-branch 3D CNN model, enhancing its capacity to comprehend the interactions and relationships between different modalities thoroughly. The paper organizes the remaining sections into four consecutive parts: related work, methodology, results and discussion, and conclusion. In the related work section II , we examine the current literature and methodologies developed for brain tumor classification, identifying gaps and opportunities for innovation. The methodology in section III describes the proposed architecture and its parameter settings. Section IV presents the quantitative and qualitative findings and discussion from experiments, which we validated using various ablation schemes and evaluation criteria. Finally, the conclusions in section VI summarize key findings, discuss implications, and propose avenues for future research. II. RELATED WORK Medical image analysis, particularly brain disease detection, has seen significant advancements due to AI- and deep learning (DL)-based algorithms. Numerous studies have explored multimodal MRI inputs, CNNs, hybrid models, and attention-based mechanisms to improve accuracy and robustness. For instance, Sekhar et al. [ 15 ] combined traditional ML with a fine-tuned GoogleNet on the (GBCE)-MRI dataset, achieving more than 98% precision in glioma detection. Similarly, transformer-based models, such as Transformer-Enhanced CNN [ 16 ] and hybrid CNN attention models [ 17 ] have achieved high performance on datasets such as BRATS and Figshare. ResNet variants [ 18 ], correlation-based methods [ 19 ], and detection strategies using YOLO and GoogleNet [ 20 ] further demonstrate the versatility of DL architectures. Attention mechanisms, such as the MANet proposed by Shaik and Cherukuri [ 21 ], also enhance classification by focusing on critical features. Comparative analyses using popular architectures (e.g., VGG-16, ResNet-50, and Inception-v3) [ 22 ] and decision support systems using pre-trained DenseNet and SVM classifiers [ 23 ] highlight ongoing efforts to optimize accuracy and interpretability. EfficientNet-based transfer learning models [ 24 ] and multi-scale feature designs such as MultiFeNet [ 25 ] continue to push performance boundaries, with metrics above 98% in most cases. These studies provide a strong foundation for our study, which builds on the strengths of existing methods while addressing key limitations in feature representation and generalizability. Xu et al. [ 26 ] proposed a cross-modality guided ResNet backbone with dual attention for tumor grading, achieving strong performance across BraTS 2018 and 2019. However, the model exhibits modality-dependent variability, suggesting limited adaptability to diverse input combinations. Similarly, Guo et al. [ 27 ] and Fang and Wang [ 28 ] used dual-path or MMDNet-based segmentation approaches but faced constraints from small sample sizes or limited contextual awareness. Current multimodal models often lack consistent performance across modalities and underutilize spatial and structural complementarity between inputs such as T2 and DWI, especially in 3D. Several studies, including those by Eitel et al. [ 29 ] and Juneja et al. [ 30 ] explored denoising and frequency preservation using CNN filters or autoencoder pipelines. Sarah et al. [ 31 ] and Sahu [ 32 ] employ handcrafted preprocessing techniques (e.g., SGLDM, LPIF) to boost performance. However, these methods do not explicitly aim to retain high-frequency diagnostic features during resolution reduction. Most focus on either denoising or traditional filters, not on channel-aware, learnable retention modules such as Pixel Unshuffle or frequency-aware fusion. Attention-enhanced networks, such as MANet [ 21 ], Tabatabaei et al. [ 17 ], and Hekmat et al. [ 33 ] improve saliency detection and feature weighting using channel or spatial attention layers. However, these typically operate at the global level and do not address the local tumor region focus or cross-modality region alignment. Region-specific attention, particularly across multimodal inputs, remains poorly developed. These models do not dynamically align spatial tumor cues across modalities, as the ARA module in our framework does. Recent studies by Cao et al. [ 34 ], Wu et al. [ 35 ], and Li et al. [ 36 ] adopted 3D CNNs or dual-branch designs to improve volumetric context understanding. However, these models still face constraints, such as early fusion bottlenecks, 2D spatial reliance, and incomplete integration of 3D modality-specific branches. Our approach introduces parallel 3D ResNet branches for each modality, retaining modality-specific features and performing late fusion after the region-level alignment, enabling deeper semantic integration. Prior studies have achieved promising results using CNNs, attention, and transformer-based models; however, most studies only use single-modality MRI or rely on standard fusion strategies with limited cross-modal integration. In contrast, our study introduces a multimodal framework that combines DW-MRI and T2-MRI using an HFIR technique and a novel ARA module. This design enables a more effective feature alignment across modalities and improves the focus on the tumor regions, which enhances diagnostic performance and helps clinicians interpret results more effectively. While prior studies demonstrate strong performance, researchers often constrain their methods by using single-modality MRI inputs, relying on pre-segmented tumor regions, or basic early fusion strategies that fail to support effective cross-modal interaction. Even recent dual-branch and attention-based designs often overlook explicit high-frequency feature preservation or region-aware refinement. To the best of our knowledge, no prior study has integrated the specific contributions of this study. In contrast, our proposed framework integrates DW-MRI and T2-MRI within a dual-branch architecture that includes an HFIR module and an ARA mechanism. This combination promotes enhanced cross-modal feature alignment, improved tumor localization, and greater robustness to modality-specific variations&#8211;ultimately contributing to both superior diagnostic accuracy and model interpretability. Although they have drawbacks, most of the cited studies applied ML and DL techniques, which are effective in identifying brain cancer. The quality and quantity of data used in these algorithms determine their outputs. These algorithms may fail to detect complex tumor classes, including outstanding and uncommon brain tumor types. Similarly, some researchers deployed different ViT versions for brain tumor classification. Contrary to current research, solo CNN and ViT-based approaches for brain tumor classification show poor performance and require further development. Table 1 presents an overview of the examined literature focusing on brain tumor detection using MRI images. The Table summarizes the technical innovations, performance, and limitations of each related study. III. METHODOLOGY The proposed framework is a multi-step approach for brain tumor classification using MRI data. First, the HFIR module preprocesses the higher-resolution T2-MRI sequences to match the dimensions of the DW-MRI inputs, preserving essential high-frequency features during downsampling. Second, two separate backbone networks independently extract features from the f T 2 (HFIR-processed T2-MRI) and f D W I (raw DW-MRI) data. The framework then combines these extracted features via the ARA module. Finally, the classification head categorizes the brain MRI data as normal, benign, or malignant. Fig. 1 illustrates the workflow of the proposed framework for brain tumor diagnosis. The dataset comprised multimodal MRI images, specifically DW-MRI and T2-MRI, with resolutions varying from 256 &#215; 256 &#215; 18 for DW-MRI to 512 &#215; 512 &#215; 18 for T2-MRI. The initial step in our pipeline involved resizing the T2-MRI images to match the dimensions of the DW-MRI images. Our model resizes the input data using the HFIR module, which preserves the essential high-frequency features needed for efficient learning. The HFIR module produced feature maps and forwarded them to the next stage, which involved a two-branch model. To improve feature extraction functionality, this model uses two 3D ResNet-18 networks: one to handle the HFIR module&#8217;s output (processed T2-MRI) and another to handle the DW-MRI images. The proposed ARA module helps distinguish between important and unimportant regions. The classification head receives the final result from the ARA module. This classification head comprises a Dropout layer with a probability of 0.5, followed by a 3D convolutional layer with a kernel size of 1 &#215; 1, designed to categorize brain tumors into normal, benign, and malignant types. Finally, we applied a 3D average pooling layer to the model. A. HIGH-FREQUENCY INFORMATION RETENTION (HFIR) Accurate brain tumor classification relies on detailed information included in multimodal MRI scans of the brain. Therefore, preserving these details is crucial for this task, particularly for maintaining high-frequency information. Although maintaining high resolution throughout the network structure might seem intuitive, it significantly increases computational demand. However, data loss and performance degradation are inevitable outcomes of downsampling using convolution with stride or pooling algorithms. To tackle these issues, we used Pixel Unshuffle to reduce the image width (W) and height (H) to half their original sizes while simultaneously increasing the number of channels without losing any high-frequency information, as illustrated in Fig. 2 . In particular, in our trials, we used a 3 &#215; 3 convolutional layer to extract shallow features s &#8712; R C &#215; D &#215; W &#215; H for every T2-MRI image x &#8712; R 1 &#215; D &#215; W &#215; H , where D = 18 , W = 512 , and H = 512 . Subsequently, our architecture reduces the channel dimensions to s &#8712; R C / r 2 &#215; D &#215; W &#215; H through an additional 3 &#215; 3 convolutional layer. Then, our architecture enlarges the feature map s using Pixel Unshuffle to s &#8712; R C &#215; D &#215; W / r &#215; H / r , where r = 2 , and then reshapes it to s &#8712; R C / r 2 * D &#215; W &#215; H . Notably, we continued the practice of using pixel-wise displacement [ 38 ] by retaining high-frequency data. B. CNN-BASED BACKBONE Various deep architecture models have been pre-trained on the ImageNet dataset, including notable examples such as ResNet [ 39 ], EfficientNet [ 40 ], DenseNet [ 41 ], Swin Transformer [ 42 ], Global Filter Network [ 43 ], FastViT [ 44 ], Res2Net [ 45 ], and Focal Modulation Network [ 46 ]. In our study, we employed a dual-branch architecture using 3D ResNet-18, which incorporates residual connections to facilitate effective training of deep models. ResNet-18 is particularly helpful in medical imaging applications, where the quality and quantity of data require sophisticated models to ensure high accuracy and reliability in diagnostic tasks. Our approach involves extracting feature maps from layer 4 (i.e., f T 2 , f D W I &#8712; R 512 &#215; 4 &#215; 8 &#215; 8 ) of the 3D ResNet-18 for each branch. The model subsequently concatenated these feature maps and passed them through the ARA for further refinement. C. ADAPTIVE REGION ATTENTION (ARA) Intuitively, the high-level local feature f T 2 &#8712; R P W &#215; P H &#215; C and f D W I &#8712; R P W &#215; P H &#215; C originate from two distinct modalities and share the same spatial dimensions P W &#215; P H and number of channels C . Consequently, they potentially lack correspondence and contain mismatched redundant and interference information. We introduced the ARA module, as shown in Fig. 3 , to adeptly align and integrate the valuable insights from both modalities, designed to accentuate common, crucial features before further interaction and fusion. In our design, the ARA module reshaped f T 2 and f D W I to produce f T 2 &#8712; R P W H &#215; C and f D W I &#8712; R P W H &#215; C . Subsequently, the ARA module takes as input the concatenated features from both branches, denoted as f : f T 2 , f D W I &#8712; R P W H &#215; 2 C , where [ &#8901; , &#8901; ] represents concatenation and P W H : P W &#215; P H . The network processes the features through two fully connected (FC) layers: the first FC layer halves the feature dimension, and the second restores it to its original value. This technique achieves both goals by reducing the number of parameters and creating an information bottleneck effect. This effect compels the network to prioritize and pass only the most relevant information through the bottleneck, effectively sieving out irrelevant or redundant information. This design ensures that the network learns the most salient features of the data, thereby improving the model&#8217;s generalization capability. Subsequently, the model applied a sigmoid function to generate element-wise attention scores that highlighted the salient regions. The model incorporates a skip connection from the input to the output to enhance training stability. The proposed method mathematically represents the overall process as follows: (1) &#915; = &#963; &#937; f &#934; 1 + b 1 &#934; 2 + b 2 (2) &#915; T 2 , &#915; D W I = &#915; (3) &#945; T 2 = &#915; T 2 &#8855; f T 2 &#8853; f T 2 (4) &#945; D W I = &#915; D W I &#8855; f D W I &#8853; f D W I Here, &#934; 1 &#8712; R 2 C &#215; ( C / 2 ) , &#934; 2 &#8712; R ( C / 2 ) &#215; 2 C , b 1 &#8712; R 1 &#215; ( C / 2 ) , and b 2 &#8712; R 1 &#215; 2 C define the weights and biases of the two fully connected layers; &#937; denotes the ReLU activation function, and &#963; denotes the sigmoid function. The operator &#8855; indicates element-wise multiplication, and &#8853; indicates element-wise addition. The model divides &#915; &#8712; R P W H &#215; 2 C into &#915; T 2 &#8712; R P W H &#215; C and &#915; D W I &#8712; R P W H &#215; C , and then uses them to obtain the attended region feature sequences &#945; T 2 &#8712; R P W H &#215; C and &#945; D W I &#8712; R P W H &#215; C from f T 2 and f D W I , respectively. IV. EXPERIMENTAL RESULTS A. SETTING We trained the model using the AdamW optimizer with a learning rate of 0.001, paired with a cosine annealing scheduler to facilitate learning rate decay. We used a consistent batch size of 8 throughout the training process. To optimize the model, we applied a cross-entropy loss function as the objective function. In addition, the model incorporated a Dropout layer with a probability of 0.5 before the final classification layer to mitigate overfitting. We initialized the 3D ResNet-18 backbone using random weights and trained the model from scratch. We adopted a leave-one-out cross-validation (LOOCV) strategy at the patient level for both training and evaluation. In each fold, we reserved all MRI scans (DW-MRI and T2-MRI) from a single patient for testing, while using data from the remaining patients for training. This approach ensured that no data from the same patient appeared in both the training and testing sets, thereby preventing data leakage and providing an unbiased assessment of the model generalizability across different individuals. We implemented the system using the PyTorch framework in Python and conducted all experiments on a Windows-based machine equipped with 32 GB of RAM, a 20 GB NVIDIA graphics card, and a 12-core i7 processor. B. DATASET In this study, our radiologists collected the dataset from 70 patients at the Diagnostic Radiology Department of Mansoura University Hospitals, Egypt. The Institutional Review Board (IRB) approved all procedures under protocol #R.21.09.1437.R1. Each patient underwent a standard MRI protocol on a 1.5T Philips Ingenia MRI scanner using a standard head coil. The dataset included four MRI sequences per subject: T1-weighted (T1-w), T2-weighted (T2-w), fluid-attenuated inversion recovery (FLAIR), and contrast-enhanced T1-weighted (T1CE) images. The acquisition parameters for the T1-w sequence included a repetition time (TR) of 580 ms, echo time (TE) of 15 ms, matrix size of 80 &#215; 80, field of view (FOV) of 250 &#215; 170 mm 2 , and slice thickness of 5 mm. The parameters for the T2-w images were TR: 4432 ms and TE: 100 ms. For the FLAIR sequence, TR: 10,000 ms, TE: 115 ms, and inversion time (TI): 2700 ms. The radiology team acquired contrast-enhanced T1-w images after intravenous administration of a gadolinium-based contrast agent using an automated injector at a dosage of 0.1 mmol/kg, flow rate of 2 mL/s, and maximum dose of 10 mL. Our radiologists divided the dataset into three diagnostic categories: (1) normal (no tumor detected), (2) benign (including meningioma and pituitary macroadenoma), and (3) malignant (including gliomas of various grades). The dataset consisted of 70 MRI samples divided into three classes: normal (20), benign (22), and malignant (28). Although the class distribution is not perfectly balanced, the imbalance reflects the natural availability of data in clinical settings. To mitigate potential bias, we report not only overall accuracy but also class-wise precision, recall, and F1-scores, which provide a fairer assessment of performance across all classes. We initially stored all MRI images in DICOM format and converted them to NIfTI format for subsequent preprocessing and model training. The dataset cannot be shared publicly due to privacy and institutional regulations; however, the IRB may grant access upon reasonable request. C. RESULTS We evaluated the performance of the proposed system using various metrics, including accuracy (ACC), sensitivity (SEN), and specificity (SPE) for each brain tumor type. We conducted various experiments to evaluate the performance of the proposed method. Tables 2 &#8211; 4 summarize these experimental results. We began our experiments by comparing several well-known classification models with the proposed approach. As shown in Table 2 , the proposed approach achieved superior performance on all evaluation metrics. We conducted a statistical analysis of the compared systems and Table 3 reports the results. To compute the 95% confidence intervals (CIs) for the mean F1-score and Cohen&#8217;s kappa, we applied bootstrap resampling to the predictions obtained from the LOOCV folds. Specifically, we generated 10,000 resampled test sets (with replacement) from the LOOCV predictions for all patients. These 10,000 samples do not represent unique patients but are resampled subsets that we used solely for statistical estimation. Following the methodology proposed by Rajpurkar [ 47 ], we identified the 95% CIs as the range between the 2.5 th and 97.5 th percentiles of the bootstrap distributions. Furthermore, we computed the mean F1-score difference between our proposed model and the baseline methods using the same bootstrap sample. As shown in the &#8220;Diff&#8221; column of the Table 3 , the 95% CIs of these differences exclude zero, providing statistical evidence that the proposed approach outperforms the competing models. An additional quantitative evaluation utilizes confusion matrices to provide a detailed breakdown of performance. Fig. 4 presents a confusion matrix comparing our approach with other models. The results illustrate that the proposed method outperforms all other models for each brain tumor type. For completeness, we illustrate the distinct regions within the input images emphasized by the proposed approach when classifying brain tumor types using Grad-CAM [ 52 ]. Fig. 5 presents the Grad-CAM overlay on cross-sectional DW-MRI images, highlighting the most influential area in the classification process. Although the current dataset is limited to three-class classification (normal, benign, malignant), the multimodal integration and attention mechanisms in the proposed framework indicate the potential for subtype classification. The HFIR module&#8217;s retention of high-frequency details and the ARA module&#8217;s focus on shared tumor regions across modalities enable the model to capture subtle textural and diffusion variations. With an expanded dataset that includes subtype annotations (e.g., meningioma, pituitary macroadenoma, or glioma grades), we anticipate that the existing architecture can achieve high accuracy in subtype tasks, pending validation in future studies. D. ABLATION STUDY While prior studies&#8212;such as those by Guo et al., Zhou et al., Xu et al., and Tabatabaei et al.&#8212;have reported strong performance using multimodal CNNs, attention mechanisms, or graph-based fusion models, their experimental designs often differ significantly. These differences include varying MRI modalities, reliance on 2D inputs, absence of 3D volumetric representation, and dependence on pre-trained feature extractors and shallow architectures. Furthermore, several approaches focus solely on segmentation or binary classification tasks using limited datasets. In contrast, our study introduces a novel full-volume dual-branch 3D CNN architecture enhanced by HFIR and ARA modules and designed specifically for multimodal brain MRI fusion. To fairly assess the contribution of each architectural component, we performed an internal ablation study under consistent training settings, rather than benchmarking against heterogeneous external baselines. To assess the contribution of each component of the proposed approach, we conducted an ablation study, with the results presented in Table 4 . In the first two scenarios, we evaluated the MRI data without integrating HFIR or ARA individually. First, we employed concatenated DW-MRI data (i.e., b0, b500, and b1000) as the inputs for the 3D ResNet-18 encoder. The system achieved an accuracy of 85.71% (first row of Table 4 ). Subsequently, the use of T2-MRI data alone achieved an accuracy of 83.33% (second row of Table 4 ). After applying the HFIR to downsample the T2-MRI images from 18&#215;512&#215;512 to 18&#215;256&#215;256, the 3D ResNet encoder achieved a similar accuracy. Next, we tested a multi-branch multimodality model, where the first branch extracted features from the HFIR-based downsampled T2-MRI images, and the second branch extracted features from the DW-MRI data. The proposed module concatenated the features from both branches and input into the classification head, achieving an accuracy of 78.57% (fourth row of Table 4 ). In the fifth row of Table 4 , the model input features from both branches into the ARA module to align and highlight common features between them, resulting in an accuracy of 92.86%. V. DISCUSSION This ongoing field of study explores the development of AI-based on streamlined computational approaches for brain tumor diagnosis and detection. This advancement in AI-powered diagnostics holds great promise for improving patient outcomes by allowing earlier detection and more effective treatment plans [ 53 ]. Recent developments in AI/ML methods provide a viable path for creative detection solutions that employ brain MRI images in response to this demand. This study aims to provide a robust multimodal analytical tool backed by a degree of explainability and interpretability for brain tumor identification. We propose a comprehensive and innovative learning architecture that integrates multiple learning modules to analyze brain MRI images for a more accurate brain tumor diagnosis. The proposed pipeline is a multimodal learning architecture that integrates a dual-branch 3D CNN encoder, HFIR, and ARA modules. We introduced the HFIR module to preserve the intricate details essential for accurate brain tumor classification by maintaining high-frequency information. Furthermore, the employed ARA-based strategy not only leverages the various perspectives provided by each modality but also significantly enhances the model&#8217;s performance by accentuating crucial modality features prior to further interaction and fusion. Since these models together, Table 2 indicates an overall accuracy of 92.86%. Our model performed better than DL architectures [ 49 ], [ 50 ] and modern transformer-based architectures [ 51 ], underscoring its potential as a valuable tool for brain tumor diagnosis. Additionally, bootstrap analysis of the results obtained by the proposed and competing methods documents the robustness of the proposed architecture. As shown in Table 3 , the 95% confidence intervals for both the F1-score and Cohen&#8217;s kappa indicate that our method is statistically significantly superior to the other models. These results demonstrate the efficacy of the proposed system in achieving improved performance and underscore its robustness and capabilities. Furthermore, we conducted an ablation analysis, broke down the pipeline under various evaluation scenarios, and summarized the results in Table 4 using ACC, SEN, and SPEC metrics. Typically, we evaluated the contribution of each modality to the overall accuracy. As indicated in Table 4 , the DWI-based diagnosis yielded a very low sensitivity of 63.89%. This low sensitivity is likely due to concatenated DWI data with varying b-values, which may have introduced noise due to the differences in these b-values. The T2-MRI-based diagnosis achieved a sensitivity of 65.00%. The use of three b-value volumes in DWI compared with a single T2 volume contributed to its higher accuracy compared to T2-MRI. Our multi-branch multimodality model, developed with the HFIR module, improved the performance (i.e., sensitivity) to 70.33%. The features from both branches with ARA enhancement increased the system sensitivity to 80.00%. This ablation study highlights the importance of ARA before feature fusion to align commonalities between various MRI-derived features. The superior performance of the proposed multimodal architecture stems from the effective integration of the HFIR and ARA modules. Unlike conventional models such as SqueezeNet, ResNet-50, EfficientNet-B0, and Uniformer-S, which often apply generic processing to entire the brain volume, our approach leverages HFIR to preserve diagnostic details during T2-MRI downsampling. ARA focuses on clinically relevant regions that are shared across modalities within the proposed model. This targeted feature enhancement improves robustness against class imbalance and dataset limitations, as shown in Fig. 5 by Grad-CAM visualizations. In addition, as shown in Table 4 , individual modality performance is strong (83.71% for DW-MRI and 83.33% for T2-MRI), but their integration within the proposed dual-branch framework significantly boosts the accuracy to 92.86%. The ablation study further confirmed the substantial contributions of HFIR and ARA to this improvement, demonstrating their critical role in outperforming existing methods. Generally, there is a trade-off between system complexity (in terms of the number of parameters) and increased accuracy. Our pipeline illustrates that the shallow design of ResNet-18, paired with enhanced feature fusion, enables more effective learning and higher accuracy. Furthermore, openness and interpretability are essential for building confidence and enabling more informed diagnoses. Therefore, our pipeline incorporates the Grad-CAM method to emphasize the MRI areas that greatly affect pipeline predictions. Fig. 5 illustrates these results in detail. The first column displays sample images from the many classes (benign, normal, malignant), and the next column shows the Grad-CAM heatmap projected for the three classes superimposed on grayscale images. The model marks the regions that most strongly affect its predictions with heightened intensity. This all-encompassing visualization method improves the interpretability and transparency of the multimodal model&#8217;s decision-making process, in addition to revealing the particular elements guiding the model predictions. The current study presents a viable method for classifying brain tumors. Nonetheless, certain limitations of this study offer opportunities for future research. First, the model considers MRI image data, which may cause it to ignore essential information in pathological or clinical feature data. Second, we utilized a private dataset in this study that was small and affected by class imbalance. Third, the system processes the whole brain volume, and there is no mechanism in the model to specifically identify and concentrate on important brain ROIs in the images. Ultimately, this study offers a comprehensive diagnostic tool that distinguishes between normal, abnormal, and benign cases. However, future studies should focus on diagnosing different brain tumor subtypes to expand the scope of this study and enhance its applicability and usefulness. Future studies that overcome these constraints may provide useful information for clinical decision-making and enhance the algorithm&#8217;s classification performance. VI. CONCLUSION AND FUTURE WORK In this study, we introduced a novel multimodal adaptive inter-region attention-guided framework for brain tumor classification, integrating DW-MRI and T2-MRI through the HFIR and ARA modules. The HFIR module preserved high-frequency details while downscaling resolution to reduce information loss, and the ARA module emphasized shared tumor-relevant regions across modalities within a dual-branch 3D ResNet-18 architecture. On a locally curated dataset of 70 patients categorized into normal, benign, and malignant classes, the proposed framework achieved an accuracy of 92.86%, sensitivity of 80.00%, and specificity of 94.12%. Statistical analyses&#8212;using bootstrap-derived 95% confidence intervals for F1-scores&#8212;confirmed significant outperformance over other methods. Meanwhile, Grad-CAM visualizations highlighted diagnostically salient regions, thereby enhancing interpretability for clinical adoption. While these results validate the framework&#8217;s efficacy, validation on larger and more heterogeneous datasets is essential to assess robustness against domain shifts such as scanner strength (e.g., 1.5T vs. 3T) and acquisition protocols. Future work will therefore focus on comprehensive evaluations using diverse multimodal datasets, potentially incorporating T1, T1c, and FLAIR sequences to extend the framework toward subtype classification and glioma grading. We also aim to expand the system to more complex clinical tasks, including tumor detection and segmentation, leveraging the HFIR module for modality-agnostic preprocessing and the ARA module for cross-modality feature alignment. We will rigorously assess the framework through multi-fold cross-validation and direct comparisons against state-of-the-art architectures across diverse modalities. This work was supported in part by the Center for Equitable Artificial Intelligence and Machine Learning Systems (CEAMLS), Morgan State University, under Project 11202202; and in part by the Office of the Director, National Institutes of Health Common Fund, under Award 1OT2OD032581. IBRAHIM ABDELHALIEM received the B.Sc. and M.Sc. degrees from the Computer Science Department, Faculty of Computers and Information, Assiut University. He is currently pursuing the Ph.D. degree in computer science and engineering with the Bioengineering Department, Speed School of Engineering, University of Louisville, USA. He is a Graduate Teaching Assistant (GTA) with the Bioengineering Department, Speed School of Engineering, University of Louisville. He is also an Assistant Lecturer with the Computer Science Department, Faculty of Computers and Information, Assiut University. He has been a Reviewer for various international journals, such as the IEEE J ournal of B iomedical and H ealth I nformatics , E xpert S ystems with A pplications (Elsevier), and IEEE ACCESS. His research interests include artificial intelligence in medicine, computer vision, and medical image analysis. JOSE DIXON received the B.Sc. and M.Sc. degrees in computer science from Morgan State University, USA, in 2023. He is currently pursuing the Ph.D. degree in computer and electrical systems engineering with Morgan State University. He is a Graduate Research Assistant with the Center for Equitable Artificial Intelligence and Machine Learning Systems, Morgan State University. He has more than five years of hands-on experience in the fields of data analytics, image processing, machine learning, medical image analysis, and computer-aided diagnosis. He has authored/co-authored about five peer-reviewed publications appearing in high-impact journals, and selective peer-reviewed top-rank international conferences. ABEER ABDELHAMID received the M.Sc. degree in electronics and communication engineering from the ECE Department, Faculty of Engineering, Mansoura University, Egypt, in 2022, where she is currently pursuing the Ph.D. degree. She has more than four years of hands on experience in the field of image/signal processing and analysis, focused on the application of AI/ML for medical data analysis for disease diagnosis. GEHAD A. SALEH received the B.Sc. and M.Sc. degrees from Mansoura University, Mansoura, Egypt, in 2010 and 2015, respectively, and the Ph.D. degree, in 2020. She is currently a Lecturer of radiology and intervention radiology with the Faculty of Medicine, Mansoura University. Her main research interests include machine learning application for medical diagnostics with application in liver, oncology and female pelvic, head and neck radiology using various imaging techniques. FAHMI KHALIFA (Senior Member, IEEE) received the B.Sc. and M.Sc. degrees in electronics and electrical communication engineering from Mansoura University, Egypt, in 2003 and 2007, respectively, and the Ph.D. degree in electrical engineering from the Electrical and Computer Engineering Department, University of Louisville, USA, in 2014. He has more than 16 years of hands-on experience in the fields of image processing, machine learning, medical image analysis, computer-aided diagnosis, and digital and analog signal processing. He has authored/co-authored more than 200 peer-reviewed publications appearing in high-impact journals, selective peer-reviewed top-rank international conferences, and leading edited books. His honors and awards include Mansoura University scholarship for distinctive undergraduate students for four consecutive years; the Theobald Scholarship Award, in 2013 (ECE, UofL); the ECE Outstanding Student Award for two times, in 2012 and 2014 (ECE, UofL); the John M. Houchens Award for the outstanding dissertation (UofL); and the second-place Post-Doctoral Fellow Award, in 2014 Research Louisville, UofL. He was nominated for the &#8220;Faculty Favorite Recognition&#8221; at the Speed School of Engineering, UofL, 2018&#8211;2019. He was a recipient of the PowerLIVE Award for faculty commitment to students and their academic success; on the final list for the &#8220;Instructional Innovator of the Year&#8221;, Morgan State University, in 2023, and the second place winner of the Heath AI Datathon, Emory University, in 2025. REFERENCES [1] Hossain A , Islam MT , Abdul Rahim SK , Rahman MA , Rahman T , Arshad H , Khandakar A , Ayari MA , and Chowdhury MEH , &#8220; A lightweight deep learning based microwave brain image network model for brain tumor classification using reconstructed microwave brain (RMB) images ,&#8221; Biosensors , vol. 13 , no. 2 , p. 238 , Feb . 2023 . 36832004 10.3390/bios13020238 PMC9954219 [2] Tandel GS , Biswas M , Kakde OG , Tiwari A , Suri HS , Turk M , Laird JR , Asare CK , Ankrah AA , Khanna NN , Madhusudhan BK , Saba L , and Suri JS , &#8220; A review on a deep learning perspective in brain cancer classification ,&#8221; Cancers , vol. 11 , no. 1 , p. 111 , 2019 . 30669406 10.3390/cancers11010111 PMC6356431 [3] Ostrom QT , Patil N , Cioffi G , Waite K , Kruchko C , and Barnholtz-Sloan JS , &#8220; CBTRUS statistical report: Primary brain and other central nervous system tumors diagnosed in the United States in 2013&#8211;2017 ,&#8221; Neuro-oncology , vol. 22 , no. 1 , pp. 1 &#8211; 96 , 2020 . 31628483 10.1093/neuonc/noz195 PMC7080218 [4] National Brain Tumor Society . ( 2025 ). 2025 Public Policy Agenda . Accessed: Dec. 21, 2024 . [Online]. Available: https://braintumor.org/advocacy/policy-agenda/ [5] Louis DN , Perry A , Wesseling P , Brat DJ , Cree IA , Figarella-Branger D , Hawkins C , Ng HK , Pfister SM , Reifenberger G , Soffietti R , von Deimling A , and Ellison DW , &#8220; The 2021 WHO classification of tumors of the central nervous system: A summary ,&#8221; Neuro-Oncology , vol. 23 , no. 8 , pp. 1231 &#8211; 1251 , Aug . 2021 . 34185076 10.1093/neuonc/noab106 PMC8328013 [6] Melmed S , &#8220; Pituitary-tumor endocrinopathies ,&#8221; New England J. Med , vol. 382 , no. 10 , pp. 937 &#8211; 950 , Mar . 2020 . 32130815 10.1056/NEJMra1810772 [7] McFaline-Figueroa JR and Lee EQ , &#8220; Brain tumors ,&#8221; Amer. J. Med , vol. 344 , no. 2 , pp. 114 &#8211; 123 , 2001 . [8] Abd-Ellah MK , Awad AI , Khalaf AAM , and Hamed HFA , &#8220; A review on brain tumor diagnosis from MRI images: Practical implications, key achievements, and lessons learned ,&#8221; Magn. Reson. Imag , vol. 61 , pp. 300 &#8211; 318 , Sep . 2019 . [9] Villanueva-Meyer JE , Mabray MC , and Cha S , &#8220; Current clinical brain tumor imaging ,&#8221; Neurosurgery , vol. 81 , no. 3 , pp. 397 &#8211; 415 , 2017 . 28486641 10.1093/neuros/nyx103 PMC5581219 [10] Elazab N , Khalifa F , Gab Allah W , and Elmogy M , &#8220; Histopathological-based brain tumor grading using 2D-3D multi-modal CNN-transformer combined with stacking classifiers ,&#8221; Sci. Rep , vol. 15 , no. 1 , p. 27764 , Jul . 2025 . 40739310 10.1038/s41598-025-11754-9 PMC12311013 [11] Pei L , Vidyaratne L , Hsu W-W , Rahman MM , and Iftekharuddin KM , &#8220; Brain tumor classification using 3D convolutional neural network ,&#8221; in Proc. Int. MICCAI Brainlesion Workshop , pp. 335 &#8211; 342 . [12] Bezdan T , &#381;ivkovic M , Tuba E , Strumberger I , Bacanin N , and Tuba M , &#8220; Glioma brain tumor grade classification from MRI using convolutional neural networks designed by modified FA ,&#8221; in Proc. Int. Conf. Intell. Fuzzy Syst , 2020 , pp. 955 &#8211; 963 . [13] Albalawi E , Mahesh T , Thakur A , Kumar VV , Gupta M , Khan SB , and Almusharraf A , &#8220; Integrated approach of federated learning with transfer learning for classification and diagnosis of brain tumor ,&#8221; BMC Med. Imag , vol. 24 , no. 1 , May 2024 , Art. no. 110 . [14] Roberts TA , Hyare H , Agliardi G , Hipwell B , D&#8217;Esposito A , Ianus A , Breen-Norris JO , Ramasawmy R , Taylor V , Atkinson D , Punwani S , Lythgoe MF , Siow B , Brandner S , Rees J , Panagiotaki E , Alexander DC , and Walker-Samuel S , &#8220; Noninvasive diffusion magnetic resonance imaging of brain tumour cell size for the early detection of therapeutic response ,&#8221; Sci. Rep , vol. 10 , no. 1 , pp. 1 &#8211; 13 , Jun . 2020 . 31913322 10.1038/s41598-019-56847-4 PMC6959339 [15] Sekhar A , Biswas S , Hazra R , Sunaniya AK , Mukherjee A , and Yang L , &#8220; Brain tumor classification using fine-tuned GoogLeNet features and machine learning algorithms: IoMT enabled CAD system ,&#8221; IEEE J. Biomed. Health Informat , vol. 26 , no. 3 , pp. 983 &#8211; 991 , Mar . 2022 . [16] Aloraini M , Khan A , Aladhadh S , Habib S , Alsharekh MF , and Islam M , &#8220; Combining the transformer and convolution for effective brain tumor classification using MRI images ,&#8221; Appl. Sci , vol. 13 , no. 6 , p. 3680 , Mar . 2023 . [17] Tabatabaei S , Rezaee K , and Zhu M , &#8220; Attention transformer mechanism and fusion-based deep learning architecture for MRI brain tumor classification system ,&#8221; Biomed. Signal Process. Control , vol. 86 , Sep . 2023 , Art. no. 105119 . [18] Chatterjee S , Nizamani FA , N&#252;rnberger A , and Speck O , &#8220; Classification of brain tumours in MR images using deep spatiospatial models ,&#8221; Sci. Rep , vol. 12 , no. 1 , p. 1505 , Jan . 2022 , doi: 10.1038/s41598-022-05572-6 . 35087174 PMC8795458 [19] Wo&#378;niak M , Si&#322;ka J , and Wieczorek M , &#8220; Deep neural network correlation learning mechanism for CT brain tumor detection ,&#8221; Neural Comput. Appl , vol. 35 , no. 20 , pp. 14611 &#8211; 14626 , Jul . 2023 . [20] Ali F , Khan S , Waseem Abbas A , Shah B , Hussain T , Song D , Ei-Sappagh S , and Singh J , &#8220; A two-tier framework based on GoogLeNet and YOLOv3 models for tumor detection in MRI ,&#8221; Comput., Mater. Continua , vol. 72 , no. 1 , pp. 73 &#8211; 92 , 2022 . [21] Shaik NS and Cherukuri TK , &#8220; Multi-level attention network: Application to brain tumor classification ,&#8221; Signal, Image Video Process , vol. 16 , no. 3 , pp. 817 &#8211; 824 , Apr . 2022 . [22] Srinivas C , Prasad KSN , Zakariah M , Alothaibi YA , Shaukat K , Partibane B , and Awal H , &#8220; Deep transfer learning approaches in performance analysis of brain tumor classification using MRI images ,&#8221; J. Healthcare Eng , vol. 2022 , pp. 1 &#8211; 17 , Mar . 2022 . [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/3264367 [23] Sharif MI , Khan MA , Alhussein M , Aurangzeb K , and Raza M , &#8220; A decision support system for multimodal brain tumor classification using deep learning ,&#8221; Complex Intell. Syst , vol. 8 , no. 4 , pp. 3007 &#8211; 3020 , 2021 . [24] Zulfiqar F , Bajwa UI , and Mehmood Y , &#8220; Multi-class classification of brain tumor types from MR images using efficientNets ,&#8221; Biomed. Signal Process. Control , vol. 84 , Jul . 2023 , Art. no. 104777 . [Online]. Available: https://api.semanticscholar.org/CorpusID:257410424 [25] Agrawal T , Choudhary P , Shankar A , Singh P , and Diwakar M , &#8220; MultiFeNet: Multi-scale feature scaling in deep neural network for the brain tumour classification in MRI images ,&#8221; Int. J. Imag. Syst. Technol , vol. 34 , no. 1 , Jan . 2024 , Art. no. e22956 . [26] Xu D , Wang X , Cai J , and Heng P-A , &#8220; Cross-modality guidance-aided multi-modal learning with dual attention for MRI brain tumor grading ,&#8221; 2024 , arXiv:2401.09029 . [27] Guo S , Wang L , Chen Q , Wang L , Zhang J , and Zhu Y , &#8220; Multimodal MRI image decision fusion-based network for glioma classification ,&#8221; Frontiers Oncol ., vol. 12 , Feb . 2022 , Art. no. 819673 . [28] Fang L and Wang X , &#8220; Brain tumor segmentation based on the dual-path network of multi-modal MRI images ,&#8221; Pattern Recognit ., vol. 124 , Apr . 2022 , Art. no. 108434 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0031320321006105 [29] Eitel F , Philipp Albrecht J , Paul F , and Ritter K , &#8220; Harnessing spatial MRI normalization: Patch individual filter layers for CNNs ,&#8221; 2019 , arXiv:1911.06278 . [30] Juneja M , Rathee A , Verma R , Bhutani R , Baghel S , Saini SK , and Jindal P , &#8220; Denoising of magnetic resonance images of brain tumor using BT-autonet ,&#8221; Biomed. Signal Process. Control , vol. 87 , Jan . 2024 , Art. no. 105477 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1746809423009102 [31] Sarah P , Krishnapriya S , Saladi S , Karuna Y , and Bavirisetti DP , &#8220; A novel approach to brain tumor detection using K-means++, SGLDM, ResNet50, and synthetic data augmentation ,&#8221; Frontiers Physiol ., vol. 15 , Jul . 2024 , Art. no. 1342572 . [32] Sahu PK , &#8220; LPIF-based image enhancement and hybrid ensemble models for brain tumor detection ,&#8221; Connection Sci ., vol. 37 , no. 1 , Dec . 2025 , Art. no. 2518983 , doi: 10.1080/09540091.2025.2518983 . [33] Hekmat A , Zhang Z , Ur Rehman Khan S , Shad I , and Bilal O , &#8220; An attention-fused architecture for brain tumor diagnosis ,&#8221; Biomed. Signal Process. Control , vol. 101 , Mar . 2025 , Art. no. 107221 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1746809424012795 [34] Cao Y , Zhou W , Zang M , An D , Feng Y , and Yu B , &#8220; MBANet: A 3D convolutional neural network with multi-branch attention for brain tumor segmentation from MRI images ,&#8221; Biomed. Signal Process. Control , vol. 80 , Feb . 2023 , Art. no. 104296 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1746809422007509 [35] Wu Q , Pei Y , Cheng Z , Hu X , and Wang C , &#8220; SDS-net: A lightweight 3D convolutional neural network with multi-branch attention for multimodal brain tumor accurate segmentation ,&#8221; Math. Biosciences Eng , vol. 20 , no. 9 , pp. 17384 &#8211; 17406 , 2023 . [Online]. Available: https://www.aimspress.com/article/doi/10.3934/mbe.2023773 [36] Li Z and Zhou X , &#8220; A global-local parallel dual-branch deep learning model with attention-enhanced feature fusion for brain tumor MRI classification ,&#8221; Comput., Mater. Continua , vol. 83 , no. 1 , pp. 739 &#8211; 760 , 2025 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1546221825002929 [37] Chaki J and Wo&#378;niak M , &#8220; A deep learning based four-fold approach to classify brain MRI: BTSCNet ,&#8221; Biomed. Signal Process. Control , vol. 85 , Aug . 2023 , Art. no. 104902 . [Online]. Available: https://www.sciencedirect.com/science/article/pii/S174680942300335X [38] Zhao H , Zhou H , Zhang Y , Chen J , Yang Y , and Zhao Y , &#8220; High-frequency stereo matching network ,&#8221; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun . 2023 , pp. 1327 &#8211; 1336 . [39] He K , Zhang X , Ren S , and Sun J , &#8220; Deep residual learning for image recognition ,&#8221; in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun . 2016 , pp. 770 &#8211; 778 . [40] Tan M and Le QV , &#8220; EfficientNet: Rethinking model scaling for convolutional neural networks ,&#8221; in Proc. Int. Conf. Mach. Learn ., 2019 , pp. 6105 &#8211; 6114 . [41] Huang G , Liu Z , Van Der Maaten L , and Weinberger KQ , &#8220; Densely connected convolutional networks ,&#8221; in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul . 2017 , pp. 4700 &#8211; 4708 . [42] Liu Z , Lin Y , Cao Y , Hu H , Wei Y , Zhang Z , Lin S , and Guo B , &#8220; Swin transformer: Hierarchical vision transformer using shifted windows ,&#8221; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , Oct . 2021 , pp. 9992 &#8211; 10002 . [43] Rao Y , Zhao W , Zhu Z , Lu J , and Zhou J , &#8220; Global filter networks for image classification ,&#8221; in Proc. Adv. Neural Inf. Process. Syst , 2021 , pp. 980 &#8211; 993 . [44] Anasosalu Vasu PK , Gabriel J , Zhu J , Tuzel O , and Ranjan A , &#8220; FastViT: A fast hybrid vision transformer using structural reparameterization ,&#8221; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , Oct . 2023 , pp. 5762 &#8211; 5772 . [45] Gao S-H , Cheng M-M , Zhao K , Zhang X-Y , Yang M-H , and Torr P , &#8220; Res2Net: A new multi-scale backbone architecture ,&#8221; IEEE Trans. Pattern Anal. Mach. Intell , vol. 43 , no. 2 , pp. 652 &#8211; 662 , Feb . 2021 . 31484108 10.1109/TPAMI.2019.2938758 [46] Yang J , Li C , and Gao J , &#8220; Focal modulation networks ,&#8221; in Proc. Adv. Neural Inf. Process. Syst , 2022 , pp. 4203 &#8211; 4217 . [47] Rajpurkar P , Irvin J , Zhu K , Yang B , Mehta H , Duan T , Ding D , Bagul A , Langlotz C , Shpanskaya K , Lungren MP , and Ng AY , &#8220; CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning ,&#8221; 2017 , arXiv:1711.05225 . [48] Iandola FN , Han S , Moskewicz MW , Ashraf K , Dally WJ , and Keutzer K , &#8220; SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size ,&#8221; 2016 , arXiv:1602.07360 . [49] Liu Z , Mao H , Wu C-Y , Feichtenhofer C , Darrell T , and Xie S , &#8220; AConvNetforthe2020s ,&#8221; inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun . 2022 , pp. 11976 &#8211; 11986 . [50] Xie S , Girshick R , Doll&#225;r P , Tu Z , and He K , &#8220; Aggregated residual transformations for deep neural networks ,&#8221; in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul . 2017 , pp. 5987 &#8211; 5995 . [51] Li K , Wang Y , Gao P , Song G , Liu Y , Li H , and Qiao Y , &#8220; UniFormer: Unified transformer for efficient spatiotemporal representation learning ,&#8221; 2022 , arXiv:2201.04676 . [52] Selvaraju RR , Cogswell M , Das A , Vedantam R , Parikh D , and Batra D , &#8220; Grad-CAM: Visual explanations from deep networks via gradient-based localization ,&#8221; in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct . 2017 , pp. 618 &#8211; 626 . [53] Deepak S and Ameer PM , &#8220; Brain tumor classification using deep CNN features via transfer learning ,&#8221; Comput. Biol. Med , vol. 111 , Aug . 2019 , Art. no. 103345 . 31279167 10.1016/j.compbiomed.2019.103345 FIGURE 1. A general review of the suggested brain tumor classification architecture. FIGURE 2. Detailed High-frequency information retention (HFIR) module. Here, B , Conv1, and Conv3 refer to the batch size, the convolution layer with a kernel size of 1 &#215; 1, and the convolution layer with a kernel size of 3 &#215; 3, respectively. FIGURE 3. Schematic of the ARA module details. Here, f D W I and f T 2 refer to the CNN-derived features from layer 4 of the 3D ResNet-18 for MRI-DWI and MRI-T2, respectively, while f C represents the concatenation of f D W I and f T 2 . FIGURE 4. The confusion matrix for the proposed approach alongside other well-known multiclass classification models. The proposed method outperformed the other models in each class. Here &#8220;B&#8221;, &#8220;M&#8221;, and &#8220;N&#8221; stand for Benign, Malignant, and Normal, respectively. FIGURE 5. This figure illustrates the focus areas for the proposed model in classifying patients into different brain tumor types using grad-CAM. It presents a cross-section of DW-MRI with grad-CAM for each tumor class, accompanied by the corresponding overlaid Grad-CAM. The model predominantly emphasizes regions associated with malignant brain tumors, with intensity levels gradually decreasing as the distance from these regions increases. TABLE 1. Summary of recent literature work for brain tumor detection. Ref Method Dataset Performance Limitation Focus Area [ 26 ] Cross-modality guided ResNet backbone with dual attention for brain tumor grading BraTS 2018 and 2019 Accuracy of 97.90% for BraTS 2018 and 97.00% for BraTS for 2019 Performance varies across modalities; lacks high-frequency detail preservation Multimodal MRI input [ 27 ] MMIDFNet for improving glioma subtype classification accuracy CPMRP 87.80% accuracy Limited tumor type scope, lacks multi-scale fusion and volumetric modeling. Multimodal MRI Input [ 28 ] Dual Path Network based on MMD-Net for brain tumor segmentation BraTS 2015 92.00% precision Inadequate spatial context and receptive field, lacks adaptive regional attention Multimodal MRI Input [ 29 ] Introducing Patch Individual Filter for CNNs UK Biobank, ADNI, Private MS 89.06% for UK Biobank, 84.43% for ADNI, and 80.92% Not optimized for tumor-specific learning; lacks frequency-aware feature retention MRI Frequency and Processing [ 31 ] ResNet50 with KMeans and SGLDM, including Grad-CAM for classification Br35H Training accuracy of 99.25% and validation accuracy of 99.50% Heavily data-dependent, no volumetric or multimodal analysis. MRI Frequency and Processing [ 37 ] UNet and ResNet brain segmentation framework Figshare 95% accuracy Uses 2D slices; lacks full 3D feature learning and multimodal integration MRI Frequency and Processing [ 30 ] Five block dense autoencoder (BT-AutoNet) with 7 Noise Filters, RDUNet, and BBAutoNet for denoising Local (Nanfang Hospital), Three Noise datasets Average SSIM of 94.58% for all noises and datasets Denoising is limited to fixed noise types; lacks generalizability to tumor classification. MRI Frequency and Processing [ 32 ] Local Pixel Inhomogeneity Factor (LPIF) for CNN BRATS 2013 Accuracy 99.46% for DS1 and 98.63% for DS2 High computational cost, lacks modality-aware attention and hierarchical fusion MRI Frequency and Processing [ 17 ] Hybrid CNN-Transformer with bidirectional fusion Figshare 98.59&#8211;99.30% accuracy No spatial alignment across modalities; lacks high-frequency and deep-level integration Attention Mechanisms or Feature Refinement [ 21 ] Multi-level Attention Network (MANet) BRATS 2018, FigShare 96.51% accuracy Focused only on malignant tumors; no adaptive feature enhancement Attention Mechanisms or Feature Refinement [ 33 ] MobileNetvl+v2 with attention + LSTM fusion Br35H 96.88% accuracy CNN backbone lacks depth; limited by dataset size and diversity Attention Mechanisms or Feature Refinement [ 34 ] Custom 3D UNet CNN for segmentation BRATS 2018 and 2019 Average DSC of 85.18% for BraTS 2018 and 83.68% for BraTS 2019 No ARA, lacks explicit preservation of frequency features. 3D Dual-Branch CNN Architecture [ 36 ] 2D Parallel, Attention-Enhanced customized CNN: ResNet50 with VGG16 Figshare 98.04% accuracy Relies on pre-segmented tumors for accuracy, lacks full 3D feature fusion and HFIR. 3D Dual-Branch CNN Architecture [ 35 ] Custom 3D CNN with multi-branch attention mechanism BraTS 2020 and 2021 Average DSC of 87.20% for BraTS 2020 and 87.03% for BRATS 2021 Employs only early fusion; no frequency or region-specific module 3D Dual-Branch CNN Architecture Abbreviations: MANet: Multi-level Attention Network; CNN: Convolutional Neural Network; ADNI: Alzheimer&#8217;s Disease Neuroimaging Initiative; LPIF: Local Pixel Inhomogeneity Factor; MS: Multiple Sclerosis; LSTM: Long Short-Term Memory; BraTS: Brain Tumor Segmentation Challenge; MMIDFNet: Multimodal MRI Image Decision Fusion Network; CPMRP: Computational Precision Medicine: Radiology-Pathology; MMDNet: Multimodal DenseNet; SGLDM: Spatial Gray Level Dependence Matrix; DSC: Dice Similarity Coefficient; Grad-CAM: Gradient-weighted Class Activation Mapping; SSIM: Structural Similarity Index Measure; RDUNet: Residual Dense Network; BBAutoNet: Block-based Autoencoder Network; VGG: Visual Geometry Group. TABLE 2. Comparison of the proposed approach with several well-known multiclass classification models using accuracy (ACC), sensitivity (SEN), and specificity (SPE). Here, &#8220;T&#8221; indicates the tiny version of the respective model, and &#8220;S&#8221; denotes the small one. Model ACC (%) SEN (%) SPE (%) SqueezeNet [ 48 ] 61.90 36.11 68.63 ConvNext-T [ 49 ] 21.43 34.78 67.09 EfficientNet-BO [ 40 ] 59.52 33.33 66.67 ResNeXt-50 [ 50 ] 73.81 72.78 86.91 UniFormer-S [ 51 ] 85.71 75.56 89.30 Proposed 92.86 80.00 94.12 TABLE 3. Comparison of the proposed approach with several well-known models based on F1-score, Cohen&#8217;s kappa, and their differences. Model F1-Score (%) (95% CI) Cohen Kappa (%) (95% CI) Diff SqueezeNet [ 48 ] 30.73 (20.00, 53.33) 7.13 (0.00, 34.78) 51.26 (50.80, 51.71) ConvNext-T [ 49 ] 18.82 (3.51, 38.7) 1.33 (0, 17.42) 63.16 (62.7, 63.60) EfficientNet-B0 [ 40 ] 25.3 (18.39, 41.67) 0.00 (0.00,0.00) 56.35 (55.95, 56.73) ResNeXt-50 [ 50 ] 65.82 (43.21, 88.57) 54.75 (24.32, 83.59) 16.16 (15.67, 16.65) UniFormer-S [ 51 ] 76.37 (48.21, 100.00) 70.57 (38.24, 100.00) 5.01 (5.09, 6.14) Proposed 81.98 (62.21, 100.00) 85.69 (62.21, 100.00) &#8211; TABLE 4. An ablation study of key components of the proposed 3D model. DW-MRI T2-MRI 3D ResNet HFIR ARA ACC (%) SEN (%) SPE (%) &#10003; &#10003; 85.71 63.89 89.08 &#10003; &#10003; 83.33 65.00 87.12 &#10003; &#10003; &#10003; 83.33 70.33 87.12 &#10003; &#10003; &#10003; &#10003; 78.57 55.56 82.35 &#10003; &#10003; &#10003; &#10003; &#10003; 92.86 80.00 94.12"
}