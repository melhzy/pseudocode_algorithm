{
  "pmcid": "PMC12682464",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:25.663079",
  "metadata": {
    "journal_title": "Quantitative Imaging in Medicine and Surgery",
    "journal_nlm_ta": "Quant Imaging Med Surg",
    "journal_iso_abbrev": "Quant Imaging Med Surg",
    "journal": "Quantitative Imaging in Medicine and Surgery",
    "pmcid": "PMC12682464",
    "doi": "10.21037/qims-2025-1321",
    "title": "Dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN): a computed tomography-integrated multimodal network for positron emission tomography denoising",
    "year": "2025",
    "month": "11",
    "day": "21",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "21"
    },
    "authors": [
      "Liu Yang",
      "Zou Guanglu",
      "Li Tao",
      "Liu Haojia"
    ],
    "abstract": "Background Positron emission tomography (PET) requires injection of radioactive tracers, which entails concerns regarding radiation exposure. Thus, there is a need for low-dose PET (LDPET) imaging that maintains full-dose PET (FDPET) quality while reducing tracer dosage. However, dose reduction often degrades image quality. Although deep learning methods have shown promise, denoising small lesions remains challenging. To address this, we propose a dual-branch network that incorporates structural information from computed tomography (CT) to enhance LDPET quality, preserve fine details, and improve small-lesion imaging. Methods We propose the dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN), a dual-branch multimodal network that leverages structural priors from paired CT images to enhance edge detail in PET reconstructions. Specifically, the network architecture consists of two parallel branches that independently extract modality-specific features from PET and CT inputs, followed by an effective fusion mechanism for reconstructing high-quality PET images. The overall architecture adopts a residual encoder-decoder design, with dedicated encoder and decoder modules at the input and output stages, respectively. It is important to note that although minor misalignments between CT and PET images may occur due to respiratory motion or patient movement, our dual-branch framework is inherently robust to such inconsistencies. For data acquisition, LDPET images were generated with an acquisition time of 30 seconds per bed position, while FDPET images were acquired over 150 seconds. For performance evaluation, we conducted extensive comparisons with several state-of-the-art deep learning methods. Moreover, the standardized uptake value (SUV) was included as an additional clinical indicator. Results The proposed DB-REDCNN demonstrated superior performance across multiple evaluation metrics. In quantitative analysis, it achieved the largest improvements compared with LDPET images in root mean square error (0.0023±0.0075), peak signal-to-noise ratio (PSNR) (1.2314±4.8354), and the structural similarity index measure (0.0078±0.0153), with a significantly higher PSNR than REDCNN and consistent advantages across all three metrics (P<0.05). Edge sharpness assessments further confirmed the superiority of DB-REDCNN, which yielded the highest |K| values across five tumor slices with statistically significant improvements over competing models (P<0.01). In SUV evaluation, although the SUV_mean error of the DB-REDCNN was slightly higher than that of LDPET, the proposed method markedly reduced SUV_max error and outperformed all other approaches. Conclusions The proposed DB-REDCNN model, by integrating structural priors from CT through a multibranch architecture, enhances the synthesis of FDPET from LDPET. It achieves superior quantitative performance, improves edge sharpness in lesion regions, and preserves peak SUV information, thereby offering a clinically valuable approach for radiation dose reduction in PET imaging.",
    "keywords": [
      "Positron emission tomography (PET)",
      "computed tomography (CT)",
      "low-dose",
      "multibranch",
      "deep learning"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"iso-abbrev\">Quant Imaging Med Surg</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1905</journal-id><journal-id journal-id-type=\"pmc-domain\">qims</journal-id><journal-id journal-id-type=\"publisher-id\">QIMS</journal-id><journal-title-group><journal-title>Quantitative Imaging in Medicine and Surgery</journal-title></journal-title-group><issn pub-type=\"ppub\">2223-4292</issn><issn pub-type=\"epub\">2223-4306</issn><publisher><publisher-name>AME Publications</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12682464</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12682464.1</article-id><article-id pub-id-type=\"pmcaid\">12682464</article-id><article-id pub-id-type=\"pmcaiid\">12682464</article-id><article-id pub-id-type=\"doi\">10.21037/qims-2025-1321</article-id><article-id pub-id-type=\"publisher-id\">qims-15-12-11778</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Original Article</subject></subj-group></article-categories><title-group><article-title>Dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN): a computed tomography-integrated multimodal network for positron emission tomography denoising</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"Y\">Yang</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Zou</surname><given-names initials=\"G\">Guanglu</given-names></name><contrib-id contrib-id-type=\"orcid\" authenticated=\"false\">https://orcid.org/0009-0006-6447-7109</contrib-id><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"T\">Tao</given-names></name><xref rid=\"aff1\" ref-type=\"aff\">\n<sup>1</sup>\n</xref></contrib><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Liu</surname><given-names initials=\"H\">Haojia</given-names></name><xref rid=\"aff2\" ref-type=\"aff\">\n<sup>2</sup>\n</xref></contrib><aff id=\"aff1\"><label>1</label><institution content-type=\"dept\">School of Electronics and Information</institution>, <institution>Zhengzhou University of Light Industry</institution>, <addr-line>Zhengzhou</addr-line>, <country country=\"cn\">China</country>;</aff><aff id=\"aff2\"><label>2</label><institution content-type=\"dept\">Department of Medical Imaging</institution>, <institution>The Fifth Affiliated Hospital of Zhengzhou University</institution>, <addr-line>Zhengzhou</addr-line>, <country country=\"cn\">China</country></aff></contrib-group><author-notes><fn id=\"afn1\"><p><italic toggle=\"yes\">Contributions:</italic> (I) Conception and design: Y Liu, G Zou; (II) Administrative support: T Li, Y Liu; (III) Provision of study materials or patients: H Liu; (IV) Collection and assembly of data: G Zou; (V) Data analysis and interpretation: G Zou; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors.</p></fn><corresp id=\"cor1\"><italic toggle=\"yes\">Correspondence to:</italic> Tao Li, PhD. School of Electronics and Information, Zhengzhou University of Light Industry, No. 136 Kexue Avenue, Zhengzhou 450000, China. Email: <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"susiecash1220@gmail.com\">susiecash1220@gmail.com</email>; Haojia Liu, M.Eng. Department of Medical Imaging, The Fifth Affiliated Hospital of Zhengzhou University, No.3 Kangfuqian Street, Zhengzhou 450052, China. Email: <email xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"2798617484@qq.com\">2798617484@qq.com</email>.</corresp></author-notes><pub-date pub-type=\"epub\"><day>21</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"ppub\"><day>01</day><month>12</month><year>2025</year></pub-date><volume>15</volume><issue>12</issue><issue-id pub-id-type=\"pmc-issue-id\">502028</issue-id><fpage>11778</fpage><lpage>11794</lpage><history><date date-type=\"received\"><day>09</day><month>6</month><year>2025</year></date><date date-type=\"accepted\"><day>18</day><month>9</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>01</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>09</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-09 00:25:14.317\"><day>09</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 AME Publishing Company.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>AME Publishing Company.</copyright-holder><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><italic toggle=\"yes\">Open Access Statement:</italic> This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">https://creativecommons.org/licenses/by-nc-nd/4.0</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"qims-15-12-11778.pdf\"/><abstract><sec><title>Background</title><p>Positron emission tomography (PET) requires injection of radioactive tracers, which entails concerns regarding radiation exposure. Thus, there is a need for low-dose PET (LDPET) imaging that maintains full-dose PET (FDPET) quality while reducing tracer dosage. However, dose reduction often degrades image quality. Although deep learning methods have shown promise, denoising small lesions remains challenging. To address this, we propose a dual-branch network that incorporates structural information from computed tomography (CT) to enhance LDPET quality, preserve fine details, and improve small-lesion imaging.</p></sec><sec><title>Methods</title><p>We propose the dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN), a dual-branch multimodal network that leverages structural priors from paired CT images to enhance edge detail in PET reconstructions. Specifically, the network architecture consists of two parallel branches that independently extract modality-specific features from PET and CT inputs, followed by an effective fusion mechanism for reconstructing high-quality PET images. The overall architecture adopts a residual encoder-decoder design, with dedicated encoder and decoder modules at the input and output stages, respectively. It is important to note that although minor misalignments between CT and PET images may occur due to respiratory motion or patient movement, our dual-branch framework is inherently robust to such inconsistencies. For data acquisition, LDPET images were generated with an acquisition time of 30 seconds per bed position, while FDPET images were acquired over 150 seconds. For performance evaluation, we conducted extensive comparisons with several state-of-the-art deep learning methods. Moreover, the standardized uptake value (SUV) was included as an additional clinical indicator.</p></sec><sec><title>Results</title><p>The proposed DB-REDCNN demonstrated superior performance across multiple evaluation metrics. In quantitative analysis, it achieved the largest improvements compared with LDPET images in root mean square error (0.0023&#177;0.0075), peak signal-to-noise ratio (PSNR) (1.2314&#177;4.8354), and the structural similarity index measure (0.0078&#177;0.0153), with a significantly higher PSNR than REDCNN and consistent advantages across all three metrics (P&lt;0.05). Edge sharpness assessments further confirmed the superiority of DB-REDCNN, which yielded the highest |K| values across five tumor slices with statistically significant improvements over competing models (P&lt;0.01). In SUV evaluation, although the SUV_mean error of the DB-REDCNN was slightly higher than that of LDPET, the proposed method markedly reduced SUV_max error and outperformed all other approaches.</p></sec><sec><title>Conclusions</title><p>The proposed DB-REDCNN model, by integrating structural priors from CT through a multibranch architecture, enhances the synthesis of FDPET from LDPET. It achieves superior quantitative performance, improves edge sharpness in lesion regions, and preserves peak SUV information, thereby offering a clinically valuable approach for radiation dose reduction in PET imaging.</p></sec></abstract><kwd-group kwd-group-type=\"author\"><title>Keywords: </title><kwd>Positron emission tomography (PET)</kwd><kwd>computed tomography (CT)</kwd><kwd>low-dose</kwd><kwd>multibranch</kwd><kwd>deep learning</kwd></kwd-group><funding-group><award-group><funding-source id=\"sp1\">the Henan Provincial Science and Technology Research Project</funding-source><award-id rid=\"sp1\">No. 252102310453</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\"><title>Introduction</title><p>The high sensitivity of positron emission tomography (PET) in detecting metabolic abnormalities and tumor activity has led to its widespread application in clinical oncology, neurology, and cardiovascular disease diagnosis. However, acquiring high-quality PET images typically requires the administration of relatively large doses of radioactive tracers [such as 18F-fluorodeoxyglucose (<sup>18</sup>F-FDG)], raising concerns regarding the potential carcinogenic risks associated with radiation exposure (<xref rid=\"r1\" ref-type=\"bibr\">1</xref>). Although the risk from a single PET scan is relatively low, the increasing utilization of high-dose imaging modalities such as PET-computed tomography (PET/CT) has contributed to a cumulative radiation burden at the population level, which may constitute a significant public health concern (<xref rid=\"r2\" ref-type=\"bibr\">2</xref>). Therefore, the development of imaging technologies that can substantially reduce radiation exposure without compromising diagnostic image quality has become a critical focus in contemporary nuclear medicine research.</p><p>To enhance image quality while reducing tracer dosage or acquisition time, previous studies have explored the simulation of full-dose PET (FDPET) images using low-dose PET (LDPET) scans. In the early stages, efforts primarily focused on non-deep learning approaches, such as total variation (TV) regularization (<xref rid=\"r3\" ref-type=\"bibr\">3</xref>), sparse representation techniques (<xref rid=\"r4\" ref-type=\"bibr\">4</xref>), and optimization models incorporating structural constraints (<xref rid=\"r5\" ref-type=\"bibr\">5</xref>). These methods typically rely on handcrafted priors&#8212;such as edge preservation and image smoothing&#8212;to guide the reconstruction process. Despite being theoretically beneficial, they often face limitations of high computational cost, poor generalizability, and difficulty in parameter tuning. Furthermore, their dependence on fixed priors frequently results in oversmoothing and the loss of fine anatomical details, potentially compromising diagnostic sensitivity.</p><p>In recent years, the rapid advancement of deep learning has brought about significant breakthroughs in medical image processing, catalyzing the development of numerous neural network-based models for enhancing LDPET and propelling the field forward. A prominent example is the residual encoder-decoder convolutional neural network (REDCNN) model, which introduced the use of residual autoencoders for denoising low-dose CT (LDCT) images, effectively reducing noise while preserving critical structural information (<xref rid=\"r6\" ref-type=\"bibr\">6</xref>); this architectural approach has since been adapted for PET image enhancement. For PET denoising, a variety of network architectures have been proposed, including three-dimensional conditional generative adversarial networks (3D-cGANs) (<xref rid=\"r7\" ref-type=\"bibr\">7</xref>), CycleWGAN (<xref rid=\"r8\" ref-type=\"bibr\">8</xref>), and SubtlePET (<xref rid=\"r9\" ref-type=\"bibr\">9</xref>), all of which leverage adversarial training and perceptual loss to maintain lesion contrast and ensure consistency in standardized uptake value (SUV), achieving promising results. Notably, SubtlePET has demonstrated subjectively comparable image quality to FDPET and exhibited high lesion detection consistency in large-scale clinical validation, further underscoring the clinical utility of deep learning in PET imaging.</p><p>Despite recent advancements, most existing approaches remain constrained to single-modality reconstruction, relying solely on information contained within PET images. This limitation hampers the recovery of structural details, particularly under ultralow-dose or short acquisition conditions. In contrast, CT images, which exhibit high resolution and clear anatomical structures, offer valuable complementary information. When effectively integrated into the PET reconstruction pipeline, they have the potential to improve edge fidelity and structural interpretability. However, the primary value of CT lies in providing structural priors and anatomical mapping. For small lesions with low contrast or those that are not visible on CT, its direct contribution is limited; nonetheless, the overall enhancement in PET image quality indirectly improves the detectability of such lesions. Consequently, multimodal reconstruction strategies have garnered increased attention. Some studies have incorporated CT images alongside LDPET through image-level registration to train neural networks (<xref rid=\"r5\" ref-type=\"bibr\">5</xref>), while others have employed structurally constrained frameworks, such as anatomy-guided networks (<xref rid=\"r10\" ref-type=\"bibr\">10</xref>), to enforce anatomical priors to promote structural consistency. Additionally, bidirectional mapping GAN (<xref rid=\"r11\" ref-type=\"bibr\">11</xref>) employs cross-modal latent space alignment to facilitate PET-magnetic resonance imaging (MRI) fusion, and multimodal super-resolved q-space deep learning (MSR-q-DL) (<xref rid=\"r12\" ref-type=\"bibr\">12</xref>) leverages attention mechanisms to guide intermodal information flow. Collectively, these approaches underscore the pivotal role of structural imaging in enhancing PET reconstruction quality under low-dose scenarios.</p><p>The study of multimodal approaches extends beyond theoretical modeling, demonstrating strong transferability across various clinical application scenarios. In LDPET imaging, for instance, deep learning methods have been shown to outperform traditional filters such as Gaussian and nonlocal means in maintaining perceptual image quality (<xref rid=\"r13\" ref-type=\"bibr\">13</xref>). In LDCT imaging, the implementation of Wasserstein GANs (WGANs) has provided valuable technical insights for cross-modal enhancement, effectively balancing noise suppression with structural detail preservation (<xref rid=\"r14\" ref-type=\"bibr\">14</xref>). Moreover, recent progress in brain structure segmentation and image registration tasks has underscored the importance of incorporating anatomical priors from CT or MRI to improve the overall quality of medical image processing (<xref rid=\"r15\" ref-type=\"bibr\">15</xref>).</p><p>The REDCNN network is recognized for its strong performance in terms of conventional evaluation metrics such as the peak signal-to-noise ratio (PSNR) and the structural similarity index measure (SSIM). However, it often produces overly smoothed outputs, which may obscure critical anatomical details in the reconstructed images. To address this limitation, this work introduces an enhanced architecture, termed dual-branch REDCNN (DB-REDCNN), which incorporates co-registered CT images as an auxiliary input branch to provide structural priors during training. The incorporation of CT images as complementary information substantially enhances the network&#8217;s ability to reconstruct high-quality PET images from LDPET inputs. This enhancement is primarily attributed to the high-resolution anatomical information provided by CT, which compensates for the blurred structures and low signal-to-noise ratio (SNR) typically observed in LDPET images due to insufficient tracer dosage (<xref rid=\"r16\" ref-type=\"bibr\">16</xref>).</p><p>CT images are primarily used to depict normal anatomical structures. By incorporating CT data, the network can be expected to be better equipped to identify structural features such as tissue boundaries and organ contours, effectively guiding the generation of FDPET images with consistent morphology and coherent spatial distribution. This, in turn, enhances the clarity and credibility of the reconstructed outputs (<xref rid=\"r17\" ref-type=\"bibr\">17</xref>). Additionally, leveraging CT as a structural prior improves the model&#8217;s generalization ability, allowing it to produce more stable and reliable predictions, particularly in preserving high-frequency details and edge features (<xref rid=\"r18\" ref-type=\"bibr\">18</xref>). Furthermore, the complementary characteristics of CT and LDPET enable the network to learn more comprehensive feature representations, thereby further enhancing image quality and increasing the diagnostic value of the generated outputs in clinical applications (<xref rid=\"r19\" ref-type=\"bibr\">19</xref>).</p><p>Recognizing the limited utility of CT in detecting lesions with low radiotracer uptake or small size, we designed our network architecture and training strategy to treat PET as the primary source of functional information, with CT being exclusively used for structural guidance. To further evaluate the model&#8217;s ability to recover small lesions, we specifically analyze changes in SUV within lesion regions in LDPET images, thereby assessing the model&#8217;s effectiveness in preserving the functional characteristics of these lesions.</p><p>The proposed framework adopts a dual-branch architecture that independently extracts modality-specific features from PET and CT images, while a shared encoder-decoder network is employed to capture long-range dependencies. To further enhance structural representation, a structural similarity constraint has been introduced, reinforcing the boundary information guided by CT. This design enables the model to preserve functional characteristics while significantly improving anatomical detail depiction. Evaluation results demonstrate that DB-REDCNN outperforms existing methods in both conventional image quality metrics and SUV-based assessments. In light of recent advances in research and clinical needs, the proposed approach offers a practical and efficient solution for LDPET imaging, with strong scalability for extension to other multimodal applications such as brain and cardiac imaging.</p></sec><sec sec-type=\"methods\"><title>Methods</title><sec><title>Network architecture</title><p>The proposed DB-REDCNN includes a dual-branch architecture consisting of a main branch and an auxiliary branch. Each branch comprises five convolutional blocks, with each block containing a convolutional layer, a batch normalization layer, and a rectified linear unit (ReLU) activation function. All convolutional layers consist of 96 two-dimensional kernels with a size of 5&#215;5 and a stride of 1. Following independent feature extraction by the two branches, the extracted features are concatenated and passed through five deconvolutional blocks, each replicating the structure of the convolutional blocks, including a convolutional layer, batch normalization, and ReLU activation (<xref rid=\"r20\" ref-type=\"bibr\">20</xref>,<xref rid=\"r21\" ref-type=\"bibr\">21</xref>). The full architecture of the network is illustrated in <xref rid=\"f1\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 1</italic></xref>.</p><fig position=\"float\" id=\"f1\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Architecture of the proposed DB-REDCNN. CONV-BN, convolution batch normalization; CONVTrans, convolutional transpose; CT, computed tomography; DB-REDCNN, dual-branch residual encoder-decoder convolutional neural network; FDPET, full-dose positron emission tomography; PET, positron emission tomography; ReLU, rectified linear unit.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f1.jpg\"/></fig><sec><title>Denoising model</title><p>By constraining the image denoising problem to the image domain (<xref rid=\"r22\" ref-type=\"bibr\">22</xref>), the image denoising problem can be simplified in the context of deep learning as follows: let <inline-formula><mml:math id=\"m1\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denote the LDPET image, <inline-formula><mml:math id=\"m2\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the FDPET image, and C the corresponding CT image. The relationship among them can be formulated as follows:</p><disp-formula id=\"e1\"><mml:math id=\"m3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[1]</label></disp-formula><p>Here, <inline-formula><mml:math id=\"m4\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes a structural correction term extracted from the corresponding CT image C, while <inline-formula><mml:math id=\"m5\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the degradation process mapping FDPET (denoted as Y) to LDPET (denoted as X). This mapping may be ambiguous due to the loss of structural information resulting from dose reduction. The auxiliary bias <inline-formula><mml:math id=\"m6\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, due to the anatomical priors derived from the CT image being embedded, helps the network preserve sharp tissue boundaries and the global anatomical structure during reconstruction. Although CT images cannot directly reveal small, low-contrast lesions, their primary value lies in providing anatomical guidance for structural recovery in PET imaging. Essentially, <inline-formula><mml:math id=\"m7\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> functions as a learnable, spatially variant offset that modulates <inline-formula><mml:math id=\"m8\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, thereby enabling a more accurate approximation of LDPET (denoted as X). The objective then becomes to find a function <inline-formula><mml:math id=\"m9\" display=\"inline\" overflow=\"scroll\"><mml:mi>f</mml:mi></mml:math></inline-formula> such that:</p><disp-formula id=\"e2\"><mml:math id=\"m10\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mrow><mml:mi>arg</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mi>min</mml:mi><mml:msubsup><mml:mrow><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>&#948;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math><label>[2]</label></disp-formula><p>Here, <inline-formula><mml:math id=\"m11\" display=\"inline\" overflow=\"scroll\"><mml:mi>f</mml:mi></mml:math></inline-formula> is treated as the optimal approximation of <inline-formula><mml:math id=\"m12\" display=\"inline\" overflow=\"scroll\"><mml:mi>&#963;</mml:mi></mml:math></inline-formula>.</p></sec><sec><title>Residual encoding network</title><p>A symmetric encoder-decoder architecture, composed of convolutional and deconvolutional layers (<xref rid=\"r23\" ref-type=\"bibr\">23</xref>), as illustrated in <xref rid=\"f2\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 2</italic></xref>, was adopted in this study. To better preserve fine structural details in PET images and minimize the risk of irreversible degradation during deep feature extraction, a residual compensation mechanism is integrated into the PET branch. As opposed to the sole reliance on stacked layers for learning the mapping from input to output, a residual learning strategy (<xref rid=\"r24\" ref-type=\"bibr\">24</xref>) is employed to enhance feature representation and improve reconstruction quality.</p><fig position=\"float\" id=\"f2\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Shortcut in the residual compensation structure. CONVTrans, convolutional transpose.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f2.jpg\"/></fig><p>With the input e denoted as <inline-formula><mml:math id=\"m13\" display=\"inline\" overflow=\"scroll\"><mml:mi>x</mml:mi></mml:math></inline-formula>, the residual mapping can be defined as follows:</p><disp-formula id=\"e3\"><mml:math id=\"m14\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math><label>[3]</label></disp-formula><p>where <inline-formula><mml:math id=\"m15\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the desired underlying mapping, and <inline-formula><mml:math id=\"m16\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the residual function. By reformulating the equation, the following is obtained:</p><disp-formula id=\"e4\"><mml:math id=\"m17\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math><label>[4]</label></disp-formula></sec></sec><sec><title>Loss function</title><p>Since the primary objective of the loss function is to evaluate the discrepancy between the predicted and the ground truth values, L1 loss is used as the core optimization criterion. L1 loss directly penalizes the absolute difference between predictions and the corresponding ground truth, thereby encouraging the model to produce outputs that closely approximate the reference. Smaller differences yield lower loss values, while larger deviations incur proportionally greater penalties.</p><p>Let <inline-formula><mml:math id=\"m18\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the predicted value and <inline-formula><mml:math id=\"m19\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the corresponding ground truth. L1 loss is defined as follows:</p><disp-formula id=\"e5\"><mml:math id=\"m20\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mover accent=\"true\"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math><label>[5]</label></disp-formula><p>where <italic toggle=\"yes\">n</italic> represents the total number of pixels in each image.</p></sec><sec><title>Evaluation metrics</title><p>To quantitatively evaluate the effectiveness of our method, we employed three widely used evaluation metrics: root mean square error (RMSE) (<xref rid=\"r25\" ref-type=\"bibr\">25</xref>), PSNR (<xref rid=\"r26\" ref-type=\"bibr\">26</xref>), and SSIM (<xref rid=\"r27\" ref-type=\"bibr\">27</xref>). Together, these metrics provide a comprehensive framework for assessing both pixel-level accuracy and perceptual similarity between the synthesized PET images and the corresponding full-dose references. These metrics are discussed in detail in the following sections.</p><sec><title>RMSE</title><p>RMSE measures the average pixel-wise deviation between the reconstructed image and the ground truth. A lower RMSE value indicates a closer match to the reference image, reflecting higher reconstruction accuracy.</p><disp-formula id=\"e6\"><mml:math id=\"m21\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:msqrt><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle=\"true\"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt></mml:mrow></mml:math><label>[6]</label></disp-formula><p>Here, <inline-formula><mml:math id=\"m22\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m23\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the pixel intensities of the reconstructed and reference images respectively, while <inline-formula><mml:math id=\"m24\" display=\"inline\" overflow=\"scroll\"><mml:mi>M</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"m25\" display=\"inline\" overflow=\"scroll\"><mml:mi>N</mml:mi></mml:math></inline-formula> are the image dimensions.</p></sec><sec><title>PSNR</title><p>The PSNR assesses the quality of image reconstruction, with a particular focus on preserving pixel intensity fidelity. Higher PSNR values indicate that the reconstructed image is more similar to the reference image in terms of both brightness accuracy and overall visual quality.</p><disp-formula id=\"e7\"><mml:math id=\"m26\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><label>[7]</label></disp-formula><p>Here, <inline-formula><mml:math id=\"m27\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> is the maximum possible pixel value of the image. Since the PSNR is inversely related to RMSE, improving pixel accuracy directly contributes to a higher PSNR.</p></sec><sec><title>SSIM</title><p>SSIM evaluates perceptual similarity by comparing local patterns of pixel intensities that have been normalized for luminance and contrast. It is particularly effective in assessing structural information, such as lesion boundaries and anatomical features, which are critical for clinical interpretation in PET imaging.</p><disp-formula id=\"e8\"><mml:math id=\"m28\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>S</mml:mi><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[8]</label></disp-formula><p>In this equation, <inline-formula><mml:math id=\"m29\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m30\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#956;</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the mean intensities of images <inline-formula><mml:math id=\"m31\" display=\"inline\" overflow=\"scroll\"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id=\"m32\" display=\"inline\" overflow=\"scroll\"><mml:mi>y</mml:mi></mml:math></inline-formula>, respectively; <inline-formula><mml:math id=\"m33\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m34\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msubsup><mml:mi>&#963;</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> are the variances; <inline-formula><mml:math id=\"m35\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>&#963;</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the covariance; and <inline-formula><mml:math id=\"m36\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m37\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are small constants used to stabilize the division.</p></sec></sec><sec><title>SUV evaluation metrics</title><p>To assess quantitative accuracy, volumes of interest (VOIs) were manually delineated within lesion regions by an experienced nuclear medicine physician, using the corresponding CT images as anatomical references. The same VOIs were consistently applied across all PET images to ensure the spatial alignment and reproducibility of SUV measurements. Both <inline-formula><mml:math id=\"m38\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m39\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> were extracted for each VOI; subsequently, the relative errors, referred to as <inline-formula><mml:math id=\"m40\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m41\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>, were calculated with the following formulae:</p><disp-formula id=\"e9\"><mml:math id=\"m42\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[9]</label></disp-formula><disp-formula id=\"e10\"><mml:math id=\"m43\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[10]</label></disp-formula><p>To comprehensively evaluate the ability of different models to maintain the quantitative accuracy of PET imaging, the errors of <inline-formula><mml:math id=\"m44\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"m45\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> within the VOIs were compared across various methods.</p></sec><sec><title>Edge sharpness index</title><p>To analyze the model&#8217;s performance specifically at the lesion boundaries, an edge evaluation was incorporated. For this purpose, slices from the test set containing visibly identifiable small lesions were selected. A novel evaluation metric, the edge sharpness index (<inline-formula><mml:math id=\"m46\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) (<xref rid=\"r28\" ref-type=\"bibr\">28</xref>), was introduced to quantitatively assess tumor boundary delineation performance. The formula used to compute <inline-formula><mml:math id=\"m47\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is as follows:</p><disp-formula id=\"e11\"><mml:math id=\"m48\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#8711;</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>&#8711;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><label>[11]</label></disp-formula><p>where <inline-formula><mml:math id=\"m49\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mo>&#8711;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> is the image gradient map, <inline-formula><mml:math id=\"m50\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#8711;</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum gradient magnitude in the image, and <inline-formula><mml:math id=\"m51\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>&#8711;</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math></inline-formula> is the standard deviation of the gradient values. A higher <inline-formula><mml:math id=\"m52\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> value corresponds to sharper and cleaner image edges.</p></sec><sec><title>Data acquisition</title><p>This study employed whole-body PET images acquired with <sup>18</sup>F-FDG and corresponding CT images obtained with a Vereos PET/CT system (Philps, Amsterdam, the Netherlands). No restrictions were imposed regarding patient sex, age, or disease type; however, cases exhibiting artifacts, registration errors, or lacking apparent lesions were excluded. Registration errors were primarily identified through manual inspection by experienced radiologists, who assessed the spatial alignment of key anatomical structures&#8212;such as the lungs, liver, and heart&#8212;on both PET and CT images. Misalignment of organ boundaries or significant overlap discrepancies was considered indicative of registration errors. To minimize the misalignment potentially caused by respiratory motion, standard breath-hold instructions were provided during image acquisition, and rigid registration was applied during preprocessing to further enhance alignment accuracy. In total, data from 33 patients were included. Each patient received an intravenous injection of <sup>18</sup>F-FDG at an activity of 3.7 MBq/kg based on body weight, which was followed by a resting period of 40 to 90 minutes in a quiet waiting area prior to scanning.</p><p>All scans were performed in the three-dimensional acquisition mode, with a duration of 2.5 minutes per bed position. Image reconstruction was conducted with the ordered subsets expectation maximization (OSEM) algorithm (<xref rid=\"r29\" ref-type=\"bibr\">29</xref>), with both time-of-flight (TOF) and point-spread-function (PSF) modeling techniques being applied (<xref rid=\"r30\" ref-type=\"bibr\">30</xref>). The reconstruction matrix was set to 288&#215;288, with two iterations and six subsets per iteration. Slice thickness and interslice spacing were both maintained at 2 mm. The number of PSF iterations and the regularization factor were set to 1 and 6, respectively. Attenuation correction for PET was performed with an LDCT scan (120 kV and 150 mAs) (<xref rid=\"r31\" ref-type=\"bibr\">31</xref>).</p><p>To simulate LDPET images, a retrospective reconstruction approach was employed with the acquisition time being reduced to 30 seconds and the same reconstruction parameters as those used for FDPET images acquired over 150 seconds being applied. Attenuation, scatter, and random coincidence corrections, as well as the image reconstruction, were performed with vendor-provided software.</p><p>In the dataset, each &#8220;pair&#8221; of images referred to a PET image slice and its corresponding CT image slice. To prevent potential bias caused by interslice correlation within the same patient, data partitioning was conducted on a per-patient basis. Specifically, 10,568 image pairs were allocated for training, 3,656 for validation, and 864 for testing, with no patient overlap across these subsets. This strategy ensured the independence and robustness of the evaluation results. The number of patients and images in the training, test, and validation sets is summarized in <xref rid=\"t1\" ref-type=\"table\"><italic toggle=\"yes\">Table 1</italic></xref>. This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments, and&#160;the study protocol was approved by the Ethics Committee of The Fifth Affiliated Hospital of Zhengzhou University (approval No. KY2025044). Patient consent was waived due to the retrospective nature of the data analysis and the use of anonymized data.</p><table-wrap position=\"float\" id=\"t1\" orientation=\"portrait\"><label>Table 1</label><caption><title>Number of patients and images in the training, test, and validation sets</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"19.58%\" span=\"1\"/><col width=\"24.55%\" span=\"1\"/><col width=\"23.27%\" span=\"1\"/><col width=\"32.6%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Items</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Training, n</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Test, n</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Validation, n</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Patients</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">23</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Images</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">10,568</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">864</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3,656</td></tr></tbody></table></table-wrap></sec><sec><title>Data preprocessing and experimental settings</title><p>Before training on the dataset comprising 33 cases, several preprocessing steps were performed. All PET images were strictly registered to their corresponding CT images during the data preprocessing stage via a voxel similarity-based registration algorithm. The alignment accuracy was further verified by experienced radiologists through visual inspection of key anatomical landmarks&#8212;such as the ventricles and liver boundaries&#8212;to ensure consistency across modalities. Subsequently, both CT and PET images were resampled to a resolution of 256&#215;256 via bilinear interpolation. As illustrated in <xref rid=\"f3\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 3</italic></xref>, the overlay image was employed to demonstrate the registration performance of representative PET/CT pairs from the dataset.</p><fig position=\"float\" id=\"f3\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Visualization of the image generated by overlaying the PET image onto the CT image via pseudocolor representation. CT, computed tomography; PET, positron emission tomography.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f3.jpg\"/></fig><p>To reduce overfitting and improve the network&#8217;s generalization capability, data augmentation techniques were applied, including random horizontal and vertical flips as well as random rotations of up to 45 degrees. As a result, a total of 31,704 image pairs were generated.</p><p>In addition, all images were normalized through the of scaling pixel intensities to the range [0, 1], ensuring a consistent numerical scale across inputs, accelerating network convergence, and mitigating issues such as gradient explosion or vanishing gradients caused by extreme intensity values.</p><p>To further enhance generalization, increase robustness, and encourage the model to focus on local features, a patch-based training strategy was employed. This involved randomly cropping smaller patches from the input images in line with strategies adopted in previous studies (<xref rid=\"r32\" ref-type=\"bibr\">32</xref>-<xref rid=\"r34\" ref-type=\"bibr\">34</xref>).</p><p>To initialize network parameters in a manner compatible with ReLU activation, the Kaiming initialization method (<xref rid=\"r35\" ref-type=\"bibr\">35</xref>) was employed. Model training was conducted with the Adam optimizer (<xref rid=\"r36\" ref-type=\"bibr\">36</xref>), which adaptively adjusts the learning rate of each parameter with decay rates set to &#946;1 =0.9 and &#946;2 =0.999. Additional training configurations, including the number of epochs, batch size, patch size, and learning rate, are summarized in <xref rid=\"t2\" ref-type=\"table\"><italic toggle=\"yes\">Table 2</italic></xref>.</p><table-wrap position=\"float\" id=\"t2\" orientation=\"portrait\"><label>Table 2</label><caption><title>Hyperparameters during training</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"15.76%\" span=\"1\"/><col width=\"16.85%\" span=\"1\"/><col width=\"16.85%\" span=\"1\"/><col width=\"16.85%\" span=\"1\"/><col width=\"16.85%\" span=\"1\"/><col width=\"16.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Items</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Epochs, n</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Learning rate</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Batch size, n</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Patches, n</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Patch size</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Value</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">50</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1e&#8722;4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">10</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">10</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">64</td></tr></tbody></table></table-wrap><p>The selection of hyperparameters was guided by empirical experience from prior experiments and informed by preliminary results on the validation set. Specifically, the number of training epochs was chosen to ensure model convergence while minimizing the risk of overfitting. Batch size and patch size were determined by balancing training stability with Graphics Processing Unit (GPU) memory limitations. The initial learning rate was selected to facilitate rapid convergence during the early stages of training. A range of hyperparameter combinations was tested&#8212;for example, batch sizes ranging from 4 to 16, patch sizes from 32 to 64, and learning rates from 1e&#8722;3 to 1e&#8722;5. The final configuration was selected based on optimal performance in terms of validation loss and PSNR.</p><p>All experiments were conducted via Python 3.8 (Python Software Foundation, Wilmington, DE, USA) with the PyTorch framework (<xref rid=\"r37\" ref-type=\"bibr\">37</xref>) on a Dell Workstation (Dell Technologies, Round Rock, TX, USA) equipped with a 12 vCPU Intel(R) Xeon(R) Platinum 8352V CPU and a vGPU (32 GB) (Intel, Santa Clara, CA, USA).</p></sec><sec><title>Models used in the ablation study</title><p>To evaluate the effectiveness of the dual-branch architecture in the DB-REDCNN network, a series of ablation experiments was conducted. All experiments were performed on the same dataset and computing platform to ensure consistent and fair comparisons of both quantitative and qualitative performance. Specifically, as illustrated in <xref rid=\"f4\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 4</italic></xref>, we compared three models: (I) the original REDCNN (REDCNN uni), which uses only PET images as input; (II) a modified version (REDCNN multi), in which PET and CT images are concatenated along the channel dimension; and (III) the proposed DB-REDCNN with a dual-branch architecture.</p><fig position=\"float\" id=\"f4\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 4</label><caption><p>The input image modalities of the three models. (A) REDCNN model with a single PET image used as input. (B) REDCNN multimodel, in which PET and CT images are concatenated along the vertical dimension as input. (C) Our proposed dual-branch network architecture. CT, computed tomography; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f4.jpg\"/></fig></sec></sec><sec sec-type=\"results\"><title>Results</title><sec><title>Comparison of loss functions</title><p>We conducted comparative experiments using L1 loss and other classical loss functions, including SSIM loss and mean square error (MSE) loss. The results demonstrated that L1 loss yielded the best performance in our model. Based on the data in <xref rid=\"t3\" ref-type=\"table\"><italic toggle=\"yes\">Table 3</italic></xref>, after simple numerical calculations, the changes in the evaluation metrics (RMSE, PSNR, SSIM) for the model using L1 loss relative to the 30s-PET images were 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153. These results were superior to those of the model using SSIM loss (&#8211;0.0072&#177;0.0333, &#8211;0.5547&#177;6.6596, and &#8211;0.0037&#177;0.0427) and the model using MSE loss (0.0014&#177;0.0079, 0.7131&#177;4.6028, and &#8211;0.0004&#177;0.0177). Here, the values in parentheses represent, respectively, the differences in the mean RMSE, PSNR, and SSIM between the PET images generated by models using different loss functions and the 30s-PET images.</p><table-wrap position=\"float\" id=\"t3\" orientation=\"portrait\"><label>Table 3</label><caption><title>Evaluation of the loss effect</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"22.49%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><col width=\"25.83%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Loss</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">RMSE</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">PSNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SSIM</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">30-s PET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0195&#177;0.0083</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.1759&#177;4.4807</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9626&#177;0.0180</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">L1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0172&#177;0.0075</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">36.4073&#177;4.8354</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9704&#177;0.0153</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">SSIM</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0267&#177;0.0333</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">34.6212&#177;6.6596</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9589&#177;0.0427</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">MSE</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0181&#177;0.0079</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.8890&#177;4.6028</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9622&#177;0.0177</td></tr></tbody></table><table-wrap-foot><p>The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. MSE, mean square error; PET, positron emission tomography; PSNR, peak signal-to-noise ratio; RMSE, root mean square error; SSIM, structural similarity index measure.</p></table-wrap-foot></table-wrap></sec><sec><title>Quantitative evaluation</title><p>The other models evaluated in this study included REDCNN, SubtlePET, and UNET_RA (<xref rid=\"r38\" ref-type=\"bibr\">38</xref>). REDCNN is a unimodal network that exclusively utilizes LDPET images as input, while SubtlePET and UNET_RA are multimodal architectures that integrate both LDPET and the corresponding CT images as inputs.</p><p>Qualitative visual assessments of FDPET, LDPET, and the PET images reconstructed by the models are provided in the axial, coronal, and sagittal views in <xref rid=\"f5\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 5</italic></xref>. Based on visual inspection, all models improved the quality of LDPET images, with our proposed method and REDCNN yielding relatively better perceptual results. However, visual inspection alone is insufficient for rigorously demonstrating the superiority of our approach; Therefore, the quantitative results are presented in <xref rid=\"t4\" ref-type=\"table\"><italic toggle=\"yes\">Table 4</italic></xref> and <xref rid=\"f6\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 6</italic></xref> to support these observations.</p><fig position=\"float\" id=\"f5\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Sample images of LDPET (30 s), FDPET, and estimated PET images from the proposed, REDCNN, SubtlePET, and UNET_RA methods. The arrows in the figure indicate the location of the small lesions. Rows 1, 2, and 3 are PET images in the axial, coronal, and sagittal directions, respectively. The grayscale bar on the top indicates the SUV (g/mL). FDPET, full-dose PET; LDPET, low-dose PET; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; SUV, standardized uptake value.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f5.jpg\"/></fig><table-wrap position=\"float\" id=\"t4\" orientation=\"portrait\"><label>Table 4</label><caption><title>Evaluation of quantitative indicators</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"22.49%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><col width=\"25.83%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">RMSE</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">PSNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SSIM</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">30-s PET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0195&#177;0.0083</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.1759&#177;4.4807</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9626&#177;0.0180</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0185&#177;0.0089</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.9540&#177;5.2702</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9714&#177;0.0157</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">UNET_RA</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0187&#177;0.0080</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.5428&#177;4.4837</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9464&#177;0.0205</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">SubtlePET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0192&#177;0.0083</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.3041&#177;4.4202</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9068&#177;0.0262</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Proposed</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0172&#177;0.0075</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">36.4073&#177;4.8354</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9704&#177;0.0153</td></tr></tbody></table><table-wrap-foot><p>The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure.</p></table-wrap-foot></table-wrap><fig position=\"float\" id=\"f6\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Box plots for the evaluation of quantitative indicators (data from <xref rid=\"t4\" ref-type=\"table\"><italic toggle=\"yes\">Table 4</italic></xref>). The improvement of model-estimated PET images in RMSE, PSNR, and SSIM with respect to the original LDPET images. LDPET, low-dose PET; PET, positron emission tomography; PSNR, peak signal-to-noise ratio; RMSE, root mean square error; SSIM, structural similarity index measure.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f6.jpg\"/></fig><p>Compared with those of LDPET images, the improvements of model-estimated PET images in terms of RMSE, PSNR, and SSIM were, respectively, 0.001&#177;0.0089, 0.7781&#177;5.2702, and 0.0088&#177;0.0157 for REDCNN; 0.0008&#177;0.0080, 0.3669&#177;4.4837, and &#8211;0.0162&#177;0.0205 for UNET_RA; 0.0003&#177;0.0083, 0.1282&#177;4.4202, and &#8211;0.0558&#177;0.0262 for SubtlePET; and 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153 for the proposed model. All estimated PET images outperformed the original low-dose data in terms of RMSE and PSNR. However, UNET_RA and SubtlePET showed lower SSIM values than did the original LDPET images. The proposed approach demonstrated consistently favorable results across all three metrics, achieving a notably higher PSNR than REDCNN, although its SSIM was slightly below that of REDCNN.</p><p>To assess the statistical significance of the performance differences across the methods, significance tests were conducted for all methods. As shown in <xref rid=\"t5\" ref-type=\"table\"><italic toggle=\"yes\">Table 5</italic></xref>, all models&#8212;except SubtlePET&#8212;showed statistically significant differences (P&lt;0.05) across all metrics when compared to the 30-s PET data. SubtlePET did not exhibit statistically significant differences in RMSE or PSNR values (P&gt;0.05). Notably, the proposed method demonstrated the most significant improvements in RMSE and PSNR values, further validating its superior performance.</p><table-wrap position=\"float\" id=\"t5\" orientation=\"portrait\"><label>Table 5</label><caption><title>Statistical significance (P values) of the differences between each model and 30-s PET</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"22.49%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><col width=\"25.83%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">RMSE</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">PSNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SSIM</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.42838e&#8722;09</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.70264e&#8722;15</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.61526e&#8722;72</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">UNET_RA</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.000195392</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.000146802</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.34925e&#8722;72</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">SubtlePET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.294267589</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.210342034</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.61363e&#8722;72</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Proposed</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.07e&#8722;35</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.19352e&#8722;56</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">2.34465e&#8722;72</td></tr></tbody></table><table-wrap-foot><p>PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure.</p></table-wrap-foot></table-wrap><p><xref rid=\"f7\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 7</italic></xref> displays the radioactivity concentration (AC; Bq/mL) distribution curves across the lesion region, with a zoomed-in view focusing on the peak area to more clearly illustrate the variations in AC within this region. Compared with the other models&#8212;REDCNN (green), SubtlePET (red), and UNET_RA (purple)&#8212;the model we developed (orange) aligned most closely with the full-dose curve (brown).</p><fig position=\"float\" id=\"f7\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 7</label><caption><p>AC contrast curve covering the lesion area. The inset in the upper left corner is a magnified image near the peak AC value. The image in the lower right corner shows the pixel region selected (covering the lesion area). For visualization purposes, AC values were proportionally scaled during plotting. AC, radioactivity concentration; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f7.jpg\"/></fig></sec><sec><title>SUV evaluation</title><p>As illustrated in <xref rid=\"f8\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 8</italic></xref>, although the <inline-formula><mml:math id=\"m53\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> of the proposed approach was slightly higher than that of the original LDPET (30 s) image, it surpassed all other compared techniques&#8212;including LDPET, REDCNN, SubtlePET, and UNET_RA&#8212;in all remaining metrics, exhibiting a particularly pronounced advantage in reducing <inline-formula><mml:math id=\"m54\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mi>S</mml:mi><mml:mi>U</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula> (<xref rid=\"r39\" ref-type=\"bibr\">39</xref>).</p><fig position=\"float\" id=\"f8\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 8</label><caption><p><italic toggle=\"yes\">SUV<sub>mean</sub></italic>\n<italic toggle=\"yes\">error</italic> and <italic toggle=\"yes\">SUV<sub>max</sub> error</italic> in lesion VOIs. SUV, standardized uptake value; VOI, volume of interest.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f8.jpg\"/></fig></sec><sec><title>Ablation study</title><p><xref rid=\"f9\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 9</italic></xref> illustrates both the cross-sectional and whole-body PET images, highlighting the impact of different network branch configurations on the REDCNN model. In these experiments, REDCNN uni was considered to be a single-branch network that processes only PET images as input, whereas REDCNN multi was considered to be a network that takes PET and CT images concatenated along the channel dimension as input.</p><fig position=\"float\" id=\"f9\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Sample images of LDPET (30 s), FDPET, and estimated PET images from the proposed, REDCNN uni, and REDCNN multi methods. The arrows in the figure indicate the locations of the small lesions. Rows 1, 2, and 3 are PET images in the axial, coronal, and sagittal direction, respectively. The grayscale bar on the top indicates the SUV (g/mL). FDPET, full-dose positron emission tomography; LDPET, low-dose positron emission tomography; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; SUV, standardized uptake value.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f9.jpg\"/></fig><p><xref rid=\"t6\" ref-type=\"table\"><italic toggle=\"yes\">Table 6</italic></xref> summarizes the RMSE, PSNR, and SSIM values for each model, reflecting their respective improvements over the LDPET images: the RMSE, PSNR, and SSIM values for the proposed model were 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153, respectively; those for REDCNN uni were 0.001&#177;0.0089, 0.7781&#177;5.2702, and 0.0088&#177;0.0157, respectively; and those for REDCNN multi were 0.0011&#177;0.0087, 0.8000&#177;5.2093, and 0.0091&#177;0.0155, respectively. Although REDCNN multi exhibited slight improvements over REDCNN uni in RMSE, PSNR, and SSIM, the significance tests (<xref rid=\"t7\" ref-type=\"table\"><italic toggle=\"yes\">Table 7</italic></xref>) indicated that the P values for RMSE and PSNR were both greater than 0.05, indicating that these differences were not statistically significant. Only the improvement in SSIM reached statistical significance (P&lt;0.05). In contrast, the proposed model yielded P values well below 0.05 across all metrics, confirming that its quantitative gains are statistically significant and supporting the effectiveness of the multibranch design strategy.</p><table-wrap position=\"float\" id=\"t6\" orientation=\"portrait\"><label>Table 6</label><caption><title>Evaluation of quantitative indicators</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"22.49%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><col width=\"25.83%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Methods</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">RMSE</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">PSNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SSIM</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">30-s PET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0195&#177;0.0083</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.1759&#177;4.4807</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9626&#177;0.0180</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Proposed</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0172&#177;0.0075</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">36.4073&#177;4.8354</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9704&#177;0.0153</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN uni</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0185&#177;0.0089</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.9540&#177;5.2702</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9714&#177;0.0157</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN multi</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.0184&#177;0.0087</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">35.9759&#177;5.2093</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.9717&#177;0.0155</td></tr></tbody></table><table-wrap-foot><p>The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure.</p></table-wrap-foot></table-wrap><table-wrap position=\"float\" id=\"t7\" orientation=\"portrait\"><label>Table 7</label><caption><title>Statistical significance (P values) of the differences between each model and REDCNN uni</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"22.49%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><col width=\"25.83%\" span=\"1\"/><col width=\"25.84%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">RMSE</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">PSNR</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SSIM</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN multi</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.104880231</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.145233327</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">1.53e&#8722;28</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">Proposed</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">8.676e&#8722;18</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.01201e&#8722;13</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.71875e&#8722;50</td></tr></tbody></table><table-wrap-foot><p>PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure.</p></table-wrap-foot></table-wrap></sec><sec><title>Edge evaluation</title><p>As shown in <xref rid=\"f10\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 10</italic></xref>, Sobel operators were first applied to the selected PET slices containing lesion regions to generate corresponding gradient images; lesion areas are highlighted with red circles. Compared to other models, the proposed model produced more distinct lesion boundaries, indicating superior performance in noise suppression and detail preservation&#8212;closely approximating the quality of FDPET images.</p><fig position=\"float\" id=\"f10\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 10</label><caption><p>The gradient images of the lesion area slices providing a more intuitive comparison of contour clarity. The lesion regions are highlighted with red circles. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f10.jpg\"/></fig><p>Subsequently, regions of interest (ROIs) were delineated on the selected slices, as indicated by the rectangular box on the PET image in the upper right corner of <xref rid=\"f11\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 11</italic></xref>. The same ROI mask was applied across all reconstructed images, and the corresponding <inline-formula><mml:math id=\"m55\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values were computed. These values were then visualized in the form of a histogram (<xref rid=\"f11\" ref-type=\"fig\"><italic toggle=\"yes\">Figure 11</italic></xref>), indicating that all other models yielded lower <inline-formula><mml:math id=\"m56\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values than the proposed model, indicating that our model achieved the best edge sharpness performance.</p><fig position=\"float\" id=\"f11\" fig-type=\"figure\" orientation=\"portrait\"><label>Figure 11</label><caption><p>Histogram of edge sharpness index |<italic toggle=\"yes\">K</italic>| value. The white box indicates the ROI area used for calculating the |<italic toggle=\"yes\">K</italic>| value. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; ROI, region of interest.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"qims-15-12-11778-f11.jpg\"/></fig><p>To ensure the generalizability of the observed improvements, five distinct tumor slices were selected, and the <inline-formula><mml:math id=\"m57\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values for each model were computed, as shown in <xref rid=\"t8\" ref-type=\"table\"><italic toggle=\"yes\">Table 8</italic></xref>. A paired<italic toggle=\"yes\"> t</italic>-test was performed, and the results (<xref rid=\"t9\" ref-type=\"table\"><italic toggle=\"yes\">Table 9</italic></xref>) demonstrated that the proposed DB-REDCNN yielded statistically significant improvements (P&lt;0.01) over other models, indicating superior performance in enhancing tumor edge sharpness.</p><table-wrap position=\"float\" id=\"t8\" orientation=\"portrait\"><label>Table 8</label><caption><title>The |<italic toggle=\"yes\">K</italic>| values of five distinct tumor ROIs</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"12.93%\" span=\"1\"/><col width=\"17.41%\" span=\"1\"/><col width=\"17.42%\" span=\"1\"/><col width=\"17.41%\" span=\"1\"/><col width=\"17.42%\" span=\"1\"/><col width=\"17.41%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Tumor</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Full dose</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Proposed</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">REDCNN</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">SubtlePET</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">UNET_RA</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">1</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.2367</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.1468</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.9822</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.8796</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.9024</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">2</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.5289</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.3452</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.0231</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.9874</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">6.0073</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">3</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.8105</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.7865</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.5379</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.5078</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">4.5190</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">4</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.9162</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.7864</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.5840</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.5342</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">5.5267</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">5</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.8421</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.7523</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.6219</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.5924</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">3.6134</td></tr></tbody></table><table-wrap-foot><p>REDCNN, residual encoder-decoder convolutional neural network; ROI, region of interest.</p></table-wrap-foot></table-wrap><table-wrap position=\"float\" id=\"t9\" orientation=\"portrait\"><label>Table 9</label><caption><title>Statistical significance (P values) of differences between each model and proposed</title></caption><table frame=\"hsides\" rules=\"groups\"><col width=\"33.94%\" span=\"1\"/><col width=\"66.06%\" span=\"1\"/><thead><tr><th valign=\"middle\" align=\"left\" scope=\"col\" rowspan=\"1\" colspan=\"1\">Method</th><th valign=\"middle\" align=\"center\" scope=\"col\" rowspan=\"1\" colspan=\"1\">P value (<italic toggle=\"yes\">vs.</italic> proposed)</th></tr></thead><tbody><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">REDCNN</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.003103**</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">SubtlePET</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.001139**</td></tr><tr><td valign=\"top\" align=\"left\" scope=\"row\" rowspan=\"1\" colspan=\"1\">UNET_RA</td><td valign=\"top\" align=\"center\" rowspan=\"1\" colspan=\"1\">0.001459**</td></tr></tbody></table><table-wrap-foot><p>**, P&lt;0.01. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network.</p></table-wrap-foot></table-wrap></sec></sec><sec sec-type=\"discussion\"><title>Discussion</title><p>In this study, we developed a novel DB-REDCNN model, an extension of the REDCNN architecture. Through its multibranch network design, the model integrates CT images as structural priors to guide the synthesis of FDPET from LDPET inputs, thereby producing outputs that more closely resemble FDPET in quality.</p><p>Regarding the choice of loss function, experimental results indicated that L1 loss consistently outperformed the other functions across three evaluation metrics, whereas SSIM loss yielded the poorest performance. In the comparative experiments, DB-REDCNN achieved competitive results on standard quantitative metrics, surpassing other models in terms of numerical performance.</p><p>To further assess the model&#8217;s ability to preserve edge sharpness, we used <inline-formula><mml:math id=\"m58\" display=\"inline\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to evaluate tumor boundary delineation within the ROIs. A series of experimental results demonstrated that our model is more effective in enhancing the representation of tumor edge contours.</p><p>In addition, we assessed the SUV, a clinically meaningful semantic-level metric, to further enhance the clinical interpretability of the proposed method. The results confirmed that DB-REDCNN not only effectively suppresses noise and restores image details but also more accurately preserves peak radiotracer uptake information within high-activity regions.</p><sec><title>Limitation</title><p>Despite the promising results achieved in our study, several noteworthy limitations should be noted. First, the experiments did not account for the potential influence of varying CT image quality on the model&#8217;s output performance. Second, to enhance the generalizability of the proposed method, future research could incorporate list-mode data reconstructed under different conditions&#8212;such as varying numbers of iterations, matrix sizes, and PSF configurations&#8212;or adopt domain adaptation techniques (<xref rid=\"r40\" ref-type=\"bibr\">40</xref>) and invariance learning strategies (<xref rid=\"r41\" ref-type=\"bibr\">41</xref>). Third, the reliance on PET/CT hardware and manual registration by clinicians cannot fully eliminate the inherent misalignment between PET and CT images. Thus, integrating more advanced registration algorithms during preprocessing, along with introducing quantitative metrics to assess registration errors, is required. Fourth, CT-guided imaging is inherently unsuited to the restoration small lesions, particularly in regions with low radiotracer uptake. The integration of functional imaging modalities or multimodal fusion techniques may help improve the reconstruction of such lesions. Finally, although the proposed model performed well across the dataset, variations in uptake time postinjection (ranging from 40 to 90 minutes) may introduce inconsistencies in SUV distribution, potentially affecting the reliability of training and evaluation. Future work should consider enforcing stricter control of uptake times or systematically assessing the impact of uptake time variation on model performance.</p></sec></sec><sec sec-type=\"conclusions\"><title>Conclusions</title><p>We developed a novel DB-REDCNN model that leverages a multibranch architecture to incorporate structural priors from CT images, thereby enhancing the synthesis of FDPET from LDPET. The model provides superior performance across quantitative evaluation metrics and significantly improves edge sharpness in lesion regions. It effectively preserves peak SUV information and offers clear advantages in both visual quality and clinical applicability.</p></sec><sec sec-type=\"supplementary-material\"><title>Supplementary</title><supplementary-material position=\"float\" content-type=\"local-data\" orientation=\"portrait\"><p>The article&#8217;s supplementary files as</p></supplementary-material><supplementary-material position=\"anchor\" id=\"su1\" content-type=\"local-data\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qims-15-12-11778-coif.pdf\" id=\"d67e2418\" position=\"anchor\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.21037/qims-2025-1321</object-id></media></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>None.</p></ack><fn-group><fn fn-type=\"financial-disclosure\"><p><italic toggle=\"yes\">Funding:</italic> This work was supported by <funding-source rid=\"sp1\">the Henan Provincial Science and Technology Research Project</funding-source> (<award-id rid=\"sp1\">No. 252102310453</award-id>).</p></fn><fn fn-type=\"COI-statement\"><p><italic toggle=\"yes\">Conflicts of Interest:</italic> All authors have completed the ICMJE uniform disclosure form (available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/coif\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/coif</ext-link>). The authors have no conflicts of interest to declare.</p></fn></fn-group><sec sec-type=\"data-availability\"><title>Data Sharing Statement</title><p>Available at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/dss\" ext-link-type=\"uri\">https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/dss</ext-link>\n<fig id=\"d67e2451\" position=\"anchor\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"qims-15-12-11778-dss.pdf\" position=\"float\" orientation=\"portrait\"><object-id pub-id-type=\"doi\">10.21037/qims-2025-1321</object-id></media></fig>\n</p></sec><notes><p content-type=\"note added in proof\"><italic toggle=\"yes\">Ethical Statement:</italic> The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. The study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. The study protocol was approved by the Ethics Committee of The Fifth Affiliated Hospital of Zhengzhou University (approval No. KY2025044). Patient consent was waived due to the retrospective nature of the data analysis and the use of anonymized data.</p></notes><ref-list><title>References</title><ref id=\"r1\"><label>1</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Brenner</surname><given-names>DJ</given-names></name><name name-style=\"western\"><surname>Hall</surname><given-names>EJ</given-names></name></person-group>. <article-title>Computed tomography--an increasing source of radiation exposure.</article-title><source>N Engl J Med</source><year>2007</year>;<volume>357</volume>:<fpage>2277</fpage>-<lpage>84</lpage>. <pub-id pub-id-type=\"doi\">10.1056/NEJMra072149</pub-id><pub-id pub-id-type=\"pmid\">18046031</pub-id></mixed-citation></ref><ref id=\"r2\"><label>2</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hosono</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Takenaka</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Monzen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Tamura</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Kudo</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Nishimura</surname><given-names>Y</given-names></name></person-group>. <article-title>Cumulative radiation doses from recurrent PET-CT examinations.</article-title><source>Br J Radiol</source><year>2021</year>;<volume>94</volume>:<elocation-id>20210388</elocation-id>. <pub-id pub-id-type=\"doi\">10.1259/bjr.20210388</pub-id><pub-id pub-id-type=\"pmid\">34111964</pub-id><pub-id pub-id-type=\"pmcid\">PMC9328066</pub-id></mixed-citation></ref><ref id=\"r3\"><label>3</label><mixed-citation publication-type=\"confproc\">Wang C, Hu Z, Shi P, Liu H. Low dose PET reconstruction with total variation regularization. 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society; Chicago, IL, USA. 2014:1917-20.<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/EMBC.2014.6943986</pub-id><pub-id pub-id-type=\"pmid\">25570354</pub-id></mixed-citation></ref><ref id=\"r4\"><label>4</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>An</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Lalush</surname><given-names>DS</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Pu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>D</given-names></name></person-group>. <article-title>Semisupervised Tripled Dictionary Learning for Standard-Dose PET Image Prediction Using Low-Dose PET and Multimodal MRI.</article-title><source>IEEE Trans Biomed Eng</source><year>2017</year>;<volume>64</volume>:<fpage>569</fpage>-<lpage>79</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TBME.2016.2564440</pub-id><pub-id pub-id-type=\"pmid\">27187939</pub-id><pub-id pub-id-type=\"pmcid\">PMC5383421</pub-id></mixed-citation></ref><ref id=\"r5\"><label>5</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xie</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Asma</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Qi</surname><given-names>J.</given-names></name></person-group><article-title>Anatomically aided PET image reconstruction using deep neural networks.</article-title><source>Med Phys</source><year>2021</year>;<volume>48</volume>:<fpage>5244</fpage>-<lpage>58</lpage>. <pub-id pub-id-type=\"doi\">10.1002/mp.15051</pub-id><pub-id pub-id-type=\"pmid\">34129690</pub-id><pub-id pub-id-type=\"pmcid\">PMC8510002</pub-id></mixed-citation></ref><ref id=\"r6\"><label>6</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Kalra</surname><given-names>MK</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>F</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Low-Dose</surname><given-names>CT</given-names></name></person-group><article-title>With a Residual Encoder-Decoder Convolutional Neural Network.</article-title><source>IEEE Trans Med Imaging</source><year>2017</year>;<volume>36</volume>:<fpage>2524</fpage>-<lpage>35</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2017.2715284</pub-id><pub-id pub-id-type=\"pmid\">28622671</pub-id><pub-id pub-id-type=\"pmcid\">PMC5727581</pub-id></mixed-citation></ref><ref id=\"r7\"><label>7</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Zu</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Lalush</surname><given-names>DS</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>L</given-names></name></person-group>. <article-title>3D conditional generative adversarial networks for high-quality PET image estimation at low dose.</article-title><source>Neuroimage</source><year>2018</year>;<volume>174</volume>:<fpage>550</fpage>-<lpage>62</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.neuroimage.2018.03.045</pub-id><pub-id pub-id-type=\"pmid\">29571715</pub-id><pub-id pub-id-type=\"pmcid\">PMC6410574</pub-id></mixed-citation></ref><ref id=\"r8\"><label>8</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Schaefferkoetter</surname><given-names>JD</given-names></name><name name-style=\"western\"><surname>Tham</surname><given-names>IWK</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>J</given-names></name></person-group>. <article-title>Supervised learning with cyclegan for low-dose FDG PET image denoising.</article-title><source>Med Image Anal</source><year>2020</year>;<volume>65</volume>:<elocation-id>101770</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.media.2020.101770</pub-id><pub-id pub-id-type=\"pmid\">32674043</pub-id></mixed-citation></ref><ref id=\"r9\"><label>9</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Weyts</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Lasnon</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Ciappuccini</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Lequesne</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Corroyer-Dulmont</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Quak</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Clarisse</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Roussel</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Bardet</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Jaudet</surname><given-names>C.</given-names></name></person-group><article-title>Artificial intelligence-based PET denoising could allow a two-fold reduction in [18F]FDG PET acquisition time in digital PET/CT.</article-title><source>Eur J Nucl Med Mol Imaging</source><year>2022</year>;<volume>49</volume>:<fpage>3750</fpage>-<lpage>60</lpage>.<pub-id pub-id-type=\"pmid\">35593925</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1007/s00259-022-05800-1</pub-id><pub-id pub-id-type=\"pmcid\">PMC9399218</pub-id></mixed-citation></ref><ref id=\"r10\"><label>10</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Oktay</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Ferrante</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Kamnitsas</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Heinrich</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Bai</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Caballero</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Cook</surname><given-names>SA</given-names></name><name name-style=\"western\"><surname>de Marvao</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Dawes</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>O'Regan</surname><given-names>DP</given-names></name><name name-style=\"western\"><surname>Kainz</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Glocker</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Rueckert</surname><given-names>D</given-names></name></person-group>. <article-title>Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation.</article-title><source>IEEE Trans Med Imaging</source><year>2018</year>;<volume>37</volume>:<fpage>384</fpage>-<lpage>95</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2017.2743464</pub-id><pub-id pub-id-type=\"pmid\">28961105</pub-id></mixed-citation></ref><ref id=\"r11\"><label>11</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hu</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Lei</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Feng</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Shen</surname><given-names>Y.</given-names></name></person-group><article-title>Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis.</article-title><source>IEEE Trans Med Imaging</source><year>2022</year>;<volume>41</volume>:<fpage>145</fpage>-<lpage>57</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2021.3107013</pub-id><pub-id pub-id-type=\"pmid\">34428138</pub-id></mixed-citation></ref><ref id=\"r12\"><label>12</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhuo</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>C.</given-names></name></person-group><article-title>Multimodal super-resolved q-space deep learning.</article-title><source>Med Image Anal</source><year>2021</year>;<volume>71</volume>:<elocation-id>102085</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.media.2021.102085</pub-id><pub-id pub-id-type=\"pmid\">33971575</pub-id></mixed-citation></ref><ref id=\"r13\"><label>13</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Amirrashedi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Sarkar</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Mamizadeh</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Ghadiri</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Ghafarian</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Zaidi</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Ay</surname><given-names>MR</given-names></name></person-group>. <article-title>Leveraging deep neural networks to improve numerical and perceptual image quality in low-dose preclinical PET imaging.</article-title><source>Comput Med Imaging Graph</source><year>2021</year>;<volume>94</volume>:<elocation-id>102010</elocation-id>. <pub-id pub-id-type=\"doi\">10.1016/j.compmedimag.2021.102010</pub-id><pub-id pub-id-type=\"pmid\">34784505</pub-id></mixed-citation></ref><ref id=\"r14\"><label>14</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yang</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Mou</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Kalra</surname><given-names>MK</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Low-Dose</surname><given-names>CT</given-names></name></person-group><article-title>Image Denoising Using a Generative Adversarial Network With Wasserstein Distance and Perceptual Loss.</article-title><source>IEEE Trans Med Imaging</source><year>2018</year>;<volume>37</volume>:<fpage>1348</fpage>-<lpage>57</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TMI.2018.2827462</pub-id><pub-id pub-id-type=\"pmid\">29870364</pub-id><pub-id pub-id-type=\"pmcid\">PMC6021013</pub-id></mixed-citation></ref><ref id=\"r15\"><label>15</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kleesiek</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Urban</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Hubert</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Schwarz</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Maier-Hein</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Bendszus</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Biller</surname><given-names>A.</given-names></name></person-group><article-title>Deep MRI brain extraction: A 3D convolutional neural network for skull stripping.</article-title><source>Neuroimage</source><year>2016</year>;<volume>129</volume>:<fpage>460</fpage>-<lpage>9</lpage>. <pub-id pub-id-type=\"doi\">10.1016/j.neuroimage.2016.01.024</pub-id><pub-id pub-id-type=\"pmid\">26808333</pub-id></mixed-citation></ref><ref id=\"r16\"><label>16</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pinton</surname><given-names>NJ</given-names></name><name name-style=\"western\"><surname>Bousse</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Cheze-Le-Rest</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Visvikis</surname><given-names>D</given-names></name></person-group>. <article-title>Multi-Branch Generative Models for Multichannel Imaging With an Application to PET/CT Synergistic Reconstruction.</article-title><source>IEEE Transactions on Radiation and Plasma Medical Sciences</source><year>2025</year>;<volume>9</volume>:<fpage>654</fpage>-<lpage>6</lpage>.</mixed-citation></ref><ref id=\"r17\"><label>17</label><mixed-citation publication-type=\"confproc\">Chen X, Zhou B, Xie H, Guo X, Zhang J, Sinusas AJ, Onofrey JA, Liu C. Dual-branch squeeze-fusion-excitation module for cross-modality registration of cardiac SPECT and CT. In: Wang L, Dou Q, Fletcher PT, Speidel S, Li S, editors. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland; 2022:46-55.</mixed-citation></ref><ref id=\"r18\"><label>18</label><mixed-citation publication-type=\"preprint\">Pinton NJ, Bousse A, Wang Z, Cheze-Le-Rest C, Maxim V, Comtat C, Sureau F, Visvikis D. Synergistic PET/CT reconstruction using a joint generative model. arXiv:2411.07339 [preprint]. 2024. Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/2411.07339\" ext-link-type=\"uri\">https://arxiv.org/abs/2411.07339</ext-link></mixed-citation></ref><ref id=\"r19\"><label>19</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ding</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>D</given-names></name><name name-style=\"western\"><surname>Nie</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Hou</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><article-title>Brain Medical Image Fusion Based on Dual-Branch CNNs in NSST Domain.</article-title><source>Biomed Res Int</source><year>2020</year>;<volume>2020</volume>:<elocation-id>6265708</elocation-id>. <pub-id pub-id-type=\"doi\">10.1155/2020/6265708</pub-id><pub-id pub-id-type=\"pmid\">32352003</pub-id><pub-id pub-id-type=\"pmcid\">PMC7178475</pub-id></mixed-citation></ref><ref id=\"r20\"><label>20</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cao</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Hung</surname><given-names>CC</given-names></name><name name-style=\"western\"><surname>Ma</surname><given-names>G</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Jin</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>J</given-names></name></person-group>. <article-title>Dual-branch residual network for lung nodule segmentation.</article-title><source>Applied Soft Computing</source><year>2020</year>;<volume>86</volume>:<elocation-id>105934</elocation-id>.</mixed-citation></ref><ref id=\"r21\"><label>21</label><mixed-citation publication-type=\"webpage\">Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Available online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf\" ext-link-type=\"uri\">https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf</ext-link></mixed-citation></ref><ref id=\"r22\"><label>22</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>W</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>G.</given-names></name></person-group><article-title>Low-dose CT via convolutional neural network.</article-title><source>Biomed Opt Express</source><year>2017</year>;<volume>8</volume>:<fpage>679</fpage>-<lpage>94</lpage>. <pub-id pub-id-type=\"doi\">10.1364/BOE.8.000679</pub-id><pub-id pub-id-type=\"pmid\">28270976</pub-id><pub-id pub-id-type=\"pmcid\">PMC5330597</pub-id></mixed-citation></ref><ref id=\"r23\"><label>23</label><mixed-citation publication-type=\"book\">Mao X, Shen C, Yang YB. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Advances in neural information processing systems 2016. Avaliable online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/1603.09056\" ext-link-type=\"uri\">https://arxiv.org/abs/1603.09056</ext-link></mixed-citation></ref><ref id=\"r24\"><label>24</label><mixed-citation publication-type=\"confproc\">He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-8.</mixed-citation></ref><ref id=\"r25\"><label>25</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hodson</surname><given-names>TO</given-names></name></person-group>. <article-title>Root mean square error (RMSE) or mean absolute error (MAE): When to use them or not.</article-title><source>Geoscientific Model Development Discussions</source><year>2022</year>;<volume>15</volume>:<fpage>5481</fpage>-<lpage>7</lpage>.</mixed-citation></ref><ref id=\"r26\"><label>26</label><mixed-citation publication-type=\"confproc\">Hore A, Ziou D. Image quality metrics: PSNR vs. SSIM. 2010 20th international conference on pattern recognition. IEEE, 2010:2366-9.</mixed-citation></ref><ref id=\"r27\"><label>27</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Bovik</surname><given-names>AC</given-names></name><name name-style=\"western\"><surname>Sheikh</surname><given-names>HR</given-names></name><name name-style=\"western\"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group>. <article-title>Image quality assessment: from error visibility to structural similarity.</article-title><source>IEEE Trans Image Process</source><year>2004</year>;<volume>13</volume>:<fpage>600</fpage>-<lpage>12</lpage>. <pub-id pub-id-type=\"doi\">10.1109/tip.2003.819861</pub-id><pub-id pub-id-type=\"pmid\">15376593</pub-id></mixed-citation></ref><ref id=\"r28\"><label>28</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ahmad</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Simonetti</surname><given-names>OP</given-names></name></person-group>. <article-title>Edge Sharpness Assessment by Parametric Modeling: Application to Magnetic Resonance Imaging.</article-title><source>Concepts Magn Reson Part A Bridg Educ Res</source><year>2015</year>;<volume>44</volume>:<fpage>138</fpage>-<lpage>49</lpage>. <pub-id pub-id-type=\"doi\">10.1002/cmr.a.21339</pub-id><pub-id pub-id-type=\"pmid\">26755895</pub-id><pub-id pub-id-type=\"pmcid\">PMC4706083</pub-id></mixed-citation></ref><ref id=\"r29\"><label>29</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yao</surname><given-names>R</given-names></name><name name-style=\"western\"><surname>Seidel</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Johnson</surname><given-names>CA</given-names></name><name name-style=\"western\"><surname>Daube-Witherspoon</surname><given-names>ME</given-names></name><name name-style=\"western\"><surname>Green</surname><given-names>MV</given-names></name><name name-style=\"western\"><surname>Carson</surname><given-names>RE</given-names></name></person-group>. <article-title>Performance characteristics of the 3-D OSEM algorithm in the reconstruction of small animal PET images.</article-title><source>IEEE transactions on medical imaging</source><year>2000</year>;<volume>19</volume>:<fpage>798</fpage>-<lpage>804</lpage>. <pub-id pub-id-type=\"doi\">10.1109/42.876305</pub-id><pub-id pub-id-type=\"pmid\">11055803</pub-id></mixed-citation></ref><ref id=\"r30\"><label>30</label><mixed-citation publication-type=\"book\">Joshi N, Szeliski R, Kriegman DJ. PSF estimation using sharp edge prediction. 2008 IEEE conference on computer vision and pattern recognition. IEEE; 2008:1-8.</mixed-citation></ref><ref id=\"r31\"><label>31</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kinahan</surname><given-names>PE</given-names></name><name name-style=\"western\"><surname>Townsend</surname><given-names>DW</given-names></name><name name-style=\"western\"><surname>Beyer</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Sashin</surname><given-names>D</given-names></name></person-group>. <article-title>Attenuation correction for a combined 3D PET/CT scanner.</article-title><source>Med Phys</source><year>1998</year>;<volume>25</volume>:<fpage>2046</fpage>-<lpage>53</lpage>. <pub-id pub-id-type=\"doi\">10.1118/1.598392</pub-id><pub-id pub-id-type=\"pmid\">9800714</pub-id></mixed-citation></ref><ref id=\"r32\"><label>32</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dong</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Loy</surname><given-names>CC</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>X</given-names></name></person-group>. <article-title>Image Super-Resolution Using Deep Convolutional Networks.</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2016</year>;<volume>38</volume>:<fpage>295</fpage>-<lpage>307</lpage>. <pub-id pub-id-type=\"doi\">10.1109/TPAMI.2015.2439281</pub-id><pub-id pub-id-type=\"pmid\">26761735</pub-id></mixed-citation></ref><ref id=\"r33\"><label>33</label><mixed-citation publication-type=\"webpage\">Xie J, Xu L, Chen E. Image denoising and inpainting with deep neural networks. Advances in neural information processing systems, 2012. Avaliable online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://dl.acm.org/doi/10.5555/2999134.2999173\" ext-link-type=\"uri\">https://dl.acm.org/doi/10.5555/2999134.2999173</ext-link></mixed-citation></ref><ref id=\"r34\"><label>34</label><mixed-citation publication-type=\"webpage\">Jain V, Seung S. Natural image denoising with convolutional networks. Advances in neural information processing systems, 2008. Avaliable online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://dl.acm.org/doi/10.5555/2981780.2981876\" ext-link-type=\"uri\">https://dl.acm.org/doi/10.5555/2981780.2981876</ext-link></mixed-citation></ref><ref id=\"r35\"><label>35</label><mixed-citation publication-type=\"confproc\">He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision. 2015:1026-34.</mixed-citation></ref><ref id=\"r36\"><label>36</label><mixed-citation publication-type=\"preprint\">Diederik K. Adam: A method for stochastic optimization. 2014. Avaliable online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://arxiv.org/abs/1412.6980\" ext-link-type=\"uri\">https://arxiv.org/abs/1412.6980</ext-link></mixed-citation></ref><ref id=\"r37\"><label>37</label><mixed-citation publication-type=\"webpage\">Paszke A, Gross S, Chintala S, et al. Automatic differentiation in pytorch. 2017. Avaliable online: <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"https://openreview.net/forum?id=BJJsrmfCZ\" ext-link-type=\"uri\">https://openreview.net/forum?id=BJJsrmfCZ</ext-link></mixed-citation></ref><ref id=\"r38\"><label>38</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>KT</given-names></name><name name-style=\"western\"><surname>Gong</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>de Carvalho Macruz</surname><given-names>FB</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Boumis</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Khalighi</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Poston</surname><given-names>KL</given-names></name><name name-style=\"western\"><surname>Sha</surname><given-names>SJ</given-names></name><name name-style=\"western\"><surname>Greicius</surname><given-names>MD</given-names></name><name name-style=\"western\"><surname>Mormino</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Pauly</surname><given-names>JM</given-names></name><name name-style=\"western\"><surname>Srinivas</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Zaharchuk</surname><given-names>G</given-names></name></person-group>. <article-title>Ultra-Low-Dose (18)F-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs.</article-title><source>Radiology</source><year>2019</year>;<volume>290</volume>:<fpage>649</fpage>-<lpage>56</lpage>. <pub-id pub-id-type=\"doi\">10.1148/radiol.2018180940</pub-id><pub-id pub-id-type=\"pmid\">30526350</pub-id><pub-id pub-id-type=\"pmcid\">PMC6394782</pub-id></mixed-citation></ref><ref id=\"r39\"><label>39</label><mixed-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Berghmans</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Dusart</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Paesmans</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Hossein-Foucher</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Buvat</surname><given-names>I</given-names></name><name name-style=\"western\"><surname>Castaigne</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Scherpereel</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mascaux</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Moreau</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Roelandts</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Alard</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Meert</surname><given-names>AP</given-names></name><name name-style=\"western\"><surname>Patz</surname><given-names>EF</given-names><suffix>Jr</suffix></name><name name-style=\"western\"><surname>Lafitte</surname><given-names>JJ</given-names></name><name name-style=\"western\"><surname>Sculier</surname><given-names>JP</given-names></name><collab>European Lung Cancer Working Party for the IASLC Lung Cancer Staging Project</collab></person-group>. <article-title>Primary tumor standardized uptake value (SUVmax) measured on fluorodeoxyglucose positron emission tomography (FDG-PET) is of prognostic value for survival in non-small cell lung cancer (NSCLC): a systematic review and meta-analysis (MA) by the European Lung Cancer Working Party for the IASLC Lung Cancer Staging Project.</article-title><source>J Thorac Oncol</source><year>2008</year>;<volume>3</volume>:<fpage>6</fpage>-<lpage>12</lpage>. <pub-id pub-id-type=\"doi\">10.1097/JTO.0b013e31815e6d6b</pub-id><pub-id pub-id-type=\"pmid\">18166834</pub-id></mixed-citation></ref><ref id=\"r40\"><label>40</label><mixed-citation publication-type=\"confproc\">Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation. Proceedings of the 32nd International Conference on Machine Learning, PMLR 2015;37:1180-9.</mixed-citation></ref><ref id=\"r41\"><label>41</label><mixed-citation publication-type=\"confproc\">Kwon J, Kim J, Park H, Choi IK. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. Proceedings of the 38th International Conference on Machine Learning, PMLR 2021;139:5905-14.</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Quant Imaging Med Surg Quant Imaging Med Surg 1905 qims QIMS Quantitative Imaging in Medicine and Surgery 2223-4292 2223-4306 AME Publications PMC12682464 PMC12682464.1 12682464 12682464 10.21037/qims-2025-1321 qims-15-12-11778 1 Original Article Dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN): a computed tomography-integrated multimodal network for positron emission tomography denoising Liu Yang 1 Zou Guanglu https://orcid.org/0009-0006-6447-7109 1 Li Tao 1 Liu Haojia 2 1 School of Electronics and Information , Zhengzhou University of Light Industry , Zhengzhou , China ; 2 Department of Medical Imaging , The Fifth Affiliated Hospital of Zhengzhou University , Zhengzhou , China Contributions: (I) Conception and design: Y Liu, G Zou; (II) Administrative support: T Li, Y Liu; (III) Provision of study materials or patients: H Liu; (IV) Collection and assembly of data: G Zou; (V) Data analysis and interpretation: G Zou; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors. Correspondence to: Tao Li, PhD. School of Electronics and Information, Zhengzhou University of Light Industry, No. 136 Kexue Avenue, Zhengzhou 450000, China. Email: susiecash1220@gmail.com ; Haojia Liu, M.Eng. Department of Medical Imaging, The Fifth Affiliated Hospital of Zhengzhou University, No.3 Kangfuqian Street, Zhengzhou 450052, China. Email: 2798617484@qq.com . 21 11 2025 01 12 2025 15 12 502028 11778 11794 09 6 2025 18 9 2025 01 12 2025 09 12 2025 09 12 2025 &#169; 2025 AME Publishing Company. 2025 AME Publishing Company. https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access Statement: This is an Open Access article distributed in accordance with the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International License (CC BY-NC-ND 4.0), which permits the non-commercial replication and distribution of the article with the strict proviso that no changes or edits are made and the original work is properly cited (including links to both the formal publication through the relevant DOI and the license). See: https://creativecommons.org/licenses/by-nc-nd/4.0 . Background Positron emission tomography (PET) requires injection of radioactive tracers, which entails concerns regarding radiation exposure. Thus, there is a need for low-dose PET (LDPET) imaging that maintains full-dose PET (FDPET) quality while reducing tracer dosage. However, dose reduction often degrades image quality. Although deep learning methods have shown promise, denoising small lesions remains challenging. To address this, we propose a dual-branch network that incorporates structural information from computed tomography (CT) to enhance LDPET quality, preserve fine details, and improve small-lesion imaging. Methods We propose the dual-branch residual encoder-decoder convolutional neural network (DB-REDCNN), a dual-branch multimodal network that leverages structural priors from paired CT images to enhance edge detail in PET reconstructions. Specifically, the network architecture consists of two parallel branches that independently extract modality-specific features from PET and CT inputs, followed by an effective fusion mechanism for reconstructing high-quality PET images. The overall architecture adopts a residual encoder-decoder design, with dedicated encoder and decoder modules at the input and output stages, respectively. It is important to note that although minor misalignments between CT and PET images may occur due to respiratory motion or patient movement, our dual-branch framework is inherently robust to such inconsistencies. For data acquisition, LDPET images were generated with an acquisition time of 30 seconds per bed position, while FDPET images were acquired over 150 seconds. For performance evaluation, we conducted extensive comparisons with several state-of-the-art deep learning methods. Moreover, the standardized uptake value (SUV) was included as an additional clinical indicator. Results The proposed DB-REDCNN demonstrated superior performance across multiple evaluation metrics. In quantitative analysis, it achieved the largest improvements compared with LDPET images in root mean square error (0.0023&#177;0.0075), peak signal-to-noise ratio (PSNR) (1.2314&#177;4.8354), and the structural similarity index measure (0.0078&#177;0.0153), with a significantly higher PSNR than REDCNN and consistent advantages across all three metrics (P&lt;0.05). Edge sharpness assessments further confirmed the superiority of DB-REDCNN, which yielded the highest |K| values across five tumor slices with statistically significant improvements over competing models (P&lt;0.01). In SUV evaluation, although the SUV_mean error of the DB-REDCNN was slightly higher than that of LDPET, the proposed method markedly reduced SUV_max error and outperformed all other approaches. Conclusions The proposed DB-REDCNN model, by integrating structural priors from CT through a multibranch architecture, enhances the synthesis of FDPET from LDPET. It achieves superior quantitative performance, improves edge sharpness in lesion regions, and preserves peak SUV information, thereby offering a clinically valuable approach for radiation dose reduction in PET imaging. Keywords: Positron emission tomography (PET) computed tomography (CT) low-dose multibranch deep learning the Henan Provincial Science and Technology Research Project No. 252102310453 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes Introduction The high sensitivity of positron emission tomography (PET) in detecting metabolic abnormalities and tumor activity has led to its widespread application in clinical oncology, neurology, and cardiovascular disease diagnosis. However, acquiring high-quality PET images typically requires the administration of relatively large doses of radioactive tracers [such as 18F-fluorodeoxyglucose ( 18 F-FDG)], raising concerns regarding the potential carcinogenic risks associated with radiation exposure ( 1 ). Although the risk from a single PET scan is relatively low, the increasing utilization of high-dose imaging modalities such as PET-computed tomography (PET/CT) has contributed to a cumulative radiation burden at the population level, which may constitute a significant public health concern ( 2 ). Therefore, the development of imaging technologies that can substantially reduce radiation exposure without compromising diagnostic image quality has become a critical focus in contemporary nuclear medicine research. To enhance image quality while reducing tracer dosage or acquisition time, previous studies have explored the simulation of full-dose PET (FDPET) images using low-dose PET (LDPET) scans. In the early stages, efforts primarily focused on non-deep learning approaches, such as total variation (TV) regularization ( 3 ), sparse representation techniques ( 4 ), and optimization models incorporating structural constraints ( 5 ). These methods typically rely on handcrafted priors&#8212;such as edge preservation and image smoothing&#8212;to guide the reconstruction process. Despite being theoretically beneficial, they often face limitations of high computational cost, poor generalizability, and difficulty in parameter tuning. Furthermore, their dependence on fixed priors frequently results in oversmoothing and the loss of fine anatomical details, potentially compromising diagnostic sensitivity. In recent years, the rapid advancement of deep learning has brought about significant breakthroughs in medical image processing, catalyzing the development of numerous neural network-based models for enhancing LDPET and propelling the field forward. A prominent example is the residual encoder-decoder convolutional neural network (REDCNN) model, which introduced the use of residual autoencoders for denoising low-dose CT (LDCT) images, effectively reducing noise while preserving critical structural information ( 6 ); this architectural approach has since been adapted for PET image enhancement. For PET denoising, a variety of network architectures have been proposed, including three-dimensional conditional generative adversarial networks (3D-cGANs) ( 7 ), CycleWGAN ( 8 ), and SubtlePET ( 9 ), all of which leverage adversarial training and perceptual loss to maintain lesion contrast and ensure consistency in standardized uptake value (SUV), achieving promising results. Notably, SubtlePET has demonstrated subjectively comparable image quality to FDPET and exhibited high lesion detection consistency in large-scale clinical validation, further underscoring the clinical utility of deep learning in PET imaging. Despite recent advancements, most existing approaches remain constrained to single-modality reconstruction, relying solely on information contained within PET images. This limitation hampers the recovery of structural details, particularly under ultralow-dose or short acquisition conditions. In contrast, CT images, which exhibit high resolution and clear anatomical structures, offer valuable complementary information. When effectively integrated into the PET reconstruction pipeline, they have the potential to improve edge fidelity and structural interpretability. However, the primary value of CT lies in providing structural priors and anatomical mapping. For small lesions with low contrast or those that are not visible on CT, its direct contribution is limited; nonetheless, the overall enhancement in PET image quality indirectly improves the detectability of such lesions. Consequently, multimodal reconstruction strategies have garnered increased attention. Some studies have incorporated CT images alongside LDPET through image-level registration to train neural networks ( 5 ), while others have employed structurally constrained frameworks, such as anatomy-guided networks ( 10 ), to enforce anatomical priors to promote structural consistency. Additionally, bidirectional mapping GAN ( 11 ) employs cross-modal latent space alignment to facilitate PET-magnetic resonance imaging (MRI) fusion, and multimodal super-resolved q-space deep learning (MSR-q-DL) ( 12 ) leverages attention mechanisms to guide intermodal information flow. Collectively, these approaches underscore the pivotal role of structural imaging in enhancing PET reconstruction quality under low-dose scenarios. The study of multimodal approaches extends beyond theoretical modeling, demonstrating strong transferability across various clinical application scenarios. In LDPET imaging, for instance, deep learning methods have been shown to outperform traditional filters such as Gaussian and nonlocal means in maintaining perceptual image quality ( 13 ). In LDCT imaging, the implementation of Wasserstein GANs (WGANs) has provided valuable technical insights for cross-modal enhancement, effectively balancing noise suppression with structural detail preservation ( 14 ). Moreover, recent progress in brain structure segmentation and image registration tasks has underscored the importance of incorporating anatomical priors from CT or MRI to improve the overall quality of medical image processing ( 15 ). The REDCNN network is recognized for its strong performance in terms of conventional evaluation metrics such as the peak signal-to-noise ratio (PSNR) and the structural similarity index measure (SSIM). However, it often produces overly smoothed outputs, which may obscure critical anatomical details in the reconstructed images. To address this limitation, this work introduces an enhanced architecture, termed dual-branch REDCNN (DB-REDCNN), which incorporates co-registered CT images as an auxiliary input branch to provide structural priors during training. The incorporation of CT images as complementary information substantially enhances the network&#8217;s ability to reconstruct high-quality PET images from LDPET inputs. This enhancement is primarily attributed to the high-resolution anatomical information provided by CT, which compensates for the blurred structures and low signal-to-noise ratio (SNR) typically observed in LDPET images due to insufficient tracer dosage ( 16 ). CT images are primarily used to depict normal anatomical structures. By incorporating CT data, the network can be expected to be better equipped to identify structural features such as tissue boundaries and organ contours, effectively guiding the generation of FDPET images with consistent morphology and coherent spatial distribution. This, in turn, enhances the clarity and credibility of the reconstructed outputs ( 17 ). Additionally, leveraging CT as a structural prior improves the model&#8217;s generalization ability, allowing it to produce more stable and reliable predictions, particularly in preserving high-frequency details and edge features ( 18 ). Furthermore, the complementary characteristics of CT and LDPET enable the network to learn more comprehensive feature representations, thereby further enhancing image quality and increasing the diagnostic value of the generated outputs in clinical applications ( 19 ). Recognizing the limited utility of CT in detecting lesions with low radiotracer uptake or small size, we designed our network architecture and training strategy to treat PET as the primary source of functional information, with CT being exclusively used for structural guidance. To further evaluate the model&#8217;s ability to recover small lesions, we specifically analyze changes in SUV within lesion regions in LDPET images, thereby assessing the model&#8217;s effectiveness in preserving the functional characteristics of these lesions. The proposed framework adopts a dual-branch architecture that independently extracts modality-specific features from PET and CT images, while a shared encoder-decoder network is employed to capture long-range dependencies. To further enhance structural representation, a structural similarity constraint has been introduced, reinforcing the boundary information guided by CT. This design enables the model to preserve functional characteristics while significantly improving anatomical detail depiction. Evaluation results demonstrate that DB-REDCNN outperforms existing methods in both conventional image quality metrics and SUV-based assessments. In light of recent advances in research and clinical needs, the proposed approach offers a practical and efficient solution for LDPET imaging, with strong scalability for extension to other multimodal applications such as brain and cardiac imaging. Methods Network architecture The proposed DB-REDCNN includes a dual-branch architecture consisting of a main branch and an auxiliary branch. Each branch comprises five convolutional blocks, with each block containing a convolutional layer, a batch normalization layer, and a rectified linear unit (ReLU) activation function. All convolutional layers consist of 96 two-dimensional kernels with a size of 5&#215;5 and a stride of 1. Following independent feature extraction by the two branches, the extracted features are concatenated and passed through five deconvolutional blocks, each replicating the structure of the convolutional blocks, including a convolutional layer, batch normalization, and ReLU activation ( 20 , 21 ). The full architecture of the network is illustrated in Figure 1 . Figure 1 Architecture of the proposed DB-REDCNN. CONV-BN, convolution batch normalization; CONVTrans, convolutional transpose; CT, computed tomography; DB-REDCNN, dual-branch residual encoder-decoder convolutional neural network; FDPET, full-dose positron emission tomography; PET, positron emission tomography; ReLU, rectified linear unit. Denoising model By constraining the image denoising problem to the image domain ( 22 ), the image denoising problem can be simplified in the context of deep learning as follows: let X &#8712; R m &#215; n denote the LDPET image, Y &#8712; R m &#215; n the FDPET image, and C the corresponding CT image. The relationship among them can be formulated as follows: X + &#948; ( C ) = &#963; ( Y ) [1] Here, &#948; ( C ) denotes a structural correction term extracted from the corresponding CT image C, while &#963; ( Y ) represents the degradation process mapping FDPET (denoted as Y) to LDPET (denoted as X). This mapping may be ambiguous due to the loss of structural information resulting from dose reduction. The auxiliary bias &#948; ( C ) , due to the anatomical priors derived from the CT image being embedded, helps the network preserve sharp tissue boundaries and the global anatomical structure during reconstruction. Although CT images cannot directly reveal small, low-contrast lesions, their primary value lies in providing anatomical guidance for structural recovery in PET imaging. Essentially, &#948; ( C ) functions as a learnable, spatially variant offset that modulates &#963; ( Y ) , thereby enabling a more accurate approximation of LDPET (denoted as X). The objective then becomes to find a function f such that: arg f min &#8214; f [ X + &#948; ( C ) ] &#8722; Y &#8214; 2 2 [2] Here, f is treated as the optimal approximation of &#963; . Residual encoding network A symmetric encoder-decoder architecture, composed of convolutional and deconvolutional layers ( 23 ), as illustrated in Figure 2 , was adopted in this study. To better preserve fine structural details in PET images and minimize the risk of irreversible degradation during deep feature extraction, a residual compensation mechanism is integrated into the PET branch. As opposed to the sole reliance on stacked layers for learning the mapping from input to output, a residual learning strategy ( 24 ) is employed to enhance feature representation and improve reconstruction quality. Figure 2 Shortcut in the residual compensation structure. CONVTrans, convolutional transpose. With the input e denoted as x , the residual mapping can be defined as follows: F ( x ) = H ( x ) &#8722; x [3] where H ( x ) represents the desired underlying mapping, and F ( x ) is the residual function. By reformulating the equation, the following is obtained: H ( x ) = F ( x ) + x [4] Loss function Since the primary objective of the loss function is to evaluate the discrepancy between the predicted and the ground truth values, L1 loss is used as the core optimization criterion. L1 loss directly penalizes the absolute difference between predictions and the corresponding ground truth, thereby encouraging the model to produce outputs that closely approximate the reference. Smaller differences yield lower loss values, while larger deviations incur proportionally greater penalties. Let y ^ l denote the predicted value and y i the corresponding ground truth. L1 loss is defined as follows: L L 1 = 1 n &#8721; i = 1 n | y i &#8722; y ^ i | [5] where n represents the total number of pixels in each image. Evaluation metrics To quantitatively evaluate the effectiveness of our method, we employed three widely used evaluation metrics: root mean square error (RMSE) ( 25 ), PSNR ( 26 ), and SSIM ( 27 ). Together, these metrics provide a comprehensive framework for assessing both pixel-level accuracy and perceptual similarity between the synthesized PET images and the corresponding full-dose references. These metrics are discussed in detail in the following sections. RMSE RMSE measures the average pixel-wise deviation between the reconstructed image and the ground truth. A lower RMSE value indicates a closer match to the reference image, reflecting higher reconstruction accuracy. R M S E = 1 M N &#8721; i = 1 N &#8721; j = 1 M [ u ( i , j ) &#8722; v ( i , j ) ] 2 [6] Here, u ( i , j ) and v ( i , j ) denote the pixel intensities of the reconstructed and reference images respectively, while M and N are the image dimensions. PSNR The PSNR assesses the quality of image reconstruction, with a particular focus on preserving pixel intensity fidelity. Higher PSNR values indicate that the reconstructed image is more similar to the reference image in terms of both brightness accuracy and overall visual quality. P S N R = 20 &#215; log 10 ( M A X R M S E ) [7] Here, M A X is the maximum possible pixel value of the image. Since the PSNR is inversely related to RMSE, improving pixel accuracy directly contributes to a higher PSNR. SSIM SSIM evaluates perceptual similarity by comparing local patterns of pixel intensities that have been normalized for luminance and contrast. It is particularly effective in assessing structural information, such as lesion boundaries and anatomical features, which are critical for clinical interpretation in PET imaging. S S I M ( x , y ) = ( 2 &#956; x &#956; y + c 1 ) ( 2 &#963; x y + c 2 ) ( &#956; x 2 + &#956; y 2 + c 1 ) ( &#963; x 2 + &#963; y 2 + c 2 ) [8] In this equation, &#956; x and &#956; y are the mean intensities of images x and y , respectively; &#963; x 2 and &#963; y 2 are the variances; &#963; x y is the covariance; and C 1 and C 2 are small constants used to stabilize the division. SUV evaluation metrics To assess quantitative accuracy, volumes of interest (VOIs) were manually delineated within lesion regions by an experienced nuclear medicine physician, using the corresponding CT images as anatomical references. The same VOIs were consistently applied across all PET images to ensure the spatial alignment and reproducibility of SUV measurements. Both S U V m e a n and S U V max were extracted for each VOI; subsequently, the relative errors, referred to as S U V m e a n e r r o r and S U V max e r r o r , were calculated with the following formulae: S U V m e a n e r r o r = | S U V m e a n p r e d &#8722; S U V m e a n t r u e | S U V m e a n t r u e [9] S U V max e r r o r = | S U V max p r e d &#8722; S U V max t r u e | S U V max t r u e [10] To comprehensively evaluate the ability of different models to maintain the quantitative accuracy of PET imaging, the errors of S U V m e a n and S U V max within the VOIs were compared across various methods. Edge sharpness index To analyze the model&#8217;s performance specifically at the lesion boundaries, an edge evaluation was incorporated. For this purpose, slices from the test set containing visibly identifiable small lesions were selected. A novel evaluation metric, the edge sharpness index ( | K | ) ( 28 ), was introduced to quantitatively assess tumor boundary delineation performance. The formula used to compute | K | is as follows: | K | = max ( &#8711; I ) &#963; &#8711; I [11] where &#8711; I is the image gradient map, max ( &#8711; I ) is the maximum gradient magnitude in the image, and &#963; &#8711; I is the standard deviation of the gradient values. A higher | K | value corresponds to sharper and cleaner image edges. Data acquisition This study employed whole-body PET images acquired with 18 F-FDG and corresponding CT images obtained with a Vereos PET/CT system (Philps, Amsterdam, the Netherlands). No restrictions were imposed regarding patient sex, age, or disease type; however, cases exhibiting artifacts, registration errors, or lacking apparent lesions were excluded. Registration errors were primarily identified through manual inspection by experienced radiologists, who assessed the spatial alignment of key anatomical structures&#8212;such as the lungs, liver, and heart&#8212;on both PET and CT images. Misalignment of organ boundaries or significant overlap discrepancies was considered indicative of registration errors. To minimize the misalignment potentially caused by respiratory motion, standard breath-hold instructions were provided during image acquisition, and rigid registration was applied during preprocessing to further enhance alignment accuracy. In total, data from 33 patients were included. Each patient received an intravenous injection of 18 F-FDG at an activity of 3.7 MBq/kg based on body weight, which was followed by a resting period of 40 to 90 minutes in a quiet waiting area prior to scanning. All scans were performed in the three-dimensional acquisition mode, with a duration of 2.5 minutes per bed position. Image reconstruction was conducted with the ordered subsets expectation maximization (OSEM) algorithm ( 29 ), with both time-of-flight (TOF) and point-spread-function (PSF) modeling techniques being applied ( 30 ). The reconstruction matrix was set to 288&#215;288, with two iterations and six subsets per iteration. Slice thickness and interslice spacing were both maintained at 2 mm. The number of PSF iterations and the regularization factor were set to 1 and 6, respectively. Attenuation correction for PET was performed with an LDCT scan (120 kV and 150 mAs) ( 31 ). To simulate LDPET images, a retrospective reconstruction approach was employed with the acquisition time being reduced to 30 seconds and the same reconstruction parameters as those used for FDPET images acquired over 150 seconds being applied. Attenuation, scatter, and random coincidence corrections, as well as the image reconstruction, were performed with vendor-provided software. In the dataset, each &#8220;pair&#8221; of images referred to a PET image slice and its corresponding CT image slice. To prevent potential bias caused by interslice correlation within the same patient, data partitioning was conducted on a per-patient basis. Specifically, 10,568 image pairs were allocated for training, 3,656 for validation, and 864 for testing, with no patient overlap across these subsets. This strategy ensured the independence and robustness of the evaluation results. The number of patients and images in the training, test, and validation sets is summarized in Table 1 . This study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments, and&#160;the study protocol was approved by the Ethics Committee of The Fifth Affiliated Hospital of Zhengzhou University (approval No. KY2025044). Patient consent was waived due to the retrospective nature of the data analysis and the use of anonymized data. Table 1 Number of patients and images in the training, test, and validation sets Items Training, n Test, n Validation, n Patients 23 2 8 Images 10,568 864 3,656 Data preprocessing and experimental settings Before training on the dataset comprising 33 cases, several preprocessing steps were performed. All PET images were strictly registered to their corresponding CT images during the data preprocessing stage via a voxel similarity-based registration algorithm. The alignment accuracy was further verified by experienced radiologists through visual inspection of key anatomical landmarks&#8212;such as the ventricles and liver boundaries&#8212;to ensure consistency across modalities. Subsequently, both CT and PET images were resampled to a resolution of 256&#215;256 via bilinear interpolation. As illustrated in Figure 3 , the overlay image was employed to demonstrate the registration performance of representative PET/CT pairs from the dataset. Figure 3 Visualization of the image generated by overlaying the PET image onto the CT image via pseudocolor representation. CT, computed tomography; PET, positron emission tomography. To reduce overfitting and improve the network&#8217;s generalization capability, data augmentation techniques were applied, including random horizontal and vertical flips as well as random rotations of up to 45 degrees. As a result, a total of 31,704 image pairs were generated. In addition, all images were normalized through the of scaling pixel intensities to the range [0, 1], ensuring a consistent numerical scale across inputs, accelerating network convergence, and mitigating issues such as gradient explosion or vanishing gradients caused by extreme intensity values. To further enhance generalization, increase robustness, and encourage the model to focus on local features, a patch-based training strategy was employed. This involved randomly cropping smaller patches from the input images in line with strategies adopted in previous studies ( 32 - 34 ). To initialize network parameters in a manner compatible with ReLU activation, the Kaiming initialization method ( 35 ) was employed. Model training was conducted with the Adam optimizer ( 36 ), which adaptively adjusts the learning rate of each parameter with decay rates set to &#946;1 =0.9 and &#946;2 =0.999. Additional training configurations, including the number of epochs, batch size, patch size, and learning rate, are summarized in Table 2 . Table 2 Hyperparameters during training Items Epochs, n Learning rate Batch size, n Patches, n Patch size Value 50 1e&#8722;4 10 10 64 The selection of hyperparameters was guided by empirical experience from prior experiments and informed by preliminary results on the validation set. Specifically, the number of training epochs was chosen to ensure model convergence while minimizing the risk of overfitting. Batch size and patch size were determined by balancing training stability with Graphics Processing Unit (GPU) memory limitations. The initial learning rate was selected to facilitate rapid convergence during the early stages of training. A range of hyperparameter combinations was tested&#8212;for example, batch sizes ranging from 4 to 16, patch sizes from 32 to 64, and learning rates from 1e&#8722;3 to 1e&#8722;5. The final configuration was selected based on optimal performance in terms of validation loss and PSNR. All experiments were conducted via Python 3.8 (Python Software Foundation, Wilmington, DE, USA) with the PyTorch framework ( 37 ) on a Dell Workstation (Dell Technologies, Round Rock, TX, USA) equipped with a 12 vCPU Intel(R) Xeon(R) Platinum 8352V CPU and a vGPU (32 GB) (Intel, Santa Clara, CA, USA). Models used in the ablation study To evaluate the effectiveness of the dual-branch architecture in the DB-REDCNN network, a series of ablation experiments was conducted. All experiments were performed on the same dataset and computing platform to ensure consistent and fair comparisons of both quantitative and qualitative performance. Specifically, as illustrated in Figure 4 , we compared three models: (I) the original REDCNN (REDCNN uni), which uses only PET images as input; (II) a modified version (REDCNN multi), in which PET and CT images are concatenated along the channel dimension; and (III) the proposed DB-REDCNN with a dual-branch architecture. Figure 4 The input image modalities of the three models. (A) REDCNN model with a single PET image used as input. (B) REDCNN multimodel, in which PET and CT images are concatenated along the vertical dimension as input. (C) Our proposed dual-branch network architecture. CT, computed tomography; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network. Results Comparison of loss functions We conducted comparative experiments using L1 loss and other classical loss functions, including SSIM loss and mean square error (MSE) loss. The results demonstrated that L1 loss yielded the best performance in our model. Based on the data in Table 3 , after simple numerical calculations, the changes in the evaluation metrics (RMSE, PSNR, SSIM) for the model using L1 loss relative to the 30s-PET images were 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153. These results were superior to those of the model using SSIM loss (&#8211;0.0072&#177;0.0333, &#8211;0.5547&#177;6.6596, and &#8211;0.0037&#177;0.0427) and the model using MSE loss (0.0014&#177;0.0079, 0.7131&#177;4.6028, and &#8211;0.0004&#177;0.0177). Here, the values in parentheses represent, respectively, the differences in the mean RMSE, PSNR, and SSIM between the PET images generated by models using different loss functions and the 30s-PET images. Table 3 Evaluation of the loss effect Loss RMSE PSNR SSIM 30-s PET 0.0195&#177;0.0083 35.1759&#177;4.4807 0.9626&#177;0.0180 L1 0.0172&#177;0.0075 36.4073&#177;4.8354 0.9704&#177;0.0153 SSIM 0.0267&#177;0.0333 34.6212&#177;6.6596 0.9589&#177;0.0427 MSE 0.0181&#177;0.0079 35.8890&#177;4.6028 0.9622&#177;0.0177 The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. MSE, mean square error; PET, positron emission tomography; PSNR, peak signal-to-noise ratio; RMSE, root mean square error; SSIM, structural similarity index measure. Quantitative evaluation The other models evaluated in this study included REDCNN, SubtlePET, and UNET_RA ( 38 ). REDCNN is a unimodal network that exclusively utilizes LDPET images as input, while SubtlePET and UNET_RA are multimodal architectures that integrate both LDPET and the corresponding CT images as inputs. Qualitative visual assessments of FDPET, LDPET, and the PET images reconstructed by the models are provided in the axial, coronal, and sagittal views in Figure 5 . Based on visual inspection, all models improved the quality of LDPET images, with our proposed method and REDCNN yielding relatively better perceptual results. However, visual inspection alone is insufficient for rigorously demonstrating the superiority of our approach; Therefore, the quantitative results are presented in Table 4 and Figure 6 to support these observations. Figure 5 Sample images of LDPET (30 s), FDPET, and estimated PET images from the proposed, REDCNN, SubtlePET, and UNET_RA methods. The arrows in the figure indicate the location of the small lesions. Rows 1, 2, and 3 are PET images in the axial, coronal, and sagittal directions, respectively. The grayscale bar on the top indicates the SUV (g/mL). FDPET, full-dose PET; LDPET, low-dose PET; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; SUV, standardized uptake value. Table 4 Evaluation of quantitative indicators Method RMSE PSNR SSIM 30-s PET 0.0195&#177;0.0083 35.1759&#177;4.4807 0.9626&#177;0.0180 REDCNN 0.0185&#177;0.0089 35.9540&#177;5.2702 0.9714&#177;0.0157 UNET_RA 0.0187&#177;0.0080 35.5428&#177;4.4837 0.9464&#177;0.0205 SubtlePET 0.0192&#177;0.0083 35.3041&#177;4.4202 0.9068&#177;0.0262 Proposed 0.0172&#177;0.0075 36.4073&#177;4.8354 0.9704&#177;0.0153 The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure. Figure 6 Box plots for the evaluation of quantitative indicators (data from Table 4 ). The improvement of model-estimated PET images in RMSE, PSNR, and SSIM with respect to the original LDPET images. LDPET, low-dose PET; PET, positron emission tomography; PSNR, peak signal-to-noise ratio; RMSE, root mean square error; SSIM, structural similarity index measure. Compared with those of LDPET images, the improvements of model-estimated PET images in terms of RMSE, PSNR, and SSIM were, respectively, 0.001&#177;0.0089, 0.7781&#177;5.2702, and 0.0088&#177;0.0157 for REDCNN; 0.0008&#177;0.0080, 0.3669&#177;4.4837, and &#8211;0.0162&#177;0.0205 for UNET_RA; 0.0003&#177;0.0083, 0.1282&#177;4.4202, and &#8211;0.0558&#177;0.0262 for SubtlePET; and 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153 for the proposed model. All estimated PET images outperformed the original low-dose data in terms of RMSE and PSNR. However, UNET_RA and SubtlePET showed lower SSIM values than did the original LDPET images. The proposed approach demonstrated consistently favorable results across all three metrics, achieving a notably higher PSNR than REDCNN, although its SSIM was slightly below that of REDCNN. To assess the statistical significance of the performance differences across the methods, significance tests were conducted for all methods. As shown in Table 5 , all models&#8212;except SubtlePET&#8212;showed statistically significant differences (P&lt;0.05) across all metrics when compared to the 30-s PET data. SubtlePET did not exhibit statistically significant differences in RMSE or PSNR values (P&gt;0.05). Notably, the proposed method demonstrated the most significant improvements in RMSE and PSNR values, further validating its superior performance. Table 5 Statistical significance (P values) of the differences between each model and 30-s PET Method RMSE PSNR SSIM REDCNN 5.42838e&#8722;09 3.70264e&#8722;15 3.61526e&#8722;72 UNET_RA 0.000195392 0.000146802 2.34925e&#8722;72 SubtlePET 0.294267589 0.210342034 1.61363e&#8722;72 Proposed 1.07e&#8722;35 4.19352e&#8722;56 2.34465e&#8722;72 PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure. Figure 7 displays the radioactivity concentration (AC; Bq/mL) distribution curves across the lesion region, with a zoomed-in view focusing on the peak area to more clearly illustrate the variations in AC within this region. Compared with the other models&#8212;REDCNN (green), SubtlePET (red), and UNET_RA (purple)&#8212;the model we developed (orange) aligned most closely with the full-dose curve (brown). Figure 7 AC contrast curve covering the lesion area. The inset in the upper left corner is a magnified image near the peak AC value. The image in the lower right corner shows the pixel region selected (covering the lesion area). For visualization purposes, AC values were proportionally scaled during plotting. AC, radioactivity concentration; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network. SUV evaluation As illustrated in Figure 8 , although the S U V m e a n e r r o r of the proposed approach was slightly higher than that of the original LDPET (30 s) image, it surpassed all other compared techniques&#8212;including LDPET, REDCNN, SubtlePET, and UNET_RA&#8212;in all remaining metrics, exhibiting a particularly pronounced advantage in reducing S U V max e r r o r ( 39 ). Figure 8 SUV mean error and SUV max error in lesion VOIs. SUV, standardized uptake value; VOI, volume of interest. Ablation study Figure 9 illustrates both the cross-sectional and whole-body PET images, highlighting the impact of different network branch configurations on the REDCNN model. In these experiments, REDCNN uni was considered to be a single-branch network that processes only PET images as input, whereas REDCNN multi was considered to be a network that takes PET and CT images concatenated along the channel dimension as input. Figure 9 Sample images of LDPET (30 s), FDPET, and estimated PET images from the proposed, REDCNN uni, and REDCNN multi methods. The arrows in the figure indicate the locations of the small lesions. Rows 1, 2, and 3 are PET images in the axial, coronal, and sagittal direction, respectively. The grayscale bar on the top indicates the SUV (g/mL). FDPET, full-dose positron emission tomography; LDPET, low-dose positron emission tomography; PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; SUV, standardized uptake value. Table 6 summarizes the RMSE, PSNR, and SSIM values for each model, reflecting their respective improvements over the LDPET images: the RMSE, PSNR, and SSIM values for the proposed model were 0.0023&#177;0.0075, 1.2314&#177;4.8354, and 0.0078&#177;0.0153, respectively; those for REDCNN uni were 0.001&#177;0.0089, 0.7781&#177;5.2702, and 0.0088&#177;0.0157, respectively; and those for REDCNN multi were 0.0011&#177;0.0087, 0.8000&#177;5.2093, and 0.0091&#177;0.0155, respectively. Although REDCNN multi exhibited slight improvements over REDCNN uni in RMSE, PSNR, and SSIM, the significance tests ( Table 7 ) indicated that the P values for RMSE and PSNR were both greater than 0.05, indicating that these differences were not statistically significant. Only the improvement in SSIM reached statistical significance (P&lt;0.05). In contrast, the proposed model yielded P values well below 0.05 across all metrics, confirming that its quantitative gains are statistically significant and supporting the effectiveness of the multibranch design strategy. Table 6 Evaluation of quantitative indicators Methods RMSE PSNR SSIM 30-s PET 0.0195&#177;0.0083 35.1759&#177;4.4807 0.9626&#177;0.0180 Proposed 0.0172&#177;0.0075 36.4073&#177;4.8354 0.9704&#177;0.0153 REDCNN uni 0.0185&#177;0.0089 35.9540&#177;5.2702 0.9714&#177;0.0157 REDCNN multi 0.0184&#177;0.0087 35.9759&#177;5.2093 0.9717&#177;0.0155 The number before the &#8220;&#177;&#8221; sign represents the mean value calculated from 864 test samples, while the number after the &#8220;&#177;&#8221; sign indicates the standard deviation. PET, positron emission tomography; PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure. Table 7 Statistical significance (P values) of the differences between each model and REDCNN uni Method RMSE PSNR SSIM REDCNN multi 0.104880231 0.145233327 1.53e&#8722;28 Proposed 8.676e&#8722;18 3.01201e&#8722;13 5.71875e&#8722;50 PSNR, peak signal-to-noise ratio; REDCNN, residual encoder-decoder convolutional neural network; RMSE, root mean square error; SSIM, structural similarity index measure. Edge evaluation As shown in Figure 10 , Sobel operators were first applied to the selected PET slices containing lesion regions to generate corresponding gradient images; lesion areas are highlighted with red circles. Compared to other models, the proposed model produced more distinct lesion boundaries, indicating superior performance in noise suppression and detail preservation&#8212;closely approximating the quality of FDPET images. Figure 10 The gradient images of the lesion area slices providing a more intuitive comparison of contour clarity. The lesion regions are highlighted with red circles. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network. Subsequently, regions of interest (ROIs) were delineated on the selected slices, as indicated by the rectangular box on the PET image in the upper right corner of Figure 11 . The same ROI mask was applied across all reconstructed images, and the corresponding | K | values were computed. These values were then visualized in the form of a histogram ( Figure 11 ), indicating that all other models yielded lower | K | values than the proposed model, indicating that our model achieved the best edge sharpness performance. Figure 11 Histogram of edge sharpness index | K | value. The white box indicates the ROI area used for calculating the | K | value. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network; ROI, region of interest. To ensure the generalizability of the observed improvements, five distinct tumor slices were selected, and the | K | values for each model were computed, as shown in Table 8 . A paired t -test was performed, and the results ( Table 9 ) demonstrated that the proposed DB-REDCNN yielded statistically significant improvements (P&lt;0.01) over other models, indicating superior performance in enhancing tumor edge sharpness. Table 8 The | K | values of five distinct tumor ROIs Tumor Full dose Proposed REDCNN SubtlePET UNET_RA 1 5.2367 5.1468 4.9822 4.8796 4.9024 2 6.5289 6.3452 6.0231 5.9874 6.0073 3 4.8105 4.7865 4.5379 4.5078 4.5190 4 5.9162 5.7864 5.5840 5.5342 5.5267 5 3.8421 3.7523 3.6219 3.5924 3.6134 REDCNN, residual encoder-decoder convolutional neural network; ROI, region of interest. Table 9 Statistical significance (P values) of differences between each model and proposed Method P value ( vs. proposed) REDCNN 0.003103** SubtlePET 0.001139** UNET_RA 0.001459** **, P&lt;0.01. PET, positron emission tomography; REDCNN, residual encoder-decoder convolutional neural network. Discussion In this study, we developed a novel DB-REDCNN model, an extension of the REDCNN architecture. Through its multibranch network design, the model integrates CT images as structural priors to guide the synthesis of FDPET from LDPET inputs, thereby producing outputs that more closely resemble FDPET in quality. Regarding the choice of loss function, experimental results indicated that L1 loss consistently outperformed the other functions across three evaluation metrics, whereas SSIM loss yielded the poorest performance. In the comparative experiments, DB-REDCNN achieved competitive results on standard quantitative metrics, surpassing other models in terms of numerical performance. To further assess the model&#8217;s ability to preserve edge sharpness, we used | K | to evaluate tumor boundary delineation within the ROIs. A series of experimental results demonstrated that our model is more effective in enhancing the representation of tumor edge contours. In addition, we assessed the SUV, a clinically meaningful semantic-level metric, to further enhance the clinical interpretability of the proposed method. The results confirmed that DB-REDCNN not only effectively suppresses noise and restores image details but also more accurately preserves peak radiotracer uptake information within high-activity regions. Limitation Despite the promising results achieved in our study, several noteworthy limitations should be noted. First, the experiments did not account for the potential influence of varying CT image quality on the model&#8217;s output performance. Second, to enhance the generalizability of the proposed method, future research could incorporate list-mode data reconstructed under different conditions&#8212;such as varying numbers of iterations, matrix sizes, and PSF configurations&#8212;or adopt domain adaptation techniques ( 40 ) and invariance learning strategies ( 41 ). Third, the reliance on PET/CT hardware and manual registration by clinicians cannot fully eliminate the inherent misalignment between PET and CT images. Thus, integrating more advanced registration algorithms during preprocessing, along with introducing quantitative metrics to assess registration errors, is required. Fourth, CT-guided imaging is inherently unsuited to the restoration small lesions, particularly in regions with low radiotracer uptake. The integration of functional imaging modalities or multimodal fusion techniques may help improve the reconstruction of such lesions. Finally, although the proposed model performed well across the dataset, variations in uptake time postinjection (ranging from 40 to 90 minutes) may introduce inconsistencies in SUV distribution, potentially affecting the reliability of training and evaluation. Future work should consider enforcing stricter control of uptake times or systematically assessing the impact of uptake time variation on model performance. Conclusions We developed a novel DB-REDCNN model that leverages a multibranch architecture to incorporate structural priors from CT images, thereby enhancing the synthesis of FDPET from LDPET. The model provides superior performance across quantitative evaluation metrics and significantly improves edge sharpness in lesion regions. It effectively preserves peak SUV information and offers clear advantages in both visual quality and clinical applicability. Supplementary The article&#8217;s supplementary files as 10.21037/qims-2025-1321 Acknowledgments None. Funding: This work was supported by the Henan Provincial Science and Technology Research Project ( No. 252102310453 ). Conflicts of Interest: All authors have completed the ICMJE uniform disclosure form (available at https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/coif ). The authors have no conflicts of interest to declare. Data Sharing Statement Available at https://qims.amegroups.com/article/view/10.21037/qims-2025-1321/dss 10.21037/qims-2025-1321 Ethical Statement: The authors are accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. The study was conducted in accordance with the Declaration of Helsinki and its subsequent amendments. The study protocol was approved by the Ethics Committee of The Fifth Affiliated Hospital of Zhengzhou University (approval No. KY2025044). Patient consent was waived due to the retrospective nature of the data analysis and the use of anonymized data. References 1 Brenner DJ Hall EJ . Computed tomography--an increasing source of radiation exposure. N Engl J Med 2007 ; 357 : 2277 - 84 . 10.1056/NEJMra072149 18046031 2 Hosono M Takenaka M Monzen H Tamura M Kudo M Nishimura Y . Cumulative radiation doses from recurrent PET-CT examinations. Br J Radiol 2021 ; 94 : 20210388 . 10.1259/bjr.20210388 34111964 PMC9328066 3 Wang C, Hu Z, Shi P, Liu H. Low dose PET reconstruction with total variation regularization. 2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society; Chicago, IL, USA. 2014:1917-20. 10.1109/EMBC.2014.6943986 25570354 4 Wang Y Ma G An L Shi F Zhang P Lalush DS Wu X Pu Y Zhou J Shen D . Semisupervised Tripled Dictionary Learning for Standard-Dose PET Image Prediction Using Low-Dose PET and Multimodal MRI. IEEE Trans Biomed Eng 2017 ; 64 : 569 - 79 . 10.1109/TBME.2016.2564440 27187939 PMC5383421 5 Xie Z Li T Zhang X Qi W Asma E Qi J. Anatomically aided PET image reconstruction using deep neural networks. Med Phys 2021 ; 48 : 5244 - 58 . 10.1002/mp.15051 34129690 PMC8510002 6 Chen H Zhang Y Kalra MK Lin F Chen Y Liao P Zhou J Wang G Low-Dose CT With a Residual Encoder-Decoder Convolutional Neural Network. IEEE Trans Med Imaging 2017 ; 36 : 2524 - 35 . 10.1109/TMI.2017.2715284 28622671 PMC5727581 7 Wang Y Yu B Wang L Zu C Lalush DS Lin W Wu X Zhou J Shen D Zhou L . 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. Neuroimage 2018 ; 174 : 550 - 62 . 10.1016/j.neuroimage.2018.03.045 29571715 PMC6410574 8 Zhou L Schaefferkoetter JD Tham IWK Huang G Yan J . Supervised learning with cyclegan for low-dose FDG PET image denoising. Med Image Anal 2020 ; 65 : 101770 . 10.1016/j.media.2020.101770 32674043 9 Weyts K Lasnon C Ciappuccini R Lequesne J Corroyer-Dulmont A Quak E Clarisse B Roussel L Bardet S Jaudet C. Artificial intelligence-based PET denoising could allow a two-fold reduction in [18F]FDG PET acquisition time in digital PET/CT. Eur J Nucl Med Mol Imaging 2022 ; 49 : 3750 - 60 . 35593925 10.1007/s00259-022-05800-1 PMC9399218 10 Oktay O Ferrante E Kamnitsas K Heinrich M Bai W Caballero J Cook SA de Marvao A Dawes T O'Regan DP Kainz B Glocker B Rueckert D . Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation. IEEE Trans Med Imaging 2018 ; 37 : 384 - 95 . 10.1109/TMI.2017.2743464 28961105 11 Hu S Lei B Wang S Wang Y Feng Z Shen Y. Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis. IEEE Trans Med Imaging 2022 ; 41 : 145 - 57 . 10.1109/TMI.2021.3107013 34428138 12 Qin Y Li Y Zhuo Z Liu Z Liu Y Ye C. Multimodal super-resolved q-space deep learning. Med Image Anal 2021 ; 71 : 102085 . 10.1016/j.media.2021.102085 33971575 13 Amirrashedi M Sarkar S Mamizadeh H Ghadiri H Ghafarian P Zaidi H Ay MR . Leveraging deep neural networks to improve numerical and perceptual image quality in low-dose preclinical PET imaging. Comput Med Imaging Graph 2021 ; 94 : 102010 . 10.1016/j.compmedimag.2021.102010 34784505 14 Yang Q Yan P Zhang Y Yu H Shi Y Mou X Kalra MK Zhang Y Sun L Wang G Low-Dose CT Image Denoising Using a Generative Adversarial Network With Wasserstein Distance and Perceptual Loss. IEEE Trans Med Imaging 2018 ; 37 : 1348 - 57 . 10.1109/TMI.2018.2827462 29870364 PMC6021013 15 Kleesiek J Urban G Hubert A Schwarz D Maier-Hein K Bendszus M Biller A. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. Neuroimage 2016 ; 129 : 460 - 9 . 10.1016/j.neuroimage.2016.01.024 26808333 16 Pinton NJ Bousse A Cheze-Le-Rest C Visvikis D . Multi-Branch Generative Models for Multichannel Imaging With an Application to PET/CT Synergistic Reconstruction. IEEE Transactions on Radiation and Plasma Medical Sciences 2025 ; 9 : 654 - 6 . 17 Chen X, Zhou B, Xie H, Guo X, Zhang J, Sinusas AJ, Onofrey JA, Liu C. Dual-branch squeeze-fusion-excitation module for cross-modality registration of cardiac SPECT and CT. In: Wang L, Dou Q, Fletcher PT, Speidel S, Li S, editors. International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland; 2022:46-55. 18 Pinton NJ, Bousse A, Wang Z, Cheze-Le-Rest C, Maxim V, Comtat C, Sureau F, Visvikis D. Synergistic PET/CT reconstruction using a joint generative model. arXiv:2411.07339 [preprint]. 2024. Available online: https://arxiv.org/abs/2411.07339 19 Ding Z Zhou D Nie R Hou R Liu Y. Brain Medical Image Fusion Based on Dual-Branch CNNs in NSST Domain. Biomed Res Int 2020 ; 2020 : 6265708 . 10.1155/2020/6265708 32352003 PMC7178475 20 Cao H Liu H Song E Hung CC Ma G Xu X Jin R Lu J . Dual-branch residual network for lung nodule segmentation. Applied Soft Computing 2020 ; 86 : 105934 . 21 Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. Available online: https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf 22 Chen H Zhang Y Zhang W Liao P Li K Zhou J Wang G. Low-dose CT via convolutional neural network. Biomed Opt Express 2017 ; 8 : 679 - 94 . 10.1364/BOE.8.000679 28270976 PMC5330597 23 Mao X, Shen C, Yang YB. Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections. Advances in neural information processing systems 2016. Avaliable online: https://arxiv.org/abs/1603.09056 24 He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-8. 25 Hodson TO . Root mean square error (RMSE) or mean absolute error (MAE): When to use them or not. Geoscientific Model Development Discussions 2022 ; 15 : 5481 - 7 . 26 Hore A, Ziou D. Image quality metrics: PSNR vs. SSIM. 2010 20th international conference on pattern recognition. IEEE, 2010:2366-9. 27 Wang Z Bovik AC Sheikh HR Simoncelli EP . Image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process 2004 ; 13 : 600 - 12 . 10.1109/tip.2003.819861 15376593 28 Ahmad R Ding Y Simonetti OP . Edge Sharpness Assessment by Parametric Modeling: Application to Magnetic Resonance Imaging. Concepts Magn Reson Part A Bridg Educ Res 2015 ; 44 : 138 - 49 . 10.1002/cmr.a.21339 26755895 PMC4706083 29 Yao R Seidel J Johnson CA Daube-Witherspoon ME Green MV Carson RE . Performance characteristics of the 3-D OSEM algorithm in the reconstruction of small animal PET images. IEEE transactions on medical imaging 2000 ; 19 : 798 - 804 . 10.1109/42.876305 11055803 30 Joshi N, Szeliski R, Kriegman DJ. PSF estimation using sharp edge prediction. 2008 IEEE conference on computer vision and pattern recognition. IEEE; 2008:1-8. 31 Kinahan PE Townsend DW Beyer T Sashin D . Attenuation correction for a combined 3D PET/CT scanner. Med Phys 1998 ; 25 : 2046 - 53 . 10.1118/1.598392 9800714 32 Dong C Loy CC He K Tang X . Image Super-Resolution Using Deep Convolutional Networks. IEEE Trans Pattern Anal Mach Intell 2016 ; 38 : 295 - 307 . 10.1109/TPAMI.2015.2439281 26761735 33 Xie J, Xu L, Chen E. Image denoising and inpainting with deep neural networks. Advances in neural information processing systems, 2012. Avaliable online: https://dl.acm.org/doi/10.5555/2999134.2999173 34 Jain V, Seung S. Natural image denoising with convolutional networks. Advances in neural information processing systems, 2008. Avaliable online: https://dl.acm.org/doi/10.5555/2981780.2981876 35 He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Proceedings of the IEEE international conference on computer vision. 2015:1026-34. 36 Diederik K. Adam: A method for stochastic optimization. 2014. Avaliable online: https://arxiv.org/abs/1412.6980 37 Paszke A, Gross S, Chintala S, et al. Automatic differentiation in pytorch. 2017. Avaliable online: https://openreview.net/forum?id=BJJsrmfCZ 38 Chen KT Gong E de Carvalho Macruz FB Xu J Boumis A Khalighi M Poston KL Sha SJ Greicius MD Mormino E Pauly JM Srinivas S Zaharchuk G . Ultra-Low-Dose (18)F-Florbetaben Amyloid PET Imaging Using Deep Learning with Multi-Contrast MRI Inputs. Radiology 2019 ; 290 : 649 - 56 . 10.1148/radiol.2018180940 30526350 PMC6394782 39 Berghmans T Dusart M Paesmans M Hossein-Foucher C Buvat I Castaigne C Scherpereel A Mascaux C Moreau M Roelandts M Alard S Meert AP Patz EF Jr Lafitte JJ Sculier JP European Lung Cancer Working Party for the IASLC Lung Cancer Staging Project . Primary tumor standardized uptake value (SUVmax) measured on fluorodeoxyglucose positron emission tomography (FDG-PET) is of prognostic value for survival in non-small cell lung cancer (NSCLC): a systematic review and meta-analysis (MA) by the European Lung Cancer Working Party for the IASLC Lung Cancer Staging Project. J Thorac Oncol 2008 ; 3 : 6 - 12 . 10.1097/JTO.0b013e31815e6d6b 18166834 40 Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation. Proceedings of the 32nd International Conference on Machine Learning, PMLR 2015;37:1180-9. 41 Kwon J, Kim J, Park H, Choi IK. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. Proceedings of the 38th International Conference on Machine Learning, PMLR 2021;139:5905-14."
}