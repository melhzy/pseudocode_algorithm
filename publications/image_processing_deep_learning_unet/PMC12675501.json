{
  "pmcid": "PMC12675501",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:27.087706",
  "metadata": {
    "journal_title": "Scientific Reports",
    "journal_nlm_ta": "Sci Rep",
    "journal_iso_abbrev": "Sci Rep",
    "journal": "Scientific Reports",
    "pmcid": "PMC12675501",
    "pmid": "41339401",
    "doi": "10.1038/s41598-025-27127-1",
    "title": "DifuzCam replacing camera lens with a mask and a diffusion model for generative AI based flat camera design",
    "year": "2025",
    "month": "12",
    "day": "3",
    "pub_date": {
      "year": "2025",
      "month": "12",
      "day": "3"
    },
    "authors": [
      "Yosef Erez",
      "Giryes Raja"
    ],
    "abstract": "Recent advances in lensless, flat camera designs hold the promise of significantly reducing size and weight by replacing bulky lenses with thin optical elements that modulate incoming light. However, recovering high-quality images from the raw sensor measurements of such systems remains challenging. We address this limitation by introducing a novel reconstruction framework that leverages a pre-trained diffusion model, guided by a control network and a learnable separable transformation. This approach delivers high-fidelity images, achieving state-of-the-art performance in both objective and perceptual metrics. Our method achieves 20.43 PSNR, 0.612 SSIM, and 0.237 LPIPS on the FlatNet dataset, representing improvements of 9.6%, 18.1%, and 26.4% respectively over the previous state-of-the-art FlatNet method. Additionally, the text-conditioned nature of the diffusion model enables optional enhancement through scene descriptions, particularly valuable for compact imaging systems where user input can help resolve reconstruction ambiguities. We demonstrate the effectiveness of our method on a 8 flat camera, paving the way for advanced lensless imaging solutions and offering a robust framework for improved reconstructions that is relevant to a broad range of computational imaging systems.",
    "keywords": [
      "Computational photography",
      "Deep learning",
      "Lensless imaging",
      "Artificial intelligence",
      "Image reconstruction",
      "Neural networks",
      "Electrical and electronic engineering",
      "Computer science",
      "Imaging and sensing"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" article-type=\"research-article\" xml:lang=\"en\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sci Rep</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sci Rep</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1579</journal-id><journal-id journal-id-type=\"pmc-domain\">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type=\"epub\">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12675501</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12675501.1</article-id><article-id pub-id-type=\"pmcaid\">12675501</article-id><article-id pub-id-type=\"pmcaiid\">12675501</article-id><article-id pub-id-type=\"pmid\">41339401</article-id><article-id pub-id-type=\"doi\">10.1038/s41598-025-27127-1</article-id><article-id pub-id-type=\"publisher-id\">27127</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>DifuzCam replacing camera lens with a mask and a diffusion model for generative AI based flat camera design</article-title></title-group><contrib-group><contrib contrib-type=\"author\" corresp=\"yes\"><name name-style=\"western\"><surname>Yosef</surname><given-names initials=\"E\">Erez</given-names></name><address><email>Erez.yo@gmail.com</email></address><xref ref-type=\"aff\" rid=\"Aff1\"/></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Giryes</surname><given-names initials=\"R\">Raja</given-names></name><xref ref-type=\"aff\" rid=\"Aff1\"/></contrib><aff id=\"Aff1\"><institution-wrap><institution-id institution-id-type=\"ROR\">https://ror.org/04mhzgx49</institution-id><institution-id institution-id-type=\"GRID\">grid.12136.37</institution-id><institution-id institution-id-type=\"ISNI\">0000 0004 1937 0546</institution-id><institution>Tel Aviv University, </institution></institution-wrap>Tel Aviv, Israel </aff></contrib-group><pub-date pub-type=\"epub\"><day>3</day><month>12</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type=\"pmc-issue-id\">478255</issue-id><elocation-id>43059</elocation-id><history><date date-type=\"received\"><day>3</day><month>6</month><year>2025</year></date><date date-type=\"accepted\"><day>31</day><month>10</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>03</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>05</day><month>12</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-12-05 00:25:12.533\"><day>05</day><month>12</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbyncndlicense\">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"41598_2025_Article_27127.pdf\"/><abstract id=\"Abs1\"><p id=\"Par1\">Recent advances in lensless, flat camera designs hold the promise of significantly reducing size and weight by replacing bulky lenses with thin optical elements that modulate incoming light. However, recovering high-quality images from the raw sensor measurements of such systems remains challenging. We address this limitation by introducing a novel reconstruction framework that leverages a pre-trained diffusion model, guided by a control network and a learnable separable transformation. This approach delivers high-fidelity images, achieving state-of-the-art performance in both objective and perceptual metrics. Our method achieves 20.43 PSNR, 0.612 SSIM, and 0.237 LPIPS on the FlatNet dataset, representing improvements of 9.6%, 18.1%, and 26.4% respectively over the previous state-of-the-art FlatNet method. Additionally, the text-conditioned nature of the diffusion model enables optional enhancement through scene descriptions, particularly valuable for compact imaging systems where user input can help resolve reconstruction ambiguities. We demonstrate the effectiveness of our method on a 8 flat camera, paving the way for advanced lensless imaging solutions and offering a robust framework for improved reconstructions that is relevant to a broad range of computational imaging systems.</p></abstract><kwd-group xml:lang=\"en\"><title>Keywords</title><kwd>Computational photography</kwd><kwd>Deep learning</kwd><kwd>Lensless imaging</kwd><kwd>Artificial intelligence</kwd><kwd>Image reconstruction</kwd><kwd>Neural networks</kwd></kwd-group><kwd-group kwd-group-type=\"npg-subject\"><title>Subject terms</title><kwd>Electrical and electronic engineering</kwd><kwd>Computer science</kwd><kwd>Imaging and sensing</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type=\"FundRef\">https://doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>101113391</award-id><award-id>101113391</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id=\"Sec1\"><title>Introduction</title><p id=\"Par2\">Cameras have become very popular and common in recent years, especially in small handheld devices. Nevertheless, reducing the size of the camera remains a difficult problem since a camera requires lenses and optical elements to get a high-quality image. Flat cameras<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup> are a computational photography method to reduce camera size by replacing the camera lens with a diffuser, namely, an amplitude mask placed very close to the sensor. Thus, the sensor captures multiplexed projections of scene reflections across its entire area, resulting in an image that is not visually understandable. Using a computational algorithm, the scene image can be retrieved. Yet, while achieving massive camera size reduction, reconstructing high-quality images from flat camera measurements is an ill-posed task and hard to achieve.</p><p id=\"Par3\">Previous approaches tried to reconstruct the scene image using different techniques, including direct optimization<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup> and deep learning<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. Despite these attempts, the resulting images are not of sufficient quality. High-quality image reconstruction from flat camera measurements remains an open challenge, and better algorithms are required.<fig id=\"Fig1\" position=\"float\" orientation=\"portrait\"><label>Fig. 1</label><caption><p>Using (<bold>a</bold>) our prototype flat camera, (<bold>b</bold>) a measurement image is captured that is not visually understandable. (<bold>c</bold>) An image is reconstructed from the measurements using our text-guided approach (DifuzCam), compared to (<bold>d</bold>) the reference image captured with a regular camera. (see Fig.&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref> for details).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO1\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig1_HTML.jpg\"/></fig></p><p id=\"Par4\">To improve the quality, we propose <bold><italic toggle=\"yes\">DifuzCam</italic></bold>, a novel strategy for flat camera image reconstruction using a strong image prior that relies on a pre-trained diffusion model<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref>,<xref ref-type=\"bibr\" rid=\"CR4\">4</xref></sup>. Figure&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref> presents an overview diagram of <italic toggle=\"yes\">DifuzCam</italic>. Using a pre-trained generative model trained on many images provides a strong prior for natural images, which facilitates reconstructing high-quality images from flat camera measurements (Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>). Moreover, we leverage the inherent text-conditioning capability of the diffusion model to optionally incorporate scene descriptions from the photographer. This additional information can help resolve ambiguities in the highly ill-posed reconstruction problem<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>, particularly valuable for compact camera systems where traditional optical cues are limited.<fig id=\"Fig2\" position=\"float\" orientation=\"portrait\"><label>Fig. 2</label><caption><p>DifuzCam proposed method. A flat lensless camera measurements are inputted to a separable linear transformation followed by a ControlNet adapter to control the image generation process by a pre-trained latent diffusion model (LDM). Using text guidance for the reconstruction process is optional in our method. On training, the <italic toggle=\"yes\">yellow</italic> blocks weights are optimized, and <italic toggle=\"yes\">orange</italic> paths are the training losses, while the <italic toggle=\"yes\">blue</italic> blocks weights are pre-trained and fixed. The dataset for training and testing was captured using our prototype flat camera by projecting images onto a screen. We present an additional loss <inline-formula id=\"IEq1\"><tex-math id=\"d33e249\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_{sep}$$\\end{document}</tex-math></inline-formula> in addition to the diffusion training loss (<inline-formula id=\"IEq2\"><tex-math id=\"d33e253\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$l_{\\mathcal {C}}$$\\end{document}</tex-math></inline-formula>) for better convergence. The reconstructed image is achieved by <italic toggle=\"yes\">T</italic> iterative diffusion steps and decoding from the latent space to the pixel space.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO2\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig2_HTML.jpg\"/></fig></p><p id=\"Par5\">We used a pretrained and fixed stable diffusion<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup> to leverage its strong image prior, and trained a ControlNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup> to guide the reconstruction using the sensor measurements. We built a flat camera prototype (Fig.&#160;<xref rid=\"Fig1\" ref-type=\"fig\">1</xref>) and captured a large-scale dataset for training. We present qualitative and quantitative evaluations of <italic toggle=\"yes\">DifuzCam</italic> using both our camera prototype and an existing dataset from prior work. Our method improves reconstruction compared to prior art and thus advances the practicality of flat cameras. Our strategy is generic, combining established diffusion model techniques with lensless imaging, with the potential to be adapted to other imaging systems and setups, leveraging the diffusion model&#8217;s strong prior and text guidance capability to improve reconstructed image quality.</p><sec id=\"Sec2\"><title>Related work</title><p id=\"Par6\">Lensless imaging has garnered significant attention in recent years since it offers a more compact, lightweight, and cost-effective imaging system. Various lensless camera designs were proposed, such as static amplitude mask<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, modulation with LCD<sup><xref ref-type=\"bibr\" rid=\"CR7\">7</xref></sup>, Fresnel zone aperture (FZA) with SLM<sup><xref ref-type=\"bibr\" rid=\"CR8\">8</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR10\">10</xref></sup>, phase mask<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref>,<xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>, programmable devices<sup><xref ref-type=\"bibr\" rid=\"CR13\">13</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>, and more as described in Ref.<sup><xref ref-type=\"bibr\" rid=\"CR17\">17</xref></sup>.</p><p id=\"Par7\">Recent advances in metasurface-based imaging<sup><xref ref-type=\"bibr\" rid=\"CR18\">18</xref></sup> have achieved high diffraction efficiency with improved light throughput. Neural nano-optics approaches<sup><xref ref-type=\"bibr\" rid=\"CR19\">19</xref></sup> combine engineered optical elements with deep learning for compact imaging. Learned thin-plate optics<sup><xref ref-type=\"bibr\" rid=\"CR20\">20</xref></sup> jointly optimize optical design and reconstruction algorithms. While these approaches offer advantages in light efficiency, they require more complex fabrication compared to our lithographic amplitude masks.</p><p id=\"Par8\">We build upon the FlatCam design<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. It employs a static amplitude mask placed near the sensor. The mask pattern is designed in a separable manner such that the imaging linear model can be simplified to a separable operation:<disp-formula id=\"Equ1\"><label>1</label><tex-math id=\"d33e337\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} Y = \\Phi _lX\\Phi _r, \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq3\"><tex-math id=\"d33e342\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Phi _L$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq4\"><tex-math id=\"d33e346\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\Phi _R$$\\end{document}</tex-math></inline-formula> are the separable operations on the image&#8217;s rows and columns, respectively. <italic toggle=\"yes\">X</italic> is the scene intensity and <italic toggle=\"yes\">Y</italic> is the sensor measurements.</p><p id=\"Par9\">In this case, the camera&#8217;s measurements consist of multiplexed projections of the scene reflectance such that reconstruction becomes an ill-posed problem. Prior methods for image reconstruction from flat camera measurements were designed as an optimization problem<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref>,<xref ref-type=\"bibr\" rid=\"CR15\">15</xref>,<xref ref-type=\"bibr\" rid=\"CR16\">16</xref></sup>. Such model-based methods are heavily dependent on the imaging model and rely on accurate calibration. Thus, they were very limited and the reconstruction quality was low.</p><p id=\"Par10\">To overcome some of these limitations, data-driven algorithms were proposed using deep learning to reconstruct high-quality images from multiplexed measurements. For example, deep learning was used to optimize the alternating direction method of multipliers (ADMM) parameters and reconstruct the image<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup>. FlatNet<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref>,<xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup> used a learned separable transform followed by a Unet<sup><xref ref-type=\"bibr\" rid=\"CR23\">23</xref></sup> architecture, which perceptually enhances the resulting image in a generative adversarial network (GAN) approach. A GAN is also presented for lensless point spread function (PSF) estimation and robust reconstruction<sup><xref ref-type=\"bibr\" rid=\"CR24\">24</xref></sup>. In<sup><xref ref-type=\"bibr\" rid=\"CR25\">25</xref></sup>, a Unet architecture followed by a deep back-projection network (DBPN)<sup><xref ref-type=\"bibr\" rid=\"CR26\">26</xref></sup> was proposed for FZA lensless camera image reconstruction. Unrolled primal-dual networks<sup><xref ref-type=\"bibr\" rid=\"CR27\">27</xref></sup> present an alternative approach by unrolling iterative optimization algorithms into learnable network architectures, potentially offering improved reconstruction quality through learned optimization steps. To address the mismatch between the ideal imaging model and the real-world model, the work in<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref></sup> presented multi-Wiener deconvolution networks in multi-scale feature spaces. A transformer architecture<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup> and a multi-level image restoration with the pix2pix generative network<sup><xref ref-type=\"bibr\" rid=\"CR30\">30</xref></sup> were employed to further improve reconstruction.</p></sec><sec id=\"Sec3\"><title>Diffusion models</title><p id=\"Par11\">References<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref>,<xref ref-type=\"bibr\" rid=\"CR31\">31</xref>,<xref ref-type=\"bibr\" rid=\"CR32\">32</xref></sup> are generative models that have an impressive ability to learn natural image distributions and are used for image generation<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref>,<xref ref-type=\"bibr\" rid=\"CR33\">33</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR35\">35</xref></sup>, segmentation<sup><xref ref-type=\"bibr\" rid=\"CR36\">36</xref>,<xref ref-type=\"bibr\" rid=\"CR37\">37</xref></sup>, inpainting<sup><xref ref-type=\"bibr\" rid=\"CR38\">38</xref></sup>, image super-resolution<sup><xref ref-type=\"bibr\" rid=\"CR39\">39</xref></sup>, and image reconstruction<sup><xref ref-type=\"bibr\" rid=\"CR4\">4</xref>,<xref ref-type=\"bibr\" rid=\"CR40\">40</xref>&#8211;<xref ref-type=\"bibr\" rid=\"CR45\">45</xref></sup>. The diffusion model is a parameterized Markov chain that produces samples of a certain data distribution after a predefined <italic toggle=\"yes\">T</italic> number of steps. In each step of the forward diffusion process, a Gaussian noise is added to the data, such that after a large number of steps, the sample is mapped to an isotropic Gaussian. In the reverse step, a neural network is trained to denoise the clean image from the noisy sample. To sample an image, we apply diffusion steps in the reverse direction, starting with pure Gaussian noise and denoising it until getting a clean natural image from the learned distribution.</p><p id=\"Par12\">In the context of low-level image restoration, diffusion models were used for image restoration of linear inverse problems<sup><xref ref-type=\"bibr\" rid=\"CR40\">40</xref>,<xref ref-type=\"bibr\" rid=\"CR46\">46</xref>,<xref ref-type=\"bibr\" rid=\"CR47\">47</xref></sup>, spatially-variant noise removal<sup><xref ref-type=\"bibr\" rid=\"CR48\">48</xref></sup>, and low-light image enhancement<sup><xref ref-type=\"bibr\" rid=\"CR49\">49</xref>,<xref ref-type=\"bibr\" rid=\"CR50\">50</xref></sup>. They were also presented for a low-light text recognition task<sup><xref ref-type=\"bibr\" rid=\"CR51\">51</xref></sup>. The use of text guidance for conventional imaging reconstruction and enhancement was recently proposed<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref>,<xref ref-type=\"bibr\" rid=\"CR52\">52</xref>,<xref ref-type=\"bibr\" rid=\"CR53\">53</xref></sup>. Concurrent with our work,<sup><xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup> explored the use of diffusion models for lensless imaging. Our approach differs in several key aspects. First, we use the Flatnet<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup> camera design and dataset for comparisons, whereas their work evaluates different designs<sup><xref ref-type=\"bibr\" rid=\"CR11\">11</xref>,<xref ref-type=\"bibr\" rid=\"CR12\">12</xref></sup>, resulting in distinct imaging conditions and reconstruction challenges. Second, beyond simulation-based results, we developed a working prototype of our flat lensless camera and captured a large-scale dataset to support future research in this field. Third, we introduce text guidance to enhance reconstruction quality, addressing the ill-posed nature of the problem by leveraging state-of-the-art multimodal artificial intelligence (AI) models. These differences highlight the novelty of our method and its contribution to the field.</p><p id=\"Par13\">For non-conventional imaging, such as a flat camera, adopting diffusion models for the recovery process is not trivial, and additional adjustments and considerations are required. This work bridges this gap and presents a novel approach to integrating diffusion models for this imaging task.</p><p id=\"Par14\">Latent diffusion model (LDM)<sup><xref ref-type=\"bibr\" rid=\"CR3\">3</xref></sup> is a method for applying the diffusion process in a latent space instead of the pixel space to get a more computationally efficient model for high-resolution images. In this approach, a pre-trained autoencoder consisting of encoder <inline-formula id=\"IEq5\"><tex-math id=\"d33e529\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {E}$$\\end{document}</tex-math></inline-formula> and decoder <inline-formula id=\"IEq6\"><tex-math id=\"d33e533\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {D}$$\\end{document}</tex-math></inline-formula> is used to train a diffusion model in a low-dimensional (latent) space.</p><p id=\"Par15\">The data samples are denoted as <inline-formula id=\"IEq7\"><tex-math id=\"d33e539\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$x \\sim q(x)$$\\end{document}</tex-math></inline-formula>, where <italic toggle=\"yes\">q</italic> is the data distribution we learn, and the latent sample is obtained by the encoder as <inline-formula id=\"IEq8\"><tex-math id=\"d33e546\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$z = \\mathcal {E}(x)$$\\end{document}</tex-math></inline-formula>. A denoiser network <inline-formula id=\"IEq9\"><tex-math id=\"d33e550\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\epsilon _\\theta$$\\end{document}</tex-math></inline-formula> is trained to predict the added noise (at the backward diffusion step) from the noisy sample <inline-formula id=\"IEq10\"><tex-math id=\"d33e554\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$z_t$$\\end{document}</tex-math></inline-formula> at the timestamp <inline-formula id=\"IEq11\"><tex-math id=\"d33e559\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$t \\in [0,\\ldots ,T]$$\\end{document}</tex-math></inline-formula>. With an additional text input <italic toggle=\"yes\">y</italic> for text guidance in image generation, the training loss for the LDM is<disp-formula id=\"Equ2\"><label>2</label><tex-math id=\"d33e566\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} l_{ldm}=\\mathbb {E}_{\\mathcal {E}(x), \\epsilon \\sim N(0,\\textbf{I}), t \\sim U(0,T)}\\Bigl [ ||\\epsilon - \\epsilon _\\theta (z_t, t,y) || \\Bigr ]. \\end{aligned}$$\\end{document}</tex-math></disp-formula></p><p id=\"Par16\">ControlNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup> adds an image encoder to control the diffusion process. This control network is initialized as the pre-trained encoder of the diffusion model UNet with additional zero convolutions. The output features of the control network are added to the pre-trained diffusion model features, and due to the zero convolutions, the initial performance of the diffusion model is not degraded. After training, ControlNet weights are optimized such that the added control network features manipulate the diffusion process for the target task. Other methods for controlling the generation process of diffusion models for a specific task were also suggested<sup><xref ref-type=\"bibr\" rid=\"CR55\">55</xref>,<xref ref-type=\"bibr\" rid=\"CR56\">56</xref></sup>.<fig id=\"Fig3\" position=\"float\" orientation=\"portrait\"><label>Fig. 3</label><caption><p>Optical design. (<bold>a</bold>) The designed separable mask, which was printed on a chrome plate using lithography. (<bold>b</bold>) The measured PSF by the prototype camera with the coded mask. Our rank-1 mask design is equivalent to prior designs as detailed in the Supplementary Information.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO3\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig3_HTML.jpg\"/></fig></p></sec><sec id=\"Sec4\"><title>Contributions</title><p id=\"Par17\">Our main contributions are: (i) A novel computational photography method based on a lensless flat camera with a reconstruction algorithm that leverages a diffusion model image prior; (ii) State-of-the-art results for flat camera reconstruction quality across all evaluated metrics; (iii) Demonstration of how text conditioning inherent in diffusion models can optionally enhance imaging results for compact optical systems; (iv) an intermediate separable loss for improving convergence and results; and (v) A new large-scale dataset captured with our flat camera prototype, which facilitates further research in this field. All data and code will be made publicly available to encourage further advancements.</p></sec></sec><sec id=\"Sec5\"><title>Results</title><p id=\"Par18\">\n<fig id=\"Fig4\" position=\"float\" orientation=\"portrait\"><label>Fig. 4</label><caption><p>Qualitative results. Examples of reconstruction results for our captured dataset with the prototype camera we designed. GT images are from the LAION dataset<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup> (CC BY 4.0 license)..</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO4\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig4_HTML.jpg\"/></fig>\n</p><p id=\"Par19\">Our <italic toggle=\"yes\">DifuzCam</italic> method was evaluated using image quality and text similarity metrics. The results of the proposed reconstruction algorithm were compared to the ground truth images using PSNR, SSIM<sup><xref ref-type=\"bibr\" rid=\"CR58\">58</xref></sup>, and LPIPS<sup><xref ref-type=\"bibr\" rid=\"CR59\">59</xref></sup> metrics. Since we also present a text guidance approach, we measured the CLIP score<sup><xref ref-type=\"bibr\" rid=\"CR21\">21</xref></sup> compared to a GT text description of the captured scene, a metric that evaluates the similarity of an image to a text. For this metric, we used the latest published and most accurate model (ViT-L/14@336px). On the test dataset that we captured, our method achieved 21.58 in PSNR, 0.541 in SSIM, 0.276 in LPIPS, and a CLIP score of 24.38. Figure&#160;<xref rid=\"Fig4\" ref-type=\"fig\">4</xref> presents examples of our reconstruction results.<table-wrap id=\"Tab1\" position=\"float\" orientation=\"portrait\"><label>Table 1</label><caption><p>Quantitative evaluation and comparison to prior work.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSNR<inline-formula id=\"IEq12\"><tex-math id=\"d33e657\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SSIM<inline-formula id=\"IEq13\"><tex-math id=\"d33e663\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">LPIPS<inline-formula id=\"IEq14\"><tex-math id=\"d33e669\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">CLIP<inline-formula id=\"IEq15\"><tex-math id=\"d33e675\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tikhonov<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">10.98</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.318</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.736</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">16.76</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">flatnet-R<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.58</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.487</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.332</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.53</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">flatnet-T<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">18.64</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.518</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.322</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.68</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Tikhonov + Diffusion</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">19.80</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.536</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.277</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22.57</td></tr><tr><td align=\"left\" colspan=\"5\" rowspan=\"1\"><bold>DifuzCam (our)</bold></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Without text</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">20.25</italic></td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.601</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.242</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22.81</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Sampled with text</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">20.20</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">0.604</italic></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">0.238</italic></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><italic toggle=\"yes\">23.50</italic></td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Fine-tuned with text</td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>20.43</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>0.612</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>0.237</bold></td><td align=\"left\" colspan=\"1\" rowspan=\"1\"><bold>23.53</bold></td></tr></tbody></table><table-wrap-foot><p>Our method outperforms prior approaches: Tikhonov<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup> and FlatNet<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>. Compared to a two-stage naive baseline that combines Tikhonov reconstruction with ControlNet and diffusion enhancement, the joint optimization in our approach demonstrates clear benefits.</p></table-wrap-foot></table-wrap></p><p id=\"Par20\">To have a fair comparison to previous works, tests should be performed on the same dataset. Since FlatNet<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup> published the dataset of their camera, we conducted experiments also on their data to present a valid comparison. Unfortunately, we could not compare to<sup><xref ref-type=\"bibr\" rid=\"CR28\">28</xref>,<xref ref-type=\"bibr\" rid=\"CR54\">54</xref></sup> as they did not publish their code for comparison. Since a prior transformer-based method<sup><xref ref-type=\"bibr\" rid=\"CR29\">29</xref></sup> was not trained or tested on the FlatNet data, we trained their method for comparison, but did not achieve good results.</p><p id=\"Par21\">To assess the contribution of the ControlNet and diffusion model components, as well as the benefits of joint optimization, we implemented a naive baseline that combines traditional Tikhonov reconstruction<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup> with ControlNet-guided diffusion enhancement as a post-processing step. This two-stage approach demonstrates that while diffusion models can enhance traditional reconstructions, the joint optimization in our method provides clear benefits across all metrics, validating our integrated approach.</p><p id=\"Par22\">The comparisons to<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup> are presented quantitatively in Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref> and visually in Fig.&#160;<xref rid=\"Fig7\" ref-type=\"fig\">7</xref>. Their results were produced using their published code and weights. Our method achieved superior results in all evaluated metrics for non-text-guided models. Adding text to the reconstruction process improves the results in the perceptual and textual similarity metrics. Note that when we just add text without fine-tuning the model, a minor degradation in PSNR is observed. When we fine-tune the model to be text-guided, we get the best improvement (Table&#160;<xref rid=\"Tab1\" ref-type=\"table\">1</xref>).</p><p id=\"Par23\">Figure&#160;<xref rid=\"Fig5\" ref-type=\"fig\">5</xref> presents results for reconstructions of real scenes (not captured from a screen) that are provided in<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>, and compares them to FlatNet<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup> and Tikhonov<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>. Figure&#160;<xref rid=\"Fig6\" ref-type=\"fig\">6</xref> presents real scene reconstructions using our prototype camera.<fig id=\"Fig5\" position=\"float\" orientation=\"portrait\"><label>Fig. 5</label><caption><p>Real scene results. Comparison of reconstructions from real scenes measurements with Tikhonov<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref></sup>, FlatNet-T<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup> and Difuzcam (our).</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO5\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig5_HTML.jpg\"/></fig><table-wrap id=\"Tab2\" position=\"float\" orientation=\"portrait\"><label>Table 2</label><caption><p>Results and ablation of our method on the dataset captured by the prototype camera.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"left\" colspan=\"1\" rowspan=\"1\">Method</th><th align=\"left\" colspan=\"1\" rowspan=\"1\">PSNR<inline-formula id=\"IEq16\"><tex-math id=\"d33e893\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">SSIM<inline-formula id=\"IEq17\"><tex-math id=\"d33e899\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">LPIPS<inline-formula id=\"IEq18\"><tex-math id=\"d33e905\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\downarrow$$\\end{document}</tex-math></inline-formula></th><th align=\"left\" colspan=\"1\" rowspan=\"1\">CLIP<inline-formula id=\"IEq19\"><tex-math id=\"d33e911\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\uparrow$$\\end{document}</tex-math></inline-formula></th></tr></thead><tbody><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">w/o Sep. loss</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">9.84</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.175</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.666</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">23.22</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">w/o Text train</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.44</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.512</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.304</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">22.50</td></tr><tr><td align=\"left\" colspan=\"1\" rowspan=\"1\">Proposed method</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">21.58</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.541</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">0.276</td><td align=\"left\" colspan=\"1\" rowspan=\"1\">24.38</td></tr></tbody></table></table-wrap></p><p id=\"Par24\">\n<fig id=\"Fig6\" position=\"float\" orientation=\"portrait\"><label>Fig. 6</label><caption><p>Our prototype results. Real objects were captured with our prototype camera and reconstructed using our proposed method (Difuzcam) with and without text. The reference images were captured with a Canon 80D for visual comparison. Note that they are not accurately aligned with the Difuzcam results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO6\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig6_HTML.jpg\"/></fig>\n<fig id=\"Fig7\" position=\"float\" orientation=\"portrait\"><label>Fig. 7</label><caption><p>Results comparison. We compare the results of DifuzCam to the previous method FlatNet-T<sup><xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup> on their published dataset, and to Tikhonov reconstruction followed by a similar diffusion model for enhancement, as a naive diffusion baseline.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO7\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig7_HTML.jpg\"/></fig>\n<fig id=\"Fig8\" position=\"float\" orientation=\"portrait\"><label>Fig. 8</label><caption><p>Ablation Results. Demonstrating the contribution of the text to the reconstruction. Without text input, the reconstruction relies solely on the flat camera measurements. With text guidance, the reconstructed image better aligns with the true scene, capturing finer details. However, when an incorrect caption is provided, the reconstructed details and high frequencies align more with the incorrect text and less compatible with the scene. Additionally, when the separable loss is not applied, the reconstruction depends solely on the input caption, disregarding sensor measurements. This underscores the crucial role of the separable loss in ensuring proper convergence. GT images are from the LAION dataset<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup> (CC BY 4.0 license)..</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"MO8\" position=\"float\" orientation=\"portrait\" xlink:href=\"41598_2025_27127_Fig8_HTML.jpg\"/></fig>\n</p><sec id=\"Sec6\"><title>Implementation details</title><p id=\"Par25\">In the dataset, the given text captions for the images are not always accurate or relevant. We acknowledge that this noise in the data might harm the results we get in the training process when using these text captions. Since we identified that these inaccuracies might be critical in the tests, we manually checked the test dataset captions to verify the accuracy and correctness of the data. This verification is very important for the text guidance reconstruction results and also for the textual CLIP score evaluation we made. Despite the potential disadvantage of training on incorrect captions, we did not manually verify the training dataset since it is not feasible to manually check such a very large dataset.</p><p id=\"Par26\">To compare our results to FlatNet<sup><xref ref-type=\"bibr\" rid=\"CR2\">2</xref></sup>, we trained our method on their published dataset, which consists of 10 k images for training and 100 for testing. This data does not contain captions for the images. Thus, to train our method with text guidance on this data, we used a large language model (LLM) for the automatic image captioning process. We used the LLaVA1.5<sup><xref ref-type=\"bibr\" rid=\"CR60\">60</xref></sup> model and generated captions for all the images in the data. Here, we also encounter the challenge of potentially incorrect captions, which is also known as LLM hallucinations. Also, for this case, the test sample captions were manually verified due to the high importance of the test captions&#8217; correctness. For this data, we trained the model for 700k steps with a similar optimizer setup to what we mentioned in the subsection <italic toggle=\"yes\">Data Acquisition and Training</italic>. We used the Allied Vision 1800 U-500 board-level camera with a pixel size of 2.2um and 5 megapixels overall for the prototype camera.</p></sec><sec id=\"Sec7\"><title>Analysis and ablations</title><p id=\"Par27\">In lensless imaging, the reconstruction problem is highly ill-posed, meaning that multiple plausible solutions can correspond to the same measurements. Since essential scene information is lost, our diffusion model, trained on the natural image distribution, infers and completes the missing details and features by leveraging natural image priors and textual description priors.</p><p id=\"Par28\">The balance between physical information and generative content in our reconstructions stems from the fundamental information loss inherent in lensless imaging. The flat camera&#8217;s amplitude mask causes significant information loss during acquisition, creating an ill-posed inverse problem where multiple scene configurations could produce similar sensor measurements. Our diffusion model addresses this information gap by generating plausible content that bridges the missing details.</p><p id=\"Par29\">The hallucinations arise from the diffusion model&#8217;s learned priors filling in the information that was fundamentally lost during the lensless acquisition process. This generative approach, while introducing content not directly captured by the sensor, demonstrates clear benefits in our evaluation. The numerical metrics (PSNR, SSIM, LPIPS) show that images with this generative content are closer to the ground truth compared to traditional reconstruction methods. Additionally, the perceptual quality is significantly enhanced, making the images more visually appealing and interpretable to human observers. The diffusion model&#8217;s strong natural image priors help reconstruct plausible high-frequency details and textures that would otherwise remain as artifacts or blur in conventional reconstruction approaches.</p><p id=\"Par30\">The details added by the model are crucial for producing a perceptually pleasing image that is closer to the true scene and for mitigating the inherent ambiguities of lensless imaging. However, this generated content, mainly in fine textures and structures, may not always faithfully represent the captured scene. This characteristic should be carefully considered in applications requiring high precision in fine details, such as microscopy or quantitative imaging tasks, where the distinction between measured and inferred content is critical.</p><p id=\"Par31\">The trade-off between reconstruction accuracy and the benefits of lensless imaging varies significantly across applications. In precision-critical domains like medical imaging, scientific microscopy, surveillance systems, or quality control systems, the generative nature of our approach may limit its applicability where pixel-level accuracy is paramount. However, in many practical scenarios, the significant size and weight reduction offered by flat cameras, combined with acceptable reconstruction quality, presents compelling advantages. For instance, in Internet of Things (IoT) applications, consumer photography, or mobile devices, the compact form factor and reduced manufacturing costs may outweigh minor inaccuracies in fine details. Similarly, for social media applications or augmented reality systems, the enhanced perceptual quality and visually appealing reconstructions may be more valuable than perfect pixel-level fidelity.</p><p id=\"Par32\">We present ablation results in Table&#160;<xref rid=\"Tab2\" ref-type=\"table\">2</xref> and Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref>. First, we observe that without our proposed separable loss, the reconstructed images are not similar to the target image. We observe that while the reconstructed images align well with the text captions, as indicated by the high CLIP score, they fail to incorporate additional information from the camera measurements during the reconstruction process. Namely, the reconstructions become independent of the input measurements. However, when applying the separable loss, the camera measurements are effectively utilized, ensuring that the reconstructed image is based on the input measurements.</p><p id=\"Par33\">Adding text information as input improves the reconstruction even further, compared to the non-text-guided model. The visual ablation images in Fig.&#160;<xref rid=\"Fig8\" ref-type=\"fig\">8</xref> demonstrate that the text captions contribute to the high-frequency details in the reconstructed images. When a text caption is provided, the reconstructed image details closely align with the caption&#8217;s description. This effect is also noticeable when an incorrect caption is given. For example, the reconstruction adopts a painting-like style when the caption mentions a painting, and elephant-like shapes emerge when elephants are described in the caption.</p></sec></sec><sec id=\"Sec8\"><title>Discussion</title><p id=\"Par34\">In this work, we introduced a novel method for reconstructing images from flat camera measurements, leveraging the strong prior capabilities of a pre-trained diffusion model. Our approach produces high-quality reconstructions both with and without text guidance, offering a significant step forward in lensless imaging. Although minor inaccuracies may appear in the finest details compared to ground truth images, these discrepancies arise from the inherently ill-posed nature of the problem, requiring our model to fill in lost details through its learned priors.</p><p id=\"Par35\">A key limitation of our amplitude-based approach is light efficiency. The binary mask blocks a significant portion of incoming light, which must be considered for practical applications. This trade-off between compactness and light gathering capability represents a fundamental challenge in amplitude-based lensless imaging. Future work could explore phase-based masks or metasurface designs that offer improved light efficiency, though at the cost of increased fabrication complexity.</p><p id=\"Par36\">Regarding computational efficiency, our diffusion-based approach requires iterative denoising steps, resulting in longer processing times compared to traditional methods. While Tikhonov reconstruction processes images in approximately 30 ms on CPU and FlatNet in 33 ms on GPU, our method requires approximately 907 ms due to the iterative nature of diffusion models. However, this computational cost enables substantial quality improvements, representing a trade-off between processing speed and reconstruction fidelity. Future work could explore acceleration techniques such as fewer diffusion steps, distilled models, or specialized hardware implementations to improve practical deployment while maintaining reconstruction quality.</p><p id=\"Par37\">By comparing our method against previous approaches, we demonstrated state-of-the-art performance and enhanced reconstruction quality. Beyond flat cameras, the principles underlying our method can be adapted to other imaging systems, highlighting the versatility of diffusion-based priors in computational photography. We envision that continued improvements in diffusion models and text guidance techniques will further enhance the fidelity of reconstructions in future work.</p></sec><sec id=\"Sec9\"><title>Methods</title><p id=\"Par38\">We turn to describe our <italic toggle=\"yes\">DifuzCam</italic> strategy. The flat camera we use is based on a similar implementation to previous works<sup><xref ref-type=\"bibr\" rid=\"CR1\">1</xref>,<xref ref-type=\"bibr\" rid=\"CR22\">22</xref></sup>. For the amplitude mask, we used a separable pattern (rank-1) obtained from the outer product of binaryized M-sequence signals of length 255. M-sequences offer favorable autocorrelation properties and relatively flat frequency response characteristics, which can improve reconstruction quality. The separable structure allows for efficient inverse algorithms that decompose the 2D reconstruction problem into two 1D operations, reducing computational complexity. We printed it by lithography onto a chrome plate on glass, with a thickness of 0.2 mm. Each feature in the pattern (i.e., a single pinhole) is of size 25 um. The binary amplitude mask inherently blocks a significant portion of the incoming light, as the opaque regions prevent light transmission to maintain the encoding pattern. This light efficiency limitation is a fundamental trade-off in amplitude-based lensless imaging, where spatial encoding requires sacrificing light throughput. As a design choice, we used a rank-1 separable mask, which is equivalent to prior works that used a rank-2 (non-separable) mask and achieved separability through mean subtraction (see equivalence in the Supplementary Information). Figure&#160;<xref rid=\"Fig3\" ref-type=\"fig\">3</xref> shows the mask pattern and the measured camera PSF achieved while the mask is attached to the sensor hot mirror.</p><p id=\"Par39\">The acquired image by a flat camera consists of multiplexed measurements of the light reflected from the scene across the sensor area. The light projections on the sensor create long-range correspondences in the captured images. To convert the image from the projections (i.e., &#8220;projection space&#8221;) to the pixel space of the target image, we apply a learned separable linear transformation to the measurements. This transformation is crucial since the input image serves as guidance for the diffusion model process, which was trained and works in the domain of natural images. The diffusion model is ignorant about the flat camera mask projections on the image, and the control network we use to guide the reconstruction process exhibits better results when operating in the image pixel domain rather than the long-range projections.</p><p id=\"Par40\">The RAW image captured by the camera (in RGGB Bayer pattern) is split into four color channels: <italic toggle=\"yes\">R</italic>, <inline-formula id=\"IEq20\"><tex-math id=\"d33e1060\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_r$$\\end{document}</tex-math></inline-formula>, <inline-formula id=\"IEq21\"><tex-math id=\"d33e1064\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$G_b$$\\end{document}</tex-math></inline-formula>, and <italic toggle=\"yes\">B</italic>. Each channel <inline-formula id=\"IEq22\"><tex-math id=\"d33e1071\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_k$$\\end{document}</tex-math></inline-formula> is linearly transformed as<disp-formula id=\"Equ3\"><label>3</label><tex-math id=\"d33e1076\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} C^o_k = \\phi _l^kC_k\\phi _r^k, \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq23\"><tex-math id=\"d33e1081\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$k \\in [R, G_r, G_b, B]$$\\end{document}</tex-math></inline-formula>, each color channel <inline-formula id=\"IEq24\"><tex-math id=\"d33e1085\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C_k$$\\end{document}</tex-math></inline-formula> of size <inline-formula id=\"IEq25\"><tex-math id=\"d33e1089\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$h_i\\times w_i$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq26\"><tex-math id=\"d33e1093\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi _l \\in \\mathbb {R}^{h_o\\times h_i}$$\\end{document}</tex-math></inline-formula> and <inline-formula id=\"IEq27\"><tex-math id=\"d33e1097\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\phi _r \\in \\mathbb {R}^{w_i\\times w_o}$$\\end{document}</tex-math></inline-formula> are two learnable matrices. The output features are stacked onto a single 4-channel tensor <inline-formula id=\"IEq28\"><tex-math id=\"d33e1102\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C^o \\in \\mathbb {R}^{4 \\times h_o\\times w_o}$$\\end{document}</tex-math></inline-formula>. The learnable matrices are initialized using a Gaussian distribution with a standard deviation of 0.001, as prior tests indicated that more complex initialization methods had no significant impact.</p><p id=\"Par41\">Since the flat camera image reconstruction is highly ill-posed, we utilized a pre-trained diffusion model as our image prior. It is a strong prior for natural images since it is trained on a huge number of samples for the image generation task.</p><p id=\"Par42\">To leverage the diffusion model for our task, we need to control its generation process such that we can generate the captured scene from the measurements of the flat camera. To do so, we use a ControlNet<sup><xref ref-type=\"bibr\" rid=\"CR6\">6</xref></sup> network <inline-formula id=\"IEq29\"><tex-math id=\"d33e1114\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {C_\\psi }$$\\end{document}</tex-math></inline-formula> which we train for our goal (as presented in Fig.&#160;<xref rid=\"Fig2\" ref-type=\"fig\">2</xref>). This network is initialized as a copy of the encoder of the diffusion model UNet with zero convolutions, such that the pre-trained weights&#8217; performance is not affected and during training, non-zero weights are learned for the reconstruction task.</p><p id=\"Par43\">The input of the control network is the output of the separable transform (<inline-formula id=\"IEq30\"><tex-math id=\"d33e1123\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C^o$$\\end{document}</tex-math></inline-formula>), and we use the control network loss<disp-formula id=\"Equ4\"><label>4</label><tex-math id=\"d33e1127\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} l_{\\mathcal {C}}=\\mathbb {E}_{\\mathcal {E}(x), \\epsilon \\sim N(0,\\textbf{I}), t \\sim U(0,T)}\\Bigl [ ||\\epsilon - \\epsilon _\\theta (z_t, t, y, \\mathcal {C_\\psi }(C^o)) ||_2 \\Bigr ], \\end{aligned}$$\\end{document}</tex-math></disp-formula>which is applied for both <inline-formula id=\"IEq31\"><tex-math id=\"d33e1132\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\psi$$\\end{document}</tex-math></inline-formula>, the set of parameters of the control network <inline-formula id=\"IEq32\"><tex-math id=\"d33e1136\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\mathcal {C_\\psi }$$\\end{document}</tex-math></inline-formula>, and the learned separable transform weights. The diffusion model parameters <inline-formula id=\"IEq33\"><tex-math id=\"d33e1140\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\theta$$\\end{document}</tex-math></inline-formula> are pre-trained and fixed.</p><p id=\"Par44\">To guide the training for better results, a separable reconstruction loss term is added to the conventional diffusion loss (Equation&#160;(<xref rid=\"Equ4\" ref-type=\"disp-formula\">4</xref>)). This loss is applied to the output of the separable transformation as<disp-formula id=\"Equ5\"><label>5</label><tex-math id=\"d33e1149\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\begin{aligned} l_{sep} = || I - f_{conv}(C^o) ||_2, \\end{aligned}$$\\end{document}</tex-math></disp-formula>where <inline-formula id=\"IEq34\"><tex-math id=\"d33e1154\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$f_{conv}$$\\end{document}</tex-math></inline-formula> is a learned <inline-formula id=\"IEq35\"><tex-math id=\"d33e1158\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$3\\times 3$$\\end{document}</tex-math></inline-formula> convolution layer which maps the 4-channel <inline-formula id=\"IEq36\"><tex-math id=\"d33e1162\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$C^o$$\\end{document}</tex-math></inline-formula> to a 3-channel image, and <italic toggle=\"yes\">I</italic> is the target (GT) image in RGB channels format. We present the improvement and significance of this term in the subsection Analysis and Ablations.</p><p id=\"Par45\">The diffusion model that we use is a text-guided model for image generation. We leverage this existing text-conditioning capability to optionally improve the image reconstruction process when scene descriptions are available from the photographer. For compact imaging systems like IoT devices or smartphone cameras, where users can easily provide contextual information, this additional prior knowledge helps the algorithm choose among multiple plausible reconstructions that are consistent with the sensor measurements<sup><xref ref-type=\"bibr\" rid=\"CR5\">5</xref></sup>. Thus, we employ this ability to improve the image reconstruction process by giving the model a text description of the captured scene. Giving additional information about the scene content enables the algorithm to have better prior knowledge of the resulting image and reconstruct better images. In this approach, the photographer describes the captured scene, and this information is input into the reconstruction algorithm. The contribution of the text to the results is presented in the subsection Analysis and Ablations.</p><sec id=\"Sec10\"><title>Data acquisition and training</title><p id=\"Par46\">As we train our DifuzCam model (separable transform and control network) in a supervised way, we need pairs of RGB images with their corresponding measurements of the flat camera. Simulating the flat camera imaging process to get realistic measurements such that the trained model will generalize to the real world is very hard since the captured measurements are very dependent on camera properties, alignment, calibration, and mask placement. Because of the small feature size (25 um), minor movement of the mask will result in totally different measurements. Due to this sensitivity of the system, we obtained real-world measurements using the optical setup that contains all the imaging properties, including space-variant PSF, diffraction, and non-ideal effects, which are hard to simulate, such as mask manufacturing inaccuracies, dust, image noise, etc.</p><p id=\"Par47\">To get a large dataset using our flat camera, we captured images from the LAION-aesthetics dataset<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup>, which were projected on a PC screen of size 34cm <inline-formula id=\"IEq37\"><tex-math id=\"d33e1186\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$\\times$$\\end{document}</tex-math></inline-formula> 34 cm at a distance of 60 cm from the camera. We used an exposure time of 12 ms and saved the raw Bayer pattern images in 12-bit depth. We used LAION-aesthetics<sup><xref ref-type=\"bibr\" rid=\"CR57\">57</xref></sup> as it consists of a large number of high-resolution images and their corresponding textual captions. In total, we captured about 55 k images. 500 images were saved for testing, while the rest were used for training. To compensate for stray light, a black screen with no image projected on it was captured. In the post-processing stage, the measured black levels were subtracted from the captured measurements.</p><p id=\"Par48\">In addition to the screen images, we also capture real objects to validate that we are not restricted only to &#8220;objects on screen&#8221;. In this case, we just show the qualitative reconstruction result as we do not have an exact RGB match as in the screen measurement case.</p><p id=\"Par49\">For the pre-trained diffusion model, we used stable-diffusion 2.1<sup><xref ref-type=\"bibr\" rid=\"CR61\">61</xref></sup>. We trained the model for 500k steps on the captured dataset using the losses in Eqs.&#160;(<xref rid=\"Equ4\" ref-type=\"disp-formula\">4</xref>) and (<xref rid=\"Equ2\" ref-type=\"disp-formula\">2</xref>) and a learning rate of <inline-formula id=\"IEq38\"><tex-math id=\"d33e1208\">\\documentclass[12pt]{minimal}\n\t\t\t\t\\usepackage{amsmath}\n\t\t\t\t\\usepackage{wasysym} \n\t\t\t\t\\usepackage{amsfonts} \n\t\t\t\t\\usepackage{amssymb} \n\t\t\t\t\\usepackage{amsbsy}\n\t\t\t\t\\usepackage{mathrsfs}\n\t\t\t\t\\usepackage{upgreek}\n\t\t\t\t\\setlength{\\oddsidemargin}{-69pt}\n\t\t\t\t\\begin{document}$$5\\times 10^{-5}$$\\end{document}</tex-math></inline-formula> with the AdamW optimizer.</p></sec></sec><sec id=\"Sec11\" sec-type=\"supplementary-material\"><title>Supplementary Information</title><p>\n<supplementary-material content-type=\"local-data\" id=\"MOESM1\" position=\"float\" orientation=\"portrait\"><media xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"41598_2025_27127_MOESM1_ESM.pdf\" position=\"float\" orientation=\"portrait\"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>\n</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-27127-1.</p></sec><notes notes-type=\"author-contribution\"><title>Author contributions</title><p>E.Y. developed the methodology, conducted the experiments, and collected the results. R.G. supervised the project, guided the research direction, and contributed to the analysis of the results. Both authors contributed to the writing and revision of the manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data availability</title><p>The data and code of this study will be made publicly available upon acceptance. For any inquiries regarding the data or code, please contact erez.yo@gmail.com.</p></notes><notes><title>Declarations</title><notes id=\"FPar1\" notes-type=\"COI-statement\"><title>Competing interests</title><p id=\"Par53\">The authors declare no competing interests.</p></notes></notes><ref-list id=\"Bib1\"><title>References</title><ref id=\"CR1\"><label>1.</label><mixed-citation publication-type=\"other\">Salman Asif, M., Ayremlou, A., Veeraraghavan, A., Baraniuk, R. &amp; Sankaranarayanan, A. Flatcam: Replacing lenses with masks and computation. In <italic toggle=\"yes\">Proceedings of the IEEE International Conference on Computer Vision Workshops</italic> 12&#8211;15 (2015).</mixed-citation></ref><ref id=\"CR2\"><label>2.</label><mixed-citation publication-type=\"other\">Khan, S.&#160;S. et al. Towards photorealistic reconstruction of highly multiplexed lensless images. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF International Conference on Computer Vision</italic> 7860&#8211;7869 (2019).</mixed-citation></ref><ref id=\"CR3\"><label>3.</label><mixed-citation publication-type=\"other\">Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B. High-resolution image synthesis with latent diffusion models. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 10684&#8211;10695 (2022).</mixed-citation></ref><ref id=\"CR4\"><label>4.</label><citation-alternatives><element-citation id=\"ec-CR4\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Croitoru</surname><given-names>F-A</given-names></name><name name-style=\"western\"><surname>Hondru</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Ionescu</surname><given-names>RT</given-names></name><name name-style=\"western\"><surname>Shah</surname><given-names>M</given-names></name></person-group><article-title>Diffusion models in vision: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>1</volume><fpage>1</fpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2023.3261988</pub-id><pub-id pub-id-type=\"pmid\">37030794</pub-id></element-citation><mixed-citation id=\"mc-CR4\" publication-type=\"journal\">Croitoru, F.-A., Hondru, V., Ionescu, R. T. &amp; Shah, M. Diffusion models in vision: A survey. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>1</bold>, 1 (2023).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2023.3261988</pub-id><pub-id pub-id-type=\"pmid\">37030794</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR5\"><label>5.</label><mixed-citation publication-type=\"other\">Yosef, E. &amp; Giryes, R. Tell me what you see: Text-guided real-world image denoising. Preprint at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2312.10191\">http://arxiv.org/abs/2312.10191</ext-link> (2023).</mixed-citation></ref><ref id=\"CR6\"><label>6.</label><mixed-citation publication-type=\"other\">Zhang, L., Rao, A. &amp; Agrawala, M. Adding conditional control to text-to-image diffusion models. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF International Conference on Computer Vision</italic> 3836&#8211;3847 (2023).</mixed-citation></ref><ref id=\"CR7\"><label>7.</label><mixed-citation publication-type=\"other\">Huang, G., Jiang, H., Matthews, K. &amp; Wilford, P. Lensless imaging by compressive sensing. In <italic toggle=\"yes\">2013 IEEE International Conference on Image Processing</italic> 2101&#8211;2105 (IEEE, 2013).</mixed-citation></ref><ref id=\"CR8\"><label>8.</label><mixed-citation publication-type=\"other\">DeWeert, M.&#160;J. &amp; Farm, B.&#160;P. Lensless coded aperture imaging with separable doubly toeplitz masks. In <italic toggle=\"yes\">Compressive Sensing III</italic>, vol. 9109, 180&#8211;191 (SPIE, 2014).</mixed-citation></ref><ref id=\"CR9\"><label>9.</label><citation-alternatives><element-citation id=\"ec-CR9\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shimano</surname><given-names>T</given-names></name><name name-style=\"western\"><surname>Nakamura</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Tajima</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Sao</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Hoshizawa</surname><given-names>T</given-names></name></person-group><article-title>Lensless light-field imaging with fresnel zone aperture: quasi-coherent coding</article-title><source>Appl. Opt.</source><year>2018</year><volume>57</volume><fpage>2841</fpage><lpage>2850</lpage><pub-id pub-id-type=\"doi\">10.1364/AO.57.002841</pub-id><pub-id pub-id-type=\"pmid\">29714287</pub-id></element-citation><mixed-citation id=\"mc-CR9\" publication-type=\"journal\">Shimano, T., Nakamura, Y., Tajima, K., Sao, M. &amp; Hoshizawa, T. Lensless light-field imaging with fresnel zone aperture: quasi-coherent coding. <italic toggle=\"yes\">Appl. Opt.</italic><bold>57</bold>, 2841&#8211;2850 (2018).<pub-id pub-id-type=\"pmid\">29714287</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/AO.57.002841</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR10\"><label>10.</label><mixed-citation publication-type=\"other\">Nakamura, Y., Shimano, T., Tajima, K., Sao, M. &amp; Hoshizawa, T. Lensless light-field imaging with fresnel zone aperture. In <italic toggle=\"yes\">ITE Technical Report 40.40 Information Sensing Technologies (IST)</italic> 7&#8211;8 (The Institute of Image Information and Television Engineers, 2016).</mixed-citation></ref><ref id=\"CR11\"><label>11.</label><citation-alternatives><element-citation id=\"ec-CR11\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Boominathan</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Adams</surname><given-names>JK</given-names></name><name name-style=\"western\"><surname>Robinson</surname><given-names>JT</given-names></name><name name-style=\"western\"><surname>Veeraraghavan</surname><given-names>A</given-names></name></person-group><article-title>Phlatcam: Designed phase-mask based thin lensless camera</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>42</volume><fpage>1618</fpage><lpage>1629</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2020.2987489</pub-id><pub-id pub-id-type=\"pmid\">32324539</pub-id><pub-id pub-id-type=\"pmcid\">PMC7439257</pub-id></element-citation><mixed-citation id=\"mc-CR11\" publication-type=\"journal\">Boominathan, V., Adams, J. K., Robinson, J. T. &amp; Veeraraghavan, A. Phlatcam: Designed phase-mask based thin lensless camera. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>42</bold>, 1618&#8211;1629 (2020).<pub-id pub-id-type=\"pmid\">32324539</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2020.2987489</pub-id><pub-id pub-id-type=\"pmcid\">PMC7439257</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR12\"><label>12.</label><citation-alternatives><element-citation id=\"ec-CR12\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Antipa</surname><given-names>N</given-names></name><etal/></person-group><article-title>Diffusercam: lensless single-exposure 3d imaging</article-title><source>Optica</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type=\"doi\">10.1364/OPTICA.5.000001</pub-id></element-citation><mixed-citation id=\"mc-CR12\" publication-type=\"journal\">Antipa, N. et al. Diffusercam: lensless single-exposure 3d imaging. <italic toggle=\"yes\">Optica</italic><bold>5</bold>, 1&#8211;9 (2018).</mixed-citation></citation-alternatives></ref><ref id=\"CR13\"><label>13.</label><mixed-citation publication-type=\"other\">Zomet, A. &amp; Nayar, S.&#160;K. Lensless imaging with a controllable aperture. In <italic toggle=\"yes\">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#8217;06)</italic>, vol.&#160;1, 339&#8211;346 (IEEE, 2006).</mixed-citation></ref><ref id=\"CR14\"><label>14.</label><citation-alternatives><element-citation id=\"ec-CR14\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Miller</surname><given-names>JR</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>C-Y</given-names></name><name name-style=\"western\"><surname>Keating</surname><given-names>CD</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Z</given-names></name></person-group><article-title>Particle-based reconfigurable scattering masks for lensless imaging</article-title><source>ACS Nano</source><year>2020</year><volume>14</volume><fpage>13038</fpage><lpage>13046</lpage><pub-id pub-id-type=\"doi\">10.1021/acsnano.0c04490</pub-id><pub-id pub-id-type=\"pmid\">32929968</pub-id></element-citation><mixed-citation id=\"mc-CR14\" publication-type=\"journal\">Miller, J. R., Wang, C.-Y., Keating, C. D. &amp; Liu, Z. Particle-based reconfigurable scattering masks for lensless imaging. <italic toggle=\"yes\">ACS Nano</italic><bold>14</bold>, 13038&#8211;13046 (2020).<pub-id pub-id-type=\"pmid\">32929968</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1021/acsnano.0c04490</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR15\"><label>15.</label><mixed-citation publication-type=\"other\">Zheng, Y., Hua, Y., Sankaranarayanan, A.&#160;C. &amp; Asif, M.&#160;S. A simple framework for 3d lensless imaging with programmable masks. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF International Conference on Computer Vision</italic> 2603&#8211;2612 (2021).</mixed-citation></ref><ref id=\"CR16\"><label>16.</label><citation-alternatives><element-citation id=\"ec-CR16\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Hua</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Nakamura</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Asif</surname><given-names>MS</given-names></name><name name-style=\"western\"><surname>Sankaranarayanan</surname><given-names>AC</given-names></name></person-group><article-title>Sweepcam depth-aware lensless imaging using programmable masks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>42</volume><fpage>1606</fpage><lpage>1617</lpage><pub-id pub-id-type=\"doi\">10.1109/TPAMI.2020.2986784</pub-id><pub-id pub-id-type=\"pmid\">32305898</pub-id></element-citation><mixed-citation id=\"mc-CR16\" publication-type=\"journal\">Hua, Y., Nakamura, S., Asif, M. S. &amp; Sankaranarayanan, A. C. Sweepcam depth-aware lensless imaging using programmable masks. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>42</bold>, 1606&#8211;1617 (2020).<pub-id pub-id-type=\"pmid\">32305898</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2020.2986784</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR17\"><label>17.</label><citation-alternatives><element-citation id=\"ec-CR17\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Boominathan</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Robinson</surname><given-names>JT</given-names></name><name name-style=\"western\"><surname>Waller</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Veeraraghavan</surname><given-names>A</given-names></name></person-group><article-title>Recent advances in lensless imaging</article-title><source>Optica</source><year>2022</year><volume>9</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type=\"doi\">10.1364/OPTICA.431361</pub-id><pub-id pub-id-type=\"pmid\">36338918</pub-id><pub-id pub-id-type=\"pmcid\">PMC9634619</pub-id></element-citation><mixed-citation id=\"mc-CR17\" publication-type=\"journal\">Boominathan, V., Robinson, J. T., Waller, L. &amp; Veeraraghavan, A. Recent advances in lensless imaging. <italic toggle=\"yes\">Optica</italic><bold>9</bold>, 1&#8211;16 (2022).<pub-id pub-id-type=\"pmid\">36338918</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/optica.431361</pub-id><pub-id pub-id-type=\"pmcid\">PMC9634619</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR18\"><label>18.</label><citation-alternatives><element-citation id=\"ec-CR18\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chen</surname><given-names>WT</given-names></name><etal/></person-group><article-title>Dispersion-engineered metasurfaces reaching broadband 90% relative diffraction efficiency</article-title><source>Nat. Commun.</source><year>2023</year><volume>14</volume><fpage>2544</fpage><pub-id pub-id-type=\"doi\">10.1038/s41467-023-38185-2</pub-id><pub-id pub-id-type=\"pmid\">37137885</pub-id><pub-id pub-id-type=\"pmcid\">PMC10156701</pub-id></element-citation><mixed-citation id=\"mc-CR18\" publication-type=\"journal\">Chen, W. T. et al. Dispersion-engineered metasurfaces reaching broadband 90% relative diffraction efficiency. <italic toggle=\"yes\">Nat. Commun.</italic><bold>14</bold>, 2544 (2023).<pub-id pub-id-type=\"pmid\">37137885</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41467-023-38185-2</pub-id><pub-id pub-id-type=\"pmcid\">PMC10156701</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR19\"><label>19.</label><citation-alternatives><element-citation id=\"ec-CR19\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tseng</surname><given-names>E</given-names></name><etal/></person-group><article-title>Neural nano-optics for high-quality thin lens imaging</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>6493</fpage><pub-id pub-id-type=\"doi\">10.1038/s41467-021-26443-0</pub-id><pub-id pub-id-type=\"pmid\">34845201</pub-id><pub-id pub-id-type=\"pmcid\">PMC8630181</pub-id></element-citation><mixed-citation id=\"mc-CR19\" publication-type=\"journal\">Tseng, E. et al. Neural nano-optics for high-quality thin lens imaging. <italic toggle=\"yes\">Nat. Commun.</italic><bold>12</bold>, 6493 (2021).<pub-id pub-id-type=\"pmid\">34845201</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1038/s41467-021-26443-0</pub-id><pub-id pub-id-type=\"pmcid\">PMC8630181</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR20\"><label>20.</label><citation-alternatives><element-citation id=\"ec-CR20\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Peng</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Learned large field-of-view imaging with thin-plate optics</article-title><source>ACM Trans. Graph.</source><year>2019</year><volume>38</volume><fpage>219</fpage><pub-id pub-id-type=\"doi\">10.1145/3355089.3356526</pub-id></element-citation><mixed-citation id=\"mc-CR20\" publication-type=\"journal\">Peng, Y. et al. Learned large field-of-view imaging with thin-plate optics. <italic toggle=\"yes\">ACM Trans. Graph.</italic><bold>38</bold>, 219 (2019).</mixed-citation></citation-alternatives></ref><ref id=\"CR21\"><label>21.</label><mixed-citation publication-type=\"other\">Radford, A. et al. Learning transferable visual models from natural language supervision. In <italic toggle=\"yes\">International Conference on Machine Learning</italic> 8748&#8211;8763 (PMLR, 2021).</mixed-citation></ref><ref id=\"CR22\"><label>22.</label><citation-alternatives><element-citation id=\"ec-CR22\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Khan</surname><given-names>SS</given-names></name><name name-style=\"western\"><surname>Sundar</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Boominathan</surname><given-names>V</given-names></name><name name-style=\"western\"><surname>Veeraraghavan</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Mitra</surname><given-names>K</given-names></name></person-group><article-title>Flatnet: Towards photorealistic scene reconstruction from lensless measurements</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>44</volume><fpage>1934</fpage><lpage>1948</lpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2020.3033882</pub-id><pub-id pub-id-type=\"pmcid\">PMC8979921</pub-id><pub-id pub-id-type=\"pmid\">33104508</pub-id></element-citation><mixed-citation id=\"mc-CR22\" publication-type=\"journal\">Khan, S. S., Sundar, V., Boominathan, V., Veeraraghavan, A. &amp; Mitra, K. Flatnet: Towards photorealistic scene reconstruction from lensless measurements. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>44</bold>, 1934&#8211;1948 (2020).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2020.3033882</pub-id><pub-id pub-id-type=\"pmcid\">PMC8979921</pub-id><pub-id pub-id-type=\"pmid\">33104508</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR23\"><label>23.</label><mixed-citation publication-type=\"other\">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In <italic toggle=\"yes\">Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015: 18th International Conference, Munich, Germany, October 5&#8211;9, 2015, Proceedings, Part III 18</italic> 234&#8211;241 (Springer, 2015).</mixed-citation></ref><ref id=\"CR24\"><label>24.</label><mixed-citation publication-type=\"other\">Rego, J.&#160;D., Kulkarni, K. &amp; Jayasuriya, S. Robust lensless image reconstruction via psf estimation. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 403&#8211;412 (2021).</mixed-citation></ref><ref id=\"CR25\"><label>25.</label><citation-alternatives><element-citation id=\"ec-CR25\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Cao</surname><given-names>L</given-names></name><name name-style=\"western\"><surname>Barbastathis</surname><given-names>G</given-names></name></person-group><article-title>Dnn-fza camera: a deep learning approach toward broadband fza lensless imaging</article-title><source>Opt. Lett.</source><year>2021</year><volume>46</volume><fpage>130</fpage><lpage>133</lpage><pub-id pub-id-type=\"doi\">10.1364/OL.411228</pub-id><pub-id pub-id-type=\"pmid\">33362033</pub-id></element-citation><mixed-citation id=\"mc-CR25\" publication-type=\"journal\">Wu, J., Cao, L. &amp; Barbastathis, G. Dnn-fza camera: a deep learning approach toward broadband fza lensless imaging. <italic toggle=\"yes\">Opt. Lett.</italic><bold>46</bold>, 130&#8211;133 (2021).<pub-id pub-id-type=\"pmid\">33362033</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/OL.411228</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR26\"><label>26.</label><mixed-citation publication-type=\"other\">Haris, M., Shakhnarovich, G. &amp; Ukita, N. Deep back-projection networks for super-resolution. In <italic toggle=\"yes\">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> 1664&#8211;1673 (2018).</mixed-citation></ref><ref id=\"CR27\"><label>27.</label><citation-alternatives><element-citation id=\"ec-CR27\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kingshott</surname><given-names>O</given-names></name><name name-style=\"western\"><surname>Antipa</surname><given-names>N</given-names></name><name name-style=\"western\"><surname>Bostan</surname><given-names>E</given-names></name><name name-style=\"western\"><surname>Ak&#351;it</surname><given-names>K</given-names></name></person-group><article-title>Unrolled primal-dual networks for lensless cameras</article-title><source>Opt. Express</source><year>2022</year><volume>30</volume><fpage>46324</fpage><lpage>46335</lpage><pub-id pub-id-type=\"doi\">10.1364/OE.475521</pub-id><pub-id pub-id-type=\"pmid\">36558589</pub-id></element-citation><mixed-citation id=\"mc-CR27\" publication-type=\"journal\">Kingshott, O., Antipa, N., Bostan, E. &amp; Ak&#351;it, K. Unrolled primal-dual networks for lensless cameras. <italic toggle=\"yes\">Opt. Express</italic><bold>30</bold>, 46324&#8211;46335 (2022).<pub-id pub-id-type=\"pmid\">36558589</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/OE.475521</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR28\"><label>28.</label><citation-alternatives><element-citation id=\"ec-CR28\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>K</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Y</given-names></name><name name-style=\"western\"><surname>Rao</surname><given-names>C</given-names></name></person-group><article-title>Mwdns: reconstruction in multi-scale feature spaces for lensless imaging</article-title><source>Opt. Express</source><year>2023</year><volume>31</volume><fpage>39088</fpage><lpage>39101</lpage><pub-id pub-id-type=\"doi\">10.1364/OE.501970</pub-id><pub-id pub-id-type=\"pmid\">38017997</pub-id></element-citation><mixed-citation id=\"mc-CR28\" publication-type=\"journal\">Li, Y., Li, Z., Chen, K., Guo, Y. &amp; Rao, C. Mwdns: reconstruction in multi-scale feature spaces for lensless imaging. <italic toggle=\"yes\">Opt. Express</italic><bold>31</bold>, 39088&#8211;39101 (2023).<pub-id pub-id-type=\"pmid\">38017997</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/OE.501970</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR29\"><label>29.</label><citation-alternatives><element-citation id=\"ec-CR29\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Pan</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X</given-names></name><name name-style=\"western\"><surname>Takeyama</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Yamaguchi</surname><given-names>M</given-names></name></person-group><article-title>Image reconstruction with transformer for mask-based lensless imaging</article-title><source>Opt. Lett.</source><year>2022</year><volume>47</volume><fpage>1843</fpage><lpage>1846</lpage><pub-id pub-id-type=\"doi\">10.1364/OL.455378</pub-id><pub-id pub-id-type=\"pmid\">35363750</pub-id></element-citation><mixed-citation id=\"mc-CR29\" publication-type=\"journal\">Pan, X., Chen, X., Takeyama, S. &amp; Yamaguchi, M. Image reconstruction with transformer for mask-based lensless imaging. <italic toggle=\"yes\">Opt. Lett.</italic><bold>47</bold>, 1843&#8211;1846 (2022).<pub-id pub-id-type=\"pmid\">35363750</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1364/OL.455378</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR30\"><label>30.</label><mixed-citation publication-type=\"other\">Liu, M., Su, X., Yao, X., Hao, W. &amp; Zhu, W. Lensless image restoration based on multi-stage deep neural networks and pix2pix architecture. In <italic toggle=\"yes\">Photonics</italic>, vol.&#160;10, 1274 (MDPI, 2023).</mixed-citation></ref><ref id=\"CR31\"><label>31.</label><citation-alternatives><element-citation id=\"ec-CR31\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ho</surname><given-names>J</given-names></name><name name-style=\"western\"><surname>Jain</surname><given-names>A</given-names></name><name name-style=\"western\"><surname>Abbeel</surname><given-names>P</given-names></name></person-group><article-title>Denoising diffusion probabilistic models</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation><mixed-citation id=\"mc-CR31\" publication-type=\"journal\">Ho, J., Jain, A. &amp; Abbeel, P. Denoising diffusion probabilistic models. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>33</bold>, 6840&#8211;6851 (2020).</mixed-citation></citation-alternatives></ref><ref id=\"CR32\"><label>32.</label><mixed-citation publication-type=\"other\">Nichol, A.&#160;Q. &amp; Dhariwal, P. Improved denoising diffusion probabilistic models. In <italic toggle=\"yes\">International Conference on Machine Learning</italic> 8162&#8211;8171 (PMLR, 2021).</mixed-citation></ref><ref id=\"CR33\"><label>33.</label><citation-alternatives><element-citation id=\"ec-CR33\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Nichol</surname><given-names>AQ</given-names></name><etal/></person-group><article-title>GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models</article-title><source>ICML</source><year>2022</year><volume>162</volume><fpage>16784</fpage><lpage>16804</lpage></element-citation><mixed-citation id=\"mc-CR33\" publication-type=\"journal\">Nichol, A. Q. et al. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. <italic toggle=\"yes\">ICML</italic><bold>162</bold>, 16784&#8211;16804 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR34\"><label>34.</label><citation-alternatives><element-citation id=\"ec-CR34\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Dhariwal</surname><given-names>P</given-names></name><name name-style=\"western\"><surname>Nichol</surname><given-names>A</given-names></name></person-group><article-title>Diffusion models beat gans on image synthesis</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>8780</fpage><lpage>8794</lpage></element-citation><mixed-citation id=\"mc-CR34\" publication-type=\"journal\">Dhariwal, P. &amp; Nichol, A. Diffusion models beat gans on image synthesis. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>34</bold>, 8780&#8211;8794 (2021).</mixed-citation></citation-alternatives></ref><ref id=\"CR35\"><label>35.</label><citation-alternatives><element-citation id=\"ec-CR35\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Abu-Hussein</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Giryes</surname><given-names>R</given-names></name></person-group><article-title>Udpm: Upsampling diffusion probabilistic models</article-title><source>NeuRIPS</source><year>2024</year><volume>37</volume><fpage>27616</fpage><lpage>27646</lpage></element-citation><mixed-citation id=\"mc-CR35\" publication-type=\"journal\">Abu-Hussein, S. &amp; Giryes, R. Udpm: Upsampling diffusion probabilistic models. <italic toggle=\"yes\">NeuRIPS</italic><bold>37</bold>, 27616&#8211;27646 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR36\"><label>36.</label><mixed-citation publication-type=\"other\">Amit, T., Shaharbany, T., Nachmani, E. &amp; Wolf, L. Segdiff: Image segmentation with diffusion probabilistic models. Preprint at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2112.00390\">http://arxiv.org/abs/2112.00390</ext-link> (2021).</mixed-citation></ref><ref id=\"CR37\"><label>37.</label><mixed-citation publication-type=\"other\">Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Label-efficient semantic segmentation with diffusion models. In <italic toggle=\"yes\">ICLR</italic> (2022).</mixed-citation></ref><ref id=\"CR38\"><label>38.</label><mixed-citation publication-type=\"other\">Lugmayr, A. et al. Repaint: Inpainting using denoising diffusion probabilistic models. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 11461&#8211;11471 (2022).</mixed-citation></ref><ref id=\"CR39\"><label>39.</label><citation-alternatives><element-citation id=\"ec-CR39\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saharia</surname><given-names>C</given-names></name><etal/></person-group><article-title>Image super-resolution via iterative refinement</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>4713</fpage><lpage>4726</lpage><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2022.3204461</pub-id><pub-id pub-id-type=\"pmid\">36094974</pub-id></element-citation><mixed-citation id=\"mc-CR39\" publication-type=\"journal\">Saharia, C. et al. Image super-resolution via iterative refinement. <italic toggle=\"yes\">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>45</bold>, 4713&#8211;4726 (2022).<pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/TPAMI.2022.3204461</pub-id><pub-id pub-id-type=\"pmid\">36094974</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR40\"><label>40.</label><citation-alternatives><element-citation id=\"ec-CR40\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kawar</surname><given-names>B</given-names></name><name name-style=\"western\"><surname>Elad</surname><given-names>M</given-names></name><name name-style=\"western\"><surname>Ermon</surname><given-names>S</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>J</given-names></name></person-group><article-title>Denoising diffusion restoration models</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>23593</fpage><lpage>23606</lpage></element-citation><mixed-citation id=\"mc-CR40\" publication-type=\"journal\">Kawar, B., Elad, M., Ermon, S. &amp; Song, J. Denoising diffusion restoration models. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>35</bold>, 23593&#8211;23606 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR41\"><label>41.</label><mixed-citation publication-type=\"other\">Saharia, C. et al. Palette: Image-to-image diffusion models. In <italic toggle=\"yes\">ACM SIGGRAPH 2022 Conference Proceedings</italic> 1&#8211;10 (2022).</mixed-citation></ref><ref id=\"CR42\"><label>42.</label><mixed-citation publication-type=\"other\">Batzolis, G., Stanczuk, J., Sch&#246;nlieb, C.-B. &amp; Etmann, C. Conditional image generation with score-based diffusion models. Preprint at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2111.13606\">http://arxiv.org/abs/2111.13606</ext-link> (2021).</mixed-citation></ref><ref id=\"CR43\"><label>43.</label><mixed-citation publication-type=\"other\">Abu-Hussein, S., Tirer, T. &amp; Giryes, R. Adir: Adaptive diffusion for image reconstruction. Preprint at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2212.03221\">http://arxiv.org/abs/2212.03221</ext-link> (2022).</mixed-citation></ref><ref id=\"CR44\"><label>44.</label><mixed-citation publication-type=\"other\">Zhu, Y. et al. Denoising diffusion models for plug-and-play image restoration. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 1219&#8211;1229 (2023).</mixed-citation></ref><ref id=\"CR45\"><label>45.</label><mixed-citation publication-type=\"other\">Fei, B. et al. Generative diffusion prior for unified image restoration and enhancement. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 9935&#8211;9946 (2023).</mixed-citation></ref><ref id=\"CR46\"><label>46.</label><mixed-citation publication-type=\"other\">Chung, H., Ye, J.&#160;C., Milanfar, P. &amp; Delbracio, M. Prompt-tuning latent diffusion models for inverse problems. In <italic toggle=\"yes\">ICML</italic> (2024).</mixed-citation></ref><ref id=\"CR47\"><label>47.</label><mixed-citation publication-type=\"other\">Delbracio, M. &amp; Milanfar, P. An alternative to denoising diffusion for image restoration. In <italic toggle=\"yes\">TMLR, Inversion by Direct Iteration</italic> (2023).</mixed-citation></ref><ref id=\"CR48\"><label>48.</label><mixed-citation publication-type=\"other\">Pearl, N. et al. SVNR: Spatially-variant noise removal with denoising diffusion. Preprint at <ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"http://arxiv.org/abs/2306.16052\">http://arxiv.org/abs/2306.16052</ext-link> (2023).</mixed-citation></ref><ref id=\"CR49\"><label>49.</label><mixed-citation publication-type=\"other\">Yi, X., Xu, H., Zhang, H., Tang, L. &amp; Ma, J. Diff-retinex: Rethinking low-light image enhancement with a generative diffusion model. In <italic toggle=\"yes\">IEEE/CVF International Conference on Computer Vision</italic> 12302&#8211;12311 (2023).</mixed-citation></ref><ref id=\"CR50\"><label>50.</label><mixed-citation publication-type=\"other\">Torem, N., Ronen, R., Schechner, Y.&#160;Y. &amp; Elad, M. Complex-valued retrievals from noisy images using diffusion models. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF International Conference on Computer Vision</italic> 3810&#8211;3820 (2023).</mixed-citation></ref><ref id=\"CR51\"><label>51.</label><mixed-citation publication-type=\"other\">Nguyen, C.&#160;M., Chan, E.&#160;R., Bergman, A.&#160;W. &amp; Wetzstein, G. <italic toggle=\"yes\">Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition</italic> (2024).</mixed-citation></ref><ref id=\"CR52\"><label>52.</label><mixed-citation publication-type=\"other\">Kim, J., Park, G.&#160;Y., Chung, H. &amp; Ye, J.&#160;C. Regularization by texts for latent diffusion inverse solvers. In <italic toggle=\"yes\">ICLR</italic> (2025).</mixed-citation></ref><ref id=\"CR53\"><label>53.</label><mixed-citation publication-type=\"other\">Qi, C. et al. SPIRE: Semantic prompt-driven image restoration. In <italic toggle=\"yes\">ECCV</italic> (2024).</mixed-citation></ref><ref id=\"CR54\"><label>54.</label><citation-alternatives><element-citation id=\"ec-CR54\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Cai</surname><given-names>X</given-names></name><etal/></person-group><article-title>Phocolens: Photorealistic and consistent reconstruction in lensless imaging</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2024</year><volume>37</volume><fpage>12219</fpage><lpage>12242</lpage><pub-id pub-id-type=\"doi\">10.52202/079017-0391</pub-id></element-citation><mixed-citation id=\"mc-CR54\" publication-type=\"journal\">Cai, X. et al. Phocolens: Photorealistic and consistent reconstruction in lensless imaging. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>37</bold>, 12219&#8211;12242 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR55\"><label>55.</label><citation-alternatives><element-citation id=\"ec-CR55\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Mou</surname><given-names>C</given-names></name><etal/></person-group><article-title>T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2024</year><volume>38</volume><fpage>4296</fpage><lpage>4304</lpage></element-citation><mixed-citation id=\"mc-CR55\" publication-type=\"journal\">Mou, C. et al. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. <italic toggle=\"yes\">Proc. AAAI Conf. Artif. Intell.</italic><bold>38</bold>, 4296&#8211;4304 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR56\"><label>56.</label><citation-alternatives><element-citation id=\"ec-CR56\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Duan</surname><given-names>X</given-names></name><etal/></person-group><article-title>Tuning-free inversion-enhanced control for consistent image editing</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2024</year><volume>38</volume><fpage>1644</fpage><lpage>1652</lpage></element-citation><mixed-citation id=\"mc-CR56\" publication-type=\"journal\">Duan, X. et al. Tuning-free inversion-enhanced control for consistent image editing. <italic toggle=\"yes\">Proc. AAAI Conf. Artif. Intell.</italic><bold>38</bold>, 1644&#8211;1652 (2024).</mixed-citation></citation-alternatives></ref><ref id=\"CR57\"><label>57.</label><citation-alternatives><element-citation id=\"ec-CR57\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Schuhmann</surname><given-names>C</given-names></name><etal/></person-group><article-title>Laion-5b: An open large-scale dataset for training next generation image-text models</article-title><source>Adv. Neural. Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>25278</fpage><lpage>25294</lpage></element-citation><mixed-citation id=\"mc-CR57\" publication-type=\"journal\">Schuhmann, C. et al. Laion-5b: An open large-scale dataset for training next generation image-text models. <italic toggle=\"yes\">Adv. Neural. Inf. Process. Syst.</italic><bold>35</bold>, 25278&#8211;25294 (2022).</mixed-citation></citation-alternatives></ref><ref id=\"CR58\"><label>58.</label><citation-alternatives><element-citation id=\"ec-CR58\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>Z</given-names></name><name name-style=\"western\"><surname>Bovik</surname><given-names>AC</given-names></name><name name-style=\"western\"><surname>Sheikh</surname><given-names>HR</given-names></name><name name-style=\"western\"><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Image quality assessment: from error visibility to structural similarity</article-title><source>IEEE Trans. Image Process.</source><year>2004</year><volume>13</volume><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type=\"doi\">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type=\"pmid\">15376593</pub-id></element-citation><mixed-citation id=\"mc-CR58\" publication-type=\"journal\">Wang, Z., Bovik, A. C., Sheikh, H. R. &amp; Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. <italic toggle=\"yes\">IEEE Trans. Image Process.</italic><bold>13</bold>, 600&#8211;612 (2004).<pub-id pub-id-type=\"pmid\">15376593</pub-id><pub-id pub-id-type=\"doi\" assigning-authority=\"pmc\">10.1109/tip.2003.819861</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR59\"><label>59.</label><mixed-citation publication-type=\"other\">Zhang, R., Isola, P., Efros, A.&#160;A., Shechtman, E. &amp; Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In <italic toggle=\"yes\">CVPR</italic> (2018).</mixed-citation></ref><ref id=\"CR60\"><label>60.</label><citation-alternatives><element-citation id=\"ec-CR60\" publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liu</surname><given-names>H</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>C</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Q</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>YJ</given-names></name></person-group><article-title>Visual instruction tuning</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2024</year><volume>36</volume><fpage>1</fpage><pub-id pub-id-type=\"pmcid\">PMC11867732</pub-id><pub-id pub-id-type=\"pmid\">40017809</pub-id></element-citation><mixed-citation id=\"mc-CR60\" publication-type=\"journal\">Liu, H., Li, C., Wu, Q. &amp; Lee, Y. J. Visual instruction tuning. <italic toggle=\"yes\">Adv. Neural Inf. Process. Syst.</italic><bold>36</bold>, 1 (2024).<pub-id pub-id-type=\"pmcid\">PMC11867732</pub-id><pub-id pub-id-type=\"pmid\">40017809</pub-id></mixed-citation></citation-alternatives></ref><ref id=\"CR61\"><label>61.</label><mixed-citation publication-type=\"other\">Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B. High-resolution image synthesis with latent diffusion models. In <italic toggle=\"yes\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 10684&#8211;10695 (2022).</mixed-citation></ref></ref-list></back></article></pmc-articleset>",
  "text": "pmc Sci Rep Sci Rep 1579 scirep Scientific Reports 2045-2322 Nature Publishing Group PMC12675501 PMC12675501.1 12675501 12675501 41339401 10.1038/s41598-025-27127-1 27127 1 Article DifuzCam replacing camera lens with a mask and a diffusion model for generative AI based flat camera design Yosef Erez Erez.yo@gmail.com Giryes Raja https://ror.org/04mhzgx49 grid.12136.37 0000 0004 1937 0546 Tel Aviv University, Tel Aviv, Israel 3 12 2025 2025 15 478255 43059 3 6 2025 31 10 2025 03 12 2025 05 12 2025 05 12 2025 &#169; The Author(s) 2025 2025 https://creativecommons.org/licenses/by-nc-nd/4.0/ Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ . Recent advances in lensless, flat camera designs hold the promise of significantly reducing size and weight by replacing bulky lenses with thin optical elements that modulate incoming light. However, recovering high-quality images from the raw sensor measurements of such systems remains challenging. We address this limitation by introducing a novel reconstruction framework that leverages a pre-trained diffusion model, guided by a control network and a learnable separable transformation. This approach delivers high-fidelity images, achieving state-of-the-art performance in both objective and perceptual metrics. Our method achieves 20.43 PSNR, 0.612 SSIM, and 0.237 LPIPS on the FlatNet dataset, representing improvements of 9.6%, 18.1%, and 26.4% respectively over the previous state-of-the-art FlatNet method. Additionally, the text-conditioned nature of the diffusion model enables optional enhancement through scene descriptions, particularly valuable for compact imaging systems where user input can help resolve reconstruction ambiguities. We demonstrate the effectiveness of our method on a 8 flat camera, paving the way for advanced lensless imaging solutions and offering a robust framework for improved reconstructions that is relevant to a broad range of computational imaging systems. Keywords Computational photography Deep learning Lensless imaging Artificial intelligence Image reconstruction Neural networks Subject terms Electrical and electronic engineering Computer science Imaging and sensing https://doi.org/10.13039/501100000781 European Research Council 101113391 101113391 pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement yes pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes issue-copyright-statement &#169; Springer Nature Limited 2025 Introduction Cameras have become very popular and common in recent years, especially in small handheld devices. Nevertheless, reducing the size of the camera remains a difficult problem since a camera requires lenses and optical elements to get a high-quality image. Flat cameras 1 are a computational photography method to reduce camera size by replacing the camera lens with a diffuser, namely, an amplitude mask placed very close to the sensor. Thus, the sensor captures multiplexed projections of scene reflections across its entire area, resulting in an image that is not visually understandable. Using a computational algorithm, the scene image can be retrieved. Yet, while achieving massive camera size reduction, reconstructing high-quality images from flat camera measurements is an ill-posed task and hard to achieve. Previous approaches tried to reconstruct the scene image using different techniques, including direct optimization 1 and deep learning 2 . Despite these attempts, the resulting images are not of sufficient quality. High-quality image reconstruction from flat camera measurements remains an open challenge, and better algorithms are required. Fig. 1 Using ( a ) our prototype flat camera, ( b ) a measurement image is captured that is not visually understandable. ( c ) An image is reconstructed from the measurements using our text-guided approach (DifuzCam), compared to ( d ) the reference image captured with a regular camera. (see Fig.&#160; 6 for details). To improve the quality, we propose DifuzCam , a novel strategy for flat camera image reconstruction using a strong image prior that relies on a pre-trained diffusion model 3 , 4 . Figure&#160; 2 presents an overview diagram of DifuzCam . Using a pre-trained generative model trained on many images provides a strong prior for natural images, which facilitates reconstructing high-quality images from flat camera measurements (Fig.&#160; 1 ). Moreover, we leverage the inherent text-conditioning capability of the diffusion model to optionally incorporate scene descriptions from the photographer. This additional information can help resolve ambiguities in the highly ill-posed reconstruction problem 5 , particularly valuable for compact camera systems where traditional optical cues are limited. Fig. 2 DifuzCam proposed method. A flat lensless camera measurements are inputted to a separable linear transformation followed by a ControlNet adapter to control the image generation process by a pre-trained latent diffusion model (LDM). Using text guidance for the reconstruction process is optional in our method. On training, the yellow blocks weights are optimized, and orange paths are the training losses, while the blue blocks weights are pre-trained and fixed. The dataset for training and testing was captured using our prototype flat camera by projecting images onto a screen. We present an additional loss \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$l_{sep}$$\\end{document} in addition to the diffusion training loss ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$l_{\\mathcal {C}}$$\\end{document} ) for better convergence. The reconstructed image is achieved by T iterative diffusion steps and decoding from the latent space to the pixel space. We used a pretrained and fixed stable diffusion 3 to leverage its strong image prior, and trained a ControlNet 6 to guide the reconstruction using the sensor measurements. We built a flat camera prototype (Fig.&#160; 1 ) and captured a large-scale dataset for training. We present qualitative and quantitative evaluations of DifuzCam using both our camera prototype and an existing dataset from prior work. Our method improves reconstruction compared to prior art and thus advances the practicality of flat cameras. Our strategy is generic, combining established diffusion model techniques with lensless imaging, with the potential to be adapted to other imaging systems and setups, leveraging the diffusion model&#8217;s strong prior and text guidance capability to improve reconstructed image quality. Related work Lensless imaging has garnered significant attention in recent years since it offers a more compact, lightweight, and cost-effective imaging system. Various lensless camera designs were proposed, such as static amplitude mask 1 , modulation with LCD 7 , Fresnel zone aperture (FZA) with SLM 8 &#8211; 10 , phase mask 11 , 12 , programmable devices 13 &#8211; 16 , and more as described in Ref. 17 . Recent advances in metasurface-based imaging 18 have achieved high diffraction efficiency with improved light throughput. Neural nano-optics approaches 19 combine engineered optical elements with deep learning for compact imaging. Learned thin-plate optics 20 jointly optimize optical design and reconstruction algorithms. While these approaches offer advantages in light efficiency, they require more complex fabrication compared to our lithographic amplitude masks. We build upon the FlatCam design 1 . It employs a static amplitude mask placed near the sensor. The mask pattern is designed in a separable manner such that the imaging linear model can be simplified to a separable operation: 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} Y = \\Phi _lX\\Phi _r, \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\Phi _L$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\Phi _R$$\\end{document} are the separable operations on the image&#8217;s rows and columns, respectively. X is the scene intensity and Y is the sensor measurements. In this case, the camera&#8217;s measurements consist of multiplexed projections of the scene reflectance such that reconstruction becomes an ill-posed problem. Prior methods for image reconstruction from flat camera measurements were designed as an optimization problem 1 , 15 , 16 . Such model-based methods are heavily dependent on the imaging model and rely on accurate calibration. Thus, they were very limited and the reconstruction quality was low. To overcome some of these limitations, data-driven algorithms were proposed using deep learning to reconstruct high-quality images from multiplexed measurements. For example, deep learning was used to optimize the alternating direction method of multipliers (ADMM) parameters and reconstruct the image 21 . FlatNet 2 , 22 used a learned separable transform followed by a Unet 23 architecture, which perceptually enhances the resulting image in a generative adversarial network (GAN) approach. A GAN is also presented for lensless point spread function (PSF) estimation and robust reconstruction 24 . In 25 , a Unet architecture followed by a deep back-projection network (DBPN) 26 was proposed for FZA lensless camera image reconstruction. Unrolled primal-dual networks 27 present an alternative approach by unrolling iterative optimization algorithms into learnable network architectures, potentially offering improved reconstruction quality through learned optimization steps. To address the mismatch between the ideal imaging model and the real-world model, the work in 28 presented multi-Wiener deconvolution networks in multi-scale feature spaces. A transformer architecture 29 and a multi-level image restoration with the pix2pix generative network 30 were employed to further improve reconstruction. Diffusion models References 4 , 31 , 32 are generative models that have an impressive ability to learn natural image distributions and are used for image generation 3 , 33 &#8211; 35 , segmentation 36 , 37 , inpainting 38 , image super-resolution 39 , and image reconstruction 4 , 40 &#8211; 45 . The diffusion model is a parameterized Markov chain that produces samples of a certain data distribution after a predefined T number of steps. In each step of the forward diffusion process, a Gaussian noise is added to the data, such that after a large number of steps, the sample is mapped to an isotropic Gaussian. In the reverse step, a neural network is trained to denoise the clean image from the noisy sample. To sample an image, we apply diffusion steps in the reverse direction, starting with pure Gaussian noise and denoising it until getting a clean natural image from the learned distribution. In the context of low-level image restoration, diffusion models were used for image restoration of linear inverse problems 40 , 46 , 47 , spatially-variant noise removal 48 , and low-light image enhancement 49 , 50 . They were also presented for a low-light text recognition task 51 . The use of text guidance for conventional imaging reconstruction and enhancement was recently proposed 5 , 52 , 53 . Concurrent with our work, 54 explored the use of diffusion models for lensless imaging. Our approach differs in several key aspects. First, we use the Flatnet 2 camera design and dataset for comparisons, whereas their work evaluates different designs 11 , 12 , resulting in distinct imaging conditions and reconstruction challenges. Second, beyond simulation-based results, we developed a working prototype of our flat lensless camera and captured a large-scale dataset to support future research in this field. Third, we introduce text guidance to enhance reconstruction quality, addressing the ill-posed nature of the problem by leveraging state-of-the-art multimodal artificial intelligence (AI) models. These differences highlight the novelty of our method and its contribution to the field. For non-conventional imaging, such as a flat camera, adopting diffusion models for the recovery process is not trivial, and additional adjustments and considerations are required. This work bridges this gap and presents a novel approach to integrating diffusion models for this imaging task. Latent diffusion model (LDM) 3 is a method for applying the diffusion process in a latent space instead of the pixel space to get a more computationally efficient model for high-resolution images. In this approach, a pre-trained autoencoder consisting of encoder \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathcal {E}$$\\end{document} and decoder \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathcal {D}$$\\end{document} is used to train a diffusion model in a low-dimensional (latent) space. The data samples are denoted as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$x \\sim q(x)$$\\end{document} , where q is the data distribution we learn, and the latent sample is obtained by the encoder as \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$z = \\mathcal {E}(x)$$\\end{document} . A denoiser network \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\epsilon _\\theta$$\\end{document} is trained to predict the added noise (at the backward diffusion step) from the noisy sample \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$z_t$$\\end{document} at the timestamp \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$t \\in [0,\\ldots ,T]$$\\end{document} . With an additional text input y for text guidance in image generation, the training loss for the LDM is 2 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} l_{ldm}=\\mathbb {E}_{\\mathcal {E}(x), \\epsilon \\sim N(0,\\textbf{I}), t \\sim U(0,T)}\\Bigl [ ||\\epsilon - \\epsilon _\\theta (z_t, t,y) || \\Bigr ]. \\end{aligned}$$\\end{document} ControlNet 6 adds an image encoder to control the diffusion process. This control network is initialized as the pre-trained encoder of the diffusion model UNet with additional zero convolutions. The output features of the control network are added to the pre-trained diffusion model features, and due to the zero convolutions, the initial performance of the diffusion model is not degraded. After training, ControlNet weights are optimized such that the added control network features manipulate the diffusion process for the target task. Other methods for controlling the generation process of diffusion models for a specific task were also suggested 55 , 56 . Fig. 3 Optical design. ( a ) The designed separable mask, which was printed on a chrome plate using lithography. ( b ) The measured PSF by the prototype camera with the coded mask. Our rank-1 mask design is equivalent to prior designs as detailed in the Supplementary Information. Contributions Our main contributions are: (i) A novel computational photography method based on a lensless flat camera with a reconstruction algorithm that leverages a diffusion model image prior; (ii) State-of-the-art results for flat camera reconstruction quality across all evaluated metrics; (iii) Demonstration of how text conditioning inherent in diffusion models can optionally enhance imaging results for compact optical systems; (iv) an intermediate separable loss for improving convergence and results; and (v) A new large-scale dataset captured with our flat camera prototype, which facilitates further research in this field. All data and code will be made publicly available to encourage further advancements. Results Fig. 4 Qualitative results. Examples of reconstruction results for our captured dataset with the prototype camera we designed. GT images are from the LAION dataset 57 (CC BY 4.0 license).. Our DifuzCam method was evaluated using image quality and text similarity metrics. The results of the proposed reconstruction algorithm were compared to the ground truth images using PSNR, SSIM 58 , and LPIPS 59 metrics. Since we also present a text guidance approach, we measured the CLIP score 21 compared to a GT text description of the captured scene, a metric that evaluates the similarity of an image to a text. For this metric, we used the latest published and most accurate model (ViT-L/14@336px). On the test dataset that we captured, our method achieved 21.58 in PSNR, 0.541 in SSIM, 0.276 in LPIPS, and a CLIP score of 24.38. Figure&#160; 4 presents examples of our reconstruction results. Table 1 Quantitative evaluation and comparison to prior work. Method PSNR \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} SSIM \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} LPIPS \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} CLIP \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} Tikhonov 1 10.98 0.318 0.736 16.76 flatnet-R 2 18.58 0.487 0.332 21.53 flatnet-T 2 18.64 0.518 0.322 21.68 Tikhonov + Diffusion 19.80 0.536 0.277 22.57 DifuzCam (our) Without text 20.25 0.601 0.242 22.81 Sampled with text 20.20 0.604 0.238 23.50 Fine-tuned with text 20.43 0.612 0.237 23.53 Our method outperforms prior approaches: Tikhonov 1 and FlatNet 2 . Compared to a two-stage naive baseline that combines Tikhonov reconstruction with ControlNet and diffusion enhancement, the joint optimization in our approach demonstrates clear benefits. To have a fair comparison to previous works, tests should be performed on the same dataset. Since FlatNet 2 published the dataset of their camera, we conducted experiments also on their data to present a valid comparison. Unfortunately, we could not compare to 28 , 54 as they did not publish their code for comparison. Since a prior transformer-based method 29 was not trained or tested on the FlatNet data, we trained their method for comparison, but did not achieve good results. To assess the contribution of the ControlNet and diffusion model components, as well as the benefits of joint optimization, we implemented a naive baseline that combines traditional Tikhonov reconstruction 1 with ControlNet-guided diffusion enhancement as a post-processing step. This two-stage approach demonstrates that while diffusion models can enhance traditional reconstructions, the joint optimization in our method provides clear benefits across all metrics, validating our integrated approach. The comparisons to 2 are presented quantitatively in Table&#160; 1 and visually in Fig.&#160; 7 . Their results were produced using their published code and weights. Our method achieved superior results in all evaluated metrics for non-text-guided models. Adding text to the reconstruction process improves the results in the perceptual and textual similarity metrics. Note that when we just add text without fine-tuning the model, a minor degradation in PSNR is observed. When we fine-tune the model to be text-guided, we get the best improvement (Table&#160; 1 ). Figure&#160; 5 presents results for reconstructions of real scenes (not captured from a screen) that are provided in 22 , and compares them to FlatNet 22 and Tikhonov 1 . Figure&#160; 6 presents real scene reconstructions using our prototype camera. Fig. 5 Real scene results. Comparison of reconstructions from real scenes measurements with Tikhonov 1 , FlatNet-T 22 and Difuzcam (our). Table 2 Results and ablation of our method on the dataset captured by the prototype camera. Method PSNR \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} SSIM \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} LPIPS \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\downarrow$$\\end{document} CLIP \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\uparrow$$\\end{document} w/o Sep. loss 9.84 0.175 0.666 23.22 w/o Text train 21.44 0.512 0.304 22.50 Proposed method 21.58 0.541 0.276 24.38 Fig. 6 Our prototype results. Real objects were captured with our prototype camera and reconstructed using our proposed method (Difuzcam) with and without text. The reference images were captured with a Canon 80D for visual comparison. Note that they are not accurately aligned with the Difuzcam results. Fig. 7 Results comparison. We compare the results of DifuzCam to the previous method FlatNet-T 22 on their published dataset, and to Tikhonov reconstruction followed by a similar diffusion model for enhancement, as a naive diffusion baseline. Fig. 8 Ablation Results. Demonstrating the contribution of the text to the reconstruction. Without text input, the reconstruction relies solely on the flat camera measurements. With text guidance, the reconstructed image better aligns with the true scene, capturing finer details. However, when an incorrect caption is provided, the reconstructed details and high frequencies align more with the incorrect text and less compatible with the scene. Additionally, when the separable loss is not applied, the reconstruction depends solely on the input caption, disregarding sensor measurements. This underscores the crucial role of the separable loss in ensuring proper convergence. GT images are from the LAION dataset 57 (CC BY 4.0 license).. Implementation details In the dataset, the given text captions for the images are not always accurate or relevant. We acknowledge that this noise in the data might harm the results we get in the training process when using these text captions. Since we identified that these inaccuracies might be critical in the tests, we manually checked the test dataset captions to verify the accuracy and correctness of the data. This verification is very important for the text guidance reconstruction results and also for the textual CLIP score evaluation we made. Despite the potential disadvantage of training on incorrect captions, we did not manually verify the training dataset since it is not feasible to manually check such a very large dataset. To compare our results to FlatNet 2 , we trained our method on their published dataset, which consists of 10 k images for training and 100 for testing. This data does not contain captions for the images. Thus, to train our method with text guidance on this data, we used a large language model (LLM) for the automatic image captioning process. We used the LLaVA1.5 60 model and generated captions for all the images in the data. Here, we also encounter the challenge of potentially incorrect captions, which is also known as LLM hallucinations. Also, for this case, the test sample captions were manually verified due to the high importance of the test captions&#8217; correctness. For this data, we trained the model for 700k steps with a similar optimizer setup to what we mentioned in the subsection Data Acquisition and Training . We used the Allied Vision 1800 U-500 board-level camera with a pixel size of 2.2um and 5 megapixels overall for the prototype camera. Analysis and ablations In lensless imaging, the reconstruction problem is highly ill-posed, meaning that multiple plausible solutions can correspond to the same measurements. Since essential scene information is lost, our diffusion model, trained on the natural image distribution, infers and completes the missing details and features by leveraging natural image priors and textual description priors. The balance between physical information and generative content in our reconstructions stems from the fundamental information loss inherent in lensless imaging. The flat camera&#8217;s amplitude mask causes significant information loss during acquisition, creating an ill-posed inverse problem where multiple scene configurations could produce similar sensor measurements. Our diffusion model addresses this information gap by generating plausible content that bridges the missing details. The hallucinations arise from the diffusion model&#8217;s learned priors filling in the information that was fundamentally lost during the lensless acquisition process. This generative approach, while introducing content not directly captured by the sensor, demonstrates clear benefits in our evaluation. The numerical metrics (PSNR, SSIM, LPIPS) show that images with this generative content are closer to the ground truth compared to traditional reconstruction methods. Additionally, the perceptual quality is significantly enhanced, making the images more visually appealing and interpretable to human observers. The diffusion model&#8217;s strong natural image priors help reconstruct plausible high-frequency details and textures that would otherwise remain as artifacts or blur in conventional reconstruction approaches. The details added by the model are crucial for producing a perceptually pleasing image that is closer to the true scene and for mitigating the inherent ambiguities of lensless imaging. However, this generated content, mainly in fine textures and structures, may not always faithfully represent the captured scene. This characteristic should be carefully considered in applications requiring high precision in fine details, such as microscopy or quantitative imaging tasks, where the distinction between measured and inferred content is critical. The trade-off between reconstruction accuracy and the benefits of lensless imaging varies significantly across applications. In precision-critical domains like medical imaging, scientific microscopy, surveillance systems, or quality control systems, the generative nature of our approach may limit its applicability where pixel-level accuracy is paramount. However, in many practical scenarios, the significant size and weight reduction offered by flat cameras, combined with acceptable reconstruction quality, presents compelling advantages. For instance, in Internet of Things (IoT) applications, consumer photography, or mobile devices, the compact form factor and reduced manufacturing costs may outweigh minor inaccuracies in fine details. Similarly, for social media applications or augmented reality systems, the enhanced perceptual quality and visually appealing reconstructions may be more valuable than perfect pixel-level fidelity. We present ablation results in Table&#160; 2 and Fig.&#160; 8 . First, we observe that without our proposed separable loss, the reconstructed images are not similar to the target image. We observe that while the reconstructed images align well with the text captions, as indicated by the high CLIP score, they fail to incorporate additional information from the camera measurements during the reconstruction process. Namely, the reconstructions become independent of the input measurements. However, when applying the separable loss, the camera measurements are effectively utilized, ensuring that the reconstructed image is based on the input measurements. Adding text information as input improves the reconstruction even further, compared to the non-text-guided model. The visual ablation images in Fig.&#160; 8 demonstrate that the text captions contribute to the high-frequency details in the reconstructed images. When a text caption is provided, the reconstructed image details closely align with the caption&#8217;s description. This effect is also noticeable when an incorrect caption is given. For example, the reconstruction adopts a painting-like style when the caption mentions a painting, and elephant-like shapes emerge when elephants are described in the caption. Discussion In this work, we introduced a novel method for reconstructing images from flat camera measurements, leveraging the strong prior capabilities of a pre-trained diffusion model. Our approach produces high-quality reconstructions both with and without text guidance, offering a significant step forward in lensless imaging. Although minor inaccuracies may appear in the finest details compared to ground truth images, these discrepancies arise from the inherently ill-posed nature of the problem, requiring our model to fill in lost details through its learned priors. A key limitation of our amplitude-based approach is light efficiency. The binary mask blocks a significant portion of incoming light, which must be considered for practical applications. This trade-off between compactness and light gathering capability represents a fundamental challenge in amplitude-based lensless imaging. Future work could explore phase-based masks or metasurface designs that offer improved light efficiency, though at the cost of increased fabrication complexity. Regarding computational efficiency, our diffusion-based approach requires iterative denoising steps, resulting in longer processing times compared to traditional methods. While Tikhonov reconstruction processes images in approximately 30 ms on CPU and FlatNet in 33 ms on GPU, our method requires approximately 907 ms due to the iterative nature of diffusion models. However, this computational cost enables substantial quality improvements, representing a trade-off between processing speed and reconstruction fidelity. Future work could explore acceleration techniques such as fewer diffusion steps, distilled models, or specialized hardware implementations to improve practical deployment while maintaining reconstruction quality. By comparing our method against previous approaches, we demonstrated state-of-the-art performance and enhanced reconstruction quality. Beyond flat cameras, the principles underlying our method can be adapted to other imaging systems, highlighting the versatility of diffusion-based priors in computational photography. We envision that continued improvements in diffusion models and text guidance techniques will further enhance the fidelity of reconstructions in future work. Methods We turn to describe our DifuzCam strategy. The flat camera we use is based on a similar implementation to previous works 1 , 22 . For the amplitude mask, we used a separable pattern (rank-1) obtained from the outer product of binaryized M-sequence signals of length 255. M-sequences offer favorable autocorrelation properties and relatively flat frequency response characteristics, which can improve reconstruction quality. The separable structure allows for efficient inverse algorithms that decompose the 2D reconstruction problem into two 1D operations, reducing computational complexity. We printed it by lithography onto a chrome plate on glass, with a thickness of 0.2 mm. Each feature in the pattern (i.e., a single pinhole) is of size 25 um. The binary amplitude mask inherently blocks a significant portion of the incoming light, as the opaque regions prevent light transmission to maintain the encoding pattern. This light efficiency limitation is a fundamental trade-off in amplitude-based lensless imaging, where spatial encoding requires sacrificing light throughput. As a design choice, we used a rank-1 separable mask, which is equivalent to prior works that used a rank-2 (non-separable) mask and achieved separability through mean subtraction (see equivalence in the Supplementary Information). Figure&#160; 3 shows the mask pattern and the measured camera PSF achieved while the mask is attached to the sensor hot mirror. The acquired image by a flat camera consists of multiplexed measurements of the light reflected from the scene across the sensor area. The light projections on the sensor create long-range correspondences in the captured images. To convert the image from the projections (i.e., &#8220;projection space&#8221;) to the pixel space of the target image, we apply a learned separable linear transformation to the measurements. This transformation is crucial since the input image serves as guidance for the diffusion model process, which was trained and works in the domain of natural images. The diffusion model is ignorant about the flat camera mask projections on the image, and the control network we use to guide the reconstruction process exhibits better results when operating in the image pixel domain rather than the long-range projections. The RAW image captured by the camera (in RGGB Bayer pattern) is split into four color channels: R , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$G_r$$\\end{document} , \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$G_b$$\\end{document} , and B . Each channel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_k$$\\end{document} is linearly transformed as 3 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} C^o_k = \\phi _l^kC_k\\phi _r^k, \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$k \\in [R, G_r, G_b, B]$$\\end{document} , each color channel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C_k$$\\end{document} of size \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$h_i\\times w_i$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi _l \\in \\mathbb {R}^{h_o\\times h_i}$$\\end{document} and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\phi _r \\in \\mathbb {R}^{w_i\\times w_o}$$\\end{document} are two learnable matrices. The output features are stacked onto a single 4-channel tensor \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C^o \\in \\mathbb {R}^{4 \\times h_o\\times w_o}$$\\end{document} . The learnable matrices are initialized using a Gaussian distribution with a standard deviation of 0.001, as prior tests indicated that more complex initialization methods had no significant impact. Since the flat camera image reconstruction is highly ill-posed, we utilized a pre-trained diffusion model as our image prior. It is a strong prior for natural images since it is trained on a huge number of samples for the image generation task. To leverage the diffusion model for our task, we need to control its generation process such that we can generate the captured scene from the measurements of the flat camera. To do so, we use a ControlNet 6 network \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathcal {C_\\psi }$$\\end{document} which we train for our goal (as presented in Fig.&#160; 2 ). This network is initialized as a copy of the encoder of the diffusion model UNet with zero convolutions, such that the pre-trained weights&#8217; performance is not affected and during training, non-zero weights are learned for the reconstruction task. The input of the control network is the output of the separable transform ( \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C^o$$\\end{document} ), and we use the control network loss 4 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} l_{\\mathcal {C}}=\\mathbb {E}_{\\mathcal {E}(x), \\epsilon \\sim N(0,\\textbf{I}), t \\sim U(0,T)}\\Bigl [ ||\\epsilon - \\epsilon _\\theta (z_t, t, y, \\mathcal {C_\\psi }(C^o)) ||_2 \\Bigr ], \\end{aligned}$$\\end{document} which is applied for both \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\psi$$\\end{document} , the set of parameters of the control network \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathcal {C_\\psi }$$\\end{document} , and the learned separable transform weights. The diffusion model parameters \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta$$\\end{document} are pre-trained and fixed. To guide the training for better results, a separable reconstruction loss term is added to the conventional diffusion loss (Equation&#160;( 4 )). This loss is applied to the output of the separable transformation as 5 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\begin{aligned} l_{sep} = || I - f_{conv}(C^o) ||_2, \\end{aligned}$$\\end{document} where \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$f_{conv}$$\\end{document} is a learned \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$3\\times 3$$\\end{document} convolution layer which maps the 4-channel \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$C^o$$\\end{document} to a 3-channel image, and I is the target (GT) image in RGB channels format. We present the improvement and significance of this term in the subsection Analysis and Ablations. The diffusion model that we use is a text-guided model for image generation. We leverage this existing text-conditioning capability to optionally improve the image reconstruction process when scene descriptions are available from the photographer. For compact imaging systems like IoT devices or smartphone cameras, where users can easily provide contextual information, this additional prior knowledge helps the algorithm choose among multiple plausible reconstructions that are consistent with the sensor measurements 5 . Thus, we employ this ability to improve the image reconstruction process by giving the model a text description of the captured scene. Giving additional information about the scene content enables the algorithm to have better prior knowledge of the resulting image and reconstruct better images. In this approach, the photographer describes the captured scene, and this information is input into the reconstruction algorithm. The contribution of the text to the results is presented in the subsection Analysis and Ablations. Data acquisition and training As we train our DifuzCam model (separable transform and control network) in a supervised way, we need pairs of RGB images with their corresponding measurements of the flat camera. Simulating the flat camera imaging process to get realistic measurements such that the trained model will generalize to the real world is very hard since the captured measurements are very dependent on camera properties, alignment, calibration, and mask placement. Because of the small feature size (25 um), minor movement of the mask will result in totally different measurements. Due to this sensitivity of the system, we obtained real-world measurements using the optical setup that contains all the imaging properties, including space-variant PSF, diffraction, and non-ideal effects, which are hard to simulate, such as mask manufacturing inaccuracies, dust, image noise, etc. To get a large dataset using our flat camera, we captured images from the LAION-aesthetics dataset 57 , which were projected on a PC screen of size 34cm \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times$$\\end{document} 34 cm at a distance of 60 cm from the camera. We used an exposure time of 12 ms and saved the raw Bayer pattern images in 12-bit depth. We used LAION-aesthetics 57 as it consists of a large number of high-resolution images and their corresponding textual captions. In total, we captured about 55 k images. 500 images were saved for testing, while the rest were used for training. To compensate for stray light, a black screen with no image projected on it was captured. In the post-processing stage, the measured black levels were subtracted from the captured measurements. In addition to the screen images, we also capture real objects to validate that we are not restricted only to &#8220;objects on screen&#8221;. In this case, we just show the qualitative reconstruction result as we do not have an exact RGB match as in the screen measurement case. For the pre-trained diffusion model, we used stable-diffusion 2.1 61 . We trained the model for 500k steps on the captured dataset using the losses in Eqs.&#160;( 4 ) and ( 2 ) and a learning rate of \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$5\\times 10^{-5}$$\\end{document} with the AdamW optimizer. Supplementary Information Supplementary Information. Publisher&#8217;s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-025-27127-1. Author contributions E.Y. developed the methodology, conducted the experiments, and collected the results. R.G. supervised the project, guided the research direction, and contributed to the analysis of the results. Both authors contributed to the writing and revision of the manuscript. Data availability The data and code of this study will be made publicly available upon acceptance. For any inquiries regarding the data or code, please contact erez.yo@gmail.com. Declarations Competing interests The authors declare no competing interests. References 1. Salman Asif, M., Ayremlou, A., Veeraraghavan, A., Baraniuk, R. &amp; Sankaranarayanan, A. Flatcam: Replacing lenses with masks and computation. In Proceedings of the IEEE International Conference on Computer Vision Workshops 12&#8211;15 (2015). 2. Khan, S.&#160;S. et al. Towards photorealistic reconstruction of highly multiplexed lensless images. In Proceedings of the IEEE/CVF International Conference on Computer Vision 7860&#8211;7869 (2019). 3. Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 10684&#8211;10695 (2022). 4. Croitoru F-A Hondru V Ionescu RT Shah M Diffusion models in vision: A survey IEEE Trans. Pattern Anal. Mach. Intell. 2023 1 1 10.1109/TPAMI.2023.3261988 37030794 Croitoru, F.-A., Hondru, V., Ionescu, R. T. &amp; Shah, M. Diffusion models in vision: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 1 , 1 (2023). 10.1109/TPAMI.2023.3261988 37030794 5. Yosef, E. &amp; Giryes, R. Tell me what you see: Text-guided real-world image denoising. Preprint at http://arxiv.org/abs/2312.10191 (2023). 6. Zhang, L., Rao, A. &amp; Agrawala, M. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision 3836&#8211;3847 (2023). 7. Huang, G., Jiang, H., Matthews, K. &amp; Wilford, P. Lensless imaging by compressive sensing. In 2013 IEEE International Conference on Image Processing 2101&#8211;2105 (IEEE, 2013). 8. DeWeert, M.&#160;J. &amp; Farm, B.&#160;P. Lensless coded aperture imaging with separable doubly toeplitz masks. In Compressive Sensing III , vol. 9109, 180&#8211;191 (SPIE, 2014). 9. Shimano T Nakamura Y Tajima K Sao M Hoshizawa T Lensless light-field imaging with fresnel zone aperture: quasi-coherent coding Appl. Opt. 2018 57 2841 2850 10.1364/AO.57.002841 29714287 Shimano, T., Nakamura, Y., Tajima, K., Sao, M. &amp; Hoshizawa, T. Lensless light-field imaging with fresnel zone aperture: quasi-coherent coding. Appl. Opt. 57 , 2841&#8211;2850 (2018). 29714287 10.1364/AO.57.002841 10. Nakamura, Y., Shimano, T., Tajima, K., Sao, M. &amp; Hoshizawa, T. Lensless light-field imaging with fresnel zone aperture. In ITE Technical Report 40.40 Information Sensing Technologies (IST) 7&#8211;8 (The Institute of Image Information and Television Engineers, 2016). 11. Boominathan V Adams JK Robinson JT Veeraraghavan A Phlatcam: Designed phase-mask based thin lensless camera IEEE Trans. Pattern Anal. Mach. Intell. 2020 42 1618 1629 10.1109/TPAMI.2020.2987489 32324539 PMC7439257 Boominathan, V., Adams, J. K., Robinson, J. T. &amp; Veeraraghavan, A. Phlatcam: Designed phase-mask based thin lensless camera. IEEE Trans. Pattern Anal. Mach. Intell. 42 , 1618&#8211;1629 (2020). 32324539 10.1109/TPAMI.2020.2987489 PMC7439257 12. Antipa N Diffusercam: lensless single-exposure 3d imaging Optica 2018 5 1 9 10.1364/OPTICA.5.000001 Antipa, N. et al. Diffusercam: lensless single-exposure 3d imaging. Optica 5 , 1&#8211;9 (2018). 13. Zomet, A. &amp; Nayar, S.&#160;K. Lensless imaging with a controllable aperture. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#8217;06) , vol.&#160;1, 339&#8211;346 (IEEE, 2006). 14. Miller JR Wang C-Y Keating CD Liu Z Particle-based reconfigurable scattering masks for lensless imaging ACS Nano 2020 14 13038 13046 10.1021/acsnano.0c04490 32929968 Miller, J. R., Wang, C.-Y., Keating, C. D. &amp; Liu, Z. Particle-based reconfigurable scattering masks for lensless imaging. ACS Nano 14 , 13038&#8211;13046 (2020). 32929968 10.1021/acsnano.0c04490 15. Zheng, Y., Hua, Y., Sankaranarayanan, A.&#160;C. &amp; Asif, M.&#160;S. A simple framework for 3d lensless imaging with programmable masks. In Proceedings of the IEEE/CVF International Conference on Computer Vision 2603&#8211;2612 (2021). 16. Hua Y Nakamura S Asif MS Sankaranarayanan AC Sweepcam depth-aware lensless imaging using programmable masks IEEE Trans. Pattern Anal. Mach. Intell. 2020 42 1606 1617 10.1109/TPAMI.2020.2986784 32305898 Hua, Y., Nakamura, S., Asif, M. S. &amp; Sankaranarayanan, A. C. Sweepcam depth-aware lensless imaging using programmable masks. IEEE Trans. Pattern Anal. Mach. Intell. 42 , 1606&#8211;1617 (2020). 32305898 10.1109/TPAMI.2020.2986784 17. Boominathan V Robinson JT Waller L Veeraraghavan A Recent advances in lensless imaging Optica 2022 9 1 16 10.1364/OPTICA.431361 36338918 PMC9634619 Boominathan, V., Robinson, J. T., Waller, L. &amp; Veeraraghavan, A. Recent advances in lensless imaging. Optica 9 , 1&#8211;16 (2022). 36338918 10.1364/optica.431361 PMC9634619 18. Chen WT Dispersion-engineered metasurfaces reaching broadband 90% relative diffraction efficiency Nat. Commun. 2023 14 2544 10.1038/s41467-023-38185-2 37137885 PMC10156701 Chen, W. T. et al. Dispersion-engineered metasurfaces reaching broadband 90% relative diffraction efficiency. Nat. Commun. 14 , 2544 (2023). 37137885 10.1038/s41467-023-38185-2 PMC10156701 19. Tseng E Neural nano-optics for high-quality thin lens imaging Nat. Commun. 2021 12 6493 10.1038/s41467-021-26443-0 34845201 PMC8630181 Tseng, E. et al. Neural nano-optics for high-quality thin lens imaging. Nat. Commun. 12 , 6493 (2021). 34845201 10.1038/s41467-021-26443-0 PMC8630181 20. Peng Y Learned large field-of-view imaging with thin-plate optics ACM Trans. Graph. 2019 38 219 10.1145/3355089.3356526 Peng, Y. et al. Learned large field-of-view imaging with thin-plate optics. ACM Trans. Graph. 38 , 219 (2019). 21. Radford, A. et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning 8748&#8211;8763 (PMLR, 2021). 22. Khan SS Sundar V Boominathan V Veeraraghavan A Mitra K Flatnet: Towards photorealistic scene reconstruction from lensless measurements IEEE Trans. Pattern Anal. Mach. Intell. 2020 44 1934 1948 10.1109/TPAMI.2020.3033882 PMC8979921 33104508 Khan, S. S., Sundar, V., Boominathan, V., Veeraraghavan, A. &amp; Mitra, K. Flatnet: Towards photorealistic scene reconstruction from lensless measurements. IEEE Trans. Pattern Anal. Mach. Intell. 44 , 1934&#8211;1948 (2020). 10.1109/TPAMI.2020.3033882 PMC8979921 33104508 23. Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015: 18th International Conference, Munich, Germany, October 5&#8211;9, 2015, Proceedings, Part III 18 234&#8211;241 (Springer, 2015). 24. Rego, J.&#160;D., Kulkarni, K. &amp; Jayasuriya, S. Robust lensless image reconstruction via psf estimation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 403&#8211;412 (2021). 25. Wu J Cao L Barbastathis G Dnn-fza camera: a deep learning approach toward broadband fza lensless imaging Opt. Lett. 2021 46 130 133 10.1364/OL.411228 33362033 Wu, J., Cao, L. &amp; Barbastathis, G. Dnn-fza camera: a deep learning approach toward broadband fza lensless imaging. Opt. Lett. 46 , 130&#8211;133 (2021). 33362033 10.1364/OL.411228 26. Haris, M., Shakhnarovich, G. &amp; Ukita, N. Deep back-projection networks for super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 1664&#8211;1673 (2018). 27. Kingshott O Antipa N Bostan E Ak&#351;it K Unrolled primal-dual networks for lensless cameras Opt. Express 2022 30 46324 46335 10.1364/OE.475521 36558589 Kingshott, O., Antipa, N., Bostan, E. &amp; Ak&#351;it, K. Unrolled primal-dual networks for lensless cameras. Opt. Express 30 , 46324&#8211;46335 (2022). 36558589 10.1364/OE.475521 28. Li Y Li Z Chen K Guo Y Rao C Mwdns: reconstruction in multi-scale feature spaces for lensless imaging Opt. Express 2023 31 39088 39101 10.1364/OE.501970 38017997 Li, Y., Li, Z., Chen, K., Guo, Y. &amp; Rao, C. Mwdns: reconstruction in multi-scale feature spaces for lensless imaging. Opt. Express 31 , 39088&#8211;39101 (2023). 38017997 10.1364/OE.501970 29. Pan X Chen X Takeyama S Yamaguchi M Image reconstruction with transformer for mask-based lensless imaging Opt. Lett. 2022 47 1843 1846 10.1364/OL.455378 35363750 Pan, X., Chen, X., Takeyama, S. &amp; Yamaguchi, M. Image reconstruction with transformer for mask-based lensless imaging. Opt. Lett. 47 , 1843&#8211;1846 (2022). 35363750 10.1364/OL.455378 30. Liu, M., Su, X., Yao, X., Hao, W. &amp; Zhu, W. Lensless image restoration based on multi-stage deep neural networks and pix2pix architecture. In Photonics , vol.&#160;10, 1274 (MDPI, 2023). 31. Ho J Jain A Abbeel P Denoising diffusion probabilistic models Adv. Neural. Inf. Process. Syst. 2020 33 6840 6851 Ho, J., Jain, A. &amp; Abbeel, P. Denoising diffusion probabilistic models. Adv. Neural. Inf. Process. Syst. 33 , 6840&#8211;6851 (2020). 32. Nichol, A.&#160;Q. &amp; Dhariwal, P. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning 8162&#8211;8171 (PMLR, 2021). 33. Nichol AQ GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models ICML 2022 162 16784 16804 Nichol, A. Q. et al. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. ICML 162 , 16784&#8211;16804 (2022). 34. Dhariwal P Nichol A Diffusion models beat gans on image synthesis Adv. Neural. Inf. Process. Syst. 2021 34 8780 8794 Dhariwal, P. &amp; Nichol, A. Diffusion models beat gans on image synthesis. Adv. Neural. Inf. Process. Syst. 34 , 8780&#8211;8794 (2021). 35. Abu-Hussein S Giryes R Udpm: Upsampling diffusion probabilistic models NeuRIPS 2024 37 27616 27646 Abu-Hussein, S. &amp; Giryes, R. Udpm: Upsampling diffusion probabilistic models. NeuRIPS 37 , 27616&#8211;27646 (2024). 36. Amit, T., Shaharbany, T., Nachmani, E. &amp; Wolf, L. Segdiff: Image segmentation with diffusion probabilistic models. Preprint at http://arxiv.org/abs/2112.00390 (2021). 37. Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V. &amp; Babenko, A. Label-efficient semantic segmentation with diffusion models. In ICLR (2022). 38. Lugmayr, A. et al. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 11461&#8211;11471 (2022). 39. Saharia C Image super-resolution via iterative refinement IEEE Trans. Pattern Anal. Mach. Intell. 2022 45 4713 4726 10.1109/TPAMI.2022.3204461 36094974 Saharia, C. et al. Image super-resolution via iterative refinement. IEEE Trans. Pattern Anal. Mach. Intell. 45 , 4713&#8211;4726 (2022). 10.1109/TPAMI.2022.3204461 36094974 40. Kawar B Elad M Ermon S Song J Denoising diffusion restoration models Adv. Neural. Inf. Process. Syst. 2022 35 23593 23606 Kawar, B., Elad, M., Ermon, S. &amp; Song, J. Denoising diffusion restoration models. Adv. Neural. Inf. Process. Syst. 35 , 23593&#8211;23606 (2022). 41. Saharia, C. et al. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings 1&#8211;10 (2022). 42. Batzolis, G., Stanczuk, J., Sch&#246;nlieb, C.-B. &amp; Etmann, C. Conditional image generation with score-based diffusion models. Preprint at http://arxiv.org/abs/2111.13606 (2021). 43. Abu-Hussein, S., Tirer, T. &amp; Giryes, R. Adir: Adaptive diffusion for image reconstruction. Preprint at http://arxiv.org/abs/2212.03221 (2022). 44. Zhu, Y. et al. Denoising diffusion models for plug-and-play image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 1219&#8211;1229 (2023). 45. Fei, B. et al. Generative diffusion prior for unified image restoration and enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 9935&#8211;9946 (2023). 46. Chung, H., Ye, J.&#160;C., Milanfar, P. &amp; Delbracio, M. Prompt-tuning latent diffusion models for inverse problems. In ICML (2024). 47. Delbracio, M. &amp; Milanfar, P. An alternative to denoising diffusion for image restoration. In TMLR, Inversion by Direct Iteration (2023). 48. Pearl, N. et al. SVNR: Spatially-variant noise removal with denoising diffusion. Preprint at http://arxiv.org/abs/2306.16052 (2023). 49. Yi, X., Xu, H., Zhang, H., Tang, L. &amp; Ma, J. Diff-retinex: Rethinking low-light image enhancement with a generative diffusion model. In IEEE/CVF International Conference on Computer Vision 12302&#8211;12311 (2023). 50. Torem, N., Ronen, R., Schechner, Y.&#160;Y. &amp; Elad, M. Complex-valued retrievals from noisy images using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision 3810&#8211;3820 (2023). 51. Nguyen, C.&#160;M., Chan, E.&#160;R., Bergman, A.&#160;W. &amp; Wetzstein, G. Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition (2024). 52. Kim, J., Park, G.&#160;Y., Chung, H. &amp; Ye, J.&#160;C. Regularization by texts for latent diffusion inverse solvers. In ICLR (2025). 53. Qi, C. et al. SPIRE: Semantic prompt-driven image restoration. In ECCV (2024). 54. Cai X Phocolens: Photorealistic and consistent reconstruction in lensless imaging Adv. Neural. Inf. Process. Syst. 2024 37 12219 12242 10.52202/079017-0391 Cai, X. et al. Phocolens: Photorealistic and consistent reconstruction in lensless imaging. Adv. Neural. Inf. Process. Syst. 37 , 12219&#8211;12242 (2024). 55. Mou C T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models Proc. AAAI Conf. Artif. Intell. 2024 38 4296 4304 Mou, C. et al. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Proc. AAAI Conf. Artif. Intell. 38 , 4296&#8211;4304 (2024). 56. Duan X Tuning-free inversion-enhanced control for consistent image editing Proc. AAAI Conf. Artif. Intell. 2024 38 1644 1652 Duan, X. et al. Tuning-free inversion-enhanced control for consistent image editing. Proc. AAAI Conf. Artif. Intell. 38 , 1644&#8211;1652 (2024). 57. Schuhmann C Laion-5b: An open large-scale dataset for training next generation image-text models Adv. Neural. Inf. Process. Syst. 2022 35 25278 25294 Schuhmann, C. et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Adv. Neural. Inf. Process. Syst. 35 , 25278&#8211;25294 (2022). 58. Wang Z Bovik AC Sheikh HR Simoncelli EP Image quality assessment: from error visibility to structural similarity IEEE Trans. Image Process. 2004 13 600 612 10.1109/TIP.2003.819861 15376593 Wang, Z., Bovik, A. C., Sheikh, H. R. &amp; Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process. 13 , 600&#8211;612 (2004). 15376593 10.1109/tip.2003.819861 59. Zhang, R., Isola, P., Efros, A.&#160;A., Shechtman, E. &amp; Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR (2018). 60. Liu H Li C Wu Q Lee YJ Visual instruction tuning Adv. Neural Inf. Process. Syst. 2024 36 1 PMC11867732 40017809 Liu, H., Li, C., Wu, Q. &amp; Lee, Y. J. Visual instruction tuning. Adv. Neural Inf. Process. Syst. 36 , 1 (2024). PMC11867732 40017809 61. Rombach, R., Blattmann, A., Lorenz, D., Esser, P. &amp; Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10684&#8211;10695 (2022)."
}