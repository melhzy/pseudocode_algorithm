{
  "pmcid": "PMC12656680",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.702650",
  "metadata": {
    "journal_title": "Sensors (Basel, Switzerland)",
    "journal_nlm_ta": "Sensors (Basel)",
    "journal_iso_abbrev": "Sensors (Basel)",
    "journal": "Sensors (Basel, Switzerland)",
    "pmcid": "PMC12656680",
    "pmid": "41305037",
    "doi": "10.3390/s25226828",
    "title": "Research on Orchard Navigation Line Recognition Method Based on U-Net",
    "year": "2025",
    "month": "11",
    "day": "07",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "07"
    },
    "authors": [
      "Xu Ning",
      "Ning Xiangsen",
      "Li Aijuan",
      "Li Zhihe",
      "Song Yumin",
      "Wu Wenxuan"
    ],
    "abstract": "Aiming at the problems of complex image background and numerous interference factors faced by visual navigation systems in orchard environments, this paper proposes an orchard navigation line recognition method based on U-Net. Firstly, the drivable areas in the collected images are labeled using Labelme (a graphical tool for image annotation) to create an orchard dataset. Then, the Spatial Attention (SA) mechanism is inserted into the downsampling stage of the traditional U-Net semantic segmentation method, and the Coordinate Attention (CA) mechanism is added to the skip connection stage to obtain complete context information and optimize the feature restoration process of the drivable area in the field, thereby improving the overall segmentation accuracy of the model. Subsequently, the improved U-Net network is trained using the enhanced dataset to obtain the drivable area segmentation model. Based on the detected drivable area segmentation mask, the navigation line information is extracted, and the geometric center points are calculated row by row. After performing sliding window processing and bidirectional interpolation filling on the center points, the navigation line is generated through spline interpolation. Finally, the proposed method is compared and verified with U-Net, SegViT, SE-Net, and DeepLabv3+ networks. The results show that the improved drivable area segmentation model has a Recall of 90.23%, a Precision of 91.71%, a mean pixel accuracy (mPA) of 87.75%, and a mean intersection over union (mIoU) of 84.84%. Moreover, when comparing the recognized navigation line with the actual center line, the average distance error of the extracted navigation line is 56 mm, which can provide an effective reference for visual autonomous navigation in orchard environments.",
    "keywords": [
      "orchard environment",
      "navigation line",
      "U-Net network",
      "attention mechanism",
      "divisible driving area"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Sensors (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">1660</journal-id><journal-id journal-id-type=\"pmc-domain\">sensors</journal-id><journal-id journal-id-type=\"publisher-id\">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type=\"epub\">1424-8220</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656680</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656680.1</article-id><article-id pub-id-type=\"pmcaid\">12656680</article-id><article-id pub-id-type=\"pmcaiid\">12656680</article-id><article-id pub-id-type=\"pmid\">41305037</article-id><article-id pub-id-type=\"doi\">10.3390/s25226828</article-id><article-id pub-id-type=\"publisher-id\">sensors-25-06828</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Research on Orchard Navigation Line Recognition Method Based on U-Net</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Xu</surname><given-names initials=\"N\">Ning</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><xref rid=\"af1-sensors-25-06828\" ref-type=\"aff\">1</xref><xref rid=\"af2-sensors-25-06828\" ref-type=\"aff\">2</xref><xref rid=\"af3-sensors-25-06828\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Ning</surname><given-names initials=\"X\">Xiangsen</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><xref rid=\"af4-sensors-25-06828\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0001-9313-6107</contrib-id><name name-style=\"western\"><surname>Li</surname><given-names initials=\"A\">Aijuan</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><xref rid=\"af4-sensors-25-06828\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names initials=\"Z\">Zhihe</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Resources\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/resources/\">Resources</role><xref rid=\"af1-sensors-25-06828\" ref-type=\"aff\">1</xref><xref rid=\"c1-sensors-25-06828\" ref-type=\"corresp\">*</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Song</surname><given-names initials=\"Y\">Yumin</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><xref rid=\"af4-sensors-25-06828\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names initials=\"W\">Wenxuan</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><xref rid=\"af2-sensors-25-06828\" ref-type=\"aff\">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Baghdadi</surname><given-names initials=\"N\">Nicolas</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-sensors-25-06828\"><label>1</label>College of Agricultural Engineering and Food Science, Shandong University of Technology, Zibo 255000, China; <email>xuning@saas.ac.cn</email></aff><aff id=\"af2-sensors-25-06828\"><label>2</label>Shandong Academy of Agricultural Machinery Sciences, Jinan 252100, China; <email>wuwenxuan@saas.ac.cn</email></aff><aff id=\"af3-sensors-25-06828\"><label>3</label>Shandong Key Laboratory of Intelligent Agricultural Equipment in Hilly and Mountainous Areas, Jinan 250100, China</aff><aff id=\"af4-sensors-25-06828\"><label>4</label>School of Automotive Engineering, Shandong Jiaotong University, Jinan 250357, China; <email>13933028705@163.com</email> (X.N.); <email>liaijuan@sdjtu.edu.cn</email> (A.L.); <email>songyumin@sdjtu.edu.cn</email> (Y.S.)</aff><author-notes><corresp id=\"c1-sensors-25-06828\"><label>*</label>Correspondence: <email>lizhihe@sdut.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>07</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>25</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501335</issue-id><elocation-id>6828</elocation-id><history><date date-type=\"received\"><day>23</day><month>9</month><year>2025</year></date><date date-type=\"rev-recd\"><day>22</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>05</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>07</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"sensors-25-06828.pdf\"/><abstract><p>Aiming at the problems of complex image background and numerous interference factors faced by visual navigation systems in orchard environments, this paper proposes an orchard navigation line recognition method based on U-Net. Firstly, the drivable areas in the collected images are labeled using Labelme (a graphical tool for image annotation) to create an orchard dataset. Then, the Spatial Attention (SA) mechanism is inserted into the downsampling stage of the traditional U-Net semantic segmentation method, and the Coordinate Attention (CA) mechanism is added to the skip connection stage to obtain complete context information and optimize the feature restoration process of the drivable area in the field, thereby improving the overall segmentation accuracy of the model. Subsequently, the improved U-Net network is trained using the enhanced dataset to obtain the drivable area segmentation model. Based on the detected drivable area segmentation mask, the navigation line information is extracted, and the geometric center points are calculated row by row. After performing sliding window processing and bidirectional interpolation filling on the center points, the navigation line is generated through spline interpolation. Finally, the proposed method is compared and verified with U-Net, SegViT, SE-Net, and DeepLabv3+ networks. The results show that the improved drivable area segmentation model has a Recall of 90.23%, a Precision of 91.71%, a mean pixel accuracy (mPA) of 87.75%, and a mean intersection over union (mIoU) of 84.84%. Moreover, when comparing the recognized navigation line with the actual center line, the average distance error of the extracted navigation line is 56 mm, which can provide an effective reference for visual autonomous navigation in orchard environments.</p></abstract><kwd-group><kwd>orchard environment</kwd><kwd>navigation line</kwd><kwd>U-Net network</kwd><kwd>attention mechanism</kwd><kwd>divisible driving area</kwd></kwd-group><funding-group><award-group><funding-source>Shandong Province Key Research and Development Plan (Major Science and Technology Innovation Project)</funding-source><award-id>2022CXGC020706</award-id></award-group><award-group><funding-source>Shandong Province Agricultural Mechanization R&amp;D, Manufacturing, Promotion and Application Integration Pilot Project</funding-source><award-id>NJYTHSD-202315</award-id></award-group><award-group><funding-source>MIIT Major agricultural machinery equipment power system key components project</funding-source><award-id>2023ZY02002</award-id></award-group><award-group><funding-source>Shandong Province Science and Technology oriented Small and Medium Enterprises Enhancement Project</funding-source><award-id>2023TSGC0288</award-id></award-group><award-group><funding-source>Jinan 2023 talent development special fund research leader studio project</funding-source><award-id>202333067</award-id></award-group><funding-statement>This research was funded by Shandong Province Key Research and Development Plan (Major Science and Technology Innovation Project) (Grant No. 2022CXGC020706). Shandong Province Agricultural Mechanization R&amp;D, Manufacturing, Promotion and Application Integration Pilot Project (Grant No. NJYTHSD-202315). MIIT Major agricultural machinery equipment power system key components project (Grant No. 2023ZY02002). Shandong Province Science and Technology oriented Small and Medium Enterprises Enhancement Project (Grant No. 2023TSGC0288). Jinan 2023 talent development special fund research leader studio project (Grant No. 202333067).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-sensors-25-06828\"><title>1. Introduction</title><p>Navigation line recognition, as one of the key technologies for autonomous driving of agricultural tractors, is widely applied in tasks such as picking, spraying, and weeding [<xref rid=\"B1-sensors-25-06828\" ref-type=\"bibr\">1</xref>,<xref rid=\"B2-sensors-25-06828\" ref-type=\"bibr\">2</xref>,<xref rid=\"B3-sensors-25-06828\" ref-type=\"bibr\">3</xref>], which can enhance agricultural productivity and address the issue of labor shortage. Common navigation techniques include LiDAR navigation, GNSS (Global Navigation Satellite System), and machine vision navigation [<xref rid=\"B4-sensors-25-06828\" ref-type=\"bibr\">4</xref>]. The orchard environment is typically characterized by dense planting conditions. The canopy can block satellite positioning signals, leading to inaccurate positioning information [<xref rid=\"B5-sensors-25-06828\" ref-type=\"bibr\">5</xref>]. LiDAR acquires point cloud information of the surrounding environment through beam scanning, but it has problems such as excessive redundant information, difficulty in feature extraction, and high equipment costs. Compared with LiDAR-based GNSS positioning technology, visual navigation has the advantages of rich semantic information, wide detection range, and controllable cost, making it one of the current research hotspots both at home and abroad. Moreover, navigation line recognition serves as a fundamental step for path tracking control, directly influencing the accuracy and stability of autonomous navigation [<xref rid=\"B6-sensors-25-06828\" ref-type=\"bibr\">6</xref>]. Reliable line detection provides essential geometric and positional information for subsequent trajectory tracking and motion planning, which are critical for safe and efficient autonomous operation. Therefore, improvements in line recognition can contribute to enhancing overall control performance [<xref rid=\"B7-sensors-25-06828\" ref-type=\"bibr\">7</xref>].</p><p>At present, the machine vision navigation in orchards mostly relies on the structure of the orchard and the characteristics of the trees to calculate the navigation lines by fitting the tree row lines with the tree root points. Ma Chi et al. [<xref rid=\"B8-sensors-25-06828\" ref-type=\"bibr\">8</xref>] proposed a navigation feature target detection method based on deep learning, where the bottom center point of the target box replaces the tree root points. The tree row lines are extracted using cubic spline interpolation, and the navigation lines are generated by fitting using the least squares method. Shubo et al. [<xref rid=\"B9-sensors-25-06828\" ref-type=\"bibr\">9</xref>] takes the midpoint of the tree trunk root as the navigation positioning base point, uses the improved YOLOv7 model to obtain the positioning reference points for the tree row lines on both sides, and then fits the tree row lines and the navigation lines using the least squares method. Xiao Ke et al. [<xref rid=\"B10-sensors-25-06828\" ref-type=\"bibr\">10</xref>] proposed a navigation line calculation method based on Mask R-CNN and a tree row line calculation method based on the random sample consensus method.</p><p>The above research obtained navigation line information by directly extracting the boundary of the drivable area or by fitting tree row lines. It performed well in specific environments, but was susceptible to interference from factors such as varying tree shapes, complex environmental backgrounds, and occlusion of tree trunk and root points. Due to the low height and dense tree canopy of the fruit trees, it is difficult to identify the tree trunks. The road edges are irregular, making it difficult to stably generate road information. The method of fitting the tree row lines on both sides using the root points of the fruit trees is not suitable for complex orchard paths.</p><p>In view of the limitations of the existing methods, this paper proposes an orchard path navigation recognition method based on U-Net. By optimizing the network structure, the adaptability to environmental changes is enhanced. The downsampling part inserts the Spatial Attention mechanism, and the Coordinate Attention mechanism is added in the skip connection to obtain complete context information. It effectively deals with the complex orchard environment, and quickly and accurately extracts the drivable area of the orchard. The geometric center points are calculated row by row, and the center points are processed by sliding window and bidirectional interpolation filling. After that, spline interpolation is used to calculate and generate the navigation line, providing effective perception data for visual autonomous navigation in orchard environments.</p><p>The main contributions of this paper are summarized as follows:<list list-type=\"simple\"><list-item><label>(1)</label><p>A complete orchard dataset was constructed using Labelme to label drivable areas, and data augmentation techniques such as brightness adjustment and Gaussian blur were applied to enhance the robustness and generalization ability of the model.</p></list-item><list-item><label>(2)</label><p>An improved U-Net-based method for orchard navigation line recognition was designed by integrating the Spatial Attention mechanism in the downsampling stage and the Coordinate Attention mechanism in the skip connections, enabling the model to effectively extract key spatial features and fuse contextual information under complex orchard conditions.</p></list-item><list-item><label>(3)</label><p>A navigation line extraction process based on drivable area segmentation was developed, in which geometric center points are calculated row by row, followed by sliding window smoothing, bidirectional interpolation, and spline fitting to generate a continuous and smooth navigation path.</p></list-item><list-item><label>(4)</label><p>A multi-dimensional validation plan combining simulation and comparative experiments was implemented to evaluate the performance of the proposed method. The improved model achieved higher segmentation accuracy and navigation line precision than U-Net, SegViT, SE-Net, and DeepLabv3+, with an average navigation line error of only 56 mm, confirming its feasibility and practical value in orchard visual navigation.</p></list-item></list></p><p>The research content of this paper is as follows: <xref rid=\"sec1-sensors-25-06828\" ref-type=\"sec\">Section 1</xref> introduces the research background, the limitations of existing methods, and the research content of this study. <xref rid=\"sec2-sensors-25-06828\" ref-type=\"sec\">Section 2</xref> describes the creation of the dataset and the improved U-Net network structure. <xref rid=\"sec3-sensors-25-06828\" ref-type=\"sec\">Section 3</xref> presents the training and validation process of the model, as well as the comparative experiments on the drivable area and the verification experiments on the navigation line. <xref rid=\"sec4-sensors-25-06828\" ref-type=\"sec\">Section 4</xref> summarizes the results, limitations, and future research directions of the study.</p></sec><sec id=\"sec2-sensors-25-06828\"><title>2. Materials and Methods</title><p>The orchard path navigation recognition method proposed in this paper consists of two parts: drivable area segmentation and navigation line recognition. The drivable area segmentation is based on the U-Net fully convolutional neural network to identify the drivable area in the orchard and obtain the corresponding mask area. The navigation line recognition, based on the semantic segmentation results, uses preprocessing such as HSV conversion, morphological processing, Gaussian blur, and edge smoothing to extract contours and center points, and then performs interpolation optimization processing to recognize the orchard navigation lines.</p><sec id=\"sec2dot1-sensors-25-06828\"><title>2.1. Image Acquisition and Dataset Creation</title><p>The data collection site is located in Yantai City, Shandong Province, China. The weather was sunny and the sunlight was abundant, which ensured the quality of the data collection. The data set was collected using the LZY 604 wheeled tractor (Luzhong Tractor Co., Ltd., Weifang, Shandong, China) as the test platform. The AR monocular camera produced by Leopard Imaging Company (Fremont, CA, USA) was installed on the tractor. The installation position of the camera is shown in <xref rid=\"sensors-25-06828-f001\" ref-type=\"fig\">Figure 1</xref>a. The collection angle of the camera is shown in <xref rid=\"sensors-25-06828-f001\" ref-type=\"fig\">Figure 1</xref>b. The tractor moved at a speed of 1.8 m/s in the orchard to obtain the video of the orchard roads.</p><p>The obtained video of the orchard was extracted into 1847 frames through the video frame extraction method to create a dataset. The dataset was divided into a training set and a validation set in a 9:1 ratio. The drivable area was labeled using Labelme [<xref rid=\"B11-sensors-25-06828\" ref-type=\"bibr\">11</xref>], and the labels were divided into two categories: the drivable area in the orchard and the background. The drivable area in the orchard was labeled, while the background was not labeled. To expand the data volume of the training samples and improve the accuracy and generalization ability of the model, before training the dataset, this paper used the ImageAug library to perform random data augmentation on the training set images, such as horizontal flipping, adjustment of brightness and saturation, and Gaussian blur, in order to improve the model&#8217;s generalization ability to the orchard environment. The data augmentation is shown in <xref rid=\"sensors-25-06828-f002\" ref-type=\"fig\">Figure 2</xref>.</p></sec><sec id=\"sec2dot2-sensors-25-06828\"><title>2.2. Apparatus for Measuring the Usable Area of Orchards Based on U-Net</title><p>U-Net is an end-to-end image segmentation model based on convolutional neural networks [<xref rid=\"B12-sensors-25-06828\" ref-type=\"bibr\">12</xref>], consisting of an encoder for extracting feature information and a decoder for reconstructing the feature images and generating segmented images. The original U-Net network is shown in <xref rid=\"sensors-25-06828-f003\" ref-type=\"fig\">Figure 3</xref>a. The improved U-Net network retains the encoder and decoder structures and skip connections of the original U-Net, and adds spatial attention mechanisms during downsampling, and coordinate attention mechanisms during skip connections. The overall network structure of the improved U-Net is shown in <xref rid=\"sensors-25-06828-f003\" ref-type=\"fig\">Figure 3</xref>b. Traditional models process images uniformly and do not distinguish the importance of regions, ignoring some regions that contain more critical information, resulting in limited performance [<xref rid=\"B13-sensors-25-06828\" ref-type=\"bibr\">13</xref>].</p><sec id=\"sec2dot2dot1-sensors-25-06828\"><title>2.2.1. Detailed Network Structure Design</title><p>The improved U-Net consists of an encoder, decoder, and skip connections. Each stage of the encoder includes two 3 &#215; 3 convolution layers with ReLU activation and a 2 &#215; 2 max pooling operation for downsampling. The number of feature channels is set to 64, 128, 256, 512, and 1024 from shallow to deep layers. This design follows the principle of gradually increasing feature maps to capture higher-level semantic information while reducing spatial resolution, which balances model complexity and feature richness.</p><p>In the decoder, each stage performs 2&#215; upsampling followed by two 3 &#215; 3 convolution layers and concatenation with the corresponding encoder features. The number of feature channels decreases symmetrically (512, 256, 128, 64) to progressively recover spatial details.</p><p>The Spatial Attention module is inserted after each encoder stage to emphasize critical spatial regions and suppress background noise. The Coordinate Attention module is added to each skip connection to embed position-sensitive information into channel attention, enhancing feature fusion between encoder and decoder without significantly increasing computation.</p></sec><sec id=\"sec2dot2dot2-sensors-25-06828\"><title>2.2.2. Spatial Attention Mechanism</title><p>When traditional models process images, they often uniformly treat the entire image without distinguishing the importance of different regions. This uniform processing approach ignores the fact that certain regions in the image may contain more crucial information, thereby limiting the performance of the model when processing images. The Spatial Attention mechanism emphasizes the regions that contribute the most to the task by weighting specific spatial positions in the feature map [<xref rid=\"B14-sensors-25-06828\" ref-type=\"bibr\">14</xref>], while suppressing irrelevant or redundant regions, thereby improving the model&#8217;s performance. The structure of the Spatial Attention mechanism is shown in <xref rid=\"sensors-25-06828-f004\" ref-type=\"fig\">Figure 4</xref>. Traditional models may lose local key information during downsampling. The Spatial Attention mechanism, during downsampling, generates a spatial weight map to enable the model to enhance the features of important regions and weaken the influence of irrelevant regions, thus retaining core information while reducing the size.</p><p>First, perform global max pooling and global average pooling on the input feature map F with dimensions H &#215; W &#215; C, resulting in two feature maps of size H &#215; W &#215; 1. Then, concatenate the results of global max pooling and global average pooling along the channels to obtain a feature map of size H &#215; W &#215; 2. Finally, perform convolution on the concatenated result to obtain a feature map of size H &#215; W &#215; 1, and apply the Sigmoid activation function to obtain the spatial attention weight matrix Ms. The calculation formula for spatial attention is as shown in Equation (4).<disp-formula id=\"FD1-sensors-25-06828\"><label>(1)</label><mml:math id=\"mm1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-sensors-25-06828\"><label>(2)</label><mml:math id=\"mm2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD3-sensors-25-06828\"><label>(3)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD4-sensors-25-06828\"><label>(4)</label><mml:math id=\"mm4\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:mi>AvgPool</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>;</mml:mo><mml:mi>MaxPool</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>avg</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id=\"mm5\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>avg</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the result of performing global average pooling on the input features, <inline-formula><mml:math id=\"mm6\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the result of performing global max pooling on the input features, <inline-formula><mml:math id=\"mm7\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the Sigmoid activation function, and <inline-formula><mml:math id=\"mm8\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the 7 &#215; 7 convolution operation.</p></sec><sec id=\"sec2dot2dot3-sensors-25-06828\"><title>2.2.3. Coordinate Attention Mechanism</title><p>The Coordinate Attention Mechanism is achieved by performing average pooling in both the horizontal and vertical directions [<xref rid=\"B15-sensors-25-06828\" ref-type=\"bibr\">15</xref>]. It not only captures information across channels but also captures direction-aware and position-sensitive information, which helps the model to more accurately locate and identify the objects of interest. The structure of the Coordinate Attention Mechanism is shown in <xref rid=\"sensors-25-06828-f005\" ref-type=\"fig\">Figure 5</xref>.</p><p>CA first divides the input feature map into two directions: height and width, and performs global average pooling separately in each direction to obtain a vertical feature vector <inline-formula><mml:math id=\"mm9\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> of size C &#215; H &#215; 1, as well as a horizontal feature vector <inline-formula><mml:math id=\"mm10\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> of size C &#215; 1 &#215; W, as shown in Equations (5) and (6).<disp-formula id=\"FD5-sensors-25-06828\"><label>(5)</label><mml:math id=\"mm11\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD6-sensors-25-06828\"><label>(6)</label><mml:math id=\"mm12\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munder><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#8804;</mml:mo><mml:mi>f</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Then, the two pooled feature maps are concatenated along the channel dimension to obtain a feature map with dimensions of C &#215; 1 &#215; (W + H). A 1 &#215; 1 convolution is applied to the concatenated feature map for dimensionality reduction, as shown in Equation (7).<disp-formula id=\"FD7-sensors-25-06828\"><label>(7)</label><mml:math id=\"mm13\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8727;</mml:mo><mml:mi>Y</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, the convolution kernels <inline-formula><mml:math id=\"mm14\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant=\"normal\">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#215;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle=\"yes\">r</italic> are the scaling factors, and <inline-formula><mml:math id=\"mm15\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#8727;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the convolution operation.</p><p>Then, normalization processing is carried out, and the formula is as shown in Equation (8).<disp-formula id=\"FD8-sensors-25-06828\"><label>(8)</label><mml:math id=\"mm16\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant=\"bold-italic\">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">b</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant=\"normal\">BatchNorm</mml:mi><mml:mfenced open=\"(\" close=\")\" separators=\"|\"><mml:mi mathvariant=\"bold-italic\">Z</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#947;</mml:mi><mml:mo>&#8901;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi mathvariant=\"bold-italic\">Z</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>&#1013;</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mi>&#946;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id=\"mm17\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the mean difference, <inline-formula><mml:math id=\"mm18\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the variance, <inline-formula><mml:math id=\"mm19\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#947;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm20\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#946;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the learnable parameters, and <inline-formula><mml:math id=\"mm21\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#1013;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the small constant to avoid division by zero.</p><p>Meanwhile, a nonlinear function h-swish is introduced to enhance the model&#8217;s expressive power. The expression of the h-swish function is shown in Equation (9).<disp-formula id=\"FD9-sensors-25-06828\"><label>(9)</label><mml:math id=\"mm22\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">h</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi mathvariant=\"normal\">swish</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8901;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mi mathvariant=\"italic\">max</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>3,0</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the feature maps that have undergone normalization and nonlinear activation are re-segmented into two branches in the vertical and horizontal directions. Then, the outputs of these two branches are respectively applied with the Sigmoid activation function to compress the output values to the range of 0&#8211;1. Subsequently, the obtained two attention weight maps are respectively multiplied with the original input feature maps in the corresponding dimensions. This achieves the weighted operation on the original feature map, thereby enhancing the important features and suppressing the non-important ones.</p></sec></sec><sec id=\"sec2dot3-sensors-25-06828\"><title>2.3. Navigation Line Calculation Method</title><p>The existing methods for calculating navigation lines mostly employ Hough transform and vertical projection for linear fitting, which are suitable for structured road sections [<xref rid=\"B16-sensors-25-06828\" ref-type=\"bibr\">16</xref>]. However, in orchard environments, the road edges are irregular, and directly calculating navigation lines based on the segmentation results will result in significant errors. In this paper, firstly, the image undergoes HSV conversion, morphological processing, Gaussian blurring, and edge smoothing for preprocessing. Then, the contours and center points are calculated. Finally, interpolation optimization processing is carried out. The flowchart of navigation line calculation is shown in <xref rid=\"sensors-25-06828-f006\" ref-type=\"fig\">Figure 6</xref>.</p><sec id=\"sec2dot3dot1-sensors-25-06828\"><title>2.3.1. Image Preprocessing</title><p>Image preprocessing lays the foundation for the subsequent calculation of navigation lines, mainly including HSV conversion, morphological processing, and Gaussian blur for edge smoothing operations.</p><list list-type=\"order\"><list-item><p>HSV Conversion</p></list-item></list><p>In the field of computer vision, the RGB color space is intuitive but is greatly affected by changes in lighting, which is not conducive to the subsequent calculation of navigation lines. Through the HSV conversion, the coupling problem between the RGB channels can be avoided. The HSV conversion is to convert the color space of the image from RGB to HSV. The HSV color space is more in line with human perception of colors [<xref rid=\"B17-sensors-25-06828\" ref-type=\"bibr\">17</xref>]. When <inline-formula><mml:math id=\"mm23\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the hue H is 0&#176;. When the value of <inline-formula><mml:math id=\"mm24\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is equal to <inline-formula><mml:math id=\"mm25\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm26\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id=\"mm27\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, the hue H calculation formulas are as shown in Equations (10)&#8211;(12). When <inline-formula><mml:math id=\"mm28\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8800;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the saturation S calculation is as shown in Equation (13). When <inline-formula><mml:math id=\"mm29\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the saturation S = 0. The calculation formula for luminance V is as shown in Equation (14).<disp-formula id=\"FD10-sensors-25-06828\"><label>(10)</label><mml:math id=\"mm30\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn><mml:mo>&#176;</mml:mo><mml:mo>&#215;</mml:mo><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mn>6</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD11-sensors-25-06828\"><label>(11)</label><mml:math id=\"mm31\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn><mml:mo>&#176;</mml:mo><mml:mo>&#215;</mml:mo><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>B</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD12-sensors-25-06828\"><label>(12)</label><mml:math id=\"mm32\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn><mml:mo>&#176;</mml:mo><mml:mo>&#215;</mml:mo><mml:mfenced open=\"[\" close=\"]\" separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD13-sensors-25-06828\"><label>(13)</label><mml:math id=\"mm33\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>&#8722;</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">min</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD14-sensors-25-06828\"><label>(14)</label><mml:math id=\"mm34\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">max</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>255</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><list list-type=\"simple\"><list-item><label>2.</label><p>Morphological Processing</p></list-item></list><p>After the HSV conversion, the image noise is removed through a combined morphological processing of dilation and erosion. The small noise in the image can be eliminated through the erosion operation, and the internal small holes can be filled through the dilation operation. In this paper, the image is processed using the opening operation, which is the sequence of erosion followed by dilation. The formula of the opening operation is shown in Equation (15).<disp-formula id=\"FD15-sensors-25-06828\"><label>(15)</label><mml:math id=\"mm35\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#8728;</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mo>&#160;</mml:mo><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>&#8712;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mo>&#160;</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators=\"|\"><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id=\"mm36\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the input image; B represents the structural element; <inline-formula><mml:math id=\"mm37\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>&#8728;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the opening operation; <inline-formula><mml:math id=\"mm38\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id=\"mm39\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represent the pixel coordinate offsets within B.</p><p>After the opening operation, smaller noise blocks can be eliminated, while the integrity of the operational area is maintained to facilitate subsequent operations.</p><list list-type=\"simple\"><list-item><label>3.</label><p>Gaussian Blur Smooths the Edges</p></list-item></list><p>Gaussian blurring is achieved by convolving the image with a Gaussian kernel. The Gaussian kernel function is shown in Equation (16).<disp-formula id=\"FD16-sensors-25-06828\"><label>(16)</label><mml:math id=\"mm40\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#960;</mml:mi><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id=\"mm41\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the degree of control.</p><p>After applying the Gaussian blur operation for edge smoothing, the edge transitions become more natural, avoiding misjudgment in centerline calculation due to sharp edges, laying the foundation for accurately obtaining the navigation line coordinates, and enhancing the stability and reliability of subsequent processing.</p></sec><sec id=\"sec2dot3dot2-sensors-25-06828\"><title>2.3.2. Calculation of Navigation Lines</title><p>After the image undergoes image preprocessing, the calculation of navigation lines requires precisely extracting the key information of the contour and processing the center point data [<xref rid=\"B18-sensors-25-06828\" ref-type=\"bibr\">18</xref>]. In this paper, Canny is used to detect the edge of the drivable area, calculate the boundary contour of the drivable area, and eliminate the background contour and interfering contours. The center points of the filtered target contours are calculated row by row. The formula for calculating the x-coordinate of the center point is as shown in Equation (17).<disp-formula id=\"FD17-sensors-25-06828\"><label>(17)</label><mml:math id=\"mm42\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id=\"mm43\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the horizontal coordinate of the center point of the drivable area in the row where the vertical coordinate of the image is y; <inline-formula><mml:math id=\"mm44\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the horizontal coordinate of the left boundary of the drivable area within this row; <inline-formula><mml:math id=\"mm45\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the horizontal coordinate of the right boundary of the drivable area within this row.</p><p>The initial center point sequence may be affected by noise, discontinuous contours, etc., and further filtering and optimization of abnormal points are required. The formulas for calculating the mean &#956; and standard deviation &#963; of the center point sequence are respectively shown in Equations (18) and (19).<disp-formula id=\"FD18-sensors-25-06828\"><label>(18)</label><mml:math id=\"mm46\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#956;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD19-sensors-25-06828\"><label>(19)</label><mml:math id=\"mm47\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#963;</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy=\"false\">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>ci</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle=\"yes\">m</italic> represents the initial number of center points.</p><p>Remove the abnormal center points that meet the <inline-formula><mml:math id=\"mm48\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:mi>&#956;</mml:mi><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>3</mml:mn><mml:mi>&#963;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> criteria, and retain the reasonably distributed center points. Construct a sliding window of size <inline-formula><mml:math id=\"mm49\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#969;</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and perform sliding average processing on the center point sequence. The formula for the smoothed coordinates of the <italic toggle=\"yes\">i</italic>-th center point within the window is shown in Equation (20).<disp-formula id=\"FD20-sensors-25-06828\"><label>(20)</label><mml:math id=\"mm50\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>&#8722;</mml:mo><mml:mo>&#8970;</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#8971;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>&#8970;</mml:mo><mml:mi>&#969;</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#8971;</mml:mo></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Among them, <inline-formula><mml:math id=\"mm51\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent=\"true\"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the output value of the <italic toggle=\"yes\">i</italic>-th center point after being smoothed by the sliding window; <inline-formula><mml:math id=\"mm52\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>&#969;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the size of the sliding window; <inline-formula><mml:math id=\"mm53\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the original sequence of center points; <italic toggle=\"yes\">i</italic> represents the current center point being processed; <italic toggle=\"yes\">j</italic> represents the center point being traversed within the window.</p><p>After being smoothed through the sliding window, the distribution of the center points becomes more uniform, reducing the influence of noise and providing a reliable foundation for subsequent processing.</p></sec><sec id=\"sec2dot3dot3-sensors-25-06828\"><title>2.3.3. Generation of Navigation Lines</title><p>Based on the optimized sequence of center points, continuous and smooth navigation lines are generated through interpolation fitting. If there are discontinuities in the center point sequence, bidirectional interpolation is used to fill the breakpoints. The interpolation formula is shown in Equation (21). The interpolation results are verified from two directions to ensure the continuity of coordinates at the breakpoints, and a complete sequence of center points is obtained.<disp-formula id=\"FD21-sensors-25-06828\"><label>(21)</label><mml:math id=\"mm54\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy=\"false\">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy=\"false\">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo stretchy=\"false\">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, (<inline-formula><mml:math id=\"mm55\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) and (<inline-formula><mml:math id=\"mm56\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) represent the adjacent valid center points at the discontinuity points.</p><p>To improve the accuracy of the navigation line fitting, based on the original central point&#8217;s vertical coordinate, a dense Y coordinate sequence <inline-formula><mml:math id=\"mm57\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant=\"sans-serif\">&#916;</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant=\"sans-serif\">&#916;</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was generated with a fixed step size, and corresponding horizontal coordinates were supplemented to prepare for spline interpolation. Using cubic spline interpolation, the interpolation function was set as <inline-formula><mml:math id=\"mm58\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which satisfied <inline-formula><mml:math id=\"mm59\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> within the region <inline-formula><mml:math id=\"mm60\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and met the boundary conditions. By solving the spline coefficients <inline-formula><mml:math id=\"mm61\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the horizontal coordinates corresponding to the dense Y coordinates were interpolated to obtain a smooth X coordinate sequence <inline-formula><mml:math id=\"mm62\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant=\"sans-serif\">&#916;</mml:mi><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>'</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The dense coordinate pairs obtained from the interpolation (denoted as <inline-formula><mml:math id=\"mm63\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mfenced open=\"{\" close=\"}\" separators=\"|\"><mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where k represents the number of dense coordinates) are the navigation line coordinates of the exercisable area.</p></sec></sec></sec><sec id=\"sec3-sensors-25-06828\"><title>3. Experiments and Result Analysis</title><p>Improving the U-Net network under the Windows 10 operating system, with a GPU (Quadro P2200) and the code training platform being Pycharm 2020.1. The specific experimental environment is shown in <xref rid=\"sensors-25-06828-t001\" ref-type=\"table\">Table 1</xref>.</p><sec id=\"sec3dot1-sensors-25-06828\"><title>3.1. Evaluation Index</title><p>This study comprehensively evaluated the network model using multiple quantitative indicators: Recall, Precision, mPA, and mIoU. These indicators measure the performance of the model from different perspectives. Recall reflects the model&#8217;s ability to identify positive samples of various categories [<xref rid=\"B19-sensors-25-06828\" ref-type=\"bibr\">19</xref>], and the calculation formula is shown in Equation (22). Precision indicates the accuracy of the predicted samples [<xref rid=\"B20-sensors-25-06828\" ref-type=\"bibr\">20</xref>], and the calculation formula is shown in Equation (23). Both of these are evaluated from the basic classification layer. mPA and mIoU focus on the characteristics of the segmentation task. mPA is the average classification accuracy of the model across all categories, measuring the consistency and accuracy of classification across various categories, and the calculation formula is shown in Formula (24). mIoU is the average value of the overlap between the predicted regions and the true regions for all categories [<xref rid=\"B21-sensors-25-06828\" ref-type=\"bibr\">21</xref>], reflecting the overall segmentation accuracy of the model, and the calculation formula is shown in Formula (25). Inference speed represents the number of inferences the model can complete per second, directly reflecting the model&#8217;s efficiency in practical application scenarios, especially in real-time processing tasks. Number of parameters indicates the scale of the model in millions, which is closely related to the model&#8217;s complexity, storage requirements, and computational cost.</p><p>Through the synergy of these six indicators, both the segmentation performance, the efficiency of the model during inference, and the complexity reflected by the number of parameters are considered, ensuring the feasibility and effectiveness of the model in practice.<disp-formula id=\"FD22-sensors-25-06828\"><label>(22)</label><mml:math id=\"mm64\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">R</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">c</mml:mi><mml:mi mathvariant=\"normal\">a</mml:mi><mml:mi mathvariant=\"normal\">l</mml:mi><mml:mi mathvariant=\"normal\">l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD23-sensors-25-06828\"><label>(23)</label><mml:math id=\"mm65\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">r</mml:mi><mml:mi mathvariant=\"normal\">e</mml:mi><mml:mi mathvariant=\"normal\">c</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">s</mml:mi><mml:mi mathvariant=\"normal\">i</mml:mi><mml:mi mathvariant=\"normal\">o</mml:mi><mml:mi mathvariant=\"normal\">n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD24-sensors-25-06828\"><label>(24)</label><mml:math id=\"mm66\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mi mathvariant=\"normal\">A</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD25-sensors-25-06828\"><label>(25)</label><mml:math id=\"mm67\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi mathvariant=\"normal\">m</mml:mi><mml:mi mathvariant=\"normal\">I</mml:mi><mml:mi mathvariant=\"normal\">o</mml:mi><mml:mi mathvariant=\"normal\">U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy=\"true\">&#8721;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#160;</mml:mo></mml:mrow></mml:mrow><mml:mfenced separators=\"|\"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#215;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <italic toggle=\"yes\">k</italic> represents the total number of categories including the background in the class; <italic toggle=\"yes\">i</italic> represents the drivable area category, and <italic toggle=\"yes\">j</italic> represents the non-drivable area category; <inline-formula><mml:math id=\"mm68\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of pixels that are actually and predictively classified as category <italic toggle=\"yes\">i</italic>; <inline-formula><mml:math id=\"mm69\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of pixels that are actually classified as category <italic toggle=\"yes\">i</italic> but predicted as category <italic toggle=\"yes\">j</italic>; <inline-formula><mml:math id=\"mm70\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of pixels that are actually classified as category <italic toggle=\"yes\">j</italic> but predicted as category <italic toggle=\"yes\">i</italic>.</p></sec><sec id=\"sec3dot2-sensors-25-06828\"><title>3.2. Model Training</title><p>During the model training process, the loss value is a key indicator for measuring the learning effect of the model. By monitoring the changes of training loss and validation loss with respect to the training epochs, one can intuitively understand the development trend of the model&#8217;s fitting and generalization capabilities. The curve of the loss value changing with the training epochs is shown in <xref rid=\"sensors-25-06828-f007\" ref-type=\"fig\">Figure 7</xref>.</p><p>Both the train loss and val loss decreased rapidly, indicating that in the initial training rounds, the model quickly learned the data patterns, reduced the prediction error, and significantly improved its fitting ability for the task. During the middle stage (Epoch 50&#8211;200): The loss decreased at a slower rate, and the fluctuations in the train loss became smaller. The val loss occasionally had small fluctuations (around 100 epochs), reflecting that the model was gradually approaching the optimal solution. At the same time, the data distribution in the validation set differed from that in the training set, causing fluctuations in the validation loss. In the later stage (Epoch 200&#8211;300): Both the train loss and val loss tended to stabilize. The values were close and the fluctuations were extremely small, indicating that the model had fully learned the data features and entered the &#8220;convergence plateau period&#8221;. From the smooth train loss and smooth val loss, it can be more clearly observed: The two smooth curves have a highly consistent trend, and eventually approached around 0.04, indicating that the model training process was stable and the convergence effect was good. The curve was flat in the later stage, proving that the model had learned stable feature representations.</p></sec><sec id=\"sec3dot3-sensors-25-06828\"><title>3.3. Model Performance Verification</title><p>To verify the completeness of the information on drivable areas and the accuracy of the navigation lines obtained by the method proposed in this paper in an orchard environment, comparative experiments on drivable area detection and verification experiments on navigation line calculation were conducted respectively.</p><sec id=\"sec3dot3dot1-sensors-25-06828\"><title>3.3.1. Comparison Validation</title><p>To verify the effectiveness of the model proposed in this paper, under the same configuration environment, the U-Net, SegViT, SE-Net and DeepLabv3+ deep learning network models were selected to conduct comparative experiments on the dataset of this paper. The performance indicators of each model are compared as shown in <xref rid=\"sensors-25-06828-t002\" ref-type=\"table\">Table 2</xref>.</p><p><xref rid=\"sensors-25-06828-f008\" ref-type=\"fig\">Figure 8</xref> shows the segmentation visualization results of each model on the test set. From <xref rid=\"sensors-25-06828-f008\" ref-type=\"fig\">Figure 8</xref>, it can be seen that most models have incomplete recognition of the drivable areas. The improved U-Net model has the best recognition effect, and can obtain more complete and accurate drivable areas in the orchard, with higher segmentation accuracy.</p><p>To present the comparison results more clearly, the model comparison radar chart is shown in <xref rid=\"sensors-25-06828-f009\" ref-type=\"fig\">Figure 9</xref>. The radar chart presents the values of the four different indicators corresponding to the models summarized in <xref rid=\"sensors-25-06828-t002\" ref-type=\"table\">Table 2</xref>. Each axis of the radar chart corresponds to one of the indicators. The shapes formed by the lines show the performance capabilities and shortcomings of each model in the multi-dimensional space. The value range of all indicators is between 0% and 100%. The closer the value is to 100%, the better the result. From <xref rid=\"sensors-25-06828-f009\" ref-type=\"fig\">Figure 9</xref>, it can be seen that the results of all models fall within a similar range. The improved U-Net method proposed in this paper demonstrates outstanding performance.</p></sec><sec id=\"sec3dot3dot2-sensors-25-06828\"><title>3.3.2. Navigation Line Verification</title><p>To meet the requirements of agricultural machinery and agricultural techniques. For large-scale and standardized orchards, the error of the navigation line needs to be controlled within &#177;50 to 80 mm. Otherwise, it may affect the connection of subsequent mechanized harvesting and other links. The error within a reasonable range can meet the safety operation requirements of the orchard. The comparison of the navigation line calculated and the navigation line observed manually is shown in <xref rid=\"sensors-25-06828-f010\" ref-type=\"fig\">Figure 10</xref>. The manually observed navigation line is used as the standard to judge the accuracy of the extraction of the orchard navigation line. The mean absolute error of the calculated navigation line compared with the manually observed navigation line is 56 mm, and the standard deviation of the error is 4.47 mm.</p><p>To further illustrate the effectiveness of the proposed method, <xref rid=\"sensors-25-06828-f011\" ref-type=\"fig\">Figure 11</xref> shows the visualization of the original image, the drivable area detection, and the corresponding navigation line calculation results.</p></sec></sec></sec><sec sec-type=\"discussion\" id=\"sec4-sensors-25-06828\"><title>4. Discussion</title><p>This study addresses the core issue of complex image backgrounds and numerous interfering factors in visual navigation of orchards. It proposes a method for segmented driving area and geometric center-based navigation path recognition based on the improved U-Net. The effectiveness and practicality of the method have been verified through experiments. The CA and SA modules are introduced on the basis of the traditional U-Net. The SA module is inserted in the downsampling stage to enhance spatial feature extraction, and the CA module is added in the skip connection stage to supplement context information. This achieves precise segmentation of complex orchard environments containing bare soil, obstructions, trees, and weeds. The experimental results show that the Recall of the improved model is 90.23%, the Precision is 91.71%, the mPA is 87.75%, and the mIoU is 84.84%. In the same environment, all indicators are superior to those of traditional U-Net, SegViT, SE-Net, and DeepLabv3+. This effectively reduces misjudgments caused by environmental interference and lays a high-quality segmentation foundation for navigation path recognition. Based on the segmented driving area results, the mask image is optimized through HSV conversion, morphological opening operation, Gaussian blur, etc. Combined with contour detection, geometric center calculation, 3&#963; outlier elimination, sliding window smoothing, and bidirectional interpolation filling, the generated continuous smooth navigation lines have an average distance error of only 56 mm from the actual center line of manual observation, fully meeting the agronomic requirements of &#177;50&#8211;80 mm for mechanized operations in large-scale and standardized orchards. It can ensure the safety and process connectivity of mechanized operations such as picking, spraying, and weeding in the orchard. Under the current orchard environment and hardware configuration, the method proposed in this study can provide effective technical references for the visual navigation task of agricultural machinery in orchards. However, there are still deficiencies, and further optimization is needed in subsequent work. There are limitations in environmental adaptability. The performance of the method in extreme environments has not been fully verified. The dynamic response of path planning is insufficient. This study did not consider dynamic interference factors in orchard operations. Based on the development trend of smart agriculture and the shortcomings of this study, subsequent research will be conducted in the following directions to further improve the orchard visual navigation technology system. Enhance multi-scenario adaptive ability. Expand experimental scenarios and datasets to ensure the stability of the method in different scenarios. Integrate dynamic path planning and multi-sensor data. Introduce sensors such as laser radar, millimeter-wave radar, etc., and combine visual data for multimodal fusion to achieve real-time adjustment of navigation lines and solve the problem of path avoidance under dynamic interference in orchards, improving the flexibility of autonomous operation of agricultural machinery.</p></sec><sec sec-type=\"conclusions\" id=\"sec5-sensors-25-06828\"><title>5. Conclusions</title><p>This study proposes an improved method based on U-Net for navigable area segmentation and geometric center-based navigation path recognition, which is applied to orchard visual navigation. The improved U-Net network emphasizes key regions and suppresses redundancy during downsampling by weighting specific spatial positions in the feature maps. This approach improves the network&#8217;s performance. In contrast, the traditional U-Net simply concatenates features during feature extraction. The improved U-Net, however, introduces the CA module without increasing model parameters or computational cost. This module effectively fuses the global information obtained during feature extraction, optimizing the feature restoration process of the drivable area in the orchard. As a result, it reduces environmental interference and further improves the overall segmentation accuracy of the model. Although this method provides an effective technical reference for the visual navigation of agricultural machinery in current conditions, it has limitations in environmental adaptability and dynamic path planning response. Future research will focus on enhancing multi-scenario adaptability by expanding experimental scenarios and datasets, and integrating dynamic path planning with multi-sensor data fusion to improve the flexibility of autonomous operation of agricultural machinery.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank all the authors for their contributions to the writing of this article.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology and writing&#8212;original draft preparation, N.X.; formal analysis and investigation, X.N.; resources, A.L. and Z.L.; data curation, Y.S.; validation, W.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id=\"B1-sensors-25-06828\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wei</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Siyi</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y.</given-names></name></person-group><article-title>A Navigation Path Recognition Method for Cross-row Tea-Picking Machines Based on Improved Unet</article-title><source>China Agric. Sci. Technol. Her.</source><year>2025</year><fpage>1</fpage><lpage>10</lpage><comment>(In Chinese and English)</comment><pub-id pub-id-type=\"doi\">10.13304/j.nykjdb.2024.0815</pub-id></element-citation></ref><ref id=\"B2-sensors-25-06828\"><label>2.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhenhui</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Xiaojuan</surname><given-names>L.</given-names></name></person-group><article-title>Research on Orchard Tree Trunk Detection and Navigation Line Fitting Algorithm</article-title><source>Chin. J. Agric. Mach. Chem.</source><year>2024</year><volume>45</volume><fpage>217</fpage><lpage>222</lpage><pub-id pub-id-type=\"doi\">10.13733/j.jcam.issn.2095-5553.2024.08.031</pub-id></element-citation></ref><ref id=\"B3-sensors-25-06828\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Tan</surname><given-names>D.N.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Yao</surname><given-names>L.B.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>Z.R.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>X.Q.</given-names></name></person-group><article-title>Semantic Segmentation of Multi-source Remote Sensing Images Based on Visual Attention Mechanism</article-title><source>Signal Process.</source><year>2022</year><volume>38</volume><fpage>1180</fpage><lpage>1191</lpage><pub-id pub-id-type=\"doi\">10.16798/j.issn.1003-0530.2022.06.005</pub-id></element-citation></ref><ref id=\"B4-sensors-25-06828\"><label>4.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Bingyue</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Fenglei</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Chongke</surname><given-names>B.</given-names></name></person-group><article-title>Segmentation of Usable Areas in Unstructured Road Scenarios</article-title><source>Optoelectron. Laser</source><year>2025</year><fpage>1</fpage><lpage>10</lpage><comment>CN 12-1182/O4</comment><issn>1005-0086</issn></element-citation></ref><ref id=\"B5-sensors-25-06828\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Anoop</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Deivanathan</surname><given-names>R.</given-names></name></person-group><article-title>Real-Time image enhancement following road scenario classification using deep learning</article-title><source>Measurement</source><year>2025</year><volume>251</volume><fpage>117192</fpage><pub-id pub-id-type=\"doi\">10.1016/j.measurement.2025.117192</pub-id></element-citation></ref><ref id=\"B6-sensors-25-06828\"><label>6.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Viadero-Monasterio</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Nguyen</surname><given-names>A.-T.</given-names></name><name name-style=\"western\"><surname>Lauber</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Boada</surname><given-names>M.J.L.</given-names></name><name name-style=\"western\"><surname>Boada</surname><given-names>B.L.</given-names></name></person-group><article-title>Event-Triggered Robust Path Tracking Control Considering Roll Stability Under Network-Induced Delays for Autonomous Vehicles</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>14743</fpage><lpage>14756</lpage><pub-id pub-id-type=\"doi\">10.1109/TITS.2023.3321415</pub-id></element-citation></ref><ref id=\"B7-sensors-25-06828\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Viadero-Monasterio</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Mel&#233;ndez-Useros</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Boada</surname><given-names>B.L.</given-names></name><name name-style=\"western\"><surname>Boada</surname><given-names>M.J.L.</given-names></name></person-group><article-title>Motion Planning and Robust Output-Feedback Trajectory Tracking Control for Multiple Intelligent and Connected Vehicles in Unsignalized Intersections</article-title><source>IEEE Trans. Veh. Technol.</source><year>2025</year><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type=\"doi\">10.1109/TVT.2025.3586769</pub-id></element-citation></ref><ref id=\"B8-sensors-25-06828\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Chi</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Ziyang</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Zhijun</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Yue</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Fuli</surname><given-names>S.</given-names></name></person-group><article-title>Research on the Generation Method of Intercrop Navigation Lines in Kiwifruit Orchards Based on Root Point Replacement</article-title><source>Arid. Land Agric. Res.</source><year>2021</year><volume>39</volume><fpage>222</fpage><lpage>230</lpage><pub-id pub-id-type=\"doi\">10.7606/j.issn.1000-7601.2021.05.29</pub-id></element-citation></ref><ref id=\"B9-sensors-25-06828\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Shubo</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Bingqi</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Jingbin</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Pengxuan</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Xiangye</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Xin</surname><given-names>F.</given-names></name><name name-style=\"western\"><surname>Hongtao</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Xionchu</surname><given-names>Z.</given-names></name></person-group><article-title>Orchard Row Navigation Line Detection Based on Improved YOLOv7</article-title><source>Tzransactions Chin. Soc. Agric. Eng.</source><year>2023</year><volume>39</volume><fpage>131</fpage><lpage>138</lpage><pub-id pub-id-type=\"doi\">10.11975/j.issn.1002-6819.202305207</pub-id></element-citation></ref><ref id=\"B10-sensors-25-06828\"><label>10.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ke</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Weiguang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Congzhe</surname><given-names>L.</given-names></name></person-group><article-title>Algorithm for Extracting Visual Navigation Paths in Orchard Environments under Complex Conditions</article-title><source>Trans. Chin. Soc. Agric. Mach.</source><year>2023</year><volume>54</volume><fpage>197</fpage><lpage>252</lpage><page-range>197&#8211;204+252</page-range><pub-id pub-id-type=\"doi\">10.6041/j.issn.1000-1298.2023.06.020</pub-id></element-citation></ref><ref id=\"B11-sensors-25-06828\"><label>11.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Liang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Zhonghua</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Jiwei</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Yali</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Xiaomei</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wentu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>L.</given-names></name></person-group><article-title>Detection Method of Potato Harvesting Guidance Line Based on Machine Vision</article-title><source>Agric. Mech. Res.</source><year>2025</year><volume>47</volume><fpage>22</fpage><lpage>34</lpage><page-range>22&#8211;27+34</page-range><pub-id pub-id-type=\"doi\">10.13427/j.issn.1003-188X.2025.09.004</pub-id></element-citation></ref><ref id=\"B12-sensors-25-06828\"><label>12.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wenhui</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Chuanqi</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Yan</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Yewei</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Kuan</surname><given-names>Q.</given-names></name></person-group><article-title>Fruit Row Interval Path Recognition Method Based on Lightweight U-Net Network</article-title><source>Trans. Chin. Soc. Agric. Mach.</source><year>2024</year><volume>55</volume><fpage>16</fpage><lpage>27</lpage><pub-id pub-id-type=\"doi\">10.6041/j.issn.1000-1298.2024.02.002</pub-id></element-citation></ref><ref id=\"B13-sensors-25-06828\"><label>13.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Xuewei</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Shaohua</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Xiao</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Jinjin</surname><given-names>Z.</given-names></name></person-group><article-title>A Non-Structured Road Usable Area Recommendation Model Based on M-shaped Deep Architecture</article-title><source>Chin. J. Highw. Transp.</source><year>2022</year><volume>35</volume><fpage>205</fpage><lpage>218</lpage><pub-id pub-id-type=\"doi\">10.19721/j.cnki.1001-7372.2022.12.017</pub-id></element-citation></ref><ref id=\"B14-sensors-25-06828\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kong</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Guo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Liang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Hong</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Xue</surname><given-names>W.</given-names></name></person-group><article-title>A method for recognizing inter-row navigation lines of rice heading stage based on improved ENet network</article-title><source>Measurement</source><year>2025</year><volume>241</volume><fpage>115677</fpage><pub-id pub-id-type=\"doi\">10.1016/j.measurement.2024.115677</pub-id></element-citation></ref><ref id=\"B15-sensors-25-06828\"><label>15.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saeedizadeh</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Jalali</surname><given-names>S.M.J.</given-names></name><name name-style=\"western\"><surname>Khan</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Kebria</surname><given-names>P.M.</given-names></name><name name-style=\"western\"><surname>Mohamed</surname><given-names>S.</given-names></name></person-group><article-title>A new optimization approach based on neural architecture search to enhance deep U-Net for efficient road segmentation</article-title><source>Knowl. Based Syst.</source><year>2024</year><volume>296</volume><fpage>111966</fpage><pub-id pub-id-type=\"doi\">10.1016/j.knosys.2024.111966</pub-id></element-citation></ref><ref id=\"B16-sensors-25-06828\"><label>16.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gong</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhuang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>Improving the maize crop row navigation line recognition method of YOLOX</article-title><source>Front. Plant Sci.</source><year>2024</year><volume>15</volume><elocation-id>1338228</elocation-id><pub-id pub-id-type=\"doi\">10.3389/fpls.2024.1338228</pub-id><pub-id pub-id-type=\"pmid\">38606066</pub-id><pub-id pub-id-type=\"pmcid\">PMC11008721</pub-id></element-citation></ref><ref id=\"B17-sensors-25-06828\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Yu</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Shu</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>209</volume><fpage>107811</fpage><issn>0168-1699</issn><pub-id pub-id-type=\"doi\">10.1016/j.compag.2023.107811</pub-id></element-citation></ref><ref id=\"B18-sensors-25-06828\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Luo</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>Instance Segmentation of Tea Garden Roads Based on an Improved YOLOv8n-seg Model</article-title><source>Agriculture</source><year>2024</year><volume>14</volume><elocation-id>1163</elocation-id><pub-id pub-id-type=\"doi\">10.3390/agriculture14071163</pub-id></element-citation></ref><ref id=\"B19-sensors-25-06828\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Gong</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Zhuang</surname><given-names>W.</given-names></name></person-group><article-title>Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology</article-title><source>Agriculture</source><year>2024</year><volume>14</volume><elocation-id>124</elocation-id><pub-id pub-id-type=\"doi\">10.3390/agriculture14010124</pub-id></element-citation></ref><ref id=\"B20-sensors-25-06828\"><label>20.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Jianjun</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Siyuan</surname><given-names>G.</given-names></name><name name-style=\"western\"><surname>Quan</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Man</surname><given-names>Z.</given-names></name></person-group><article-title>A Deep-Learning Extraction Method for Orchard Visual Navigation Lines</article-title><source>Agriculture</source><year>2022</year><volume>12</volume><elocation-id>1650</elocation-id><pub-id pub-id-type=\"doi\">10.3390/agriculture12101650</pub-id></element-citation></ref><ref id=\"B21-sensors-25-06828\"><label>21.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Maoyong</surname><given-names>C.</given-names></name><name name-style=\"western\"><surname>Fangfang</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Fengying</surname><given-names>M.</given-names></name></person-group><article-title>Improved Real-Time Semantic Segmentation Network Model for Crop Vision Navigation Line Detection</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>13</volume><elocation-id>898131</elocation-id><pub-id pub-id-type=\"doi\">10.3389/fpls.2022.898131</pub-id><pub-id pub-id-type=\"pmid\">35720554</pub-id><pub-id pub-id-type=\"pmcid\">PMC9201824</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"sensors-25-06828-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>Dataset collection. (<bold>a</bold>) Camera installation position; (<bold>b</bold>) The acquisition angle of the camera.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g001.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Data augmentation.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g002.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Overall network structure. (<bold>a</bold>) Original U-Net network structure; (<bold>b</bold>) Improved U-Net network structure.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g003.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Structure of the spatial attention mechanism.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g004.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Structure of the Coordinate Attention Mechanism.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g005.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Flowchart of route calculation process.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g006.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Curve showing the variation of loss value with the number of training rounds.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g007.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f008\" orientation=\"portrait\"><label>Figure 8</label><caption><p>Comparison of visualization results.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g008.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f009\" orientation=\"portrait\"><label>Figure 9</label><caption><p>Model comparison radar chart.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g009.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f010\" orientation=\"portrait\"><label>Figure 10</label><caption><p>Comparison of calculated navigation line and manually observed navigation line. Note: The (yellow) line in the figure is the fitting navigation line, and the (blue) line is the manual observation navigation line.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g010.jpg\"/></fig><fig position=\"float\" id=\"sensors-25-06828-f011\" orientation=\"portrait\"><label>Figure 11</label><caption><p>Experimental results. (<bold>a</bold>) Original image; (<bold>b</bold>) Segmentation result of the drivable area; (<bold>c</bold>) Navigation line calculation result.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"sensors-25-06828-g011.jpg\"/></fig><table-wrap position=\"float\" id=\"sensors-25-06828-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06828-t001_Table 1</object-id><label>Table 1</label><caption><p>Specific Experimental Environment.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Item</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Content</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Operating System</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Windows 10</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">CPU</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Intel(R) Core(TM)i7-10700F CPU @ 2.90 GHZ (Intel&#174;: Santa Clara, CA, USA)</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">GPU</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">NVIDIA Quadro P2200 (NVIDIA: Santa Clara, CA, USA)</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Platform Environment</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Pycharm 2020.1, Pytorch 1.10.0, <break/>opencv 4.10.0, Cuda 10.1 with cudann</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"sensors-25-06828-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">sensors-25-06828-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of performance indicators for each model.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Method</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Recall/(%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Precision/(%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">mPA/(%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">mIoU/(%)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Inference<break/> Speed/(FPS)</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Number of <break/> Parameters/(M)</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">U-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">78.17</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.57</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">75.16</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">70.40</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">38.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">31.1</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SegViT</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">85.75</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.74</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.90</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">78.15</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">46.7</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">38.6</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SE-Net</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">80.06</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.19</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">77.12</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">73.36</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">37.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">36.7</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">DeepLabv3+</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.24</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">90.05</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">85.64</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.89</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">53.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">62.4</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Our</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">90.23</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">91.71</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">87.75</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">84.84</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">72.2</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">45.7</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Sensors (Basel) Sensors (Basel) 1660 sensors sensors Sensors (Basel, Switzerland) 1424-8220 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656680 PMC12656680.1 12656680 12656680 41305037 10.3390/s25226828 sensors-25-06828 1 Article Research on Orchard Navigation Line Recognition Method Based on U-Net Xu Ning Writing &#8211; original draft Methodology 1 2 3 Ning Xiangsen Formal analysis Investigation 4 https://orcid.org/0000-0001-9313-6107 Li Aijuan Resources 4 Li Zhihe Resources 1 * Song Yumin Data curation 4 Wu Wenxuan Validation 2 Baghdadi Nicolas Academic Editor 1 College of Agricultural Engineering and Food Science, Shandong University of Technology, Zibo 255000, China; xuning@saas.ac.cn 2 Shandong Academy of Agricultural Machinery Sciences, Jinan 252100, China; wuwenxuan@saas.ac.cn 3 Shandong Key Laboratory of Intelligent Agricultural Equipment in Hilly and Mountainous Areas, Jinan 250100, China 4 School of Automotive Engineering, Shandong Jiaotong University, Jinan 250357, China; 13933028705@163.com (X.N.); liaijuan@sdjtu.edu.cn (A.L.); songyumin@sdjtu.edu.cn (Y.S.) * Correspondence: lizhihe@sdut.edu.cn 07 11 2025 11 2025 25 22 501335 6828 23 9 2025 22 10 2025 05 11 2025 07 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). Aiming at the problems of complex image background and numerous interference factors faced by visual navigation systems in orchard environments, this paper proposes an orchard navigation line recognition method based on U-Net. Firstly, the drivable areas in the collected images are labeled using Labelme (a graphical tool for image annotation) to create an orchard dataset. Then, the Spatial Attention (SA) mechanism is inserted into the downsampling stage of the traditional U-Net semantic segmentation method, and the Coordinate Attention (CA) mechanism is added to the skip connection stage to obtain complete context information and optimize the feature restoration process of the drivable area in the field, thereby improving the overall segmentation accuracy of the model. Subsequently, the improved U-Net network is trained using the enhanced dataset to obtain the drivable area segmentation model. Based on the detected drivable area segmentation mask, the navigation line information is extracted, and the geometric center points are calculated row by row. After performing sliding window processing and bidirectional interpolation filling on the center points, the navigation line is generated through spline interpolation. Finally, the proposed method is compared and verified with U-Net, SegViT, SE-Net, and DeepLabv3+ networks. The results show that the improved drivable area segmentation model has a Recall of 90.23%, a Precision of 91.71%, a mean pixel accuracy (mPA) of 87.75%, and a mean intersection over union (mIoU) of 84.84%. Moreover, when comparing the recognized navigation line with the actual center line, the average distance error of the extracted navigation line is 56 mm, which can provide an effective reference for visual autonomous navigation in orchard environments. orchard environment navigation line U-Net network attention mechanism divisible driving area Shandong Province Key Research and Development Plan (Major Science and Technology Innovation Project) 2022CXGC020706 Shandong Province Agricultural Mechanization R&amp;D, Manufacturing, Promotion and Application Integration Pilot Project NJYTHSD-202315 MIIT Major agricultural machinery equipment power system key components project 2023ZY02002 Shandong Province Science and Technology oriented Small and Medium Enterprises Enhancement Project 2023TSGC0288 Jinan 2023 talent development special fund research leader studio project 202333067 This research was funded by Shandong Province Key Research and Development Plan (Major Science and Technology Innovation Project) (Grant No. 2022CXGC020706). Shandong Province Agricultural Mechanization R&amp;D, Manufacturing, Promotion and Application Integration Pilot Project (Grant No. NJYTHSD-202315). MIIT Major agricultural machinery equipment power system key components project (Grant No. 2023ZY02002). Shandong Province Science and Technology oriented Small and Medium Enterprises Enhancement Project (Grant No. 2023TSGC0288). Jinan 2023 talent development special fund research leader studio project (Grant No. 202333067). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction Navigation line recognition, as one of the key technologies for autonomous driving of agricultural tractors, is widely applied in tasks such as picking, spraying, and weeding [ 1 , 2 , 3 ], which can enhance agricultural productivity and address the issue of labor shortage. Common navigation techniques include LiDAR navigation, GNSS (Global Navigation Satellite System), and machine vision navigation [ 4 ]. The orchard environment is typically characterized by dense planting conditions. The canopy can block satellite positioning signals, leading to inaccurate positioning information [ 5 ]. LiDAR acquires point cloud information of the surrounding environment through beam scanning, but it has problems such as excessive redundant information, difficulty in feature extraction, and high equipment costs. Compared with LiDAR-based GNSS positioning technology, visual navigation has the advantages of rich semantic information, wide detection range, and controllable cost, making it one of the current research hotspots both at home and abroad. Moreover, navigation line recognition serves as a fundamental step for path tracking control, directly influencing the accuracy and stability of autonomous navigation [ 6 ]. Reliable line detection provides essential geometric and positional information for subsequent trajectory tracking and motion planning, which are critical for safe and efficient autonomous operation. Therefore, improvements in line recognition can contribute to enhancing overall control performance [ 7 ]. At present, the machine vision navigation in orchards mostly relies on the structure of the orchard and the characteristics of the trees to calculate the navigation lines by fitting the tree row lines with the tree root points. Ma Chi et al. [ 8 ] proposed a navigation feature target detection method based on deep learning, where the bottom center point of the target box replaces the tree root points. The tree row lines are extracted using cubic spline interpolation, and the navigation lines are generated by fitting using the least squares method. Shubo et al. [ 9 ] takes the midpoint of the tree trunk root as the navigation positioning base point, uses the improved YOLOv7 model to obtain the positioning reference points for the tree row lines on both sides, and then fits the tree row lines and the navigation lines using the least squares method. Xiao Ke et al. [ 10 ] proposed a navigation line calculation method based on Mask R-CNN and a tree row line calculation method based on the random sample consensus method. The above research obtained navigation line information by directly extracting the boundary of the drivable area or by fitting tree row lines. It performed well in specific environments, but was susceptible to interference from factors such as varying tree shapes, complex environmental backgrounds, and occlusion of tree trunk and root points. Due to the low height and dense tree canopy of the fruit trees, it is difficult to identify the tree trunks. The road edges are irregular, making it difficult to stably generate road information. The method of fitting the tree row lines on both sides using the root points of the fruit trees is not suitable for complex orchard paths. In view of the limitations of the existing methods, this paper proposes an orchard path navigation recognition method based on U-Net. By optimizing the network structure, the adaptability to environmental changes is enhanced. The downsampling part inserts the Spatial Attention mechanism, and the Coordinate Attention mechanism is added in the skip connection to obtain complete context information. It effectively deals with the complex orchard environment, and quickly and accurately extracts the drivable area of the orchard. The geometric center points are calculated row by row, and the center points are processed by sliding window and bidirectional interpolation filling. After that, spline interpolation is used to calculate and generate the navigation line, providing effective perception data for visual autonomous navigation in orchard environments. The main contributions of this paper are summarized as follows: (1) A complete orchard dataset was constructed using Labelme to label drivable areas, and data augmentation techniques such as brightness adjustment and Gaussian blur were applied to enhance the robustness and generalization ability of the model. (2) An improved U-Net-based method for orchard navigation line recognition was designed by integrating the Spatial Attention mechanism in the downsampling stage and the Coordinate Attention mechanism in the skip connections, enabling the model to effectively extract key spatial features and fuse contextual information under complex orchard conditions. (3) A navigation line extraction process based on drivable area segmentation was developed, in which geometric center points are calculated row by row, followed by sliding window smoothing, bidirectional interpolation, and spline fitting to generate a continuous and smooth navigation path. (4) A multi-dimensional validation plan combining simulation and comparative experiments was implemented to evaluate the performance of the proposed method. The improved model achieved higher segmentation accuracy and navigation line precision than U-Net, SegViT, SE-Net, and DeepLabv3+, with an average navigation line error of only 56 mm, confirming its feasibility and practical value in orchard visual navigation. The research content of this paper is as follows: Section 1 introduces the research background, the limitations of existing methods, and the research content of this study. Section 2 describes the creation of the dataset and the improved U-Net network structure. Section 3 presents the training and validation process of the model, as well as the comparative experiments on the drivable area and the verification experiments on the navigation line. Section 4 summarizes the results, limitations, and future research directions of the study. 2. Materials and Methods The orchard path navigation recognition method proposed in this paper consists of two parts: drivable area segmentation and navigation line recognition. The drivable area segmentation is based on the U-Net fully convolutional neural network to identify the drivable area in the orchard and obtain the corresponding mask area. The navigation line recognition, based on the semantic segmentation results, uses preprocessing such as HSV conversion, morphological processing, Gaussian blur, and edge smoothing to extract contours and center points, and then performs interpolation optimization processing to recognize the orchard navigation lines. 2.1. Image Acquisition and Dataset Creation The data collection site is located in Yantai City, Shandong Province, China. The weather was sunny and the sunlight was abundant, which ensured the quality of the data collection. The data set was collected using the LZY 604 wheeled tractor (Luzhong Tractor Co., Ltd., Weifang, Shandong, China) as the test platform. The AR monocular camera produced by Leopard Imaging Company (Fremont, CA, USA) was installed on the tractor. The installation position of the camera is shown in Figure 1 a. The collection angle of the camera is shown in Figure 1 b. The tractor moved at a speed of 1.8 m/s in the orchard to obtain the video of the orchard roads. The obtained video of the orchard was extracted into 1847 frames through the video frame extraction method to create a dataset. The dataset was divided into a training set and a validation set in a 9:1 ratio. The drivable area was labeled using Labelme [ 11 ], and the labels were divided into two categories: the drivable area in the orchard and the background. The drivable area in the orchard was labeled, while the background was not labeled. To expand the data volume of the training samples and improve the accuracy and generalization ability of the model, before training the dataset, this paper used the ImageAug library to perform random data augmentation on the training set images, such as horizontal flipping, adjustment of brightness and saturation, and Gaussian blur, in order to improve the model&#8217;s generalization ability to the orchard environment. The data augmentation is shown in Figure 2 . 2.2. Apparatus for Measuring the Usable Area of Orchards Based on U-Net U-Net is an end-to-end image segmentation model based on convolutional neural networks [ 12 ], consisting of an encoder for extracting feature information and a decoder for reconstructing the feature images and generating segmented images. The original U-Net network is shown in Figure 3 a. The improved U-Net network retains the encoder and decoder structures and skip connections of the original U-Net, and adds spatial attention mechanisms during downsampling, and coordinate attention mechanisms during skip connections. The overall network structure of the improved U-Net is shown in Figure 3 b. Traditional models process images uniformly and do not distinguish the importance of regions, ignoring some regions that contain more critical information, resulting in limited performance [ 13 ]. 2.2.1. Detailed Network Structure Design The improved U-Net consists of an encoder, decoder, and skip connections. Each stage of the encoder includes two 3 &#215; 3 convolution layers with ReLU activation and a 2 &#215; 2 max pooling operation for downsampling. The number of feature channels is set to 64, 128, 256, 512, and 1024 from shallow to deep layers. This design follows the principle of gradually increasing feature maps to capture higher-level semantic information while reducing spatial resolution, which balances model complexity and feature richness. In the decoder, each stage performs 2&#215; upsampling followed by two 3 &#215; 3 convolution layers and concatenation with the corresponding encoder features. The number of feature channels decreases symmetrically (512, 256, 128, 64) to progressively recover spatial details. The Spatial Attention module is inserted after each encoder stage to emphasize critical spatial regions and suppress background noise. The Coordinate Attention module is added to each skip connection to embed position-sensitive information into channel attention, enhancing feature fusion between encoder and decoder without significantly increasing computation. 2.2.2. Spatial Attention Mechanism When traditional models process images, they often uniformly treat the entire image without distinguishing the importance of different regions. This uniform processing approach ignores the fact that certain regions in the image may contain more crucial information, thereby limiting the performance of the model when processing images. The Spatial Attention mechanism emphasizes the regions that contribute the most to the task by weighting specific spatial positions in the feature map [ 14 ], while suppressing irrelevant or redundant regions, thereby improving the model&#8217;s performance. The structure of the Spatial Attention mechanism is shown in Figure 4 . Traditional models may lose local key information during downsampling. The Spatial Attention mechanism, during downsampling, generates a spatial weight map to enable the model to enhance the features of important regions and weaken the influence of irrelevant regions, thus retaining core information while reducing the size. First, perform global max pooling and global average pooling on the input feature map F with dimensions H &#215; W &#215; C, resulting in two feature maps of size H &#215; W &#215; 1. Then, concatenate the results of global max pooling and global average pooling along the channels to obtain a feature map of size H &#215; W &#215; 2. Finally, perform convolution on the concatenated result to obtain a feature map of size H &#215; W &#215; 1, and apply the Sigmoid activation function to obtain the spatial attention weight matrix Ms. The calculation formula for spatial attention is as shown in Equation (4). (1) M s F &#8712; R H , W (2) F a v g s &#8712; R 1 &#215; H &#215; W (3) F m a x s &#8712; R 1 &#215; H &#215; W (4) M s F = &#963; ( f 7 &#215; 7 ( [ AvgPool ( F ) ; MaxPool ( F ) ] ) ) = &#963; ( f 7 &#215; 7 [ F avg s ; F max s ] ) Among them, F avg s represents the result of performing global average pooling on the input features, F max s represents the result of performing global max pooling on the input features, &#963; represents the Sigmoid activation function, and f 7 &#215; 7 represents the 7 &#215; 7 convolution operation. 2.2.3. Coordinate Attention Mechanism The Coordinate Attention Mechanism is achieved by performing average pooling in both the horizontal and vertical directions [ 15 ]. It not only captures information across channels but also captures direction-aware and position-sensitive information, which helps the model to more accurately locate and identify the objects of interest. The structure of the Coordinate Attention Mechanism is shown in Figure 5 . CA first divides the input feature map into two directions: height and width, and performs global average pooling separately in each direction to obtain a vertical feature vector z c h of size C &#215; H &#215; 1, as well as a horizontal feature vector z c w of size C &#215; 1 &#215; W, as shown in Equations (5) and (6). (5) z c h ( h ) = 1 W &#8721; 0 &#8804; i &lt; W &#160; x c ( h , i ) (6) z c w ( w ) = 1 H &#8721; 0 &#8804; f &lt; H &#160; x c ( j , w ) Then, the two pooled feature maps are concatenated along the channel dimension to obtain a feature map with dimensions of C &#215; 1 &#215; (W + H). A 1 &#215; 1 convolution is applied to the concatenated feature map for dimensionality reduction, as shown in Equation (7). (7) Z = K 1 &#8727; Y &#8712; R C r &#215; 1 &#215; ( W + H ) Among them, the convolution kernels K 1 &#8712; R C &#215; C r &#215; 1 &#215; 1 and r are the scaling factors, and &#8727; represents the convolution operation. Then, normalization processing is carried out, and the formula is as shown in Equation (8). (8) Z b n = BatchNorm Z = &#947; &#8901; Z &#8722; &#956; &#963; 2 + &#1013; + &#946; Here, &#956; represents the mean difference, &#963; 2 represents the variance, &#947; , &#946; represents the learnable parameters, and &#1013; represents the small constant to avoid division by zero. Meanwhile, a nonlinear function h-swish is introduced to enhance the model&#8217;s expressive power. The expression of the h-swish function is shown in Equation (9). (9) h &#8722; swish x = x &#8901; min max x + 3,0 , 6 6 Finally, the feature maps that have undergone normalization and nonlinear activation are re-segmented into two branches in the vertical and horizontal directions. Then, the outputs of these two branches are respectively applied with the Sigmoid activation function to compress the output values to the range of 0&#8211;1. Subsequently, the obtained two attention weight maps are respectively multiplied with the original input feature maps in the corresponding dimensions. This achieves the weighted operation on the original feature map, thereby enhancing the important features and suppressing the non-important ones. 2.3. Navigation Line Calculation Method The existing methods for calculating navigation lines mostly employ Hough transform and vertical projection for linear fitting, which are suitable for structured road sections [ 16 ]. However, in orchard environments, the road edges are irregular, and directly calculating navigation lines based on the segmentation results will result in significant errors. In this paper, firstly, the image undergoes HSV conversion, morphological processing, Gaussian blurring, and edge smoothing for preprocessing. Then, the contours and center points are calculated. Finally, interpolation optimization processing is carried out. The flowchart of navigation line calculation is shown in Figure 6 . 2.3.1. Image Preprocessing Image preprocessing lays the foundation for the subsequent calculation of navigation lines, mainly including HSV conversion, morphological processing, and Gaussian blur for edge smoothing operations. HSV Conversion In the field of computer vision, the RGB color space is intuitive but is greatly affected by changes in lighting, which is not conducive to the subsequent calculation of navigation lines. Through the HSV conversion, the coupling problem between the RGB channels can be avoided. The HSV conversion is to convert the color space of the image from RGB to HSV. The HSV color space is more in line with human perception of colors [ 17 ]. When max R 255 , G 255 , B 255 = min R 255 , G 255 , B 255 , the hue H is 0&#176;. When the value of max R 255 , G 255 , B 255 is equal to R 255 , G 255 , B 255 , the hue H calculation formulas are as shown in Equations (10)&#8211;(12). When max R 255 , G 255 , B 255 &#8800; 0 , the saturation S calculation is as shown in Equation (13). When max R 255 , G 255 , B 255 = 0 , the saturation S = 0. The calculation formula for luminance V is as shown in Equation (14). (10) H = 60 &#176; &#215; G &#8722; B / 255 max R 255 , G 255 , B 255 &#8722; min R 255 , G 255 , B 255 m o d 6 (11) H = 60 &#176; &#215; B &#8722; R / 255 max R 255 , G 255 , B 255 &#8722; min R 255 , G 255 , B 255 + 2 (12) H = 60 &#176; &#215; R &#8722; G / 255 max R 255 , G 255 , B 255 &#8722; min R 255 , G 255 , B 255 + 4 (13) S = max R 255 , G 255 , B 255 &#8722; min R 255 , G 255 , B 255 max R 255 , G 255 , B 255 (14) V = max R 255 , G 255 , B 255 2. Morphological Processing After the HSV conversion, the image noise is removed through a combined morphological processing of dilation and erosion. The small noise in the image can be eliminated through the erosion operation, and the internal small holes can be filled through the dilation operation. In this paper, the image is processed using the opening operation, which is the sequence of erosion followed by dilation. The formula of the opening operation is shown in Equation (15). (15) f &#8728; B ( x , y ) = m a x ( s , t ) &#8712; B &#160; m i n ( s ' , t ' ) &#8712; B &#160; f x + ( s + s ' ) , y + ( t + t ' ) Among them, f represents the input image; B represents the structural element; &#8728; represents the opening operation; ( s , t ) and ( s ' , t ' ) represent the pixel coordinate offsets within B. After the opening operation, smaller noise blocks can be eliminated, while the integrity of the operational area is maintained to facilitate subsequent operations. 3. Gaussian Blur Smooths the Edges Gaussian blurring is achieved by convolving the image with a Gaussian kernel. The Gaussian kernel function is shown in Equation (16). (16) G ( x , y ) = 1 2 &#960; &#963; 2 e &#8722; x 2 + y 2 2 &#963; 2 Among them, &#963; represents the degree of control. After applying the Gaussian blur operation for edge smoothing, the edge transitions become more natural, avoiding misjudgment in centerline calculation due to sharp edges, laying the foundation for accurately obtaining the navigation line coordinates, and enhancing the stability and reliability of subsequent processing. 2.3.2. Calculation of Navigation Lines After the image undergoes image preprocessing, the calculation of navigation lines requires precisely extracting the key information of the contour and processing the center point data [ 18 ]. In this paper, Canny is used to detect the edge of the drivable area, calculate the boundary contour of the drivable area, and eliminate the background contour and interfering contours. The center points of the filtered target contours are calculated row by row. The formula for calculating the x-coordinate of the center point is as shown in Equation (17). (17) x c ( y ) = x m i n ( y ) + x m a x ( y ) 2 Among them, x c ( y ) represents the horizontal coordinate of the center point of the drivable area in the row where the vertical coordinate of the image is y; x m i n ( y ) represents the horizontal coordinate of the left boundary of the drivable area within this row; x m a x ( y ) represents the horizontal coordinate of the right boundary of the drivable area within this row. The initial center point sequence may be affected by noise, discontinuous contours, etc., and further filtering and optimization of abnormal points are required. The formulas for calculating the mean &#956; and standard deviation &#963; of the center point sequence are respectively shown in Equations (18) and (19). (18) &#956; = 1 m &#8721; i = 1 m &#160; x c i (19) &#963; = 1 m &#8721; i = 1 m &#160; ( x ci &#8722; &#956; ) 2 Here, m represents the initial number of center points. Remove the abnormal center points that meet the | x c i &#8722; &#956; | &gt; 3 &#963; criteria, and retain the reasonably distributed center points. Construct a sliding window of size &#969; = 5 , and perform sliding average processing on the center point sequence. The formula for the smoothed coordinates of the i -th center point within the window is shown in Equation (20). (20) x ^ c i = 1 &#969; &#8721; j = i &#8722; &#8970; &#969; / 2 &#8971; i + &#8970; &#969; / 2 &#8971; &#160; x c j Among them, x ^ c i represents the output value of the i -th center point after being smoothed by the sliding window; &#969; represents the size of the sliding window; x c j represents the original sequence of center points; i represents the current center point being processed; j represents the center point being traversed within the window. After being smoothed through the sliding window, the distribution of the center points becomes more uniform, reducing the influence of noise and providing a reliable foundation for subsequent processing. 2.3.3. Generation of Navigation Lines Based on the optimized sequence of center points, continuous and smooth navigation lines are generated through interpolation fitting. If there are discontinuities in the center point sequence, bidirectional interpolation is used to fill the breakpoints. The interpolation formula is shown in Equation (21). The interpolation results are verified from two directions to ensure the continuity of coordinates at the breakpoints, and a complete sequence of center points is obtained. (21) x ( y ) = x a + x b &#8722; x a y b &#8722; y a ( y &#8722; y a ) Here, ( x a &#8722; y a ) and ( x b &#8722; y b ) represent the adjacent valid center points at the discontinuity points. To improve the accuracy of the navigation line fitting, based on the original central point&#8217;s vertical coordinate, a dense Y coordinate sequence Y = { y 1 , y 1 + &#916; y , y 1 + 2 &#916; y , &#8230; , y m ' } was generated with a fixed step size, and corresponding horizontal coordinates were supplemented to prepare for spline interpolation. Using cubic spline interpolation, the interpolation function was set as S ( y ) , which satisfied [ y i , y i + 1 ] within the region S ( y ) = a i + b i ( y &#8722; y i ) + c i ( y &#8722; y i ) 2 + d i ( y &#8722; y i ) 3 , and met the boundary conditions. By solving the spline coefficients a i , b i , c i , d i , the horizontal coordinates corresponding to the dense Y coordinates were interpolated to obtain a smooth X coordinate sequence X = { S ( y 1 ) , S ( y 1 + &#916; y ) , &#8230; , S ( y m ' ) } . The dense coordinate pairs obtained from the interpolation (denoted as X 1 , Y 1 , X 2 , Y 2 , &#8230; , X k , Y k , where k represents the number of dense coordinates) are the navigation line coordinates of the exercisable area. 3. Experiments and Result Analysis Improving the U-Net network under the Windows 10 operating system, with a GPU (Quadro P2200) and the code training platform being Pycharm 2020.1. The specific experimental environment is shown in Table 1 . 3.1. Evaluation Index This study comprehensively evaluated the network model using multiple quantitative indicators: Recall, Precision, mPA, and mIoU. These indicators measure the performance of the model from different perspectives. Recall reflects the model&#8217;s ability to identify positive samples of various categories [ 19 ], and the calculation formula is shown in Equation (22). Precision indicates the accuracy of the predicted samples [ 20 ], and the calculation formula is shown in Equation (23). Both of these are evaluated from the basic classification layer. mPA and mIoU focus on the characteristics of the segmentation task. mPA is the average classification accuracy of the model across all categories, measuring the consistency and accuracy of classification across various categories, and the calculation formula is shown in Formula (24). mIoU is the average value of the overlap between the predicted regions and the true regions for all categories [ 21 ], reflecting the overall segmentation accuracy of the model, and the calculation formula is shown in Formula (25). Inference speed represents the number of inferences the model can complete per second, directly reflecting the model&#8217;s efficiency in practical application scenarios, especially in real-time processing tasks. Number of parameters indicates the scale of the model in millions, which is closely related to the model&#8217;s complexity, storage requirements, and computational cost. Through the synergy of these six indicators, both the segmentation performance, the efficiency of the model during inference, and the complexity reflected by the number of parameters are considered, ensuring the feasibility and effectiveness of the model in practice. (22) R e c a l l = P i i P i i + P i j (23) P r e c i s i o n = P i i P i i + P j i (24) m P A = 1 k + 1 &#8721; i = 0 k &#160; P i i &#8721; j = 0 k &#160; P i j &#215; 100 % (25) m I o U = 1 k + 1 &#8721; i = 0 k &#160; P i i &#8721; j = 0 k &#160; P i j + P j i &#8722; P i i &#215; 100 % Here, k represents the total number of categories including the background in the class; i represents the drivable area category, and j represents the non-drivable area category; P i i is the number of pixels that are actually and predictively classified as category i ; P i j is the number of pixels that are actually classified as category i but predicted as category j ; P j i is the number of pixels that are actually classified as category j but predicted as category i . 3.2. Model Training During the model training process, the loss value is a key indicator for measuring the learning effect of the model. By monitoring the changes of training loss and validation loss with respect to the training epochs, one can intuitively understand the development trend of the model&#8217;s fitting and generalization capabilities. The curve of the loss value changing with the training epochs is shown in Figure 7 . Both the train loss and val loss decreased rapidly, indicating that in the initial training rounds, the model quickly learned the data patterns, reduced the prediction error, and significantly improved its fitting ability for the task. During the middle stage (Epoch 50&#8211;200): The loss decreased at a slower rate, and the fluctuations in the train loss became smaller. The val loss occasionally had small fluctuations (around 100 epochs), reflecting that the model was gradually approaching the optimal solution. At the same time, the data distribution in the validation set differed from that in the training set, causing fluctuations in the validation loss. In the later stage (Epoch 200&#8211;300): Both the train loss and val loss tended to stabilize. The values were close and the fluctuations were extremely small, indicating that the model had fully learned the data features and entered the &#8220;convergence plateau period&#8221;. From the smooth train loss and smooth val loss, it can be more clearly observed: The two smooth curves have a highly consistent trend, and eventually approached around 0.04, indicating that the model training process was stable and the convergence effect was good. The curve was flat in the later stage, proving that the model had learned stable feature representations. 3.3. Model Performance Verification To verify the completeness of the information on drivable areas and the accuracy of the navigation lines obtained by the method proposed in this paper in an orchard environment, comparative experiments on drivable area detection and verification experiments on navigation line calculation were conducted respectively. 3.3.1. Comparison Validation To verify the effectiveness of the model proposed in this paper, under the same configuration environment, the U-Net, SegViT, SE-Net and DeepLabv3+ deep learning network models were selected to conduct comparative experiments on the dataset of this paper. The performance indicators of each model are compared as shown in Table 2 . Figure 8 shows the segmentation visualization results of each model on the test set. From Figure 8 , it can be seen that most models have incomplete recognition of the drivable areas. The improved U-Net model has the best recognition effect, and can obtain more complete and accurate drivable areas in the orchard, with higher segmentation accuracy. To present the comparison results more clearly, the model comparison radar chart is shown in Figure 9 . The radar chart presents the values of the four different indicators corresponding to the models summarized in Table 2 . Each axis of the radar chart corresponds to one of the indicators. The shapes formed by the lines show the performance capabilities and shortcomings of each model in the multi-dimensional space. The value range of all indicators is between 0% and 100%. The closer the value is to 100%, the better the result. From Figure 9 , it can be seen that the results of all models fall within a similar range. The improved U-Net method proposed in this paper demonstrates outstanding performance. 3.3.2. Navigation Line Verification To meet the requirements of agricultural machinery and agricultural techniques. For large-scale and standardized orchards, the error of the navigation line needs to be controlled within &#177;50 to 80 mm. Otherwise, it may affect the connection of subsequent mechanized harvesting and other links. The error within a reasonable range can meet the safety operation requirements of the orchard. The comparison of the navigation line calculated and the navigation line observed manually is shown in Figure 10 . The manually observed navigation line is used as the standard to judge the accuracy of the extraction of the orchard navigation line. The mean absolute error of the calculated navigation line compared with the manually observed navigation line is 56 mm, and the standard deviation of the error is 4.47 mm. To further illustrate the effectiveness of the proposed method, Figure 11 shows the visualization of the original image, the drivable area detection, and the corresponding navigation line calculation results. 4. Discussion This study addresses the core issue of complex image backgrounds and numerous interfering factors in visual navigation of orchards. It proposes a method for segmented driving area and geometric center-based navigation path recognition based on the improved U-Net. The effectiveness and practicality of the method have been verified through experiments. The CA and SA modules are introduced on the basis of the traditional U-Net. The SA module is inserted in the downsampling stage to enhance spatial feature extraction, and the CA module is added in the skip connection stage to supplement context information. This achieves precise segmentation of complex orchard environments containing bare soil, obstructions, trees, and weeds. The experimental results show that the Recall of the improved model is 90.23%, the Precision is 91.71%, the mPA is 87.75%, and the mIoU is 84.84%. In the same environment, all indicators are superior to those of traditional U-Net, SegViT, SE-Net, and DeepLabv3+. This effectively reduces misjudgments caused by environmental interference and lays a high-quality segmentation foundation for navigation path recognition. Based on the segmented driving area results, the mask image is optimized through HSV conversion, morphological opening operation, Gaussian blur, etc. Combined with contour detection, geometric center calculation, 3&#963; outlier elimination, sliding window smoothing, and bidirectional interpolation filling, the generated continuous smooth navigation lines have an average distance error of only 56 mm from the actual center line of manual observation, fully meeting the agronomic requirements of &#177;50&#8211;80 mm for mechanized operations in large-scale and standardized orchards. It can ensure the safety and process connectivity of mechanized operations such as picking, spraying, and weeding in the orchard. Under the current orchard environment and hardware configuration, the method proposed in this study can provide effective technical references for the visual navigation task of agricultural machinery in orchards. However, there are still deficiencies, and further optimization is needed in subsequent work. There are limitations in environmental adaptability. The performance of the method in extreme environments has not been fully verified. The dynamic response of path planning is insufficient. This study did not consider dynamic interference factors in orchard operations. Based on the development trend of smart agriculture and the shortcomings of this study, subsequent research will be conducted in the following directions to further improve the orchard visual navigation technology system. Enhance multi-scenario adaptive ability. Expand experimental scenarios and datasets to ensure the stability of the method in different scenarios. Integrate dynamic path planning and multi-sensor data. Introduce sensors such as laser radar, millimeter-wave radar, etc., and combine visual data for multimodal fusion to achieve real-time adjustment of navigation lines and solve the problem of path avoidance under dynamic interference in orchards, improving the flexibility of autonomous operation of agricultural machinery. 5. Conclusions This study proposes an improved method based on U-Net for navigable area segmentation and geometric center-based navigation path recognition, which is applied to orchard visual navigation. The improved U-Net network emphasizes key regions and suppresses redundancy during downsampling by weighting specific spatial positions in the feature maps. This approach improves the network&#8217;s performance. In contrast, the traditional U-Net simply concatenates features during feature extraction. The improved U-Net, however, introduces the CA module without increasing model parameters or computational cost. This module effectively fuses the global information obtained during feature extraction, optimizing the feature restoration process of the drivable area in the orchard. As a result, it reduces environmental interference and further improves the overall segmentation accuracy of the model. Although this method provides an effective technical reference for the visual navigation of agricultural machinery in current conditions, it has limitations in environmental adaptability and dynamic path planning response. Future research will focus on enhancing multi-scenario adaptability by expanding experimental scenarios and datasets, and integrating dynamic path planning with multi-sensor data fusion to improve the flexibility of autonomous operation of agricultural machinery. Acknowledgments We thank all the authors for their contributions to the writing of this article. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Methodology and writing&#8212;original draft preparation, N.X.; formal analysis and investigation, X.N.; resources, A.L. and Z.L.; data curation, Y.S.; validation, W.W. All authors have read and agreed to the published version of the manuscript. Institutional Review Board Statement Not applicable. Informed Consent Statement Not applicable. Data Availability Statement The data presented in this study are available on request from the corresponding author. Conflicts of Interest The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results. References 1. Chang H. Wei Z. Xue C. Siyi H. Chen Y. A Navigation Path Recognition Method for Cross-row Tea-Picking Machines Based on Improved Unet China Agric. Sci. Technol. Her. 2025 1 10 (In Chinese and English) 10.13304/j.nykjdb.2024.0815 2. Zhenhui X. Xiaojuan L. Research on Orchard Tree Trunk Detection and Navigation Line Fitting Algorithm Chin. J. Agric. Mach. Chem. 2024 45 217 222 10.13733/j.jcam.issn.2095-5553.2024.08.031 3. Tan D.N. Liu Y. Yao L.B. Ding Z.R. Lu X.Q. Semantic Segmentation of Multi-source Remote Sensing Images Based on Visual Attention Mechanism Signal Process. 2022 38 1180 1191 10.16798/j.issn.1003-0530.2022.06.005 4. Lu Y. Bingyue S. Fenglei R. Chongke B. Segmentation of Usable Areas in Unstructured Road Scenarios Optoelectron. Laser 2025 1 10 CN 12-1182/O4 1005-0086 5. Anoop P. Deivanathan R. Real-Time image enhancement following road scenario classification using deep learning Measurement 2025 251 117192 10.1016/j.measurement.2025.117192 6. Viadero-Monasterio F. Nguyen A.-T. Lauber J. Boada M.J.L. Boada B.L. Event-Triggered Robust Path Tracking Control Considering Roll Stability Under Network-Induced Delays for Autonomous Vehicles IEEE Trans. Intell. Transp. Syst. 2023 24 14743 14756 10.1109/TITS.2023.3321415 7. Viadero-Monasterio F. Mel&#233;ndez-Useros M. Zhang N. Zhang H. Boada B.L. Boada M.J.L. Motion Planning and Robust Output-Feedback Trajectory Tracking Control for Multiple Intelligent and Connected Vehicles in Unsignalized Intersections IEEE Trans. Veh. Technol. 2025 1 13 10.1109/TVT.2025.3586769 8. Chi M. Ziyang D. Zhijun C. Yue Z. Fuli S. Research on the Generation Method of Intercrop Navigation Lines in Kiwifruit Orchards Based on Root Point Replacement Arid. Land Agric. Res. 2021 39 222 230 10.7606/j.issn.1000-7601.2021.05.29 9. Shubo P. Bingqi C. Jingbin L. Pengxuan F. Xiangye L. Xin F. Hongtao D. Xionchu Z. Orchard Row Navigation Line Detection Based on Improved YOLOv7 Tzransactions Chin. Soc. Agric. Eng. 2023 39 131 138 10.11975/j.issn.1002-6819.202305207 10. Ke X. Weiguang X. Congzhe L. Algorithm for Extracting Visual Navigation Paths in Orchard Environments under Complex Conditions Trans. Chin. Soc. Agric. Mach. 2023 54 197 252 197&#8211;204+252 10.6041/j.issn.1000-1298.2023.06.020 11. Liang P. Zhonghua C. Jiwei D. Yali L. Xiaomei Z. Wentu L. Guo L. Detection Method of Potato Harvesting Guidance Line Based on Machine Vision Agric. Mech. Res. 2025 47 22 34 22&#8211;27+34 10.13427/j.issn.1003-188X.2025.09.004 12. Wenhui H. Chuanqi Z. Yan C. Yewei W. Lu L. Kuan Q. Fruit Row Interval Path Recognition Method Based on Lightweight U-Net Network Trans. Chin. Soc. Agric. Mach. 2024 55 16 27 10.6041/j.issn.1000-1298.2024.02.002 13. Xuewei W. Shaohua L. Xiao L. Jinjin Z. A Non-Structured Road Usable Area Recommendation Model Based on M-shaped Deep Architecture Chin. J. Highw. Transp. 2022 35 205 218 10.19721/j.cnki.1001-7372.2022.12.017 14. Kong X. Guo Y. Liang Z. Zhang R. Hong Z. Xue W. A method for recognizing inter-row navigation lines of rice heading stage based on improved ENet network Measurement 2025 241 115677 10.1016/j.measurement.2024.115677 15. Saeedizadeh N. Jalali S.M.J. Khan B. Kebria P.M. Mohamed S. A new optimization approach based on neural architecture search to enhance deep U-Net for efficient road segmentation Knowl. Based Syst. 2024 296 111966 10.1016/j.knosys.2024.111966 16. Gong H. Zhuang W. Wang X. Improving the maize crop row navigation line recognition method of YOLOX Front. Plant Sci. 2024 15 1338228 10.3389/fpls.2024.1338228 38606066 PMC11008721 17. Yu J. Zhang J. Shu A. Chen Y. Chen J. Yang Y. Tang W. Zhang Y. Study of convolutional neural network-based semantic segmentation methods on edge intelligence devices for field agricultural robot navigation line extraction Comput. Electron. Agric. 2023 209 107811 0168-1699 10.1016/j.compag.2023.107811 18. Wu W. He Z. Li J. Chen T. Luo Q. Luo Y. Wu W. Zhang Z. Instance Segmentation of Tea Garden Roads Based on an Improved YOLOv8n-seg Model Agriculture 2024 14 1163 10.3390/agriculture14071163 19. Gong H. Wang X. Zhuang W. Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology Agriculture 2024 14 124 10.3390/agriculture14010124 20. Jianjun Z. Siyuan G. Quan Q. Yang S. Man Z. A Deep-Learning Extraction Method for Orchard Visual Navigation Lines Agriculture 2022 12 1650 10.3390/agriculture12101650 21. Maoyong C. Fangfang T. Peng J. Fengying M. Improved Real-Time Semantic Segmentation Network Model for Crop Vision Navigation Line Detection Front. Plant Sci. 2022 13 898131 10.3389/fpls.2022.898131 35720554 PMC9201824 Figure 1 Dataset collection. ( a ) Camera installation position; ( b ) The acquisition angle of the camera. Figure 2 Data augmentation. Figure 3 Overall network structure. ( a ) Original U-Net network structure; ( b ) Improved U-Net network structure. Figure 4 Structure of the spatial attention mechanism. Figure 5 Structure of the Coordinate Attention Mechanism. Figure 6 Flowchart of route calculation process. Figure 7 Curve showing the variation of loss value with the number of training rounds. Figure 8 Comparison of visualization results. Figure 9 Model comparison radar chart. Figure 10 Comparison of calculated navigation line and manually observed navigation line. Note: The (yellow) line in the figure is the fitting navigation line, and the (blue) line is the manual observation navigation line. Figure 11 Experimental results. ( a ) Original image; ( b ) Segmentation result of the drivable area; ( c ) Navigation line calculation result. sensors-25-06828-t001_Table 1 Table 1 Specific Experimental Environment. Item Content Operating System Windows 10 CPU Intel(R) Core(TM)i7-10700F CPU @ 2.90 GHZ (Intel&#174;: Santa Clara, CA, USA) GPU NVIDIA Quadro P2200 (NVIDIA: Santa Clara, CA, USA) Platform Environment Pycharm 2020.1, Pytorch 1.10.0, opencv 4.10.0, Cuda 10.1 with cudann sensors-25-06828-t002_Table 2 Table 2 Comparison of performance indicators for each model. Method Recall/(%) Precision/(%) mPA/(%) mIoU/(%) Inference Speed/(FPS) Number of Parameters/(M) U-Net 78.17 82.57 75.16 70.40 38.5 31.1 SegViT 85.75 88.74 82.90 78.15 46.7 38.6 SE-Net 80.06 84.19 77.12 73.36 37.8 36.7 DeepLabv3+ 88.24 90.05 85.64 82.89 53.3 62.4 Our 90.23 91.71 87.75 84.84 72.2 45.7"
}