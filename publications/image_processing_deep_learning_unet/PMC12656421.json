{
  "pmcid": "PMC12656421",
  "source": "PMC",
  "download_date": "2025-12-09T16:11:30.997306",
  "metadata": {
    "journal_title": "Plants",
    "journal_nlm_ta": "Plants (Basel)",
    "journal_iso_abbrev": "Plants (Basel)",
    "journal": "Plants",
    "pmcid": "PMC12656421",
    "pmid": "41304587",
    "doi": "10.3390/plants14223436",
    "title": "Ultra-High-Resolution Optical Remote Sensing Satellite Identification of Pine-Wood-Nematode-Infected Trees",
    "year": "2025",
    "month": "11",
    "day": "10",
    "pub_date": {
      "year": "2025",
      "month": "11",
      "day": "10"
    },
    "authors": [
      "Nie Ziqi",
      "Qin Lin",
      "Xing Peng",
      "Meng Xuelian",
      "Meng Xianjin",
      "Qin Kaitong",
      "Wang Changwei"
    ],
    "abstract": "The pine wood nematode (PWN), one of the globally significant forest diseases, has driven the demand for precise detection methods. Recent advances in satellite remote sensing technology, particularly ultra-high-resolution optical imagery, have opened new avenues for identifying PWN-infected trees. In order to systematically evaluate the ability of ultra-high-resolution optical remote sensing and the influence of spatial and spectral resolution in detecting PWN-infected trees, this study utilized a U-Net network model to identify PWN-infected trees using three remote sensing datasets of the ultra-high-resolution multispectral imagery from Beijing 3 International Cooperative Remote Sensing Satellite (BJ3N), with a panchromatic band spatial resolution of 0.3 m and six multispectral bands at 1.2 m; the high-resolution multispectral imagery from the Beijing 3A satellite (BJ3A), with a panchromatic band resolution of 0.5 m and four multispectral bands at 2 m; and unmanned aerial vehicle (UAV) imagery with five multispectral bands at 0.07 m. Comparison of the identification results demonstrated that (1) UAV multispectral imagery with 0.07 m spatial resolution achieved the highest accuracy, with an F1 score of 89.1%. Next is the fused ultra-high-resolution BJ3N satellite imagery at 0.3 m, with an F1 score of 88.9%. In contrast, BJ3A imagery with a raw spatial resolution of 2 m performed poorly, with an F1 score of only 28%. These results underscore that finer spatial resolution in remote sensing imagery directly enhances the ability to detect subtle canopy changes indicative of PWN infestation. (2) For UAV, BJ3N, and BJ3A imagery, the identification accuracy for PWN-infected trees showed no significant differences across various band combinations at equivalent spatial resolutions. This indicates that spectral resolution plays a secondary role to spatial resolution in detecting PWN-infected trees using ultra-high-resolution optical imagery. (3) The 0.3 m BJ3N satellite imagery exhibits low false-detection and omission rates, with F1 scores comparable to higher-resolution UAV imagery. This indicates that a spatial resolution of 0.3 m is sufficient for identifying PWN-infected trees and is approaching a point of saturation in a subtropical mountain monsoon climate zone. In conclusion, ultra-high-resolution satellite remote sensing, characterized by frequent data revisit cycles, broad spatial coverage, and balanced spatial-spectral performance, provides an optimal remote sensing data source for identifying PWN-infected trees. As such, it is poised to become a cornerstone of future research and practical applications in detecting and managing PWN infestations globally.",
    "keywords": [
      "pine wood nematode",
      "Beijing3N remote sensing image",
      "U-NET deep learning",
      "image resolution",
      "number of bands of the image"
    ]
  },
  "xml": "<?xml version=\"1.0\"  ?><!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dtd.nlm.nih.gov/ncbi/pmc/articleset/nlm-articleset-2.0.dtd\"><pmc-articleset><article xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:ali=\"http://www.niso.org/schemas/ali/1.0/\" xml:lang=\"en\" article-type=\"research-article\" dtd-version=\"1.4\"><processing-meta base-tagset=\"archiving\" mathml-version=\"3.0\" table-model=\"xhtml\" tagset-family=\"jats\"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type=\"nlm-ta\">Plants (Basel)</journal-id><journal-id journal-id-type=\"iso-abbrev\">Plants (Basel)</journal-id><journal-id journal-id-type=\"pmc-domain-id\">2909</journal-id><journal-id journal-id-type=\"pmc-domain\">plants</journal-id><journal-id journal-id-type=\"publisher-id\">plants</journal-id><journal-title-group><journal-title>Plants</journal-title></journal-title-group><issn pub-type=\"epub\">2223-7747</issn><publisher><publisher-name>Multidisciplinary Digital Publishing Institute  (MDPI)</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type=\"pmcid\">PMC12656421</article-id><article-id pub-id-type=\"pmcid-ver\">PMC12656421.1</article-id><article-id pub-id-type=\"pmcaid\">12656421</article-id><article-id pub-id-type=\"pmcaiid\">12656421</article-id><article-id pub-id-type=\"pmid\">41304587</article-id><article-id pub-id-type=\"doi\">10.3390/plants14223436</article-id><article-id pub-id-type=\"publisher-id\">plants-14-03436</article-id><article-version article-version-type=\"pmc-version\">1</article-version><article-categories><subj-group subj-group-type=\"heading\"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Ultra-High-Resolution Optical Remote Sensing Satellite Identification of Pine-Wood-Nematode-Infected Trees</article-title></title-group><contrib-group><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Nie</surname><given-names initials=\"Z\">Ziqi</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Software\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/software/\">Software</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-plants-14-03436\" ref-type=\"aff\">1</xref><xref rid=\"af2-plants-14-03436\" ref-type=\"aff\">2</xref><xref rid=\"af3-plants-14-03436\" ref-type=\"aff\">3</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names initials=\"L\">Lin</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Supervision\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/supervision/\">Supervision</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af4-plants-14-03436\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Xing</surname><given-names initials=\"P\">Peng</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af5-plants-14-03436\" ref-type=\"aff\">5</xref></contrib><contrib contrib-type=\"author\"><contrib-id contrib-id-type=\"orcid\" authenticated=\"true\">https://orcid.org/0000-0001-6953-1916</contrib-id><name name-style=\"western\"><surname>Meng</surname><given-names initials=\"X\">Xuelian</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Methodology\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/methodology/\">Methodology</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af6-plants-14-03436\" ref-type=\"aff\">6</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Meng</surname><given-names initials=\"X\">Xianjin</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Investigation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/investigation/\">Investigation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Data curation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/data-curation/\">Data curation</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Validation\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/validation/\">Validation</role><xref rid=\"af4-plants-14-03436\" ref-type=\"aff\">4</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names initials=\"K\">Kaitong</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><xref rid=\"af1-plants-14-03436\" ref-type=\"aff\">1</xref><xref rid=\"af7-plants-14-03436\" ref-type=\"aff\">7</xref></contrib><contrib contrib-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names initials=\"C\">Changwei</given-names></name><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Conceptualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/conceptualization/\">Conceptualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Formal analysis\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/formal-analysis/\">Formal analysis</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Funding acquisition\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/funding-acquisition/\">Funding acquisition</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Visualization\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/visualization/\">Visualization</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; original draft\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-original-draft/\">Writing &#8211; original draft</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Writing &#x2013; review &amp; editing\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/writing-review-editing/\">Writing &#8211; review &amp; editing</role><role vocab=\"credit\" vocab-identifier=\"https://credit.niso.org/\" vocab-term=\"Project administration\" vocab-term-identifier=\"https://credit.niso.org/contributor-roles/project-administration/\">Project administration</role><xref rid=\"af1-plants-14-03436\" ref-type=\"aff\">1</xref><xref rid=\"af7-plants-14-03436\" ref-type=\"aff\">7</xref><xref rid=\"c1-plants-14-03436\" ref-type=\"corresp\">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type=\"editor\"><name name-style=\"western\"><surname>Rossi</surname><given-names initials=\"V\">Vittorio</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id=\"af1-plants-14-03436\"><label>1</label>College of Natural Resources and Environment, South China Agricultural University, Guangzhou 510642, China; <email>nzq2022331@163.com</email> (Z.N.); <email>kaitong_qin@163.com</email> (K.Q.)</aff><aff id=\"af2-plants-14-03436\"><label>2</label>State Key Laboratory of Surveying, Mapping and Remote Sensing Information Engineering, Wuhan University, Wuhan 430079, China</aff><aff id=\"af3-plants-14-03436\"><label>3</label>Aerial Photogrammetry and Remote Sensing Group Co., Ltd., Xi&#8217;an 710199, China</aff><aff id=\"af4-plants-14-03436\"><label>4</label>Guangdong Provincial Institute of Forestry Survey and Planning, Guangzhou 510520, China; <email>luckykql@263.net</email> (L.Q.); <email>18933986718@163.com</email> (X.M.)</aff><aff id=\"af5-plants-14-03436\"><label>5</label>Guangzhou Institute of Geography, Guangzhou 510070, China; <email>kaiers@126.com</email></aff><aff id=\"af6-plants-14-03436\"><label>6</label>Department of Geography &amp; Anthropology, Louisiana State University, Baton Rouge, LA 70803, USA; <email>smeng@lsu.edu</email></aff><aff id=\"af7-plants-14-03436\"><label>7</label>Guangdong Engineering Technology Research Center of Land Information, Guangzhou 510642, China</aff><author-notes><corresp id=\"c1-plants-14-03436\"><label>*</label>Correspondence: <email>changwei_wang@scau.edu.cn</email></corresp></author-notes><pub-date pub-type=\"epub\"><day>10</day><month>11</month><year>2025</year></pub-date><pub-date pub-type=\"collection\"><month>11</month><year>2025</year></pub-date><volume>14</volume><issue>22</issue><issue-id pub-id-type=\"pmc-issue-id\">501331</issue-id><elocation-id>3436</elocation-id><history><date date-type=\"received\"><day>18</day><month>9</month><year>2025</year></date><date date-type=\"rev-recd\"><day>31</day><month>10</month><year>2025</year></date><date date-type=\"accepted\"><day>06</day><month>11</month><year>2025</year></date></history><pub-history><event event-type=\"pmc-release\"><date><day>10</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-live\"><date><day>27</day><month>11</month><year>2025</year></date></event><event event-type=\"pmc-last-change\"><date iso-8601-date=\"2025-11-28 09:25:12.970\"><day>28</day><month>11</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use=\"textmining\" content-type=\"ccbylicense\">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link xmlns:xlink=\"http://www.w3.org/1999/xlink\" ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><self-uri xmlns:xlink=\"http://www.w3.org/1999/xlink\" content-type=\"pmc-pdf\" xlink:href=\"plants-14-03436.pdf\"/><abstract><p>The pine wood nematode (PWN), one of the globally significant forest diseases, has driven the demand for precise detection methods. Recent advances in satellite remote sensing technology, particularly ultra-high-resolution optical imagery, have opened new avenues for identifying PWN-infected trees. In order to systematically evaluate the ability of ultra-high-resolution optical remote sensing and the influence of spatial and spectral resolution in detecting PWN-infected trees, this study utilized a U-Net network model to identify PWN-infected trees using three remote sensing datasets of the ultra-high-resolution multispectral imagery from Beijing 3 International Cooperative Remote Sensing Satellite (BJ3N), with a panchromatic band spatial resolution of 0.3 m and six multispectral bands at 1.2 m; the high-resolution multispectral imagery from the Beijing 3A satellite (BJ3A), with a panchromatic band resolution of 0.5 m and four multispectral bands at 2 m; and unmanned aerial vehicle (UAV) imagery with five multispectral bands at 0.07 m. Comparison of the identification results demonstrated that (1) UAV multispectral imagery with 0.07 m spatial resolution achieved the highest accuracy, with an F1 score of 89.1%. Next is the fused ultra-high-resolution BJ3N satellite imagery at 0.3 m, with an F1 score of 88.9%. In contrast, BJ3A imagery with a raw spatial resolution of 2 m performed poorly, with an F1 score of only 28%. These results underscore that finer spatial resolution in remote sensing imagery directly enhances the ability to detect subtle canopy changes indicative of PWN infestation. (2) For UAV, BJ3N, and BJ3A imagery, the identification accuracy for PWN-infected trees showed no significant differences across various band combinations at equivalent spatial resolutions. This indicates that spectral resolution plays a secondary role to spatial resolution in detecting PWN-infected trees using ultra-high-resolution optical imagery. (3) The 0.3 m BJ3N satellite imagery exhibits low false-detection and omission rates, with F1 scores comparable to higher-resolution UAV imagery. This indicates that a spatial resolution of 0.3 m is sufficient for identifying PWN-infected trees and is approaching a point of saturation in a subtropical mountain monsoon climate zone. In conclusion, ultra-high-resolution satellite remote sensing, characterized by frequent data revisit cycles, broad spatial coverage, and balanced spatial-spectral performance, provides an optimal remote sensing data source for identifying PWN-infected trees. As such, it is poised to become a cornerstone of future research and practical applications in detecting and managing PWN infestations globally.</p></abstract><kwd-group><kwd>pine wood nematode</kwd><kwd>Beijing3N remote sensing image</kwd><kwd>U-NET deep learning</kwd><kwd>image resolution</kwd><kwd>number of bands of the image</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>42471340</award-id><award-id>42474045</award-id></award-group><funding-statement>This work was supported in part by the National Natural Science Foundation of China (Grant No. 42471340, 42474045).</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type=\"intro\" id=\"sec1-plants-14-03436\"><title>1. Introduction</title><p>The pine wood nematode (PWN, <italic toggle=\"yes\">Bursaphelenchus xylophilus</italic>), the causative agent of pine wilt disease, ranks among the world&#8217;s most destructive forest pathogens due to its rapid spread, high mortality rates, and severe economic impacts [<xref rid=\"B1-plants-14-03436\" ref-type=\"bibr\">1</xref>,<xref rid=\"B2-plants-14-03436\" ref-type=\"bibr\">2</xref>]. Since its first detection in Nanjing, China, in 1982, PWN has devastated over one billion pine trees nationwide, incurring losses exceeding hundreds of billions of yuan [<xref rid=\"B3-plants-14-03436\" ref-type=\"bibr\">3</xref>]. Early detection of PWN-infected trees remains critical for containing outbreaks, as timely and accurate identification of PWN-infected trees is important for prevention and control [<xref rid=\"B4-plants-14-03436\" ref-type=\"bibr\">4</xref>].</p><p>Traditional methods for identifying PWN-infected trees have relied on labor-intensive field surveys. These approaches lack the capacity for rapid, large-scale monitoring of PWN infestation dynamics and often lead to delayed interventions that hinder effective disease management [<xref rid=\"B1-plants-14-03436\" ref-type=\"bibr\">1</xref>,<xref rid=\"B5-plants-14-03436\" ref-type=\"bibr\">5</xref>,<xref rid=\"B6-plants-14-03436\" ref-type=\"bibr\">6</xref>,<xref rid=\"B7-plants-14-03436\" ref-type=\"bibr\">7</xref>]. Remote sensing technology has emerged as a primary investigative tool for detecting PWN-infected trees due to its cost-effectiveness, operational efficiency, and streamlined data-collection capabilities [<xref rid=\"B8-plants-14-03436\" ref-type=\"bibr\">8</xref>,<xref rid=\"B9-plants-14-03436\" ref-type=\"bibr\">9</xref>,<xref rid=\"B10-plants-14-03436\" ref-type=\"bibr\">10</xref>]. Early identification of PWN-infected trees through satellite remote sensing mainly depended on medium-resolution satellites, such as Landsat, which utilized the spectral differences between the canopies of infected and healthy pine trees [<xref rid=\"B11-plants-14-03436\" ref-type=\"bibr\">11</xref>,<xref rid=\"B12-plants-14-03436\" ref-type=\"bibr\">12</xref>]. Franklin et al. [<xref rid=\"B13-plants-14-03436\" ref-type=\"bibr\">13</xref>] achieved approximately 73% detection accuracy for PWN-infected trees using single-date 30 m resolution Landsat imagery. Skakun et al. [<xref rid=\"B14-plants-14-03436\" ref-type=\"bibr\">14</xref>] enhanced PWN identification accuracy by to 78% by analyzing multi-temporal Landsat data in the Prince George Forest Region of British Columbia, Canada. However, mixed-pixel effects in medium-resolution imagery obscure sparse infections, limiting practical utility.</p><p>Advances in high-resolution imagery (&#8804;1 m) have improved detection precision. Wang et al. [<xref rid=\"B15-plants-14-03436\" ref-type=\"bibr\">15</xref>] utilized Gaofen-2 satellite imagery with 1 m spatial resolution and four spectral bands, achieving an MIoU of 68.36%. Poona and Ismail [<xref rid=\"B16-plants-14-03436\" ref-type=\"bibr\">16</xref>] aimed to explore the utility of transformed high spatial resolution QuickBird imagery, which has a 0.6 m spatial resolution and four spectral bands, combined with artificial neural networks (ANNs), to detect and map trees infested with PWN, achieving a kappa coefficient of 0.65. Takenaka et al. [<xref rid=\"B17-plants-14-03436\" ref-type=\"bibr\">17</xref>] employed WorldView-3 imagery with 0.5 m spatial resolution to construct 18 vegetation indices for identifying PWN-infected trees in the Matsumoto region of central Japan, achieving a total precision of 72%. Despite these improvements, resolution thresholds for reliable PWN detection remain undefined, and spectral-spatial tradeoffs are poorly quantified.</p><p>The recent deployment of ultra-high-resolution satellites, with a resolution of less than 0.5 m, presents significant potential. The Beijing 3 International Cooperative Remote Sensing Satellite (BJ3N), launched on 29 April 2021, is currently the highest-resolution commercial remote sensing satellite. It offers a 0.3 m panchromatic band and six spectral bands with a 1.2 m resolution, as well as pansharpened 0.3 m fused multispectral images. Such capabilities promise unprecedented precision in detecting subtle canopy changes indicative of early PWN infestation. However, despite this promising potential, the application of ultra-high-resolution optical imagery for PWN detection remains unexplored, and no study has yet systematically evaluated its performance against established platforms like UAVs.</p><p>To address this research gap, the study conducted a comprehensive comparison of ultra-high-resolution BJ3N imagery with high-resolution Beijing 3A satellite (BJ3A) imagery, which has a 0.5 m spatial resolution for the panchromatic band and 2 m for the four multispectral bands, and UAV multispectral imagery with a 0.07 m spatial resolution for five multispectral bands. The objectives are to systematically evaluate the capabilities of ultra-high-resolution remote sensing images and the impact of spatial and spectral resolution on the detection of PWN-infected trees, and to identify the optimal data sources that can balance spatial resolution, spectral capabilities, coverage, and practical applicability, ultimately enhancing the efficiency of future PWN monitoring.</p></sec><sec id=\"sec2-plants-14-03436\"><title>2. Materials and Methods</title><sec id=\"sec2dot1-plants-14-03436\"><title>2.1. Study Area</title><p>The study area is located in Shaoguan, Guangdong Province, China (geographical coordinates 113&#176;35&#8242; to 113&#176;37&#8242; E, 24&#176;45&#8242;&#8211;24&#176;48&#8242; N), as shown in <xref rid=\"plants-14-03436-f001\" ref-type=\"fig\">Figure 1</xref>. The study area lies within a subtropical mountain monsoon climate zone, characterized by an average annual temperature of approximately 20 &#176;C, an average annual rainfall of 1600 mm, and an annual frost-free period of about 310 days. These climatic conditions are conducive to the proliferation of the pine bark beetle, leading to a particularly severe PWN outbreak in the area. The area covers approximately 5 km<sup>2</sup>, with elevations ranging from 60 to 255 m. Forest coverage reaches around 90%, dominated by species such as horsetail pine, fir, and camphor. A schematic overview of the study area is provided in <xref rid=\"plants-14-03436-f001\" ref-type=\"fig\">Figure 1</xref>.</p></sec><sec id=\"sec2dot2-plants-14-03436\"><title>2.2. Data</title><p>The data used in this study included ultra-high-resolution BJ3N satellite imagery and high-resolution Beijing 3A (BJ3A) satellite multispectral imagery, as well as very-high-resolution unmanned aerial vehicle (UAV) multispectral imagery.</p><sec id=\"sec2dot2dot1-plants-14-03436\"><title>2.2.1. BJ3N</title><p>The BJ3N satellite was launched on 29 April 2021, as a state-of-the-art, ultra-high-resolution optical remote sensing satellite, jointly developed by China 21st Century Space Technology Co., Ltd. (Beijing, China) and Airbus. It acquires imagery at a spatial resolution of 0.3 m in the panchromatic band and 1.2 m in the multispectral bands, ranking among the highest-resolution commercial remote sensing currently in operation. The main parameters of BJ3N are shown in <xref rid=\"plants-14-03436-t001\" ref-type=\"table\">Table 1</xref>.</p><p>The imagery of BJ3N for the study area was acquired on 8 December 2021. The processing applied to the BJ3N data included calibration, atmospheric correction, and orthorectification.</p><p>To fully leverage the spatial information of BJ3N data and enhance the spatial resolution of its multispectral bands, this study adopts the Gram&#8211;Schmidt pansharpening method for data fusion. This approach establishes a transformation by simulating a panchromatic band, enabling the effective incorporation of spatial details while minimizing spectral distortion.</p></sec><sec id=\"sec2dot2dot2-plants-14-03436\"><title>2.2.2. BJ3A</title><p>The BJ3A satellite was launched on 11 June 2021 and was developed by China 21st Century Space Technology Co., Ltd. It is known for its &#8220;three supers&#8221; feature, which includes ultra-high agility, stability, and precision. The satellite is equipped with a high-resolution, wide-swath panchromatic and multispectral bi-directional camera system, providing imagery at spatial resolutions of 0.5 m for the panchromatic band and 2 m for the multispectral bands. The main parameters of BJ3A are shown in <xref rid=\"plants-14-03436-t002\" ref-type=\"table\">Table 2</xref>.</p><p>The imagery of BJ3A for the study area was acquired on 6 December 2021. The data preprocessing procedure applied to the BJ3A imagery was consistent with that used for the BJ3N data.</p></sec><sec id=\"sec2dot2dot3-plants-14-03436\"><title>2.2.3. UAV</title><p>The multispectral UAV imagery for the study area was acquired using a DJI P4 Multispectral RTK (DJI, Shenzhen, China). The UAV is equipped with an integrated MicaSense RedEdge multispectral sensor and a high-precision real-time kinematic (RTK) module, which provides centimeter-level positioning accuracy to support the generation of high-accuracy geotagged images. The sensor captures imagery in five spectral bands within the visible to red-edge and infrared spectrum. The main characteristics of the multispectral sensor are given in <xref rid=\"plants-14-03436-t003\" ref-type=\"table\">Table 3</xref>.</p><p>The multispectral UAV remote sensing data were acquired on December 18, 2021, at around 11 a.m. local time. The weather conditions during the flight were adequate with enough solar illumination, calm wind with a slight breeze, and no clouds. The raw imagery was processed using DJI Zhitu software 3.0.0, to produce a digital orthophoto map of the study area with a spatial resolution of 0.07 m.</p></sec></sec><sec id=\"sec2dot3-plants-14-03436\"><title>2.3. Sample Plotting</title><p>The samples were divided into a training set and a validation set, which were collected from two spatially segregated areas within the study area to ensure independence, as illustrated in <xref rid=\"plants-14-03436-f001\" ref-type=\"fig\">Figure 1</xref>.</p><sec id=\"sec2dot3dot1-plants-14-03436\"><title>2.3.1. Training Set</title><p>Pine trees infected with PWN disease typically exhibit visible symptoms. As the infection progresses, the needles of affected trees transition from green to yellowish-brown, then reddish-brown, and eventually dark reddish-brown upon complete death [<xref rid=\"B18-plants-14-03436\" ref-type=\"bibr\">18</xref>]. These color shifts are accompanied by changes in the reflectance of specific wavelength bands, causing the spectral characteristics of the infected trees to deviate from those of healthy vegetation. These spectral changes provide a robust theoretical foundation for detecting PWN-infected trees using optical remote sensing imagery [<xref rid=\"B19-plants-14-03436\" ref-type=\"bibr\">19</xref>]. Notably, healthy pine trees can be easily distinguished from those infected with PWN in the visible light spectrum [<xref rid=\"B20-plants-14-03436\" ref-type=\"bibr\">20</xref>]. Based on these symptomatic color features, this study establishes visual interpretation markers for identifying PWN-infected trees.</p><p>Given the diversity of features within the forest area and the influence of light and shadow, this study selects a sub-region with a high concentration of PWN-infected trees as a common training sample area for all three types of remote sensing data. The sample area was characterized by five main feature types: PWN-infected trees, healthy trees, lake, land, and other. Corresponding labels for these features were manually delineated on the BJ3A, BJ3N, and UAV imagery through visual interpretation. These labeled datasets were then used for model training. The image data and corresponding labels for the selected sample areas are presented in <xref rid=\"plants-14-03436-t004\" ref-type=\"table\">Table 4</xref>.</p></sec><sec id=\"sec2dot3dot2-plants-14-03436\"><title>2.3.2. Validation Sets</title><p>To evaluate the accuracy of the BJ3A, BJ3N, and UAV imagery in detecting PWN-infected trees, a field survey was conducted to collect the geographic coordinates of 51 PWN-infected trees using a GNSS receiver as validation sets (<xref rid=\"plants-14-03436-f002\" ref-type=\"fig\">Figure 2</xref>).</p></sec></sec><sec sec-type=\"methods\" id=\"sec2dot4-plants-14-03436\"><title>2.4. Research Methodology</title><sec id=\"sec2dot4dot1-plants-14-03436\"><title>2.4.1. U-Net Network Model</title><p>Semantic segmentation networks are widely used deep learning models for identifying PWN-infected trees in high-resolution remote sensing imagery [<xref rid=\"B21-plants-14-03436\" ref-type=\"bibr\">21</xref>,<xref rid=\"B22-plants-14-03436\" ref-type=\"bibr\">22</xref>,<xref rid=\"B23-plants-14-03436\" ref-type=\"bibr\">23</xref>]. Previous studies have demonstrated the effectiveness of U-Net in this context. For example, Ye et al. [<xref rid=\"B23-plants-14-03436\" ref-type=\"bibr\">23</xref>] compared the performance of U-Net and SVM algorithms for identifying PWN-infected trees using Landsat imagery, with U-Net achieving an accuracy of 0.60, compared to 0.21 for SVM. Similarly, Han et al. [<xref rid=\"B24-plants-14-03436\" ref-type=\"bibr\">24</xref>] evaluated several deep learning algorithms and found that the U-Net model yielded the highest accuracy for detecting PWN outbreaks in high-resolution imagery. Based on these findings, the U-Net model was selected as the most suitable approach for extracting PWN-infected trees in this study.</p><p>The U-Net network model [<xref rid=\"B25-plants-14-03436\" ref-type=\"bibr\">25</xref>], initially introduced in 2015 at the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), is a classic architecture for semantic segmentation. It builds upon the Fully Convolutional Network (FCN) by incorporating convolutional layers, max-pooling layers, inverse convolutional layers, and ReLU nonlinear activation functions. It utilizes a symmetric U-shaped architecture, consisting of two main components: the encoder and the decoder. The encoder, situated on the left, performs downsampling to extract imagery details, whereas the decoder on the right executes upsampling to restore these details, thus enabling the accurate localization of the target [<xref rid=\"B25-plants-14-03436\" ref-type=\"bibr\">25</xref>]. This model exhibits strong segmentation capabilities, effectively fusing both high- and low-dimensional information. It can be trained with relatively limited data, yielding a robust model for edge extraction [<xref rid=\"B26-plants-14-03436\" ref-type=\"bibr\">26</xref>]. The network ultimately produces a feature map with the same resolution as the input image.</p></sec><sec id=\"sec2dot4dot2-plants-14-03436\"><title>2.4.2. Accuracy Evaluation Method</title><p>To evaluate model performance, this study employs precision (P), recall (R), and the F1 score [<xref rid=\"B27-plants-14-03436\" ref-type=\"bibr\">27</xref>]. These accuracy metrics are defined as follows:<disp-formula id=\"FD1-plants-14-03436\"><label>(1)</label><mml:math id=\"mm1\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD2-plants-14-03436\"><label>(2)</label><mml:math id=\"mm2\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id=\"FD3-plants-14-03436\"><label>(3)</label><mml:math id=\"mm3\" display=\"block\" overflow=\"scroll\"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel=\"0\" displaystyle=\"true\"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>P</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>\nwhere TP denotes the number of correctly identified infected trees, FP denotes the number of falsely identified trees, and FN denotes the number of undetected infected trees.</p><p>P, also known as the positive predictive value, reflects the proportion of true positives among all predicted positives, indicating the accuracy of the prediction model. A higher P value suggests a greater likelihood of correct predictions, and consequently, better model performance. R, or sensitivity, measures the proportion of actual infected trees correctly identified by the model. A higher R value indicates a better ability to correctly predict the true positives. The F1 score is the harmonic mean of precision and recall, balancing the tradeoff between the two metrics. Since precision and recall are often inversely related, improving one typically sacrifices the other. Thus, the F1 score provides a comprehensive measure of model performance, where a higher value of F1 indicates an optimal balance between precision and recall, reflecting a more accurate and reliable model.</p></sec></sec></sec><sec sec-type=\"results\" id=\"sec3-plants-14-03436\"><title>3. Results and Analyses</title><sec id=\"sec3dot1-plants-14-03436\"><title>3.1. Experimental Setup</title><p>The U-Net network model was conducted in Matlab software R2023b, on a computer equipped with an AMD Ryzen 7 5800H CPU, 16 GB of memory, and an NVIDIA GeForce RTX 3050 Ti graphics card. A set of training options was configured through programming, and the Stochastic Gradient Descent with Momentum (SGDM) optimization algorithm was used for training with momentum. During the training process, the base learning rate was set to 0.01, with 2000 iterations per epoch. Each iteration used a mini-batch size of 8, and MaxEpochs was set to 1. The BJ3A, BJ3N, and UAV image data were paired with the corresponding sample training set. In each iteration of every training epoch, mini-batches consisting of eight image patches of size 128 &#215; 128 pixels were fed into the network. To avoid excessive memory usage by large images and effectively augment the available training data, 16,000 mini-batches were processed per epoch.</p></sec><sec id=\"sec3dot2-plants-14-03436\"><title>3.2. Identifying PWN-Infected Trees Using BJ3N Imagery</title><sec id=\"sec3dot2dot1-plants-14-03436\"><title>3.2.1. Using Multispectral Imagery with Raw Spatial Resolution of 1.2 m</title><p>The BJ3N satellite image comprises six raw multispectral bands at a resolution of 1.2 m, including red (R), green (G), blue (B), near-infrared (NIR), red-edge, and deep-blue. Four different band combinations were sequentially used as input to the training samples: the three-band (R, G, B), four-band (R, G, B, NIR), five-band (R, G, B, NIR, Red-edge), and six-band (R, G, B, NIR, Red-edge, Deep-blue) combinations. For each combination, a corresponding U-Net model was constructed under the specified training environment and parameters, incorporating the respective labeled data. The models&#8217; accuracies were evaluated using validation sets, as presented in <xref rid=\"plants-14-03436-t005\" ref-type=\"table\">Table 5</xref>.</p><p>Four raw band combinations of the BJ3N satellite at 1.2 m resolution showed low accuracy in identifying PWN-infected trees, and these identification results are illustrated in <xref rid=\"plants-14-03436-f003\" ref-type=\"fig\">Figure 3</xref>. The three-band combination with red (R), green (G), and blue (B) achieved the highest F1 score of 66.6%. The four-band combination (R, G, B, and NIR) and the five-band combination (R, G, B, NIR, and Red-edge) yielded F1 scores of 50.5% and 50.4%, respectively, while the six-band combination (R, G, B, NIR, Red-edge, and Deep-blue) performed the worst, with an F1 score of only 24%.</p></sec><sec id=\"sec3dot2dot2-plants-14-03436\"><title>3.2.2. Using Multispectral Imagery with a Fused Spatial Resolution of 0.3 m</title><p>The BJ3N satellite&#8217;s 0.3 m panchromatic band and six 1.2 m multispectral bands were fused using the pansharpening fusion method to generate six fused multispectral bands at a spatial resolution of 0.3 m. Like raw spatial resolution multispectral imagery, the fused three-band (R, G, B), fused four-band (R, G, B, NIR), fused five-band (R, G, B, NIR, Red-edge), and fused six-band (R, G, B, NIR, Red-edge, Deep-blue) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The models&#8217; accuracy was evaluated using validation sets, as presented in <xref rid=\"plants-14-03436-t005\" ref-type=\"table\">Table 5</xref>.</p><p>The accuracy of identifying PWN-infected trees using the fused 0.3 m BJ3N multispectral imagery was significantly higher than that achieved with the raw 1.2 m data, with only minor differences observed among the various band combinations. Among these, the fused five-band combination (R, G, B, NIR, Red-edge) achieved the highest F1 value of 88.9%, followed by the fused three-band (R, G, B) combination with an F1 value of 85.1%, and the fused six-band (R, G, B, NIR, Red-edge, Deep-blue) combination with an F1 value of 84.3%. The fused four-band (R, G, B, NIR) combination demonstrated the lowest precision, with an F1 value of 81.9%. All these identification results are illustrated in <xref rid=\"plants-14-03436-f004\" ref-type=\"fig\">Figure 4</xref>.</p></sec></sec><sec id=\"sec3dot3-plants-14-03436\"><title>3.3. Identifying PWN-Infected Trees Using BJ3A Imagery</title><sec id=\"sec3dot3dot1-plants-14-03436\"><title>3.3.1. Using Multispectral Imagery with a Raw Spatial Resolution of 2 m</title><p>The BJ3A satellite image comprises four raw multispectral bands with a spatial resolution of 2 m, including R, G, B, and NIR. During the training process, three-band combinations (R/G/B) and four-band combinations (R/G/B/NIR) serve as input variables for the U-Net model. The model was built using the predefined training environment and parameters, along with the corresponding labeled data. The models&#8217; accuracy was evaluated using the test samples, as summarized in <xref rid=\"plants-14-03436-t006\" ref-type=\"table\">Table 6</xref>.</p><p>Two raw band combinations of the BJ3A satellite at 2 m resolution showed limited accuracy in identifying PWN-infected trees, with F1 scores of 58% for the three-band (R, G, B) and 28% for the four-band (R, G, B, NIR) combination. All these identification results are illustrated in <xref rid=\"plants-14-03436-f005\" ref-type=\"fig\">Figure 5</xref>.</p></sec><sec id=\"sec3dot3dot2-plants-14-03436\"><title>3.3.2. Using Multispectral Imagery with a Fused Spatial Resolution of 0.5 m</title><p>The pansharpening fusion technique was applied to combine the 0.5 m panchromatic band with the four 2 m multispectral bands from the BJ3A satellite, resulting in four fused multispectral bands at 0.5 m resolution. The fused three-band (R, G, B) and fused four-band (R, G, B, NIR) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The models&#8217; accuracy was evaluated using validation sets, as presented in <xref rid=\"plants-14-03436-t006\" ref-type=\"table\">Table 6</xref>, and the identification results are illustrated in <xref rid=\"plants-14-03436-f006\" ref-type=\"fig\">Figure 6</xref>.</p><p>The accuracy of identifying PWN-infected trees was significantly improved by using 0.5 m fused band combinations of the BJ3A satellite, compared with the raw band combinations. Notably, the F1 score for the fused three-band combination of R, G, and B reached 86.3%. The fused four-band combination of R, G, B, and NIR exhibited a similar performance, with an F1 score of 85.8%, indicating comparable performance between the two band combinations in identifying PWN-infected trees.</p></sec></sec><sec id=\"sec3dot4-plants-14-03436\"><title>3.4. Identifying PWN-Infected Trees Using UAV Multispectral Imagery</title><p>The UAV multispectral imagery used in this study has five spectral bands: R, G, B, NIR, and Red-edge. The three-band (R, G, B), four-band (R, G, B, NIR), and five-band (R, G, B, NIR, Red-edge) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The accuracy of each model was assessed based on the validation sets, as summarized in <xref rid=\"plants-14-03436-t007\" ref-type=\"table\">Table 7</xref>.</p><p>Comparison with BJ3N and BJ3A satellite imagery, 0.07 m UAV multispectral imagery achieved the highest accuracy in identifying PWN-infected trees. Among UAV band combinations, there were few differences in the identification accuracy. The five-band combinations of R, G, B, NIR, and Red-edge demonstrated the highest accuracy, achieving an F1 score of 89.1%. Both the four-band combinations of R, G, B, and NIR and the three-band combinations of R, G, and B exhibited similar performance, with F1 scores of 87.7% and 88.5%, respectively. All these identification results are illustrated in <xref rid=\"plants-14-03436-f007\" ref-type=\"fig\">Figure 7</xref>.</p></sec></sec><sec sec-type=\"discussion\" id=\"sec4-plants-14-03436\"><title>4. Discussion</title><sec id=\"sec4dot1-plants-14-03436\"><title>4.1. Effect of Spatial Resolution of Remote Sensing Images on Identification of PWN-Infected Trees</title><p>When comparing the F1 accuracy of BJ3N, BJ3A, and UAV band combination in identifying PWN-infected trees separately, it was found that there was a clear relationship between spatial resolution and identifying accuracy. The UAV imagery at 0.07 m resolution achieved the highest F1 score of 89.1%, followed by the fused BJ3N imagery at 0.3 m and the fused BJ3A imagery at 0.5 m, with F1 scores of 88.9% and 88.5%, respectively. In comparison, the raw-resolution BJ3N imagery at 1.2 m and raw-resolution BJ3A imagery at 2.0 m attained maximum F1 scores of only 66.6% and 58.0%, respectively. These results demonstrate that higher spatial resolution in remote sensing imagery leads to improved accuracy in identifying PWN-infected trees. Ota et al. [<xref rid=\"B28-plants-14-03436\" ref-type=\"bibr\">28</xref>] also found that spatial resolution has a certain impact on tree species identification. The accuracy of identifying tree species using 4 m high-resolution remote sensing images is significantly higher than that using 30 m coarse resolution images. However, there is not much difference in the accuracy of identifying tree species using 25 m and 30 m resolution remote sensing images. Similarly, Zhang et al. [<xref rid=\"B10-plants-14-03436\" ref-type=\"bibr\">10</xref>] examined a sample set of semantic segmentation model input sizes and found that increasing the resolution of images enhances the recognition accuracy of each category of the semantic segmentation model, further supporting the importance of spatial resolution in remote sensing-based detection tasks.</p><p>Bolch et al. [<xref rid=\"B29-plants-14-03436\" ref-type=\"bibr\">29</xref>] demonstrated that increasing the spatial resolution of hyperspectral UAV imagery improved the detection accuracy of small invasive aquatic plant patches, though at a significantly higher cost, highlighting the need to balance detection accuracy with resolution in practical applications. Similarly, Saltiel et al. [<xref rid=\"B30-plants-14-03436\" ref-type=\"bibr\">30</xref>] discovered that enhancing image spatial resolution from 22.8 cm to 7.8 cm only marginally improved wetland vegetation recognition accuracy using a deep learning semantic segmentation model, while substantially increasing processing time and reducing coverage. These findings align with the study, in which the F1 accuracy for identifying PWN-infected trees using fused 0.3 m BJ3N multispectral imagery differed by only 0.2% from that achieved with 0.07 m UAV multispectral imagery.</p><p>In contrast, M&#252;llerov&#225; et al. [<xref rid=\"B31-plants-14-03436\" ref-type=\"bibr\">31</xref>] resampled 0.05 m UAV imagery into 0.5 m Pleiades remote sensing image data and found that spatial resolution had little effect on the identification of tree species. However, the study reveals that for the interval of spatial resolution between 0.07 m UAV images and 0.5 m satellite remote sensing images, there is a certain impact on recognizing tree species, with the F1 value decreasing by 2.8%.</p><p>Based on the above discussion, it is evident that 0.3 m ultra-high-resolution may represent the optimal choice for detecting PWN-infected trees in a subtropical mountain monsoon climate zone.</p></sec><sec id=\"sec4dot2-plants-14-03436\"><title>4.2. Effect of Spectral Features of Remote Sensing Imagery on Identification of PWN-Infected Trees</title><p>Based on the analysis of 0.07 m UAV imagery, the identification accuracy achieved with the three-band combination (R, G, B) was comparable to that of the four-band and five-band combinations across precision, recall, and F1-score metrics. A similar pattern was observed for both BJ3N and BJ3A imagery, where no significant differences in identification accuracy were found among the various band combinations at same spatial resolutions. Collectively, these findings indicate that the number of spectral bands has a negligible impact on the accuracy of identifying PWN-infected trees, and that a limited number of bands can be sufficient to achieve high performance.</p><p>This phenomenon is supported by Lopatin et al. [<xref rid=\"B32-plants-14-03436\" ref-type=\"bibr\">32</xref>], who compared UAV RGB and hyperspectral imagery for detecting three invasive species in the forests of south-central Chile. Their study demonstrated that the recognition accuracy of RGB imagery using only three bands exceeded that of hyperspectral imagery with 41 bands for two of the invasive species. This result challenges the conventional assumption that a higher number of spectral bands necessarily leads to improved classification accuracy, suggesting instead that increasing spectral resolution does not always enhance species identification outcomes. Therefore, the number of spectral features is not a critical factor in identifying PWN-infected trees, and a three-band combination (R, G, B) can be sufficient for effective detection using high-resolution remote sensing imagery.</p></sec></sec><sec sec-type=\"conclusions\" id=\"sec5-plants-14-03436\"><title>5. Conclusions</title><p>This study investigates the identification of PWN-infected trees using ultra-high-resolution optical remote sensing satellite images. By comparing the accuracy of identifying PWN-infected trees using BJ3N, as well as BJ3N and UAV imagery, it was found that the higher the spatial resolution, the higher the accuracy of identifying PWN-infected trees. The significance of spectral features in identifying PWN-infected trees varies among different spatial resolution remote sensing images. Nevertheless, the impact of differences in spectral resolution diminishes for identifying PWN-infected trees at higher spatial resolutions. The ultra-high-resolution BJ3N remote sensing imagery with a spatial resolution of 0.3 m exhibited lower false detection and missed detection rates when identifying trees affected by pine wilt nematode (PWN), indicating that a spatial resolution of 0.3 m is sufficient for detecting PWN-affected trees and is the closest to the resolution saturation point in a subtropical mountain monsoon climate zone. Therefore, ultra-high-resolution optical remote sensing satellite imagery clearly demonstrates the optimal resolution for detecting PWN-infected trees, while also offering advantages such as repeatable data acquisition and extensive spatial coverage. Moreover, ultra-high-resolution remote sensing can be applied to forest disease monitoring and prevention strategies to enhance the practical value of research.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#8217;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, C.W., L.Q. and P.X.; formal analysis, Z.N. and C.W.; funding acquisition, C.W. and L.Q.; investigation, Z.N. and X.M. (Xianjin Meng); methodology, Z.N. and X.M. (Xuelian Meng); software, Z.N.; supervision, Z.N. and L.Q.; validation, Z.N. and X.M. (Xuelian Meng); visualization, C.W. and L.Q.; writing&#8212;original draft, Z.N. and K.Q.; writing&#8212;review and editing, Z.N., L.Q., P.X., X.M. (Xuelian Meng), X.M. (Xianjin Meng), K.Q. and C.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type=\"data-availability\"><title>Data Availability Statement</title><p>The data supporting the study findings are available on request from the corresponding author. The data are not publicly available due to the authors&#8217; plan to conduct a series of follow-up studies based on this dataset.</p></notes><notes notes-type=\"COI-statement\"><title>Conflicts of Interest</title><p>Author Ziqi Nie was employed by the company Aerial Photogrammetry and Remote Sensing Group Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:\n<array orientation=\"portrait\"><tbody><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">PWN</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Pine wood nematode</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">BJ3N</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Beijing 3 International Cooperative Remote Sensing Satellite</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">BJ3A</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Beijing 3A satellite</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">UAV</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Unmanned aerial vehicle</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">ANNs</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">artificial neural networks</td></tr><tr><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">SGDM</td><td align=\"left\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Stochastic Gradient Descent with Momentum</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id=\"B1-plants-14-03436\"><label>1.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ye</surname><given-names>J.</given-names></name></person-group><article-title>Epidemic Status of Pine Wilt Disease in China and Its Prevention and Control Techniques and Counter Measures</article-title><source>Sci. Silvae Sin.</source><year>2019</year><volume>55</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type=\"doi\">10.11707/j.1001-7488.20190901</pub-id></element-citation></ref><ref id=\"B2-plants-14-03436\"><label>2.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Suzuki</surname><given-names>K.</given-names></name></person-group><article-title>Pine Wilt Disease&#8212;A Threat to Pine Forests in Europe</article-title><source>The Pinewood Nematode, Bursaphelenchus xylophilus</source><person-group person-group-type=\"editor\"><name name-style=\"western\"><surname>Mota</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Vieira</surname><given-names>P.</given-names></name></person-group><publisher-name>BRILL</publisher-name><publisher-loc>Leiden, The Netherlands</publisher-loc><year>2004</year><fpage>25</fpage><lpage>30</lpage><pub-id pub-id-type=\"doi\">10.1163/9789047413097_008</pub-id><isbn>978-90-474-1309-7</isbn></element-citation></ref><ref id=\"B3-plants-14-03436\"><label>3.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Qin</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Zhu</surname><given-names>H.</given-names></name></person-group><article-title>Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>162</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs13020162</pub-id></element-citation></ref><ref id=\"B4-plants-14-03436\"><label>4.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Giordan</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Manconi</surname><given-names>A.</given-names></name><name name-style=\"western\"><surname>Tannant</surname><given-names>D.D.</given-names></name><name name-style=\"western\"><surname>Allasia</surname><given-names>P.</given-names></name></person-group><article-title>UAV: Low-Cost Remote Sensing for High-Resolution Investigation of Landslides</article-title><source>Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</source><conf-loc>Milan, Italy</conf-loc><conf-date>26&#8211;31 July 2025</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Milan, Italy</publisher-loc><year>2015</year><fpage>5344</fpage><lpage>5347</lpage><pub-id pub-id-type=\"doi\">10.1109/IGARSS.2015.7327042</pub-id></element-citation></ref><ref id=\"B5-plants-14-03436\"><label>5.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Li</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Ding</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>F.</given-names></name></person-group><article-title>The Detection of Pine Wilt Disease: A Literature Review</article-title><source>Int. J. Mol. Sci.</source><year>2022</year><volume>23</volume><elocation-id>10797</elocation-id><pub-id pub-id-type=\"doi\">10.3390/ijms231810797</pub-id><pub-id pub-id-type=\"pmid\">36142710</pub-id><pub-id pub-id-type=\"pmcid\">PMC9505960</pub-id></element-citation></ref><ref id=\"B6-plants-14-03436\"><label>6.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wulder</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Dymond</surname><given-names>C.C.</given-names></name><name name-style=\"western\"><surname>White</surname><given-names>J.C.</given-names></name><name name-style=\"western\"><surname>Erickson</surname><given-names>R.D.</given-names></name></person-group><article-title>Detection, Mapping, and Monitoring of the Mountain Pine Beetle</article-title><source>The Mountain Pine Beetle: A Synthesis of Biology, Management, and Impacts on Lodgepole Pine</source><publisher-name>Natural Resources Canada, Canadian Forest Service, Pacific Forestry Centre</publisher-name><publisher-loc>Victoria, BC, Canada</publisher-loc><year>2006</year><isbn>978-0-662-42623-3</isbn></element-citation></ref><ref id=\"B7-plants-14-03436\"><label>7.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>S.-R.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>W.-K.</given-names></name><name name-style=\"western\"><surname>Lim</surname><given-names>C.-H.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Kafatos</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>S.-H.</given-names></name><name name-style=\"western\"><surname>Lee</surname><given-names>S.-S.</given-names></name></person-group><article-title>Hyperspectral Analysis of Pine Wilt Disease to Determine an Optimal Detection Index</article-title><source>Forests</source><year>2018</year><volume>9</volume><elocation-id>115</elocation-id><pub-id pub-id-type=\"doi\">10.3390/f9030115</pub-id></element-citation></ref><ref id=\"B8-plants-14-03436\"><label>8.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Liao</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Cheng</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>Q.</given-names></name></person-group><article-title>Extraction of the individual tree infected by pine wilt disease using unmanned aerial vehicle optical imagery</article-title><source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2020</year><volume>XLIII-B3-2020</volume><fpage>247</fpage><lpage>252</lpage><pub-id pub-id-type=\"doi\">10.5194/isprs-archives-XLIII-B3-2020-247-2020</pub-id></element-citation></ref><ref id=\"B9-plants-14-03436\"><label>9.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Qin</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>He</surname><given-names>A.</given-names></name></person-group><article-title>Recognition of Abnormal Individuals Based on Lightweight Deep Learning Using Aerial Images in Complex Forest Landscapes: A Case Study of Pine Wood Nematode</article-title><source>Remote Sens.</source><year>2023</year><volume>15</volume><elocation-id>1181</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs15051181</pub-id></element-citation></ref><ref id=\"B10-plants-14-03436\"><label>10.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Ye</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>B.</given-names></name><name name-style=\"western\"><surname>Hao</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>H.</given-names></name></person-group><article-title>A Spatiotemporal Change Detection Method for Monitoring Pine Wilt Disease in a Complex Landscape Using High-Resolution Remote Sensing Imagery</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>2083</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs13112083</pub-id></element-citation></ref><ref id=\"B11-plants-14-03436\"><label>11.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Kim</surname><given-names>J.-B.</given-names></name><name name-style=\"western\"><surname>Jo</surname><given-names>M.-H.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>I.-H.</given-names></name><name name-style=\"western\"><surname>Kim</surname><given-names>Y.-K.</given-names></name></person-group><article-title>A Study on the Extraction of Damaged Area by Pine Wood Nematode Using High Resolution IKONOS Satellite Images and GPS</article-title><source>J. Korean Soc. For. Sci.</source><year>2003</year><volume>92</volume><fpage>362</fpage><lpage>366</lpage></element-citation></ref><ref id=\"B12-plants-14-03436\"><label>12.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>White</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wulder</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Brooks</surname><given-names>D.</given-names></name><name name-style=\"western\"><surname>Reich</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Wheate</surname><given-names>R.</given-names></name></person-group><article-title>Detection of Red Attack Stage Mountain Pine Beetle Infestation with High Spatial Resolution Satellite Imagery</article-title><source>Remote Sens. Environ.</source><year>2005</year><volume>96</volume><fpage>340</fpage><lpage>351</lpage><pub-id pub-id-type=\"doi\">10.1016/j.rse.2005.03.007</pub-id></element-citation></ref><ref id=\"B13-plants-14-03436\"><label>13.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Franklin</surname><given-names>S.E.</given-names></name><name name-style=\"western\"><surname>Wulder</surname><given-names>M.A.</given-names></name><name name-style=\"western\"><surname>Skakun</surname><given-names>R.S.</given-names></name><name name-style=\"western\"><surname>Carroll</surname><given-names>A.L.</given-names></name></person-group><article-title>Mountain Pine Beetle Red-Attack Forest Damage Classification Using Stratified Landsat TM Data in British Columbia, Canada</article-title><source>Photogramm. Eng. Remote Sens.</source><year>2003</year><volume>69</volume><fpage>283</fpage><lpage>288</lpage><pub-id pub-id-type=\"doi\">10.14358/PERS.69.3.283</pub-id></element-citation></ref><ref id=\"B14-plants-14-03436\"><label>14.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Skakun</surname><given-names>R.S.</given-names></name><name name-style=\"western\"><surname>Wulder</surname><given-names>M.A.</given-names></name><name name-style=\"western\"><surname>Franklin</surname><given-names>S.E.</given-names></name></person-group><article-title>Sensitivity of the Thematic Mapper Enhanced Wetness Difference Index to Detect Mountain Pine Beetle Red-Attack Damage</article-title><source>Remote Sens. Environ.</source><year>2003</year><volume>86</volume><fpage>433</fpage><lpage>443</lpage><pub-id pub-id-type=\"doi\">10.1016/S0034-4257(03)00112-3</pub-id></element-citation></ref><ref id=\"B15-plants-14-03436\"><label>15.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>G.</given-names></name></person-group><article-title>Satellite Remote Sensing Identification of Discolored Standing Trees for Pine Wilt Disease Based on Semi-Supervised Deep Learning</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>5936</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs14235936</pub-id></element-citation></ref><ref id=\"B16-plants-14-03436\"><label>16.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Poona</surname><given-names>N.K.</given-names></name><name name-style=\"western\"><surname>Ismail</surname><given-names>R.</given-names></name></person-group><article-title>Discriminating the Occurrence of Pitch Canker Infection in Pinus Radiata Forests Using High Spatial Resolution QuickBird Data and Artificial Neural Networks</article-title><source>Proceedings of the 2012 IEEE International Geoscience and Remote Sensing Symposium</source><conf-loc>Munich, Germany</conf-loc><conf-date>22&#8211;27 July 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Munich, Germany</publisher-loc><year>2012</year><fpage>3371</fpage><lpage>3374</lpage><pub-id pub-id-type=\"doi\">10.1109/IGARSS.2012.6350698</pub-id></element-citation></ref><ref id=\"B17-plants-14-03436\"><label>17.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Takenaka</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Katoh</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Deng</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Cheung</surname><given-names>K.</given-names></name></person-group><article-title>Detecting forests damaged by pine wilt disease at the individual tree level using airborne laser data and worldview-2/3 images over two seasons</article-title><source>Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2017</year><volume>XLII-3/W3</volume><fpage>181</fpage><lpage>184</lpage><pub-id pub-id-type=\"doi\">10.5194/isprs-archives-XLII-3-W3-181-2017</pub-id></element-citation></ref><ref id=\"B18-plants-14-03436\"><label>18.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Vollenweider</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>G&#252;nthardt-Goerg</surname><given-names>M.S.</given-names></name></person-group><article-title>Diagnosis of Abiotic and Biotic Stress Factors Using the Visible Symptoms in Foliage</article-title><source>Environ. Pollut.</source><year>2005</year><volume>137</volume><fpage>455</fpage><lpage>465</lpage><pub-id pub-id-type=\"doi\">10.1016/j.envpol.2005.01.032</pub-id><pub-id pub-id-type=\"pmid\">16005758</pub-id></element-citation></ref><ref id=\"B19-plants-14-03436\"><label>19.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Wang</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Shu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Wu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Ji</surname><given-names>Y.</given-names></name></person-group><article-title>Application of Imaging Spectral Remote Sensing Techniques in Monitoring of Forestry Disease and Insect Pests</article-title><source>China Plant Prot.</source><year>2018</year><volume>38</volume><fpage>24</fpage><lpage>28</lpage><pub-id pub-id-type=\"doi\">10.3969/j.issn.1672-6820.2018.01.004</pub-id></element-citation></ref><ref id=\"B20-plants-14-03436\"><label>20.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Qin</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Tang</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Huang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Song</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Min</surname><given-names>J.</given-names></name></person-group><article-title>Spectral Characteristics and Evaluation Model of <italic toggle=\"yes\">Pinus massoniana</italic> Suffering from <italic toggle=\"yes\">Bursaphelenchus xylophilus</italic> Disease</article-title><source>Spectrosc. Spectr. Anal.</source><year>2019</year><volume>39</volume><fpage>865</fpage><lpage>872</lpage><pub-id pub-id-type=\"doi\">10.3964/j.issn.1000-0593(2019)03-0865-08</pub-id></element-citation></ref><ref id=\"B21-plants-14-03436\"><label>21.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Huang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Lu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>L.</given-names></name><name name-style=\"western\"><surname>Sun</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Fang</surname><given-names>G.</given-names></name></person-group><article-title>Accurate Identification of Pine Wood Nematode Disease with a Deep Convolution Neural Network</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>913</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs14040913</pub-id></element-citation></ref><ref id=\"B22-plants-14-03436\"><label>22.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>R.</given-names></name><name name-style=\"western\"><surname>Shi</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Yu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>X.</given-names></name><name name-style=\"western\"><surname>Chen</surname><given-names>X.</given-names></name></person-group><article-title>Automatic Detection and Classification of Dead Nematode-Infested Pine Wood in Stages Based on YOLO v4 and GoogLeNet</article-title><source>Forests</source><year>2023</year><volume>14</volume><elocation-id>601</elocation-id><pub-id pub-id-type=\"doi\">10.3390/f14030601</pub-id></element-citation></ref><ref id=\"B23-plants-14-03436\"><label>23.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ye</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Lao</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Liu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Chang</surname><given-names>C.-C.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Li</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>H.</given-names></name></person-group><article-title>Pine Pest Detection Using Remote Sensing Satellite Images Combined with a Multi-Scale Attention-UNet Model</article-title><source>Ecol. Inform.</source><year>2022</year><volume>72</volume><fpage>101906</fpage><pub-id pub-id-type=\"doi\">10.1016/j.ecoinf.2022.101906</pub-id></element-citation></ref><ref id=\"B24-plants-14-03436\"><label>24.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Han</surname><given-names>Z.</given-names></name><name name-style=\"western\"><surname>Hu</surname><given-names>W.</given-names></name><name name-style=\"western\"><surname>Peng</surname><given-names>S.</given-names></name><name name-style=\"western\"><surname>Lin</surname><given-names>H.</given-names></name><name name-style=\"western\"><surname>Zhang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Zhou</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Wang</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Dian</surname><given-names>Y.</given-names></name></person-group><article-title>Detection of Standing Dead Trees after Pine Wilt Disease Outbreak with Airborne Remote Sensing Imagery by Multi-Scale Spatial Attention Deep Learning and Gaussian Kernel Approach</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>3075</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs14133075</pub-id></element-citation></ref><ref id=\"B25-plants-14-03436\"><label>25.</label><element-citation publication-type=\"confproc\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ronneberger</surname><given-names>O.</given-names></name><name name-style=\"western\"><surname>Fischer</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>Brox</surname><given-names>T.</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#8211;9 October 2015</conf-date><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2015</year><volume>Volume 9351</volume><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type=\"doi\">10.1007/978-3-319-24574-4_28</pub-id></element-citation></ref><ref id=\"B26-plants-14-03436\"><label>26.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhu</surname><given-names>Y.</given-names></name><name name-style=\"western\"><surname>Xu</surname><given-names>Q.</given-names></name><name name-style=\"western\"><surname>Yang</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Mo</surname><given-names>H.</given-names></name></person-group><article-title>Full Convolution Neural Network Based Building Extraction Approach from High Resolution Aerial Image</article-title><source>Geomat. World</source><year>2020</year><volume>27</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type=\"doi\">10.3969/j.issn.1672-1586.2020.02.017</pub-id></element-citation></ref><ref id=\"B27-plants-14-03436\"><label>27.</label><element-citation publication-type=\"book\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Zhou</surname><given-names>Z.</given-names></name></person-group><source>Machine Learning</source><publisher-name>Tsinghua University Press</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2016</year></element-citation></ref><ref id=\"B28-plants-14-03436\"><label>28.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Ota</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Mizoue</surname><given-names>N.</given-names></name><name name-style=\"western\"><surname>Yoshida</surname><given-names>S.</given-names></name></person-group><article-title>Influence of Using Texture Information in Remote Sensed Data on the Accuracy of Forest Type Classification at Different Levels of Spatial Resolution</article-title><source>J. For. Res.</source><year>2011</year><volume>16</volume><fpage>432</fpage><lpage>437</lpage><pub-id pub-id-type=\"doi\">10.1007/s10310-010-0233-6</pub-id></element-citation></ref><ref id=\"B29-plants-14-03436\"><label>29.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Bolch</surname><given-names>E.A.</given-names></name><name name-style=\"western\"><surname>Hestir</surname><given-names>E.L.</given-names></name><name name-style=\"western\"><surname>Khanna</surname><given-names>S.</given-names></name></person-group><article-title>Performance and Feasibility of Drone-Mounted Imaging Spectroscopy for Invasive Aquatic Vegetation Detection</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>582</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs13040582</pub-id></element-citation></ref><ref id=\"B30-plants-14-03436\"><label>30.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Saltiel</surname><given-names>T.M.</given-names></name><name name-style=\"western\"><surname>Dennison</surname><given-names>P.E.</given-names></name><name name-style=\"western\"><surname>Campbell</surname><given-names>M.J.</given-names></name><name name-style=\"western\"><surname>Thompson</surname><given-names>T.R.</given-names></name><name name-style=\"western\"><surname>Hambrecht</surname><given-names>K.R.</given-names></name></person-group><article-title>Tradeoffs between UAS Spatial Resolution and Accuracy for Deep Learning Semantic Segmentation Applied to Wetland Vegetation Species Mapping</article-title><source>Remote Sens.</source><year>2022</year><volume>14</volume><elocation-id>2703</elocation-id><pub-id pub-id-type=\"doi\">10.3390/rs14112703</pub-id></element-citation></ref><ref id=\"B31-plants-14-03436\"><label>31.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>M&#252;llerov&#225;</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Br&#367;na</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Bartalo&#353;</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Dvo&#345;&#225;k</surname><given-names>P.</given-names></name><name name-style=\"western\"><surname>V&#237;tkov&#225;</surname><given-names>M.</given-names></name><name name-style=\"western\"><surname>Py&#353;ek</surname><given-names>P.</given-names></name></person-group><article-title>Timing Is Important: Unmanned Aircraft vs. Satellite Imagery in Plant Invasion Monitoring</article-title><source>Front. Plant Sci.</source><year>2017</year><volume>8</volume><elocation-id>887</elocation-id><pub-id pub-id-type=\"doi\">10.3389/fpls.2017.00887</pub-id><pub-id pub-id-type=\"pmid\">28620399</pub-id><pub-id pub-id-type=\"pmcid\">PMC5449470</pub-id></element-citation></ref><ref id=\"B32-plants-14-03436\"><label>32.</label><element-citation publication-type=\"journal\"><person-group person-group-type=\"author\"><name name-style=\"western\"><surname>Lopatin</surname><given-names>J.</given-names></name><name name-style=\"western\"><surname>Dolos</surname><given-names>K.</given-names></name><name name-style=\"western\"><surname>Kattenborn</surname><given-names>T.</given-names></name><name name-style=\"western\"><surname>Fassnacht</surname><given-names>F.E.</given-names></name></person-group><article-title>How Canopy Shadow Affects Invasive Plant Species Classification in High Spatial Resolution Remote Sensing</article-title><source>Remote Sens. Ecol. Conserv.</source><year>2019</year><volume>5</volume><fpage>302</fpage><lpage>317</lpage><pub-id pub-id-type=\"doi\">10.1002/rse2.109</pub-id></element-citation></ref></ref-list></back><floats-group><fig position=\"float\" id=\"plants-14-03436-f001\" orientation=\"portrait\"><label>Figure 1</label><caption><p>The location of the study area in Shaoguan City, Guandong Province, China.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g001.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f002\" orientation=\"portrait\"><label>Figure 2</label><caption><p>Checkpoint data imagery. The numbers in the figures are the numbers of Validation Sets.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g002.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f003\" orientation=\"portrait\"><label>Figure 3</label><caption><p>Results of BJ3N satellite with 0.5 m raw spatial resolution for identifying PWN-infected trees in this study area.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g003.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f004\" orientation=\"portrait\"><label>Figure 4</label><caption><p>Results of BJ3N satellite with 0.3 m fused spatial resolution for identifying PWN-infected trees in this study area.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g004.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f005\" orientation=\"portrait\"><label>Figure 5</label><caption><p>Results of the BJ3A satellite with 2 m raw spatial resolution for identifying PWN-infected trees in this study area.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g005.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f006\" orientation=\"portrait\"><label>Figure 6</label><caption><p>Results of the BJ3A satellite with 0.5 m fused spatial resolution for identifying PWN-infected trees in this study area.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g006.jpg\"/></fig><fig position=\"float\" id=\"plants-14-03436-f007\" orientation=\"portrait\"><label>Figure 7</label><caption><p>Results of UAV multispectral bands with a spatial resolution of 0.7 m for identifying PWN-infected trees in this study area.</p></caption><graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" position=\"float\" orientation=\"portrait\" xlink:href=\"plants-14-03436-g007.jpg\"/></fig><table-wrap position=\"float\" id=\"plants-14-03436-t001\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t001_Table 1</object-id><label>Table 1</label><caption><p>BJ3N satellite parameters.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Category</th><th colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\">Parameter</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Satellite Orbit</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Sun-synchronous orbit</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Orbit Altitude</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">620 km</td></tr><tr><td rowspan=\"2\" align=\"center\" valign=\"middle\" colspan=\"1\">Spatial Resolution</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Panchromatic</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.3 m</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Multispectral</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1.2 m</td></tr><tr><td rowspan=\"7\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">Spectral Bands</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Panchromatic</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">450&#8211;800 nm</td></tr><tr><td rowspan=\"6\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">Multispectral</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Deep Blue: 400&#8211;450 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Blue: 450&#8211;520 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Green: 530&#8211;590 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Red: 620&#8211;690 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Red Edge: 700&#8211;750 nm</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Near-Infrared: 770&#8211;880 nm</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t002\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t002_Table 2</object-id><label>Table 2</label><caption><p>BJ3A satellite parameters.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Category</th><th colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\">Parameter</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Satellite Orbit</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Sun-synchronous orbit</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Orbit Altitude</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">500 km</td></tr><tr><td rowspan=\"2\" align=\"center\" valign=\"middle\" colspan=\"1\">Spatial Resolution</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Panchromatic</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">0.5 m</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Multispectral</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">2.0 m</td></tr><tr><td rowspan=\"5\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">Spectral Bands</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Panchromatic</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">450&#8211;700 nm</td></tr><tr><td rowspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">Multispectral</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Blue: 450&#8211;520 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Green: 520&#8211;590 nm</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Red: 630&#8211;690 nm</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Near-Infrared: 770&#8211;890 nm</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t003\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t003_Table 3</object-id><label>Table 3</label><caption><p>Characteristics of the multispectral sensor.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Category</th><th colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\">Parameter</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Lens</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">FOV: 62.7&#176;; Focal length: 5.74 mm; <break/>Fixed focus at infinity; Aperture: f/2.2</td></tr><tr><td rowspan=\"2\" align=\"center\" valign=\"middle\" colspan=\"1\">Imaging Sensor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">1/2.9inch CMOS</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Including 1 color sensor for visible light imaging and 5 monochrome sensors for multispectral imaging</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Individual Sensor</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">Effective pixels: 2.08 million (total pixels: 2.12 million)</td></tr><tr><td rowspan=\"5\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">Spectral bands</td><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Blue: 434&#8211;466 nm</td></tr><tr><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Green: 544&#8211;576 nm</td></tr><tr><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Red: 634&#8211;666 nm</td></tr><tr><td colspan=\"2\" align=\"center\" valign=\"middle\" rowspan=\"1\">Red Edge: 714&#8211;746 nm</td></tr><tr><td colspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\">Near-Infrared: 814&#8211;866 nm</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t004\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t004_Table 4</object-id><label>Table 4</label><caption><p>Sample area and label drawing results of BJ3A images, BJ3N images, and UAV imagery.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Image Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Sample Area</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Label Drawing Results</th></tr></thead><tbody><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">BJ3A imagery</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i001.jpg\"/>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i002.jpg\"/>\n</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">BJ3N imagery</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i003.jpg\"/>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i004.jpg\"/>\n</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">UAV imagery</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i005.jpg\"/>\n</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">\n<inline-graphic xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"plants-14-03436-i006.jpg\"/>\n</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The sample labels have been categorized into five groups: land is represented by yellow, other features by orange, lakes by blue, healthy trees by green, and PWN-infected trees by red.</p></fn></table-wrap-foot></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t005\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t005_Table 5</object-id><label>Table 5</label><caption><p>The accuracy of identifying PWN-infected trees using the BJ3N satellite&#8217;s multispectral bands with a raw spatial resolution of 1.2 m.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Image Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Resolution/m</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Band Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">P/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">F1/%</th></tr></thead><tbody><tr><td rowspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">raw spatial resolution </td><td rowspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">1.2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR, Red-edge, Deep-blue</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">25.8</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">22.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">24</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR, Red-edge</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">41.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">64.7</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">50.4</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">52.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">49</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">50.5</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R, G, B</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">71.1</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">62.7</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">66.6</td></tr><tr><td rowspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">fused spatial resolution</td><td rowspan=\"4\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">0.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR, Red-edge, Deep-blue</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">84.3</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR, Red-edge</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">91.7</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">86.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">88.9</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.7</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">81.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">81.9</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R, G, B</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">86</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">84.3</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">85.1</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t006\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t006_Table 6</object-id><label>Table 6</label><caption><p>The accuracy of identifying PWN-infected trees using the BJ3A satellite&#8217;s multispectral bands with a raw spatial resolution of 2 m.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Image Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Resolution/m</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Band Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">P/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">F1/%</th></tr></thead><tbody><tr><td rowspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">raw spatial resolution </td><td rowspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">2</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">22.4</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">37.3</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">28</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R, G, B</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">64.3</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">52.9</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">58</td></tr><tr><td rowspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">fused spatial resolution </td><td rowspan=\"2\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">0.5</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.4</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">82.4</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">85.8</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R, G, B</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">93.2</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">80.4</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">86.3</td></tr></tbody></table></table-wrap><table-wrap position=\"float\" id=\"plants-14-03436-t007\" orientation=\"portrait\"><object-id pub-id-type=\"pii\">plants-14-03436-t007_Table 7</object-id><label>Table 7</label><caption><p>The accuracy of identifying PWN-infected trees using the UAV multispectral bands with a spatial resolution of 0.7 m.</p></caption><table frame=\"hsides\" rules=\"groups\"><thead><tr><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Image Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Resolution/m</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">Band Type</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">P/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R/%</th><th align=\"center\" valign=\"middle\" style=\"border-top:solid thin;border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">F1/%</th></tr></thead><tbody><tr><td rowspan=\"3\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">UAV multispectral imagery</td><td rowspan=\"3\" align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" colspan=\"1\">0.07</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR, Red-edge</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">83.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">96.1</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">89.1</td></tr><tr><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">R, G, B, NIR</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">79.4</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">98</td><td align=\"center\" valign=\"middle\" rowspan=\"1\" colspan=\"1\">87.7</td></tr><tr><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">R, G, B</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">80.6</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">98</td><td align=\"center\" valign=\"middle\" style=\"border-bottom:solid thin\" rowspan=\"1\" colspan=\"1\">88.5</td></tr></tbody></table></table-wrap></floats-group></article></pmc-articleset>",
  "text": "pmc Plants (Basel) Plants (Basel) 2909 plants plants Plants 2223-7747 Multidisciplinary Digital Publishing Institute (MDPI) PMC12656421 PMC12656421.1 12656421 12656421 41304587 10.3390/plants14223436 plants-14-03436 1 Article Ultra-High-Resolution Optical Remote Sensing Satellite Identification of Pine-Wood-Nematode-Infected Trees Nie Ziqi Formal analysis Investigation Methodology Software Validation Writing &#8211; original draft Data curation Writing &#8211; review &amp; editing 1 2 3 Qin Lin Conceptualization Funding acquisition Supervision Visualization Writing &#8211; review &amp; editing 4 Xing Peng Conceptualization Data curation Writing &#8211; review &amp; editing 5 https://orcid.org/0000-0001-6953-1916 Meng Xuelian Methodology Writing &#8211; review &amp; editing 6 Meng Xianjin Investigation Data curation Validation 4 Qin Kaitong Writing &#8211; original draft Writing &#8211; review &amp; editing 1 7 Wang Changwei Conceptualization Formal analysis Funding acquisition Visualization Writing &#8211; original draft Writing &#8211; review &amp; editing Project administration 1 7 * Rossi Vittorio Academic Editor 1 College of Natural Resources and Environment, South China Agricultural University, Guangzhou 510642, China; nzq2022331@163.com (Z.N.); kaitong_qin@163.com (K.Q.) 2 State Key Laboratory of Surveying, Mapping and Remote Sensing Information Engineering, Wuhan University, Wuhan 430079, China 3 Aerial Photogrammetry and Remote Sensing Group Co., Ltd., Xi&#8217;an 710199, China 4 Guangdong Provincial Institute of Forestry Survey and Planning, Guangzhou 510520, China; luckykql@263.net (L.Q.); 18933986718@163.com (X.M.) 5 Guangzhou Institute of Geography, Guangzhou 510070, China; kaiers@126.com 6 Department of Geography &amp; Anthropology, Louisiana State University, Baton Rouge, LA 70803, USA; smeng@lsu.edu 7 Guangdong Engineering Technology Research Center of Land Information, Guangzhou 510642, China * Correspondence: changwei_wang@scau.edu.cn 10 11 2025 11 2025 14 22 501331 3436 18 9 2025 31 10 2025 06 11 2025 10 11 2025 27 11 2025 28 11 2025 &#169; 2025 by the authors. 2025 https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license ( https://creativecommons.org/licenses/by/4.0/ ). The pine wood nematode (PWN), one of the globally significant forest diseases, has driven the demand for precise detection methods. Recent advances in satellite remote sensing technology, particularly ultra-high-resolution optical imagery, have opened new avenues for identifying PWN-infected trees. In order to systematically evaluate the ability of ultra-high-resolution optical remote sensing and the influence of spatial and spectral resolution in detecting PWN-infected trees, this study utilized a U-Net network model to identify PWN-infected trees using three remote sensing datasets of the ultra-high-resolution multispectral imagery from Beijing 3 International Cooperative Remote Sensing Satellite (BJ3N), with a panchromatic band spatial resolution of 0.3 m and six multispectral bands at 1.2 m; the high-resolution multispectral imagery from the Beijing 3A satellite (BJ3A), with a panchromatic band resolution of 0.5 m and four multispectral bands at 2 m; and unmanned aerial vehicle (UAV) imagery with five multispectral bands at 0.07 m. Comparison of the identification results demonstrated that (1) UAV multispectral imagery with 0.07 m spatial resolution achieved the highest accuracy, with an F1 score of 89.1%. Next is the fused ultra-high-resolution BJ3N satellite imagery at 0.3 m, with an F1 score of 88.9%. In contrast, BJ3A imagery with a raw spatial resolution of 2 m performed poorly, with an F1 score of only 28%. These results underscore that finer spatial resolution in remote sensing imagery directly enhances the ability to detect subtle canopy changes indicative of PWN infestation. (2) For UAV, BJ3N, and BJ3A imagery, the identification accuracy for PWN-infected trees showed no significant differences across various band combinations at equivalent spatial resolutions. This indicates that spectral resolution plays a secondary role to spatial resolution in detecting PWN-infected trees using ultra-high-resolution optical imagery. (3) The 0.3 m BJ3N satellite imagery exhibits low false-detection and omission rates, with F1 scores comparable to higher-resolution UAV imagery. This indicates that a spatial resolution of 0.3 m is sufficient for identifying PWN-infected trees and is approaching a point of saturation in a subtropical mountain monsoon climate zone. In conclusion, ultra-high-resolution satellite remote sensing, characterized by frequent data revisit cycles, broad spatial coverage, and balanced spatial-spectral performance, provides an optimal remote sensing data source for identifying PWN-infected trees. As such, it is poised to become a cornerstone of future research and practical applications in detecting and managing PWN infestations globally. pine wood nematode Beijing3N remote sensing image U-NET deep learning image resolution number of bands of the image National Natural Science Foundation of China 42471340 42474045 This work was supported in part by the National Natural Science Foundation of China (Grant No. 42471340, 42474045). pmc-status-qastatus 0 pmc-status-live yes pmc-status-embargo no pmc-status-released yes pmc-prop-open-access yes pmc-prop-olf no pmc-prop-manuscript no pmc-prop-legally-suppressed no pmc-prop-has-pdf yes pmc-prop-has-supplement no pmc-prop-pdf-only no pmc-prop-suppress-copyright no pmc-prop-is-real-version no pmc-prop-is-scanned-article no pmc-prop-preprint no pmc-prop-in-epmc yes 1. Introduction The pine wood nematode (PWN, Bursaphelenchus xylophilus ), the causative agent of pine wilt disease, ranks among the world&#8217;s most destructive forest pathogens due to its rapid spread, high mortality rates, and severe economic impacts [ 1 , 2 ]. Since its first detection in Nanjing, China, in 1982, PWN has devastated over one billion pine trees nationwide, incurring losses exceeding hundreds of billions of yuan [ 3 ]. Early detection of PWN-infected trees remains critical for containing outbreaks, as timely and accurate identification of PWN-infected trees is important for prevention and control [ 4 ]. Traditional methods for identifying PWN-infected trees have relied on labor-intensive field surveys. These approaches lack the capacity for rapid, large-scale monitoring of PWN infestation dynamics and often lead to delayed interventions that hinder effective disease management [ 1 , 5 , 6 , 7 ]. Remote sensing technology has emerged as a primary investigative tool for detecting PWN-infected trees due to its cost-effectiveness, operational efficiency, and streamlined data-collection capabilities [ 8 , 9 , 10 ]. Early identification of PWN-infected trees through satellite remote sensing mainly depended on medium-resolution satellites, such as Landsat, which utilized the spectral differences between the canopies of infected and healthy pine trees [ 11 , 12 ]. Franklin et al. [ 13 ] achieved approximately 73% detection accuracy for PWN-infected trees using single-date 30 m resolution Landsat imagery. Skakun et al. [ 14 ] enhanced PWN identification accuracy by to 78% by analyzing multi-temporal Landsat data in the Prince George Forest Region of British Columbia, Canada. However, mixed-pixel effects in medium-resolution imagery obscure sparse infections, limiting practical utility. Advances in high-resolution imagery (&#8804;1 m) have improved detection precision. Wang et al. [ 15 ] utilized Gaofen-2 satellite imagery with 1 m spatial resolution and four spectral bands, achieving an MIoU of 68.36%. Poona and Ismail [ 16 ] aimed to explore the utility of transformed high spatial resolution QuickBird imagery, which has a 0.6 m spatial resolution and four spectral bands, combined with artificial neural networks (ANNs), to detect and map trees infested with PWN, achieving a kappa coefficient of 0.65. Takenaka et al. [ 17 ] employed WorldView-3 imagery with 0.5 m spatial resolution to construct 18 vegetation indices for identifying PWN-infected trees in the Matsumoto region of central Japan, achieving a total precision of 72%. Despite these improvements, resolution thresholds for reliable PWN detection remain undefined, and spectral-spatial tradeoffs are poorly quantified. The recent deployment of ultra-high-resolution satellites, with a resolution of less than 0.5 m, presents significant potential. The Beijing 3 International Cooperative Remote Sensing Satellite (BJ3N), launched on 29 April 2021, is currently the highest-resolution commercial remote sensing satellite. It offers a 0.3 m panchromatic band and six spectral bands with a 1.2 m resolution, as well as pansharpened 0.3 m fused multispectral images. Such capabilities promise unprecedented precision in detecting subtle canopy changes indicative of early PWN infestation. However, despite this promising potential, the application of ultra-high-resolution optical imagery for PWN detection remains unexplored, and no study has yet systematically evaluated its performance against established platforms like UAVs. To address this research gap, the study conducted a comprehensive comparison of ultra-high-resolution BJ3N imagery with high-resolution Beijing 3A satellite (BJ3A) imagery, which has a 0.5 m spatial resolution for the panchromatic band and 2 m for the four multispectral bands, and UAV multispectral imagery with a 0.07 m spatial resolution for five multispectral bands. The objectives are to systematically evaluate the capabilities of ultra-high-resolution remote sensing images and the impact of spatial and spectral resolution on the detection of PWN-infected trees, and to identify the optimal data sources that can balance spatial resolution, spectral capabilities, coverage, and practical applicability, ultimately enhancing the efficiency of future PWN monitoring. 2. Materials and Methods 2.1. Study Area The study area is located in Shaoguan, Guangdong Province, China (geographical coordinates 113&#176;35&#8242; to 113&#176;37&#8242; E, 24&#176;45&#8242;&#8211;24&#176;48&#8242; N), as shown in Figure 1 . The study area lies within a subtropical mountain monsoon climate zone, characterized by an average annual temperature of approximately 20 &#176;C, an average annual rainfall of 1600 mm, and an annual frost-free period of about 310 days. These climatic conditions are conducive to the proliferation of the pine bark beetle, leading to a particularly severe PWN outbreak in the area. The area covers approximately 5 km 2 , with elevations ranging from 60 to 255 m. Forest coverage reaches around 90%, dominated by species such as horsetail pine, fir, and camphor. A schematic overview of the study area is provided in Figure 1 . 2.2. Data The data used in this study included ultra-high-resolution BJ3N satellite imagery and high-resolution Beijing 3A (BJ3A) satellite multispectral imagery, as well as very-high-resolution unmanned aerial vehicle (UAV) multispectral imagery. 2.2.1. BJ3N The BJ3N satellite was launched on 29 April 2021, as a state-of-the-art, ultra-high-resolution optical remote sensing satellite, jointly developed by China 21st Century Space Technology Co., Ltd. (Beijing, China) and Airbus. It acquires imagery at a spatial resolution of 0.3 m in the panchromatic band and 1.2 m in the multispectral bands, ranking among the highest-resolution commercial remote sensing currently in operation. The main parameters of BJ3N are shown in Table 1 . The imagery of BJ3N for the study area was acquired on 8 December 2021. The processing applied to the BJ3N data included calibration, atmospheric correction, and orthorectification. To fully leverage the spatial information of BJ3N data and enhance the spatial resolution of its multispectral bands, this study adopts the Gram&#8211;Schmidt pansharpening method for data fusion. This approach establishes a transformation by simulating a panchromatic band, enabling the effective incorporation of spatial details while minimizing spectral distortion. 2.2.2. BJ3A The BJ3A satellite was launched on 11 June 2021 and was developed by China 21st Century Space Technology Co., Ltd. It is known for its &#8220;three supers&#8221; feature, which includes ultra-high agility, stability, and precision. The satellite is equipped with a high-resolution, wide-swath panchromatic and multispectral bi-directional camera system, providing imagery at spatial resolutions of 0.5 m for the panchromatic band and 2 m for the multispectral bands. The main parameters of BJ3A are shown in Table 2 . The imagery of BJ3A for the study area was acquired on 6 December 2021. The data preprocessing procedure applied to the BJ3A imagery was consistent with that used for the BJ3N data. 2.2.3. UAV The multispectral UAV imagery for the study area was acquired using a DJI P4 Multispectral RTK (DJI, Shenzhen, China). The UAV is equipped with an integrated MicaSense RedEdge multispectral sensor and a high-precision real-time kinematic (RTK) module, which provides centimeter-level positioning accuracy to support the generation of high-accuracy geotagged images. The sensor captures imagery in five spectral bands within the visible to red-edge and infrared spectrum. The main characteristics of the multispectral sensor are given in Table 3 . The multispectral UAV remote sensing data were acquired on December 18, 2021, at around 11 a.m. local time. The weather conditions during the flight were adequate with enough solar illumination, calm wind with a slight breeze, and no clouds. The raw imagery was processed using DJI Zhitu software 3.0.0, to produce a digital orthophoto map of the study area with a spatial resolution of 0.07 m. 2.3. Sample Plotting The samples were divided into a training set and a validation set, which were collected from two spatially segregated areas within the study area to ensure independence, as illustrated in Figure 1 . 2.3.1. Training Set Pine trees infected with PWN disease typically exhibit visible symptoms. As the infection progresses, the needles of affected trees transition from green to yellowish-brown, then reddish-brown, and eventually dark reddish-brown upon complete death [ 18 ]. These color shifts are accompanied by changes in the reflectance of specific wavelength bands, causing the spectral characteristics of the infected trees to deviate from those of healthy vegetation. These spectral changes provide a robust theoretical foundation for detecting PWN-infected trees using optical remote sensing imagery [ 19 ]. Notably, healthy pine trees can be easily distinguished from those infected with PWN in the visible light spectrum [ 20 ]. Based on these symptomatic color features, this study establishes visual interpretation markers for identifying PWN-infected trees. Given the diversity of features within the forest area and the influence of light and shadow, this study selects a sub-region with a high concentration of PWN-infected trees as a common training sample area for all three types of remote sensing data. The sample area was characterized by five main feature types: PWN-infected trees, healthy trees, lake, land, and other. Corresponding labels for these features were manually delineated on the BJ3A, BJ3N, and UAV imagery through visual interpretation. These labeled datasets were then used for model training. The image data and corresponding labels for the selected sample areas are presented in Table 4 . 2.3.2. Validation Sets To evaluate the accuracy of the BJ3A, BJ3N, and UAV imagery in detecting PWN-infected trees, a field survey was conducted to collect the geographic coordinates of 51 PWN-infected trees using a GNSS receiver as validation sets ( Figure 2 ). 2.4. Research Methodology 2.4.1. U-Net Network Model Semantic segmentation networks are widely used deep learning models for identifying PWN-infected trees in high-resolution remote sensing imagery [ 21 , 22 , 23 ]. Previous studies have demonstrated the effectiveness of U-Net in this context. For example, Ye et al. [ 23 ] compared the performance of U-Net and SVM algorithms for identifying PWN-infected trees using Landsat imagery, with U-Net achieving an accuracy of 0.60, compared to 0.21 for SVM. Similarly, Han et al. [ 24 ] evaluated several deep learning algorithms and found that the U-Net model yielded the highest accuracy for detecting PWN outbreaks in high-resolution imagery. Based on these findings, the U-Net model was selected as the most suitable approach for extracting PWN-infected trees in this study. The U-Net network model [ 25 ], initially introduced in 2015 at the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), is a classic architecture for semantic segmentation. It builds upon the Fully Convolutional Network (FCN) by incorporating convolutional layers, max-pooling layers, inverse convolutional layers, and ReLU nonlinear activation functions. It utilizes a symmetric U-shaped architecture, consisting of two main components: the encoder and the decoder. The encoder, situated on the left, performs downsampling to extract imagery details, whereas the decoder on the right executes upsampling to restore these details, thus enabling the accurate localization of the target [ 25 ]. This model exhibits strong segmentation capabilities, effectively fusing both high- and low-dimensional information. It can be trained with relatively limited data, yielding a robust model for edge extraction [ 26 ]. The network ultimately produces a feature map with the same resolution as the input image. 2.4.2. Accuracy Evaluation Method To evaluate model performance, this study employs precision (P), recall (R), and the F1 score [ 27 ]. These accuracy metrics are defined as follows: (1) P = T P T P + F P (2) R = T P T P + F N (3) F 1 = 2 P &#215; R P + R where TP denotes the number of correctly identified infected trees, FP denotes the number of falsely identified trees, and FN denotes the number of undetected infected trees. P, also known as the positive predictive value, reflects the proportion of true positives among all predicted positives, indicating the accuracy of the prediction model. A higher P value suggests a greater likelihood of correct predictions, and consequently, better model performance. R, or sensitivity, measures the proportion of actual infected trees correctly identified by the model. A higher R value indicates a better ability to correctly predict the true positives. The F1 score is the harmonic mean of precision and recall, balancing the tradeoff between the two metrics. Since precision and recall are often inversely related, improving one typically sacrifices the other. Thus, the F1 score provides a comprehensive measure of model performance, where a higher value of F1 indicates an optimal balance between precision and recall, reflecting a more accurate and reliable model. 3. Results and Analyses 3.1. Experimental Setup The U-Net network model was conducted in Matlab software R2023b, on a computer equipped with an AMD Ryzen 7 5800H CPU, 16 GB of memory, and an NVIDIA GeForce RTX 3050 Ti graphics card. A set of training options was configured through programming, and the Stochastic Gradient Descent with Momentum (SGDM) optimization algorithm was used for training with momentum. During the training process, the base learning rate was set to 0.01, with 2000 iterations per epoch. Each iteration used a mini-batch size of 8, and MaxEpochs was set to 1. The BJ3A, BJ3N, and UAV image data were paired with the corresponding sample training set. In each iteration of every training epoch, mini-batches consisting of eight image patches of size 128 &#215; 128 pixels were fed into the network. To avoid excessive memory usage by large images and effectively augment the available training data, 16,000 mini-batches were processed per epoch. 3.2. Identifying PWN-Infected Trees Using BJ3N Imagery 3.2.1. Using Multispectral Imagery with Raw Spatial Resolution of 1.2 m The BJ3N satellite image comprises six raw multispectral bands at a resolution of 1.2 m, including red (R), green (G), blue (B), near-infrared (NIR), red-edge, and deep-blue. Four different band combinations were sequentially used as input to the training samples: the three-band (R, G, B), four-band (R, G, B, NIR), five-band (R, G, B, NIR, Red-edge), and six-band (R, G, B, NIR, Red-edge, Deep-blue) combinations. For each combination, a corresponding U-Net model was constructed under the specified training environment and parameters, incorporating the respective labeled data. The models&#8217; accuracies were evaluated using validation sets, as presented in Table 5 . Four raw band combinations of the BJ3N satellite at 1.2 m resolution showed low accuracy in identifying PWN-infected trees, and these identification results are illustrated in Figure 3 . The three-band combination with red (R), green (G), and blue (B) achieved the highest F1 score of 66.6%. The four-band combination (R, G, B, and NIR) and the five-band combination (R, G, B, NIR, and Red-edge) yielded F1 scores of 50.5% and 50.4%, respectively, while the six-band combination (R, G, B, NIR, Red-edge, and Deep-blue) performed the worst, with an F1 score of only 24%. 3.2.2. Using Multispectral Imagery with a Fused Spatial Resolution of 0.3 m The BJ3N satellite&#8217;s 0.3 m panchromatic band and six 1.2 m multispectral bands were fused using the pansharpening fusion method to generate six fused multispectral bands at a spatial resolution of 0.3 m. Like raw spatial resolution multispectral imagery, the fused three-band (R, G, B), fused four-band (R, G, B, NIR), fused five-band (R, G, B, NIR, Red-edge), and fused six-band (R, G, B, NIR, Red-edge, Deep-blue) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The models&#8217; accuracy was evaluated using validation sets, as presented in Table 5 . The accuracy of identifying PWN-infected trees using the fused 0.3 m BJ3N multispectral imagery was significantly higher than that achieved with the raw 1.2 m data, with only minor differences observed among the various band combinations. Among these, the fused five-band combination (R, G, B, NIR, Red-edge) achieved the highest F1 value of 88.9%, followed by the fused three-band (R, G, B) combination with an F1 value of 85.1%, and the fused six-band (R, G, B, NIR, Red-edge, Deep-blue) combination with an F1 value of 84.3%. The fused four-band (R, G, B, NIR) combination demonstrated the lowest precision, with an F1 value of 81.9%. All these identification results are illustrated in Figure 4 . 3.3. Identifying PWN-Infected Trees Using BJ3A Imagery 3.3.1. Using Multispectral Imagery with a Raw Spatial Resolution of 2 m The BJ3A satellite image comprises four raw multispectral bands with a spatial resolution of 2 m, including R, G, B, and NIR. During the training process, three-band combinations (R/G/B) and four-band combinations (R/G/B/NIR) serve as input variables for the U-Net model. The model was built using the predefined training environment and parameters, along with the corresponding labeled data. The models&#8217; accuracy was evaluated using the test samples, as summarized in Table 6 . Two raw band combinations of the BJ3A satellite at 2 m resolution showed limited accuracy in identifying PWN-infected trees, with F1 scores of 58% for the three-band (R, G, B) and 28% for the four-band (R, G, B, NIR) combination. All these identification results are illustrated in Figure 5 . 3.3.2. Using Multispectral Imagery with a Fused Spatial Resolution of 0.5 m The pansharpening fusion technique was applied to combine the 0.5 m panchromatic band with the four 2 m multispectral bands from the BJ3A satellite, resulting in four fused multispectral bands at 0.5 m resolution. The fused three-band (R, G, B) and fused four-band (R, G, B, NIR) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The models&#8217; accuracy was evaluated using validation sets, as presented in Table 6 , and the identification results are illustrated in Figure 6 . The accuracy of identifying PWN-infected trees was significantly improved by using 0.5 m fused band combinations of the BJ3A satellite, compared with the raw band combinations. Notably, the F1 score for the fused three-band combination of R, G, and B reached 86.3%. The fused four-band combination of R, G, B, and NIR exhibited a similar performance, with an F1 score of 85.8%, indicating comparable performance between the two band combinations in identifying PWN-infected trees. 3.4. Identifying PWN-Infected Trees Using UAV Multispectral Imagery The UAV multispectral imagery used in this study has five spectral bands: R, G, B, NIR, and Red-edge. The three-band (R, G, B), four-band (R, G, B, NIR), and five-band (R, G, B, NIR, Red-edge) combinations were subsequently employed as input to the training samples for constructing U-Net models to identify PWN-infected trees. The models were trained under the predefined environment and parameters, incorporating the corresponding labeled data. The accuracy of each model was assessed based on the validation sets, as summarized in Table 7 . Comparison with BJ3N and BJ3A satellite imagery, 0.07 m UAV multispectral imagery achieved the highest accuracy in identifying PWN-infected trees. Among UAV band combinations, there were few differences in the identification accuracy. The five-band combinations of R, G, B, NIR, and Red-edge demonstrated the highest accuracy, achieving an F1 score of 89.1%. Both the four-band combinations of R, G, B, and NIR and the three-band combinations of R, G, and B exhibited similar performance, with F1 scores of 87.7% and 88.5%, respectively. All these identification results are illustrated in Figure 7 . 4. Discussion 4.1. Effect of Spatial Resolution of Remote Sensing Images on Identification of PWN-Infected Trees When comparing the F1 accuracy of BJ3N, BJ3A, and UAV band combination in identifying PWN-infected trees separately, it was found that there was a clear relationship between spatial resolution and identifying accuracy. The UAV imagery at 0.07 m resolution achieved the highest F1 score of 89.1%, followed by the fused BJ3N imagery at 0.3 m and the fused BJ3A imagery at 0.5 m, with F1 scores of 88.9% and 88.5%, respectively. In comparison, the raw-resolution BJ3N imagery at 1.2 m and raw-resolution BJ3A imagery at 2.0 m attained maximum F1 scores of only 66.6% and 58.0%, respectively. These results demonstrate that higher spatial resolution in remote sensing imagery leads to improved accuracy in identifying PWN-infected trees. Ota et al. [ 28 ] also found that spatial resolution has a certain impact on tree species identification. The accuracy of identifying tree species using 4 m high-resolution remote sensing images is significantly higher than that using 30 m coarse resolution images. However, there is not much difference in the accuracy of identifying tree species using 25 m and 30 m resolution remote sensing images. Similarly, Zhang et al. [ 10 ] examined a sample set of semantic segmentation model input sizes and found that increasing the resolution of images enhances the recognition accuracy of each category of the semantic segmentation model, further supporting the importance of spatial resolution in remote sensing-based detection tasks. Bolch et al. [ 29 ] demonstrated that increasing the spatial resolution of hyperspectral UAV imagery improved the detection accuracy of small invasive aquatic plant patches, though at a significantly higher cost, highlighting the need to balance detection accuracy with resolution in practical applications. Similarly, Saltiel et al. [ 30 ] discovered that enhancing image spatial resolution from 22.8 cm to 7.8 cm only marginally improved wetland vegetation recognition accuracy using a deep learning semantic segmentation model, while substantially increasing processing time and reducing coverage. These findings align with the study, in which the F1 accuracy for identifying PWN-infected trees using fused 0.3 m BJ3N multispectral imagery differed by only 0.2% from that achieved with 0.07 m UAV multispectral imagery. In contrast, M&#252;llerov&#225; et al. [ 31 ] resampled 0.05 m UAV imagery into 0.5 m Pleiades remote sensing image data and found that spatial resolution had little effect on the identification of tree species. However, the study reveals that for the interval of spatial resolution between 0.07 m UAV images and 0.5 m satellite remote sensing images, there is a certain impact on recognizing tree species, with the F1 value decreasing by 2.8%. Based on the above discussion, it is evident that 0.3 m ultra-high-resolution may represent the optimal choice for detecting PWN-infected trees in a subtropical mountain monsoon climate zone. 4.2. Effect of Spectral Features of Remote Sensing Imagery on Identification of PWN-Infected Trees Based on the analysis of 0.07 m UAV imagery, the identification accuracy achieved with the three-band combination (R, G, B) was comparable to that of the four-band and five-band combinations across precision, recall, and F1-score metrics. A similar pattern was observed for both BJ3N and BJ3A imagery, where no significant differences in identification accuracy were found among the various band combinations at same spatial resolutions. Collectively, these findings indicate that the number of spectral bands has a negligible impact on the accuracy of identifying PWN-infected trees, and that a limited number of bands can be sufficient to achieve high performance. This phenomenon is supported by Lopatin et al. [ 32 ], who compared UAV RGB and hyperspectral imagery for detecting three invasive species in the forests of south-central Chile. Their study demonstrated that the recognition accuracy of RGB imagery using only three bands exceeded that of hyperspectral imagery with 41 bands for two of the invasive species. This result challenges the conventional assumption that a higher number of spectral bands necessarily leads to improved classification accuracy, suggesting instead that increasing spectral resolution does not always enhance species identification outcomes. Therefore, the number of spectral features is not a critical factor in identifying PWN-infected trees, and a three-band combination (R, G, B) can be sufficient for effective detection using high-resolution remote sensing imagery. 5. Conclusions This study investigates the identification of PWN-infected trees using ultra-high-resolution optical remote sensing satellite images. By comparing the accuracy of identifying PWN-infected trees using BJ3N, as well as BJ3N and UAV imagery, it was found that the higher the spatial resolution, the higher the accuracy of identifying PWN-infected trees. The significance of spectral features in identifying PWN-infected trees varies among different spatial resolution remote sensing images. Nevertheless, the impact of differences in spectral resolution diminishes for identifying PWN-infected trees at higher spatial resolutions. The ultra-high-resolution BJ3N remote sensing imagery with a spatial resolution of 0.3 m exhibited lower false detection and missed detection rates when identifying trees affected by pine wilt nematode (PWN), indicating that a spatial resolution of 0.3 m is sufficient for detecting PWN-affected trees and is the closest to the resolution saturation point in a subtropical mountain monsoon climate zone. Therefore, ultra-high-resolution optical remote sensing satellite imagery clearly demonstrates the optimal resolution for detecting PWN-infected trees, while also offering advantages such as repeatable data acquisition and extensive spatial coverage. Moreover, ultra-high-resolution remote sensing can be applied to forest disease monitoring and prevention strategies to enhance the practical value of research. Disclaimer/Publisher&#8217;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content. Author Contributions Conceptualization, C.W., L.Q. and P.X.; formal analysis, Z.N. and C.W.; funding acquisition, C.W. and L.Q.; investigation, Z.N. and X.M. (Xianjin Meng); methodology, Z.N. and X.M. (Xuelian Meng); software, Z.N.; supervision, Z.N. and L.Q.; validation, Z.N. and X.M. (Xuelian Meng); visualization, C.W. and L.Q.; writing&#8212;original draft, Z.N. and K.Q.; writing&#8212;review and editing, Z.N., L.Q., P.X., X.M. (Xuelian Meng), X.M. (Xianjin Meng), K.Q. and C.W. All authors have read and agreed to the published version of the manuscript. Data Availability Statement The data supporting the study findings are available on request from the corresponding author. The data are not publicly available due to the authors&#8217; plan to conduct a series of follow-up studies based on this dataset. Conflicts of Interest Author Ziqi Nie was employed by the company Aerial Photogrammetry and Remote Sensing Group Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Abbreviations The following abbreviations are used in this manuscript: PWN Pine wood nematode BJ3N Beijing 3 International Cooperative Remote Sensing Satellite BJ3A Beijing 3A satellite UAV Unmanned aerial vehicle ANNs artificial neural networks SGDM Stochastic Gradient Descent with Momentum References 1. Ye J. Epidemic Status of Pine Wilt Disease in China and Its Prevention and Control Techniques and Counter Measures Sci. Silvae Sin. 2019 55 1 10 10.11707/j.1001-7488.20190901 2. Suzuki K. Pine Wilt Disease&#8212;A Threat to Pine Forests in Europe The Pinewood Nematode, Bursaphelenchus xylophilus Mota M. Vieira P. BRILL Leiden, The Netherlands 2004 25 30 10.1163/9789047413097_008 978-90-474-1309-7 3. Qin J. Wang B. Wu Y. Lu Q. Zhu H. Identifying Pine Wood Nematode Disease Using UAV Images and Deep Learning Algorithms Remote Sens. 2021 13 162 10.3390/rs13020162 4. Giordan D. Manconi A. Tannant D.D. Allasia P. UAV: Low-Cost Remote Sensing for High-Resolution Investigation of Landslides Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS) Milan, Italy 26&#8211;31 July 2025 IEEE Milan, Italy 2015 5344 5347 10.1109/IGARSS.2015.7327042 5. Li M. Li H. Ding X. Wang L. Wang X. Chen F. The Detection of Pine Wilt Disease: A Literature Review Int. J. Mol. Sci. 2022 23 10797 10.3390/ijms231810797 36142710 PMC9505960 6. Wulder M. Dymond C.C. White J.C. Erickson R.D. Detection, Mapping, and Monitoring of the Mountain Pine Beetle The Mountain Pine Beetle: A Synthesis of Biology, Management, and Impacts on Lodgepole Pine Natural Resources Canada, Canadian Forest Service, Pacific Forestry Centre Victoria, BC, Canada 2006 978-0-662-42623-3 7. Kim S.-R. Lee W.-K. Lim C.-H. Kim M. Kafatos M. Lee S.-H. Lee S.-S. Hyperspectral Analysis of Pine Wilt Disease to Determine an Optimal Detection Index Forests 2018 9 115 10.3390/f9030115 8. Zhou X. Liao L. Cheng D. Chen X. Huang Q. Extraction of the individual tree infected by pine wilt disease using unmanned aerial vehicle optical imagery Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2020 XLIII-B3-2020 247 252 10.5194/isprs-archives-XLIII-B3-2020-247-2020 9. Zhang Z. Wang B. Chen W. Wu Y. Qin J. Chen P. Sun H. He A. Recognition of Abnormal Individuals Based on Lightweight Deep Learning Using Aerial Images in Complex Forest Landscapes: A Case Study of Pine Wood Nematode Remote Sens. 2023 15 1181 10.3390/rs15051181 10. Zhang B. Ye H. Lu W. Huang W. Wu B. Hao Z. Sun H. A Spatiotemporal Change Detection Method for Monitoring Pine Wilt Disease in a Complex Landscape Using High-Resolution Remote Sensing Imagery Remote Sens. 2021 13 2083 10.3390/rs13112083 11. Kim J.-B. Jo M.-H. Kim I.-H. Kim Y.-K. A Study on the Extraction of Damaged Area by Pine Wood Nematode Using High Resolution IKONOS Satellite Images and GPS J. Korean Soc. For. Sci. 2003 92 362 366 12. White J. Wulder M. Brooks D. Reich R. Wheate R. Detection of Red Attack Stage Mountain Pine Beetle Infestation with High Spatial Resolution Satellite Imagery Remote Sens. Environ. 2005 96 340 351 10.1016/j.rse.2005.03.007 13. Franklin S.E. Wulder M.A. Skakun R.S. Carroll A.L. Mountain Pine Beetle Red-Attack Forest Damage Classification Using Stratified Landsat TM Data in British Columbia, Canada Photogramm. Eng. Remote Sens. 2003 69 283 288 10.14358/PERS.69.3.283 14. Skakun R.S. Wulder M.A. Franklin S.E. Sensitivity of the Thematic Mapper Enhanced Wetness Difference Index to Detect Mountain Pine Beetle Red-Attack Damage Remote Sens. Environ. 2003 86 433 443 10.1016/S0034-4257(03)00112-3 15. Wang J. Zhao J. Sun H. Lu X. Huang J. Wang S. Fang G. Satellite Remote Sensing Identification of Discolored Standing Trees for Pine Wilt Disease Based on Semi-Supervised Deep Learning Remote Sens. 2022 14 5936 10.3390/rs14235936 16. Poona N.K. Ismail R. Discriminating the Occurrence of Pitch Canker Infection in Pinus Radiata Forests Using High Spatial Resolution QuickBird Data and Artificial Neural Networks Proceedings of the 2012 IEEE International Geoscience and Remote Sensing Symposium Munich, Germany 22&#8211;27 July 2012 IEEE Munich, Germany 2012 3371 3374 10.1109/IGARSS.2012.6350698 17. Takenaka Y. Katoh M. Deng S. Cheung K. Detecting forests damaged by pine wilt disease at the individual tree level using airborne laser data and worldview-2/3 images over two seasons Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017 XLII-3/W3 181 184 10.5194/isprs-archives-XLII-3-W3-181-2017 18. Vollenweider P. G&#252;nthardt-Goerg M.S. Diagnosis of Abiotic and Biotic Stress Factors Using the Visible Symptoms in Foliage Environ. Pollut. 2005 137 455 465 10.1016/j.envpol.2005.01.032 16005758 19. Wang H. Shu Q. Wu Q. Liu Y. Ji Y. Application of Imaging Spectral Remote Sensing Techniques in Monitoring of Forestry Disease and Insect Pests China Plant Prot. 2018 38 24 28 10.3969/j.issn.1672-6820.2018.01.004 20. Zhang S. Qin J. Tang X. Wang Y. Huang J. Song Q. Min J. Spectral Characteristics and Evaluation Model of Pinus massoniana Suffering from Bursaphelenchus xylophilus Disease Spectrosc. Spectr. Anal. 2019 39 865 872 10.3964/j.issn.1000-0593(2019)03-0865-08 21. Huang J. Lu X. Chen L. Sun H. Wang S. Fang G. Accurate Identification of Pine Wood Nematode Disease with a Deep Convolution Neural Network Remote Sens. 2022 14 913 10.3390/rs14040913 22. Zhu X. Wang R. Shi W. Yu Q. Li X. Chen X. Automatic Detection and Classification of Dead Nematode-Infested Pine Wood in Stages Based on YOLO v4 and GoogLeNet Forests 2023 14 601 10.3390/f14030601 23. Ye W. Lao J. Liu Y. Chang C.-C. Zhang Z. Li H. Zhou H. Pine Pest Detection Using Remote Sensing Satellite Images Combined with a Multi-Scale Attention-UNet Model Ecol. Inform. 2022 72 101906 10.1016/j.ecoinf.2022.101906 24. Han Z. Hu W. Peng S. Lin H. Zhang J. Zhou J. Wang P. Dian Y. Detection of Standing Dead Trees after Pine Wilt Disease Outbreak with Airborne Remote Sensing Imagery by Multi-Scale Spatial Attention Deep Learning and Gaussian Kernel Approach Remote Sens. 2022 14 3075 10.3390/rs14133075 25. Ronneberger O. Fischer P. Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#8212;MICCAI 2015 Munich, Germany 5&#8211;9 October 2015 Springer International Publishing Cham, Switzerland 2015 Volume 9351 234 241 10.1007/978-3-319-24574-4_28 26. Zhu Y. Xu Q. Yang J. Mo H. Full Convolution Neural Network Based Building Extraction Approach from High Resolution Aerial Image Geomat. World 2020 27 101 106 10.3969/j.issn.1672-1586.2020.02.017 27. Zhou Z. Machine Learning Tsinghua University Press Beijing, China 2016 28. Ota T. Mizoue N. Yoshida S. Influence of Using Texture Information in Remote Sensed Data on the Accuracy of Forest Type Classification at Different Levels of Spatial Resolution J. For. Res. 2011 16 432 437 10.1007/s10310-010-0233-6 29. Bolch E.A. Hestir E.L. Khanna S. Performance and Feasibility of Drone-Mounted Imaging Spectroscopy for Invasive Aquatic Vegetation Detection Remote Sens. 2021 13 582 10.3390/rs13040582 30. Saltiel T.M. Dennison P.E. Campbell M.J. Thompson T.R. Hambrecht K.R. Tradeoffs between UAS Spatial Resolution and Accuracy for Deep Learning Semantic Segmentation Applied to Wetland Vegetation Species Mapping Remote Sens. 2022 14 2703 10.3390/rs14112703 31. M&#252;llerov&#225; J. Br&#367;na J. Bartalo&#353; T. Dvo&#345;&#225;k P. V&#237;tkov&#225; M. Py&#353;ek P. Timing Is Important: Unmanned Aircraft vs. Satellite Imagery in Plant Invasion Monitoring Front. Plant Sci. 2017 8 887 10.3389/fpls.2017.00887 28620399 PMC5449470 32. Lopatin J. Dolos K. Kattenborn T. Fassnacht F.E. How Canopy Shadow Affects Invasive Plant Species Classification in High Spatial Resolution Remote Sensing Remote Sens. Ecol. Conserv. 2019 5 302 317 10.1002/rse2.109 Figure 1 The location of the study area in Shaoguan City, Guandong Province, China. Figure 2 Checkpoint data imagery. The numbers in the figures are the numbers of Validation Sets. Figure 3 Results of BJ3N satellite with 0.5 m raw spatial resolution for identifying PWN-infected trees in this study area. Figure 4 Results of BJ3N satellite with 0.3 m fused spatial resolution for identifying PWN-infected trees in this study area. Figure 5 Results of the BJ3A satellite with 2 m raw spatial resolution for identifying PWN-infected trees in this study area. Figure 6 Results of the BJ3A satellite with 0.5 m fused spatial resolution for identifying PWN-infected trees in this study area. Figure 7 Results of UAV multispectral bands with a spatial resolution of 0.7 m for identifying PWN-infected trees in this study area. plants-14-03436-t001_Table 1 Table 1 BJ3N satellite parameters. Category Parameter Satellite Orbit Sun-synchronous orbit Orbit Altitude 620 km Spatial Resolution Panchromatic 0.3 m Multispectral 1.2 m Spectral Bands Panchromatic 450&#8211;800 nm Multispectral Deep Blue: 400&#8211;450 nm Blue: 450&#8211;520 nm Green: 530&#8211;590 nm Red: 620&#8211;690 nm Red Edge: 700&#8211;750 nm Near-Infrared: 770&#8211;880 nm plants-14-03436-t002_Table 2 Table 2 BJ3A satellite parameters. Category Parameter Satellite Orbit Sun-synchronous orbit Orbit Altitude 500 km Spatial Resolution Panchromatic 0.5 m Multispectral 2.0 m Spectral Bands Panchromatic 450&#8211;700 nm Multispectral Blue: 450&#8211;520 nm Green: 520&#8211;590 nm Red: 630&#8211;690 nm Near-Infrared: 770&#8211;890 nm plants-14-03436-t003_Table 3 Table 3 Characteristics of the multispectral sensor. Category Parameter Lens FOV: 62.7&#176;; Focal length: 5.74 mm; Fixed focus at infinity; Aperture: f/2.2 Imaging Sensor 1/2.9inch CMOS Including 1 color sensor for visible light imaging and 5 monochrome sensors for multispectral imaging Individual Sensor Effective pixels: 2.08 million (total pixels: 2.12 million) Spectral bands Blue: 434&#8211;466 nm Green: 544&#8211;576 nm Red: 634&#8211;666 nm Red Edge: 714&#8211;746 nm Near-Infrared: 814&#8211;866 nm plants-14-03436-t004_Table 4 Table 4 Sample area and label drawing results of BJ3A images, BJ3N images, and UAV imagery. Image Type Sample Area Label Drawing Results BJ3A imagery BJ3N imagery UAV imagery Note: The sample labels have been categorized into five groups: land is represented by yellow, other features by orange, lakes by blue, healthy trees by green, and PWN-infected trees by red. plants-14-03436-t005_Table 5 Table 5 The accuracy of identifying PWN-infected trees using the BJ3N satellite&#8217;s multispectral bands with a raw spatial resolution of 1.2 m. Image Type Resolution/m Band Type P/% R/% F1/% raw spatial resolution 1.2 R, G, B, NIR, Red-edge, Deep-blue 25.8 22.5 24 R, G, B, NIR, Red-edge 41.3 64.7 50.4 R, G, B, NIR 52.1 49 50.5 R, G, B 71.1 62.7 66.6 fused spatial resolution 0.3 R, G, B, NIR, Red-edge, Deep-blue 84.3 84.3 84.3 R, G, B, NIR, Red-edge 91.7 86.3 88.9 R, G, B, NIR 82.7 81.1 81.9 R, G, B 86 84.3 85.1 plants-14-03436-t006_Table 6 Table 6 The accuracy of identifying PWN-infected trees using the BJ3A satellite&#8217;s multispectral bands with a raw spatial resolution of 2 m. Image Type Resolution/m Band Type P/% R/% F1/% raw spatial resolution 2 R, G, B, NIR 22.4 37.3 28 R, G, B 64.3 52.9 58 fused spatial resolution 0.5 R, G, B, NIR 89.4 82.4 85.8 R, G, B 93.2 80.4 86.3 plants-14-03436-t007_Table 7 Table 7 The accuracy of identifying PWN-infected trees using the UAV multispectral bands with a spatial resolution of 0.7 m. Image Type Resolution/m Band Type P/% R/% F1/% UAV multispectral imagery 0.07 R, G, B, NIR, Red-edge 83.1 96.1 89.1 R, G, B, NIR 79.4 98 87.7 R, G, B 80.6 98 88.5"
}